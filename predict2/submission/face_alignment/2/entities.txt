2	40	54	Face Alignment
4	0	14	Face Alignment
22	19	28	introduce
22	62	87	Face Alignment ( DeCaFA )
23	0	6	DeCaFA
23	10	21	composed of
23	22	36	several stages
23	55	85	landmark - wise attention maps
23	88	101	relatively to
23	102	134	heterogeneous annotation markups
25	3	14	illustrates
25	25	39	attention maps
25	44	51	refined
25	52	59	through
25	64	81	successive stages
25	96	122	different prediction tasks
25	127	139	benefit from
25	140	150	each other
130	4	17	DeCaFA models
130	50	53	use
130	54	67	1 to 4 stages
130	87	114	12 3 3 convolutional layers
130	115	119	with
130	120	160	64 ? 64 ? 128 ? 128 ? 256 ? 256 channels
130	161	164	for
130	169	189	downsampling portion
130	209	212	for
130	217	235	upsampling portion
131	4	16	input images
131	21	31	resized to
131	32	56	128 128 grayscale images
131	72	84	processed by
131	89	96	network
132	0	16	Each convolution
132	20	31	followed by
132	34	59	batch normalization layer
132	60	64	with
132	65	80	ReLU activation
133	9	20	to generate
133	21	40	smooth feature maps
133	44	54	do not use
133	55	77	transposed convolution
133	82	107	bilinear image upsampling
133	108	121	followed with
133	122	146	3 3 convolutional layers
134	4	22	whole architecture
134	26	39	trained using
134	40	54	ADAM optimizer
134	55	59	with
134	62	82	5e ? 4 learning rate
134	83	87	with
134	88	100	momentum 0.9
134	105	118	learning rate
134	105	128	learning rate annealing
134	119	128	annealing
134	129	133	with
134	134	143	power 0.9
135	3	8	apply
135	9	23	400000 updates
135	24	28	with
135	29	39	batch size
135	29	41	batch size 8
135	40	41	8
135	42	45	for
135	46	59	each database
135	62	66	with
135	67	86	alternating updates
135	87	94	between
135	99	108	databases
153	4	12	accuracy
153	13	31	steadily increases
153	38	41	add
153	42	53	more stages
153	60	69	saturates
153	70	75	after
153	80	85	third
153	86	88	on
153	89	103	LFPW and HELEN
153	237	244	cascade
156	0	39	Coarsely annotated data ( 5 landmarks )
156	40	59	significantly helps
156	64	100	fine - grained landmark localization
160	8	19	reinjecting
160	24	78	whole input image ( F 3 - Equation vs F 2 - Equation )
160	79	101	significantly improves
160	106	114	accuracy
160	115	117	on
160	118	134	challenging data
160	135	142	such as
160	143	162	300 W - challenging
160	166	177	WFLW - pose
161	56	61	using
161	62	88	local + global information
161	89	95	rivals
162	14	35	F 5 - Equation fusion
162	44	48	uses
162	49	70	local and global cues
162	71	73	is
162	78	82	best
162	83	85	by
162	88	106	significant margin
163	14	22	chaining
163	27	42	transfer layers
163	43	45	is
163	46	63	better than using
163	64	91	independant transfer layers
163	126	146	first transfer layer
163	147	160	benefits from
163	165	174	gradients
163	175	179	from
163	184	201	subsequents layer
163	202	204	at
163	205	215	train time
176	29	31	of
176	81	83	on
176	84	91	Celeb A
181	10	16	DeCaFA
181	17	21	sets
181	24	50	new state - of - the - art
181	51	53	on
181	58	73	three databases
