2	47	66	Text Classification
9	61	108	https://github.com/wikipedia2vec/wikipedia2vec.
21	11	19	proposes
21	24	73	Neural Attentive Bagof - Entities ( NABoE ) model
21	82	84	is
21	87	107	neural network model
21	113	122	addresses
21	127	154	text classification problem
21	155	166	by modeling
21	171	180	semantics
21	181	183	in
21	188	204	target documents
21	205	210	using
21	211	219	entities
21	220	222	in
21	227	229	KB
22	0	3	For
22	4	20	each entity name
22	21	23	in
22	26	34	document
22	68	81	first detects
22	82	90	entities
22	102	116	referred to by
22	177	187	represents
22	192	200	document
22	201	206	using
22	211	227	weighted average
22	228	230	of
22	235	245	embeddings
22	246	248	of
22	255	263	entities
23	4	11	weights
23	16	30	computed using
23	33	65	novel neural attention mechanism
23	71	78	enables
23	83	88	model
23	89	100	to focus on
23	103	115	small subset
23	116	118	of
23	123	131	entities
23	132	140	that are
23	141	155	less ambiguous
23	156	158	in
23	159	166	meaning
23	171	187	more relevant to
23	192	200	document
24	21	40	attention mechanism
24	44	63	designed to compute
24	64	71	weights
24	72	93	by jointly addressing
24	94	144	entity linking and entity salience detection tasks
81	14	27	trained using
81	28	42	mini-batch SGD
81	43	47	with
81	52	65	learning rate
81	66	79	controlled by
81	80	84	Adam
81	93	108	mini-batch size
81	109	115	set to
81	116	118	32
83	4	8	size
83	9	11	of
83	16	26	embeddings
83	27	29	of
83	30	48	words and entities
83	53	59	set to
83	60	67	d = 300
86	0	9	Baselines
88	0	3	BoW
89	14	22	based on
89	25	55	logistic regression classifier
89	56	60	with
89	61	93	conventional binary BoW features
90	0	9	FTS- BRNN
91	14	22	based on
91	25	42	bidirectional RNN
91	43	47	with
91	48	77	gated recurrent units ( GRU )
93	0	4	NTEE
93	16	18	is
93	21	49	state - of - the - art model
93	55	59	uses
93	62	97	multi - layer perceptron classifier
93	98	102	with
93	107	115	features
93	116	130	computed using
93	149	167	words and entities
93	168	178	trained on
93	179	188	Wikipedia
93	189	194	using
93	199	219	neural network model
101	0	11	Relative to
101	16	25	baselines
101	28	38	our models
101	39	46	yielded
101	47	76	enhanced over all performance
101	77	79	on
101	80	93	both datasets
102	4	22	NABoE - full model
102	23	35	outperformed
102	36	55	all baseline models
102	56	67	in terms of
102	68	81	both measures
102	82	84	on
102	85	98	both datasets
103	18	36	NABoE-entity model
103	37	49	outperformed
103	50	73	all the baseline models
103	74	85	in terms of
103	86	99	both measures
103	100	102	on
103	107	119	20NG dataset
103	130	139	F 1 score
103	140	142	on
103	147	157	R8 dataset
