2	53	72	Text Classification
30	19	30	investigate
30	31	44	modifications
30	45	47	on
30	52	59	network
30	103	111	reducing
30	116	136	number of parameters
30	164	168	with
30	169	200	minimal performance degradation
31	33	37	used
31	38	78	Temporal Depthwise Separable Convolution
31	83	116	Global Average Pooling techniques
32	40	47	propose
32	52	111	Squeezed Very Deep Convolutional Neural Networks ( SVDCNN )
32	116	141	text classification model
32	148	156	requires
32	157	187	significantly fewer parameters
32	188	199	compared to
32	204	228	stateof - the - art CNNs
141	0	3	For
141	4	25	SVDCNN and Char - CNN
141	31	41	calculated
141	68	72	from
141	77	97	network architecture
141	98	112	implemented in
141	113	120	PyTorch
146	4	32	SVDCNN experimental settings
146	37	47	similar to
146	75	80	using
146	85	100	same dictionary
146	109	128	same embedding size
146	129	131	of
146	132	134	16
147	4	12	training
147	21	35	performed with
147	36	39	SGD
147	42	51	utilizing
147	52	62	size batch
147	63	65	of
147	66	68	64
147	71	75	with
147	78	99	maximum of 100 epochs
148	10	31	initial learning rate
148	32	34	of
148	35	39	0.01
148	44	52	momentum
148	53	55	of
148	56	59	0.9
148	66	78	weight decay
148	79	81	of
148	82	87	0.001
149	25	37	performed on
149	41	60	NVIDIA GTX 1060 GPU
149	63	87	Intel Core i 7 4770s CPU
158	4	21	network reduction
158	22	33	obtained by
158	38	41	GAP
158	42	44	is
158	45	69	even more representative
159	0	11	Considering
159	14	21	dataset
159	22	26	with
159	27	46	four target classes
159	53	62	comparing
159	63	69	SVDCNN
159	70	74	with
159	75	80	VDCNN
159	87	107	number of parameters
159	108	110	of
159	115	124	FC layers
159	136	140	from
159	141	173	12.59 to 0.02 million parameters
159	176	188	representing
159	191	200	reduction
159	201	203	of
159	204	211	99.84 %
160	44	54	Char - CNN
160	61	75	proposed model
160	76	78	is
160	102	109	against
160	110	123	11.36 million
160	124	126	of
160	127	140	FC parameters
162	10	31	most in - depth model
162	39	47	occupies
162	60	65	VDCNN
162	66	70	with
162	75	85	same depth
162	86	94	occupies
162	95	104	64. 16 MB
162	105	107	of
162	108	115	storage
166	10	26	accuracy results
166	114	130	in comparison to
166	135	149	original model
167	19	41	performance difference
167	42	49	between
167	50	73	VDCNN and SVDCNN models
167	74	88	varies between
167	89	102	0.4 and 1.3 %
