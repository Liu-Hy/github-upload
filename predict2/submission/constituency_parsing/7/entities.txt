4	0	42	Recurrent neural network grammars ( RNNG )
12	3	11	focus on
12	12	17	RNNGs
12	18	20	as
12	21	52	generative probabilistic models
12	53	57	over
12	58	63	trees
21	11	22	manipulates
21	27	41	inductive bias
21	42	44	of
21	45	50	RNNGs
21	51	58	to test
21	59	80	linguistic hypotheses
22	3	13	begin with
22	32	43	to discover
22	48	58	importance
22	59	61	of
22	66	86	composition function
23	27	34	augment
23	39	64	RNNG composition function
23	65	69	with
23	72	103	novel gated attention mechanism
23	106	116	leading to
23	121	130	GA - RNNG
23	133	147	to incorporate
23	148	169	more interpretability
23	170	174	into
23	179	184	model
24	0	5	Using
24	10	19	GA - RNNG
24	36	49	investigating
24	54	58	role
24	64	80	individual heads
24	81	85	play
24	86	88	in
24	89	111	phrasal representation
24	136	163	nonterminal category labels
24	164	168	play
88	4	8	RNNG
88	9	13	with
88	14	26	only a stack
88	27	29	is
88	51	60	ablations
88	75	86	outperforms
88	91	104	" full " RNNG
88	105	109	with
88	110	135	all three data structures
89	0	8	Ablating
89	13	18	stack
89	19	24	gives
89	29	34	worst
89	35	40	among
89	45	56	new results
93	45	62	language modeling
93	69	86	stack - only RNNG
93	87	95	achieves
93	100	116	best performance
93	123	131	ablating
93	136	141	stack
93	142	144	is
93	145	157	most harmful
94	9	17	modeling
94	18	24	syntax
94	25	32	without
94	33	53	explicit composition
94	80	88	provides
94	89	103	little benefit
94	104	108	over
94	111	141	sequential LSTM language model
95	3	9	remark
95	19	39	stack - only results
95	40	43	are
95	48	74	best published PTB results
95	75	78	for
95	84	122	phrasestructure and dependency parsing
95	123	128	among
95	129	146	supervised models
96	0	20	Gated Attention RNNG
134	21	26	model
134	27	38	outperforms
134	43	56	baseline RNNG
134	57	61	with
134	62	82	all three structures
134	95	103	achieves
134	104	127	competitive performance
134	128	132	with
134	137	176	strongest , stack - only , RNNG variant
135	0	21	Headedness in Phrases
173	16	30	higher overlap
173	31	35	with
173	40	50	conversion
173	51	56	using
173	57	88	Collins head rules ( 49.8 UAS )
173	89	100	rather than
173	127	137	40.4 UAS )
175	17	46	attention - based tree output
175	53	79	high error rate ( ? 90 % )
175	80	84	when
175	89	98	dependent
175	99	101	is
175	104	108	verb
176	4	23	conversion accuracy
176	27	37	better for
176	38	60	nouns ( ? 50 % error )
176	67	78	much better
176	67	82	much better for
176	79	82	for
176	83	125	determiners ( 30 % ) and particles ( 6 % )
176	126	141	with respect to
176	146	164	Collins head rules
192	0	2	On
192	3	12	test data
192	15	19	with
192	24	35	usual split
192	44	53	GA - RNNG
192	54	62	achieves
192	63	69	94.2 %
192	82	95	U - GA - RNNG
192	96	104	achieves
192	105	111	93.5 %
