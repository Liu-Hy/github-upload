4	30	41	pretraining
4	44	76	bi-directional transformer model
4	82	90	provides
4	91	120	significant performance gains
4	121	127	across
9	0	26	Language model pretraining
11	19	23	show
11	29	58	even larger performance gains
11	63	74	possible by
11	75	94	jointly pretraining
11	95	110	both directions
11	111	113	of
11	116	178	large language - model - inspired self - attention cloze model
12	4	43	bi-directional transformer architecture
12	44	52	predicts
12	53	64	every token
12	65	67	in
12	72	85	training data
13	19	30	introducing
13	33	65	cloze - style training objective
13	66	71	where
13	76	81	model
13	99	110	center word
13	111	116	given
13	117	180	left - to - right and right - to - left context representations
14	10	29	separately computes
14	35	62	forward and backward states
128	0	10	CNN models
128	11	14	use
128	18	34	adaptive softmax
128	35	37	in
128	42	48	output
128	55	63	headband
128	64	72	contains
128	77	100	60K most frequent types
128	101	105	with
128	106	120	dimensionality
128	121	125	1024
128	128	139	followed by
128	142	152	160 K band
128	153	157	with
128	158	172	dimensionality
128	173	176	256
128	179	183	with
128	186	194	momentum
128	195	197	of
128	198	202	0.99
128	210	221	renormalize
128	222	231	gradients
128	232	234	if
128	241	245	norm
128	246	253	exceeds
128	254	257	0.1
129	4	17	learning rate
129	21	39	linearly warmed up
129	40	44	from
129	45	56	10 ? 7 to 1
129	57	60	for
129	61	71	16 K steps
129	81	95	annealed using
129	98	127	cosine learning rate schedule
129	128	132	with
129	135	147	single phase
129	148	150	to
129	151	157	0.0001
130	3	6	run
130	7	18	experiments
130	19	21	on
130	22	38	DGX - 1 machines
130	39	43	with
130	44	62	8 NVIDIA V100 GPUs
130	80	97	interconnected by
130	98	108	Infiniband
131	8	11	use
131	16	29	NCCL2 library
131	38	43	torch
136	0	4	GLUE
151	15	25	outperform
151	30	72	uni-directional transformer ( OpenAI GPT )
153	0	18	Our CNN base model
153	19	27	performs
153	28	38	as well as
153	39	45	STILTs
153	46	48	in
153	49	58	aggregate
153	71	73	on
153	114	120	STILTs
153	121	129	performs
153	130	141	much better
153	144	148	MRPC
163	0	24	Named Entity Recognition
167	20	38	with comparison to
167	90	101	fine tuning
167	102	107	gives
167	112	124	biggest gain
168	0	20	Constituency Parsing
180	5	15	results in
180	20	29	bilm loss
180	30	40	dominating
180	45	57	triplet loss
180	76	83	scaling
180	88	97	bilm term
180	98	100	by
180	103	109	factor
180	110	112	of
180	113	117	0.15
180	118	128	results in
180	129	147	better performance
181	0	5	shows
181	15	25	cloze loss
181	26	34	performs
181	35	55	significantly better
181	56	60	than
181	65	74	bilm loss
181	84	93	combining
181	98	112	two loss types
181	130	134	over
181	139	149	cloze loss
