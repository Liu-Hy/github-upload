(BERT||used||model)
(model||named||bert - largecased)
(bert - largecased||of||PyTorch implementation)
(bert - largecased||consists of||vectors)
(PyTorch implementation||consists of||vectors)
(model||name||bert - largecased)
(Experimental setup||For||BERT)
(Transformer encoder layers||used||same parameters)
(same parameters||as||" base " model)
(6 layers||with||8 attention heads)
(6 layers||with||dropout)
(hidden size||of||2048)
(dropout||of||0.1)
(Transformer encoder layers||has||same parameters)
(same parameters||has||" base " model)
(Experimental setup||For||Transformer encoder layers)
(Contribution||has||Experimental setup)
