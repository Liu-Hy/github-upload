2	27	59	Neural Word Sense Disambiguation
4	0	33	Word Sense Disambiguation ( WSD )
5	0	17	Lexical resources
5	18	22	like
5	23	30	WordNet
5	68	71	for
5	72	75	WSD
31	19	26	propose
31	29	44	novel model GAS
31	49	85	gloss - augmented WSD neural network
31	97	107	variant of
31	112	126	memory network
32	0	3	GAS
32	4	19	jointly encodes
32	24	43	context and glosses
32	44	46	of
32	51	62	target word
32	67	73	models
32	78	99	semantic relationship
32	100	107	between
32	112	131	context and glosses
32	132	134	in
32	139	152	memory module
33	24	42	inner relationship
33	43	50	between
33	51	70	glosses and context
33	92	98	employ
33	99	124	multiple passes operation
33	125	131	within
33	136	142	memory
33	143	145	as
33	150	168	re-reading process
33	173	178	adopt
33	179	209	two memory updating mechanisms
196	3	6	use
196	7	34	pre-trained word embeddings
196	35	39	with
196	40	54	300 dimensions
196	74	79	fixed
196	91	107	training process
197	3	9	employ
197	10	26	256 hidden units
197	27	29	in
197	60	74	context module
198	0	25	Orthogonal initialization
198	29	37	used for
198	38	45	weights
198	46	48	in
198	49	53	LSTM
198	58	87	random uniform initialization
198	88	92	with
198	93	114	range [ - 0.1 , 0.1 ]
198	118	126	used for
198	127	133	others
199	3	9	assign
199	10	33	gloss expansion depth K
199	38	43	value
199	47	48	4
200	8	23	experiment with
200	28	52	number of passes | T M |
200	53	57	from
200	58	64	1 to 5
200	65	67	in
200	68	81	our framework
200	84	91	finding
200	104	112	performs
200	113	117	best
201	3	6	use
201	7	21	Adam optimizer
201	22	24	in
201	29	45	training process
201	46	50	with
201	51	78	0.001 initial learning rate
202	9	17	to avoid
202	18	29	overfitting
202	35	38	use
202	39	61	dropout regularization
202	66	69	set
202	70	79	drop rate
202	80	82	to
202	83	86	0.5
203	0	8	Training
203	14	17	for
203	18	34	up to 100 epochs
203	35	39	with
203	40	54	early stopping
203	55	57	if
203	62	77	validation loss
203	78	94	does n't improve
203	95	101	within
203	106	120	last 10 epochs
228	0	27	English all - words results
233	0	15	GAS and GAS ext
233	16	24	achieves
233	29	60	state - of - theart performance
233	61	63	on
233	68	84	concatenation of
233	85	102	all test datasets
237	7	15	performs
237	16	20	best
237	21	23	on
237	24	41	all the test sets
237	54	63	find that
237	64	71	GAS ext
237	72	76	with
237	77	115	concatenation memory updating strategy
237	116	124	achieves
237	129	141	best results
237	142	146	70.6
237	147	149	on
237	154	170	concatenation of
237	175	193	four test datasets
238	0	13	Compared with
238	14	48	other three neural - based methods
238	78	87	find that
238	88	102	our best model
238	103	114	outperforms
238	119	154	previous best neural network models
238	155	157	on
238	158	183	every individual test set
240	34	38	beat
240	39	48	IMS + emb
240	49	51	on
240	56	85	SE3 , SE13 and SE15 test sets
241	0	13	Incorporating
241	14	21	glosses
241	22	26	into
241	27	37	neural WSD
241	42	57	greatly improve
241	62	73	performance
241	78	87	extending
241	92	106	original gloss
241	111	124	further boost
241	129	136	results
242	0	13	Compared with
242	18	36	Bi - LSTM baseline
242	53	65	labeled data
242	68	86	our proposed model
242	87	103	greatly improves
242	108	116	WSD task
242	117	119	by
242	120	136	2.2 % F1 - score
242	137	153	with the help of
242	154	169	gloss knowledge
243	14	27	compared with
243	32	35	GAS
243	52	66	original gloss
243	67	69	as
243	74	94	background knowledge
243	97	100	GAS
243	109	124	further improve
243	129	140	performance
243	141	157	with the help of
243	162	178	extended glosses
243	179	186	through
243	191	209	semantic relations
