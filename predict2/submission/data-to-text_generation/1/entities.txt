2	46	98	Sequence - to - Sequence Natural Language Generation
4	0	27	Natural language generation
28	8	15	present
28	18	60	neural ensemble natural language generator
28	72	89	train and test on
28	90	120	three large unaligned datasets
28	121	123	in
28	128	172	restaurant , television , and laptop domains
29	3	10	explore
29	11	21	novel ways
29	22	34	to represent
29	39	48	MR inputs
29	51	60	including
29	61	74	novel methods
29	75	93	for delexicalizing
29	119	143	automatic slot alignment
29	170	187	semantic reranker
158	3	8	built
158	13	27	ensemble model
158	28	33	using
158	38	55	seq2seq framework
158	56	59	for
158	60	70	TensorFlow
159	4	26	individual LSTM models
159	27	30	use
159	33	59	bidirectional LSTM encoder
159	60	64	with
159	65	84	512 cells per layer
159	95	105	CNN models
159	106	109	use
159	112	127	pooling encoder
160	4	11	decoder
160	12	14	in
160	15	25	all models
160	26	29	was
160	32	53	4 - layer RNN decoder
160	54	58	with
160	59	83	512 LSTM cells per layer
160	88	92	with
160	93	102	attention
165	19	30	E2E Dataset
170	0	27	Automatic Metric Evaluation
172	15	19	show
172	34	57	LSTM and the CNN models
172	66	78	benefit from
172	79	106	additional pseudo - samples
172	107	109	in
172	114	126	training set
180	0	2	On
180	7	28	official E2E test set
180	31	49	our ensemble model
180	50	58	performs
180	59	69	comparably
180	70	72	to
180	77	91	baseline model
180	94	98	TGen
180	101	112	in terms of
180	113	130	automatic metrics
210	15	37	TV and Laptop Datasets
212	11	29	our ensemble model
212	30	38	performs
212	39	52	competitively
212	53	57	with
212	62	70	baseline
212	71	73	on
212	78	88	TV dataset
212	98	109	outperforms
212	113	115	on
212	120	134	Laptop dataset
212	135	137	by
212	140	151	wide margin
