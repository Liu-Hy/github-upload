{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.models'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cf78cabd34f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m from simpletransformers.classification import (\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mClassificationArgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mClassificationModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\simpletransformers\\classification\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_label_classification_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiLabelClassificationModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_modal_classification_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiModalClassificationModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m from simpletransformers.config.model_args import (\n\u001b[0;32m      5\u001b[0m     \u001b[0mClassificationArgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malbert_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAlbertForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcamembert_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCamembertForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistilbert_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\simpletransformers\\classification\\transformer_models\\bert_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling_bert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertPreTrainedModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval as load\n",
    "import sklearn\n",
    "\n",
    "from simpletransformers.classification import (\n",
    "    ClassificationArgs,\n",
    "    ClassificationModel,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6128\n",
      "4075\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../interim/try1.csv')\n",
    "df = df.rename(columns={'idx': 'indx'})\n",
    "df['msk']=1\n",
    "print(len(df))\n",
    "for i in range(len(df)):\n",
    "    if len(load(df.iloc[i,3]))<2:\n",
    "        df.iloc[i,16]=0\n",
    "df = df[df['msk']==1]\n",
    "print(len(df))\n",
    "df.insert(loc=0, column='idx', value=np.arange(len(df)))\n",
    "df = df.sample(frac=1, random_state=1)\n",
    "bound = int(0.9*len(df))\n",
    "t_df = df[:bound]\n",
    "e_df = df[bound:]\n",
    "t_df = t_df.values\n",
    "e_df = e_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(arr):\n",
    "    ls=[]\n",
    "    for i in range(len(arr)):\n",
    "        lth = len(load(arr[i,4]))\n",
    "        for p1 in range(lth-1):\n",
    "            for p2 in range(p1+1, lth):\n",
    "                phrase1=load(arr[i,4])[p1]\n",
    "                phrase2=load(arr[i,4])[p2]\n",
    "                word_ls=arr[i,2].split(' ')\n",
    "                word_ls.insert(phrase1[1][0], '[[')\n",
    "                word_ls.insert(phrase1[1][1]+1, ']]')\n",
    "                word_ls.insert(phrase2[1][0]+2, '[[')\n",
    "                word_ls.insert(phrase2[1][1]+3, ']]')\n",
    "                flg=0\n",
    "                if arr[i,6]=='[]':\n",
    "                    trip_ls=[]\n",
    "                else:\n",
    "                    trip_ls=load(arr[i,6])\n",
    "                    for trip in trip_ls:\n",
    "                        if trip[0]==phrase1[0] and trip[2]==phrase2[0]:\n",
    "                            if trip[1]=='has':\n",
    "                                flg=1\n",
    "                                break\n",
    "                            elif trip[1]=='name':\n",
    "                                flg==2\n",
    "                                break\n",
    "                ls.append([phrase1[0],phrase2[0],trip_ls,' '.join(word_ls),flg])\n",
    "    dataframe = pd.DataFrame(ls)\n",
    "    dataframe.columns = ['phrase1', 'phrase2', 'triples', 'text', 'labels']\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = convert(t_df)\n",
    "eval_df = convert(e_df)\n",
    "num_negative = len(train_df[train_df['labels'] == 0])\n",
    "weight_list = [1]\n",
    "for i in range(1,3):\n",
    "    weight_list.append(num_negative/len(df[df['labels'] == i]))\n",
    "train_df.to_csv('train_B.csv')\n",
    "eval_df.to_csv('eval_B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "# arguments for early stop\n",
    "model_args.use_early_stopping = True\n",
    "# model_args.early_stopping_delta = 0.01\n",
    "model_args.early_stopping_metric = \"F1_score\"\n",
    "model_args.early_stopping_metric_minimize = False\n",
    "model_args.early_stopping_patience = 18\n",
    "model_args.early_stopping_consider_epochs = True\n",
    "# model_args.evaluate_during_training_steps = 500\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "model_args.train_batch_size = 16\n",
    "model_args.learning_rate = 1e-5\n",
    "model_args.scheduler = \"linear_schedule_with_warmup\"\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.save_steps = -1\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.no_save = True\n",
    "model_args.output_dir = 'outputsB/'\n",
    "model_args.best_model_dir = 'outputsB/best_model'\n",
    "model_args.fp16 = False\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.manual_seed = 1\n",
    "model_args.use_multiprocessing = True\n",
    "model_args.num_train_epochs = 20\n",
    "model_args.gradient_accumulation_steps = 4\n",
    "model_args.do_lower_case = True  # when using uncased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_F1(ref, pred):\n",
    "    TP=FP=FN=0\n",
    "    for i in range(len(pred)):\n",
    "        ref_ls = eval_df.iloc[i,2]\n",
    "        pred_ls = []\n",
    "        if pred[i]!=0:\n",
    "            if pred[i]==1:\n",
    "                trip = [eval_df.iloc[i,0], 'has', eval_df.iloc[i,1]]\n",
    "            elif pred[i]==2:\n",
    "                trip = [eval_df.iloc[i,0], 'name', eval_df.iloc[i,1]]\n",
    "            pred_ls.append(trip)\n",
    "        TP+=len([t for t in pred_ls if t in ref_ls])\n",
    "        FP+=len([t for t in pred_ls if t not in ref_ls]) \n",
    "        FN+=len([t for t in ref_ls if t not in pred_ls])\n",
    "    FN+=missed\n",
    "    pc=TP/(TP+FP)\n",
    "    rc=TP/(TP+FN)\n",
    "    F1=2*pc*rc/(pc+rc)\n",
    "    print(f'precision {pc}, recall {rc}, F1 {F1}')\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    weight=weight_list,\n",
    "    num_labels=3,\n",
    "    args=model_args,\n",
    ")\n",
    "print(f'weight list: {weight_list}')\n",
    "# Train the model\n",
    "model.train_model(train_df, eval_df=eval_df,F1_score=triple_F1)\n",
    "# Evaluate the model\n",
    "#result, model_outputs, wrong_predictions = model.eval_model(eval_df, F1_score=triple_F1)"
   ]
  }
 ]
}