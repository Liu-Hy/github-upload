,phrase1,phrase2,triples,text,labels
0,Re 3 Sum,soft templates,[],"As can be seen from the rows "" LEN DIF "" and "" LESS 3 "" , the performance of [[ Re 3 Sum ]] is almost the same as that of [[ soft templates ]] .",0
1,FLOW mechanism,remarkably effective,[],"This [[ FLOW mechanism ]] is also [[ remarkably effective ]] at tracking the world states for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",0
2,FLOW mechanism,world states,[],"This [[ FLOW mechanism ]] is also remarkably effective at tracking the [[ world states ]] for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",0
3,FLOW mechanism,sequential instruction understanding,[],"This [[ FLOW mechanism ]] is also remarkably effective at tracking the world states for [[ sequential instruction understanding ]] ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",0
4,remarkably effective,world states,[],"This FLOW mechanism is also [[ remarkably effective ]] at tracking the [[ world states ]] for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",0
5,remarkably effective,sequential instruction understanding,[],"This FLOW mechanism is also [[ remarkably effective ]] at tracking the world states for [[ sequential instruction understanding ]] ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",0
6,world states,sequential instruction understanding,[],"This FLOW mechanism is also remarkably effective at tracking the [[ world states ]] for [[ sequential instruction understanding ]] ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",0
7,SNLI best model,MultiNLI dataset,[],The results of applying [[ SNLI best model ]] to [[ MultiNLI dataset ]] without additional parameter tuning are presented in .,0
8,entire similarity focus layer,random dropout layer ( p = 0.3 ),"[['random dropout layer ( p = 0.3 )', 'has', 'hurts'], ['hurts', 'has', 'accuracy']]","When we replaced the [[ entire similarity focus layer ]] with a [[ random dropout layer ( p = 0.3 ) ]] , the dropout layer hurts accuracy ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .",0
9,entire similarity focus layer,hurts,"[['random dropout layer ( p = 0.3 )', 'has', 'hurts'], ['hurts', 'has', 'accuracy']]","When we replaced the [[ entire similarity focus layer ]] with a random dropout layer ( p = 0.3 ) , the dropout layer [[ hurts ]] accuracy ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .",0
10,entire similarity focus layer,accuracy,"[['random dropout layer ( p = 0.3 )', 'has', 'hurts'], ['hurts', 'has', 'accuracy']]","When we replaced the [[ entire similarity focus layer ]] with a random dropout layer ( p = 0.3 ) , the dropout layer hurts [[ accuracy ]] ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .",0
11,random dropout layer ( p = 0.3 ),hurts,"[['random dropout layer ( p = 0.3 )', 'has', 'hurts'], ['hurts', 'has', 'accuracy']]","When we replaced the entire similarity focus layer with a [[ random dropout layer ( p = 0.3 ) ]] , the dropout layer [[ hurts ]] accuracy ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .",1
12,random dropout layer ( p = 0.3 ),accuracy,"[['random dropout layer ( p = 0.3 )', 'has', 'hurts'], ['hurts', 'has', 'accuracy']]","When we replaced the entire similarity focus layer with a [[ random dropout layer ( p = 0.3 ) ]] , the dropout layer hurts [[ accuracy ]] ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .",0
13,hurts,accuracy,"[['random dropout layer ( p = 0.3 )', 'has', 'hurts'], ['hurts', 'has', 'accuracy']]","When we replaced the entire similarity focus layer with a random dropout layer ( p = 0.3 ) , the dropout layer [[ hurts ]] [[ accuracy ]] ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .",1
14,importance,aspect,[],"After taking the [[ importance ]] of the [[ aspect ]] into account with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .",0
15,importance,account,[],"After taking the [[ importance ]] of the aspect into [[ account ]] with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .",0
16,importance,attention mechanism,[],"After taking the [[ importance ]] of the aspect into account with [[ attention mechanism ]] , they achieve a stable improvement comparing to the TD - LSTM .",0
17,importance,stable improvement,[],"After taking the [[ importance ]] of the aspect into account with attention mechanism , they achieve a [[ stable improvement ]] comparing to the TD - LSTM .",0
18,importance,TD - LSTM,[],"After taking the [[ importance ]] of the aspect into account with attention mechanism , they achieve a stable improvement comparing to the [[ TD - LSTM ]] .",0
19,aspect,account,[],"After taking the importance of the [[ aspect ]] into [[ account ]] with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .",0
20,aspect,attention mechanism,[],"After taking the importance of the [[ aspect ]] into account with [[ attention mechanism ]] , they achieve a stable improvement comparing to the TD - LSTM .",0
21,aspect,stable improvement,[],"After taking the importance of the [[ aspect ]] into account with attention mechanism , they achieve a [[ stable improvement ]] comparing to the TD - LSTM .",0
22,aspect,TD - LSTM,[],"After taking the importance of the [[ aspect ]] into account with attention mechanism , they achieve a stable improvement comparing to the [[ TD - LSTM ]] .",0
23,account,attention mechanism,[],"After taking the importance of the aspect into [[ account ]] with [[ attention mechanism ]] , they achieve a stable improvement comparing to the TD - LSTM .",0
24,account,stable improvement,[],"After taking the importance of the aspect into [[ account ]] with attention mechanism , they achieve a [[ stable improvement ]] comparing to the TD - LSTM .",0
25,account,TD - LSTM,[],"After taking the importance of the aspect into [[ account ]] with attention mechanism , they achieve a stable improvement comparing to the [[ TD - LSTM ]] .",0
26,attention mechanism,stable improvement,[],"After taking the importance of the aspect into account with [[ attention mechanism ]] , they achieve a [[ stable improvement ]] comparing to the TD - LSTM .",0
27,attention mechanism,TD - LSTM,[],"After taking the importance of the aspect into account with [[ attention mechanism ]] , they achieve a stable improvement comparing to the [[ TD - LSTM ]] .",0
28,stable improvement,TD - LSTM,[],"After taking the importance of the aspect into account with attention mechanism , they achieve a [[ stable improvement ]] comparing to the [[ TD - LSTM ]] .",0
29,evidence aggregation,answer re-ranking problem,[],We formulate the above [[ evidence aggregation ]] as an [[ answer re-ranking problem ]] .,0
30,left - center - right separated LSTMs,three LSTMs,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a [[ left - center - right separated LSTMs ]] that contains [[ three LSTMs ]] , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and right context ) .",0
31,left - center - right separated LSTMs,"left - , center - and right - LSTM","[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a [[ left - center - right separated LSTMs ]] that contains three LSTMs , i.e. , [[ left - , center - and right - LSTM ]] , respectively modeling the three parts of a review ( left context , target phrase and right context ) .",0
32,left - center - right separated LSTMs,three parts,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a [[ left - center - right separated LSTMs ]] that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the [[ three parts ]] of a review ( left context , target phrase and right context ) .",0
33,left - center - right separated LSTMs,review,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a [[ left - center - right separated LSTMs ]] that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a [[ review ]] ( left context , target phrase and right context ) .",0
34,left - center - right separated LSTMs,left context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a [[ left - center - right separated LSTMs ]] that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( [[ left context ]] , target phrase and right context ) .",0
35,left - center - right separated LSTMs,target phrase,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a [[ left - center - right separated LSTMs ]] that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , [[ target phrase ]] and right context ) .",0
36,left - center - right separated LSTMs,right context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a [[ left - center - right separated LSTMs ]] that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and [[ right context ]] ) .",0
37,three LSTMs,"left - , center - and right - LSTM","[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains [[ three LSTMs ]] , i.e. , [[ left - , center - and right - LSTM ]] , respectively modeling the three parts of a review ( left context , target phrase and right context ) .",0
38,three LSTMs,three parts,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains [[ three LSTMs ]] , i.e. , left - , center - and right - LSTM , respectively modeling the [[ three parts ]] of a review ( left context , target phrase and right context ) .",0
39,three LSTMs,review,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains [[ three LSTMs ]] , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a [[ review ]] ( left context , target phrase and right context ) .",0
40,three LSTMs,left context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains [[ three LSTMs ]] , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( [[ left context ]] , target phrase and right context ) .",0
41,three LSTMs,target phrase,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains [[ three LSTMs ]] , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , [[ target phrase ]] and right context ) .",0
42,three LSTMs,right context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains [[ three LSTMs ]] , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and [[ right context ]] ) .",0
43,"left - , center - and right - LSTM",three parts,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , [[ left - , center - and right - LSTM ]] , respectively modeling the [[ three parts ]] of a review ( left context , target phrase and right context ) .",0
44,"left - , center - and right - LSTM",review,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , [[ left - , center - and right - LSTM ]] , respectively modeling the three parts of a [[ review ]] ( left context , target phrase and right context ) .",0
45,"left - , center - and right - LSTM",left context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , [[ left - , center - and right - LSTM ]] , respectively modeling the three parts of a review ( [[ left context ]] , target phrase and right context ) .",0
46,"left - , center - and right - LSTM",target phrase,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , [[ left - , center - and right - LSTM ]] , respectively modeling the three parts of a review ( left context , [[ target phrase ]] and right context ) .",0
47,"left - , center - and right - LSTM",right context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , [[ left - , center - and right - LSTM ]] , respectively modeling the three parts of a review ( left context , target phrase and [[ right context ]] ) .",0
48,three parts,review,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the [[ three parts ]] of a [[ review ]] ( left context , target phrase and right context ) .",0
49,three parts,left context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the [[ three parts ]] of a review ( [[ left context ]] , target phrase and right context ) .",0
50,three parts,target phrase,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the [[ three parts ]] of a review ( left context , [[ target phrase ]] and right context ) .",0
51,three parts,right context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the [[ three parts ]] of a review ( left context , target phrase and [[ right context ]] ) .",0
52,review,left context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a [[ review ]] ( [[ left context ]] , target phrase and right context ) .",0
53,review,target phrase,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a [[ review ]] ( left context , [[ target phrase ]] and right context ) .",0
54,review,right context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a [[ review ]] ( left context , target phrase and [[ right context ]] ) .",0
55,left context,target phrase,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( [[ left context ]] , [[ target phrase ]] and right context ) .",0
56,left context,right context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( [[ left context ]] , target phrase and [[ right context ]] ) .",0
57,target phrase,right context,"[['review', 'name', 'left context'], ['review', 'name', 'target phrase'], ['review', 'name', 'right context']]","Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , [[ target phrase ]] and [[ right context ]] ) .",0
58,"our single model "" BiMPM """,state - of - the - art single models,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that [[ our single model "" BiMPM "" ]] is on par with the [[ state - of - the - art single models ]] , and our ' BiMPM ( Ensemble ) "" works much better than "" ( Ensemble ) "" .",0
59,"our single model "" BiMPM """,our ' BiMPM ( Ensemble ),[],"Finally , comparing our models with all the state - of - the - art models , we can observe that [[ our single model "" BiMPM "" ]] is on par with the state - of - the - art single models , and [[ our ' BiMPM ( Ensemble ) ]] "" works much better than "" ( Ensemble ) "" .",0
60,"our single model "" BiMPM """,much better,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that [[ our single model "" BiMPM "" ]] is on par with the state - of - the - art single models , and our ' BiMPM ( Ensemble ) "" works [[ much better ]] than "" ( Ensemble ) "" .",0
61,"our single model "" BiMPM """,Ensemble,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that [[ our single model "" BiMPM "" ]] is on par with the state - of - the - art single models , and our ' BiMPM ( Ensemble ) "" works much better than "" ( [[ Ensemble ]] ) "" .",0
62,state - of - the - art single models,our ' BiMPM ( Ensemble ),[],"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the [[ state - of - the - art single models ]] , and [[ our ' BiMPM ( Ensemble ) ]] "" works much better than "" ( Ensemble ) "" .",0
63,state - of - the - art single models,much better,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the [[ state - of - the - art single models ]] , and our ' BiMPM ( Ensemble ) "" works [[ much better ]] than "" ( Ensemble ) "" .",0
64,state - of - the - art single models,Ensemble,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the [[ state - of - the - art single models ]] , and our ' BiMPM ( Ensemble ) "" works much better than "" ( [[ Ensemble ]] ) "" .",0
65,our ' BiMPM ( Ensemble ),much better,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the state - of - the - art single models , and [[ our ' BiMPM ( Ensemble ) ]] "" works [[ much better ]] than "" ( Ensemble ) "" .",0
66,our ' BiMPM ( Ensemble ),Ensemble,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the state - of - the - art single models , and [[ our ' BiMPM ( Ensemble ) ]] "" works much better than "" ( [[ Ensemble ]] ) "" .",0
67,much better,Ensemble,[],"Finally , comparing our models with all the state - of - the - art models , we can observe that our single model "" BiMPM "" is on par with the state - of - the - art single models , and our ' BiMPM ( Ensemble ) "" works [[ much better ]] than "" ( [[ Ensemble ]] ) "" .",0
68,maximum number of epochs,"[ 2 , 5 ]",[],"The [[ maximum number of epochs ]] is set in [[ [ 2 , 5 ] ]] depending on tasks .",0
69,maximum number of epochs,tasks,[],"The [[ maximum number of epochs ]] is set in [ 2 , 5 ] depending on [[ tasks ]] .",0
70,"[ 2 , 5 ]",tasks,[],"The maximum number of epochs is set in [[ [ 2 , 5 ] ]] depending on [[ tasks ]] .",0
71,test set,average entailment score,"[['test set', 'has', 'average entailment score']]","For the [[ test set ]] of , the [[ average entailment score ]] for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .",1
72,test set,reference,"[['test set', 'has', 'average entailment score']]","For the [[ test set ]] of , the average entailment score for the [[ reference ]] is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .",0
73,test set,0.72,"[['test set', 'has', 'average entailment score']]","For the [[ test set ]] of , the average entailment score for the reference is [[ 0.72 ]] , while for the basic seq2seq model , the entailment score is only 0.46 .",0
74,test set,basic seq2seq model,"[['test set', 'has', 'average entailment score']]","For the [[ test set ]] of , the average entailment score for the reference is 0.72 , while for the [[ basic seq2seq model ]] , the entailment score is only 0.46 .",0
75,test set,0.46,"[['test set', 'has', 'average entailment score']]","For the [[ test set ]] of , the average entailment score for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only [[ 0.46 ]] .",0
76,average entailment score,reference,"[['test set', 'has', 'average entailment score']]","For the test set of , the [[ average entailment score ]] for the [[ reference ]] is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .",0
77,average entailment score,0.72,"[['test set', 'has', 'average entailment score']]","For the test set of , the [[ average entailment score ]] for the reference is [[ 0.72 ]] , while for the basic seq2seq model , the entailment score is only 0.46 .",0
78,average entailment score,basic seq2seq model,"[['test set', 'has', 'average entailment score']]","For the test set of , the [[ average entailment score ]] for the reference is 0.72 , while for the [[ basic seq2seq model ]] , the entailment score is only 0.46 .",0
79,average entailment score,0.46,"[['test set', 'has', 'average entailment score']]","For the test set of , the [[ average entailment score ]] for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only [[ 0.46 ]] .",0
80,reference,0.72,"[['test set', 'has', 'average entailment score']]","For the test set of , the average entailment score for the [[ reference ]] is [[ 0.72 ]] , while for the basic seq2seq model , the entailment score is only 0.46 .",0
81,reference,basic seq2seq model,"[['test set', 'has', 'average entailment score']]","For the test set of , the average entailment score for the [[ reference ]] is 0.72 , while for the [[ basic seq2seq model ]] , the entailment score is only 0.46 .",0
82,reference,0.46,"[['test set', 'has', 'average entailment score']]","For the test set of , the average entailment score for the [[ reference ]] is 0.72 , while for the basic seq2seq model , the entailment score is only [[ 0.46 ]] .",0
83,0.72,basic seq2seq model,"[['test set', 'has', 'average entailment score']]","For the test set of , the average entailment score for the reference is [[ 0.72 ]] , while for the [[ basic seq2seq model ]] , the entailment score is only 0.46 .",0
84,0.72,0.46,"[['test set', 'has', 'average entailment score']]","For the test set of , the average entailment score for the reference is [[ 0.72 ]] , while for the basic seq2seq model , the entailment score is only [[ 0.46 ]] .",0
85,basic seq2seq model,0.46,"[['test set', 'has', 'average entailment score']]","For the test set of , the average entailment score for the reference is 0.72 , while for the [[ basic seq2seq model ]] , the entailment score is only [[ 0.46 ]] .",0
86,recent coreference resolution model,central,[],"Our model builds on a [[ recent coreference resolution model ]] , by making [[ central ]] use of learned , contextualized span representations .",0
87,recent coreference resolution model,"learned , contextualized span representations",[],"Our model builds on a [[ recent coreference resolution model ]] , by making central use of [[ learned , contextualized span representations ]] .",0
88,central,"learned , contextualized span representations",[],"Our model builds on a recent coreference resolution model , by making [[ central ]] use of [[ learned , contextualized span representations ]] .",0
89,distant elements,each other,[],"Therefore , [[ distant elements ]] can interact with [[ each other ]] by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .",0
90,distant elements,shorter paths,[],"Therefore , [[ distant elements ]] can interact with each other by [[ shorter paths ]] ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .",0
91,distant elements,unimpeded information flow,[],"Therefore , [[ distant elements ]] can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows [[ unimpeded information flow ]] through the network .",0
92,distant elements,network,[],"Therefore , [[ distant elements ]] can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the [[ network ]] .",0
93,each other,shorter paths,[],"Therefore , distant elements can interact with [[ each other ]] by [[ shorter paths ]] ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .",0
94,each other,unimpeded information flow,[],"Therefore , distant elements can interact with [[ each other ]] by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows [[ unimpeded information flow ]] through the network .",0
95,each other,network,[],"Therefore , distant elements can interact with [[ each other ]] by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the [[ network ]] .",0
96,shorter paths,unimpeded information flow,[],"Therefore , distant elements can interact with each other by [[ shorter paths ]] ( O ( 1 ) v.s. O ( n ) ) , which allows [[ unimpeded information flow ]] through the network .",0
97,shorter paths,network,[],"Therefore , distant elements can interact with each other by [[ shorter paths ]] ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the [[ network ]] .",0
98,unimpeded information flow,network,[],"Therefore , distant elements can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows [[ unimpeded information flow ]] through the [[ network ]] .",0
99,Viterbi decoding,ID - CNN,[],"When paired with [[ Viterbi decoding ]] , our [[ ID - CNN ]] performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .",0
100,Viterbi decoding,Bi - LSTM,[],"When paired with [[ Viterbi decoding ]] , our ID - CNN performs on par with the [[ Bi - LSTM ]] , showing that the ID - CNN is also an effective token encoder for structured inference .",0
101,Viterbi decoding,ID - CNN is also an effective token encoder,[],"When paired with [[ Viterbi decoding ]] , our ID - CNN performs on par with the Bi - LSTM , showing that the [[ ID - CNN is also an effective token encoder ]] for structured inference .",0
102,Viterbi decoding,structured inference,[],"When paired with [[ Viterbi decoding ]] , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for [[ structured inference ]] .",0
103,ID - CNN,Bi - LSTM,[],"When paired with Viterbi decoding , our [[ ID - CNN ]] performs on par with the [[ Bi - LSTM ]] , showing that the ID - CNN is also an effective token encoder for structured inference .",0
104,ID - CNN,ID - CNN is also an effective token encoder,[],"When paired with Viterbi decoding , our [[ ID - CNN ]] performs on par with the Bi - LSTM , showing that the [[ ID - CNN is also an effective token encoder ]] for structured inference .",0
105,ID - CNN,structured inference,[],"When paired with Viterbi decoding , our [[ ID - CNN ]] performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for [[ structured inference ]] .",0
106,Bi - LSTM,ID - CNN is also an effective token encoder,[],"When paired with Viterbi decoding , our ID - CNN performs on par with the [[ Bi - LSTM ]] , showing that the [[ ID - CNN is also an effective token encoder ]] for structured inference .",0
107,Bi - LSTM,structured inference,[],"When paired with Viterbi decoding , our ID - CNN performs on par with the [[ Bi - LSTM ]] , showing that the ID - CNN is also an effective token encoder for [[ structured inference ]] .",0
108,ID - CNN is also an effective token encoder,structured inference,[],"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the [[ ID - CNN is also an effective token encoder ]] for [[ structured inference ]] .",0
109,dropout rate,0.3,[],The [[ dropout rate ]] is chosen as [[ 0.3 ]] .,0
110,proposed method,all variations,[],"This is in contrast to the [[ proposed method ]] where [[ all variations ]] will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a latent space .",0
111,proposed method,relatively better quality,[],"This is in contrast to the [[ proposed method ]] where all variations will be of [[ relatively better quality ]] since they are the top beam - search result , generated based on different z sampled from a latent space .",0
112,proposed method,top beam - search result,[],"This is in contrast to the [[ proposed method ]] where all variations will be of relatively better quality since they are the [[ top beam - search result ]] , generated based on different z sampled from a latent space .",0
113,proposed method,different z,[],"This is in contrast to the [[ proposed method ]] where all variations will be of relatively better quality since they are the top beam - search result , generated based on [[ different z ]] sampled from a latent space .",0
114,proposed method,latent space,[],"This is in contrast to the [[ proposed method ]] where all variations will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a [[ latent space ]] .",0
115,all variations,relatively better quality,[],"This is in contrast to the proposed method where [[ all variations ]] will be of [[ relatively better quality ]] since they are the top beam - search result , generated based on different z sampled from a latent space .",0
116,all variations,top beam - search result,[],"This is in contrast to the proposed method where [[ all variations ]] will be of relatively better quality since they are the [[ top beam - search result ]] , generated based on different z sampled from a latent space .",0
117,all variations,different z,[],"This is in contrast to the proposed method where [[ all variations ]] will be of relatively better quality since they are the top beam - search result , generated based on [[ different z ]] sampled from a latent space .",0
118,all variations,latent space,[],"This is in contrast to the proposed method where [[ all variations ]] will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a [[ latent space ]] .",0
119,relatively better quality,top beam - search result,[],"This is in contrast to the proposed method where all variations will be of [[ relatively better quality ]] since they are the [[ top beam - search result ]] , generated based on different z sampled from a latent space .",0
120,relatively better quality,different z,[],"This is in contrast to the proposed method where all variations will be of [[ relatively better quality ]] since they are the top beam - search result , generated based on [[ different z ]] sampled from a latent space .",0
121,relatively better quality,latent space,[],"This is in contrast to the proposed method where all variations will be of [[ relatively better quality ]] since they are the top beam - search result , generated based on different z sampled from a [[ latent space ]] .",0
122,top beam - search result,different z,[],"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the [[ top beam - search result ]] , generated based on [[ different z ]] sampled from a latent space .",0
123,top beam - search result,latent space,[],"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the [[ top beam - search result ]] , generated based on different z sampled from a [[ latent space ]] .",0
124,different z,latent space,[],"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the top beam - search result , generated based on [[ different z ]] sampled from a [[ latent space ]] .",0
125,baselines,context - less sequence to sequence ( seq2seq ) model,[],"We compete on the summaries setting , in which the [[ baselines ]] are a [[ context - less sequence to sequence ( seq2seq ) model ]] , ASR and BiDAF .",0
126,baselines,ASR,[],"We compete on the summaries setting , in which the [[ baselines ]] are a context - less sequence to sequence ( seq2seq ) model , [[ ASR ]] and BiDAF .",0
127,baselines,BiDAF,[],"We compete on the summaries setting , in which the [[ baselines ]] are a context - less sequence to sequence ( seq2seq ) model , ASR and [[ BiDAF ]] .",0
128,context - less sequence to sequence ( seq2seq ) model,ASR,[],"We compete on the summaries setting , in which the baselines are a [[ context - less sequence to sequence ( seq2seq ) model ]] , [[ ASR ]] and BiDAF .",0
129,context - less sequence to sequence ( seq2seq ) model,BiDAF,[],"We compete on the summaries setting , in which the baselines are a [[ context - less sequence to sequence ( seq2seq ) model ]] , ASR and [[ BiDAF ]] .",0
130,ASR,BiDAF,[],"We compete on the summaries setting , in which the baselines are a context - less sequence to sequence ( seq2seq ) model , [[ ASR ]] and [[ BiDAF ]] .",0
131,coverage,existing systems,[],"First , as an effort to study the [[ coverage ]] of [[ existing systems ]] and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
132,coverage,jointly,[],"First , as an effort to study the [[ coverage ]] of existing systems and the possibility to train [[ jointly ]] on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
133,coverage,different data sources,[],"First , as an effort to study the [[ coverage ]] of existing systems and the possibility to train jointly on [[ different data sources ]] via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
134,coverage,multitasking,[],"First , as an effort to study the [[ coverage ]] of existing systems and the possibility to train jointly on different data sources via [[ multitasking ]] , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
135,coverage,first large - scale dataset of questions and answers based on a KB,[],"First , as an effort to study the [[ coverage ]] of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the [[ first large - scale dataset of questions and answers based on a KB ]] , called SimpleQuestions .",0
136,coverage,SimpleQuestions,[],"First , as an effort to study the [[ coverage ]] of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called [[ SimpleQuestions ]] .",0
137,existing systems,jointly,[],"First , as an effort to study the coverage of [[ existing systems ]] and the possibility to train [[ jointly ]] on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
138,existing systems,different data sources,[],"First , as an effort to study the coverage of [[ existing systems ]] and the possibility to train jointly on [[ different data sources ]] via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
139,existing systems,multitasking,[],"First , as an effort to study the coverage of [[ existing systems ]] and the possibility to train jointly on different data sources via [[ multitasking ]] , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
140,existing systems,first large - scale dataset of questions and answers based on a KB,[],"First , as an effort to study the coverage of [[ existing systems ]] and the possibility to train jointly on different data sources via multitasking , we collected the [[ first large - scale dataset of questions and answers based on a KB ]] , called SimpleQuestions .",0
141,existing systems,SimpleQuestions,[],"First , as an effort to study the coverage of [[ existing systems ]] and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called [[ SimpleQuestions ]] .",0
142,jointly,different data sources,[],"First , as an effort to study the coverage of existing systems and the possibility to train [[ jointly ]] on [[ different data sources ]] via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
143,jointly,multitasking,[],"First , as an effort to study the coverage of existing systems and the possibility to train [[ jointly ]] on different data sources via [[ multitasking ]] , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
144,jointly,first large - scale dataset of questions and answers based on a KB,[],"First , as an effort to study the coverage of existing systems and the possibility to train [[ jointly ]] on different data sources via multitasking , we collected the [[ first large - scale dataset of questions and answers based on a KB ]] , called SimpleQuestions .",0
145,jointly,SimpleQuestions,[],"First , as an effort to study the coverage of existing systems and the possibility to train [[ jointly ]] on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called [[ SimpleQuestions ]] .",0
146,different data sources,multitasking,[],"First , as an effort to study the coverage of existing systems and the possibility to train jointly on [[ different data sources ]] via [[ multitasking ]] , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",0
147,different data sources,first large - scale dataset of questions and answers based on a KB,[],"First , as an effort to study the coverage of existing systems and the possibility to train jointly on [[ different data sources ]] via multitasking , we collected the [[ first large - scale dataset of questions and answers based on a KB ]] , called SimpleQuestions .",0
148,different data sources,SimpleQuestions,[],"First , as an effort to study the coverage of existing systems and the possibility to train jointly on [[ different data sources ]] via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called [[ SimpleQuestions ]] .",0
149,multitasking,first large - scale dataset of questions and answers based on a KB,[],"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via [[ multitasking ]] , we collected the [[ first large - scale dataset of questions and answers based on a KB ]] , called SimpleQuestions .",0
150,multitasking,SimpleQuestions,[],"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via [[ multitasking ]] , we collected the first large - scale dataset of questions and answers based on a KB , called [[ SimpleQuestions ]] .",0
151,first large - scale dataset of questions and answers based on a KB,SimpleQuestions,[],"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the [[ first large - scale dataset of questions and answers based on a KB ]] , called [[ SimpleQuestions ]] .",0
152,batch size,64 to 512,[],The [[ batch size ]] is tuned from [[ 64 to 512 ]] .,0
153,Feature - based SVM,traditional support vector machine based model,[],[[ Feature - based SVM ]] is a [[ traditional support vector machine based model ]] with extensive feature engineering .,0
154,Feature - based SVM,extensive feature engineering,[],[[ Feature - based SVM ]] is a traditional support vector machine based model with [[ extensive feature engineering ]] .,0
155,traditional support vector machine based model,extensive feature engineering,[],Feature - based SVM is a [[ traditional support vector machine based model ]] with [[ extensive feature engineering ]] .,0
156,weight matrix,last fully connect layer,[],"The [[ weight matrix ]] of [[ last fully connect layer ]] is randomly initialized by a normal distribution N ( 0 , 1 ) .",0
157,weight matrix,"normal distribution N ( 0 , 1 )",[],"The [[ weight matrix ]] of last fully connect layer is randomly initialized by a [[ normal distribution N ( 0 , 1 ) ]] .",0
158,last fully connect layer,"normal distribution N ( 0 , 1 )",[],"The weight matrix of [[ last fully connect layer ]] is randomly initialized by a [[ normal distribution N ( 0 , 1 ) ]] .",0
159,our 2 - way MTL model,entailment generation,[],We found that [[ our 2 - way MTL model ]] with [[ entailment generation ]] reduces this extraneous count by 17.2 % w.r.t. the baseline .,0
160,our 2 - way MTL model,extraneous count,[],We found that [[ our 2 - way MTL model ]] with entailment generation reduces this [[ extraneous count ]] by 17.2 % w.r.t. the baseline .,0
161,our 2 - way MTL model,17.2 % w.r.t. the baseline,[],We found that [[ our 2 - way MTL model ]] with entailment generation reduces this extraneous count by [[ 17.2 % w.r.t. the baseline ]] .,0
162,entailment generation,extraneous count,[],We found that our 2 - way MTL model with [[ entailment generation ]] reduces this [[ extraneous count ]] by 17.2 % w.r.t. the baseline .,0
163,entailment generation,17.2 % w.r.t. the baseline,[],We found that our 2 - way MTL model with [[ entailment generation ]] reduces this extraneous count by [[ 17.2 % w.r.t. the baseline ]] .,0
164,extraneous count,17.2 % w.r.t. the baseline,[],We found that our 2 - way MTL model with entailment generation reduces this [[ extraneous count ]] by [[ 17.2 % w.r.t. the baseline ]] .,0
165,1024 units,ReLU activation,[],"In addition , the MLP has a hidden layer with [[ 1024 units ]] with [[ ReLU activation ]] and a sof tmax layer .",0
166,1024 units,sof tmax layer,[],"In addition , the MLP has a hidden layer with [[ 1024 units ]] with ReLU activation and a [[ sof tmax layer ]] .",0
167,ReLU activation,sof tmax layer,[],"In addition , the MLP has a hidden layer with 1024 units with [[ ReLU activation ]] and a [[ sof tmax layer ]] .",0
168,bc - LSTM,preceding and following information,[],"Since [[ bc - LSTM ]] has access to both the [[ preceding and following information ]] of the utterance sequence , it performs consistently better on all the datasets over sc - LSTM .",0
169,bc - LSTM,utterance sequence,[],"Since [[ bc - LSTM ]] has access to both the preceding and following information of the [[ utterance sequence ]] , it performs consistently better on all the datasets over sc - LSTM .",0
170,bc - LSTM,consistently better,[],"Since [[ bc - LSTM ]] has access to both the preceding and following information of the utterance sequence , it performs [[ consistently better ]] on all the datasets over sc - LSTM .",0
171,bc - LSTM,all the datasets over sc - LSTM,[],"Since [[ bc - LSTM ]] has access to both the preceding and following information of the utterance sequence , it performs consistently better on [[ all the datasets over sc - LSTM ]] .",0
172,preceding and following information,utterance sequence,[],"Since bc - LSTM has access to both the [[ preceding and following information ]] of the [[ utterance sequence ]] , it performs consistently better on all the datasets over sc - LSTM .",0
173,preceding and following information,consistently better,[],"Since bc - LSTM has access to both the [[ preceding and following information ]] of the utterance sequence , it performs [[ consistently better ]] on all the datasets over sc - LSTM .",0
174,preceding and following information,all the datasets over sc - LSTM,[],"Since bc - LSTM has access to both the [[ preceding and following information ]] of the utterance sequence , it performs consistently better on [[ all the datasets over sc - LSTM ]] .",0
175,utterance sequence,consistently better,[],"Since bc - LSTM has access to both the preceding and following information of the [[ utterance sequence ]] , it performs [[ consistently better ]] on all the datasets over sc - LSTM .",0
176,utterance sequence,all the datasets over sc - LSTM,[],"Since bc - LSTM has access to both the preceding and following information of the [[ utterance sequence ]] , it performs consistently better on [[ all the datasets over sc - LSTM ]] .",0
177,consistently better,all the datasets over sc - LSTM,[],"Since bc - LSTM has access to both the preceding and following information of the utterance sequence , it performs [[ consistently better ]] on [[ all the datasets over sc - LSTM ]] .",0
178,iterated dilated CNN architecture ( ID - CNN ),same block of dilated convolutions,[],Our over all [[ iterated dilated CNN architecture ( ID - CNN ) ]] repeatedly applies the [[ same block of dilated convolutions ]] to token - wise representations .,0
179,iterated dilated CNN architecture ( ID - CNN ),token - wise representations,[],Our over all [[ iterated dilated CNN architecture ( ID - CNN ) ]] repeatedly applies the same block of dilated convolutions to [[ token - wise representations ]] .,0
180,same block of dilated convolutions,token - wise representations,[],Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the [[ same block of dilated convolutions ]] to [[ token - wise representations ]] .,0
181,context - based similarity matching,M 3,[],"For [[ context - based similarity matching ]] , we simply took out the [[ M 3 ]] from the linear function and it is proved to be contributory to the performance of full - orientation matching .",0
182,context - based similarity matching,linear function,[],"For [[ context - based similarity matching ]] , we simply took out the M 3 from the [[ linear function ]] and it is proved to be contributory to the performance of full - orientation matching .",0
183,context - based similarity matching,contributory,[],"For [[ context - based similarity matching ]] , we simply took out the M 3 from the linear function and it is proved to be [[ contributory ]] to the performance of full - orientation matching .",0
184,context - based similarity matching,performance,[],"For [[ context - based similarity matching ]] , we simply took out the M 3 from the linear function and it is proved to be contributory to the [[ performance ]] of full - orientation matching .",0
185,context - based similarity matching,full - orientation matching,[],"For [[ context - based similarity matching ]] , we simply took out the M 3 from the linear function and it is proved to be contributory to the performance of [[ full - orientation matching ]] .",0
186,M 3,linear function,[],"For context - based similarity matching , we simply took out the [[ M 3 ]] from the [[ linear function ]] and it is proved to be contributory to the performance of full - orientation matching .",0
187,M 3,contributory,[],"For context - based similarity matching , we simply took out the [[ M 3 ]] from the linear function and it is proved to be [[ contributory ]] to the performance of full - orientation matching .",0
188,M 3,performance,[],"For context - based similarity matching , we simply took out the [[ M 3 ]] from the linear function and it is proved to be contributory to the [[ performance ]] of full - orientation matching .",0
189,M 3,full - orientation matching,[],"For context - based similarity matching , we simply took out the [[ M 3 ]] from the linear function and it is proved to be contributory to the performance of [[ full - orientation matching ]] .",0
190,linear function,contributory,[],"For context - based similarity matching , we simply took out the M 3 from the [[ linear function ]] and it is proved to be [[ contributory ]] to the performance of full - orientation matching .",0
191,linear function,performance,[],"For context - based similarity matching , we simply took out the M 3 from the [[ linear function ]] and it is proved to be contributory to the [[ performance ]] of full - orientation matching .",0
192,linear function,full - orientation matching,[],"For context - based similarity matching , we simply took out the M 3 from the [[ linear function ]] and it is proved to be contributory to the performance of [[ full - orientation matching ]] .",0
193,contributory,performance,[],"For context - based similarity matching , we simply took out the M 3 from the linear function and it is proved to be [[ contributory ]] to the [[ performance ]] of full - orientation matching .",0
194,contributory,full - orientation matching,[],"For context - based similarity matching , we simply took out the M 3 from the linear function and it is proved to be [[ contributory ]] to the performance of [[ full - orientation matching ]] .",0
195,performance,full - orientation matching,[],"For context - based similarity matching , we simply took out the M 3 from the linear function and it is proved to be contributory to the [[ performance ]] of [[ full - orientation matching ]] .",0
196,second scaffold,BiLSTM - Attn + citation worthiness scaffold,"[['similar improvements', 'has', '56.3 (?= 4.5 )']]",Adding the [[ second scaffold ]] in ' [[ BiLSTM - Attn + citation worthiness scaffold ]] ' also results in similar improvements : 56.3 (?= 4.5 ) .,0
197,second scaffold,similar improvements,"[['similar improvements', 'has', '56.3 (?= 4.5 )']]",Adding the [[ second scaffold ]] in ' BiLSTM - Attn + citation worthiness scaffold ' also results in [[ similar improvements ]] : 56.3 (?= 4.5 ) .,0
198,second scaffold,56.3 (?= 4.5 ),"[['similar improvements', 'has', '56.3 (?= 4.5 )']]",Adding the [[ second scaffold ]] in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : [[ 56.3 (?= 4.5 ) ]] .,0
199,BiLSTM - Attn + citation worthiness scaffold,similar improvements,"[['similar improvements', 'has', '56.3 (?= 4.5 )']]",Adding the second scaffold in ' [[ BiLSTM - Attn + citation worthiness scaffold ]] ' also results in [[ similar improvements ]] : 56.3 (?= 4.5 ) .,0
200,BiLSTM - Attn + citation worthiness scaffold,56.3 (?= 4.5 ),"[['similar improvements', 'has', '56.3 (?= 4.5 )']]",Adding the second scaffold in ' [[ BiLSTM - Attn + citation worthiness scaffold ]] ' also results in similar improvements : [[ 56.3 (?= 4.5 ) ]] .,0
201,similar improvements,56.3 (?= 4.5 ),"[['similar improvements', 'has', '56.3 (?= 4.5 )']]",Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in [[ similar improvements ]] : [[ 56.3 (?= 4.5 ) ]] .,1
202,selective encoding model,information flow,[],employ a [[ selective encoding model ]] to control the [[ information flow ]] from encoder to decoder .,0
203,selective encoding model,encoder to decoder,[],employ a [[ selective encoding model ]] to control the information flow from [[ encoder to decoder ]] .,0
204,information flow,encoder to decoder,[],employ a selective encoding model to control the [[ information flow ]] from [[ encoder to decoder ]] .,0
205,in - domain setting,our BiLSTM method,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the [[ in - domain setting ]] , with the same amount of training data ( 8,000 ) , [[ our BiLSTM method ]] with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .",1
206,in - domain setting,syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ),"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the [[ in - domain setting ]] , with the same amount of training data ( 8,000 ) , our BiLSTM method with [[ syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) ]] performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .",0
207,in - domain setting,similarly to or better,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the [[ in - domain setting ]] , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs [[ similarly to or better ]] than the LSTM + method proposed by , in terms of both F1 and accuracy .",0
208,in - domain setting,LSTM + method,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the [[ in - domain setting ]] , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the [[ LSTM + method ]] proposed by , in terms of both F1 and accuracy .",0
209,in - domain setting,both F1 and accuracy,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the [[ in - domain setting ]] , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of [[ both F1 and accuracy ]] .",0
210,our BiLSTM method,syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ),"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , [[ our BiLSTM method ]] with [[ syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) ]] performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .",0
211,our BiLSTM method,similarly to or better,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , [[ our BiLSTM method ]] with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs [[ similarly to or better ]] than the LSTM + method proposed by , in terms of both F1 and accuracy .",0
212,our BiLSTM method,LSTM + method,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , [[ our BiLSTM method ]] with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the [[ LSTM + method ]] proposed by , in terms of both F1 and accuracy .",0
213,our BiLSTM method,both F1 and accuracy,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , [[ our BiLSTM method ]] with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of [[ both F1 and accuracy ]] .",0
214,syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ),similarly to or better,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with [[ syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) ]] performs [[ similarly to or better ]] than the LSTM + method proposed by , in terms of both F1 and accuracy .",0
215,syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ),LSTM + method,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with [[ syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) ]] performs similarly to or better than the [[ LSTM + method ]] proposed by , in terms of both F1 and accuracy .",0
216,syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ),both F1 and accuracy,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with [[ syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) ]] performs similarly to or better than the LSTM + method proposed by , in terms of [[ both F1 and accuracy ]] .",0
217,similarly to or better,LSTM + method,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs [[ similarly to or better ]] than the [[ LSTM + method ]] proposed by , in terms of both F1 and accuracy .",0
218,similarly to or better,both F1 and accuracy,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs [[ similarly to or better ]] than the LSTM + method proposed by , in terms of [[ both F1 and accuracy ]] .",0
219,LSTM + method,both F1 and accuracy,"[['in - domain setting', 'has', 'our BiLSTM method']]","( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the [[ LSTM + method ]] proposed by , in terms of [[ both F1 and accuracy ]] .",0
220,label unreliability issue,label smoothing regularization,[],"To deal with the [[ label unreliability issue ]] , we employ a [[ label smoothing regularization ]] to encourage the model to be less confident with fuzzy labels .",0
221,label unreliability issue,model,[],"To deal with the [[ label unreliability issue ]] , we employ a label smoothing regularization to encourage the [[ model ]] to be less confident with fuzzy labels .",0
222,label unreliability issue,less confident,[],"To deal with the [[ label unreliability issue ]] , we employ a label smoothing regularization to encourage the model to be [[ less confident ]] with fuzzy labels .",0
223,label unreliability issue,fuzzy labels,[],"To deal with the [[ label unreliability issue ]] , we employ a label smoothing regularization to encourage the model to be less confident with [[ fuzzy labels ]] .",0
224,label smoothing regularization,model,[],"To deal with the label unreliability issue , we employ a [[ label smoothing regularization ]] to encourage the [[ model ]] to be less confident with fuzzy labels .",0
225,label smoothing regularization,less confident,[],"To deal with the label unreliability issue , we employ a [[ label smoothing regularization ]] to encourage the model to be [[ less confident ]] with fuzzy labels .",0
226,label smoothing regularization,fuzzy labels,[],"To deal with the label unreliability issue , we employ a [[ label smoothing regularization ]] to encourage the model to be less confident with [[ fuzzy labels ]] .",0
227,model,less confident,[],"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the [[ model ]] to be [[ less confident ]] with fuzzy labels .",0
228,model,fuzzy labels,[],"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the [[ model ]] to be less confident with [[ fuzzy labels ]] .",0
229,less confident,fuzzy labels,[],"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be [[ less confident ]] with [[ fuzzy labels ]] .",0
230,S2SA,Sequence - to - sequence framework,"[['S2SA', 'name', 'Sequence - to - sequence framework']]",( 1 ) [[ S2SA ]] : [[ Sequence - to - sequence framework ]] has been proposed for language generation task .,0
231,S2SA,language generation task,"[['S2SA', 'name', 'Sequence - to - sequence framework']]",( 1 ) [[ S2SA ]] : Sequence - to - sequence framework has been proposed for [[ language generation task ]] .,0
232,Sequence - to - sequence framework,language generation task,"[['S2SA', 'name', 'Sequence - to - sequence framework']]",( 1 ) S2SA : [[ Sequence - to - sequence framework ]] has been proposed for [[ language generation task ]] .,0
233,MemNet,multiple attentions,[],"[[ MemNet ]] adopts [[ multiple attentions ]] in order to improve the attention results , given the assumption that the result of an attention at a later hop should be better than that at the beginning .",0
234,MemNet,attention results,[],"[[ MemNet ]] adopts multiple attentions in order to improve the [[ attention results ]] , given the assumption that the result of an attention at a later hop should be better than that at the beginning .",0
235,multiple attentions,attention results,[],"MemNet adopts [[ multiple attentions ]] in order to improve the [[ attention results ]] , given the assumption that the result of an attention at a later hop should be better than that at the beginning .",0
236,RASOR,endpoint prediction model,[],"[[ RASOR ]] outperforms the [[ endpoint prediction model ]] by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
237,RASOR,1.1 in exact match,[],"[[ RASOR ]] outperforms the endpoint prediction model by [[ 1.1 in exact match ]] , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
238,endpoint prediction model,1.1 in exact match,[],"RASOR outperforms the [[ endpoint prediction model ]] by [[ 1.1 in exact match ]] , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
239,Each edge,independently predicting,[],"[[ Each edge ]] is identified by [[ independently predicting ]] which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",0
240,Each edge,role,[],"[[ Each edge ]] is identified by independently predicting which [[ role ]] , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",0
241,Each edge,every possible pair of text spans,[],"[[ Each edge ]] is identified by independently predicting which role , if any , holds between [[ every possible pair of text spans ]] , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",0
242,Each edge,https://github.com/luheng/lsgn,[],"[[ Each edge ]] is identified by independently predicting which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : [[ https://github.com/luheng/lsgn ]] pruning for efficiency .",0
243,independently predicting,role,[],"Each edge is identified by [[ independently predicting ]] which [[ role ]] , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",0
244,independently predicting,every possible pair of text spans,[],"Each edge is identified by [[ independently predicting ]] which role , if any , holds between [[ every possible pair of text spans ]] , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",0
245,independently predicting,https://github.com/luheng/lsgn,[],"Each edge is identified by [[ independently predicting ]] which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : [[ https://github.com/luheng/lsgn ]] pruning for efficiency .",0
246,role,every possible pair of text spans,[],"Each edge is identified by independently predicting which [[ role ]] , if any , holds between [[ every possible pair of text spans ]] , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",0
247,role,https://github.com/luheng/lsgn,[],"Each edge is identified by independently predicting which [[ role ]] , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : [[ https://github.com/luheng/lsgn ]] pruning for efficiency .",0
248,every possible pair of text spans,https://github.com/luheng/lsgn,[],"Each edge is identified by independently predicting which role , if any , holds between [[ every possible pair of text spans ]] , while using aggressive beam 1 Code and models : [[ https://github.com/luheng/lsgn ]] pruning for efficiency .",0
249,number of epochs,10,[],"The [[ number of epochs ]] is set to be [[ 10 ]] , and the feedforward dropout rate is 0.2 .",0
250,number of epochs,feedforward dropout rate,[],"The [[ number of epochs ]] is set to be 10 , and the [[ feedforward dropout rate ]] is 0.2 .",0
251,number of epochs,0.2,[],"The [[ number of epochs ]] is set to be 10 , and the feedforward dropout rate is [[ 0.2 ]] .",0
252,10,feedforward dropout rate,[],"The number of epochs is set to be [[ 10 ]] , and the [[ feedforward dropout rate ]] is 0.2 .",0
253,10,0.2,[],"The number of epochs is set to be [[ 10 ]] , and the feedforward dropout rate is [[ 0.2 ]] .",0
254,feedforward dropout rate,0.2,[],"The number of epochs is set to be 10 , and the [[ feedforward dropout rate ]] is [[ 0.2 ]] .",0
255,t- test statistical analysis results,BiHDM,[],"shows the [[ t- test statistical analysis results ]] , from which we can see [[ BiHDM ]] is significantly better than the baseline method .",0
256,t- test statistical analysis results,significantly better,[],"shows the [[ t- test statistical analysis results ]] , from which we can see BiHDM is [[ significantly better ]] than the baseline method .",0
257,t- test statistical analysis results,baseline method,[],"shows the [[ t- test statistical analysis results ]] , from which we can see BiHDM is significantly better than the [[ baseline method ]] .",0
258,BiHDM,significantly better,[],"shows the t- test statistical analysis results , from which we can see [[ BiHDM ]] is [[ significantly better ]] than the baseline method .",0
259,BiHDM,baseline method,[],"shows the t- test statistical analysis results , from which we can see [[ BiHDM ]] is significantly better than the [[ baseline method ]] .",0
260,significantly better,baseline method,[],"shows the t- test statistical analysis results , from which we can see BiHDM is [[ significantly better ]] than the [[ baseline method ]] .",0
261,GCNs,L layers,"[['GCNs', 'has', 'L layers']]","For [[ GCNs ]] , [[ L layers ]] will be needed in order to capture neighborhood information that is L hops away .",1
262,GCNs,neighborhood information,"[['GCNs', 'has', 'L layers']]","For [[ GCNs ]] , L layers will be needed in order to capture [[ neighborhood information ]] that is L hops away .",0
263,GCNs,L hops away,"[['GCNs', 'has', 'L layers']]","For [[ GCNs ]] , L layers will be needed in order to capture neighborhood information that is [[ L hops away ]] .",0
264,L layers,neighborhood information,"[['GCNs', 'has', 'L layers']]","For GCNs , [[ L layers ]] will be needed in order to capture [[ neighborhood information ]] that is L hops away .",0
265,L layers,L hops away,"[['GCNs', 'has', 'L layers']]","For GCNs , [[ L layers ]] will be needed in order to capture neighborhood information that is [[ L hops away ]] .",0
266,neighborhood information,L hops away,"[['GCNs', 'has', 'L layers']]","For GCNs , L layers will be needed in order to capture [[ neighborhood information ]] that is [[ L hops away ]] .",0
267,minibatch size,128,[],"The [[ minibatch size ]] is set as [[ 128 ]] , and a dropout rate of 0.2 is utilized on the embedding layer .",0
268,minibatch size,dropout rate,[],"The [[ minibatch size ]] is set as 128 , and a [[ dropout rate ]] of 0.2 is utilized on the embedding layer .",0
269,minibatch size,0.2,[],"The [[ minibatch size ]] is set as 128 , and a dropout rate of [[ 0.2 ]] is utilized on the embedding layer .",0
270,minibatch size,embedding layer,[],"The [[ minibatch size ]] is set as 128 , and a dropout rate of 0.2 is utilized on the [[ embedding layer ]] .",0
271,128,dropout rate,[],"The minibatch size is set as [[ 128 ]] , and a [[ dropout rate ]] of 0.2 is utilized on the embedding layer .",0
272,128,0.2,[],"The minibatch size is set as [[ 128 ]] , and a dropout rate of [[ 0.2 ]] is utilized on the embedding layer .",0
273,128,embedding layer,[],"The minibatch size is set as [[ 128 ]] , and a dropout rate of 0.2 is utilized on the [[ embedding layer ]] .",0
274,dropout rate,0.2,[],"The minibatch size is set as 128 , and a [[ dropout rate ]] of [[ 0.2 ]] is utilized on the embedding layer .",0
275,dropout rate,embedding layer,[],"The minibatch size is set as 128 , and a [[ dropout rate ]] of 0.2 is utilized on the [[ embedding layer ]] .",0
276,0.2,embedding layer,[],"The minibatch size is set as 128 , and a dropout rate of [[ 0.2 ]] is utilized on the [[ embedding layer ]] .",0
277,three different Recursive Neural Net models,one,[],"Next , we devise [[ three different Recursive Neural Net models ]] , each designed for [[ one ]] of discourse structure prediction , discourse relation prediction and sentiment analysis .",0
278,three different Recursive Neural Net models,discourse structure prediction,[],"Next , we devise [[ three different Recursive Neural Net models ]] , each designed for one of [[ discourse structure prediction ]] , discourse relation prediction and sentiment analysis .",0
279,three different Recursive Neural Net models,discourse relation prediction,[],"Next , we devise [[ three different Recursive Neural Net models ]] , each designed for one of discourse structure prediction , [[ discourse relation prediction ]] and sentiment analysis .",0
280,three different Recursive Neural Net models,sentiment analysis,[],"Next , we devise [[ three different Recursive Neural Net models ]] , each designed for one of discourse structure prediction , discourse relation prediction and [[ sentiment analysis ]] .",0
281,one,discourse structure prediction,[],"Next , we devise three different Recursive Neural Net models , each designed for [[ one ]] of [[ discourse structure prediction ]] , discourse relation prediction and sentiment analysis .",0
282,one,discourse relation prediction,[],"Next , we devise three different Recursive Neural Net models , each designed for [[ one ]] of discourse structure prediction , [[ discourse relation prediction ]] and sentiment analysis .",0
283,one,sentiment analysis,[],"Next , we devise three different Recursive Neural Net models , each designed for [[ one ]] of discourse structure prediction , discourse relation prediction and [[ sentiment analysis ]] .",0
284,discourse structure prediction,discourse relation prediction,[],"Next , we devise three different Recursive Neural Net models , each designed for one of [[ discourse structure prediction ]] , [[ discourse relation prediction ]] and sentiment analysis .",0
285,discourse structure prediction,sentiment analysis,[],"Next , we devise three different Recursive Neural Net models , each designed for one of [[ discourse structure prediction ]] , discourse relation prediction and [[ sentiment analysis ]] .",0
286,discourse relation prediction,sentiment analysis,[],"Next , we devise three different Recursive Neural Net models , each designed for one of discourse structure prediction , [[ discourse relation prediction ]] and [[ sentiment analysis ]] .",0
287,test time,parse,"[['interpret', 'has', 'unparsed sentences']]","At [[ test time ]] , it can simultaneously [[ parse ]] and interpret unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .",0
288,test time,interpret,"[['interpret', 'has', 'unparsed sentences']]","At [[ test time ]] , it can simultaneously parse and [[ interpret ]] unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .",0
289,test time,unparsed sentences,"[['interpret', 'has', 'unparsed sentences']]","At [[ test time ]] , it can simultaneously parse and interpret [[ unparsed sentences ]] , removing the dependence on an external parser at nearly no additional computational cost .",0
290,test time,dependence,"[['interpret', 'has', 'unparsed sentences']]","At [[ test time ]] , it can simultaneously parse and interpret unparsed sentences , removing the [[ dependence ]] on an external parser at nearly no additional computational cost .",0
291,test time,external parser,"[['interpret', 'has', 'unparsed sentences']]","At [[ test time ]] , it can simultaneously parse and interpret unparsed sentences , removing the dependence on an [[ external parser ]] at nearly no additional computational cost .",0
292,parse,interpret,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously [[ parse ]] and [[ interpret ]] unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .",0
293,parse,unparsed sentences,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously [[ parse ]] and interpret [[ unparsed sentences ]] , removing the dependence on an external parser at nearly no additional computational cost .",0
294,parse,dependence,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously [[ parse ]] and interpret unparsed sentences , removing the [[ dependence ]] on an external parser at nearly no additional computational cost .",0
295,parse,external parser,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously [[ parse ]] and interpret unparsed sentences , removing the dependence on an [[ external parser ]] at nearly no additional computational cost .",0
296,interpret,unparsed sentences,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously parse and [[ interpret ]] [[ unparsed sentences ]] , removing the dependence on an external parser at nearly no additional computational cost .",1
297,interpret,dependence,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously parse and [[ interpret ]] unparsed sentences , removing the [[ dependence ]] on an external parser at nearly no additional computational cost .",0
298,interpret,external parser,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously parse and [[ interpret ]] unparsed sentences , removing the dependence on an [[ external parser ]] at nearly no additional computational cost .",0
299,unparsed sentences,dependence,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously parse and interpret [[ unparsed sentences ]] , removing the [[ dependence ]] on an external parser at nearly no additional computational cost .",0
300,unparsed sentences,external parser,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously parse and interpret [[ unparsed sentences ]] , removing the dependence on an [[ external parser ]] at nearly no additional computational cost .",0
301,dependence,external parser,"[['interpret', 'has', 'unparsed sentences']]","At test time , it can simultaneously parse and interpret unparsed sentences , removing the [[ dependence ]] on an [[ external parser ]] at nearly no additional computational cost .",0
302,curriculum rate ?,probability,[],A [[ curriculum rate ? ]] is used to control the [[ probability ]] of replacing the true tokens with the generated ones .,0
303,curriculum rate ?,replacing the true tokens,[],A [[ curriculum rate ? ]] is used to control the probability of [[ replacing the true tokens ]] with the generated ones .,0
304,curriculum rate ?,generated ones,[],A [[ curriculum rate ? ]] is used to control the probability of replacing the true tokens with the [[ generated ones ]] .,0
305,probability,replacing the true tokens,[],A curriculum rate ? is used to control the [[ probability ]] of [[ replacing the true tokens ]] with the generated ones .,0
306,probability,generated ones,[],A curriculum rate ? is used to control the [[ probability ]] of replacing the true tokens with the [[ generated ones ]] .,0
307,replacing the true tokens,generated ones,[],A curriculum rate ? is used to control the probability of [[ replacing the true tokens ]] with the [[ generated ones ]] .,0
308,our model,Adadelta,[],We train [[ our model ]] using [[ Adadelta ]] with gradient clipping .,0
309,our model,gradient clipping,[],We train [[ our model ]] using Adadelta with [[ gradient clipping ]] .,0
310,Adadelta,gradient clipping,[],We train our model using [[ Adadelta ]] with [[ gradient clipping ]] .,0
311,span representations,entity mention detection,[],The [[ span representations ]] are used to perform [[ entity mention detection ]] on all spans in parallel .,0
312,span representations,all spans,[],The [[ span representations ]] are used to perform entity mention detection on [[ all spans ]] in parallel .,0
313,span representations,parallel,[],The [[ span representations ]] are used to perform entity mention detection on all spans in [[ parallel ]] .,0
314,entity mention detection,all spans,[],The span representations are used to perform [[ entity mention detection ]] on [[ all spans ]] in parallel .,0
315,entity mention detection,parallel,[],The span representations are used to perform [[ entity mention detection ]] on all spans in [[ parallel ]] .,0
316,all spans,parallel,[],The span representations are used to perform entity mention detection on [[ all spans ]] in [[ parallel ]] .,0
317,key competitors,Neural Tensor LSTM ( NTN - LSTM ),[],- The [[ key competitors ]] of this dataset are the [[ Neural Tensor LSTM ( NTN - LSTM ) ]] and HD - LSTM from Tay et al.,0
318,key competitors,HD - LSTM,[],- The [[ key competitors ]] of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and [[ HD - LSTM ]] from Tay et al.,0
319,key competitors,Tay et al.,[],- The [[ key competitors ]] of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and HD - LSTM from [[ Tay et al. ]],0
320,Neural Tensor LSTM ( NTN - LSTM ),HD - LSTM,[],- The key competitors of this dataset are the [[ Neural Tensor LSTM ( NTN - LSTM ) ]] and [[ HD - LSTM ]] from Tay et al.,0
321,Neural Tensor LSTM ( NTN - LSTM ),Tay et al.,[],- The key competitors of this dataset are the [[ Neural Tensor LSTM ( NTN - LSTM ) ]] and HD - LSTM from [[ Tay et al. ]],0
322,HD - LSTM,Tay et al.,[],- The key competitors of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and [[ HD - LSTM ]] from [[ Tay et al. ]],0
323,model,at most 30 epochs,[],"We trained the [[ model ]] for [[ at most 30 epochs ]] , and in case the accuracy did not improve for 10 epochs , we stopped training .",0
324,our model,averaged dev accuracy,[],We early stop [[ our model ]] when the [[ averaged dev accuracy ]] stop increasing .,0
325,our model,increasing,[],We early stop [[ our model ]] when the averaged dev accuracy stop [[ increasing ]] .,0
326,averaged dev accuracy,increasing,[],We early stop our model when the [[ averaged dev accuracy ]] stop [[ increasing ]] .,0
327,number of layers,convolutional encoder,[],The [[ number of layers ]] of the [[ convolutional encoder ]] is tuned from 1 to 3 .,0
328,number of layers,1,[],The [[ number of layers ]] of the convolutional encoder is tuned from [[ 1 ]] to 3 .,0
329,number of layers,3,[],The [[ number of layers ]] of the convolutional encoder is tuned from 1 to [[ 3 ]] .,0
330,convolutional encoder,1,[],The number of layers of the [[ convolutional encoder ]] is tuned from [[ 1 ]] to 3 .,0
331,convolutional encoder,3,[],The number of layers of the [[ convolutional encoder ]] is tuned from 1 to [[ 3 ]] .,0
332,1,3,[],The number of layers of the convolutional encoder is tuned from [[ 1 ]] to [[ 3 ]] .,0
333,widely - used Information Retrieval ( IR ) platform,candidate soft templates,[],We utilize a [[ widely - used Information Retrieval ( IR ) platform ]] to find out [[ candidate soft templates ]] from the training corpus .,0
334,widely - used Information Retrieval ( IR ) platform,training corpus,[],We utilize a [[ widely - used Information Retrieval ( IR ) platform ]] to find out candidate soft templates from the [[ training corpus ]] .,0
335,candidate soft templates,training corpus,[],We utilize a widely - used Information Retrieval ( IR ) platform to find out [[ candidate soft templates ]] from the [[ training corpus ]] .,0
336,improvements,Relation predictions,[],"The [[ improvements ]] on the [[ Relation predictions ]] were mainly on the Contrastive set , specifically the class of Contrast , Comparison and Cause relations as .",0
337,improvements,Contrastive set,[],"The [[ improvements ]] on the Relation predictions were mainly on the [[ Contrastive set ]] , specifically the class of Contrast , Comparison and Cause relations as .",0
338,improvements,Contrast,[],"The [[ improvements ]] on the Relation predictions were mainly on the Contrastive set , specifically the class of [[ Contrast ]] , Comparison and Cause relations as .",0
339,improvements,Comparison,[],"The [[ improvements ]] on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , [[ Comparison ]] and Cause relations as .",0
340,improvements,Cause,[],"The [[ improvements ]] on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , Comparison and [[ Cause ]] relations as .",0
341,Relation predictions,Contrastive set,[],"The improvements on the [[ Relation predictions ]] were mainly on the [[ Contrastive set ]] , specifically the class of Contrast , Comparison and Cause relations as .",0
342,Relation predictions,Contrast,[],"The improvements on the [[ Relation predictions ]] were mainly on the Contrastive set , specifically the class of [[ Contrast ]] , Comparison and Cause relations as .",0
343,Relation predictions,Comparison,[],"The improvements on the [[ Relation predictions ]] were mainly on the Contrastive set , specifically the class of Contrast , [[ Comparison ]] and Cause relations as .",0
344,Relation predictions,Cause,[],"The improvements on the [[ Relation predictions ]] were mainly on the Contrastive set , specifically the class of Contrast , Comparison and [[ Cause ]] relations as .",0
345,Contrastive set,Contrast,[],"The improvements on the Relation predictions were mainly on the [[ Contrastive set ]] , specifically the class of [[ Contrast ]] , Comparison and Cause relations as .",0
346,Contrastive set,Comparison,[],"The improvements on the Relation predictions were mainly on the [[ Contrastive set ]] , specifically the class of Contrast , [[ Comparison ]] and Cause relations as .",0
347,Contrastive set,Cause,[],"The improvements on the Relation predictions were mainly on the [[ Contrastive set ]] , specifically the class of Contrast , Comparison and [[ Cause ]] relations as .",0
348,Contrast,Comparison,[],"The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of [[ Contrast ]] , [[ Comparison ]] and Cause relations as .",0
349,Contrast,Cause,[],"The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of [[ Contrast ]] , Comparison and [[ Cause ]] relations as .",0
350,Comparison,Cause,[],"The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , [[ Comparison ]] and [[ Cause ]] relations as .",0
351,sentiment classifier,dropout,[],The [[ sentiment classifier ]] was additionally regularized using [[ dropout ]] with a dropout rate of 0.5 .,0
352,sentiment classifier,dropout rate,[],The [[ sentiment classifier ]] was additionally regularized using dropout with a [[ dropout rate ]] of 0.5 .,0
353,sentiment classifier,0.5,[],The [[ sentiment classifier ]] was additionally regularized using dropout with a dropout rate of [[ 0.5 ]] .,0
354,dropout,dropout rate,[],The sentiment classifier was additionally regularized using [[ dropout ]] with a [[ dropout rate ]] of 0.5 .,0
355,dropout,0.5,[],The sentiment classifier was additionally regularized using [[ dropout ]] with a dropout rate of [[ 0.5 ]] .,0
356,dropout rate,0.5,[],The sentiment classifier was additionally regularized using dropout with a [[ dropout rate ]] of [[ 0.5 ]] .,0
357,selection mask,source document,[],Our approach first selects a [[ selection mask ]] for the [[ source document ]] and then constrains a standard neural model by this mask .,0
358,selection mask,constrains a standard neural model,[],Our approach first selects a [[ selection mask ]] for the source document and then [[ constrains a standard neural model ]] by this mask .,0
359,source document,constrains a standard neural model,[],Our approach first selects a selection mask for the [[ source document ]] and then [[ constrains a standard neural model ]] by this mask .,0
360,performance,paragraph vector model,[],"For comparison , we also report the [[ performance ]] of the [[ paragraph vector model ]] ( PV ; ; see , second block ) which neither operates on trees nor sequences but learns distributed document representations parameterized directly .",0
361,MGAN - CF,coarse - grained and fine - grained attentions,[],"[[ MGAN - CF ]] adopts both the [[ coarse - grained and fine - grained attentions ]] , while without applying the aspect alignment loss .",0
362,binary relation extraction,AGGCN,"[['binary relation extraction', 'has', 'AGGCN']]","For [[ binary relation extraction ]] ( third and fourth columns in ) , [[ AGGCN ]] consistently outperforms GS GLSTM and GCN as well .",1
363,binary relation extraction,GS GLSTM,"[['binary relation extraction', 'has', 'AGGCN']]","For [[ binary relation extraction ]] ( third and fourth columns in ) , AGGCN consistently outperforms [[ GS GLSTM ]] and GCN as well .",0
364,binary relation extraction,GCN,"[['binary relation extraction', 'has', 'AGGCN']]","For [[ binary relation extraction ]] ( third and fourth columns in ) , AGGCN consistently outperforms GS GLSTM and [[ GCN ]] as well .",0
365,AGGCN,GS GLSTM,"[['binary relation extraction', 'has', 'AGGCN']]","For binary relation extraction ( third and fourth columns in ) , [[ AGGCN ]] consistently outperforms [[ GS GLSTM ]] and GCN as well .",0
366,AGGCN,GCN,"[['binary relation extraction', 'has', 'AGGCN']]","For binary relation extraction ( third and fourth columns in ) , [[ AGGCN ]] consistently outperforms GS GLSTM and [[ GCN ]] as well .",0
367,GS GLSTM,GCN,"[['binary relation extraction', 'has', 'AGGCN']]","For binary relation extraction ( third and fourth columns in ) , AGGCN consistently outperforms [[ GS GLSTM ]] and [[ GCN ]] as well .",0
368,Dropout,keep probability,[],[[ Dropout ]] with a [[ keep probability ]] of 0.8 is applied before every fully - connected or convolutional layer .,0
369,Dropout,0.8,[],[[ Dropout ]] with a keep probability of [[ 0.8 ]] is applied before every fully - connected or convolutional layer .,0
370,Dropout,fully - connected,[],[[ Dropout ]] with a keep probability of 0.8 is applied before every [[ fully - connected ]] or convolutional layer .,0
371,Dropout,convolutional layer,[],[[ Dropout ]] with a keep probability of 0.8 is applied before every fully - connected or [[ convolutional layer ]] .,0
372,keep probability,0.8,[],Dropout with a [[ keep probability ]] of [[ 0.8 ]] is applied before every fully - connected or convolutional layer .,0
373,keep probability,fully - connected,[],Dropout with a [[ keep probability ]] of 0.8 is applied before every [[ fully - connected ]] or convolutional layer .,0
374,keep probability,convolutional layer,[],Dropout with a [[ keep probability ]] of 0.8 is applied before every fully - connected or [[ convolutional layer ]] .,0
375,0.8,fully - connected,[],Dropout with a keep probability of [[ 0.8 ]] is applied before every [[ fully - connected ]] or convolutional layer .,0
376,0.8,convolutional layer,[],Dropout with a keep probability of [[ 0.8 ]] is applied before every fully - connected or [[ convolutional layer ]] .,0
377,fully - connected,convolutional layer,[],Dropout with a keep probability of 0.8 is applied before every [[ fully - connected ]] or [[ convolutional layer ]] .,0
378,difference and elementwise product,local inference enhancement layer,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the [[ difference and elementwise product ]] from the [[ local inference enhancement layer ]] , the accuracy drops to 87.0 % .",0
379,difference and elementwise product,accuracy,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the [[ difference and elementwise product ]] from the local inference enhancement layer , the [[ accuracy ]] drops to 87.0 % .",0
380,difference and elementwise product,drops,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the [[ difference and elementwise product ]] from the local inference enhancement layer , the accuracy [[ drops ]] to 87.0 % .",0
381,difference and elementwise product,87.0 %,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the [[ difference and elementwise product ]] from the local inference enhancement layer , the accuracy drops to [[ 87.0 % ]] .",0
382,local inference enhancement layer,accuracy,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the difference and elementwise product from the [[ local inference enhancement layer ]] , the [[ accuracy ]] drops to 87.0 % .",1
383,local inference enhancement layer,drops,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the difference and elementwise product from the [[ local inference enhancement layer ]] , the accuracy [[ drops ]] to 87.0 % .",0
384,local inference enhancement layer,87.0 %,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the difference and elementwise product from the [[ local inference enhancement layer ]] , the accuracy drops to [[ 87.0 % ]] .",0
385,accuracy,drops,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the difference and elementwise product from the local inference enhancement layer , the [[ accuracy ]] [[ drops ]] to 87.0 % .",0
386,accuracy,87.0 %,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the difference and elementwise product from the local inference enhancement layer , the [[ accuracy ]] drops to [[ 87.0 % ]] .",0
387,drops,87.0 %,"[['local inference enhancement layer', 'has', 'accuracy']]","If we remove the difference and elementwise product from the local inference enhancement layer , the accuracy [[ drops ]] to [[ 87.0 % ]] .",0
388,maximum characters per word,16,[],The [[ maximum characters per word ]] is set to [[ 16 ]] .,0
389,semantic cues,simple sequential encoding model,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the [[ semantic cues ]] , the [[ simple sequential encoding model ]] yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .",1
390,semantic cues,substantial gains,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the [[ semantic cues ]] , the simple sequential encoding model yields [[ substantial gains ]] , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .",0
391,semantic cues,our single BERT LARGE model,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the [[ semantic cues ]] , the simple sequential encoding model yields substantial gains , and [[ our single BERT LARGE model ]] also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .",1
392,semantic cues,new stateof - the - art,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the [[ semantic cues ]] , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a [[ new stateof - the - art ]] , even outperforms all the ensemble models in the leaderboard 8 .",0
393,semantic cues,outperforms,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the [[ semantic cues ]] , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even [[ outperforms ]] all the ensemble models in the leaderboard 8 .",0
394,semantic cues,all the ensemble models,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the [[ semantic cues ]] , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms [[ all the ensemble models ]] in the leaderboard 8 .",0
395,semantic cues,leaderboard,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the [[ semantic cues ]] , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the [[ leaderboard ]] 8 .",0
396,simple sequential encoding model,substantial gains,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the [[ simple sequential encoding model ]] yields [[ substantial gains ]] , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .",0
397,simple sequential encoding model,our single BERT LARGE model,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the [[ simple sequential encoding model ]] yields substantial gains , and [[ our single BERT LARGE model ]] also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .",0
398,simple sequential encoding model,new stateof - the - art,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the [[ simple sequential encoding model ]] yields substantial gains , and our single BERT LARGE model also achieves a [[ new stateof - the - art ]] , even outperforms all the ensemble models in the leaderboard 8 .",0
399,simple sequential encoding model,outperforms,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the [[ simple sequential encoding model ]] yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even [[ outperforms ]] all the ensemble models in the leaderboard 8 .",0
400,simple sequential encoding model,all the ensemble models,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the [[ simple sequential encoding model ]] yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms [[ all the ensemble models ]] in the leaderboard 8 .",0
401,simple sequential encoding model,leaderboard,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the [[ simple sequential encoding model ]] yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the [[ leaderboard ]] 8 .",0
402,substantial gains,our single BERT LARGE model,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields [[ substantial gains ]] , and [[ our single BERT LARGE model ]] also achieves a new stateof - the - art , even outperforms all the ensemble models in the leaderboard 8 .",0
403,substantial gains,new stateof - the - art,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields [[ substantial gains ]] , and our single BERT LARGE model also achieves a [[ new stateof - the - art ]] , even outperforms all the ensemble models in the leaderboard 8 .",0
404,substantial gains,outperforms,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields [[ substantial gains ]] , and our single BERT LARGE model also achieves a new stateof - the - art , even [[ outperforms ]] all the ensemble models in the leaderboard 8 .",0
405,substantial gains,all the ensemble models,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields [[ substantial gains ]] , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms [[ all the ensemble models ]] in the leaderboard 8 .",0
406,substantial gains,leaderboard,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields [[ substantial gains ]] , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms all the ensemble models in the [[ leaderboard ]] 8 .",0
407,our single BERT LARGE model,new stateof - the - art,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and [[ our single BERT LARGE model ]] also achieves a [[ new stateof - the - art ]] , even outperforms all the ensemble models in the leaderboard 8 .",0
408,our single BERT LARGE model,outperforms,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and [[ our single BERT LARGE model ]] also achieves a new stateof - the - art , even [[ outperforms ]] all the ensemble models in the leaderboard 8 .",1
409,our single BERT LARGE model,all the ensemble models,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and [[ our single BERT LARGE model ]] also achieves a new stateof - the - art , even outperforms [[ all the ensemble models ]] in the leaderboard 8 .",0
410,our single BERT LARGE model,leaderboard,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and [[ our single BERT LARGE model ]] also achieves a new stateof - the - art , even outperforms all the ensemble models in the [[ leaderboard ]] 8 .",0
411,new stateof - the - art,outperforms,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a [[ new stateof - the - art ]] , even [[ outperforms ]] all the ensemble models in the leaderboard 8 .",0
412,new stateof - the - art,all the ensemble models,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a [[ new stateof - the - art ]] , even outperforms [[ all the ensemble models ]] in the leaderboard 8 .",0
413,new stateof - the - art,leaderboard,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a [[ new stateof - the - art ]] , even outperforms all the ensemble models in the [[ leaderboard ]] 8 .",0
414,outperforms,all the ensemble models,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even [[ outperforms ]] [[ all the ensemble models ]] in the leaderboard 8 .",1
415,outperforms,leaderboard,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even [[ outperforms ]] all the ensemble models in the [[ leaderboard ]] 8 .",0
416,all the ensemble models,leaderboard,"[['semantic cues', 'has', 'simple sequential encoding model'], ['semantic cues', 'has', 'our single BERT LARGE model'], ['our single BERT LARGE model', 'has', 'outperforms'], ['outperforms', 'has', 'all the ensemble models']]","With the semantic cues , the simple sequential encoding model yields substantial gains , and our single BERT LARGE model also achieves a new stateof - the - art , even outperforms [[ all the ensemble models ]] in the [[ leaderboard ]] 8 .",0
417,Gumbel - softmax,y from q ( y|x ),[],We use [[ Gumbel - softmax ]] to sample [[ y from q ( y|x ) ]] .,0
418,neural attention model,combination of semantic similarity - based attention and positionbased attention,[],This means that the [[ neural attention model ]] can effectively exploit the [[ combination of semantic similarity - based attention and positionbased attention ]] .,0
419,Feature - based SVM,state - of - the - art,[],( 2 ) [[ Feature - based SVM ]] performs [[ state - of - the - art ]] on aspect level sentiment classification .,0
420,Feature - based SVM,aspect level sentiment classification,[],( 2 ) [[ Feature - based SVM ]] performs state - of - the - art on [[ aspect level sentiment classification ]] .,0
421,state - of - the - art,aspect level sentiment classification,[],( 2 ) Feature - based SVM performs [[ state - of - the - art ]] on [[ aspect level sentiment classification ]] .,0
422,BM25,Anserini open - source IR toolkit,[],[[ BM25 ]] : We use the [[ Anserini open - source IR toolkit ]] 3 to index the original ( non -expanded ) documents and BM25 to rank the passages .,0
423,BM25,original ( non -expanded ) documents,[],[[ BM25 ]] : We use the Anserini open - source IR toolkit 3 to index the [[ original ( non -expanded ) documents ]] and BM25 to rank the passages .,0
424,BM25,passages,[],[[ BM25 ]] : We use the Anserini open - source IR toolkit 3 to index the original ( non -expanded ) documents and BM25 to rank the [[ passages ]] .,0
425,Anserini open - source IR toolkit,original ( non -expanded ) documents,[],BM25 : We use the [[ Anserini open - source IR toolkit ]] 3 to index the [[ original ( non -expanded ) documents ]] and BM25 to rank the passages .,0
426,Anserini open - source IR toolkit,passages,[],BM25 : We use the [[ Anserini open - source IR toolkit ]] 3 to index the original ( non -expanded ) documents and BM25 to rank the [[ passages ]] .,0
427,original ( non -expanded ) documents,passages,[],BM25 : We use the Anserini open - source IR toolkit 3 to index the [[ original ( non -expanded ) documents ]] and BM25 to rank the [[ passages ]] .,0
428,inserting,memory network module,"[['inserting', 'has', 'memory network module']]",This is realized by [[ inserting ]] a [[ memory network module ]] in the update of a recurrent network together with attention for memory addressing .,1
429,inserting,update,"[['inserting', 'has', 'memory network module']]",This is realized by [[ inserting ]] a memory network module in the [[ update ]] of a recurrent network together with attention for memory addressing .,0
430,inserting,recurrent network,"[['inserting', 'has', 'memory network module']]",This is realized by [[ inserting ]] a memory network module in the update of a [[ recurrent network ]] together with attention for memory addressing .,0
431,inserting,attention,"[['inserting', 'has', 'memory network module']]",This is realized by [[ inserting ]] a memory network module in the update of a recurrent network together with [[ attention ]] for memory addressing .,0
432,inserting,memory addressing,"[['inserting', 'has', 'memory network module']]",This is realized by [[ inserting ]] a memory network module in the update of a recurrent network together with attention for [[ memory addressing ]] .,0
433,memory network module,update,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a [[ memory network module ]] in the [[ update ]] of a recurrent network together with attention for memory addressing .,0
434,memory network module,recurrent network,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a [[ memory network module ]] in the update of a [[ recurrent network ]] together with attention for memory addressing .,0
435,memory network module,attention,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a [[ memory network module ]] in the update of a recurrent network together with [[ attention ]] for memory addressing .,0
436,memory network module,memory addressing,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a [[ memory network module ]] in the update of a recurrent network together with attention for [[ memory addressing ]] .,0
437,update,recurrent network,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a memory network module in the [[ update ]] of a [[ recurrent network ]] together with attention for memory addressing .,0
438,update,attention,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a memory network module in the [[ update ]] of a recurrent network together with [[ attention ]] for memory addressing .,0
439,update,memory addressing,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a memory network module in the [[ update ]] of a recurrent network together with attention for [[ memory addressing ]] .,0
440,recurrent network,attention,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a memory network module in the update of a [[ recurrent network ]] together with [[ attention ]] for memory addressing .,0
441,recurrent network,memory addressing,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a memory network module in the update of a [[ recurrent network ]] together with attention for [[ memory addressing ]] .,0
442,attention,memory addressing,"[['inserting', 'has', 'memory network module']]",This is realized by inserting a memory network module in the update of a recurrent network together with [[ attention ]] for [[ memory addressing ]] .,0
443,PyTorch,single Tesla P40 GPU,[],"All models are implemented in [[ PyTorch ]] and trained on [[ single Tesla P40 GPU ]] , based on NAVER Smart Machine Learning ( NSML ) platform .",0
444,PyTorch,NAVER Smart Machine Learning ( NSML ) platform,[],"All models are implemented in [[ PyTorch ]] and trained on single Tesla P40 GPU , based on [[ NAVER Smart Machine Learning ( NSML ) platform ]] .",0
445,single Tesla P40 GPU,NAVER Smart Machine Learning ( NSML ) platform,[],"All models are implemented in PyTorch and trained on [[ single Tesla P40 GPU ]] , based on [[ NAVER Smart Machine Learning ( NSML ) platform ]] .",0
446,network,8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ),[],"Our [[ network ]] consists of [[ 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) ]] with 300 dimensional hidden units , and a softmax layer for predicting the output distribution .",0
447,network,300 dimensional hidden units,[],"Our [[ network ]] consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with [[ 300 dimensional hidden units ]] , and a softmax layer for predicting the output distribution .",0
448,network,softmax layer,[],"Our [[ network ]] consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with 300 dimensional hidden units , and a [[ softmax layer ]] for predicting the output distribution .",0
449,network,output distribution,[],"Our [[ network ]] consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with 300 dimensional hidden units , and a softmax layer for predicting the [[ output distribution ]] .",0
450,8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ),300 dimensional hidden units,[],"Our network consists of [[ 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) ]] with [[ 300 dimensional hidden units ]] , and a softmax layer for predicting the output distribution .",0
451,8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ),softmax layer,[],"Our network consists of [[ 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) ]] with 300 dimensional hidden units , and a [[ softmax layer ]] for predicting the output distribution .",0
452,8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ),output distribution,[],"Our network consists of [[ 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) ]] with 300 dimensional hidden units , and a softmax layer for predicting the [[ output distribution ]] .",0
453,300 dimensional hidden units,softmax layer,[],"Our network consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with [[ 300 dimensional hidden units ]] , and a [[ softmax layer ]] for predicting the output distribution .",0
454,300 dimensional hidden units,output distribution,[],"Our network consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with [[ 300 dimensional hidden units ]] , and a softmax layer for predicting the [[ output distribution ]] .",0
455,softmax layer,output distribution,[],"Our network consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with 300 dimensional hidden units , and a [[ softmax layer ]] for predicting the [[ output distribution ]] .",0
456,10 %,training set,[],We use [[ 10 % ]] of the [[ training set ]] as a held - out validation set for hyperparameter tuning .,0
457,10 %,held - out validation set,[],We use [[ 10 % ]] of the training set as a [[ held - out validation set ]] for hyperparameter tuning .,0
458,10 %,hyperparameter tuning,[],We use [[ 10 % ]] of the training set as a held - out validation set for [[ hyperparameter tuning ]] .,0
459,training set,held - out validation set,[],We use 10 % of the [[ training set ]] as a [[ held - out validation set ]] for hyperparameter tuning .,0
460,training set,hyperparameter tuning,[],We use 10 % of the [[ training set ]] as a held - out validation set for [[ hyperparameter tuning ]] .,0
461,held - out validation set,hyperparameter tuning,[],We use 10 % of the training set as a [[ held - out validation set ]] for [[ hyperparameter tuning ]] .,0
462,1 k data,QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ),"[['1 k data', 'has', ""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )""]]","In [[ 1 k data ]] , [[ QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) ]] outperforms all other models by a large margin ( 2.8 + % ) .",1
463,1 k data,all other models,"[['1 k data', 'has', ""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )""]]","In [[ 1 k data ]] , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms [[ all other models ]] by a large margin ( 2.8 + % ) .",0
464,1 k data,large margin ( 2.8 + % ),"[['1 k data', 'has', ""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )""]]","In [[ 1 k data ]] , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a [[ large margin ( 2.8 + % ) ]] .",0
465,QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ),all other models,"[['1 k data', 'has', ""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )""]]","In 1 k data , [[ QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) ]] outperforms [[ all other models ]] by a large margin ( 2.8 + % ) .",0
466,QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ),large margin ( 2.8 + % ),"[['1 k data', 'has', ""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )""]]","In 1 k data , [[ QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) ]] outperforms all other models by a [[ large margin ( 2.8 + % ) ]] .",0
467,all other models,large margin ( 2.8 + % ),"[['1 k data', 'has', ""QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )""]]","In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms [[ all other models ]] by a [[ large margin ( 2.8 + % ) ]] .",0
468,word embeddings ( + POLYGLOT ),off - the - shelf languagespecific embeddings,[],Initializing the [[ word embeddings ( + POLYGLOT ) ]] with [[ off - the - shelf languagespecific embeddings ]] further improves accuracy .,0
469,word embeddings ( + POLYGLOT ),accuracy,[],Initializing the [[ word embeddings ( + POLYGLOT ) ]] with off - the - shelf languagespecific embeddings further improves [[ accuracy ]] .,0
470,off - the - shelf languagespecific embeddings,accuracy,[],Initializing the word embeddings ( + POLYGLOT ) with [[ off - the - shelf languagespecific embeddings ]] further improves [[ accuracy ]] .,0
471,document expansion,query expansion,[],"To compare [[ document expansion ]] with [[ query expansion ]] , we applied the RM3 query expansion technique .",0
472,document expansion,RM3 query expansion technique,[],"To compare [[ document expansion ]] with query expansion , we applied the [[ RM3 query expansion technique ]] .",0
473,query expansion,RM3 query expansion technique,[],"To compare document expansion with [[ query expansion ]] , we applied the [[ RM3 query expansion technique ]] .",0
474,L2 regularization decay factors,5 10 ?5 and 10 ? 4,[],"The [[ L2 regularization decay factors ]] ? are [[ 5 10 ?5 and 10 ? 4 ]] for language inference and sentiment analysis , respectively .",0
475,L2 regularization decay factors,language inference and sentiment analysis,[],"The [[ L2 regularization decay factors ]] ? are 5 10 ?5 and 10 ? 4 for [[ language inference and sentiment analysis ]] , respectively .",0
476,5 10 ?5 and 10 ? 4,language inference and sentiment analysis,[],"The L2 regularization decay factors ? are [[ 5 10 ?5 and 10 ? 4 ]] for [[ language inference and sentiment analysis ]] , respectively .",0
477,our model,TensorFlow,[],We implement [[ our model ]] in [[ TensorFlow ]] and train them on Nvidia P100 GPUs .,0
478,our model,Nvidia P100 GPUs,[],We implement [[ our model ]] in TensorFlow and train them on [[ Nvidia P100 GPUs ]] .,0
479,TensorFlow,Nvidia P100 GPUs,[],We implement our model in [[ TensorFlow ]] and train them on [[ Nvidia P100 GPUs ]] .,0
480,batch size,"{ 128 , 256 , 512 }",[],"The [[ batch size ]] is tuned amongst [[ { 128 , 256 , 512 } ]] .",0
481,two methods,performing prediction,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides [[ two methods ]] for [[ performing prediction ]] : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .",0
482,two methods,each token 's label independently,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides [[ two methods ]] for performing prediction : we can predict [[ each token 's label independently ]] , or by running Viterbi inference in a chain structured graphical model .",0
483,two methods,Viterbi inference,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides [[ two methods ]] for performing prediction : we can predict each token 's label independently , or by running [[ Viterbi inference ]] in a chain structured graphical model .",0
484,two methods,chain structured graphical model,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides [[ two methods ]] for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a [[ chain structured graphical model ]] .",0
485,performing prediction,each token 's label independently,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for [[ performing prediction ]] : we can predict [[ each token 's label independently ]] , or by running Viterbi inference in a chain structured graphical model .",0
486,performing prediction,Viterbi inference,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for [[ performing prediction ]] : we can predict each token 's label independently , or by running [[ Viterbi inference ]] in a chain structured graphical model .",0
487,performing prediction,chain structured graphical model,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for [[ performing prediction ]] : we can predict each token 's label independently , or by running Viterbi inference in a [[ chain structured graphical model ]] .",0
488,each token 's label independently,Viterbi inference,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict [[ each token 's label independently ]] , or by running [[ Viterbi inference ]] in a chain structured graphical model .",0
489,each token 's label independently,chain structured graphical model,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict [[ each token 's label independently ]] , or by running Viterbi inference in a [[ chain structured graphical model ]] .",0
490,Viterbi inference,chain structured graphical model,[],"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running [[ Viterbi inference ]] in a [[ chain structured graphical model ]] .",0
491,MTB based training,low - resource cases,[],"For all tasks , we see that [[ MTB based training ]] is even more effective for [[ low - resource cases ]] , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .",0
492,best use of LSTM,the resulting model,[],"We pursue the [[ best use of LSTM ]] for our purpose , and then compare [[ the resulting model ]] with the previous best methods including one - hot CNN and previous LSTM .",0
493,best use of LSTM,previous best methods,[],"We pursue the [[ best use of LSTM ]] for our purpose , and then compare the resulting model with the [[ previous best methods ]] including one - hot CNN and previous LSTM .",0
494,best use of LSTM,one - hot CNN and previous LSTM,[],"We pursue the [[ best use of LSTM ]] for our purpose , and then compare the resulting model with the previous best methods including [[ one - hot CNN and previous LSTM ]] .",0
495,the resulting model,previous best methods,[],"We pursue the best use of LSTM for our purpose , and then compare [[ the resulting model ]] with the [[ previous best methods ]] including one - hot CNN and previous LSTM .",0
496,the resulting model,one - hot CNN and previous LSTM,[],"We pursue the best use of LSTM for our purpose , and then compare [[ the resulting model ]] with the previous best methods including [[ one - hot CNN and previous LSTM ]] .",0
497,previous best methods,one - hot CNN and previous LSTM,[],"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the [[ previous best methods ]] including [[ one - hot CNN and previous LSTM ]] .",0
498,"dual - history variants ( variants 3 , 5 , and 7 )",these models,"[['dual - history variants ( variants 3 , 5 , and 7 )', 'has', 'these models']]","Compared to the [[ dual - history variants ( variants 3 , 5 , and 7 ) ]] , [[ these models ]] provide lesser performance .",1
499,"dual - history variants ( variants 3 , 5 , and 7 )",lesser performance,"[['dual - history variants ( variants 3 , 5 , and 7 )', 'has', 'these models']]","Compared to the [[ dual - history variants ( variants 3 , 5 , and 7 ) ]] , these models provide [[ lesser performance ]] .",0
500,these models,lesser performance,"[['dual - history variants ( variants 3 , 5 , and 7 )', 'has', 'these models']]","Compared to the dual - history variants ( variants 3 , 5 , and 7 ) , [[ these models ]] provide [[ lesser performance ]] .",0
501,distribution,sampling negative words,[],"For the [[ distribution ]] for [[ sampling negative words ]] , we used the n-gram distribution raised to the 3 / 4 th power in accordance with .",0
502,distribution,n-gram distribution,[],"For the [[ distribution ]] for sampling negative words , we used the [[ n-gram distribution ]] raised to the 3 / 4 th power in accordance with .",0
503,distribution,3 / 4 th power,[],"For the [[ distribution ]] for sampling negative words , we used the n-gram distribution raised to the [[ 3 / 4 th power ]] in accordance with .",0
504,sampling negative words,n-gram distribution,[],"For the distribution for [[ sampling negative words ]] , we used the [[ n-gram distribution ]] raised to the 3 / 4 th power in accordance with .",0
505,sampling negative words,3 / 4 th power,[],"For the distribution for [[ sampling negative words ]] , we used the n-gram distribution raised to the [[ 3 / 4 th power ]] in accordance with .",0
506,n-gram distribution,3 / 4 th power,[],"For the distribution for sampling negative words , we used the [[ n-gram distribution ]] raised to the [[ 3 / 4 th power ]] in accordance with .",0
507,300d MRU,comparable performance,[],"First , we observe that [[ 300d MRU ]] can achieve [[ comparable performance ]] with BiDAF .",0
508,300d MRU,BiDAF,[],"First , we observe that [[ 300d MRU ]] can achieve comparable performance with [[ BiDAF ]] .",0
509,comparable performance,BiDAF,[],"First , we observe that 300d MRU can achieve [[ comparable performance ]] with [[ BiDAF ]] .",0
510,global GRU,corresponding party information,[],The [[ global GRU ]] encodes [[ corresponding party information ]] while encoding an utterance .,0
511,global GRU,utterance,[],The [[ global GRU ]] encodes corresponding party information while encoding an [[ utterance ]] .,0
512,corresponding party information,utterance,[],The global GRU encodes [[ corresponding party information ]] while encoding an [[ utterance ]] .,0
513,scheduled sampling,gradually changes,[],"In the [[ scheduled sampling ]] , the training process [[ gradually changes ]] from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",0
514,scheduled sampling,fully guided scheme feeding the true previous tokens,[],"In the [[ scheduled sampling ]] , the training process gradually changes from a [[ fully guided scheme feeding the true previous tokens ]] into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",0
515,scheduled sampling,LSTM,[],"In the [[ scheduled sampling ]] , the training process gradually changes from a fully guided scheme feeding the true previous tokens into [[ LSTM ]] , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",0
516,scheduled sampling,less guided scheme which mostly feeds the LSTM,[],"In the [[ scheduled sampling ]] , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a [[ less guided scheme which mostly feeds the LSTM ]] with its generated tokens .",0
517,scheduled sampling,generated tokens,[],"In the [[ scheduled sampling ]] , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its [[ generated tokens ]] .",0
518,gradually changes,fully guided scheme feeding the true previous tokens,[],"In the scheduled sampling , the training process [[ gradually changes ]] from a [[ fully guided scheme feeding the true previous tokens ]] into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",0
519,gradually changes,LSTM,[],"In the scheduled sampling , the training process [[ gradually changes ]] from a fully guided scheme feeding the true previous tokens into [[ LSTM ]] , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",0
520,gradually changes,less guided scheme which mostly feeds the LSTM,[],"In the scheduled sampling , the training process [[ gradually changes ]] from a fully guided scheme feeding the true previous tokens into LSTM , towards a [[ less guided scheme which mostly feeds the LSTM ]] with its generated tokens .",0
521,gradually changes,generated tokens,[],"In the scheduled sampling , the training process [[ gradually changes ]] from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its [[ generated tokens ]] .",0
522,fully guided scheme feeding the true previous tokens,LSTM,[],"In the scheduled sampling , the training process gradually changes from a [[ fully guided scheme feeding the true previous tokens ]] into [[ LSTM ]] , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",0
523,fully guided scheme feeding the true previous tokens,less guided scheme which mostly feeds the LSTM,[],"In the scheduled sampling , the training process gradually changes from a [[ fully guided scheme feeding the true previous tokens ]] into LSTM , towards a [[ less guided scheme which mostly feeds the LSTM ]] with its generated tokens .",0
524,fully guided scheme feeding the true previous tokens,generated tokens,[],"In the scheduled sampling , the training process gradually changes from a [[ fully guided scheme feeding the true previous tokens ]] into LSTM , towards a less guided scheme which mostly feeds the LSTM with its [[ generated tokens ]] .",0
525,LSTM,less guided scheme which mostly feeds the LSTM,[],"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into [[ LSTM ]] , towards a [[ less guided scheme which mostly feeds the LSTM ]] with its generated tokens .",0
526,LSTM,generated tokens,[],"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into [[ LSTM ]] , towards a less guided scheme which mostly feeds the LSTM with its [[ generated tokens ]] .",0
527,less guided scheme which mostly feeds the LSTM,generated tokens,[],"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a [[ less guided scheme which mostly feeds the LSTM ]] with its [[ generated tokens ]] .",0
528,stochastic gradient descent,optimization of models,[],"In other words , We use [[ stochastic gradient descent ]] for the [[ optimization of models ]] .",0
529,tokenizers,step,"[['preprocessing', 'has', 'data']]",The [[ tokenizers ]] we use in the [[ step ]] of preprocessing data are from Stanford CoreNLP .,0
530,tokenizers,preprocessing,"[['preprocessing', 'has', 'data']]",The [[ tokenizers ]] we use in the step of [[ preprocessing ]] data are from Stanford CoreNLP .,0
531,tokenizers,data,"[['preprocessing', 'has', 'data']]",The [[ tokenizers ]] we use in the step of preprocessing [[ data ]] are from Stanford CoreNLP .,0
532,tokenizers,Stanford CoreNLP,"[['preprocessing', 'has', 'data']]",The [[ tokenizers ]] we use in the step of preprocessing data are from [[ Stanford CoreNLP ]] .,0
533,step,preprocessing,"[['preprocessing', 'has', 'data']]",The tokenizers we use in the [[ step ]] of [[ preprocessing ]] data are from Stanford CoreNLP .,0
534,step,data,"[['preprocessing', 'has', 'data']]",The tokenizers we use in the [[ step ]] of preprocessing [[ data ]] are from Stanford CoreNLP .,0
535,step,Stanford CoreNLP,"[['preprocessing', 'has', 'data']]",The tokenizers we use in the [[ step ]] of preprocessing data are from [[ Stanford CoreNLP ]] .,0
536,preprocessing,data,"[['preprocessing', 'has', 'data']]",The tokenizers we use in the step of [[ preprocessing ]] [[ data ]] are from Stanford CoreNLP .,1
537,preprocessing,Stanford CoreNLP,"[['preprocessing', 'has', 'data']]",The tokenizers we use in the step of [[ preprocessing ]] data are from [[ Stanford CoreNLP ]] .,0
538,data,Stanford CoreNLP,"[['preprocessing', 'has', 'data']]",The tokenizers we use in the step of preprocessing [[ data ]] are from [[ Stanford CoreNLP ]] .,0
539,syntactic embeddings,semantic embeddings,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both [[ syntactic embeddings ]] and [[ semantic embeddings ]] contribute towards the model 's performance and the POS tags seem to be more important .",0
540,syntactic embeddings,contribute,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both [[ syntactic embeddings ]] and semantic embeddings [[ contribute ]] towards the model 's performance and the POS tags seem to be more important .",0
541,syntactic embeddings,model 's performance,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both [[ syntactic embeddings ]] and semantic embeddings contribute towards the [[ model 's performance ]] and the POS tags seem to be more important .",0
542,syntactic embeddings,POS tags,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both [[ syntactic embeddings ]] and semantic embeddings contribute towards the model 's performance and the [[ POS tags ]] seem to be more important .",0
543,syntactic embeddings,more important,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both [[ syntactic embeddings ]] and semantic embeddings contribute towards the model 's performance and the POS tags seem to be [[ more important ]] .",0
544,semantic embeddings,contribute,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and [[ semantic embeddings ]] [[ contribute ]] towards the model 's performance and the POS tags seem to be more important .",0
545,semantic embeddings,model 's performance,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and [[ semantic embeddings ]] contribute towards the [[ model 's performance ]] and the POS tags seem to be more important .",0
546,semantic embeddings,POS tags,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and [[ semantic embeddings ]] contribute towards the model 's performance and the [[ POS tags ]] seem to be more important .",0
547,semantic embeddings,more important,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and [[ semantic embeddings ]] contribute towards the model 's performance and the POS tags seem to be [[ more important ]] .",0
548,contribute,model 's performance,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and semantic embeddings [[ contribute ]] towards the [[ model 's performance ]] and the POS tags seem to be more important .",0
549,contribute,POS tags,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and semantic embeddings [[ contribute ]] towards the model 's performance and the [[ POS tags ]] seem to be more important .",0
550,contribute,more important,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and semantic embeddings [[ contribute ]] towards the model 's performance and the POS tags seem to be [[ more important ]] .",0
551,model 's performance,POS tags,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and semantic embeddings contribute towards the [[ model 's performance ]] and the [[ POS tags ]] seem to be more important .",1
552,model 's performance,more important,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and semantic embeddings contribute towards the [[ model 's performance ]] and the POS tags seem to be [[ more important ]] .",0
553,POS tags,more important,"[[""model 's performance"", 'has', 'POS tags']]","As shows , both syntactic embeddings and semantic embeddings contribute towards the model 's performance and the [[ POS tags ]] seem to be [[ more important ]] .",0
554,Including vector gates,1 k datasets,[],"( c ) [[ Including vector gates ]] hurts in [[ 1 k datasets ]] , as the model either overfits to the training data or converges to local minima .",0
555,test time,beam search,[],"At [[ test time ]] , we run [[ beam search ]] to produce the summary with a beam size of 5 .",0
556,test time,summary,[],"At [[ test time ]] , we run beam search to produce the [[ summary ]] with a beam size of 5 .",0
557,test time,beam size of 5,[],"At [[ test time ]] , we run beam search to produce the summary with a [[ beam size of 5 ]] .",0
558,beam search,summary,[],"At test time , we run [[ beam search ]] to produce the [[ summary ]] with a beam size of 5 .",0
559,beam search,beam size of 5,[],"At test time , we run [[ beam search ]] to produce the summary with a [[ beam size of 5 ]] .",0
560,summary,beam size of 5,[],"At test time , we run beam search to produce the [[ summary ]] with a [[ beam size of 5 ]] .",0
561,RASOR,endpoint prediction model,[],"[[ RASOR ]] outperforms the [[ endpoint prediction model ]] by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
562,RASOR,1.1,[],"[[ RASOR ]] outperforms the endpoint prediction model by [[ 1.1 ]] in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
563,RASOR,exact match,[],"[[ RASOR ]] outperforms the endpoint prediction model by 1.1 in [[ exact match ]] , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
564,endpoint prediction model,1.1,[],"RASOR outperforms the [[ endpoint prediction model ]] by [[ 1.1 ]] in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
565,endpoint prediction model,exact match,[],"RASOR outperforms the [[ endpoint prediction model ]] by 1.1 in [[ exact match ]] , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
566,1.1,exact match,[],"RASOR outperforms the endpoint prediction model by [[ 1.1 ]] in [[ exact match ]] , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",0
567,Additional baseline,CNN and DAN models,"[['Additional baseline', 'has', 'CNN and DAN models']]",[[ Additional baseline ]] [[ CNN and DAN models ]] are trained without using any pretrained word or sentence embeddings .,1
568,Additional baseline,pretrained word or sentence embeddings,"[['Additional baseline', 'has', 'CNN and DAN models']]",[[ Additional baseline ]] CNN and DAN models are trained without using any [[ pretrained word or sentence embeddings ]] .,0
569,CNN and DAN models,pretrained word or sentence embeddings,"[['Additional baseline', 'has', 'CNN and DAN models']]",Additional baseline [[ CNN and DAN models ]] are trained without using any [[ pretrained word or sentence embeddings ]] .,0
570,BioBERT,biomedical domain corpora ( PubMed abstracts and PMC full - text articles ),[],"Then , [[ BioBERT ]] is pre-trained on [[ biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) ]] .",0
571,Key- value attention,NUTM converge,[],[[ Key- value attention ]] helps [[ NUTM converge ]] completely with fewer iterations .,0
572,Key- value attention,fewer iterations,[],[[ Key- value attention ]] helps NUTM converge completely with [[ fewer iterations ]] .,0
573,NUTM converge,fewer iterations,[],Key- value attention helps [[ NUTM converge ]] completely with [[ fewer iterations ]] .,0
574,Qualitative Examples on Entailment and Saliency Improvements,summaries,[],"[[ Qualitative Examples on Entailment and Saliency Improvements ]] presents an example of output [[ summaries ]] generated by , our baseline , and our 3 - way multitask model .",0
575,integrative models,finer - grained text comprehension and inference,[],"In this work , to alleviate such an obvious shortcoming about semantics , we make attempt to explore [[ integrative models ]] for [[ finer - grained text comprehension and inference ]] .",0
576,side information,improved relation extraction,[],"Further , the improvement from [[ side information ]] shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that inducing side information leads to [[ improved relation extraction ]] .",0
577,Spanish and Dutch,64 - dimensional Polyglot embeddings,[],"For [[ Spanish and Dutch ]] , we use the [[ 64 - dimensional Polyglot embeddings ]] .",0
578,BERT SP with entity indicators on input layer,our structured attention layer,[],"[[ BERT SP with entity indicators on input layer ]] : it replaces [[ our structured attention layer ]] , and adds indicators of entities ( transformed to embeddings )",0
579,BERT SP with entity indicators on input layer,indicators,[],"[[ BERT SP with entity indicators on input layer ]] : it replaces our structured attention layer , and adds [[ indicators ]] of entities ( transformed to embeddings )",0
580,BERT SP with entity indicators on input layer,entities ( transformed to embeddings ),[],"[[ BERT SP with entity indicators on input layer ]] : it replaces our structured attention layer , and adds indicators of [[ entities ( transformed to embeddings ) ]]",0
581,our structured attention layer,indicators,[],"BERT SP with entity indicators on input layer : it replaces [[ our structured attention layer ]] , and adds [[ indicators ]] of entities ( transformed to embeddings )",0
582,our structured attention layer,entities ( transformed to embeddings ),[],"BERT SP with entity indicators on input layer : it replaces [[ our structured attention layer ]] , and adds indicators of [[ entities ( transformed to embeddings ) ]]",0
583,indicators,entities ( transformed to embeddings ),[],"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds [[ indicators ]] of [[ entities ( transformed to embeddings ) ]]",0
584,more high - quality templates,higher ROUGE scores,[],"As illustrated in , the [[ more high - quality templates ]] are provided , the [[ higher ROUGE scores ]] are achieved .",0
585,gradient clipping,exploding gradients,[],We use [[ gradient clipping ]] to avoid [[ exploding gradients ]] .,0
586,FVTA without photos,contribution,[],"Finally , we train [[ FVTA without photos ]] to see the [[ contribution ]] of visual information .",0
587,FVTA without photos,visual information,[],"Finally , we train [[ FVTA without photos ]] to see the contribution of [[ visual information ]] .",0
588,contribution,visual information,[],"Finally , we train FVTA without photos to see the [[ contribution ]] of [[ visual information ]] .",0
589,QRN,context sentences,[],"[[ QRN ]] considers the [[ context sentences ]] as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",0
590,QRN,state - changing triggers,[],"[[ QRN ]] considers the context sentences as a sequence of [[ state - changing triggers ]] , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",0
591,QRN,original query,[],"[[ QRN ]] considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the [[ original query ]] to a more informed query as it observes each trigger through time .",0
592,QRN,more informed query,[],"[[ QRN ]] considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a [[ more informed query ]] as it observes each trigger through time .",0
593,QRN,trigger,[],"[[ QRN ]] considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each [[ trigger ]] through time .",0
594,QRN,time,[],"[[ QRN ]] considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through [[ time ]] .",0
595,context sentences,state - changing triggers,[],"QRN considers the [[ context sentences ]] as a sequence of [[ state - changing triggers ]] , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",0
596,context sentences,original query,[],"QRN considers the [[ context sentences ]] as a sequence of state - changing triggers , and transforms ( reduces ) the [[ original query ]] to a more informed query as it observes each trigger through time .",0
597,context sentences,more informed query,[],"QRN considers the [[ context sentences ]] as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a [[ more informed query ]] as it observes each trigger through time .",0
598,context sentences,trigger,[],"QRN considers the [[ context sentences ]] as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each [[ trigger ]] through time .",0
599,context sentences,time,[],"QRN considers the [[ context sentences ]] as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through [[ time ]] .",0
600,state - changing triggers,original query,[],"QRN considers the context sentences as a sequence of [[ state - changing triggers ]] , and transforms ( reduces ) the [[ original query ]] to a more informed query as it observes each trigger through time .",0
601,state - changing triggers,more informed query,[],"QRN considers the context sentences as a sequence of [[ state - changing triggers ]] , and transforms ( reduces ) the original query to a [[ more informed query ]] as it observes each trigger through time .",0
602,state - changing triggers,trigger,[],"QRN considers the context sentences as a sequence of [[ state - changing triggers ]] , and transforms ( reduces ) the original query to a more informed query as it observes each [[ trigger ]] through time .",0
603,state - changing triggers,time,[],"QRN considers the context sentences as a sequence of [[ state - changing triggers ]] , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through [[ time ]] .",0
604,original query,more informed query,[],"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the [[ original query ]] to a [[ more informed query ]] as it observes each trigger through time .",0
605,original query,trigger,[],"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the [[ original query ]] to a more informed query as it observes each [[ trigger ]] through time .",0
606,original query,time,[],"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the [[ original query ]] to a more informed query as it observes each trigger through [[ time ]] .",0
607,more informed query,trigger,[],"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a [[ more informed query ]] as it observes each [[ trigger ]] through time .",0
608,more informed query,time,[],"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a [[ more informed query ]] as it observes each trigger through [[ time ]] .",0
609,trigger,time,[],"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each [[ trigger ]] through [[ time ]] .",0
610,corpus,dataset,[],"The [[ corpus ]] is based on the [[ dataset ]] introduced by and consists of 11,855 single sentences extracted from movie reviews .",0
611,corpus,"11,855 single sentences",[],"The [[ corpus ]] is based on the dataset introduced by and consists of [[ 11,855 single sentences ]] extracted from movie reviews .",0
612,corpus,movie reviews,[],"The [[ corpus ]] is based on the dataset introduced by and consists of 11,855 single sentences extracted from [[ movie reviews ]] .",0
613,dataset,"11,855 single sentences",[],"The corpus is based on the [[ dataset ]] introduced by and consists of [[ 11,855 single sentences ]] extracted from movie reviews .",0
614,dataset,movie reviews,[],"The corpus is based on the [[ dataset ]] introduced by and consists of 11,855 single sentences extracted from [[ movie reviews ]] .",0
615,"11,855 single sentences",movie reviews,[],"The corpus is based on the dataset introduced by and consists of [[ 11,855 single sentences ]] extracted from [[ movie reviews ]] .",0
616,Google Production dataset,1.01 higher,[],"On the [[ Google Production dataset ]] , our model achieved [[ 1.01 higher ]] test BLEU score even after training for only one sixth of the time .",0
617,coverage,assertions,[],"Furthermore , increasing the [[ coverage ]] of [[ assertions ]] in ConceptNet would most likely yield improved performance even without retraining our models .",0
618,coverage,ConceptNet,[],"Furthermore , increasing the [[ coverage ]] of assertions in [[ ConceptNet ]] would most likely yield improved performance even without retraining our models .",0
619,coverage,improved performance,[],"Furthermore , increasing the [[ coverage ]] of assertions in ConceptNet would most likely yield [[ improved performance ]] even without retraining our models .",0
620,coverage,our models,[],"Furthermore , increasing the [[ coverage ]] of assertions in ConceptNet would most likely yield improved performance even without retraining [[ our models ]] .",0
621,assertions,ConceptNet,[],"Furthermore , increasing the coverage of [[ assertions ]] in [[ ConceptNet ]] would most likely yield improved performance even without retraining our models .",0
622,assertions,improved performance,[],"Furthermore , increasing the coverage of [[ assertions ]] in ConceptNet would most likely yield [[ improved performance ]] even without retraining our models .",0
623,assertions,our models,[],"Furthermore , increasing the coverage of [[ assertions ]] in ConceptNet would most likely yield improved performance even without retraining [[ our models ]] .",0
624,ConceptNet,improved performance,[],"Furthermore , increasing the coverage of assertions in [[ ConceptNet ]] would most likely yield [[ improved performance ]] even without retraining our models .",0
625,ConceptNet,our models,[],"Furthermore , increasing the coverage of assertions in [[ ConceptNet ]] would most likely yield improved performance even without retraining [[ our models ]] .",0
626,improved performance,our models,[],"Furthermore , increasing the coverage of assertions in ConceptNet would most likely yield [[ improved performance ]] even without retraining [[ our models ]] .",0
627,joint model,previous best pipeline system,[],"As shown in , 2 our [[ joint model ]] outperforms the [[ previous best pipeline system ]] by an F1 difference of anywhere between 1.3 and 6.0 in every setting .",0
628,joint model,F1 difference,[],"As shown in , 2 our [[ joint model ]] outperforms the previous best pipeline system by an [[ F1 difference ]] of anywhere between 1.3 and 6.0 in every setting .",0
629,joint model,anywhere between 1.3 and 6.0,[],"As shown in , 2 our [[ joint model ]] outperforms the previous best pipeline system by an F1 difference of [[ anywhere between 1.3 and 6.0 ]] in every setting .",0
630,previous best pipeline system,F1 difference,[],"As shown in , 2 our joint model outperforms the [[ previous best pipeline system ]] by an [[ F1 difference ]] of anywhere between 1.3 and 6.0 in every setting .",0
631,previous best pipeline system,anywhere between 1.3 and 6.0,[],"As shown in , 2 our joint model outperforms the [[ previous best pipeline system ]] by an F1 difference of [[ anywhere between 1.3 and 6.0 ]] in every setting .",0
632,F1 difference,anywhere between 1.3 and 6.0,[],"As shown in , 2 our joint model outperforms the previous best pipeline system by an [[ F1 difference ]] of [[ anywhere between 1.3 and 6.0 ]] in every setting .",0
633,neural ranking,relationships,[],Our [[ neural ranking ]] models the [[ relationships ]] between QA pairs in Hyperbolic space instead of Euclidean space .,0
634,neural ranking,QA pairs,[],Our [[ neural ranking ]] models the relationships between [[ QA pairs ]] in Hyperbolic space instead of Euclidean space .,0
635,neural ranking,Hyperbolic space,[],Our [[ neural ranking ]] models the relationships between QA pairs in [[ Hyperbolic space ]] instead of Euclidean space .,0
636,neural ranking,Euclidean space,[],Our [[ neural ranking ]] models the relationships between QA pairs in Hyperbolic space instead of [[ Euclidean space ]] .,0
637,relationships,QA pairs,[],Our neural ranking models the [[ relationships ]] between [[ QA pairs ]] in Hyperbolic space instead of Euclidean space .,0
638,relationships,Hyperbolic space,[],Our neural ranking models the [[ relationships ]] between QA pairs in [[ Hyperbolic space ]] instead of Euclidean space .,0
639,relationships,Euclidean space,[],Our neural ranking models the [[ relationships ]] between QA pairs in Hyperbolic space instead of [[ Euclidean space ]] .,0
640,QA pairs,Hyperbolic space,[],Our neural ranking models the relationships between [[ QA pairs ]] in [[ Hyperbolic space ]] instead of Euclidean space .,0
641,QA pairs,Euclidean space,[],Our neural ranking models the relationships between [[ QA pairs ]] in Hyperbolic space instead of [[ Euclidean space ]] .,0
642,Hyperbolic space,Euclidean space,[],Our neural ranking models the relationships between QA pairs in [[ Hyperbolic space ]] instead of [[ Euclidean space ]] .,0
643,tokenizer,Stanford CoreNLP,[],We use the [[ tokenizer ]] from [[ Stanford CoreNLP ]] to preprocess each passage and question .,0
644,tokenizer,each passage and question,[],We use the [[ tokenizer ]] from Stanford CoreNLP to preprocess [[ each passage and question ]] .,0
645,Stanford CoreNLP,each passage and question,[],We use the tokenizer from [[ Stanford CoreNLP ]] to preprocess [[ each passage and question ]] .,0
646,both target representation and context representation concatenated,sentiment polarity,[],"Finally , with [[ both target representation and context representation concatenated ]] , IAN predicts the [[ sentiment polarity ]] for the target within its context .",0
647,both target representation and context representation concatenated,target,[],"Finally , with [[ both target representation and context representation concatenated ]] , IAN predicts the sentiment polarity for the [[ target ]] within its context .",0
648,both target representation and context representation concatenated,its context,[],"Finally , with [[ both target representation and context representation concatenated ]] , IAN predicts the sentiment polarity for the target within [[ its context ]] .",0
649,sentiment polarity,target,[],"Finally , with both target representation and context representation concatenated , IAN predicts the [[ sentiment polarity ]] for the [[ target ]] within its context .",0
650,sentiment polarity,its context,[],"Finally , with both target representation and context representation concatenated , IAN predicts the [[ sentiment polarity ]] for the target within [[ its context ]] .",0
651,target,its context,[],"Finally , with both target representation and context representation concatenated , IAN predicts the sentiment polarity for the [[ target ]] within [[ its context ]] .",0
652,corresponding emotion,words,[],"Thus , to detect the [[ corresponding emotion ]] , more attention needs to be paid to [[ words ]] .",0
653,DCRL,SCST,[],Replacing [[ DCRL ]] with [[ SCST ]] also causes a marginal decline of performance on both metrics .,0
654,DCRL,marginal decline,[],Replacing [[ DCRL ]] with SCST also causes a [[ marginal decline ]] of performance on both metrics .,0
655,DCRL,performance,[],Replacing [[ DCRL ]] with SCST also causes a marginal decline of [[ performance ]] on both metrics .,0
656,DCRL,both metrics,[],Replacing [[ DCRL ]] with SCST also causes a marginal decline of performance on [[ both metrics ]] .,0
657,SCST,marginal decline,[],Replacing DCRL with [[ SCST ]] also causes a [[ marginal decline ]] of performance on both metrics .,0
658,SCST,performance,[],Replacing DCRL with [[ SCST ]] also causes a marginal decline of [[ performance ]] on both metrics .,0
659,SCST,both metrics,[],Replacing DCRL with [[ SCST ]] also causes a marginal decline of performance on [[ both metrics ]] .,0
660,marginal decline,performance,[],Replacing DCRL with SCST also causes a [[ marginal decline ]] of [[ performance ]] on both metrics .,0
661,marginal decline,both metrics,[],Replacing DCRL with SCST also causes a [[ marginal decline ]] of performance on [[ both metrics ]] .,0
662,performance,both metrics,[],Replacing DCRL with SCST also causes a marginal decline of [[ performance ]] on [[ both metrics ]] .,0
663,PTB dataset,AWD - LSTM and AWD - LSTM - MoS baseline,[],"On [[ PTB dataset ]] , our method improves the [[ AWD - LSTM and AWD - LSTM - MoS baseline ]] by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .",0
664,PTB dataset,0.8/1.2/1.0 and 0.76/1.13/1.15 points,[],"On [[ PTB dataset ]] , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by [[ 0.8/1.2/1.0 and 0.76/1.13/1.15 points ]] in test set at different checkpoints .",0
665,AWD - LSTM and AWD - LSTM - MoS baseline,0.8/1.2/1.0 and 0.76/1.13/1.15 points,[],"On PTB dataset , our method improves the [[ AWD - LSTM and AWD - LSTM - MoS baseline ]] by [[ 0.8/1.2/1.0 and 0.76/1.13/1.15 points ]] in test set at different checkpoints .",0
666,BERT SP with position embeddings,final attention layer,[],"For [[ BERT SP with position embeddings ]] on the [[ final attention layer ]] , we train the model in the single - relation setting and test with two different settings , so the results are the same .",0
667,BERT SP with position embeddings,model,[],"For [[ BERT SP with position embeddings ]] on the final attention layer , we train the [[ model ]] in the single - relation setting and test with two different settings , so the results are the same .",0
668,BERT SP with position embeddings,single - relation setting,[],"For [[ BERT SP with position embeddings ]] on the final attention layer , we train the model in the [[ single - relation setting ]] and test with two different settings , so the results are the same .",0
669,BERT SP with position embeddings,two different settings,[],"For [[ BERT SP with position embeddings ]] on the final attention layer , we train the model in the single - relation setting and test with [[ two different settings ]] , so the results are the same .",0
670,BERT SP with position embeddings,results,[],"For [[ BERT SP with position embeddings ]] on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the [[ results ]] are the same .",0
671,BERT SP with position embeddings,same,[],"For [[ BERT SP with position embeddings ]] on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the [[ same ]] .",0
672,final attention layer,model,[],"For BERT SP with position embeddings on the [[ final attention layer ]] , we train the [[ model ]] in the single - relation setting and test with two different settings , so the results are the same .",0
673,final attention layer,single - relation setting,[],"For BERT SP with position embeddings on the [[ final attention layer ]] , we train the model in the [[ single - relation setting ]] and test with two different settings , so the results are the same .",0
674,final attention layer,two different settings,[],"For BERT SP with position embeddings on the [[ final attention layer ]] , we train the model in the single - relation setting and test with [[ two different settings ]] , so the results are the same .",0
675,final attention layer,results,[],"For BERT SP with position embeddings on the [[ final attention layer ]] , we train the model in the single - relation setting and test with two different settings , so the [[ results ]] are the same .",0
676,final attention layer,same,[],"For BERT SP with position embeddings on the [[ final attention layer ]] , we train the model in the single - relation setting and test with two different settings , so the results are the [[ same ]] .",0
677,model,single - relation setting,[],"For BERT SP with position embeddings on the final attention layer , we train the [[ model ]] in the [[ single - relation setting ]] and test with two different settings , so the results are the same .",0
678,model,two different settings,[],"For BERT SP with position embeddings on the final attention layer , we train the [[ model ]] in the single - relation setting and test with [[ two different settings ]] , so the results are the same .",0
679,model,results,[],"For BERT SP with position embeddings on the final attention layer , we train the [[ model ]] in the single - relation setting and test with two different settings , so the [[ results ]] are the same .",0
680,model,same,[],"For BERT SP with position embeddings on the final attention layer , we train the [[ model ]] in the single - relation setting and test with two different settings , so the results are the [[ same ]] .",0
681,single - relation setting,two different settings,[],"For BERT SP with position embeddings on the final attention layer , we train the model in the [[ single - relation setting ]] and test with [[ two different settings ]] , so the results are the same .",0
682,single - relation setting,results,[],"For BERT SP with position embeddings on the final attention layer , we train the model in the [[ single - relation setting ]] and test with two different settings , so the [[ results ]] are the same .",0
683,single - relation setting,same,[],"For BERT SP with position embeddings on the final attention layer , we train the model in the [[ single - relation setting ]] and test with two different settings , so the results are the [[ same ]] .",0
684,two different settings,results,[],"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with [[ two different settings ]] , so the [[ results ]] are the same .",0
685,two different settings,same,[],"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with [[ two different settings ]] , so the results are the [[ same ]] .",0
686,results,same,[],"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the [[ results ]] are the [[ same ]] .",0
687,MFN,multimodal scenario,[],"[[ MFN ]] ) : Specific to [[ multimodal scenario ]] , this model utilizes multi-view learning by modeling view - specific and cross - view interactions .",0
688,MFN,multi-view learning,[],"[[ MFN ]] ) : Specific to multimodal scenario , this model utilizes [[ multi-view learning ]] by modeling view - specific and cross - view interactions .",0
689,MFN,view - specific and cross - view interactions,[],"[[ MFN ]] ) : Specific to multimodal scenario , this model utilizes multi-view learning by modeling [[ view - specific and cross - view interactions ]] .",0
690,multimodal scenario,multi-view learning,[],"MFN ) : Specific to [[ multimodal scenario ]] , this model utilizes [[ multi-view learning ]] by modeling view - specific and cross - view interactions .",0
691,multimodal scenario,view - specific and cross - view interactions,[],"MFN ) : Specific to [[ multimodal scenario ]] , this model utilizes multi-view learning by modeling [[ view - specific and cross - view interactions ]] .",0
692,multi-view learning,view - specific and cross - view interactions,[],"MFN ) : Specific to multimodal scenario , this model utilizes [[ multi-view learning ]] by modeling [[ view - specific and cross - view interactions ]] .",0
693,RERANKING EXTRACTIVE QUESTION ANSWERING,TRIVIAQA,[],[[ RERANKING EXTRACTIVE QUESTION ANSWERING ]] ON [[ TRIVIAQA ]],0
694,stacked bidirectional LSTMs ( BiL - STM ),encoders,[],We use [[ stacked bidirectional LSTMs ( BiL - STM ) ]] as the [[ encoders ]] .,0
695,CNN,text classification task,[],[[ CNN ]] is widely used on [[ text classification task ]] .,0
696,fifth baseline and DiSAN,directional self - attention,[],A comparison between the [[ fifth baseline and DiSAN ]] shows that [[ directional self - attention ]] with forward and backward masks ( with temporal order encoded ) can bring 0.96 % improvement .,0
697,fifth baseline and DiSAN,forward and backward masks ( with temporal order encoded ),[],A comparison between the [[ fifth baseline and DiSAN ]] shows that directional self - attention with [[ forward and backward masks ( with temporal order encoded ) ]] can bring 0.96 % improvement .,0
698,fifth baseline and DiSAN,0.96 % improvement,[],A comparison between the [[ fifth baseline and DiSAN ]] shows that directional self - attention with forward and backward masks ( with temporal order encoded ) can bring [[ 0.96 % improvement ]] .,0
699,directional self - attention,forward and backward masks ( with temporal order encoded ),[],A comparison between the fifth baseline and DiSAN shows that [[ directional self - attention ]] with [[ forward and backward masks ( with temporal order encoded ) ]] can bring 0.96 % improvement .,0
700,directional self - attention,0.96 % improvement,[],A comparison between the fifth baseline and DiSAN shows that [[ directional self - attention ]] with forward and backward masks ( with temporal order encoded ) can bring [[ 0.96 % improvement ]] .,0
701,forward and backward masks ( with temporal order encoded ),0.96 % improvement,[],A comparison between the fifth baseline and DiSAN shows that directional self - attention with [[ forward and backward masks ( with temporal order encoded ) ]] can bring [[ 0.96 % improvement ]] .,0
702,document expansion,state - of - the - art re-ranker ( BM25 + Doc2query + BERT ),[],"When we combine [[ document expansion ]] with a [[ state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) ]] , we achieve the best - known results to date on TREC CAR ; for MS MARCO , we are near the state of the art .",0
703,document expansion,best - known results,[],"When we combine [[ document expansion ]] with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the [[ best - known results ]] to date on TREC CAR ; for MS MARCO , we are near the state of the art .",0
704,document expansion,TREC CAR,[],"When we combine [[ document expansion ]] with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on [[ TREC CAR ]] ; for MS MARCO , we are near the state of the art .",0
705,document expansion,MS MARCO,[],"When we combine [[ document expansion ]] with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on TREC CAR ; for [[ MS MARCO ]] , we are near the state of the art .",0
706,document expansion,state of the art,[],"When we combine [[ document expansion ]] with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on TREC CAR ; for MS MARCO , we are near the [[ state of the art ]] .",0
707,state - of - the - art re-ranker ( BM25 + Doc2query + BERT ),best - known results,[],"When we combine document expansion with a [[ state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) ]] , we achieve the [[ best - known results ]] to date on TREC CAR ; for MS MARCO , we are near the state of the art .",0
708,state - of - the - art re-ranker ( BM25 + Doc2query + BERT ),TREC CAR,[],"When we combine document expansion with a [[ state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) ]] , we achieve the best - known results to date on [[ TREC CAR ]] ; for MS MARCO , we are near the state of the art .",0
709,state - of - the - art re-ranker ( BM25 + Doc2query + BERT ),MS MARCO,[],"When we combine document expansion with a [[ state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) ]] , we achieve the best - known results to date on TREC CAR ; for [[ MS MARCO ]] , we are near the state of the art .",0
710,state - of - the - art re-ranker ( BM25 + Doc2query + BERT ),state of the art,[],"When we combine document expansion with a [[ state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) ]] , we achieve the best - known results to date on TREC CAR ; for MS MARCO , we are near the [[ state of the art ]] .",0
711,best - known results,TREC CAR,[],"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the [[ best - known results ]] to date on [[ TREC CAR ]] ; for MS MARCO , we are near the state of the art .",0
712,best - known results,MS MARCO,[],"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the [[ best - known results ]] to date on TREC CAR ; for [[ MS MARCO ]] , we are near the state of the art .",0
713,best - known results,state of the art,[],"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the [[ best - known results ]] to date on TREC CAR ; for MS MARCO , we are near the [[ state of the art ]] .",0
714,TREC CAR,MS MARCO,[],"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on [[ TREC CAR ]] ; for [[ MS MARCO ]] , we are near the state of the art .",0
715,TREC CAR,state of the art,[],"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on [[ TREC CAR ]] ; for MS MARCO , we are near the [[ state of the art ]] .",0
716,MS MARCO,state of the art,[],"When we combine document expansion with a state - of - the - art re-ranker ( BM25 + Doc2query + BERT ) , we achieve the best - known results to date on TREC CAR ; for [[ MS MARCO ]] , we are near the [[ state of the art ]] .",0
717,features,aspect,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the [[ features ]] extracted from the [[ aspect ]] analyzing the event - sequence have the strongest predictive power , followed by those characterizing Sentiment - trajectory .",0
718,features,event - sequence,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the [[ features ]] extracted from the aspect analyzing the [[ event - sequence ]] have the strongest predictive power , followed by those characterizing Sentiment - trajectory .",0
719,features,strongest predictive power,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the [[ features ]] extracted from the aspect analyzing the event - sequence have the [[ strongest predictive power ]] , followed by those characterizing Sentiment - trajectory .",0
720,features,characterizing,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the [[ features ]] extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those [[ characterizing ]] Sentiment - trajectory .",0
721,features,Sentiment - trajectory,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the [[ features ]] extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those characterizing [[ Sentiment - trajectory ]] .",0
722,aspect,event - sequence,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the [[ aspect ]] analyzing the [[ event - sequence ]] have the strongest predictive power , followed by those characterizing Sentiment - trajectory .",0
723,aspect,strongest predictive power,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the [[ aspect ]] analyzing the event - sequence have the [[ strongest predictive power ]] , followed by those characterizing Sentiment - trajectory .",0
724,aspect,characterizing,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the [[ aspect ]] analyzing the event - sequence have the strongest predictive power , followed by those [[ characterizing ]] Sentiment - trajectory .",0
725,aspect,Sentiment - trajectory,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the [[ aspect ]] analyzing the event - sequence have the strongest predictive power , followed by those characterizing [[ Sentiment - trajectory ]] .",0
726,event - sequence,strongest predictive power,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the aspect analyzing the [[ event - sequence ]] have the [[ strongest predictive power ]] , followed by those characterizing Sentiment - trajectory .",0
727,event - sequence,characterizing,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the aspect analyzing the [[ event - sequence ]] have the strongest predictive power , followed by those [[ characterizing ]] Sentiment - trajectory .",0
728,event - sequence,Sentiment - trajectory,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the aspect analyzing the [[ event - sequence ]] have the strongest predictive power , followed by those characterizing [[ Sentiment - trajectory ]] .",0
729,strongest predictive power,characterizing,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the aspect analyzing the event - sequence have the [[ strongest predictive power ]] , followed by those [[ characterizing ]] Sentiment - trajectory .",0
730,strongest predictive power,Sentiment - trajectory,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the aspect analyzing the event - sequence have the [[ strongest predictive power ]] , followed by those characterizing [[ Sentiment - trajectory ]] .",0
731,characterizing,Sentiment - trajectory,"[['characterizing', 'has', 'Sentiment - trajectory']]","We can see that the features extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those [[ characterizing ]] [[ Sentiment - trajectory ]] .",1
732,novel path - centric pruning technique,irrelevant information from the tree,[],"We also apply a [[ novel path - centric pruning technique ]] to remove [[ irrelevant information from the tree ]] while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",0
733,novel path - centric pruning technique,relevant content,[],"We also apply a [[ novel path - centric pruning technique ]] to remove irrelevant information from the tree while maximally keeping [[ relevant content ]] , which further improves the performance of several dependencybased models including ours .",0
734,irrelevant information from the tree,relevant content,[],"We also apply a novel path - centric pruning technique to remove [[ irrelevant information from the tree ]] while maximally keeping [[ relevant content ]] , which further improves the performance of several dependencybased models including ours .",0
735,autoencoder structure,neural attention - based sentiment classifier,[],The [[ autoencoder structure ]] is jointly trained with a [[ neural attention - based sentiment classifier ]] to provide a good target representation as well as a high accuracy on the predicted sentiment .,0
736,autoencoder structure,good target representation,[],The [[ autoencoder structure ]] is jointly trained with a neural attention - based sentiment classifier to provide a [[ good target representation ]] as well as a high accuracy on the predicted sentiment .,0
737,autoencoder structure,high accuracy,[],The [[ autoencoder structure ]] is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a [[ high accuracy ]] on the predicted sentiment .,0
738,autoencoder structure,predicted sentiment,[],The [[ autoencoder structure ]] is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the [[ predicted sentiment ]] .,0
739,neural attention - based sentiment classifier,good target representation,[],The autoencoder structure is jointly trained with a [[ neural attention - based sentiment classifier ]] to provide a [[ good target representation ]] as well as a high accuracy on the predicted sentiment .,0
740,neural attention - based sentiment classifier,high accuracy,[],The autoencoder structure is jointly trained with a [[ neural attention - based sentiment classifier ]] to provide a good target representation as well as a [[ high accuracy ]] on the predicted sentiment .,0
741,neural attention - based sentiment classifier,predicted sentiment,[],The autoencoder structure is jointly trained with a [[ neural attention - based sentiment classifier ]] to provide a good target representation as well as a high accuracy on the [[ predicted sentiment ]] .,0
742,good target representation,high accuracy,[],The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a [[ good target representation ]] as well as a [[ high accuracy ]] on the predicted sentiment .,0
743,good target representation,predicted sentiment,[],The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a [[ good target representation ]] as well as a high accuracy on the [[ predicted sentiment ]] .,0
744,high accuracy,predicted sentiment,[],The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a [[ high accuracy ]] on the [[ predicted sentiment ]] .,0
745,AOA,most important parts,[],That is why we choose [[ AOA ]] to attend to the [[ most important parts ]] in both aspect and sentence .,0
746,AOA,aspect and sentence,[],That is why we choose [[ AOA ]] to attend to the most important parts in both [[ aspect and sentence ]] .,0
747,most important parts,aspect and sentence,[],That is why we choose AOA to attend to the [[ most important parts ]] in both [[ aspect and sentence ]] .,0
748,ReSA,two parameter - untied RSS,"[['ReSA', 'has', 'two parameter - untied RSS']]","In [[ ReSA ]] , [[ two parameter - untied RSS ]] are respectively applied to two copies of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .",1
749,ReSA,two copies,"[['ReSA', 'has', 'two parameter - untied RSS']]","In [[ ReSA ]] , two parameter - untied RSS are respectively applied to [[ two copies ]] of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .",0
750,ReSA,input sequence,"[['ReSA', 'has', 'two parameter - untied RSS']]","In [[ ReSA ]] , two parameter - untied RSS are respectively applied to two copies of the [[ input sequence ]] , where the tokens from one and another are called dependent and head tokens , respectively .",0
751,two parameter - untied RSS,two copies,"[['ReSA', 'has', 'two parameter - untied RSS']]","In ReSA , [[ two parameter - untied RSS ]] are respectively applied to [[ two copies ]] of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .",0
752,two parameter - untied RSS,input sequence,"[['ReSA', 'has', 'two parameter - untied RSS']]","In ReSA , [[ two parameter - untied RSS ]] are respectively applied to two copies of the [[ input sequence ]] , where the tokens from one and another are called dependent and head tokens , respectively .",0
753,two copies,input sequence,"[['ReSA', 'has', 'two parameter - untied RSS']]","In ReSA , two parameter - untied RSS are respectively applied to [[ two copies ]] of the [[ input sequence ]] , where the tokens from one and another are called dependent and head tokens , respectively .",0
754,NER task,CRF loss layer,[],"Finally , we conduct experiments for the [[ NER task ]] by removing the [[ CRF loss layer ]] and substituting it with a softmax .",0
755,NER task,softmax,[],"Finally , we conduct experiments for the [[ NER task ]] by removing the CRF loss layer and substituting it with a [[ softmax ]] .",0
756,CRF loss layer,softmax,[],"Finally , we conduct experiments for the NER task by removing the [[ CRF loss layer ]] and substituting it with a [[ softmax ]] .",0
757,two distributional sentence models,bag - of - words model,[],"We construct [[ two distributional sentence models ]] ; first a [[ bag - of - words model ]] , and second , a bigram model based on a convolutional neural network .",0
758,two distributional sentence models,bigram model,[],"We construct [[ two distributional sentence models ]] ; first a bag - of - words model , and second , a [[ bigram model ]] based on a convolutional neural network .",0
759,two distributional sentence models,convolutional neural network,[],"We construct [[ two distributional sentence models ]] ; first a bag - of - words model , and second , a bigram model based on a [[ convolutional neural network ]] .",0
760,bag - of - words model,bigram model,[],"We construct two distributional sentence models ; first a [[ bag - of - words model ]] , and second , a [[ bigram model ]] based on a convolutional neural network .",0
761,bag - of - words model,convolutional neural network,[],"We construct two distributional sentence models ; first a [[ bag - of - words model ]] , and second , a bigram model based on a [[ convolutional neural network ]] .",0
762,bigram model,convolutional neural network,[],"We construct two distributional sentence models ; first a bag - of - words model , and second , a [[ bigram model ]] based on a [[ convolutional neural network ]] .",0
763,knowledge distillation,large amount of unlabeled words,[],"First , we use [[ knowledge distillation ]] to leverage the [[ large amount of unlabeled words ]] .",0
764,sum of words model,slightly worse,[],"The [[ sum of words model ]] performed [[ slightly worse ]] than the fundamentally similar lexicalized classifier while the sum of words model can use pretrained word embeddings to better handle rare words , it lacks even the rudimentary sensitivity to word order that the lexicalized model 's bigram features provide .",0
765,sum of words model,fundamentally similar lexicalized classifier,[],"The [[ sum of words model ]] performed slightly worse than the [[ fundamentally similar lexicalized classifier ]] while the sum of words model can use pretrained word embeddings to better handle rare words , it lacks even the rudimentary sensitivity to word order that the lexicalized model 's bigram features provide .",0
766,slightly worse,fundamentally similar lexicalized classifier,[],"The sum of words model performed [[ slightly worse ]] than the [[ fundamentally similar lexicalized classifier ]] while the sum of words model can use pretrained word embeddings to better handle rare words , it lacks even the rudimentary sensitivity to word order that the lexicalized model 's bigram features provide .",0
767,best hyper - parameters,gridsearch,[],"Finally , the [[ best hyper - parameters ]] are decided using a [[ gridsearch ]] .",0
768,each task,hyperparameters,[],"For [[ each task ]] , we take the [[ hyperparameters ]] which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
769,each task,best performance,[],"For [[ each task ]] , we take the hyperparameters which achieve the [[ best performance ]] on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
770,each task,development set,[],"For [[ each task ]] , we take the hyperparameters which achieve the best performance on the [[ development set ]] via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
771,each task,small grid search,[],"For [[ each task ]] , we take the hyperparameters which achieve the best performance on the development set via an [[ small grid search ]] over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
772,each task,combinations,[],"For [[ each task ]] , we take the hyperparameters which achieve the best performance on the development set via an small grid search over [[ combinations ]] of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
773,each task,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",[],"For [[ each task ]] , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
774,each task,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",[],"For [[ each task ]] , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the threshold value",0
775,each task,threshold value,[],"For [[ each task ]] , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the [[ threshold value ]]",0
776,hyperparameters,best performance,[],"For each task , we take the [[ hyperparameters ]] which achieve the [[ best performance ]] on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
777,hyperparameters,development set,[],"For each task , we take the [[ hyperparameters ]] which achieve the best performance on the [[ development set ]] via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
778,hyperparameters,small grid search,[],"For each task , we take the [[ hyperparameters ]] which achieve the best performance on the development set via an [[ small grid search ]] over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
779,hyperparameters,combinations,[],"For each task , we take the [[ hyperparameters ]] which achieve the best performance on the development set via an small grid search over [[ combinations ]] of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
780,hyperparameters,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",[],"For each task , we take the [[ hyperparameters ]] which achieve the best performance on the development set via an small grid search over combinations of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
781,hyperparameters,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",[],"For each task , we take the [[ hyperparameters ]] which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the threshold value",0
782,hyperparameters,threshold value,[],"For each task , we take the [[ hyperparameters ]] which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the [[ threshold value ]]",0
783,best performance,development set,[],"For each task , we take the hyperparameters which achieve the [[ best performance ]] on the [[ development set ]] via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
784,best performance,small grid search,[],"For each task , we take the hyperparameters which achieve the [[ best performance ]] on the development set via an [[ small grid search ]] over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
785,best performance,combinations,[],"For each task , we take the hyperparameters which achieve the [[ best performance ]] on the development set via an small grid search over [[ combinations ]] of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
786,best performance,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",[],"For each task , we take the hyperparameters which achieve the [[ best performance ]] on the development set via an small grid search over combinations of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
787,best performance,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",[],"For each task , we take the hyperparameters which achieve the [[ best performance ]] on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the threshold value",0
788,best performance,threshold value,[],"For each task , we take the hyperparameters which achieve the [[ best performance ]] on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the [[ threshold value ]]",0
789,development set,small grid search,[],"For each task , we take the hyperparameters which achieve the best performance on the [[ development set ]] via an [[ small grid search ]] over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
790,development set,combinations,[],"For each task , we take the hyperparameters which achieve the best performance on the [[ development set ]] via an small grid search over [[ combinations ]] of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
791,development set,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",[],"For each task , we take the hyperparameters which achieve the best performance on the [[ development set ]] via an small grid search over combinations of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
792,development set,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",[],"For each task , we take the hyperparameters which achieve the best performance on the [[ development set ]] via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the threshold value",0
793,development set,threshold value,[],"For each task , we take the hyperparameters which achieve the best performance on the [[ development set ]] via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the [[ threshold value ]]",0
794,small grid search,combinations,[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an [[ small grid search ]] over [[ combinations ]] of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
795,small grid search,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an [[ small grid search ]] over combinations of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
796,small grid search,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an [[ small grid search ]] over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the threshold value",0
797,small grid search,threshold value,[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an [[ small grid search ]] over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the [[ threshold value ]]",0
798,combinations,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over [[ combinations ]] of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",0
799,combinations,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over [[ combinations ]] of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the threshold value",0
800,combinations,threshold value,[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over [[ combinations ]] of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the [[ threshold value ]]",0
801,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]","l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the threshold value",0
802,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",threshold value,[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the [[ initial learning rate [ 0.05 , 0.0005 , 0.0001 ] ]] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the [[ threshold value ]]",0
803,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",threshold value,[],"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , [[ l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] ]] and the [[ threshold value ]]",0
804,Bayesian subspace multinomial model ( Bayesian SMM ),generative model,[],"In this paper , we present [[ Bayesian subspace multinomial model ( Bayesian SMM ) ]] as a [[ generative model ]] for bag - ofwords representation of documents .",0
805,Bayesian subspace multinomial model ( Bayesian SMM ),bag - ofwords representation of documents,[],"In this paper , we present [[ Bayesian subspace multinomial model ( Bayesian SMM ) ]] as a generative model for [[ bag - ofwords representation of documents ]] .",0
806,generative model,bag - ofwords representation of documents,[],"In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a [[ generative model ]] for [[ bag - ofwords representation of documents ]] .",0
807,power,pre-trained BERT,[],"Benefits from the [[ power ]] of [[ pre-trained BERT ]] , BERT - based models have shown huge superiority over GloVe - based models .",0
808,power,BERT - based models,[],"Benefits from the [[ power ]] of pre-trained BERT , [[ BERT - based models ]] have shown huge superiority over GloVe - based models .",0
809,power,huge superiority,[],"Benefits from the [[ power ]] of pre-trained BERT , BERT - based models have shown [[ huge superiority ]] over GloVe - based models .",0
810,power,GloVe - based models,[],"Benefits from the [[ power ]] of pre-trained BERT , BERT - based models have shown huge superiority over [[ GloVe - based models ]] .",0
811,pre-trained BERT,BERT - based models,[],"Benefits from the power of [[ pre-trained BERT ]] , [[ BERT - based models ]] have shown huge superiority over GloVe - based models .",0
812,pre-trained BERT,huge superiority,[],"Benefits from the power of [[ pre-trained BERT ]] , BERT - based models have shown [[ huge superiority ]] over GloVe - based models .",0
813,pre-trained BERT,GloVe - based models,[],"Benefits from the power of [[ pre-trained BERT ]] , BERT - based models have shown huge superiority over [[ GloVe - based models ]] .",0
814,BERT - based models,huge superiority,[],"Benefits from the power of pre-trained BERT , [[ BERT - based models ]] have shown [[ huge superiority ]] over GloVe - based models .",0
815,BERT - based models,GloVe - based models,[],"Benefits from the power of pre-trained BERT , [[ BERT - based models ]] have shown huge superiority over [[ GloVe - based models ]] .",0
816,huge superiority,GloVe - based models,[],"Benefits from the power of pre-trained BERT , BERT - based models have shown [[ huge superiority ]] over [[ GloVe - based models ]] .",0
817,char - IntNet - 5,more effective,[],"Also , we observe that [[ char - IntNet - 5 ]] is [[ more effective ]] for learning character - to - word representations than char - IntNet - 9 in most of the cases .",0
818,char - IntNet - 5,character - to - word representations,[],"Also , we observe that [[ char - IntNet - 5 ]] is more effective for learning [[ character - to - word representations ]] than char - IntNet - 9 in most of the cases .",0
819,char - IntNet - 5,char - IntNet - 9,[],"Also , we observe that [[ char - IntNet - 5 ]] is more effective for learning character - to - word representations than [[ char - IntNet - 9 ]] in most of the cases .",0
820,char - IntNet - 5,most of the cases,[],"Also , we observe that [[ char - IntNet - 5 ]] is more effective for learning character - to - word representations than char - IntNet - 9 in [[ most of the cases ]] .",0
821,more effective,character - to - word representations,[],"Also , we observe that char - IntNet - 5 is [[ more effective ]] for learning [[ character - to - word representations ]] than char - IntNet - 9 in most of the cases .",0
822,more effective,char - IntNet - 9,[],"Also , we observe that char - IntNet - 5 is [[ more effective ]] for learning character - to - word representations than [[ char - IntNet - 9 ]] in most of the cases .",0
823,more effective,most of the cases,[],"Also , we observe that char - IntNet - 5 is [[ more effective ]] for learning character - to - word representations than char - IntNet - 9 in [[ most of the cases ]] .",0
824,character - to - word representations,char - IntNet - 9,[],"Also , we observe that char - IntNet - 5 is more effective for learning [[ character - to - word representations ]] than [[ char - IntNet - 9 ]] in most of the cases .",0
825,character - to - word representations,most of the cases,[],"Also , we observe that char - IntNet - 5 is more effective for learning [[ character - to - word representations ]] than char - IntNet - 9 in [[ most of the cases ]] .",0
826,char - IntNet - 9,most of the cases,[],"Also , we observe that char - IntNet - 5 is more effective for learning character - to - word representations than [[ char - IntNet - 9 ]] in [[ most of the cases ]] .",0
827,embedding layer,300 - dimensional CBOW Word2vec embeddings,[],The [[ embedding layer ]] was initialized using [[ 300 - dimensional CBOW Word2vec embeddings ]] trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,0
828,embedding layer,3B - word UMBC WebBase corpus,[],The [[ embedding layer ]] was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the [[ 3B - word UMBC WebBase corpus ]] with standard hyperparameters,0
829,embedding layer,standard hyperparameters,[],The [[ embedding layer ]] was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with [[ standard hyperparameters ]],0
830,300 - dimensional CBOW Word2vec embeddings,3B - word UMBC WebBase corpus,[],The embedding layer was initialized using [[ 300 - dimensional CBOW Word2vec embeddings ]] trained on the [[ 3B - word UMBC WebBase corpus ]] with standard hyperparameters,0
831,300 - dimensional CBOW Word2vec embeddings,standard hyperparameters,[],The embedding layer was initialized using [[ 300 - dimensional CBOW Word2vec embeddings ]] trained on the 3B - word UMBC WebBase corpus with [[ standard hyperparameters ]],0
832,3B - word UMBC WebBase corpus,standard hyperparameters,[],The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the [[ 3B - word UMBC WebBase corpus ]] with [[ standard hyperparameters ]],0
833,Parameterized filters and gates,same size and number,[],[[ Parameterized filters and gates ]] have the [[ same size and number ]] as normal filters .,0
834,Parameterized filters and gates,normal filters,[],[[ Parameterized filters and gates ]] have the same size and number as [[ normal filters ]] .,0
835,same size and number,normal filters,[],Parameterized filters and gates have the [[ same size and number ]] as [[ normal filters ]] .,0
836,neural network architecture,sequence labeling,[],"In this paper , we propose a [[ neural network architecture ]] for [[ sequence labeling ]] .",0
837,multilayer network,single maxout hidden layer,[],"In both cases , we use a [[ multilayer network ]] with a [[ single maxout hidden layer ]] to compute the conditional probability of each target word .",0
838,multilayer network,conditional probability,[],"In both cases , we use a [[ multilayer network ]] with a single maxout hidden layer to compute the [[ conditional probability ]] of each target word .",0
839,multilayer network,each target word,[],"In both cases , we use a [[ multilayer network ]] with a single maxout hidden layer to compute the conditional probability of [[ each target word ]] .",0
840,single maxout hidden layer,conditional probability,[],"In both cases , we use a multilayer network with a [[ single maxout hidden layer ]] to compute the [[ conditional probability ]] of each target word .",0
841,single maxout hidden layer,each target word,[],"In both cases , we use a multilayer network with a [[ single maxout hidden layer ]] to compute the conditional probability of [[ each target word ]] .",0
842,conditional probability,each target word,[],"In both cases , we use a multilayer network with a single maxout hidden layer to compute the [[ conditional probability ]] of [[ each target word ]] .",0
843,exact match feature,effectiveness,[],The [[ exact match feature ]] also demonstrates its [[ effectiveness ]] in the ablation result .,0
844,exact match feature,ablation result,[],The [[ exact match feature ]] also demonstrates its effectiveness in the [[ ablation result ]] .,0
845,effectiveness,ablation result,[],The exact match feature also demonstrates its [[ effectiveness ]] in the [[ ablation result ]] .,0
846,feature engineering,Feature - enhanced SVM,"[['feature engineering', 'has', 'Feature - enhanced SVM']]","With the help of [[ feature engineering ]] , the [[ Feature - enhanced SVM ]] achieves much better results .",1
847,feature engineering,much better results,"[['feature engineering', 'has', 'Feature - enhanced SVM']]","With the help of [[ feature engineering ]] , the Feature - enhanced SVM achieves [[ much better results ]] .",0
848,Feature - enhanced SVM,much better results,"[['feature engineering', 'has', 'Feature - enhanced SVM']]","With the help of feature engineering , the [[ Feature - enhanced SVM ]] achieves [[ much better results ]] .",0
849,BioBERT,weights,[],"First , we initialize [[ BioBERT ]] with [[ weights ]] from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",0
850,BioBERT,BERT,[],"First , we initialize [[ BioBERT ]] with weights from [[ BERT ]] , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",0
851,BioBERT,general domain corpora ( English Wikipedia and Books Corpus ),[],"First , we initialize [[ BioBERT ]] with weights from BERT , which was pretrained on [[ general domain corpora ( English Wikipedia and Books Corpus ) ]] .",0
852,weights,BERT,[],"First , we initialize BioBERT with [[ weights ]] from [[ BERT ]] , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",0
853,weights,general domain corpora ( English Wikipedia and Books Corpus ),[],"First , we initialize BioBERT with [[ weights ]] from BERT , which was pretrained on [[ general domain corpora ( English Wikipedia and Books Corpus ) ]] .",0
854,BERT,general domain corpora ( English Wikipedia and Books Corpus ),[],"First , we initialize BioBERT with weights from [[ BERT ]] , which was pretrained on [[ general domain corpora ( English Wikipedia and Books Corpus ) ]] .",0
855,AP - CNN,AP - biLSTM,"[['decreases', 'has', 'learning rate']]","For [[ AP - CNN ]] , [[ AP - biLSTM ]] and QA - LSTM , we also use a learning rate schedule that decreases the learning rate ?",0
856,AP - CNN,QA - LSTM,"[['decreases', 'has', 'learning rate']]","For [[ AP - CNN ]] , AP - biLSTM and [[ QA - LSTM ]] , we also use a learning rate schedule that decreases the learning rate ?",0
857,AP - CNN,learning rate schedule,"[['decreases', 'has', 'learning rate']]","For [[ AP - CNN ]] , AP - biLSTM and QA - LSTM , we also use a [[ learning rate schedule ]] that decreases the learning rate ?",0
858,AP - CNN,decreases,"[['decreases', 'has', 'learning rate']]","For [[ AP - CNN ]] , AP - biLSTM and QA - LSTM , we also use a learning rate schedule that [[ decreases ]] the learning rate ?",0
859,AP - CNN,learning rate,"[['decreases', 'has', 'learning rate']]","For [[ AP - CNN ]] , AP - biLSTM and QA - LSTM , we also use a learning rate schedule that decreases the [[ learning rate ]] ?",0
860,AP - biLSTM,QA - LSTM,"[['decreases', 'has', 'learning rate']]","For AP - CNN , [[ AP - biLSTM ]] and [[ QA - LSTM ]] , we also use a learning rate schedule that decreases the learning rate ?",0
861,AP - biLSTM,learning rate schedule,"[['decreases', 'has', 'learning rate']]","For AP - CNN , [[ AP - biLSTM ]] and QA - LSTM , we also use a [[ learning rate schedule ]] that decreases the learning rate ?",0
862,AP - biLSTM,decreases,"[['decreases', 'has', 'learning rate']]","For AP - CNN , [[ AP - biLSTM ]] and QA - LSTM , we also use a learning rate schedule that [[ decreases ]] the learning rate ?",0
863,AP - biLSTM,learning rate,"[['decreases', 'has', 'learning rate']]","For AP - CNN , [[ AP - biLSTM ]] and QA - LSTM , we also use a learning rate schedule that decreases the [[ learning rate ]] ?",0
864,QA - LSTM,learning rate schedule,"[['decreases', 'has', 'learning rate']]","For AP - CNN , AP - biLSTM and [[ QA - LSTM ]] , we also use a [[ learning rate schedule ]] that decreases the learning rate ?",0
865,QA - LSTM,decreases,"[['decreases', 'has', 'learning rate']]","For AP - CNN , AP - biLSTM and [[ QA - LSTM ]] , we also use a learning rate schedule that [[ decreases ]] the learning rate ?",0
866,QA - LSTM,learning rate,"[['decreases', 'has', 'learning rate']]","For AP - CNN , AP - biLSTM and [[ QA - LSTM ]] , we also use a learning rate schedule that decreases the [[ learning rate ]] ?",0
867,learning rate schedule,decreases,"[['decreases', 'has', 'learning rate']]","For AP - CNN , AP - biLSTM and QA - LSTM , we also use a [[ learning rate schedule ]] that [[ decreases ]] the learning rate ?",0
868,learning rate schedule,learning rate,"[['decreases', 'has', 'learning rate']]","For AP - CNN , AP - biLSTM and QA - LSTM , we also use a [[ learning rate schedule ]] that decreases the [[ learning rate ]] ?",0
869,decreases,learning rate,"[['decreases', 'has', 'learning rate']]","For AP - CNN , AP - biLSTM and QA - LSTM , we also use a learning rate schedule that [[ decreases ]] the [[ learning rate ]] ?",1
870,Glo Ve embeddings,best perplexity,[],Using [[ Glo Ve embeddings ]] results in the [[ best perplexity ]] .,0
871,BERT - MRC,improvement,[],"[[ BERT - MRC ]] has almost no [[ improvement ]] on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .",0
872,BERT - MRC,restaurant,[],"[[ BERT - MRC ]] has almost no improvement on [[ restaurant ]] , which indicates Wikipedia may have no knowledge about aspects of restaurant .",0
873,improvement,restaurant,[],"BERT - MRC has almost no [[ improvement ]] on [[ restaurant ]] , which indicates Wikipedia may have no knowledge about aspects of restaurant .",0
874,off - path information,previous shortest dependency path - based model ( SDP - LSTM ),[],"Notably , by properly incorporating [[ off - path information ]] , our model outperforms the [[ previous shortest dependency path - based model ( SDP - LSTM ) ]] .",0
875,conditional recurrent neural network,decoder,[],"Inspired by the recently proposed architectures for machine translation , our model consists of a [[ conditional recurrent neural network ]] , which acts as a [[ decoder ]] to generate the summary of an input sentence , much like a standard recurrent language model .",0
876,conditional recurrent neural network,summary,[],"Inspired by the recently proposed architectures for machine translation , our model consists of a [[ conditional recurrent neural network ]] , which acts as a decoder to generate the [[ summary ]] of an input sentence , much like a standard recurrent language model .",0
877,conditional recurrent neural network,input sentence,[],"Inspired by the recently proposed architectures for machine translation , our model consists of a [[ conditional recurrent neural network ]] , which acts as a decoder to generate the summary of an [[ input sentence ]] , much like a standard recurrent language model .",0
878,decoder,summary,[],"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a [[ decoder ]] to generate the [[ summary ]] of an input sentence , much like a standard recurrent language model .",0
879,decoder,input sentence,[],"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a [[ decoder ]] to generate the summary of an [[ input sentence ]] , much like a standard recurrent language model .",0
880,summary,input sentence,[],"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the [[ summary ]] of an [[ input sentence ]] , much like a standard recurrent language model .",0
881,both datasets,performance,[],"On the other hand , training on [[ both datasets ]] only improves [[ performance ]] ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .",0
882,BERT - pair - NLI - B model,best performance,[],The [[ BERT - pair - NLI - B model ]] achieves the [[ best performance ]] for aspect category detection .,0
883,BERT - pair - NLI - B model,aspect category detection,[],The [[ BERT - pair - NLI - B model ]] achieves the best performance for [[ aspect category detection ]] .,0
884,best performance,aspect category detection,[],The BERT - pair - NLI - B model achieves the [[ best performance ]] for [[ aspect category detection ]] .,0
885,hidden state size,d,"[['hidden state size', 'has', 'd']]","The [[ hidden state size ]] , [[ d ]] , was set to 300 for all GRUs .",1
886,hidden state size,300,"[['hidden state size', 'has', 'd']]","The [[ hidden state size ]] , d , was set to [[ 300 ]] for all GRUs .",0
887,hidden state size,all GRUs,"[['hidden state size', 'has', 'd']]","The [[ hidden state size ]] , d , was set to 300 for [[ all GRUs ]] .",0
888,d,300,"[['hidden state size', 'has', 'd']]","The hidden state size , [[ d ]] , was set to [[ 300 ]] for all GRUs .",0
889,d,all GRUs,"[['hidden state size', 'has', 'd']]","The hidden state size , [[ d ]] , was set to 300 for [[ all GRUs ]] .",0
890,300,all GRUs,"[['hidden state size', 'has', 'd']]","The hidden state size , d , was set to [[ 300 ]] for [[ all GRUs ]] .",0
891,IntNet,other character embedding models,[],"Our [[ IntNet ]] significantly outperforms [[ other character embedding models ]] , for example , the improvement is more than 2 % in terms of F 1 score for German and Dutch .",0
892,state - of - theart language model,data,[],We then use a [[ state - of - theart language model ]] fine - tuned on this [[ data ]] to massively oversample a diverse set of possible negative sentence endings ( or counterfactuals ) .,0
893,state - of - theart language model,diverse set,[],We then use a [[ state - of - theart language model ]] fine - tuned on this data to massively oversample a [[ diverse set ]] of possible negative sentence endings ( or counterfactuals ) .,0
894,state - of - theart language model,possible negative sentence endings ( or counterfactuals ),[],We then use a [[ state - of - theart language model ]] fine - tuned on this data to massively oversample a diverse set of [[ possible negative sentence endings ( or counterfactuals ) ]] .,0
895,data,diverse set,[],We then use a state - of - theart language model fine - tuned on this [[ data ]] to massively oversample a [[ diverse set ]] of possible negative sentence endings ( or counterfactuals ) .,0
896,data,possible negative sentence endings ( or counterfactuals ),[],We then use a state - of - theart language model fine - tuned on this [[ data ]] to massively oversample a diverse set of [[ possible negative sentence endings ( or counterfactuals ) ]] .,0
897,diverse set,possible negative sentence endings ( or counterfactuals ),[],We then use a state - of - theart language model fine - tuned on this data to massively oversample a [[ diverse set ]] of [[ possible negative sentence endings ( or counterfactuals ) ]] .,0
898,RGNN model,simple graph convolution network ( SGC ),[],"Our [[ RGNN model ]] extends the [[ simple graph convolution network ( SGC ) ]] and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
899,RGNN model,topological structure,[],"Our [[ RGNN model ]] extends the simple graph convolution network ( SGC ) and leverages the [[ topological structure ]] of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
900,RGNN model,EEG signals,[],"Our [[ RGNN model ]] extends the simple graph convolution network ( SGC ) and leverages the topological structure of [[ EEG signals ]] , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
901,RGNN model,economy of brain network organization,[],"Our [[ RGNN model ]] extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the [[ economy of brain network organization ]] , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
902,RGNN model,biologically supported sparse adjacency matrix,[],"Our [[ RGNN model ]] extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a [[ biologically supported sparse adjacency matrix ]] to capture both local and global inter-channel relations .",0
903,RGNN model,local and global inter-channel relations,[],"Our [[ RGNN model ]] extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both [[ local and global inter-channel relations ]] .",0
904,simple graph convolution network ( SGC ),topological structure,[],"Our RGNN model extends the [[ simple graph convolution network ( SGC ) ]] and leverages the [[ topological structure ]] of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
905,simple graph convolution network ( SGC ),EEG signals,[],"Our RGNN model extends the [[ simple graph convolution network ( SGC ) ]] and leverages the topological structure of [[ EEG signals ]] , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
906,simple graph convolution network ( SGC ),economy of brain network organization,[],"Our RGNN model extends the [[ simple graph convolution network ( SGC ) ]] and leverages the topological structure of EEG signals , i.e. , according to the [[ economy of brain network organization ]] , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
907,simple graph convolution network ( SGC ),biologically supported sparse adjacency matrix,[],"Our RGNN model extends the [[ simple graph convolution network ( SGC ) ]] and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a [[ biologically supported sparse adjacency matrix ]] to capture both local and global inter-channel relations .",0
908,simple graph convolution network ( SGC ),local and global inter-channel relations,[],"Our RGNN model extends the [[ simple graph convolution network ( SGC ) ]] and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both [[ local and global inter-channel relations ]] .",0
909,topological structure,EEG signals,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the [[ topological structure ]] of [[ EEG signals ]] , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
910,topological structure,economy of brain network organization,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the [[ topological structure ]] of EEG signals , i.e. , according to the [[ economy of brain network organization ]] , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
911,topological structure,biologically supported sparse adjacency matrix,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the [[ topological structure ]] of EEG signals , i.e. , according to the economy of brain network organization , we propose a [[ biologically supported sparse adjacency matrix ]] to capture both local and global inter-channel relations .",0
912,topological structure,local and global inter-channel relations,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the [[ topological structure ]] of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both [[ local and global inter-channel relations ]] .",0
913,EEG signals,economy of brain network organization,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of [[ EEG signals ]] , i.e. , according to the [[ economy of brain network organization ]] , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",0
914,EEG signals,biologically supported sparse adjacency matrix,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of [[ EEG signals ]] , i.e. , according to the economy of brain network organization , we propose a [[ biologically supported sparse adjacency matrix ]] to capture both local and global inter-channel relations .",0
915,EEG signals,local and global inter-channel relations,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of [[ EEG signals ]] , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both [[ local and global inter-channel relations ]] .",0
916,economy of brain network organization,biologically supported sparse adjacency matrix,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the [[ economy of brain network organization ]] , we propose a [[ biologically supported sparse adjacency matrix ]] to capture both local and global inter-channel relations .",0
917,economy of brain network organization,local and global inter-channel relations,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the [[ economy of brain network organization ]] , we propose a biologically supported sparse adjacency matrix to capture both [[ local and global inter-channel relations ]] .",0
918,biologically supported sparse adjacency matrix,local and global inter-channel relations,[],"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a [[ biologically supported sparse adjacency matrix ]] to capture both [[ local and global inter-channel relations ]] .",0
919,data augmentation,helpful,[],"As the last block of rows in the table shows , [[ data augmentation ]] proves to be [[ helpful ]] in further boosting performance .",0
920,data augmentation,further boosting performance,[],"As the last block of rows in the table shows , [[ data augmentation ]] proves to be helpful in [[ further boosting performance ]] .",0
921,helpful,further boosting performance,[],"As the last block of rows in the table shows , data augmentation proves to be [[ helpful ]] in [[ further boosting performance ]] .",0
922,self - attention,DEEP - ATT,"[['self - attention', 'has', 'DEEP - ATT']]","Along with [[ self - attention ]] , [[ DEEP - ATT ]] comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",1
923,self - attention,three variants,"[['self - attention', 'has', 'DEEP - ATT']]","Along with [[ self - attention ]] , DEEP - ATT comes with [[ three variants ]] which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",0
924,self - attention,recurrent ( RNN ),"[['self - attention', 'has', 'DEEP - ATT']]","Along with [[ self - attention ]] , DEEP - ATT comes with three variants which uses [[ recurrent ( RNN ) ]] , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",0
925,self - attention,convolutional ( CNN ),"[['self - attention', 'has', 'DEEP - ATT']]","Along with [[ self - attention ]] , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , [[ convolutional ( CNN ) ]] and feed - forward ( FFN ) neural network to further enhance the representations .",0
926,self - attention,feed - forward ( FFN ) neural network,"[['self - attention', 'has', 'DEEP - ATT']]","Along with [[ self - attention ]] , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and [[ feed - forward ( FFN ) neural network ]] to further enhance the representations .",0
927,self - attention,representations,"[['self - attention', 'has', 'DEEP - ATT']]","Along with [[ self - attention ]] , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the [[ representations ]] .",0
928,DEEP - ATT,three variants,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , [[ DEEP - ATT ]] comes with [[ three variants ]] which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",0
929,DEEP - ATT,recurrent ( RNN ),"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , [[ DEEP - ATT ]] comes with three variants which uses [[ recurrent ( RNN ) ]] , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",0
930,DEEP - ATT,convolutional ( CNN ),"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , [[ DEEP - ATT ]] comes with three variants which uses recurrent ( RNN ) , [[ convolutional ( CNN ) ]] and feed - forward ( FFN ) neural network to further enhance the representations .",0
931,DEEP - ATT,feed - forward ( FFN ) neural network,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , [[ DEEP - ATT ]] comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and [[ feed - forward ( FFN ) neural network ]] to further enhance the representations .",0
932,DEEP - ATT,representations,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , [[ DEEP - ATT ]] comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the [[ representations ]] .",0
933,three variants,recurrent ( RNN ),"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with [[ three variants ]] which uses [[ recurrent ( RNN ) ]] , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",0
934,three variants,convolutional ( CNN ),"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with [[ three variants ]] which uses recurrent ( RNN ) , [[ convolutional ( CNN ) ]] and feed - forward ( FFN ) neural network to further enhance the representations .",0
935,three variants,feed - forward ( FFN ) neural network,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with [[ three variants ]] which uses recurrent ( RNN ) , convolutional ( CNN ) and [[ feed - forward ( FFN ) neural network ]] to further enhance the representations .",0
936,three variants,representations,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with [[ three variants ]] which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the [[ representations ]] .",0
937,recurrent ( RNN ),convolutional ( CNN ),"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with three variants which uses [[ recurrent ( RNN ) ]] , [[ convolutional ( CNN ) ]] and feed - forward ( FFN ) neural network to further enhance the representations .",0
938,recurrent ( RNN ),feed - forward ( FFN ) neural network,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with three variants which uses [[ recurrent ( RNN ) ]] , convolutional ( CNN ) and [[ feed - forward ( FFN ) neural network ]] to further enhance the representations .",0
939,recurrent ( RNN ),representations,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with three variants which uses [[ recurrent ( RNN ) ]] , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the [[ representations ]] .",0
940,convolutional ( CNN ),feed - forward ( FFN ) neural network,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , [[ convolutional ( CNN ) ]] and [[ feed - forward ( FFN ) neural network ]] to further enhance the representations .",0
941,convolutional ( CNN ),representations,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , [[ convolutional ( CNN ) ]] and feed - forward ( FFN ) neural network to further enhance the [[ representations ]] .",0
942,feed - forward ( FFN ) neural network,representations,"[['self - attention', 'has', 'DEEP - ATT']]","Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and [[ feed - forward ( FFN ) neural network ]] to further enhance the [[ representations ]] .",0
943,greatly improve,model,[],"Finally , we demonstrate that we [[ greatly improve ]] upon the [[ model ]] of , which actually corresponds to a setting with the Path representation and C 1 as candidate set .",0
944,greatly improve,setting,[],"Finally , we demonstrate that we [[ greatly improve ]] upon the model of , which actually corresponds to a [[ setting ]] with the Path representation and C 1 as candidate set .",0
945,greatly improve,Path representation,[],"Finally , we demonstrate that we [[ greatly improve ]] upon the model of , which actually corresponds to a setting with the [[ Path representation ]] and C 1 as candidate set .",0
946,greatly improve,C 1,[],"Finally , we demonstrate that we [[ greatly improve ]] upon the model of , which actually corresponds to a setting with the Path representation and [[ C 1 ]] as candidate set .",0
947,greatly improve,candidate set,[],"Finally , we demonstrate that we [[ greatly improve ]] upon the model of , which actually corresponds to a setting with the Path representation and C 1 as [[ candidate set ]] .",0
948,model,setting,[],"Finally , we demonstrate that we greatly improve upon the [[ model ]] of , which actually corresponds to a [[ setting ]] with the Path representation and C 1 as candidate set .",0
949,model,Path representation,[],"Finally , we demonstrate that we greatly improve upon the [[ model ]] of , which actually corresponds to a setting with the [[ Path representation ]] and C 1 as candidate set .",0
950,model,C 1,[],"Finally , we demonstrate that we greatly improve upon the [[ model ]] of , which actually corresponds to a setting with the Path representation and [[ C 1 ]] as candidate set .",0
951,model,candidate set,[],"Finally , we demonstrate that we greatly improve upon the [[ model ]] of , which actually corresponds to a setting with the Path representation and C 1 as [[ candidate set ]] .",0
952,setting,Path representation,[],"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a [[ setting ]] with the [[ Path representation ]] and C 1 as candidate set .",0
953,setting,C 1,[],"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a [[ setting ]] with the Path representation and [[ C 1 ]] as candidate set .",0
954,setting,candidate set,[],"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a [[ setting ]] with the Path representation and C 1 as [[ candidate set ]] .",0
955,Path representation,C 1,[],"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the [[ Path representation ]] and [[ C 1 ]] as candidate set .",0
956,Path representation,candidate set,[],"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the [[ Path representation ]] and C 1 as [[ candidate set ]] .",0
957,C 1,candidate set,[],"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the Path representation and [[ C 1 ]] as [[ candidate set ]] .",0
958,encoder - decoder architecture,machine translation,[],The [[ encoder - decoder architecture ]] has been widely used for [[ machine translation ]] and machine comprehension tasks .,0
959,encoder - decoder architecture,machine comprehension,[],The [[ encoder - decoder architecture ]] has been widely used for machine translation and [[ machine comprehension ]] tasks .,0
960,machine translation,machine comprehension,[],The encoder - decoder architecture has been widely used for [[ machine translation ]] and [[ machine comprehension ]] tasks .,0
961,all weights,our model,[],"We initialize [[ all weights ]] of [[ our model ]] by sampling from the normal distribution N ( 0 , 0.05 ) .",0
962,all weights,sampling,[],"We initialize [[ all weights ]] of our model by [[ sampling ]] from the normal distribution N ( 0 , 0.05 ) .",0
963,all weights,"normal distribution N ( 0 , 0.05 )",[],"We initialize [[ all weights ]] of our model by sampling from the [[ normal distribution N ( 0 , 0.05 ) ]] .",0
964,our model,sampling,[],"We initialize all weights of [[ our model ]] by [[ sampling ]] from the normal distribution N ( 0 , 0.05 ) .",0
965,our model,"normal distribution N ( 0 , 0.05 )",[],"We initialize all weights of [[ our model ]] by sampling from the [[ normal distribution N ( 0 , 0.05 ) ]] .",0
966,sampling,"normal distribution N ( 0 , 0.05 )",[],"We initialize all weights of our model by [[ sampling ]] from the [[ normal distribution N ( 0 , 0.05 ) ]] .",0
967,beam search,beam size,[],We use [[ beam search ]] with a [[ beam size ]] of 4 and length penalty ? = 0.6 .,0
968,beam search,4,[],We use [[ beam search ]] with a beam size of [[ 4 ]] and length penalty ? = 0.6 .,0
969,beam search,length penalty,[],We use [[ beam search ]] with a beam size of 4 and [[ length penalty ]] ? = 0.6 .,0
970,beam search,0.6,[],We use [[ beam search ]] with a beam size of 4 and length penalty ? = [[ 0.6 ]] .,0
971,beam size,4,[],We use beam search with a [[ beam size ]] of [[ 4 ]] and length penalty ? = 0.6 .,0
972,beam size,length penalty,[],We use beam search with a [[ beam size ]] of 4 and [[ length penalty ]] ? = 0.6 .,0
973,beam size,0.6,[],We use beam search with a [[ beam size ]] of 4 and length penalty ? = [[ 0.6 ]] .,0
974,4,length penalty,[],We use beam search with a beam size of [[ 4 ]] and [[ length penalty ]] ? = 0.6 .,0
975,4,0.6,[],We use beam search with a beam size of [[ 4 ]] and length penalty ? = [[ 0.6 ]] .,0
976,length penalty,0.6,[],We use beam search with a beam size of 4 and [[ length penalty ]] ? = [[ 0.6 ]] .,0
977,Distancebased Self - Attention Network,distance mask,[],"To tackle this limitation , we propose [[ Distancebased Self - Attention Network ]] which introduces a [[ distance mask ]] which models the relative distance between words .",0
978,Distancebased Self - Attention Network,relative distance,[],"To tackle this limitation , we propose [[ Distancebased Self - Attention Network ]] which introduces a distance mask which models the [[ relative distance ]] between words .",0
979,Distancebased Self - Attention Network,words,[],"To tackle this limitation , we propose [[ Distancebased Self - Attention Network ]] which introduces a distance mask which models the relative distance between [[ words ]] .",0
980,distance mask,relative distance,[],"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a [[ distance mask ]] which models the [[ relative distance ]] between words .",0
981,distance mask,words,[],"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a [[ distance mask ]] which models the relative distance between [[ words ]] .",0
982,relative distance,words,[],"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a distance mask which models the [[ relative distance ]] between [[ words ]] .",0
983,neural networks,CNN baseline,"[['neural networks', 'has', 'CNN baseline']]","Amongst the [[ neural networks ]] , the [[ CNN baseline ]] receives the least performance .",1
984,neural networks,least performance,"[['neural networks', 'has', 'CNN baseline']]","Amongst the [[ neural networks ]] , the CNN baseline receives the [[ least performance ]] .",0
985,CNN baseline,least performance,"[['neural networks', 'has', 'CNN baseline']]","Amongst the neural networks , the [[ CNN baseline ]] receives the [[ least performance ]] .",0
986,initial learning rate,accumulator value,"[['0.1', 'has', 'accumulator value'], ['0.15', 'has', 'initial learning rate']]","The [[ initial learning rate ]] and the [[ accumulator value ]] were set to 0.15 and 0.1 , respectively .",0
987,initial learning rate,0.15,"[['0.1', 'has', 'accumulator value'], ['0.15', 'has', 'initial learning rate']]","The [[ initial learning rate ]] and the accumulator value were set to [[ 0.15 ]] and 0.1 , respectively .",0
988,initial learning rate,0.1,"[['0.1', 'has', 'accumulator value'], ['0.15', 'has', 'initial learning rate']]","The [[ initial learning rate ]] and the accumulator value were set to 0.15 and [[ 0.1 ]] , respectively .",0
989,accumulator value,0.15,"[['0.1', 'has', 'accumulator value'], ['0.15', 'has', 'initial learning rate']]","The initial learning rate and the [[ accumulator value ]] were set to [[ 0.15 ]] and 0.1 , respectively .",0
990,accumulator value,0.1,"[['0.1', 'has', 'accumulator value'], ['0.15', 'has', 'initial learning rate']]","The initial learning rate and the [[ accumulator value ]] were set to 0.15 and [[ 0.1 ]] , respectively .",0
991,0.15,0.1,"[['0.1', 'has', 'accumulator value'], ['0.15', 'has', 'initial learning rate']]","The initial learning rate and the accumulator value were set to [[ 0.15 ]] and [[ 0.1 ]] , respectively .",0
992,our model,recurrence,[],"Specifically , [[ our model ]] eschews [[ recurrence ]] and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .",0
993,our model,attention,[],"Specifically , [[ our model ]] eschews recurrence and employs [[ attention ]] as a competitive alternative to draw the introspective and interactive semantics between target and context words .",0
994,our model,competitive alternative,[],"Specifically , [[ our model ]] eschews recurrence and employs attention as a [[ competitive alternative ]] to draw the introspective and interactive semantics between target and context words .",0
995,our model,introspective and interactive semantics,[],"Specifically , [[ our model ]] eschews recurrence and employs attention as a competitive alternative to draw the [[ introspective and interactive semantics ]] between target and context words .",0
996,our model,target and context words,[],"Specifically , [[ our model ]] eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between [[ target and context words ]] .",0
997,recurrence,attention,[],"Specifically , our model eschews [[ recurrence ]] and employs [[ attention ]] as a competitive alternative to draw the introspective and interactive semantics between target and context words .",0
998,recurrence,competitive alternative,[],"Specifically , our model eschews [[ recurrence ]] and employs attention as a [[ competitive alternative ]] to draw the introspective and interactive semantics between target and context words .",0
999,recurrence,introspective and interactive semantics,[],"Specifically , our model eschews [[ recurrence ]] and employs attention as a competitive alternative to draw the [[ introspective and interactive semantics ]] between target and context words .",0
1000,recurrence,target and context words,[],"Specifically , our model eschews [[ recurrence ]] and employs attention as a competitive alternative to draw the introspective and interactive semantics between [[ target and context words ]] .",0
1001,attention,competitive alternative,[],"Specifically , our model eschews recurrence and employs [[ attention ]] as a [[ competitive alternative ]] to draw the introspective and interactive semantics between target and context words .",0
1002,attention,introspective and interactive semantics,[],"Specifically , our model eschews recurrence and employs [[ attention ]] as a competitive alternative to draw the [[ introspective and interactive semantics ]] between target and context words .",0
1003,attention,target and context words,[],"Specifically , our model eschews recurrence and employs [[ attention ]] as a competitive alternative to draw the introspective and interactive semantics between [[ target and context words ]] .",0
1004,competitive alternative,introspective and interactive semantics,[],"Specifically , our model eschews recurrence and employs attention as a [[ competitive alternative ]] to draw the [[ introspective and interactive semantics ]] between target and context words .",0
1005,competitive alternative,target and context words,[],"Specifically , our model eschews recurrence and employs attention as a [[ competitive alternative ]] to draw the introspective and interactive semantics between [[ target and context words ]] .",0
1006,introspective and interactive semantics,target and context words,[],"Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the [[ introspective and interactive semantics ]] between [[ target and context words ]] .",0
1007,paired t- test,significance testing,[],The [[ paired t- test ]] is used for the [[ significance testing ]] .,0
1008,AdaGrad optimizer,initial learning rate,[],"We adopt the [[ AdaGrad optimizer ]] with [[ initial learning rate ]] tuned amongst { 0.2 , 0.1 , 0.05 , 0.01 } .",0
1009,AdaGrad optimizer,"{ 0.2 , 0.1 , 0.05 , 0.01 }",[],"We adopt the [[ AdaGrad optimizer ]] with initial learning rate tuned amongst [[ { 0.2 , 0.1 , 0.05 , 0.01 } ]] .",0
1010,initial learning rate,"{ 0.2 , 0.1 , 0.05 , 0.01 }",[],"We adopt the AdaGrad optimizer with [[ initial learning rate ]] tuned amongst [[ { 0.2 , 0.1 , 0.05 , 0.01 } ]] .",0
1011,Our model,In - fer Sent,[],[[ Our model ]] also outperforms [[ In - fer Sent ]] which achieves an accuracy of 85.1 % in our experiments .,0
1012,Our model,achieves,[],[[ Our model ]] also outperforms In - fer Sent which [[ achieves ]] an accuracy of 85.1 % in our experiments .,0
1013,Our model,85.1 %,[],[[ Our model ]] also outperforms In - fer Sent which achieves an accuracy of [[ 85.1 % ]] in our experiments .,0
1014,In - fer Sent,achieves,[],Our model also outperforms [[ In - fer Sent ]] which [[ achieves ]] an accuracy of 85.1 % in our experiments .,0
1015,In - fer Sent,85.1 %,[],Our model also outperforms [[ In - fer Sent ]] which achieves an accuracy of [[ 85.1 % ]] in our experiments .,0
1016,achieves,85.1 %,[],Our model also outperforms In - fer Sent which [[ achieves ]] an accuracy of [[ 85.1 % ]] in our experiments .,0
1017,IMD b,test error,[],On [[ IMD b ]] we lower the [[ test error ]] from 5.30 of a single model to 4.58 for the bidirectional model .,0
1018,IMD b,5.30,[],On [[ IMD b ]] we lower the test error from [[ 5.30 ]] of a single model to 4.58 for the bidirectional model .,0
1019,IMD b,single model,[],On [[ IMD b ]] we lower the test error from 5.30 of a [[ single model ]] to 4.58 for the bidirectional model .,0
1020,IMD b,4.58,[],On [[ IMD b ]] we lower the test error from 5.30 of a single model to [[ 4.58 ]] for the bidirectional model .,0
1021,IMD b,bidirectional model,[],On [[ IMD b ]] we lower the test error from 5.30 of a single model to 4.58 for the [[ bidirectional model ]] .,0
1022,test error,5.30,[],On IMD b we lower the [[ test error ]] from [[ 5.30 ]] of a single model to 4.58 for the bidirectional model .,0
1023,test error,single model,[],On IMD b we lower the [[ test error ]] from 5.30 of a [[ single model ]] to 4.58 for the bidirectional model .,0
1024,test error,4.58,[],On IMD b we lower the [[ test error ]] from 5.30 of a single model to [[ 4.58 ]] for the bidirectional model .,0
1025,test error,bidirectional model,[],On IMD b we lower the [[ test error ]] from 5.30 of a single model to 4.58 for the [[ bidirectional model ]] .,0
1026,5.30,single model,[],On IMD b we lower the test error from [[ 5.30 ]] of a [[ single model ]] to 4.58 for the bidirectional model .,0
1027,5.30,4.58,[],On IMD b we lower the test error from [[ 5.30 ]] of a single model to [[ 4.58 ]] for the bidirectional model .,0
1028,5.30,bidirectional model,[],On IMD b we lower the test error from [[ 5.30 ]] of a single model to 4.58 for the [[ bidirectional model ]] .,0
1029,single model,4.58,[],On IMD b we lower the test error from 5.30 of a [[ single model ]] to [[ 4.58 ]] for the bidirectional model .,0
1030,single model,bidirectional model,[],On IMD b we lower the test error from 5.30 of a [[ single model ]] to 4.58 for the [[ bidirectional model ]] .,0
1031,4.58,bidirectional model,[],On IMD b we lower the test error from 5.30 of a single model to [[ 4.58 ]] for the [[ bidirectional model ]] .,0
1032,preprocessing mechanism,further improvements,[],We can see that our [[ preprocessing mechanism ]] leads to [[ further improvements ]] of 0.4 % and 0.3 % on the SNLI test set for our single and ensemble models respectively .,0
1033,preprocessing mechanism,0.4 % and 0.3 %,[],We can see that our [[ preprocessing mechanism ]] leads to further improvements of [[ 0.4 % and 0.3 % ]] on the SNLI test set for our single and ensemble models respectively .,0
1034,preprocessing mechanism,SNLI test set,[],We can see that our [[ preprocessing mechanism ]] leads to further improvements of 0.4 % and 0.3 % on the [[ SNLI test set ]] for our single and ensemble models respectively .,0
1035,preprocessing mechanism,our single and ensemble models,[],We can see that our [[ preprocessing mechanism ]] leads to further improvements of 0.4 % and 0.3 % on the SNLI test set for [[ our single and ensemble models ]] respectively .,0
1036,further improvements,0.4 % and 0.3 %,[],We can see that our preprocessing mechanism leads to [[ further improvements ]] of [[ 0.4 % and 0.3 % ]] on the SNLI test set for our single and ensemble models respectively .,0
1037,further improvements,SNLI test set,[],We can see that our preprocessing mechanism leads to [[ further improvements ]] of 0.4 % and 0.3 % on the [[ SNLI test set ]] for our single and ensemble models respectively .,0
1038,further improvements,our single and ensemble models,[],We can see that our preprocessing mechanism leads to [[ further improvements ]] of 0.4 % and 0.3 % on the SNLI test set for [[ our single and ensemble models ]] respectively .,0
1039,0.4 % and 0.3 %,SNLI test set,[],We can see that our preprocessing mechanism leads to further improvements of [[ 0.4 % and 0.3 % ]] on the [[ SNLI test set ]] for our single and ensemble models respectively .,0
1040,0.4 % and 0.3 %,our single and ensemble models,[],We can see that our preprocessing mechanism leads to further improvements of [[ 0.4 % and 0.3 % ]] on the SNLI test set for [[ our single and ensemble models ]] respectively .,0
1041,SNLI test set,our single and ensemble models,[],We can see that our preprocessing mechanism leads to further improvements of 0.4 % and 0.3 % on the [[ SNLI test set ]] for [[ our single and ensemble models ]] respectively .,0
1042,Dynamically Fused Graph Network ( DFGN ),novel method,"[['Dynamically Fused Graph Network ( DFGN )', 'has', 'novel method']]","In this paper , we propose [[ Dynamically Fused Graph Network ( DFGN ) ]] , a [[ novel method ]] to address the aforementioned concerns for multi-hop text - based QA .",1
1043,allowing interactions,the endpoints,[],"Second , we observe the importance of [[ allowing interactions ]] between [[ the endpoints ]] using the spanlevel FFNN .",0
1044,allowing interactions,spanlevel FFNN,[],"Second , we observe the importance of [[ allowing interactions ]] between the endpoints using the [[ spanlevel FFNN ]] .",0
1045,the endpoints,spanlevel FFNN,[],"Second , we observe the importance of allowing interactions between [[ the endpoints ]] using the [[ spanlevel FFNN ]] .",0
1046,Residual LSTM,current state - of - the - art,[],[[ Residual LSTM ]] is also the [[ current state - of - the - art ]] on the MSCOCO dataset .,0
1047,Residual LSTM,MSCOCO dataset,[],[[ Residual LSTM ]] is also the current state - of - the - art on the [[ MSCOCO dataset ]] .,0
1048,current state - of - the - art,MSCOCO dataset,[],Residual LSTM is also the [[ current state - of - the - art ]] on the [[ MSCOCO dataset ]] .,0
1049,answer verification,great contribution,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the [[ answer verification ]] makes a [[ great contribution ]] to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .",0
1050,answer verification,over all improvement,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the [[ answer verification ]] makes a great contribution to the [[ over all improvement ]] , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .",0
1051,answer verification,our hypothesis,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the [[ answer verification ]] makes a great contribution to the over all improvement , which confirms [[ our hypothesis ]] that cross - passage answer verification is useful for the multi-passage MRC .",0
1052,answer verification,cross - passage answer verification,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the [[ answer verification ]] makes a great contribution to the over all improvement , which confirms our hypothesis that [[ cross - passage answer verification ]] is useful for the multi-passage MRC .",0
1053,answer verification,useful,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the [[ answer verification ]] makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is [[ useful ]] for the multi-passage MRC .",0
1054,answer verification,multi-passage MRC,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the [[ answer verification ]] makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the [[ multi-passage MRC ]] .",0
1055,great contribution,over all improvement,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a [[ great contribution ]] to the [[ over all improvement ]] , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .",0
1056,great contribution,our hypothesis,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a [[ great contribution ]] to the over all improvement , which confirms [[ our hypothesis ]] that cross - passage answer verification is useful for the multi-passage MRC .",0
1057,great contribution,cross - passage answer verification,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a [[ great contribution ]] to the over all improvement , which confirms our hypothesis that [[ cross - passage answer verification ]] is useful for the multi-passage MRC .",0
1058,great contribution,useful,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a [[ great contribution ]] to the over all improvement , which confirms our hypothesis that cross - passage answer verification is [[ useful ]] for the multi-passage MRC .",0
1059,great contribution,multi-passage MRC,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a [[ great contribution ]] to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the [[ multi-passage MRC ]] .",0
1060,over all improvement,our hypothesis,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the [[ over all improvement ]] , which confirms [[ our hypothesis ]] that cross - passage answer verification is useful for the multi-passage MRC .",0
1061,over all improvement,cross - passage answer verification,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the [[ over all improvement ]] , which confirms our hypothesis that [[ cross - passage answer verification ]] is useful for the multi-passage MRC .",0
1062,over all improvement,useful,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the [[ over all improvement ]] , which confirms our hypothesis that cross - passage answer verification is [[ useful ]] for the multi-passage MRC .",0
1063,over all improvement,multi-passage MRC,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the [[ over all improvement ]] , which confirms our hypothesis that cross - passage answer verification is useful for the [[ multi-passage MRC ]] .",0
1064,our hypothesis,cross - passage answer verification,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms [[ our hypothesis ]] that [[ cross - passage answer verification ]] is useful for the multi-passage MRC .",0
1065,our hypothesis,useful,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms [[ our hypothesis ]] that cross - passage answer verification is [[ useful ]] for the multi-passage MRC .",0
1066,our hypothesis,multi-passage MRC,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms [[ our hypothesis ]] that cross - passage answer verification is useful for the [[ multi-passage MRC ]] .",0
1067,cross - passage answer verification,useful,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that [[ cross - passage answer verification ]] is [[ useful ]] for the multi-passage MRC .",1
1068,cross - passage answer verification,multi-passage MRC,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that [[ cross - passage answer verification ]] is useful for the [[ multi-passage MRC ]] .",0
1069,useful,multi-passage MRC,"[['cross - passage answer verification', 'has', 'useful']]","From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is [[ useful ]] for the [[ multi-passage MRC ]] .",0
1070,second approach,Multi - Layer Perceptron and an LSTM classifier,[],"In the [[ second approach ]] , we build a [[ Multi - Layer Perceptron and an LSTM classifier ]] to recognize emotion given a speech signal .",0
1071,second approach,emotion,[],"In the [[ second approach ]] , we build a Multi - Layer Perceptron and an LSTM classifier to recognize [[ emotion ]] given a speech signal .",0
1072,second approach,speech signal,[],"In the [[ second approach ]] , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a [[ speech signal ]] .",0
1073,Multi - Layer Perceptron and an LSTM classifier,emotion,[],"In the second approach , we build a [[ Multi - Layer Perceptron and an LSTM classifier ]] to recognize [[ emotion ]] given a speech signal .",0
1074,Multi - Layer Perceptron and an LSTM classifier,speech signal,[],"In the second approach , we build a [[ Multi - Layer Perceptron and an LSTM classifier ]] to recognize emotion given a [[ speech signal ]] .",0
1075,emotion,speech signal,[],"In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize [[ emotion ]] given a [[ speech signal ]] .",0
1076,our system ( single model ),further improvement,[],"Finally , we implement our system based on this new strategy , and [[ our system ( single model ) ]] achieves [[ further improvement ]] by a large margin .",0
1077,our system ( single model ),large margin,[],"Finally , we implement our system based on this new strategy , and [[ our system ( single model ) ]] achieves further improvement by a [[ large margin ]] .",0
1078,further improvement,large margin,[],"Finally , we implement our system based on this new strategy , and our system ( single model ) achieves [[ further improvement ]] by a [[ large margin ]] .",0
1079,hypothesis - based attention ( model 24 ),decrease,"[['hypothesis - based attention ( model 24 )', 'has', 'decrease'], ['decrease', 'has', 'accuracy']]","Removing the [[ hypothesis - based attention ( model 24 ) ]] [[ decrease ]] the accuracy to 86.5 % , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .",1
1080,hypothesis - based attention ( model 24 ),accuracy,"[['hypothesis - based attention ( model 24 )', 'has', 'decrease'], ['decrease', 'has', 'accuracy']]","Removing the [[ hypothesis - based attention ( model 24 ) ]] decrease the [[ accuracy ]] to 86.5 % , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .",0
1081,hypothesis - based attention ( model 24 ),86.5 %,"[['hypothesis - based attention ( model 24 )', 'has', 'decrease'], ['decrease', 'has', 'accuracy']]","Removing the [[ hypothesis - based attention ( model 24 ) ]] decrease the accuracy to [[ 86.5 % ]] , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .",0
1082,decrease,accuracy,"[['hypothesis - based attention ( model 24 )', 'has', 'decrease'], ['decrease', 'has', 'accuracy']]","Removing the hypothesis - based attention ( model 24 ) [[ decrease ]] the [[ accuracy ]] to 86.5 % , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .",1
1083,decrease,86.5 %,"[['hypothesis - based attention ( model 24 )', 'has', 'decrease'], ['decrease', 'has', 'accuracy']]","Removing the hypothesis - based attention ( model 24 ) [[ decrease ]] the accuracy to [[ 86.5 % ]] , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .",0
1084,accuracy,86.5 %,"[['hypothesis - based attention ( model 24 )', 'has', 'decrease'], ['decrease', 'has', 'accuracy']]","Removing the hypothesis - based attention ( model 24 ) decrease the [[ accuracy ]] to [[ 86.5 % ]] , where hypothesis - based attention is the attention performed on the other direction for the sentence pairs .",0
1085,contextual information,discourse,[],"Second , it extracts [[ contextual information ]] from the [[ discourse ]] of comments in the discussion forums .",0
1086,contextual information,comments,[],"Second , it extracts [[ contextual information ]] from the discourse of [[ comments ]] in the discussion forums .",0
1087,contextual information,discussion forums,[],"Second , it extracts [[ contextual information ]] from the discourse of comments in the [[ discussion forums ]] .",0
1088,discourse,comments,[],"Second , it extracts contextual information from the [[ discourse ]] of [[ comments ]] in the discussion forums .",0
1089,discourse,discussion forums,[],"Second , it extracts contextual information from the [[ discourse ]] of comments in the [[ discussion forums ]] .",0
1090,comments,discussion forums,[],"Second , it extracts contextual information from the discourse of [[ comments ]] in the [[ discussion forums ]] .",0
1091,Memn2n,memory representations,[],"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .",0
1092,Memn2n,each historical utterance,[],"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without sequential modeling .",0
1093,Memn2n,embedding matrix B,[],"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
1094,Memn2n,sequential modeling,[],"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
1095,memory representations,each historical utterance,[],"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without sequential modeling .",0
1096,memory representations,embedding matrix B,[],"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
1097,memory representations,sequential modeling,[],"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
1098,each historical utterance,embedding matrix B,[],"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
1099,each historical utterance,sequential modeling,[],"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
1100,embedding matrix B,sequential modeling,[],"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without [[ sequential modeling ]] .",0
1101,special separate tokens and the hidden entity vectors,important contributions,[],This ablation study demonstrates that both the [[ special separate tokens and the hidden entity vectors ]] make [[ important contributions ]] to our approach .,0
1102,task - specific loss,task - specific parameters,[],"For a given NLP task , in addition to minimize the [[ task - specific loss ]] by optimizing the [[ task - specific parameters ]] together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",0
1103,task - specific loss,word embeddings,[],"For a given NLP task , in addition to minimize the [[ task - specific loss ]] by optimizing the task - specific parameters together with [[ word embeddings ]] , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",0
1104,task - specific loss,another discriminator,[],"For a given NLP task , in addition to minimize the [[ task - specific loss ]] by optimizing the task - specific parameters together with word embeddings , we introduce [[ another discriminator ]] , which takes a word embedding as input and classifies whether it is a popular / rare word .",0
1105,task - specific loss,a word embedding,[],"For a given NLP task , in addition to minimize the [[ task - specific loss ]] by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes [[ a word embedding ]] as input and classifies whether it is a popular / rare word .",0
1106,task - specific loss,popular / rare word,[],"For a given NLP task , in addition to minimize the [[ task - specific loss ]] by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a [[ popular / rare word ]] .",0
1107,task - specific parameters,word embeddings,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the [[ task - specific parameters ]] together with [[ word embeddings ]] , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",0
1108,task - specific parameters,another discriminator,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the [[ task - specific parameters ]] together with word embeddings , we introduce [[ another discriminator ]] , which takes a word embedding as input and classifies whether it is a popular / rare word .",0
1109,task - specific parameters,a word embedding,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the [[ task - specific parameters ]] together with word embeddings , we introduce another discriminator , which takes [[ a word embedding ]] as input and classifies whether it is a popular / rare word .",0
1110,task - specific parameters,popular / rare word,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the [[ task - specific parameters ]] together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a [[ popular / rare word ]] .",0
1111,word embeddings,another discriminator,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with [[ word embeddings ]] , we introduce [[ another discriminator ]] , which takes a word embedding as input and classifies whether it is a popular / rare word .",0
1112,word embeddings,a word embedding,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with [[ word embeddings ]] , we introduce another discriminator , which takes [[ a word embedding ]] as input and classifies whether it is a popular / rare word .",0
1113,word embeddings,popular / rare word,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with [[ word embeddings ]] , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a [[ popular / rare word ]] .",0
1114,another discriminator,a word embedding,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce [[ another discriminator ]] , which takes [[ a word embedding ]] as input and classifies whether it is a popular / rare word .",0
1115,another discriminator,popular / rare word,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce [[ another discriminator ]] , which takes a word embedding as input and classifies whether it is a [[ popular / rare word ]] .",0
1116,a word embedding,popular / rare word,[],"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes [[ a word embedding ]] as input and classifies whether it is a [[ popular / rare word ]] .",0
1117,Tree-LSTM,Memory cells,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","[[ Tree-LSTM ]] : [[ Memory cells ]] was introduced by Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",1
1118,Tree-LSTM,Tree - Structured Long Short - Term Memory,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","[[ Tree-LSTM ]] : Memory cells was introduced by [[ Tree - Structured Long Short - Term Memory ]] and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1119,Tree-LSTM,gates,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","[[ Tree-LSTM ]] : Memory cells was introduced by Tree - Structured Long Short - Term Memory and [[ gates ]] into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",1
1120,Tree-LSTM,tree - structured neural network,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","[[ Tree-LSTM ]] : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1121,Memory cells,Tree - Structured Long Short - Term Memory,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","Tree-LSTM : [[ Memory cells ]] was introduced by [[ Tree - Structured Long Short - Term Memory ]] and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1122,Memory cells,gates,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","Tree-LSTM : [[ Memory cells ]] was introduced by Tree - Structured Long Short - Term Memory and [[ gates ]] into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1123,Memory cells,tree - structured neural network,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","Tree-LSTM : [[ Memory cells ]] was introduced by Tree - Structured Long Short - Term Memory and gates into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1124,Tree - Structured Long Short - Term Memory,gates,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","Tree-LSTM : Memory cells was introduced by [[ Tree - Structured Long Short - Term Memory ]] and [[ gates ]] into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1125,Tree - Structured Long Short - Term Memory,tree - structured neural network,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","Tree-LSTM : Memory cells was introduced by [[ Tree - Structured Long Short - Term Memory ]] and gates into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1126,gates,tree - structured neural network,"[['Tree-LSTM', 'has', 'gates'], ['Tree-LSTM', 'has', 'Memory cells']]","Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and [[ gates ]] into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1127,ACE04 dataset,effectiveness of the various parts of our joint model,[],We conduct ablation tests on the [[ ACE04 dataset ]] reported in to analyze the [[ effectiveness of the various parts of our joint model ]] .,0
1128,BL - MN,Our basic memory network,"[['BL - MN', 'has', 'Our basic memory network']]","[[ BL - MN ]] : [[ Our basic memory network ]] presented in Section 2 , which does not use the proposed techniques for capturing target - sensitive sentiments .",1
1129,BL - MN,proposed techniques,"[['BL - MN', 'has', 'Our basic memory network']]","[[ BL - MN ]] : Our basic memory network presented in Section 2 , which does not use the [[ proposed techniques ]] for capturing target - sensitive sentiments .",0
1130,BL - MN,target - sensitive sentiments,"[['BL - MN', 'has', 'Our basic memory network']]","[[ BL - MN ]] : Our basic memory network presented in Section 2 , which does not use the proposed techniques for capturing [[ target - sensitive sentiments ]] .",0
1131,Our basic memory network,proposed techniques,"[['BL - MN', 'has', 'Our basic memory network']]","BL - MN : [[ Our basic memory network ]] presented in Section 2 , which does not use the [[ proposed techniques ]] for capturing target - sensitive sentiments .",0
1132,Our basic memory network,target - sensitive sentiments,"[['BL - MN', 'has', 'Our basic memory network']]","BL - MN : [[ Our basic memory network ]] presented in Section 2 , which does not use the proposed techniques for capturing [[ target - sensitive sentiments ]] .",0
1133,proposed techniques,target - sensitive sentiments,"[['BL - MN', 'has', 'Our basic memory network']]","BL - MN : Our basic memory network presented in Section 2 , which does not use the [[ proposed techniques ]] for capturing [[ target - sensitive sentiments ]] .",0
1134,Adam optimizer,gradient clipping,[],"We use the [[ Adam optimizer ]] , including [[ gradient clipping ]] , by the norm at a threshold of 5 .",0
1135,Adam optimizer,norm,[],"We use the [[ Adam optimizer ]] , including gradient clipping , by the [[ norm ]] at a threshold of 5 .",0
1136,Adam optimizer,threshold,[],"We use the [[ Adam optimizer ]] , including gradient clipping , by the norm at a [[ threshold ]] of 5 .",0
1137,Adam optimizer,5,[],"We use the [[ Adam optimizer ]] , including gradient clipping , by the norm at a threshold of [[ 5 ]] .",0
1138,gradient clipping,norm,[],"We use the Adam optimizer , including [[ gradient clipping ]] , by the [[ norm ]] at a threshold of 5 .",0
1139,gradient clipping,threshold,[],"We use the Adam optimizer , including [[ gradient clipping ]] , by the norm at a [[ threshold ]] of 5 .",0
1140,gradient clipping,5,[],"We use the Adam optimizer , including [[ gradient clipping ]] , by the norm at a threshold of [[ 5 ]] .",0
1141,norm,threshold,[],"We use the Adam optimizer , including gradient clipping , by the [[ norm ]] at a [[ threshold ]] of 5 .",0
1142,norm,5,[],"We use the Adam optimizer , including gradient clipping , by the [[ norm ]] at a threshold of [[ 5 ]] .",0
1143,threshold,5,[],"We use the Adam optimizer , including gradient clipping , by the norm at a [[ threshold ]] of [[ 5 ]] .",0
1144,smoothing parameter,0.1,[],The [[ smoothing parameter ]] was set to [[ 0.1 ]] .,0
1145,GRNN - G3,Gated - RNN,[],8 . [[ GRNN - G3 ]] adopts a [[ Gated - RNN ]] to represent sentence and use a three - way structure to leverage contexts .,0
1146,GRNN - G3,sentence,[],8 . [[ GRNN - G3 ]] adopts a Gated - RNN to represent [[ sentence ]] and use a three - way structure to leverage contexts .,0
1147,GRNN - G3,three - way structure,[],8 . [[ GRNN - G3 ]] adopts a Gated - RNN to represent sentence and use a [[ three - way structure ]] to leverage contexts .,0
1148,GRNN - G3,contexts,[],8 . [[ GRNN - G3 ]] adopts a Gated - RNN to represent sentence and use a three - way structure to leverage [[ contexts ]] .,0
1149,Gated - RNN,sentence,[],8 . GRNN - G3 adopts a [[ Gated - RNN ]] to represent [[ sentence ]] and use a three - way structure to leverage contexts .,0
1150,Gated - RNN,three - way structure,[],8 . GRNN - G3 adopts a [[ Gated - RNN ]] to represent sentence and use a [[ three - way structure ]] to leverage contexts .,0
1151,Gated - RNN,contexts,[],8 . GRNN - G3 adopts a [[ Gated - RNN ]] to represent sentence and use a three - way structure to leverage [[ contexts ]] .,0
1152,sentence,three - way structure,[],8 . GRNN - G3 adopts a Gated - RNN to represent [[ sentence ]] and use a [[ three - way structure ]] to leverage contexts .,0
1153,sentence,contexts,[],8 . GRNN - G3 adopts a Gated - RNN to represent [[ sentence ]] and use a three - way structure to leverage [[ contexts ]] .,0
1154,three - way structure,contexts,[],8 . GRNN - G3 adopts a Gated - RNN to represent sentence and use a [[ three - way structure ]] to leverage [[ contexts ]] .,0
1155,size,training data,"[['training data', 'has', 'increases']]","When the [[ size ]] of [[ training data ]] increases , we can observe that the performance gap becomes more obvious .",0
1156,size,increases,"[['training data', 'has', 'increases']]","When the [[ size ]] of training data [[ increases ]] , we can observe that the performance gap becomes more obvious .",0
1157,size,performance gap,"[['training data', 'has', 'increases']]","When the [[ size ]] of training data increases , we can observe that the [[ performance gap ]] becomes more obvious .",0
1158,size,more obvious,"[['training data', 'has', 'increases']]","When the [[ size ]] of training data increases , we can observe that the performance gap becomes [[ more obvious ]] .",0
1159,training data,increases,"[['training data', 'has', 'increases']]","When the size of [[ training data ]] [[ increases ]] , we can observe that the performance gap becomes more obvious .",1
1160,training data,performance gap,"[['training data', 'has', 'increases']]","When the size of [[ training data ]] increases , we can observe that the [[ performance gap ]] becomes more obvious .",0
1161,training data,more obvious,"[['training data', 'has', 'increases']]","When the size of [[ training data ]] increases , we can observe that the performance gap becomes [[ more obvious ]] .",0
1162,increases,performance gap,"[['training data', 'has', 'increases']]","When the size of training data [[ increases ]] , we can observe that the [[ performance gap ]] becomes more obvious .",0
1163,increases,more obvious,"[['training data', 'has', 'increases']]","When the size of training data [[ increases ]] , we can observe that the performance gap becomes [[ more obvious ]] .",0
1164,performance gap,more obvious,"[['training data', 'has', 'increases']]","When the size of training data increases , we can observe that the [[ performance gap ]] becomes [[ more obvious ]] .",0
1165,SciTail dataset,our model,[],"On the [[ SciTail dataset ]] we compared [[ our model ]] also against non-sentence embedding - based models , as no results have been previously published which are based on independent sentence embeddings .",0
1166,SciTail dataset,non-sentence embedding - based models,[],"On the [[ SciTail dataset ]] we compared our model also against [[ non-sentence embedding - based models ]] , as no results have been previously published which are based on independent sentence embeddings .",0
1167,our model,non-sentence embedding - based models,[],"On the SciTail dataset we compared [[ our model ]] also against [[ non-sentence embedding - based models ]] , as no results have been previously published which are based on independent sentence embeddings .",0
1168,very large corpus,similar distribution,[],We train on a [[ very large corpus ]] picked to have a [[ similar distribution ]] as our task of interest .,0
1169,very large corpus,our task of interest,[],We train on a [[ very large corpus ]] picked to have a similar distribution as [[ our task of interest ]] .,0
1170,similar distribution,our task of interest,[],We train on a very large corpus picked to have a [[ similar distribution ]] as [[ our task of interest ]] .,0
1171,data enrichment method,WordNet,[],"On the one hand , we propose a [[ data enrichment method ]] , which uses [[ WordNet ]] to extract inter-word semantic connections as general knowledge from each given passage - question pair .",0
1172,data enrichment method,inter-word semantic connections,[],"On the one hand , we propose a [[ data enrichment method ]] , which uses WordNet to extract [[ inter-word semantic connections ]] as general knowledge from each given passage - question pair .",0
1173,data enrichment method,general knowledge,[],"On the one hand , we propose a [[ data enrichment method ]] , which uses WordNet to extract inter-word semantic connections as [[ general knowledge ]] from each given passage - question pair .",0
1174,data enrichment method,each given passage - question pair,[],"On the one hand , we propose a [[ data enrichment method ]] , which uses WordNet to extract inter-word semantic connections as general knowledge from [[ each given passage - question pair ]] .",0
1175,WordNet,inter-word semantic connections,[],"On the one hand , we propose a data enrichment method , which uses [[ WordNet ]] to extract [[ inter-word semantic connections ]] as general knowledge from each given passage - question pair .",0
1176,WordNet,general knowledge,[],"On the one hand , we propose a data enrichment method , which uses [[ WordNet ]] to extract inter-word semantic connections as [[ general knowledge ]] from each given passage - question pair .",0
1177,WordNet,each given passage - question pair,[],"On the one hand , we propose a data enrichment method , which uses [[ WordNet ]] to extract inter-word semantic connections as general knowledge from [[ each given passage - question pair ]] .",0
1178,inter-word semantic connections,general knowledge,[],"On the one hand , we propose a data enrichment method , which uses WordNet to extract [[ inter-word semantic connections ]] as [[ general knowledge ]] from each given passage - question pair .",0
1179,inter-word semantic connections,each given passage - question pair,[],"On the one hand , we propose a data enrichment method , which uses WordNet to extract [[ inter-word semantic connections ]] as general knowledge from [[ each given passage - question pair ]] .",0
1180,general knowledge,each given passage - question pair,[],"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as [[ general knowledge ]] from [[ each given passage - question pair ]] .",0
1181,cue detection,Sherlock dataset test data,"[['outperform', 'has', 'best system']]","For [[ cue detection ]] , on the [[ Sherlock dataset test data ]] , we see that we outperform the best system [ FBK Chowdhury ] by 0.6 F1 measure .",0
1182,cue detection,outperform,"[['outperform', 'has', 'best system']]","For [[ cue detection ]] , on the Sherlock dataset test data , we see that we [[ outperform ]] the best system [ FBK Chowdhury ] by 0.6 F1 measure .",0
1183,cue detection,best system,"[['outperform', 'has', 'best system']]","For [[ cue detection ]] , on the Sherlock dataset test data , we see that we outperform the [[ best system ]] [ FBK Chowdhury ] by 0.6 F1 measure .",0
1184,cue detection,0.6 F1 measure,"[['outperform', 'has', 'best system']]","For [[ cue detection ]] , on the Sherlock dataset test data , we see that we outperform the best system [ FBK Chowdhury ] by [[ 0.6 F1 measure ]] .",0
1185,Sherlock dataset test data,outperform,"[['outperform', 'has', 'best system']]","For cue detection , on the [[ Sherlock dataset test data ]] , we see that we [[ outperform ]] the best system [ FBK Chowdhury ] by 0.6 F1 measure .",0
1186,Sherlock dataset test data,best system,"[['outperform', 'has', 'best system']]","For cue detection , on the [[ Sherlock dataset test data ]] , we see that we outperform the [[ best system ]] [ FBK Chowdhury ] by 0.6 F1 measure .",0
1187,Sherlock dataset test data,0.6 F1 measure,"[['outperform', 'has', 'best system']]","For cue detection , on the [[ Sherlock dataset test data ]] , we see that we outperform the best system [ FBK Chowdhury ] by [[ 0.6 F1 measure ]] .",0
1188,outperform,best system,"[['outperform', 'has', 'best system']]","For cue detection , on the Sherlock dataset test data , we see that we [[ outperform ]] the [[ best system ]] [ FBK Chowdhury ] by 0.6 F1 measure .",1
1189,outperform,0.6 F1 measure,"[['outperform', 'has', 'best system']]","For cue detection , on the Sherlock dataset test data , we see that we [[ outperform ]] the best system [ FBK Chowdhury ] by [[ 0.6 F1 measure ]] .",0
1190,best system,0.6 F1 measure,"[['outperform', 'has', 'best system']]","For cue detection , on the Sherlock dataset test data , we see that we outperform the [[ best system ]] [ FBK Chowdhury ] by [[ 0.6 F1 measure ]] .",0
1191,TD - LSTM,two LSTM networks,[],[[ TD - LSTM ]] uses [[ two LSTM networks ]] to model the preceding and following contexts of the target to generate target - dependent representation for sentiment prediction .,0
1192,TD - LSTM,preceding and following contexts,[],[[ TD - LSTM ]] uses two LSTM networks to model the [[ preceding and following contexts ]] of the target to generate target - dependent representation for sentiment prediction .,0
1193,TD - LSTM,target,[],[[ TD - LSTM ]] uses two LSTM networks to model the preceding and following contexts of the [[ target ]] to generate target - dependent representation for sentiment prediction .,0
1194,TD - LSTM,target - dependent representation,[],[[ TD - LSTM ]] uses two LSTM networks to model the preceding and following contexts of the target to generate [[ target - dependent representation ]] for sentiment prediction .,0
1195,TD - LSTM,sentiment prediction,[],[[ TD - LSTM ]] uses two LSTM networks to model the preceding and following contexts of the target to generate target - dependent representation for [[ sentiment prediction ]] .,0
1196,two LSTM networks,preceding and following contexts,[],TD - LSTM uses [[ two LSTM networks ]] to model the [[ preceding and following contexts ]] of the target to generate target - dependent representation for sentiment prediction .,0
1197,two LSTM networks,target,[],TD - LSTM uses [[ two LSTM networks ]] to model the preceding and following contexts of the [[ target ]] to generate target - dependent representation for sentiment prediction .,0
1198,two LSTM networks,target - dependent representation,[],TD - LSTM uses [[ two LSTM networks ]] to model the preceding and following contexts of the target to generate [[ target - dependent representation ]] for sentiment prediction .,0
1199,two LSTM networks,sentiment prediction,[],TD - LSTM uses [[ two LSTM networks ]] to model the preceding and following contexts of the target to generate target - dependent representation for [[ sentiment prediction ]] .,0
1200,preceding and following contexts,target,[],TD - LSTM uses two LSTM networks to model the [[ preceding and following contexts ]] of the [[ target ]] to generate target - dependent representation for sentiment prediction .,0
1201,preceding and following contexts,target - dependent representation,[],TD - LSTM uses two LSTM networks to model the [[ preceding and following contexts ]] of the target to generate [[ target - dependent representation ]] for sentiment prediction .,0
1202,preceding and following contexts,sentiment prediction,[],TD - LSTM uses two LSTM networks to model the [[ preceding and following contexts ]] of the target to generate target - dependent representation for [[ sentiment prediction ]] .,0
1203,target,target - dependent representation,[],TD - LSTM uses two LSTM networks to model the preceding and following contexts of the [[ target ]] to generate [[ target - dependent representation ]] for sentiment prediction .,0
1204,target,sentiment prediction,[],TD - LSTM uses two LSTM networks to model the preceding and following contexts of the [[ target ]] to generate target - dependent representation for [[ sentiment prediction ]] .,0
1205,target - dependent representation,sentiment prediction,[],TD - LSTM uses two LSTM networks to model the preceding and following contexts of the target to generate [[ target - dependent representation ]] for [[ sentiment prediction ]] .,0
1206,Dropout rate,"{ 0.1 , 0.2 , 0.3 }",[],"[[ Dropout rate ]] is tuned amongst [[ { 0.1 , 0.2 , 0.3 } ]] and applied to all RNN and fully - connected layers .",0
1207,Dropout rate,all RNN and fully - connected layers,[],"[[ Dropout rate ]] is tuned amongst { 0.1 , 0.2 , 0.3 } and applied to [[ all RNN and fully - connected layers ]] .",0
1208,"{ 0.1 , 0.2 , 0.3 }",all RNN and fully - connected layers,[],"Dropout rate is tuned amongst [[ { 0.1 , 0.2 , 0.3 } ]] and applied to [[ all RNN and fully - connected layers ]] .",0
1209,shared - normalization objective,paragraphs,"[['paragraphs', 'has', 'probability']]","We then use a [[ shared - normalization objective ]] where [[ paragraphs ]] are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .",0
1210,shared - normalization objective,independently,"[['paragraphs', 'has', 'probability']]","We then use a [[ shared - normalization objective ]] where paragraphs are processed [[ independently ]] , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .",0
1211,shared - normalization objective,probability,"[['paragraphs', 'has', 'probability']]","We then use a [[ shared - normalization objective ]] where paragraphs are processed independently , but the [[ probability ]] of an answer candidate is marginalized over all paragraphs sampled from the same document .",0
1212,shared - normalization objective,answer candidate,"[['paragraphs', 'has', 'probability']]","We then use a [[ shared - normalization objective ]] where paragraphs are processed independently , but the probability of an [[ answer candidate ]] is marginalized over all paragraphs sampled from the same document .",0
1213,shared - normalization objective,marginalized,"[['paragraphs', 'has', 'probability']]","We then use a [[ shared - normalization objective ]] where paragraphs are processed independently , but the probability of an answer candidate is [[ marginalized ]] over all paragraphs sampled from the same document .",0
1214,shared - normalization objective,all paragraphs,"[['paragraphs', 'has', 'probability']]","We then use a [[ shared - normalization objective ]] where paragraphs are processed independently , but the probability of an answer candidate is marginalized over [[ all paragraphs ]] sampled from the same document .",0
1215,shared - normalization objective,same document,"[['paragraphs', 'has', 'probability']]","We then use a [[ shared - normalization objective ]] where paragraphs are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the [[ same document ]] .",0
1216,paragraphs,independently,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where [[ paragraphs ]] are processed [[ independently ]] , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .",0
1217,paragraphs,probability,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where [[ paragraphs ]] are processed independently , but the [[ probability ]] of an answer candidate is marginalized over all paragraphs sampled from the same document .",1
1218,paragraphs,answer candidate,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where [[ paragraphs ]] are processed independently , but the probability of an [[ answer candidate ]] is marginalized over all paragraphs sampled from the same document .",0
1219,paragraphs,marginalized,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where [[ paragraphs ]] are processed independently , but the probability of an answer candidate is [[ marginalized ]] over all paragraphs sampled from the same document .",0
1220,paragraphs,all paragraphs,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where [[ paragraphs ]] are processed independently , but the probability of an answer candidate is marginalized over [[ all paragraphs ]] sampled from the same document .",0
1221,paragraphs,same document,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where [[ paragraphs ]] are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the [[ same document ]] .",0
1222,independently,probability,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed [[ independently ]] , but the [[ probability ]] of an answer candidate is marginalized over all paragraphs sampled from the same document .",0
1223,independently,answer candidate,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed [[ independently ]] , but the probability of an [[ answer candidate ]] is marginalized over all paragraphs sampled from the same document .",0
1224,independently,marginalized,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed [[ independently ]] , but the probability of an answer candidate is [[ marginalized ]] over all paragraphs sampled from the same document .",0
1225,independently,all paragraphs,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed [[ independently ]] , but the probability of an answer candidate is marginalized over [[ all paragraphs ]] sampled from the same document .",0
1226,independently,same document,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed [[ independently ]] , but the probability of an answer candidate is marginalized over all paragraphs sampled from the [[ same document ]] .",0
1227,probability,answer candidate,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the [[ probability ]] of an [[ answer candidate ]] is marginalized over all paragraphs sampled from the same document .",0
1228,probability,marginalized,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the [[ probability ]] of an answer candidate is [[ marginalized ]] over all paragraphs sampled from the same document .",0
1229,probability,all paragraphs,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the [[ probability ]] of an answer candidate is marginalized over [[ all paragraphs ]] sampled from the same document .",0
1230,probability,same document,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the [[ probability ]] of an answer candidate is marginalized over all paragraphs sampled from the [[ same document ]] .",0
1231,answer candidate,marginalized,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an [[ answer candidate ]] is [[ marginalized ]] over all paragraphs sampled from the same document .",0
1232,answer candidate,all paragraphs,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an [[ answer candidate ]] is marginalized over [[ all paragraphs ]] sampled from the same document .",0
1233,answer candidate,same document,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an [[ answer candidate ]] is marginalized over all paragraphs sampled from the [[ same document ]] .",0
1234,marginalized,all paragraphs,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is [[ marginalized ]] over [[ all paragraphs ]] sampled from the same document .",0
1235,marginalized,same document,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is [[ marginalized ]] over all paragraphs sampled from the [[ same document ]] .",0
1236,all paragraphs,same document,"[['paragraphs', 'has', 'probability']]","We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is marginalized over [[ all paragraphs ]] sampled from the [[ same document ]] .",0
1237,most of the models,biggest improvement,"[['most of the models', 'has', 'biggest improvement']]",For [[ most of the models ]] the [[ biggest improvement ]] in performance is achieved when moving from 1 % of the training examples to 5 % .,1
1238,most of the models,performance,"[['most of the models', 'has', 'biggest improvement']]",For [[ most of the models ]] the biggest improvement in [[ performance ]] is achieved when moving from 1 % of the training examples to 5 % .,0
1239,most of the models,moving,"[['most of the models', 'has', 'biggest improvement']]",For [[ most of the models ]] the biggest improvement in performance is achieved when [[ moving ]] from 1 % of the training examples to 5 % .,0
1240,most of the models,1 %,"[['most of the models', 'has', 'biggest improvement']]",For [[ most of the models ]] the biggest improvement in performance is achieved when moving from [[ 1 % ]] of the training examples to 5 % .,0
1241,most of the models,training examples,"[['most of the models', 'has', 'biggest improvement']]",For [[ most of the models ]] the biggest improvement in performance is achieved when moving from 1 % of the [[ training examples ]] to 5 % .,0
1242,most of the models,5 %,"[['most of the models', 'has', 'biggest improvement']]",For [[ most of the models ]] the biggest improvement in performance is achieved when moving from 1 % of the training examples to [[ 5 % ]] .,0
1243,biggest improvement,performance,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the [[ biggest improvement ]] in [[ performance ]] is achieved when moving from 1 % of the training examples to 5 % .,0
1244,biggest improvement,moving,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the [[ biggest improvement ]] in performance is achieved when [[ moving ]] from 1 % of the training examples to 5 % .,0
1245,biggest improvement,1 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the [[ biggest improvement ]] in performance is achieved when moving from [[ 1 % ]] of the training examples to 5 % .,0
1246,biggest improvement,training examples,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the [[ biggest improvement ]] in performance is achieved when moving from 1 % of the [[ training examples ]] to 5 % .,0
1247,biggest improvement,5 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the [[ biggest improvement ]] in performance is achieved when moving from 1 % of the training examples to [[ 5 % ]] .,0
1248,performance,moving,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in [[ performance ]] is achieved when [[ moving ]] from 1 % of the training examples to 5 % .,0
1249,performance,1 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in [[ performance ]] is achieved when moving from [[ 1 % ]] of the training examples to 5 % .,0
1250,performance,training examples,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in [[ performance ]] is achieved when moving from 1 % of the [[ training examples ]] to 5 % .,0
1251,performance,5 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in [[ performance ]] is achieved when moving from 1 % of the training examples to [[ 5 % ]] .,0
1252,moving,1 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in performance is achieved when [[ moving ]] from [[ 1 % ]] of the training examples to 5 % .,0
1253,moving,training examples,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in performance is achieved when [[ moving ]] from 1 % of the [[ training examples ]] to 5 % .,0
1254,moving,5 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in performance is achieved when [[ moving ]] from 1 % of the training examples to [[ 5 % ]] .,0
1255,1 %,training examples,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in performance is achieved when moving from [[ 1 % ]] of the [[ training examples ]] to 5 % .,0
1256,1 %,5 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in performance is achieved when moving from [[ 1 % ]] of the training examples to [[ 5 % ]] .,0
1257,training examples,5 %,"[['most of the models', 'has', 'biggest improvement']]",For most of the models the biggest improvement in performance is achieved when moving from 1 % of the [[ training examples ]] to [[ 5 % ]] .,0
1258,output mel-spectrograms,audio samples using the pretrained WaveGlow,[],"In the inference process , the [[ output mel-spectrograms ]] of our FastSpeech model are transformed into [[ audio samples using the pretrained WaveGlow ]] [ 20 ] 5 .",0
1259,outperforms,state - of - the - art,"[['outperforms', 'has', 'state - of - the - art']]","Third , we [[ outperforms ]] the published [[ state - of - the - art ]] on both dataset .",1
1260,outperforms,both dataset,"[['outperforms', 'has', 'state - of - the - art']]","Third , we [[ outperforms ]] the published state - of - the - art on [[ both dataset ]] .",0
1261,state - of - the - art,both dataset,"[['outperforms', 'has', 'state - of - the - art']]","Third , we outperforms the published [[ state - of - the - art ]] on [[ both dataset ]] .",0
1262,Transformer,default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ),[],We choose [[ Transformer ]] as the student model and use the [[ default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ) ]] unless otherwise stated .,0
1263,important information,given aspect,"[['design', 'has', 'attentionbased LSTM']]","In order to capture [[ important information ]] in response to a [[ given aspect ]] , we design an attentionbased LSTM .",0
1264,important information,design,"[['design', 'has', 'attentionbased LSTM']]","In order to capture [[ important information ]] in response to a given aspect , we [[ design ]] an attentionbased LSTM .",0
1265,important information,attentionbased LSTM,"[['design', 'has', 'attentionbased LSTM']]","In order to capture [[ important information ]] in response to a given aspect , we design an [[ attentionbased LSTM ]] .",0
1266,given aspect,design,"[['design', 'has', 'attentionbased LSTM']]","In order to capture important information in response to a [[ given aspect ]] , we [[ design ]] an attentionbased LSTM .",0
1267,given aspect,attentionbased LSTM,"[['design', 'has', 'attentionbased LSTM']]","In order to capture important information in response to a [[ given aspect ]] , we design an [[ attentionbased LSTM ]] .",0
1268,design,attentionbased LSTM,"[['design', 'has', 'attentionbased LSTM']]","In order to capture important information in response to a given aspect , we [[ design ]] an [[ attentionbased LSTM ]] .",1
1269,neural network based framework,Theano,[],"Our [[ neural network based framework ]] is implemented using [[ Theano ]] ( Theano Development Team , 2016 ) .",0
1270,other systems,competed,"[['Kaggle challenge', 'has', 'our system']]","In comparison to [[ other systems ]] that [[ competed ]] in the Kaggle challenge , our system comes in in 7th place out of 170 competitors ( top 4 % ) .",0
1271,other systems,Kaggle challenge,"[['Kaggle challenge', 'has', 'our system']]","In comparison to [[ other systems ]] that competed in the [[ Kaggle challenge ]] , our system comes in in 7th place out of 170 competitors ( top 4 % ) .",0
1272,other systems,our system,"[['Kaggle challenge', 'has', 'our system']]","In comparison to [[ other systems ]] that competed in the Kaggle challenge , [[ our system ]] comes in in 7th place out of 170 competitors ( top 4 % ) .",0
1273,other systems,7th place,"[['Kaggle challenge', 'has', 'our system']]","In comparison to [[ other systems ]] that competed in the Kaggle challenge , our system comes in in [[ 7th place ]] out of 170 competitors ( top 4 % ) .",0
1274,other systems,170 competitors,"[['Kaggle challenge', 'has', 'our system']]","In comparison to [[ other systems ]] that competed in the Kaggle challenge , our system comes in in 7th place out of [[ 170 competitors ]] ( top 4 % ) .",0
1275,competed,Kaggle challenge,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that [[ competed ]] in the [[ Kaggle challenge ]] , our system comes in in 7th place out of 170 competitors ( top 4 % ) .",0
1276,competed,our system,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that [[ competed ]] in the Kaggle challenge , [[ our system ]] comes in in 7th place out of 170 competitors ( top 4 % ) .",0
1277,competed,7th place,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that [[ competed ]] in the Kaggle challenge , our system comes in in [[ 7th place ]] out of 170 competitors ( top 4 % ) .",0
1278,competed,170 competitors,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that [[ competed ]] in the Kaggle challenge , our system comes in in 7th place out of [[ 170 competitors ]] ( top 4 % ) .",0
1279,Kaggle challenge,our system,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that competed in the [[ Kaggle challenge ]] , [[ our system ]] comes in in 7th place out of 170 competitors ( top 4 % ) .",1
1280,Kaggle challenge,7th place,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that competed in the [[ Kaggle challenge ]] , our system comes in in [[ 7th place ]] out of 170 competitors ( top 4 % ) .",0
1281,Kaggle challenge,170 competitors,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that competed in the [[ Kaggle challenge ]] , our system comes in in 7th place out of [[ 170 competitors ]] ( top 4 % ) .",0
1282,our system,7th place,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that competed in the Kaggle challenge , [[ our system ]] comes in in [[ 7th place ]] out of 170 competitors ( top 4 % ) .",0
1283,our system,170 competitors,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that competed in the Kaggle challenge , [[ our system ]] comes in in 7th place out of [[ 170 competitors ]] ( top 4 % ) .",0
1284,7th place,170 competitors,"[['Kaggle challenge', 'has', 'our system']]","In comparison to other systems that competed in the Kaggle challenge , our system comes in in [[ 7th place ]] out of [[ 170 competitors ]] ( top 4 % ) .",0
1285,embeddings,two different levels,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate [[ embeddings ]] learned at [[ two different levels ]] to represent each word in the sentence : the character composition and holistic word - level embedding .,0
1286,embeddings,each word,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate [[ embeddings ]] learned at two different levels to represent [[ each word ]] in the sentence : the character composition and holistic word - level embedding .,0
1287,embeddings,sentence,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate [[ embeddings ]] learned at two different levels to represent each word in the [[ sentence ]] : the character composition and holistic word - level embedding .,0
1288,embeddings,character composition,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate [[ embeddings ]] learned at two different levels to represent each word in the sentence : the [[ character composition ]] and holistic word - level embedding .,0
1289,embeddings,holistic word - level embedding,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate [[ embeddings ]] learned at two different levels to represent each word in the sentence : the character composition and [[ holistic word - level embedding ]] .,0
1290,two different levels,each word,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at [[ two different levels ]] to represent [[ each word ]] in the sentence : the character composition and holistic word - level embedding .,0
1291,two different levels,sentence,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at [[ two different levels ]] to represent each word in the [[ sentence ]] : the character composition and holistic word - level embedding .,0
1292,two different levels,character composition,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at [[ two different levels ]] to represent each word in the sentence : the [[ character composition ]] and holistic word - level embedding .,0
1293,two different levels,holistic word - level embedding,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at [[ two different levels ]] to represent each word in the sentence : the character composition and [[ holistic word - level embedding ]] .,0
1294,each word,sentence,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at two different levels to represent [[ each word ]] in the [[ sentence ]] : the character composition and holistic word - level embedding .,0
1295,each word,character composition,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at two different levels to represent [[ each word ]] in the sentence : the [[ character composition ]] and holistic word - level embedding .,0
1296,each word,holistic word - level embedding,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at two different levels to represent [[ each word ]] in the sentence : the character composition and [[ holistic word - level embedding ]] .,0
1297,sentence,character composition,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at two different levels to represent each word in the [[ sentence ]] : the [[ character composition ]] and holistic word - level embedding .,0
1298,sentence,holistic word - level embedding,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at two different levels to represent each word in the [[ sentence ]] : the character composition and [[ holistic word - level embedding ]] .,0
1299,character composition,holistic word - level embedding,"[['two different levels', 'name', 'character composition'], ['two different levels', 'name', 'holistic word - level embedding']]",We concatenate embeddings learned at two different levels to represent each word in the sentence : the [[ character composition ]] and [[ holistic word - level embedding ]] .,0
1300,AP,pooling layer,[],"Specifically , [[ AP ]] enables the [[ pooling layer ]] to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .",0
1301,AP,current input pair,[],"Specifically , [[ AP ]] enables the pooling layer to be aware of the [[ current input pair ]] , in a way that information from the two input items can directly influence the computation of each other 's representations .",0
1302,AP,information,[],"Specifically , [[ AP ]] enables the pooling layer to be aware of the current input pair , in a way that [[ information ]] from the two input items can directly influence the computation of each other 's representations .",0
1303,AP,two input items,[],"Specifically , [[ AP ]] enables the pooling layer to be aware of the current input pair , in a way that information from the [[ two input items ]] can directly influence the computation of each other 's representations .",0
1304,AP,computation,[],"Specifically , [[ AP ]] enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the [[ computation ]] of each other 's representations .",0
1305,AP,each other 's representations,[],"Specifically , [[ AP ]] enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of [[ each other 's representations ]] .",0
1306,pooling layer,current input pair,[],"Specifically , AP enables the [[ pooling layer ]] to be aware of the [[ current input pair ]] , in a way that information from the two input items can directly influence the computation of each other 's representations .",0
1307,pooling layer,information,[],"Specifically , AP enables the [[ pooling layer ]] to be aware of the current input pair , in a way that [[ information ]] from the two input items can directly influence the computation of each other 's representations .",0
1308,pooling layer,two input items,[],"Specifically , AP enables the [[ pooling layer ]] to be aware of the current input pair , in a way that information from the [[ two input items ]] can directly influence the computation of each other 's representations .",0
1309,pooling layer,computation,[],"Specifically , AP enables the [[ pooling layer ]] to be aware of the current input pair , in a way that information from the two input items can directly influence the [[ computation ]] of each other 's representations .",0
1310,pooling layer,each other 's representations,[],"Specifically , AP enables the [[ pooling layer ]] to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of [[ each other 's representations ]] .",0
1311,current input pair,information,[],"Specifically , AP enables the pooling layer to be aware of the [[ current input pair ]] , in a way that [[ information ]] from the two input items can directly influence the computation of each other 's representations .",0
1312,current input pair,two input items,[],"Specifically , AP enables the pooling layer to be aware of the [[ current input pair ]] , in a way that information from the [[ two input items ]] can directly influence the computation of each other 's representations .",0
1313,current input pair,computation,[],"Specifically , AP enables the pooling layer to be aware of the [[ current input pair ]] , in a way that information from the two input items can directly influence the [[ computation ]] of each other 's representations .",0
1314,current input pair,each other 's representations,[],"Specifically , AP enables the pooling layer to be aware of the [[ current input pair ]] , in a way that information from the two input items can directly influence the computation of [[ each other 's representations ]] .",0
1315,information,two input items,[],"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that [[ information ]] from the [[ two input items ]] can directly influence the computation of each other 's representations .",0
1316,information,computation,[],"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that [[ information ]] from the two input items can directly influence the [[ computation ]] of each other 's representations .",0
1317,information,each other 's representations,[],"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that [[ information ]] from the two input items can directly influence the computation of [[ each other 's representations ]] .",0
1318,two input items,computation,[],"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the [[ two input items ]] can directly influence the [[ computation ]] of each other 's representations .",0
1319,two input items,each other 's representations,[],"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the [[ two input items ]] can directly influence the computation of [[ each other 's representations ]] .",0
1320,computation,each other 's representations,[],"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the [[ computation ]] of [[ each other 's representations ]] .",0
1321,mini-batch size,32,[],The [[ mini-batch size ]] is set to [[ 32 ]] and Adamax is used as our optimizer .,0
1322,mini-batch size,Adamax,[],The [[ mini-batch size ]] is set to 32 and [[ Adamax ]] is used as our optimizer .,0
1323,mini-batch size,our optimizer,[],The [[ mini-batch size ]] is set to 32 and Adamax is used as [[ our optimizer ]] .,0
1324,32,Adamax,[],The mini-batch size is set to [[ 32 ]] and [[ Adamax ]] is used as our optimizer .,0
1325,32,our optimizer,[],The mini-batch size is set to [[ 32 ]] and Adamax is used as [[ our optimizer ]] .,0
1326,Adamax,our optimizer,[],The mini-batch size is set to 32 and [[ Adamax ]] is used as [[ our optimizer ]] .,0
1327,"Adam optimizer ( Kingma and Ba , 2014 )","default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8",[],"We use [[ Adam optimizer ( Kingma and Ba , 2014 ) ]] with the [[ default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 ]] .",0
1328,LSTM,LSTM based recurrent model,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In [[ LSTM ]] , a [[ LSTM based recurrent model ]] is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .",1
1329,LSTM,start,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In [[ LSTM ]] , a LSTM based recurrent model is applied from the [[ start ]] to the end of a sentence , and the last hidden vector is used as the sentence representation .",0
1330,LSTM,end,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In [[ LSTM ]] , a LSTM based recurrent model is applied from the start to the [[ end ]] of a sentence , and the last hidden vector is used as the sentence representation .",0
1331,LSTM,sentence,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In [[ LSTM ]] , a LSTM based recurrent model is applied from the start to the end of a [[ sentence ]] , and the last hidden vector is used as the sentence representation .",0
1332,LSTM,last hidden vector,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In [[ LSTM ]] , a LSTM based recurrent model is applied from the start to the end of a sentence , and the [[ last hidden vector ]] is used as the sentence representation .",1
1333,LSTM,sentence representation,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In [[ LSTM ]] , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the [[ sentence representation ]] .",0
1334,LSTM based recurrent model,start,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a [[ LSTM based recurrent model ]] is applied from the [[ start ]] to the end of a sentence , and the last hidden vector is used as the sentence representation .",0
1335,LSTM based recurrent model,end,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a [[ LSTM based recurrent model ]] is applied from the start to the [[ end ]] of a sentence , and the last hidden vector is used as the sentence representation .",0
1336,LSTM based recurrent model,sentence,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a [[ LSTM based recurrent model ]] is applied from the start to the end of a [[ sentence ]] , and the last hidden vector is used as the sentence representation .",0
1337,LSTM based recurrent model,last hidden vector,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a [[ LSTM based recurrent model ]] is applied from the start to the end of a sentence , and the [[ last hidden vector ]] is used as the sentence representation .",0
1338,LSTM based recurrent model,sentence representation,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a [[ LSTM based recurrent model ]] is applied from the start to the end of a sentence , and the last hidden vector is used as the [[ sentence representation ]] .",0
1339,start,end,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the [[ start ]] to the [[ end ]] of a sentence , and the last hidden vector is used as the sentence representation .",0
1340,start,sentence,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the [[ start ]] to the end of a [[ sentence ]] , and the last hidden vector is used as the sentence representation .",0
1341,start,last hidden vector,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the [[ start ]] to the end of a sentence , and the [[ last hidden vector ]] is used as the sentence representation .",0
1342,start,sentence representation,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the [[ start ]] to the end of a sentence , and the last hidden vector is used as the [[ sentence representation ]] .",0
1343,end,sentence,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the start to the [[ end ]] of a [[ sentence ]] , and the last hidden vector is used as the sentence representation .",0
1344,end,last hidden vector,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the start to the [[ end ]] of a sentence , and the [[ last hidden vector ]] is used as the sentence representation .",0
1345,end,sentence representation,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the start to the [[ end ]] of a sentence , and the last hidden vector is used as the [[ sentence representation ]] .",0
1346,sentence,last hidden vector,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the start to the end of a [[ sentence ]] , and the [[ last hidden vector ]] is used as the sentence representation .",0
1347,sentence,sentence representation,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the start to the end of a [[ sentence ]] , and the last hidden vector is used as the [[ sentence representation ]] .",0
1348,last hidden vector,sentence representation,"[['LSTM', 'has', 'LSTM based recurrent model'], ['LSTM', 'has', 'last hidden vector']]","In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the [[ last hidden vector ]] is used as the [[ sentence representation ]] .",0
1349,NVDM,best performance,[],The experimental results indicate that [[ NVDM ]] achieves the [[ best performance ]] on both datasets .,0
1350,NVDM,both datasets,[],The experimental results indicate that [[ NVDM ]] achieves the best performance on [[ both datasets ]] .,0
1351,best performance,both datasets,[],The experimental results indicate that NVDM achieves the [[ best performance ]] on [[ both datasets ]] .,0
1352,inter-annotator agreement,satisfactory,[],The [[ inter-annotator agreement ]] is [[ satisfactory ]] considering the difficulty of human evaluation .,0
1353,SUBJ,Subjectivity of sentences,"[['SUBJ', 'has', 'Subjectivity of sentences']]",[[ SUBJ ]] : [[ Subjectivity of sentences ]] from movie reviews and plot summaries .,1
1354,SUBJ,movie reviews and plot summaries,"[['SUBJ', 'has', 'Subjectivity of sentences']]",[[ SUBJ ]] : Subjectivity of sentences from [[ movie reviews and plot summaries ]] .,0
1355,Subjectivity of sentences,movie reviews and plot summaries,"[['SUBJ', 'has', 'Subjectivity of sentences']]",SUBJ : [[ Subjectivity of sentences ]] from [[ movie reviews and plot summaries ]] .,0
1356,BGRU - WLA ( + STP ) + EWA,BGRU (+ STP ),[],"From and , we can obtain : ( 1 ) Regardless of the dataset that we employ , [[ BGRU - WLA ( + STP ) + EWA ]] outperforms [[ BGRU (+ STP ) ]] .",0
1357,Training,mini-batch stochastic gradient descent ( SGD ),[],[[ Training ]] is carried out by [[ mini-batch stochastic gradient descent ( SGD ) ]] with a momentum of 0.9 and a gradient clipping of 5.0 .,0
1358,Training,momentum of 0.9,[],[[ Training ]] is carried out by mini-batch stochastic gradient descent ( SGD ) with a [[ momentum of 0.9 ]] and a gradient clipping of 5.0 .,0
1359,Training,gradient clipping of 5.0,[],[[ Training ]] is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a [[ gradient clipping of 5.0 ]] .,0
1360,mini-batch stochastic gradient descent ( SGD ),momentum of 0.9,[],Training is carried out by [[ mini-batch stochastic gradient descent ( SGD ) ]] with a [[ momentum of 0.9 ]] and a gradient clipping of 5.0 .,0
1361,mini-batch stochastic gradient descent ( SGD ),gradient clipping of 5.0,[],Training is carried out by [[ mini-batch stochastic gradient descent ( SGD ) ]] with a momentum of 0.9 and a [[ gradient clipping of 5.0 ]] .,0
1362,momentum of 0.9,gradient clipping of 5.0,[],Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a [[ momentum of 0.9 ]] and a [[ gradient clipping of 5.0 ]] .,0
1363,10 % randomly held - out training data,target domain,[],The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the [[ target domain ]] in R1?L task and are fixed to be used in all transfer pairs .,0
1364,10 % randomly held - out training data,R1?L task,[],The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the target domain in [[ R1?L task ]] and are fixed to be used in all transfer pairs .,0
1365,10 % randomly held - out training data,transfer pairs,[],The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the target domain in R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
1366,target domain,R1?L task,[],The hyperparameters are tuned on 10 % randomly held - out training data of the [[ target domain ]] in [[ R1?L task ]] and are fixed to be used in all transfer pairs .,0
1367,target domain,transfer pairs,[],The hyperparameters are tuned on 10 % randomly held - out training data of the [[ target domain ]] in R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
1368,R1?L task,transfer pairs,[],The hyperparameters are tuned on 10 % randomly held - out training data of the target domain in [[ R1?L task ]] and are fixed to be used in all [[ transfer pairs ]] .,0
1369,C2 F,very sharp attention,[],"In [[ C2 F ]] , we see that we get [[ very sharp attention ]] on some rows as we had hoped .",0
1370,C2 F,some rows,[],"In [[ C2 F ]] , we see that we get very sharp attention on [[ some rows ]] as we had hoped .",0
1371,very sharp attention,some rows,[],"In C2 F , we see that we get [[ very sharp attention ]] on [[ some rows ]] as we had hoped .",0
1372,quadratic number of possible answers,14 % error reduction,[],"In contrast , RASOR can efficiently and explicitly model the [[ quadratic number of possible answers ]] , which leads to a [[ 14 % error reduction ]] over the best performing Match - LSTM model .",0
1373,quadratic number of possible answers,best performing Match - LSTM model,[],"In contrast , RASOR can efficiently and explicitly model the [[ quadratic number of possible answers ]] , which leads to a 14 % error reduction over the [[ best performing Match - LSTM model ]] .",0
1374,14 % error reduction,best performing Match - LSTM model,[],"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a [[ 14 % error reduction ]] over the [[ best performing Match - LSTM model ]] .",0
1375,model,state - of - the - art,"[['Flickr30 k', 'has', 'model']]","For fair comparison with To get a deeper understanding of our [[ model ]] , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the [[ state - of - the - art ]] method on Flickr30 k .",0
1376,model,Flickr30 k,"[['Flickr30 k', 'has', 'model']]","For fair comparison with To get a deeper understanding of our [[ model ]] , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the state - of - the - art method on [[ Flickr30 k ]] .",0
1377,state - of - the - art,Flickr30 k,"[['Flickr30 k', 'has', 'model']]","For fair comparison with To get a deeper understanding of our model , we first report in category - wise pointing game accuracy and attention correctness ( percentage of the heatmap falling into the ground truth bounding box ) and compare with the [[ state - of - the - art ]] method on [[ Flickr30 k ]] .",0
1378,pooling layer,sequential representations,[],A [[ pooling layer ]] aggregates [[ sequential representations ]] into vectors which are finally processed by a prediction layer to give the final prediction .,0
1379,pooling layer,vectors,[],A [[ pooling layer ]] aggregates sequential representations into [[ vectors ]] which are finally processed by a prediction layer to give the final prediction .,0
1380,pooling layer,prediction layer,[],A [[ pooling layer ]] aggregates sequential representations into vectors which are finally processed by a [[ prediction layer ]] to give the final prediction .,0
1381,pooling layer,final prediction,[],A [[ pooling layer ]] aggregates sequential representations into vectors which are finally processed by a prediction layer to give the [[ final prediction ]] .,0
1382,sequential representations,vectors,[],A pooling layer aggregates [[ sequential representations ]] into [[ vectors ]] which are finally processed by a prediction layer to give the final prediction .,0
1383,sequential representations,prediction layer,[],A pooling layer aggregates [[ sequential representations ]] into vectors which are finally processed by a [[ prediction layer ]] to give the final prediction .,0
1384,sequential representations,final prediction,[],A pooling layer aggregates [[ sequential representations ]] into vectors which are finally processed by a prediction layer to give the [[ final prediction ]] .,0
1385,vectors,prediction layer,[],A pooling layer aggregates sequential representations into [[ vectors ]] which are finally processed by a [[ prediction layer ]] to give the final prediction .,0
1386,vectors,final prediction,[],A pooling layer aggregates sequential representations into [[ vectors ]] which are finally processed by a prediction layer to give the [[ final prediction ]] .,0
1387,prediction layer,final prediction,[],A pooling layer aggregates sequential representations into vectors which are finally processed by a [[ prediction layer ]] to give the [[ final prediction ]] .,0
1388,vocabulary size,dataset,[],"The [[ vocabulary size ]] of the [[ dataset ]] is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1389,vocabulary size,"3,747",[],"The [[ vocabulary size ]] of the dataset is [[ 3,747 ]] , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1390,vocabulary size,""" UNK "" token",[],"The [[ vocabulary size ]] of the dataset is 3,747 , including the [[ "" UNK "" token ]] , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1391,vocabulary size,unknown words,[],"The [[ vocabulary size ]] of the dataset is 3,747 , including the "" UNK "" token , which represents [[ unknown words ]] , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1392,vocabulary size,""" PAD "" token",[],"The [[ vocabulary size ]] of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the [[ "" PAD "" token ]] , which is used to indicate padding information added while preparing mini-batch data .",0
1393,vocabulary size,padding information,[],"The [[ vocabulary size ]] of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate [[ padding information ]] added while preparing mini-batch data .",0
1394,vocabulary size,mini-batch data,[],"The [[ vocabulary size ]] of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing [[ mini-batch data ]] .",0
1395,dataset,"3,747",[],"The vocabulary size of the [[ dataset ]] is [[ 3,747 ]] , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1396,dataset,""" UNK "" token",[],"The vocabulary size of the [[ dataset ]] is 3,747 , including the [[ "" UNK "" token ]] , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1397,dataset,unknown words,[],"The vocabulary size of the [[ dataset ]] is 3,747 , including the "" UNK "" token , which represents [[ unknown words ]] , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1398,dataset,""" PAD "" token",[],"The vocabulary size of the [[ dataset ]] is 3,747 , including the "" UNK "" token , which represents unknown words , and the [[ "" PAD "" token ]] , which is used to indicate padding information added while preparing mini-batch data .",0
1399,dataset,padding information,[],"The vocabulary size of the [[ dataset ]] is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate [[ padding information ]] added while preparing mini-batch data .",0
1400,dataset,mini-batch data,[],"The vocabulary size of the [[ dataset ]] is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing [[ mini-batch data ]] .",0
1401,"3,747",""" UNK "" token",[],"The vocabulary size of the dataset is [[ 3,747 ]] , including the [[ "" UNK "" token ]] , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1402,"3,747",unknown words,[],"The vocabulary size of the dataset is [[ 3,747 ]] , including the "" UNK "" token , which represents [[ unknown words ]] , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1403,"3,747",""" PAD "" token",[],"The vocabulary size of the dataset is [[ 3,747 ]] , including the "" UNK "" token , which represents unknown words , and the [[ "" PAD "" token ]] , which is used to indicate padding information added while preparing mini-batch data .",0
1404,"3,747",padding information,[],"The vocabulary size of the dataset is [[ 3,747 ]] , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate [[ padding information ]] added while preparing mini-batch data .",0
1405,"3,747",mini-batch data,[],"The vocabulary size of the dataset is [[ 3,747 ]] , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing [[ mini-batch data ]] .",0
1406,""" UNK "" token",unknown words,[],"The vocabulary size of the dataset is 3,747 , including the [[ "" UNK "" token ]] , which represents [[ unknown words ]] , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",0
1407,""" UNK "" token",""" PAD "" token",[],"The vocabulary size of the dataset is 3,747 , including the [[ "" UNK "" token ]] , which represents unknown words , and the [[ "" PAD "" token ]] , which is used to indicate padding information added while preparing mini-batch data .",0
1408,""" UNK "" token",padding information,[],"The vocabulary size of the dataset is 3,747 , including the [[ "" UNK "" token ]] , which represents unknown words , and the "" PAD "" token , which is used to indicate [[ padding information ]] added while preparing mini-batch data .",0
1409,""" UNK "" token",mini-batch data,[],"The vocabulary size of the dataset is 3,747 , including the [[ "" UNK "" token ]] , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing [[ mini-batch data ]] .",0
1410,unknown words,""" PAD "" token",[],"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents [[ unknown words ]] , and the [[ "" PAD "" token ]] , which is used to indicate padding information added while preparing mini-batch data .",0
1411,unknown words,padding information,[],"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents [[ unknown words ]] , and the "" PAD "" token , which is used to indicate [[ padding information ]] added while preparing mini-batch data .",0
1412,unknown words,mini-batch data,[],"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents [[ unknown words ]] , and the "" PAD "" token , which is used to indicate padding information added while preparing [[ mini-batch data ]] .",0
1413,""" PAD "" token",padding information,[],"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the [[ "" PAD "" token ]] , which is used to indicate [[ padding information ]] added while preparing mini-batch data .",0
1414,""" PAD "" token",mini-batch data,[],"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the [[ "" PAD "" token ]] , which is used to indicate padding information added while preparing [[ mini-batch data ]] .",0
1415,padding information,mini-batch data,[],"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate [[ padding information ]] added while preparing [[ mini-batch data ]] .",0
1416,sequence weighted attention,better results,[],"However , the [[ sequence weighted attention ]] gives [[ better results ]] by about 1 point of the F1-score .",0
1417,sequence weighted attention,about 1 point,[],"However , the [[ sequence weighted attention ]] gives better results by [[ about 1 point ]] of the F1-score .",0
1418,sequence weighted attention,F1-score,[],"However , the [[ sequence weighted attention ]] gives better results by about 1 point of the [[ F1-score ]] .",0
1419,better results,about 1 point,[],"However , the sequence weighted attention gives [[ better results ]] by [[ about 1 point ]] of the F1-score .",0
1420,better results,F1-score,[],"However , the sequence weighted attention gives [[ better results ]] by about 1 point of the [[ F1-score ]] .",0
1421,about 1 point,F1-score,[],"However , the sequence weighted attention gives better results by [[ about 1 point ]] of the [[ F1-score ]] .",0
1422,label smoothing technique,smoothing value,[],We also employ [[ label smoothing technique ]] with a [[ smoothing value ]] of 0.1 during training .,0
1423,label smoothing technique,0.1,[],We also employ [[ label smoothing technique ]] with a smoothing value of [[ 0.1 ]] during training .,0
1424,label smoothing technique,training,[],We also employ [[ label smoothing technique ]] with a smoothing value of 0.1 during [[ training ]] .,0
1425,smoothing value,0.1,[],We also employ label smoothing technique with a [[ smoothing value ]] of [[ 0.1 ]] during training .,0
1426,smoothing value,training,[],We also employ label smoothing technique with a [[ smoothing value ]] of 0.1 during [[ training ]] .,0
1427,0.1,training,[],We also employ label smoothing technique with a smoothing value of [[ 0.1 ]] during [[ training ]] .,0
1428,our GCN models,complementary strengths,[],"As we will show in Section 6.2 , we find that [[ our GCN models ]] have [[ complementary strengths ]] when compared to the PA - LSTM .",0
1429,our GCN models,PA - LSTM,[],"As we will show in Section 6.2 , we find that [[ our GCN models ]] have complementary strengths when compared to the [[ PA - LSTM ]] .",0
1430,complementary strengths,PA - LSTM,[],"As we will show in Section 6.2 , we find that our GCN models have [[ complementary strengths ]] when compared to the [[ PA - LSTM ]] .",0
1431,efficient Bidirectional Attention Connectors ( BAC ),base building block,[],"To this end , we propose [[ efficient Bidirectional Attention Connectors ( BAC ) ]] as a [[ base building block ]] to connect two sequences at arbitrary layers .",0
1432,efficient Bidirectional Attention Connectors ( BAC ),two sequences,[],"To this end , we propose [[ efficient Bidirectional Attention Connectors ( BAC ) ]] as a base building block to connect [[ two sequences ]] at arbitrary layers .",0
1433,efficient Bidirectional Attention Connectors ( BAC ),arbitrary layers,[],"To this end , we propose [[ efficient Bidirectional Attention Connectors ( BAC ) ]] as a base building block to connect two sequences at [[ arbitrary layers ]] .",0
1434,base building block,two sequences,[],"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a [[ base building block ]] to connect [[ two sequences ]] at arbitrary layers .",0
1435,base building block,arbitrary layers,[],"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a [[ base building block ]] to connect two sequences at [[ arbitrary layers ]] .",0
1436,two sequences,arbitrary layers,[],"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect [[ two sequences ]] at [[ arbitrary layers ]] .",0
1437,ARE model,baseline performance,[],"First , our [[ ARE model ]] shows the [[ baseline performance ]] because we use minimal audio features , such as the MFCC and prosodic features with simple architectures .",0
1438,ARE model,minimal audio features,[],"First , our [[ ARE model ]] shows the baseline performance because we use [[ minimal audio features ]] , such as the MFCC and prosodic features with simple architectures .",0
1439,ARE model,MFCC and prosodic features,[],"First , our [[ ARE model ]] shows the baseline performance because we use minimal audio features , such as the [[ MFCC and prosodic features ]] with simple architectures .",0
1440,ARE model,simple architectures,[],"First , our [[ ARE model ]] shows the baseline performance because we use minimal audio features , such as the MFCC and prosodic features with [[ simple architectures ]] .",0
1441,baseline performance,minimal audio features,[],"First , our ARE model shows the [[ baseline performance ]] because we use [[ minimal audio features ]] , such as the MFCC and prosodic features with simple architectures .",0
1442,baseline performance,MFCC and prosodic features,[],"First , our ARE model shows the [[ baseline performance ]] because we use minimal audio features , such as the [[ MFCC and prosodic features ]] with simple architectures .",0
1443,baseline performance,simple architectures,[],"First , our ARE model shows the [[ baseline performance ]] because we use minimal audio features , such as the MFCC and prosodic features with [[ simple architectures ]] .",0
1444,minimal audio features,MFCC and prosodic features,[],"First , our ARE model shows the baseline performance because we use [[ minimal audio features ]] , such as the [[ MFCC and prosodic features ]] with simple architectures .",0
1445,minimal audio features,simple architectures,[],"First , our ARE model shows the baseline performance because we use [[ minimal audio features ]] , such as the MFCC and prosodic features with [[ simple architectures ]] .",0
1446,MFCC and prosodic features,simple architectures,[],"First , our ARE model shows the baseline performance because we use minimal audio features , such as the [[ MFCC and prosodic features ]] with [[ simple architectures ]] .",0
1447,vanilla beam search,total likelihood of the generated candidate,[],We use [[ vanilla beam search ]] according to the [[ total likelihood of the generated candidate ]] and accept only candidates which end in a end -of - sentence token .,0
1448,vanilla beam search,candidates which end in a end -of - sentence token,[],We use [[ vanilla beam search ]] according to the total likelihood of the generated candidate and accept only [[ candidates which end in a end -of - sentence token ]] .,0
1449,total likelihood of the generated candidate,candidates which end in a end -of - sentence token,[],We use vanilla beam search according to the [[ total likelihood of the generated candidate ]] and accept only [[ candidates which end in a end -of - sentence token ]] .,0
1450,novel Attention Guided Graph Convolutional Networks ( AGGCNs ),full tree,[],"In this paper , we propose the [[ novel Attention Guided Graph Convolutional Networks ( AGGCNs ) ]] , which operate directly on the [[ full tree ]] .",0
1451,dropout ratio,0.2,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 [[ dropout ratio ]] ( [[ 0.2 ]] ) and learning rate ( 0.05 - vanilla , 0.025 - intra-attention ) .",1
1452,dropout ratio,learning rate,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 [[ dropout ratio ]] ( 0.2 ) and [[ learning rate ]] ( 0.05 - vanilla , 0.025 - intra-attention ) .",0
1453,dropout ratio,0.05,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 [[ dropout ratio ]] ( 0.2 ) and learning rate ( [[ 0.05 ]] - vanilla , 0.025 - intra-attention ) .",0
1454,dropout ratio,vanilla,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 [[ dropout ratio ]] ( 0.2 ) and learning rate ( 0.05 - [[ vanilla ]] , 0.025 - intra-attention ) .",0
1455,dropout ratio,0.025,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 [[ dropout ratio ]] ( 0.2 ) and learning rate ( 0.05 - vanilla , [[ 0.025 ]] - intra-attention ) .",0
1456,dropout ratio,intra-attention,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 [[ dropout ratio ]] ( 0.2 ) and learning rate ( 0.05 - vanilla , 0.025 - [[ intra-attention ]] ) .",0
1457,0.2,learning rate,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( [[ 0.2 ]] ) and [[ learning rate ]] ( 0.05 - vanilla , 0.025 - intra-attention ) .",0
1458,0.2,0.05,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( [[ 0.2 ]] ) and learning rate ( [[ 0.05 ]] - vanilla , 0.025 - intra-attention ) .",0
1459,0.2,vanilla,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( [[ 0.2 ]] ) and learning rate ( 0.05 - [[ vanilla ]] , 0.025 - intra-attention ) .",0
1460,0.2,0.025,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( [[ 0.2 ]] ) and learning rate ( 0.05 - vanilla , [[ 0.025 ]] - intra-attention ) .",0
1461,0.2,intra-attention,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( [[ 0.2 ]] ) and learning rate ( 0.05 - vanilla , 0.025 - [[ intra-attention ]] ) .",0
1462,learning rate,0.05,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and [[ learning rate ]] ( [[ 0.05 ]] - vanilla , 0.025 - intra-attention ) .",0
1463,learning rate,vanilla,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and [[ learning rate ]] ( 0.05 - [[ vanilla ]] , 0.025 - intra-attention ) .",1
1464,learning rate,0.025,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and [[ learning rate ]] ( 0.05 - vanilla , [[ 0.025 ]] - intra-attention ) .",0
1465,learning rate,intra-attention,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and [[ learning rate ]] ( 0.05 - vanilla , 0.025 - [[ intra-attention ]] ) .",1
1466,0.05,vanilla,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( [[ 0.05 ]] - [[ vanilla ]] , 0.025 - intra-attention ) .",0
1467,0.05,0.025,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( [[ 0.05 ]] - vanilla , [[ 0.025 ]] - intra-attention ) .",0
1468,0.05,intra-attention,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( [[ 0.05 ]] - vanilla , 0.025 - [[ intra-attention ]] ) .",0
1469,vanilla,0.025,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( 0.05 - [[ vanilla ]] , [[ 0.025 ]] - intra-attention ) .",0
1470,vanilla,intra-attention,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( 0.05 - [[ vanilla ]] , 0.025 - [[ intra-attention ]] ) .",0
1471,0.025,intra-attention,"[['dropout ratio', 'has', '0.2'], ['learning rate', 'has', 'intra-attention'], ['intra-attention', 'has', '0.025'], ['learning rate', 'has', 'vanilla'], ['vanilla', 'has', '0.05']]","We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( 0.05 - vanilla , [[ 0.025 ]] - [[ intra-attention ]] ) .",0
1472,fast and effective neural network,ACSA and ATSA,[],"In this paper , we propose a [[ fast and effective neural network ]] for [[ ACSA and ATSA ]] based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .",0
1473,fast and effective neural network,convolutions and gating mechanisms,[],"In this paper , we propose a [[ fast and effective neural network ]] for ACSA and ATSA based on [[ convolutions and gating mechanisms ]] , which has much less training time than LSTM based networks , but with better accuracy .",0
1474,ACSA and ATSA,convolutions and gating mechanisms,[],"In this paper , we propose a fast and effective neural network for [[ ACSA and ATSA ]] based on [[ convolutions and gating mechanisms ]] , which has much less training time than LSTM based networks , but with better accuracy .",0
1475,our model,robust,[],"This finding indicates that [[ our model ]] is [[ robust ]] across datasets with varying training sizes , context lengths and domains .",0
1476,our model,datasets,[],"This finding indicates that [[ our model ]] is robust across [[ datasets ]] with varying training sizes , context lengths and domains .",0
1477,our model,"varying training sizes , context lengths and domains",[],"This finding indicates that [[ our model ]] is robust across datasets with [[ varying training sizes , context lengths and domains ]] .",0
1478,robust,datasets,[],"This finding indicates that our model is [[ robust ]] across [[ datasets ]] with varying training sizes , context lengths and domains .",0
1479,robust,"varying training sizes , context lengths and domains",[],"This finding indicates that our model is [[ robust ]] across datasets with [[ varying training sizes , context lengths and domains ]] .",0
1480,datasets,"varying training sizes , context lengths and domains",[],"This finding indicates that our model is robust across [[ datasets ]] with [[ varying training sizes , context lengths and domains ]] .",0
1481,BLEU and METEOR,best results,"[['BLEU and METEOR', 'has', 'best results']]","For the [[ BLEU and METEOR ]] , our [[ best results ]] are 4.7 % and 4 % absolute point improvement over the state - of - the - art .",1
1482,BLEU and METEOR,4.7 % and 4 % absolute point improvement,"[['BLEU and METEOR', 'has', 'best results']]","For the [[ BLEU and METEOR ]] , our best results are [[ 4.7 % and 4 % absolute point improvement ]] over the state - of - the - art .",0
1483,BLEU and METEOR,state - of - the - art,"[['BLEU and METEOR', 'has', 'best results']]","For the [[ BLEU and METEOR ]] , our best results are 4.7 % and 4 % absolute point improvement over the [[ state - of - the - art ]] .",0
1484,best results,4.7 % and 4 % absolute point improvement,"[['BLEU and METEOR', 'has', 'best results']]","For the BLEU and METEOR , our [[ best results ]] are [[ 4.7 % and 4 % absolute point improvement ]] over the state - of - the - art .",0
1485,best results,state - of - the - art,"[['BLEU and METEOR', 'has', 'best results']]","For the BLEU and METEOR , our [[ best results ]] are 4.7 % and 4 % absolute point improvement over the [[ state - of - the - art ]] .",0
1486,4.7 % and 4 % absolute point improvement,state - of - the - art,"[['BLEU and METEOR', 'has', 'best results']]","For the BLEU and METEOR , our best results are [[ 4.7 % and 4 % absolute point improvement ]] over the [[ state - of - the - art ]] .",0
1487,Glo Ve vectors,initialization,[],[[ Glo Ve vectors ]] are used as the [[ initialization ]] for word embeddings .,0
1488,Glo Ve vectors,word embeddings,[],[[ Glo Ve vectors ]] are used as the initialization for [[ word embeddings ]] .,0
1489,initialization,word embeddings,[],Glo Ve vectors are used as the [[ initialization ]] for [[ word embeddings ]] .,0
1490,MRU - LSTM,significantly outperforms,"[['MRU - LSTM', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all models']]","Finally , the [[ MRU - LSTM ]] [[ significantly outperforms ]] all models , including BiDAF on this dataset .",1
1491,MRU - LSTM,all models,"[['MRU - LSTM', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all models']]","Finally , the [[ MRU - LSTM ]] significantly outperforms [[ all models ]] , including BiDAF on this dataset .",0
1492,MRU - LSTM,BiDAF,"[['MRU - LSTM', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all models']]","Finally , the [[ MRU - LSTM ]] significantly outperforms all models , including [[ BiDAF ]] on this dataset .",0
1493,significantly outperforms,all models,"[['MRU - LSTM', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all models']]","Finally , the MRU - LSTM [[ significantly outperforms ]] [[ all models ]] , including BiDAF on this dataset .",1
1494,significantly outperforms,BiDAF,"[['MRU - LSTM', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all models']]","Finally , the MRU - LSTM [[ significantly outperforms ]] all models , including [[ BiDAF ]] on this dataset .",0
1495,all models,BiDAF,"[['MRU - LSTM', 'has', 'significantly outperforms'], ['significantly outperforms', 'has', 'all models']]","Finally , the MRU - LSTM significantly outperforms [[ all models ]] , including [[ BiDAF ]] on this dataset .",0
1496,both kinds of attention supervision information,MN ( + AS ),"[['both kinds of attention supervision information', 'has', 'MN ( + AS )']]","Finally , when we use [[ both kinds of attention supervision information ]] , no matter for which metric , [[ MN ( + AS ) ]] remarkably outperforms MN on all test sets .",1
1497,both kinds of attention supervision information,MN,"[['both kinds of attention supervision information', 'has', 'MN ( + AS )']]","Finally , when we use [[ both kinds of attention supervision information ]] , no matter for which metric , MN ( + AS ) remarkably outperforms [[ MN ]] on all test sets .",0
1498,both kinds of attention supervision information,all test sets,"[['both kinds of attention supervision information', 'has', 'MN ( + AS )']]","Finally , when we use [[ both kinds of attention supervision information ]] , no matter for which metric , MN ( + AS ) remarkably outperforms MN on [[ all test sets ]] .",0
1499,MN ( + AS ),MN,"[['both kinds of attention supervision information', 'has', 'MN ( + AS )']]","Finally , when we use both kinds of attention supervision information , no matter for which metric , [[ MN ( + AS ) ]] remarkably outperforms [[ MN ]] on all test sets .",0
1500,MN ( + AS ),all test sets,"[['both kinds of attention supervision information', 'has', 'MN ( + AS )']]","Finally , when we use both kinds of attention supervision information , no matter for which metric , [[ MN ( + AS ) ]] remarkably outperforms MN on [[ all test sets ]] .",0
1501,MN,all test sets,"[['both kinds of attention supervision information', 'has', 'MN ( + AS )']]","Finally , when we use both kinds of attention supervision information , no matter for which metric , MN ( + AS ) remarkably outperforms [[ MN ]] on [[ all test sets ]] .",0
1502,SVM,"surface features , lexicon features and parsing features",[],"[[ SVM ]] : The traditional state - of - the - art method using SVMs on [[ surface features , lexicon features and parsing features ]] , which is the best team in SemEval 2014 .",0
1503,Orthonormal parameter initialization,surprisingly important,[],"[[ Orthonormal parameter initialization ]] is [[ surprisingly important ]] - without this , the model achieves only 65 F1 within the first 50 epochs .",0
1504,Orthonormal parameter initialization,model,[],"[[ Orthonormal parameter initialization ]] is surprisingly important - without this , the [[ model ]] achieves only 65 F1 within the first 50 epochs .",0
1505,Orthonormal parameter initialization,only 65 F1,[],"[[ Orthonormal parameter initialization ]] is surprisingly important - without this , the model achieves [[ only 65 F1 ]] within the first 50 epochs .",0
1506,Orthonormal parameter initialization,first 50 epochs,[],"[[ Orthonormal parameter initialization ]] is surprisingly important - without this , the model achieves only 65 F1 within the [[ first 50 epochs ]] .",0
1507,surprisingly important,model,[],"Orthonormal parameter initialization is [[ surprisingly important ]] - without this , the [[ model ]] achieves only 65 F1 within the first 50 epochs .",0
1508,surprisingly important,only 65 F1,[],"Orthonormal parameter initialization is [[ surprisingly important ]] - without this , the model achieves [[ only 65 F1 ]] within the first 50 epochs .",0
1509,surprisingly important,first 50 epochs,[],"Orthonormal parameter initialization is [[ surprisingly important ]] - without this , the model achieves only 65 F1 within the [[ first 50 epochs ]] .",0
1510,model,only 65 F1,[],"Orthonormal parameter initialization is surprisingly important - without this , the [[ model ]] achieves [[ only 65 F1 ]] within the first 50 epochs .",0
1511,model,first 50 epochs,[],"Orthonormal parameter initialization is surprisingly important - without this , the [[ model ]] achieves only 65 F1 within the [[ first 50 epochs ]] .",0
1512,only 65 F1,first 50 epochs,[],"Orthonormal parameter initialization is surprisingly important - without this , the model achieves [[ only 65 F1 ]] within the [[ first 50 epochs ]] .",0
1513,TensorFlow 3,SQUAD training set,[],All models are implemented using [[ TensorFlow 3 ]] and trained on the [[ SQUAD training set ]] using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,0
1514,TensorFlow 3,ADAM optimizer,[],All models are implemented using [[ TensorFlow 3 ]] and trained on the SQUAD training set using the [[ ADAM optimizer ]] with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,0
1515,TensorFlow 3,mini-batch size,[],All models are implemented using [[ TensorFlow 3 ]] and trained on the SQUAD training set using the ADAM optimizer with a [[ mini-batch size ]] of 4 and trained using 10 asynchronous training threads on a single machine .,0
1516,TensorFlow 3,4,[],All models are implemented using [[ TensorFlow 3 ]] and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of [[ 4 ]] and trained using 10 asynchronous training threads on a single machine .,0
1517,TensorFlow 3,10 asynchronous training threads,[],All models are implemented using [[ TensorFlow 3 ]] and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using [[ 10 asynchronous training threads ]] on a single machine .,0
1518,TensorFlow 3,single machine,[],All models are implemented using [[ TensorFlow 3 ]] and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a [[ single machine ]] .,0
1519,SQUAD training set,ADAM optimizer,[],All models are implemented using TensorFlow 3 and trained on the [[ SQUAD training set ]] using the [[ ADAM optimizer ]] with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,0
1520,SQUAD training set,mini-batch size,[],All models are implemented using TensorFlow 3 and trained on the [[ SQUAD training set ]] using the ADAM optimizer with a [[ mini-batch size ]] of 4 and trained using 10 asynchronous training threads on a single machine .,0
1521,SQUAD training set,4,[],All models are implemented using TensorFlow 3 and trained on the [[ SQUAD training set ]] using the ADAM optimizer with a mini-batch size of [[ 4 ]] and trained using 10 asynchronous training threads on a single machine .,0
1522,SQUAD training set,10 asynchronous training threads,[],All models are implemented using TensorFlow 3 and trained on the [[ SQUAD training set ]] using the ADAM optimizer with a mini-batch size of 4 and trained using [[ 10 asynchronous training threads ]] on a single machine .,0
1523,SQUAD training set,single machine,[],All models are implemented using TensorFlow 3 and trained on the [[ SQUAD training set ]] using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a [[ single machine ]] .,0
1524,ADAM optimizer,mini-batch size,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the [[ ADAM optimizer ]] with a [[ mini-batch size ]] of 4 and trained using 10 asynchronous training threads on a single machine .,0
1525,ADAM optimizer,4,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the [[ ADAM optimizer ]] with a mini-batch size of [[ 4 ]] and trained using 10 asynchronous training threads on a single machine .,0
1526,ADAM optimizer,10 asynchronous training threads,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the [[ ADAM optimizer ]] with a mini-batch size of 4 and trained using [[ 10 asynchronous training threads ]] on a single machine .,0
1527,ADAM optimizer,single machine,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the [[ ADAM optimizer ]] with a mini-batch size of 4 and trained using 10 asynchronous training threads on a [[ single machine ]] .,0
1528,mini-batch size,4,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a [[ mini-batch size ]] of [[ 4 ]] and trained using 10 asynchronous training threads on a single machine .,0
1529,mini-batch size,10 asynchronous training threads,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a [[ mini-batch size ]] of 4 and trained using [[ 10 asynchronous training threads ]] on a single machine .,0
1530,mini-batch size,single machine,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a [[ mini-batch size ]] of 4 and trained using 10 asynchronous training threads on a [[ single machine ]] .,0
1531,4,10 asynchronous training threads,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of [[ 4 ]] and trained using [[ 10 asynchronous training threads ]] on a single machine .,0
1532,4,single machine,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of [[ 4 ]] and trained using 10 asynchronous training threads on a [[ single machine ]] .,0
1533,10 asynchronous training threads,single machine,[],All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using [[ 10 asynchronous training threads ]] on a [[ single machine ]] .,0
1534,learning rate,1.0,[],The [[ learning rate ]] is initialized to [[ 1.0 ]] .,0
1535,upper-bound frequency,each target vocabulary,[],The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,0
1536,upper-bound frequency,summary,[],The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,0
1537,upper-bound frequency,encoding process,[],The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
1538,upper-bound frequency,output words,[],The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
1539,upper-bound frequency,each decoding step,[],The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
1540,each target vocabulary,summary,[],The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,0
1541,each target vocabulary,encoding process,[],The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
1542,each target vocabulary,output words,[],The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
1543,each target vocabulary,each decoding step,[],The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
1544,summary,encoding process,[],The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
1545,summary,output words,[],The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
1546,summary,each decoding step,[],The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
1547,encoding process,output words,[],The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the [[ output words ]] in each decoding step .,0
1548,encoding process,each decoding step,[],The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in [[ each decoding step ]] .,0
1549,output words,each decoding step,[],The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in [[ each decoding step ]] .,0
1550,GANs approaches,RankGAN,"[['GANs approaches', 'has', 'RankGAN']]","Among the [[ GANs approaches ]] , [[ RankGAN ]] receives better score than SeqGAN , which is consistent to the finding in the Chinese poem composition .",1
1551,GANs approaches,better score,"[['GANs approaches', 'has', 'RankGAN']]","Among the [[ GANs approaches ]] , RankGAN receives [[ better score ]] than SeqGAN , which is consistent to the finding in the Chinese poem composition .",0
1552,GANs approaches,SeqGAN,"[['GANs approaches', 'has', 'RankGAN']]","Among the [[ GANs approaches ]] , RankGAN receives better score than [[ SeqGAN ]] , which is consistent to the finding in the Chinese poem composition .",0
1553,RankGAN,better score,"[['GANs approaches', 'has', 'RankGAN']]","Among the GANs approaches , [[ RankGAN ]] receives [[ better score ]] than SeqGAN , which is consistent to the finding in the Chinese poem composition .",0
1554,RankGAN,SeqGAN,"[['GANs approaches', 'has', 'RankGAN']]","Among the GANs approaches , [[ RankGAN ]] receives better score than [[ SeqGAN ]] , which is consistent to the finding in the Chinese poem composition .",0
1555,better score,SeqGAN,"[['GANs approaches', 'has', 'RankGAN']]","Among the GANs approaches , RankGAN receives [[ better score ]] than [[ SeqGAN ]] , which is consistent to the finding in the Chinese poem composition .",0
1556,our 100D and 300D model,all other models,[],"Secondly , comparing ours with other models , we find that [[ our 100D and 300D model ]] outperform [[ all other models ]] of similar numbers of parameters .",0
1557,our 100D and 300D model,similar numbers of parameters,[],"Secondly , comparing ours with other models , we find that [[ our 100D and 300D model ]] outperform all other models of [[ similar numbers of parameters ]] .",0
1558,all other models,similar numbers of parameters,[],"Secondly , comparing ours with other models , we find that our 100D and 300D model outperform [[ all other models ]] of [[ similar numbers of parameters ]] .",0
1559,Single Edge model,more complex Pooled Edges model,[],The [[ Single Edge model ]] outperforms the [[ more complex Pooled Edges model ]] by a noticeable margin .,0
1560,Single Edge model,noticeable margin,[],The [[ Single Edge model ]] outperforms the more complex Pooled Edges model by a [[ noticeable margin ]] .,0
1561,more complex Pooled Edges model,noticeable margin,[],The Single Edge model outperforms the [[ more complex Pooled Edges model ]] by a [[ noticeable margin ]] .,0
1562,vanilla RNN,hidden size,[],We employed the [[ vanilla RNN ]] with a [[ hidden size ]] of 512 for both the policy network and neural language model .,0
1563,vanilla RNN,512,[],We employed the [[ vanilla RNN ]] with a hidden size of [[ 512 ]] for both the policy network and neural language model .,0
1564,vanilla RNN,policy network,[],We employed the [[ vanilla RNN ]] with a hidden size of 512 for both the [[ policy network ]] and neural language model .,0
1565,vanilla RNN,neural language model,[],We employed the [[ vanilla RNN ]] with a hidden size of 512 for both the policy network and [[ neural language model ]] .,0
1566,hidden size,512,[],We employed the vanilla RNN with a [[ hidden size ]] of [[ 512 ]] for both the policy network and neural language model .,0
1567,hidden size,policy network,[],We employed the vanilla RNN with a [[ hidden size ]] of 512 for both the [[ policy network ]] and neural language model .,0
1568,hidden size,neural language model,[],We employed the vanilla RNN with a [[ hidden size ]] of 512 for both the policy network and [[ neural language model ]] .,0
1569,512,policy network,[],We employed the vanilla RNN with a hidden size of [[ 512 ]] for both the [[ policy network ]] and neural language model .,0
1570,512,neural language model,[],We employed the vanilla RNN with a hidden size of [[ 512 ]] for both the policy network and [[ neural language model ]] .,0
1571,policy network,neural language model,[],We employed the vanilla RNN with a hidden size of 512 for both the [[ policy network ]] and [[ neural language model ]] .,0
1572,fine - tune embedding - based models,a matrix,[],"Thus , we propose a method to [[ fine - tune embedding - based models ]] by carefully optimizing [[ a matrix ]] parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .",0
1573,fine - tune embedding - based models,similarity,[],"Thus , we propose a method to [[ fine - tune embedding - based models ]] by carefully optimizing a matrix parameterizing the [[ similarity ]] used in the embedding space , leading to a consistent improvement in performance .",0
1574,fine - tune embedding - based models,embedding space,[],"Thus , we propose a method to [[ fine - tune embedding - based models ]] by carefully optimizing a matrix parameterizing the similarity used in the [[ embedding space ]] , leading to a consistent improvement in performance .",0
1575,fine - tune embedding - based models,consistent improvement in performance,[],"Thus , we propose a method to [[ fine - tune embedding - based models ]] by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a [[ consistent improvement in performance ]] .",0
1576,a matrix,similarity,[],"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing [[ a matrix ]] parameterizing the [[ similarity ]] used in the embedding space , leading to a consistent improvement in performance .",0
1577,a matrix,embedding space,[],"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing [[ a matrix ]] parameterizing the similarity used in the [[ embedding space ]] , leading to a consistent improvement in performance .",0
1578,a matrix,consistent improvement in performance,[],"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing [[ a matrix ]] parameterizing the similarity used in the embedding space , leading to a [[ consistent improvement in performance ]] .",0
1579,similarity,embedding space,[],"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the [[ similarity ]] used in the [[ embedding space ]] , leading to a consistent improvement in performance .",0
1580,similarity,consistent improvement in performance,[],"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the [[ similarity ]] used in the embedding space , leading to a [[ consistent improvement in performance ]] .",0
1581,embedding space,consistent improvement in performance,[],"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the [[ embedding space ]] , leading to a [[ consistent improvement in performance ]] .",0
1582,treating text,raw signal,[],"In this article we explore [[ treating text ]] as a kind of [[ raw signal ]] at character level , and applying temporal ( one-dimensional ) ConvNets to it .",0
1583,treating text,character level,[],"In this article we explore [[ treating text ]] as a kind of raw signal at [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",0
1584,treating text,temporal ( one-dimensional ) ConvNets,[],"In this article we explore [[ treating text ]] as a kind of raw signal at character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
1585,raw signal,character level,[],"In this article we explore treating text as a kind of [[ raw signal ]] at [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",0
1586,raw signal,temporal ( one-dimensional ) ConvNets,[],"In this article we explore treating text as a kind of [[ raw signal ]] at character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
1587,character level,temporal ( one-dimensional ) ConvNets,[],"In this article we explore treating text as a kind of raw signal at [[ character level ]] , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
1588,PHYS relation,easier identified,[],[[ PHYS relation ]] is [[ easier identified ]] with respect to GPE entity than PER entity .,0
1589,PHYS relation,GPE entity,[],[[ PHYS relation ]] is easier identified with respect to [[ GPE entity ]] than PER entity .,0
1590,PHYS relation,PER entity,[],[[ PHYS relation ]] is easier identified with respect to GPE entity than [[ PER entity ]] .,0
1591,easier identified,GPE entity,[],PHYS relation is [[ easier identified ]] with respect to [[ GPE entity ]] than PER entity .,0
1592,easier identified,PER entity,[],PHYS relation is [[ easier identified ]] with respect to GPE entity than [[ PER entity ]] .,0
1593,GPE entity,PER entity,[],PHYS relation is easier identified with respect to [[ GPE entity ]] than [[ PER entity ]] .,0
1594,Comp - Clip model,context projection weight matrix,[],"To implement the [[ Comp - Clip model ]] , we apply a [[ context projection weight matrix ]] with 100 dimensions that are shared between the question part and the answer part ( eq. 1 ) .",0
1595,Comp - Clip model,100 dimensions,[],"To implement the [[ Comp - Clip model ]] , we apply a context projection weight matrix with [[ 100 dimensions ]] that are shared between the question part and the answer part ( eq. 1 ) .",0
1596,Comp - Clip model,question part,[],"To implement the [[ Comp - Clip model ]] , we apply a context projection weight matrix with 100 dimensions that are shared between the [[ question part ]] and the answer part ( eq. 1 ) .",0
1597,Comp - Clip model,answer part,[],"To implement the [[ Comp - Clip model ]] , we apply a context projection weight matrix with 100 dimensions that are shared between the question part and the [[ answer part ]] ( eq. 1 ) .",0
1598,context projection weight matrix,100 dimensions,[],"To implement the Comp - Clip model , we apply a [[ context projection weight matrix ]] with [[ 100 dimensions ]] that are shared between the question part and the answer part ( eq. 1 ) .",0
1599,context projection weight matrix,question part,[],"To implement the Comp - Clip model , we apply a [[ context projection weight matrix ]] with 100 dimensions that are shared between the [[ question part ]] and the answer part ( eq. 1 ) .",0
1600,context projection weight matrix,answer part,[],"To implement the Comp - Clip model , we apply a [[ context projection weight matrix ]] with 100 dimensions that are shared between the question part and the [[ answer part ]] ( eq. 1 ) .",0
1601,100 dimensions,question part,[],"To implement the Comp - Clip model , we apply a context projection weight matrix with [[ 100 dimensions ]] that are shared between the [[ question part ]] and the answer part ( eq. 1 ) .",0
1602,100 dimensions,answer part,[],"To implement the Comp - Clip model , we apply a context projection weight matrix with [[ 100 dimensions ]] that are shared between the question part and the [[ answer part ]] ( eq. 1 ) .",0
1603,question part,answer part,[],"To implement the Comp - Clip model , we apply a context projection weight matrix with 100 dimensions that are shared between the [[ question part ]] and the [[ answer part ]] ( eq. 1 ) .",0
1604,performance,soft templates,[],We also examine the [[ performance ]] of directly regarding [[ soft templates ]] as output summaries .,0
1605,performance,output summaries,[],We also examine the [[ performance ]] of directly regarding soft templates as [[ output summaries ]] .,0
1606,soft templates,output summaries,[],We also examine the performance of directly regarding [[ soft templates ]] as [[ output summaries ]] .,0
1607,aspect - term sentiment analysis ( ATSA ),aspect - category sentiment analysis ( ACSA ),[],"Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely [[ aspect - term sentiment analysis ( ATSA ) ]] and [[ aspect - category sentiment analysis ( ACSA ) ]] .",0
1608,C2 F,soft attention results,[],[[ C2 F ]] results are significantly worse than [[ soft attention results ]] .,0
1609,Learning rate,0.5,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","[[ Learning rate ]] starts at [[ 0.5 ]] , and decreases to 0.2 once the model stops improving .",0
1610,Learning rate,0.2,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","[[ Learning rate ]] starts at 0.5 , and decreases to [[ 0.2 ]] once the model stops improving .",0
1611,Learning rate,model,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","[[ Learning rate ]] starts at 0.5 , and decreases to 0.2 once the [[ model ]] stops improving .",0
1612,Learning rate,stops improving,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","[[ Learning rate ]] starts at 0.5 , and decreases to 0.2 once the model [[ stops improving ]] .",0
1613,0.5,0.2,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","Learning rate starts at [[ 0.5 ]] , and decreases to [[ 0.2 ]] once the model stops improving .",0
1614,0.5,model,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","Learning rate starts at [[ 0.5 ]] , and decreases to 0.2 once the [[ model ]] stops improving .",0
1615,0.5,stops improving,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","Learning rate starts at [[ 0.5 ]] , and decreases to 0.2 once the model [[ stops improving ]] .",0
1616,0.2,model,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","Learning rate starts at 0.5 , and decreases to [[ 0.2 ]] once the [[ model ]] stops improving .",0
1617,0.2,stops improving,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","Learning rate starts at 0.5 , and decreases to [[ 0.2 ]] once the model [[ stops improving ]] .",1
1618,model,stops improving,"[['0.2', 'has', 'stops improving'], ['stops improving', 'has', 'model']]","Learning rate starts at 0.5 , and decreases to 0.2 once the [[ model ]] [[ stops improving ]] .",0
1619,WEAT,Word pairs,"[['WEAT', 'has', 'Word pairs']]",[[ WEAT ]] : [[ Word pairs ]] from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,1
1620,WEAT,psychology literature,"[['WEAT', 'has', 'Word pairs']]",[[ WEAT ]] : Word pairs from the [[ psychology literature ]] on implicit association tests ( IAT ) that are used to characterize model bias .,0
1621,WEAT,implicit association tests ( IAT ),"[['WEAT', 'has', 'Word pairs']]",[[ WEAT ]] : Word pairs from the psychology literature on [[ implicit association tests ( IAT ) ]] that are used to characterize model bias .,0
1622,Word pairs,psychology literature,"[['WEAT', 'has', 'Word pairs']]",WEAT : [[ Word pairs ]] from the [[ psychology literature ]] on implicit association tests ( IAT ) that are used to characterize model bias .,0
1623,Word pairs,implicit association tests ( IAT ),"[['WEAT', 'has', 'Word pairs']]",WEAT : [[ Word pairs ]] from the psychology literature on [[ implicit association tests ( IAT ) ]] that are used to characterize model bias .,0
1624,psychology literature,implicit association tests ( IAT ),"[['WEAT', 'has', 'Word pairs']]",WEAT : Word pairs from the [[ psychology literature ]] on [[ implicit association tests ( IAT ) ]] that are used to characterize model bias .,0
1625,components,previous aligned features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These [[ components ]] , which the name RE2 stands for , are [[ previous aligned features ]] ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",1
1626,components,Residual vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These [[ components ]] , which the name RE2 stands for , are previous aligned features ( [[ Residual vectors ]] ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",0
1627,components,original point - wise features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These [[ components ]] , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , [[ original point - wise features ]] ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",1
1628,components,Embedding vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These [[ components ]] , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( [[ Embedding vectors ]] ) , and contextual features ( Encoded vectors ) .",0
1629,components,contextual features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These [[ components ]] , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and [[ contextual features ]] ( Encoded vectors ) .",1
1630,components,Encoded vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These [[ components ]] , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( [[ Encoded vectors ]] ) .",0
1631,previous aligned features,Residual vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are [[ previous aligned features ]] ( [[ Residual vectors ]] ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",0
1632,previous aligned features,original point - wise features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are [[ previous aligned features ]] ( Residual vectors ) , [[ original point - wise features ]] ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",0
1633,previous aligned features,Embedding vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are [[ previous aligned features ]] ( Residual vectors ) , original point - wise features ( [[ Embedding vectors ]] ) , and contextual features ( Encoded vectors ) .",0
1634,previous aligned features,contextual features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are [[ previous aligned features ]] ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and [[ contextual features ]] ( Encoded vectors ) .",0
1635,previous aligned features,Encoded vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are [[ previous aligned features ]] ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( [[ Encoded vectors ]] ) .",0
1636,Residual vectors,original point - wise features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( [[ Residual vectors ]] ) , [[ original point - wise features ]] ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",0
1637,Residual vectors,Embedding vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( [[ Residual vectors ]] ) , original point - wise features ( [[ Embedding vectors ]] ) , and contextual features ( Encoded vectors ) .",0
1638,Residual vectors,contextual features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( [[ Residual vectors ]] ) , original point - wise features ( Embedding vectors ) , and [[ contextual features ]] ( Encoded vectors ) .",0
1639,Residual vectors,Encoded vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( [[ Residual vectors ]] ) , original point - wise features ( Embedding vectors ) , and contextual features ( [[ Encoded vectors ]] ) .",0
1640,original point - wise features,Embedding vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , [[ original point - wise features ]] ( [[ Embedding vectors ]] ) , and contextual features ( Encoded vectors ) .",0
1641,original point - wise features,contextual features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , [[ original point - wise features ]] ( Embedding vectors ) , and [[ contextual features ]] ( Encoded vectors ) .",0
1642,original point - wise features,Encoded vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , [[ original point - wise features ]] ( Embedding vectors ) , and contextual features ( [[ Encoded vectors ]] ) .",0
1643,Embedding vectors,contextual features,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( [[ Embedding vectors ]] ) , and [[ contextual features ]] ( Encoded vectors ) .",0
1644,Embedding vectors,Encoded vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( [[ Embedding vectors ]] ) , and contextual features ( [[ Encoded vectors ]] ) .",0
1645,contextual features,Encoded vectors,"[['components', 'has', 'contextual features'], ['contextual features', 'name', 'Encoded vectors'], ['components', 'has', 'previous aligned features'], ['previous aligned features', 'name', 'Residual vectors'], ['components', 'has', 'original point - wise features'], ['original point - wise features', 'name', 'Embedding vectors']]","These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and [[ contextual features ]] ( [[ Encoded vectors ]] ) .",0
1646,novel framework,problems,[],"In this paper , we propose a [[ novel framework ]] to solve the above [[ problems ]] in target sentiment analysis .",0
1647,novel framework,target sentiment analysis,[],"In this paper , we propose a [[ novel framework ]] to solve the above problems in [[ target sentiment analysis ]] .",0
1648,problems,target sentiment analysis,[],"In this paper , we propose a novel framework to solve the above [[ problems ]] in [[ target sentiment analysis ]] .",0
1649,state of the art,53.6 %,[],"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the [[ state of the art ]] of [[ 53.6 % ]] on the fine - grained subtask , achieving 52.9 % .",0
1650,state of the art,fine - grained subtask,[],"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the [[ state of the art ]] of 53.6 % on the [[ fine - grained subtask ]] , achieving 52.9 % .",0
1651,state of the art,52.9 %,[],"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the [[ state of the art ]] of 53.6 % on the fine - grained subtask , achieving [[ 52.9 % ]] .",0
1652,53.6 %,fine - grained subtask,[],"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of [[ 53.6 % ]] on the [[ fine - grained subtask ]] , achieving 52.9 % .",0
1653,53.6 %,52.9 %,[],"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of [[ 53.6 % ]] on the fine - grained subtask , achieving [[ 52.9 % ]] .",0
1654,fine - grained subtask,52.9 %,[],"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of 53.6 % on the [[ fine - grained subtask ]] , achieving [[ 52.9 % ]] .",0
1655,generating,sketch,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after [[ generating ]] the [[ sketch ]] , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .",1
1656,generating,decoder,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after [[ generating ]] the sketch , the [[ decoder ]] knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .",0
1657,generating,basic meaning of the utterance,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after [[ generating ]] the sketch , the decoder knows what the [[ basic meaning of the utterance ]] looks like , and the model can use it as global context to improve the prediction of the final details .",0
1658,generating,model,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after [[ generating ]] the sketch , the decoder knows what the basic meaning of the utterance looks like , and the [[ model ]] can use it as global context to improve the prediction of the final details .",0
1659,generating,global context,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after [[ generating ]] the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as [[ global context ]] to improve the prediction of the final details .",0
1660,generating,prediction,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after [[ generating ]] the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the [[ prediction ]] of the final details .",0
1661,generating,final details,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after [[ generating ]] the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the [[ final details ]] .",0
1662,sketch,decoder,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the [[ sketch ]] , the [[ decoder ]] knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .",1
1663,sketch,basic meaning of the utterance,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the [[ sketch ]] , the decoder knows what the [[ basic meaning of the utterance ]] looks like , and the model can use it as global context to improve the prediction of the final details .",0
1664,sketch,model,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the [[ sketch ]] , the decoder knows what the basic meaning of the utterance looks like , and the [[ model ]] can use it as global context to improve the prediction of the final details .",0
1665,sketch,global context,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the [[ sketch ]] , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as [[ global context ]] to improve the prediction of the final details .",0
1666,sketch,prediction,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the [[ sketch ]] , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the [[ prediction ]] of the final details .",0
1667,sketch,final details,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the [[ sketch ]] , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the [[ final details ]] .",0
1668,decoder,basic meaning of the utterance,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the [[ decoder ]] knows what the [[ basic meaning of the utterance ]] looks like , and the model can use it as global context to improve the prediction of the final details .",0
1669,decoder,model,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the [[ decoder ]] knows what the basic meaning of the utterance looks like , and the [[ model ]] can use it as global context to improve the prediction of the final details .",0
1670,decoder,global context,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the [[ decoder ]] knows what the basic meaning of the utterance looks like , and the model can use it as [[ global context ]] to improve the prediction of the final details .",0
1671,decoder,prediction,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the [[ decoder ]] knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the [[ prediction ]] of the final details .",0
1672,decoder,final details,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the [[ decoder ]] knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the [[ final details ]] .",0
1673,basic meaning of the utterance,model,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the [[ basic meaning of the utterance ]] looks like , and the [[ model ]] can use it as global context to improve the prediction of the final details .",0
1674,basic meaning of the utterance,global context,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the [[ basic meaning of the utterance ]] looks like , and the model can use it as [[ global context ]] to improve the prediction of the final details .",0
1675,basic meaning of the utterance,prediction,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the [[ basic meaning of the utterance ]] looks like , and the model can use it as global context to improve the [[ prediction ]] of the final details .",0
1676,basic meaning of the utterance,final details,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the [[ basic meaning of the utterance ]] looks like , and the model can use it as global context to improve the prediction of the [[ final details ]] .",0
1677,model,global context,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the [[ model ]] can use it as [[ global context ]] to improve the prediction of the final details .",0
1678,model,prediction,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the [[ model ]] can use it as global context to improve the [[ prediction ]] of the final details .",0
1679,model,final details,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the [[ model ]] can use it as global context to improve the prediction of the [[ final details ]] .",0
1680,global context,prediction,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as [[ global context ]] to improve the [[ prediction ]] of the final details .",0
1681,global context,final details,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as [[ global context ]] to improve the prediction of the [[ final details ]] .",0
1682,prediction,final details,"[['generating', 'has', 'sketch'], ['sketch', 'has', 'decoder']]","Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the [[ prediction ]] of the [[ final details ]] .",0
1683,20 voice discrimination task,EER of 2.86 %,[],"On this [[ 20 voice discrimination task ]] we obtain an [[ EER of 2.86 % ]] , demonstrating that , while the synthetic speech tends to be close to the target speaker ( cosine similarity > 0.6 , and as in ) , it is nearly always even closer to other synthetic utterances for the same speaker ( similarity > 0.7 ) .",0
1684,modules,temporal and causal relation classification,[],"The [[ modules ]] for [[ temporal and causal relation classification ]] rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",0
1685,modules,sieve - based architecture,[],"The [[ modules ]] for temporal and causal relation classification rely both on a [[ sieve - based architecture ]] , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",0
1686,modules,remaining unlabelled pairs,[],"The [[ modules ]] for temporal and causal relation classification rely both on a sieve - based architecture , in which the [[ remaining unlabelled pairs ]] - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",0
1687,modules,rule - based component,[],"The [[ modules ]] for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a [[ rule - based component ]] and / or a transitive reasoner are fed into a supervised classifier .",0
1688,modules,transitive reasoner,[],"The [[ modules ]] for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a [[ transitive reasoner ]] are fed into a supervised classifier .",0
1689,modules,supervised classifier,[],"The [[ modules ]] for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a [[ supervised classifier ]] .",0
1690,temporal and causal relation classification,sieve - based architecture,[],"The modules for [[ temporal and causal relation classification ]] rely both on a [[ sieve - based architecture ]] , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",0
1691,temporal and causal relation classification,remaining unlabelled pairs,[],"The modules for [[ temporal and causal relation classification ]] rely both on a sieve - based architecture , in which the [[ remaining unlabelled pairs ]] - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",0
1692,temporal and causal relation classification,rule - based component,[],"The modules for [[ temporal and causal relation classification ]] rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a [[ rule - based component ]] and / or a transitive reasoner are fed into a supervised classifier .",0
1693,temporal and causal relation classification,transitive reasoner,[],"The modules for [[ temporal and causal relation classification ]] rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a [[ transitive reasoner ]] are fed into a supervised classifier .",0
1694,temporal and causal relation classification,supervised classifier,[],"The modules for [[ temporal and causal relation classification ]] rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a [[ supervised classifier ]] .",0
1695,sieve - based architecture,remaining unlabelled pairs,[],"The modules for temporal and causal relation classification rely both on a [[ sieve - based architecture ]] , in which the [[ remaining unlabelled pairs ]] - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",0
1696,sieve - based architecture,rule - based component,[],"The modules for temporal and causal relation classification rely both on a [[ sieve - based architecture ]] , in which the remaining unlabelled pairs - after running a [[ rule - based component ]] and / or a transitive reasoner are fed into a supervised classifier .",0
1697,sieve - based architecture,transitive reasoner,[],"The modules for temporal and causal relation classification rely both on a [[ sieve - based architecture ]] , in which the remaining unlabelled pairs - after running a rule - based component and / or a [[ transitive reasoner ]] are fed into a supervised classifier .",0
1698,sieve - based architecture,supervised classifier,[],"The modules for temporal and causal relation classification rely both on a [[ sieve - based architecture ]] , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a [[ supervised classifier ]] .",0
1699,remaining unlabelled pairs,rule - based component,[],"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the [[ remaining unlabelled pairs ]] - after running a [[ rule - based component ]] and / or a transitive reasoner are fed into a supervised classifier .",0
1700,remaining unlabelled pairs,transitive reasoner,[],"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the [[ remaining unlabelled pairs ]] - after running a rule - based component and / or a [[ transitive reasoner ]] are fed into a supervised classifier .",0
1701,remaining unlabelled pairs,supervised classifier,[],"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the [[ remaining unlabelled pairs ]] - after running a rule - based component and / or a transitive reasoner are fed into a [[ supervised classifier ]] .",0
1702,rule - based component,transitive reasoner,[],"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a [[ rule - based component ]] and / or a [[ transitive reasoner ]] are fed into a supervised classifier .",0
1703,rule - based component,supervised classifier,[],"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a [[ rule - based component ]] and / or a transitive reasoner are fed into a [[ supervised classifier ]] .",0
1704,transitive reasoner,supervised classifier,[],"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a [[ transitive reasoner ]] are fed into a [[ supervised classifier ]] .",0
1705,BERTbased model,highest accuracy,[],"The [[ BERTbased model ]] gets the [[ highest accuracy ]] of 83.2 % and 68.6 % in the 2 - way and 3 - way classification tasks , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/",0
1706,BERTbased model,83.2 % and 68.6 %,[],"The [[ BERTbased model ]] gets the highest accuracy of [[ 83.2 % and 68.6 % ]] in the 2 - way and 3 - way classification tasks , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/",0
1707,BERTbased model,2 - way and 3 - way classification tasks,[],"The [[ BERTbased model ]] gets the highest accuracy of 83.2 % and 68.6 % in the [[ 2 - way and 3 - way classification tasks ]] , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/",0
1708,highest accuracy,83.2 % and 68.6 %,[],"The BERTbased model gets the [[ highest accuracy ]] of [[ 83.2 % and 68.6 % ]] in the 2 - way and 3 - way classification tasks , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/",0
1709,highest accuracy,2 - way and 3 - way classification tasks,[],"The BERTbased model gets the [[ highest accuracy ]] of 83.2 % and 68.6 % in the [[ 2 - way and 3 - way classification tasks ]] , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/",0
1710,83.2 % and 68.6 %,2 - way and 3 - way classification tasks,[],"The BERTbased model gets the highest accuracy of [[ 83.2 % and 68.6 % ]] in the [[ 2 - way and 3 - way classification tasks ]] , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/",0
1711,greedy model,Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ),[],"Our [[ greedy model ]] is out - performed by the [[ Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) ]] as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .",0
1712,negative sampling rate,tuned,[],The [[ negative sampling rate ]] is [[ tuned ]] from 2 to 8 .,0
1713,negative sampling rate,2 to 8,[],The [[ negative sampling rate ]] is tuned from [[ 2 to 8 ]] .,0
1714,tuned,2 to 8,[],The negative sampling rate is [[ tuned ]] from [[ 2 to 8 ]] .,0
1715,Variational Auto - Encoders ( VAEs ),base model,[],We employ [[ Variational Auto - Encoders ( VAEs ) ]] as the [[ base model ]] for our generative framework which can handle the inference problem associated with complex generative modeling .,0
1716,Variational Auto - Encoders ( VAEs ),our generative framework,[],We employ [[ Variational Auto - Encoders ( VAEs ) ]] as the base model for [[ our generative framework ]] which can handle the inference problem associated with complex generative modeling .,0
1717,Variational Auto - Encoders ( VAEs ),inference problem,[],We employ [[ Variational Auto - Encoders ( VAEs ) ]] as the base model for our generative framework which can handle the [[ inference problem ]] associated with complex generative modeling .,0
1718,Variational Auto - Encoders ( VAEs ),complex generative modeling,[],We employ [[ Variational Auto - Encoders ( VAEs ) ]] as the base model for our generative framework which can handle the inference problem associated with [[ complex generative modeling ]] .,0
1719,base model,our generative framework,[],We employ Variational Auto - Encoders ( VAEs ) as the [[ base model ]] for [[ our generative framework ]] which can handle the inference problem associated with complex generative modeling .,0
1720,base model,inference problem,[],We employ Variational Auto - Encoders ( VAEs ) as the [[ base model ]] for our generative framework which can handle the [[ inference problem ]] associated with complex generative modeling .,0
1721,base model,complex generative modeling,[],We employ Variational Auto - Encoders ( VAEs ) as the [[ base model ]] for our generative framework which can handle the inference problem associated with [[ complex generative modeling ]] .,0
1722,our generative framework,inference problem,[],We employ Variational Auto - Encoders ( VAEs ) as the base model for [[ our generative framework ]] which can handle the [[ inference problem ]] associated with complex generative modeling .,0
1723,our generative framework,complex generative modeling,[],We employ Variational Auto - Encoders ( VAEs ) as the base model for [[ our generative framework ]] which can handle the inference problem associated with [[ complex generative modeling ]] .,0
1724,inference problem,complex generative modeling,[],We employ Variational Auto - Encoders ( VAEs ) as the base model for our generative framework which can handle the [[ inference problem ]] associated with [[ complex generative modeling ]] .,0
1725,IAN,attentions,[],"[[ IAN ]] interactively learns [[ attentions ]] in the contexts and targets , and generates the representations for targets and contexts separately .",0
1726,IAN,contexts and targets,[],"[[ IAN ]] interactively learns attentions in the [[ contexts and targets ]] , and generates the representations for targets and contexts separately .",0
1727,IAN,representations,[],"[[ IAN ]] interactively learns attentions in the contexts and targets , and generates the [[ representations ]] for targets and contexts separately .",0
1728,IAN,targets and contexts,[],"[[ IAN ]] interactively learns attentions in the contexts and targets , and generates the representations for [[ targets and contexts ]] separately .",0
1729,attentions,contexts and targets,[],"IAN interactively learns [[ attentions ]] in the [[ contexts and targets ]] , and generates the representations for targets and contexts separately .",0
1730,attentions,representations,[],"IAN interactively learns [[ attentions ]] in the contexts and targets , and generates the [[ representations ]] for targets and contexts separately .",0
1731,attentions,targets and contexts,[],"IAN interactively learns [[ attentions ]] in the contexts and targets , and generates the representations for [[ targets and contexts ]] separately .",0
1732,contexts and targets,representations,[],"IAN interactively learns attentions in the [[ contexts and targets ]] , and generates the [[ representations ]] for targets and contexts separately .",0
1733,contexts and targets,targets and contexts,[],"IAN interactively learns attentions in the [[ contexts and targets ]] , and generates the representations for [[ targets and contexts ]] separately .",0
1734,representations,targets and contexts,[],"IAN interactively learns attentions in the contexts and targets , and generates the [[ representations ]] for [[ targets and contexts ]] separately .",0
1735,simple interpolation,GCN and a PA - LSTM,[],"This [[ simple interpolation ]] between a [[ GCN and a PA - LSTM ]] achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .",0
1736,simple interpolation,F 1 score,[],"This [[ simple interpolation ]] between a GCN and a PA - LSTM achieves an [[ F 1 score ]] of 67.1 , outperforming each model alone by at least 2.0 F 1 .",0
1737,simple interpolation,67.1,[],"This [[ simple interpolation ]] between a GCN and a PA - LSTM achieves an F 1 score of [[ 67.1 ]] , outperforming each model alone by at least 2.0 F 1 .",0
1738,simple interpolation,each model alone,[],"This [[ simple interpolation ]] between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming [[ each model alone ]] by at least 2.0 F 1 .",0
1739,simple interpolation,at least 2.0 F 1,[],"This [[ simple interpolation ]] between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by [[ at least 2.0 F 1 ]] .",0
1740,GCN and a PA - LSTM,F 1 score,[],"This simple interpolation between a [[ GCN and a PA - LSTM ]] achieves an [[ F 1 score ]] of 67.1 , outperforming each model alone by at least 2.0 F 1 .",0
1741,GCN and a PA - LSTM,67.1,[],"This simple interpolation between a [[ GCN and a PA - LSTM ]] achieves an F 1 score of [[ 67.1 ]] , outperforming each model alone by at least 2.0 F 1 .",0
1742,GCN and a PA - LSTM,each model alone,[],"This simple interpolation between a [[ GCN and a PA - LSTM ]] achieves an F 1 score of 67.1 , outperforming [[ each model alone ]] by at least 2.0 F 1 .",0
1743,GCN and a PA - LSTM,at least 2.0 F 1,[],"This simple interpolation between a [[ GCN and a PA - LSTM ]] achieves an F 1 score of 67.1 , outperforming each model alone by [[ at least 2.0 F 1 ]] .",0
1744,F 1 score,67.1,[],"This simple interpolation between a GCN and a PA - LSTM achieves an [[ F 1 score ]] of [[ 67.1 ]] , outperforming each model alone by at least 2.0 F 1 .",0
1745,F 1 score,each model alone,[],"This simple interpolation between a GCN and a PA - LSTM achieves an [[ F 1 score ]] of 67.1 , outperforming [[ each model alone ]] by at least 2.0 F 1 .",0
1746,F 1 score,at least 2.0 F 1,[],"This simple interpolation between a GCN and a PA - LSTM achieves an [[ F 1 score ]] of 67.1 , outperforming each model alone by [[ at least 2.0 F 1 ]] .",0
1747,67.1,each model alone,[],"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of [[ 67.1 ]] , outperforming [[ each model alone ]] by at least 2.0 F 1 .",0
1748,67.1,at least 2.0 F 1,[],"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of [[ 67.1 ]] , outperforming each model alone by [[ at least 2.0 F 1 ]] .",0
1749,each model alone,at least 2.0 F 1,[],"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming [[ each model alone ]] by [[ at least 2.0 F 1 ]] .",0
1750,released code,BERT BASE and DialogueRNN,[],We use the [[ released code ]] for [[ BERT BASE and DialogueRNN ]] .,0
1751,single - hidden layer MLP,ReLU activation function,[],is a [[ single - hidden layer MLP ]] with the [[ ReLU activation function ]] .,0
1752,modified version,GNMT model,[],Our model was a [[ modified version ]] of the [[ GNMT model ]] described in .,0
1753,positions,two target entities,[],We then locate the [[ positions ]] of the [[ two target entities ]] in the output embedding from BERT model .,0
1754,positions,output embedding,[],We then locate the [[ positions ]] of the two target entities in the [[ output embedding ]] from BERT model .,0
1755,positions,BERT model,[],We then locate the [[ positions ]] of the two target entities in the output embedding from [[ BERT model ]] .,0
1756,two target entities,output embedding,[],We then locate the positions of the [[ two target entities ]] in the [[ output embedding ]] from BERT model .,0
1757,two target entities,BERT model,[],We then locate the positions of the [[ two target entities ]] in the output embedding from [[ BERT model ]] .,0
1758,output embedding,BERT model,[],We then locate the positions of the two target entities in the [[ output embedding ]] from [[ BERT model ]] .,0
1759,running time,hierarchical softmax,[],"In order to improve our [[ running time ]] , we use a [[ hierarchical softmax ]] ) based on the Huffman coding tree .",0
1760,running time,Huffman coding tree,[],"In order to improve our [[ running time ]] , we use a hierarchical softmax ) based on the [[ Huffman coding tree ]] .",0
1761,hierarchical softmax,Huffman coding tree,[],"In order to improve our running time , we use a [[ hierarchical softmax ]] ) based on the [[ Huffman coding tree ]] .",0
1762,reader comments words and document words,reader attention,[],"We first calculate alignment between the [[ reader comments words and document words ]] , and this alignment information is regarded as [[ reader attention ]] representing the "" reader focused aspect "" .",0
1763,reader comments words and document words,reader focused aspect,[],"We first calculate alignment between the [[ reader comments words and document words ]] , and this alignment information is regarded as reader attention representing the "" [[ reader focused aspect ]] "" .",0
1764,reader attention,reader focused aspect,[],"We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as [[ reader attention ]] representing the "" [[ reader focused aspect ]] "" .",0
1765,topics,additional context,[],"Using [[ topics ]] as [[ additional context ]] also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",0
1766,topics,performance,[],"Using [[ topics ]] as additional context also decreases the [[ performance ]] of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",0
1767,topics,CNN classifier,[],"Using [[ topics ]] as additional context also decreases the performance of the [[ CNN classifier ]] on most data sets , giving an adverse effect to the CNN classifier .",0
1768,topics,most data sets,[],"Using [[ topics ]] as additional context also decreases the performance of the CNN classifier on [[ most data sets ]] , giving an adverse effect to the CNN classifier .",0
1769,additional context,performance,[],"Using topics as [[ additional context ]] also decreases the [[ performance ]] of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",0
1770,additional context,CNN classifier,[],"Using topics as [[ additional context ]] also decreases the performance of the [[ CNN classifier ]] on most data sets , giving an adverse effect to the CNN classifier .",0
1771,additional context,most data sets,[],"Using topics as [[ additional context ]] also decreases the performance of the CNN classifier on [[ most data sets ]] , giving an adverse effect to the CNN classifier .",0
1772,performance,CNN classifier,[],"Using topics as additional context also decreases the [[ performance ]] of the [[ CNN classifier ]] on most data sets , giving an adverse effect to the CNN classifier .",0
1773,performance,most data sets,[],"Using topics as additional context also decreases the [[ performance ]] of the CNN classifier on [[ most data sets ]] , giving an adverse effect to the CNN classifier .",0
1774,CNN classifier,most data sets,[],"Using topics as additional context also decreases the performance of the [[ CNN classifier ]] on [[ most data sets ]] , giving an adverse effect to the CNN classifier .",0
1775,version,loopy belief propagation,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a [[ version ]] of [[ loopy belief propagation ]] modified specifically for solving Sudokus that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1776,version,modified specifically,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a [[ version ]] of loopy belief propagation [[ modified specifically ]] for solving Sudokus that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1777,version,Sudokus,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a [[ version ]] of loopy belief propagation modified specifically for solving [[ Sudokus ]] that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1778,version,250 steps,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a [[ version ]] of loopy belief propagation modified specifically for solving Sudokus that uses [[ 250 steps ]] , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1779,loopy belief propagation,modified specifically,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a version of [[ loopy belief propagation ]] [[ modified specifically ]] for solving Sudokus that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",1
1780,loopy belief propagation,Sudokus,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a version of [[ loopy belief propagation ]] modified specifically for solving [[ Sudokus ]] that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1781,loopy belief propagation,250 steps,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a version of [[ loopy belief propagation ]] modified specifically for solving Sudokus that uses [[ 250 steps ]] , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1782,modified specifically,Sudokus,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a version of loopy belief propagation [[ modified specifically ]] for solving [[ Sudokus ]] that uses 250 steps , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1783,modified specifically,250 steps,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a version of loopy belief propagation [[ modified specifically ]] for solving Sudokus that uses [[ 250 steps ]] , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1784,Sudokus,250 steps,"[['loopy belief propagation', 'has', 'modified specifically']]","It also outperforms a version of loopy belief propagation modified specifically for solving [[ Sudokus ]] that uses [[ 250 steps ]] , Sinkhorn balancing every two steps and iteratively picks the most probable digit .",0
1785,sentence encoder,input words,[],"First , the [[ sentence encoder ]] reads the [[ input words ]] through an RNN unit to construct the first level sentence representation .",0
1786,sentence encoder,RNN unit,[],"First , the [[ sentence encoder ]] reads the input words through an [[ RNN unit ]] to construct the first level sentence representation .",0
1787,sentence encoder,first level sentence representation,[],"First , the [[ sentence encoder ]] reads the input words through an RNN unit to construct the [[ first level sentence representation ]] .",0
1788,input words,RNN unit,[],"First , the sentence encoder reads the [[ input words ]] through an [[ RNN unit ]] to construct the first level sentence representation .",0
1789,input words,first level sentence representation,[],"First , the sentence encoder reads the [[ input words ]] through an RNN unit to construct the [[ first level sentence representation ]] .",0
1790,RNN unit,first level sentence representation,[],"First , the sentence encoder reads the input words through an [[ RNN unit ]] to construct the [[ first level sentence representation ]] .",0
1791,best hyperparameters,grid search,[],To determine the [[ best hyperparameters ]] we performed a [[ grid search ]] over 150 settings based on validation - set accuracy .,0
1792,best hyperparameters,150 settings,[],To determine the [[ best hyperparameters ]] we performed a grid search over [[ 150 settings ]] based on validation - set accuracy .,0
1793,best hyperparameters,validation - set accuracy,[],To determine the [[ best hyperparameters ]] we performed a grid search over 150 settings based on [[ validation - set accuracy ]] .,0
1794,grid search,150 settings,[],To determine the best hyperparameters we performed a [[ grid search ]] over [[ 150 settings ]] based on validation - set accuracy .,0
1795,grid search,validation - set accuracy,[],To determine the best hyperparameters we performed a [[ grid search ]] over 150 settings based on [[ validation - set accuracy ]] .,0
1796,150 settings,validation - set accuracy,[],To determine the best hyperparameters we performed a grid search over [[ 150 settings ]] based on [[ validation - set accuracy ]] .,0
1797,WikiQA dataset,pointwise learning approach,"[['WikiQA dataset', 'has', 'pointwise learning approach']]","Wiki QA : For the [[ WikiQA dataset ]] , the [[ pointwise learning approach ]] shows a better performance than the listwise learning approach .",1
1798,WikiQA dataset,better performance,"[['WikiQA dataset', 'has', 'pointwise learning approach']]","Wiki QA : For the [[ WikiQA dataset ]] , the pointwise learning approach shows a [[ better performance ]] than the listwise learning approach .",0
1799,WikiQA dataset,listwise learning approach,"[['WikiQA dataset', 'has', 'pointwise learning approach']]","Wiki QA : For the [[ WikiQA dataset ]] , the pointwise learning approach shows a better performance than the [[ listwise learning approach ]] .",0
1800,pointwise learning approach,better performance,"[['WikiQA dataset', 'has', 'pointwise learning approach']]","Wiki QA : For the WikiQA dataset , the [[ pointwise learning approach ]] shows a [[ better performance ]] than the listwise learning approach .",0
1801,pointwise learning approach,listwise learning approach,"[['WikiQA dataset', 'has', 'pointwise learning approach']]","Wiki QA : For the WikiQA dataset , the [[ pointwise learning approach ]] shows a better performance than the [[ listwise learning approach ]] .",0
1802,better performance,listwise learning approach,"[['WikiQA dataset', 'has', 'pointwise learning approach']]","Wiki QA : For the WikiQA dataset , the pointwise learning approach shows a [[ better performance ]] than the [[ listwise learning approach ]] .",0
1803,' local ' loss,each recurrent unit cell,[],"In general , the model ensures a [[ ' local ' loss ]] that is incurred for [[ each recurrent unit cell ]] .",0
1804,classifier,hidden layer,"[['classifier', 'has', 'hidden layer']]",The [[ classifier ]] has a [[ hidden layer ]] of size 50 .,1
1805,classifier,50,"[['classifier', 'has', 'hidden layer']]",The [[ classifier ]] has a hidden layer of size [[ 50 ]] .,0
1806,hidden layer,50,"[['classifier', 'has', 'hidden layer']]",The classifier has a [[ hidden layer ]] of size [[ 50 ]] .,0
1807,two types,standard regularizations,[],We employ [[ two types ]] of [[ standard regularizations ]] .,0
1808,deep networks,better representations,[],"First , our model uses [[ deep networks ]] to learn [[ better representations ]] for candidate answer chunks , instead of using fixed feature representations as in .",0
1809,deep networks,candidate answer chunks,[],"First , our model uses [[ deep networks ]] to learn better representations for [[ candidate answer chunks ]] , instead of using fixed feature representations as in .",0
1810,deep networks,fixed feature representations,[],"First , our model uses [[ deep networks ]] to learn better representations for candidate answer chunks , instead of using [[ fixed feature representations ]] as in .",0
1811,better representations,candidate answer chunks,[],"First , our model uses deep networks to learn [[ better representations ]] for [[ candidate answer chunks ]] , instead of using fixed feature representations as in .",0
1812,better representations,fixed feature representations,[],"First , our model uses deep networks to learn [[ better representations ]] for candidate answer chunks , instead of using [[ fixed feature representations ]] as in .",0
1813,candidate answer chunks,fixed feature representations,[],"First , our model uses deep networks to learn better representations for [[ candidate answer chunks ]] , instead of using [[ fixed feature representations ]] as in .",0
1814,basic LSTM approach,worst,[],"Among LSTM based neural networks described in this paper , the [[ basic LSTM approach ]] performs the [[ worst ]] .",0
1815,novel neural architecture,RASOR,[],"To overcome this , we present a [[ novel neural architecture ]] called [[ RASOR ]] that builds fixed - length span representations , reusing recurrent computations for shared substructures .",0
1816,novel neural architecture,fixed - length span representations,[],"To overcome this , we present a [[ novel neural architecture ]] called RASOR that builds [[ fixed - length span representations ]] , reusing recurrent computations for shared substructures .",0
1817,novel neural architecture,recurrent computations,[],"To overcome this , we present a [[ novel neural architecture ]] called RASOR that builds fixed - length span representations , reusing [[ recurrent computations ]] for shared substructures .",0
1818,novel neural architecture,shared substructures,[],"To overcome this , we present a [[ novel neural architecture ]] called RASOR that builds fixed - length span representations , reusing recurrent computations for [[ shared substructures ]] .",0
1819,RASOR,fixed - length span representations,[],"To overcome this , we present a novel neural architecture called [[ RASOR ]] that builds [[ fixed - length span representations ]] , reusing recurrent computations for shared substructures .",0
1820,RASOR,recurrent computations,[],"To overcome this , we present a novel neural architecture called [[ RASOR ]] that builds fixed - length span representations , reusing [[ recurrent computations ]] for shared substructures .",0
1821,RASOR,shared substructures,[],"To overcome this , we present a novel neural architecture called [[ RASOR ]] that builds fixed - length span representations , reusing recurrent computations for [[ shared substructures ]] .",0
1822,fixed - length span representations,recurrent computations,[],"To overcome this , we present a novel neural architecture called RASOR that builds [[ fixed - length span representations ]] , reusing [[ recurrent computations ]] for shared substructures .",0
1823,fixed - length span representations,shared substructures,[],"To overcome this , we present a novel neural architecture called RASOR that builds [[ fixed - length span representations ]] , reusing recurrent computations for [[ shared substructures ]] .",0
1824,recurrent computations,shared substructures,[],"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing [[ recurrent computations ]] for [[ shared substructures ]] .",0
1825,FP16 computation,size,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage [[ FP16 computation ]] 11 to reduce the [[ size ]] of both the model and hidden representations of data .",0
1826,FP16 computation,model,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage [[ FP16 computation ]] 11 to reduce the size of both the [[ model ]] and hidden representations of data .",0
1827,FP16 computation,hidden representations,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage [[ FP16 computation ]] 11 to reduce the size of both the model and [[ hidden representations ]] of data .",0
1828,FP16 computation,data,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage [[ FP16 computation ]] 11 to reduce the size of both the model and hidden representations of [[ data ]] .",0
1829,size,model,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the [[ size ]] of both the [[ model ]] and hidden representations of data .",0
1830,size,hidden representations,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the [[ size ]] of both the model and [[ hidden representations ]] of data .",0
1831,size,data,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the [[ size ]] of both the model and hidden representations of [[ data ]] .",0
1832,model,hidden representations,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the [[ model ]] and [[ hidden representations ]] of data .",0
1833,model,data,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the [[ model ]] and hidden representations of [[ data ]] .",0
1834,hidden representations,data,[],"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and [[ hidden representations ]] of [[ data ]] .",0
1835,BioBERT,pre-trained language representation model,[],"In this article , we introduce [[ BioBERT ]] , which is a [[ pre-trained language representation model ]] for the biomedical domain .",0
1836,BioBERT,biomedical domain,[],"In this article , we introduce [[ BioBERT ]] , which is a pre-trained language representation model for the [[ biomedical domain ]] .",0
1837,pre-trained language representation model,biomedical domain,[],"In this article , we introduce BioBERT , which is a [[ pre-trained language representation model ]] for the [[ biomedical domain ]] .",0
1838,SDP - LSTM model,our model,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , [[ our model ]] achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",1
1839,SDP - LSTM model,improved F 1 scores,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1840,SDP - LSTM model,26 out of the 41 slot types,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1841,SDP - LSTM model,top 5 slot,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1842,SDP - LSTM model,org : political / religious affiliation,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being [[ org : political / religious affiliation ]] , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1843,SDP - LSTM model,per: country of death,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , [[ per: country of death ]] , org : alternate names , per:religion and per: alternate names .",0
1844,SDP - LSTM model,org : alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , [[ org : alternate names ]] , per:religion and per: alternate names .",0
1845,SDP - LSTM model,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , [[ per:religion ]] and per: alternate names .",0
1846,SDP - LSTM model,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with [[ SDP - LSTM model ]] , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and [[ per: alternate names ]] .",0
1847,our model,improved F 1 scores,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1848,our model,26 out of the 41 slot types,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1849,our model,top 5 slot,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves improved F 1 scores on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1850,our model,org : political / religious affiliation,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being [[ org : political / religious affiliation ]] , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1851,our model,per: country of death,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , [[ per: country of death ]] , org : alternate names , per:religion and per: alternate names .",0
1852,our model,org : alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , [[ org : alternate names ]] , per:religion and per: alternate names .",0
1853,our model,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , [[ per:religion ]] and per: alternate names .",0
1854,our model,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , [[ our model ]] achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and [[ per: alternate names ]] .",0
1855,improved F 1 scores,26 out of the 41 slot types,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves [[ improved F 1 scores ]] on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1856,improved F 1 scores,top 5 slot,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1857,improved F 1 scores,org : political / religious affiliation,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the top 5 slot types being [[ org : political / religious affiliation ]] , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1858,improved F 1 scores,per: country of death,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , [[ per: country of death ]] , org : alternate names , per:religion and per: alternate names .",0
1859,improved F 1 scores,org : alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , [[ org : alternate names ]] , per:religion and per: alternate names .",0
1860,improved F 1 scores,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , [[ per:religion ]] and per: alternate names .",0
1861,improved F 1 scores,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves [[ improved F 1 scores ]] on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and [[ per: alternate names ]] .",0
1862,26 out of the 41 slot types,top 5 slot,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the [[ top 5 slot ]] types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1863,26 out of the 41 slot types,org : political / religious affiliation,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being [[ org : political / religious affiliation ]] , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1864,26 out of the 41 slot types,per: country of death,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being org : political / religious affiliation , [[ per: country of death ]] , org : alternate names , per:religion and per: alternate names .",0
1865,26 out of the 41 slot types,org : alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being org : political / religious affiliation , per: country of death , [[ org : alternate names ]] , per:religion and per: alternate names .",0
1866,26 out of the 41 slot types,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , [[ per:religion ]] and per: alternate names .",0
1867,26 out of the 41 slot types,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on [[ 26 out of the 41 slot types ]] , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and [[ per: alternate names ]] .",0
1868,top 5 slot,org : political / religious affiliation,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being [[ org : political / religious affiliation ]] , per: country of death , org : alternate names , per:religion and per: alternate names .",0
1869,top 5 slot,per: country of death,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being org : political / religious affiliation , [[ per: country of death ]] , org : alternate names , per:religion and per: alternate names .",0
1870,top 5 slot,org : alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being org : political / religious affiliation , per: country of death , [[ org : alternate names ]] , per:religion and per: alternate names .",0
1871,top 5 slot,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being org : political / religious affiliation , per: country of death , org : alternate names , [[ per:religion ]] and per: alternate names .",0
1872,top 5 slot,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the [[ top 5 slot ]] types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and [[ per: alternate names ]] .",0
1873,org : political / religious affiliation,per: country of death,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being [[ org : political / religious affiliation ]] , [[ per: country of death ]] , org : alternate names , per:religion and per: alternate names .",0
1874,org : political / religious affiliation,org : alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being [[ org : political / religious affiliation ]] , per: country of death , [[ org : alternate names ]] , per:religion and per: alternate names .",0
1875,org : political / religious affiliation,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being [[ org : political / religious affiliation ]] , per: country of death , org : alternate names , [[ per:religion ]] and per: alternate names .",0
1876,org : political / religious affiliation,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being [[ org : political / religious affiliation ]] , per: country of death , org : alternate names , per:religion and [[ per: alternate names ]] .",0
1877,per: country of death,org : alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , [[ per: country of death ]] , [[ org : alternate names ]] , per:religion and per: alternate names .",0
1878,per: country of death,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , [[ per: country of death ]] , org : alternate names , [[ per:religion ]] and per: alternate names .",0
1879,per: country of death,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , [[ per: country of death ]] , org : alternate names , per:religion and [[ per: alternate names ]] .",0
1880,org : alternate names,per:religion,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , [[ org : alternate names ]] , [[ per:religion ]] and per: alternate names .",0
1881,org : alternate names,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , [[ org : alternate names ]] , per:religion and [[ per: alternate names ]] .",0
1882,per:religion,per: alternate names,"[['SDP - LSTM model', 'has', 'our model']]","When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , [[ per:religion ]] and [[ per: alternate names ]] .",0
1883,integral query matching,result,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the [[ result ]] drops about 2 % on both metrics and it shows that the integral information of query for each word in passage is crucial .",1
1884,integral query matching,drops,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the result [[ drops ]] about 2 % on both metrics and it shows that the integral information of query for each word in passage is crucial .",0
1885,integral query matching,2 %,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the result drops about [[ 2 % ]] on both metrics and it shows that the integral information of query for each word in passage is crucial .",0
1886,integral query matching,integral information,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the result drops about 2 % on both metrics and it shows that the [[ integral information ]] of query for each word in passage is crucial .",0
1887,integral query matching,query,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the result drops about 2 % on both metrics and it shows that the integral information of [[ query ]] for each word in passage is crucial .",0
1888,integral query matching,each word,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the result drops about 2 % on both metrics and it shows that the integral information of query for [[ each word ]] in passage is crucial .",0
1889,integral query matching,passage,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the result drops about 2 % on both metrics and it shows that the integral information of query for each word in [[ passage ]] is crucial .",0
1890,integral query matching,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating [[ integral query matching ]] , the result drops about 2 % on both metrics and it shows that the integral information of query for each word in passage is [[ crucial ]] .",0
1891,result,drops,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the [[ result ]] [[ drops ]] about 2 % on both metrics and it shows that the integral information of query for each word in passage is crucial .",1
1892,result,2 %,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the [[ result ]] drops about [[ 2 % ]] on both metrics and it shows that the integral information of query for each word in passage is crucial .",0
1893,result,integral information,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the [[ result ]] drops about 2 % on both metrics and it shows that the [[ integral information ]] of query for each word in passage is crucial .",0
1894,result,query,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the [[ result ]] drops about 2 % on both metrics and it shows that the integral information of [[ query ]] for each word in passage is crucial .",0
1895,result,each word,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the [[ result ]] drops about 2 % on both metrics and it shows that the integral information of query for [[ each word ]] in passage is crucial .",0
1896,result,passage,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the [[ result ]] drops about 2 % on both metrics and it shows that the integral information of query for each word in [[ passage ]] is crucial .",0
1897,result,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the [[ result ]] drops about 2 % on both metrics and it shows that the integral information of query for each word in passage is [[ crucial ]] .",0
1898,drops,2 %,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result [[ drops ]] about [[ 2 % ]] on both metrics and it shows that the integral information of query for each word in passage is crucial .",0
1899,drops,integral information,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result [[ drops ]] about 2 % on both metrics and it shows that the [[ integral information ]] of query for each word in passage is crucial .",0
1900,drops,query,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result [[ drops ]] about 2 % on both metrics and it shows that the integral information of [[ query ]] for each word in passage is crucial .",0
1901,drops,each word,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result [[ drops ]] about 2 % on both metrics and it shows that the integral information of query for [[ each word ]] in passage is crucial .",0
1902,drops,passage,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result [[ drops ]] about 2 % on both metrics and it shows that the integral information of query for each word in [[ passage ]] is crucial .",0
1903,drops,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result [[ drops ]] about 2 % on both metrics and it shows that the integral information of query for each word in passage is [[ crucial ]] .",0
1904,2 %,integral information,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about [[ 2 % ]] on both metrics and it shows that the [[ integral information ]] of query for each word in passage is crucial .",0
1905,2 %,query,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about [[ 2 % ]] on both metrics and it shows that the integral information of [[ query ]] for each word in passage is crucial .",0
1906,2 %,each word,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about [[ 2 % ]] on both metrics and it shows that the integral information of query for [[ each word ]] in passage is crucial .",0
1907,2 %,passage,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about [[ 2 % ]] on both metrics and it shows that the integral information of query for each word in [[ passage ]] is crucial .",0
1908,2 %,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about [[ 2 % ]] on both metrics and it shows that the integral information of query for each word in passage is [[ crucial ]] .",0
1909,integral information,query,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the [[ integral information ]] of [[ query ]] for each word in passage is crucial .",0
1910,integral information,each word,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the [[ integral information ]] of query for [[ each word ]] in passage is crucial .",0
1911,integral information,passage,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the [[ integral information ]] of query for each word in [[ passage ]] is crucial .",0
1912,integral information,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the [[ integral information ]] of query for each word in passage is [[ crucial ]] .",0
1913,query,each word,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of [[ query ]] for [[ each word ]] in passage is crucial .",0
1914,query,passage,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of [[ query ]] for each word in [[ passage ]] is crucial .",0
1915,query,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of [[ query ]] for each word in passage is [[ crucial ]] .",0
1916,each word,passage,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of query for [[ each word ]] in [[ passage ]] is crucial .",0
1917,each word,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of query for [[ each word ]] in passage is [[ crucial ]] .",0
1918,passage,crucial,"[['integral query matching', 'has', 'result'], ['result', 'has', 'drops']]","For ablating integral query matching , the result drops about 2 % on both metrics and it shows that the integral information of query for each word in [[ passage ]] is [[ crucial ]] .",0
1919,both tasks,our model 's Precision,[],"For [[ both tasks ]] , [[ our model 's Precision ]] is close to and Recall is significantly higher than previous works .",0
1920,both tasks,Recall,[],"For [[ both tasks ]] , our model 's Precision is close to and [[ Recall ]] is significantly higher than previous works .",0
1921,both tasks,previous works,[],"For [[ both tasks ]] , our model 's Precision is close to and Recall is significantly higher than [[ previous works ]] .",0
1922,our model 's Precision,Recall,[],"For both tasks , [[ our model 's Precision ]] is close to and [[ Recall ]] is significantly higher than previous works .",0
1923,our model 's Precision,previous works,[],"For both tasks , [[ our model 's Precision ]] is close to and Recall is significantly higher than [[ previous works ]] .",0
1924,Recall,previous works,[],"For both tasks , our model 's Precision is close to and [[ Recall ]] is significantly higher than [[ previous works ]] .",0
1925,linguistic quality,generated summaries,[],"We also measure the [[ linguistic quality ]] of [[ generated summaries ]] from various aspects , and the results are present in .",0
1926,interactive attention,memory network,[],We then develop the [[ interactive attention ]] with [[ memory network ]] to mimic human reading procedure .,0
1927,interactive attention,human reading procedure,[],We then develop the [[ interactive attention ]] with memory network to mimic [[ human reading procedure ]] .,0
1928,memory network,human reading procedure,[],We then develop the interactive attention with [[ memory network ]] to mimic [[ human reading procedure ]] .,0
1929,historical dependencies,latent variables,[],"Inspired by , we add [[ historical dependencies ]] on the [[ latent variables ]] of VAEs and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .",0
1930,historical dependencies,VAEs,[],"Inspired by , we add [[ historical dependencies ]] on the latent variables of [[ VAEs ]] and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .",0
1931,historical dependencies,deep recurrent generative decoder ( DRGD ),[],"Inspired by , we add [[ historical dependencies ]] on the latent variables of VAEs and propose a [[ deep recurrent generative decoder ( DRGD ) ]] for latent structure modeling .",0
1932,historical dependencies,latent structure modeling,[],"Inspired by , we add [[ historical dependencies ]] on the latent variables of VAEs and propose a deep recurrent generative decoder ( DRGD ) for [[ latent structure modeling ]] .",0
1933,latent variables,VAEs,[],"Inspired by , we add historical dependencies on the [[ latent variables ]] of [[ VAEs ]] and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .",0
1934,latent variables,deep recurrent generative decoder ( DRGD ),[],"Inspired by , we add historical dependencies on the [[ latent variables ]] of VAEs and propose a [[ deep recurrent generative decoder ( DRGD ) ]] for latent structure modeling .",0
1935,latent variables,latent structure modeling,[],"Inspired by , we add historical dependencies on the [[ latent variables ]] of VAEs and propose a deep recurrent generative decoder ( DRGD ) for [[ latent structure modeling ]] .",0
1936,VAEs,deep recurrent generative decoder ( DRGD ),[],"Inspired by , we add historical dependencies on the latent variables of [[ VAEs ]] and propose a [[ deep recurrent generative decoder ( DRGD ) ]] for latent structure modeling .",0
1937,VAEs,latent structure modeling,[],"Inspired by , we add historical dependencies on the latent variables of [[ VAEs ]] and propose a deep recurrent generative decoder ( DRGD ) for [[ latent structure modeling ]] .",0
1938,deep recurrent generative decoder ( DRGD ),latent structure modeling,[],"Inspired by , we add historical dependencies on the latent variables of VAEs and propose a [[ deep recurrent generative decoder ( DRGD ) ]] for [[ latent structure modeling ]] .",0
1939,results,official leaderboard of SNLI,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the [[ results ]] from the [[ official leaderboard of SNLI ]] in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1940,results,DiSAN,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the [[ results ]] from the official leaderboard of SNLI in , [[ DiSAN ]] outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1941,results,previous works,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the [[ results ]] from the official leaderboard of SNLI in , DiSAN outperforms [[ previous works ]] and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1942,results,best latest test accuracy,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the [[ results ]] from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the [[ best latest test accuracy ]] ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1943,results,memory - based NSE encoder network,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the [[ results ]] from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a [[ memory - based NSE encoder network ]] ) by a remarkable margin of 1.02 % .",0
1944,results,remarkable margin,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the [[ results ]] from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a [[ remarkable margin ]] of 1.02 % .",0
1945,results,1.02 %,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the [[ results ]] from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of [[ 1.02 % ]] .",0
1946,official leaderboard of SNLI,DiSAN,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the [[ official leaderboard of SNLI ]] in , [[ DiSAN ]] outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",1
1947,official leaderboard of SNLI,previous works,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the [[ official leaderboard of SNLI ]] in , DiSAN outperforms [[ previous works ]] and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1948,official leaderboard of SNLI,best latest test accuracy,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the [[ official leaderboard of SNLI ]] in , DiSAN outperforms previous works and improves the [[ best latest test accuracy ]] ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1949,official leaderboard of SNLI,memory - based NSE encoder network,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the [[ official leaderboard of SNLI ]] in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a [[ memory - based NSE encoder network ]] ) by a remarkable margin of 1.02 % .",0
1950,official leaderboard of SNLI,remarkable margin,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the [[ official leaderboard of SNLI ]] in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a [[ remarkable margin ]] of 1.02 % .",0
1951,official leaderboard of SNLI,1.02 %,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the [[ official leaderboard of SNLI ]] in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of [[ 1.02 % ]] .",0
1952,DiSAN,previous works,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , [[ DiSAN ]] outperforms [[ previous works ]] and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1953,DiSAN,best latest test accuracy,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , [[ DiSAN ]] outperforms previous works and improves the [[ best latest test accuracy ]] ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1954,DiSAN,memory - based NSE encoder network,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , [[ DiSAN ]] outperforms previous works and improves the best latest test accuracy ( achieved by a [[ memory - based NSE encoder network ]] ) by a remarkable margin of 1.02 % .",0
1955,DiSAN,remarkable margin,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , [[ DiSAN ]] outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a [[ remarkable margin ]] of 1.02 % .",0
1956,DiSAN,1.02 %,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , [[ DiSAN ]] outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of [[ 1.02 % ]] .",0
1957,previous works,best latest test accuracy,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms [[ previous works ]] and improves the [[ best latest test accuracy ]] ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",0
1958,previous works,memory - based NSE encoder network,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms [[ previous works ]] and improves the best latest test accuracy ( achieved by a [[ memory - based NSE encoder network ]] ) by a remarkable margin of 1.02 % .",0
1959,previous works,remarkable margin,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms [[ previous works ]] and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a [[ remarkable margin ]] of 1.02 % .",0
1960,previous works,1.02 %,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms [[ previous works ]] and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of [[ 1.02 % ]] .",0
1961,best latest test accuracy,memory - based NSE encoder network,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the [[ best latest test accuracy ]] ( achieved by a [[ memory - based NSE encoder network ]] ) by a remarkable margin of 1.02 % .",0
1962,best latest test accuracy,remarkable margin,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the [[ best latest test accuracy ]] ( achieved by a memory - based NSE encoder network ) by a [[ remarkable margin ]] of 1.02 % .",0
1963,best latest test accuracy,1.02 %,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the [[ best latest test accuracy ]] ( achieved by a memory - based NSE encoder network ) by a remarkable margin of [[ 1.02 % ]] .",0
1964,memory - based NSE encoder network,remarkable margin,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a [[ memory - based NSE encoder network ]] ) by a [[ remarkable margin ]] of 1.02 % .",0
1965,memory - based NSE encoder network,1.02 %,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a [[ memory - based NSE encoder network ]] ) by a remarkable margin of [[ 1.02 % ]] .",0
1966,remarkable margin,1.02 %,"[['official leaderboard of SNLI', 'has', 'DiSAN']]","Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a [[ remarkable margin ]] of [[ 1.02 % ]] .",0
1967,recurrent neural network ( RNN ) based decoder,product - aware review representation and attributes,[],"Eventually , we propose a [[ recurrent neural network ( RNN ) based decoder ]] , which combines [[ product - aware review representation and attributes ]] to generate the answer .",0
1968,recurrent neural network ( RNN ) based decoder,answer,[],"Eventually , we propose a [[ recurrent neural network ( RNN ) based decoder ]] , which combines product - aware review representation and attributes to generate the [[ answer ]] .",0
1969,product - aware review representation and attributes,answer,[],"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which combines [[ product - aware review representation and attributes ]] to generate the [[ answer ]] .",0
1970,Adam optimizer,"standard settings ( Kingma and Ba , 2014 )",[],"We used the [[ Adam optimizer ]] with the [[ standard settings ( Kingma and Ba , 2014 ) ]] and a learning rate of 0.003 .",0
1971,Adam optimizer,0.003,[],"We used the [[ Adam optimizer ]] with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of [[ 0.003 ]] .",0
1972,"standard settings ( Kingma and Ba , 2014 )",0.003,[],"We used the Adam optimizer with the [[ standard settings ( Kingma and Ba , 2014 ) ]] and a learning rate of [[ 0.003 ]] .",0
1973,Initial learning rate,0.5,[],[[ Initial learning rate ]] is set to [[ 0.5 ]] .,0
1974,Both the coarse - grain module and the fine - grain module,model performance,[],[[ Both the coarse - grain module and the fine - grain module ]] significantly contribute to [[ model performance ]] .,0
1975,IMN ?d wo DE,DECNN - dTrans,[],"[[ IMN ?d wo DE ]] is competitive with [[ DECNN - dTrans ]] even without utilizing additional knowledge , which suggests the effectiveness of the proposed network structure .",0
1976,sliding window,linear distance between matched words,[],"We also use a [[ sliding window ]] acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the [[ linear distance between matched words ]] .",0
1977,each of the competing spans,global normalization,[],"We demonstrate that directly classifying [[ each of the competing spans ]] , and training with [[ global normalization ]] over all possible spans , leads to a significant increase in performance .",0
1978,each of the competing spans,possible spans,[],"We demonstrate that directly classifying [[ each of the competing spans ]] , and training with global normalization over all [[ possible spans ]] , leads to a significant increase in performance .",0
1979,each of the competing spans,significant increase in performance,[],"We demonstrate that directly classifying [[ each of the competing spans ]] , and training with global normalization over all possible spans , leads to a [[ significant increase in performance ]] .",0
1980,global normalization,possible spans,[],"We demonstrate that directly classifying each of the competing spans , and training with [[ global normalization ]] over all [[ possible spans ]] , leads to a significant increase in performance .",0
1981,global normalization,significant increase in performance,[],"We demonstrate that directly classifying each of the competing spans , and training with [[ global normalization ]] over all possible spans , leads to a [[ significant increase in performance ]] .",0
1982,possible spans,significant increase in performance,[],"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all [[ possible spans ]] , leads to a [[ significant increase in performance ]] .",0
1983,RMR + ELMo + Verifier,best precision,[],We observe that [[ RMR + ELMo + Verifier ]] achieves the [[ best precision ]] when the recall is less than 80 .,0
1984,RMR + ELMo + Verifier,recall,[],We observe that [[ RMR + ELMo + Verifier ]] achieves the best precision when the [[ recall ]] is less than 80 .,0
1985,RMR + ELMo + Verifier,less than 80,[],We observe that [[ RMR + ELMo + Verifier ]] achieves the best precision when the recall is [[ less than 80 ]] .,0
1986,best precision,recall,[],We observe that RMR + ELMo + Verifier achieves the [[ best precision ]] when the [[ recall ]] is less than 80 .,0
1987,best precision,less than 80,[],We observe that RMR + ELMo + Verifier achieves the [[ best precision ]] when the recall is [[ less than 80 ]] .,0
1988,recall,less than 80,[],We observe that RMR + ELMo + Verifier achieves the best precision when the [[ recall ]] is [[ less than 80 ]] .,0
1989,texts,tokenized,[],"All the [[ texts ]] are [[ tokenized ]] using wordpieces , and the maximum input length is set to 384 for both of SQuAD and RACE .",0
1990,texts,wordpieces,[],"All the [[ texts ]] are tokenized using [[ wordpieces ]] , and the maximum input length is set to 384 for both of SQuAD and RACE .",0
1991,texts,maximum input length,[],"All the [[ texts ]] are tokenized using wordpieces , and the [[ maximum input length ]] is set to 384 for both of SQuAD and RACE .",0
1992,texts,384,[],"All the [[ texts ]] are tokenized using wordpieces , and the maximum input length is set to [[ 384 ]] for both of SQuAD and RACE .",0
1993,texts,SQuAD and RACE,[],"All the [[ texts ]] are tokenized using wordpieces , and the maximum input length is set to 384 for both of [[ SQuAD and RACE ]] .",0
1994,tokenized,wordpieces,[],"All the texts are [[ tokenized ]] using [[ wordpieces ]] , and the maximum input length is set to 384 for both of SQuAD and RACE .",0
1995,tokenized,maximum input length,[],"All the texts are [[ tokenized ]] using wordpieces , and the [[ maximum input length ]] is set to 384 for both of SQuAD and RACE .",0
1996,tokenized,384,[],"All the texts are [[ tokenized ]] using wordpieces , and the maximum input length is set to [[ 384 ]] for both of SQuAD and RACE .",0
1997,tokenized,SQuAD and RACE,[],"All the texts are [[ tokenized ]] using wordpieces , and the maximum input length is set to 384 for both of [[ SQuAD and RACE ]] .",0
1998,wordpieces,maximum input length,[],"All the texts are tokenized using [[ wordpieces ]] , and the [[ maximum input length ]] is set to 384 for both of SQuAD and RACE .",0
1999,wordpieces,384,[],"All the texts are tokenized using [[ wordpieces ]] , and the maximum input length is set to [[ 384 ]] for both of SQuAD and RACE .",0
2000,wordpieces,SQuAD and RACE,[],"All the texts are tokenized using [[ wordpieces ]] , and the maximum input length is set to 384 for both of [[ SQuAD and RACE ]] .",0
2001,maximum input length,384,[],"All the texts are tokenized using wordpieces , and the [[ maximum input length ]] is set to [[ 384 ]] for both of SQuAD and RACE .",0
2002,maximum input length,SQuAD and RACE,[],"All the texts are tokenized using wordpieces , and the [[ maximum input length ]] is set to 384 for both of [[ SQuAD and RACE ]] .",0
2003,384,SQuAD and RACE,[],"All the texts are tokenized using wordpieces , and the maximum input length is set to [[ 384 ]] for both of [[ SQuAD and RACE ]] .",0
2004,summed objective function,model 's output,[],"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the [[ model 's output ]] over all locations the answer text occurs .",0
2005,summed objective function,all locations,[],"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the model 's output over [[ all locations ]] the answer text occurs .",0
2006,summed objective function,answer text,[],"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the model 's output over all locations the [[ answer text ]] occurs .",0
2007,model 's output,all locations,[],"To handle the noise this creates , we use a summed objective function that marginalizes the [[ model 's output ]] over [[ all locations ]] the answer text occurs .",0
2008,model 's output,answer text,[],"To handle the noise this creates , we use a summed objective function that marginalizes the [[ model 's output ]] over all locations the [[ answer text ]] occurs .",0
2009,all locations,answer text,[],"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output over [[ all locations ]] the [[ answer text ]] occurs .",0
2010,spans,10,[],The BoW model is trained on [[ spans ]] up to length [[ 10 ]] to keep the computation tractable .,0
2011,question word baseline,any query,[],The [[ question word baseline ]] that classifies [[ any query ]] starting with a question word word n-grams char n-grams POS n -grams pwf ( q ) :,0
2012,question word baseline,question word,[],The [[ question word baseline ]] that classifies any query starting with a [[ question word ]] word n-grams char n-grams POS n -grams pwf ( q ) :,0
2013,any query,question word,[],The question word baseline that classifies [[ any query ]] starting with a [[ question word ]] word n-grams char n-grams POS n -grams pwf ( q ) :,0
2014,significant boost,performance over all,[],We observe a [[ significant boost ]] in [[ performance over all ]] compared to the model trained only on SLNI .,0
2015,significant boost,model,[],We observe a [[ significant boost ]] in performance over all compared to the [[ model ]] trained only on SLNI .,0
2016,significant boost,SLNI,[],We observe a [[ significant boost ]] in performance over all compared to the model trained only on [[ SLNI ]] .,0
2017,performance over all,model,[],We observe a significant boost in [[ performance over all ]] compared to the [[ model ]] trained only on SLNI .,0
2018,performance over all,SLNI,[],We observe a significant boost in [[ performance over all ]] compared to the model trained only on [[ SLNI ]] .,0
2019,model,SLNI,[],We observe a significant boost in performance over all compared to the [[ model ]] trained only on [[ SLNI ]] .,0
2020,dimensions,characterlevel embedding and word embedding ( Glo Ve ),[],"In our experiments , the [[ dimensions ]] of [[ characterlevel embedding and word embedding ( Glo Ve ) ]] are both set to 300 .",0
2021,dimensions,300,[],"In our experiments , the [[ dimensions ]] of characterlevel embedding and word embedding ( Glo Ve ) are both set to [[ 300 ]] .",0
2022,characterlevel embedding and word embedding ( Glo Ve ),300,[],"In our experiments , the dimensions of [[ characterlevel embedding and word embedding ( Glo Ve ) ]] are both set to [[ 300 ]] .",0
2023,inter-attention alignment features,naturally impact,"[['inter-attention alignment features', 'has', 'naturally impact'], ['naturally impact', 'has', 'model performance'], ['model performance', 'has', 'significantly']]","In ( 4 ) , we remove the [[ inter-attention alignment features ]] , which [[ naturally impact ]] the model performance significantly .",1
2024,inter-attention alignment features,model performance,"[['inter-attention alignment features', 'has', 'naturally impact'], ['naturally impact', 'has', 'model performance'], ['model performance', 'has', 'significantly']]","In ( 4 ) , we remove the [[ inter-attention alignment features ]] , which naturally impact the [[ model performance ]] significantly .",0
2025,inter-attention alignment features,significantly,"[['inter-attention alignment features', 'has', 'naturally impact'], ['naturally impact', 'has', 'model performance'], ['model performance', 'has', 'significantly']]","In ( 4 ) , we remove the [[ inter-attention alignment features ]] , which naturally impact the model performance [[ significantly ]] .",0
2026,naturally impact,model performance,"[['inter-attention alignment features', 'has', 'naturally impact'], ['naturally impact', 'has', 'model performance'], ['model performance', 'has', 'significantly']]","In ( 4 ) , we remove the inter-attention alignment features , which [[ naturally impact ]] the [[ model performance ]] significantly .",1
2027,naturally impact,significantly,"[['inter-attention alignment features', 'has', 'naturally impact'], ['naturally impact', 'has', 'model performance'], ['model performance', 'has', 'significantly']]","In ( 4 ) , we remove the inter-attention alignment features , which [[ naturally impact ]] the model performance [[ significantly ]] .",0
2028,model performance,significantly,"[['inter-attention alignment features', 'has', 'naturally impact'], ['naturally impact', 'has', 'model performance'], ['model performance', 'has', 'significantly']]","In ( 4 ) , we remove the inter-attention alignment features , which naturally impact the [[ model performance ]] [[ significantly ]] .",1
2029,Traditional ILP method,better,[],( 5 ) The [[ Traditional ILP method ]] also works [[ better ]] than the LSTM and LSTM + methods in the out - of - domain setting .,0
2030,Traditional ILP method,LSTM and LSTM + methods,[],( 5 ) The [[ Traditional ILP method ]] also works better than the [[ LSTM and LSTM + methods ]] in the out - of - domain setting .,0
2031,Traditional ILP method,out - of - domain setting,[],( 5 ) The [[ Traditional ILP method ]] also works better than the LSTM and LSTM + methods in the [[ out - of - domain setting ]] .,0
2032,better,LSTM and LSTM + methods,[],( 5 ) The Traditional ILP method also works [[ better ]] than the [[ LSTM and LSTM + methods ]] in the out - of - domain setting .,0
2033,better,out - of - domain setting,[],( 5 ) The Traditional ILP method also works [[ better ]] than the LSTM and LSTM + methods in the [[ out - of - domain setting ]] .,0
2034,LSTM and LSTM + methods,out - of - domain setting,[],( 5 ) The Traditional ILP method also works better than the [[ LSTM and LSTM + methods ]] in the [[ out - of - domain setting ]] .,0
2035,mini- batch size,16 or 32,[],The [[ mini- batch size ]] was set to [[ 16 or 32 ]] .,0
2036,character embeddings,WDW and CBT,[],"Finally , removing the [[ character embeddings ]] , which were only used for [[ WDW and CBT ]] , leads to a reduction of about 1 % in the performance .",0
2037,character embeddings,reduction,[],"Finally , removing the [[ character embeddings ]] , which were only used for WDW and CBT , leads to a [[ reduction ]] of about 1 % in the performance .",0
2038,character embeddings,about 1 % in the performance,[],"Finally , removing the [[ character embeddings ]] , which were only used for WDW and CBT , leads to a reduction of [[ about 1 % in the performance ]] .",0
2039,WDW and CBT,reduction,[],"Finally , removing the character embeddings , which were only used for [[ WDW and CBT ]] , leads to a [[ reduction ]] of about 1 % in the performance .",0
2040,WDW and CBT,about 1 % in the performance,[],"Finally , removing the character embeddings , which were only used for [[ WDW and CBT ]] , leads to a reduction of [[ about 1 % in the performance ]] .",0
2041,reduction,about 1 % in the performance,[],"Finally , removing the character embeddings , which were only used for WDW and CBT , leads to a [[ reduction ]] of [[ about 1 % in the performance ]] .",0
2042,3 - way multi-task model,logical entailment,[],"Hence , our [[ 3 - way multi-task model ]] generates summaries that are both better at [[ logical entailment ]] and contain more salient information .",0
2043,3 - way multi-task model,contain more salient information,[],"Hence , our [[ 3 - way multi-task model ]] generates summaries that are both better at logical entailment and [[ contain more salient information ]] .",0
2044,logical entailment,contain more salient information,[],"Hence , our 3 - way multi-task model generates summaries that are both better at [[ logical entailment ]] and [[ contain more salient information ]] .",0
2045,false alignment,Contrastive Feature Alignment ( CFA ),[],"To prevent [[ false alignment ]] , we adopt the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) to semantically align aspect - specific representations .",0
2046,false alignment,aspect - specific representations,[],"To prevent [[ false alignment ]] , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align [[ aspect - specific representations ]] .",0
2047,Contrastive Feature Alignment ( CFA ),aspect - specific representations,[],"To prevent false alignment , we adopt the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) to semantically align [[ aspect - specific representations ]] .",0
2048,RNTN,highest performance,[],"The [[ RNTN ]] gets the [[ highest performance ]] , followed by the MV - RNN and RNN .",0
2049,RNTN,MV - RNN and RNN,[],"The [[ RNTN ]] gets the highest performance , followed by the [[ MV - RNN and RNN ]] .",0
2050,highest performance,MV - RNN and RNN,[],"The RNTN gets the [[ highest performance ]] , followed by the [[ MV - RNN and RNN ]] .",0
2051,nl,account,"[['nl', 'has', 'averaged LAS score']]","Without taking "" [[ nl ]] "" into [[ account ]] , our averaged LAS score over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .",0
2052,nl,averaged LAS score,"[['nl', 'has', 'averaged LAS score']]","Without taking "" [[ nl ]] "" into account , our [[ averaged LAS score ]] over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .",1
2053,nl,all remaining languages,"[['nl', 'has', 'averaged LAS score']]","Without taking "" [[ nl ]] "" into account , our averaged LAS score over [[ all remaining languages ]] is 1.1 % absolute higher than Stack - propagation 's .",0
2054,nl,1.1 % absolute higher,"[['nl', 'has', 'averaged LAS score']]","Without taking "" [[ nl ]] "" into account , our averaged LAS score over all remaining languages is [[ 1.1 % absolute higher ]] than Stack - propagation 's .",0
2055,nl,Stack - propagation 's,"[['nl', 'has', 'averaged LAS score']]","Without taking "" [[ nl ]] "" into account , our averaged LAS score over all remaining languages is 1.1 % absolute higher than [[ Stack - propagation 's ]] .",0
2056,account,averaged LAS score,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into [[ account ]] , our [[ averaged LAS score ]] over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .",0
2057,account,all remaining languages,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into [[ account ]] , our averaged LAS score over [[ all remaining languages ]] is 1.1 % absolute higher than Stack - propagation 's .",0
2058,account,1.1 % absolute higher,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into [[ account ]] , our averaged LAS score over all remaining languages is [[ 1.1 % absolute higher ]] than Stack - propagation 's .",0
2059,account,Stack - propagation 's,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into [[ account ]] , our averaged LAS score over all remaining languages is 1.1 % absolute higher than [[ Stack - propagation 's ]] .",0
2060,averaged LAS score,all remaining languages,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into account , our [[ averaged LAS score ]] over [[ all remaining languages ]] is 1.1 % absolute higher than Stack - propagation 's .",0
2061,averaged LAS score,1.1 % absolute higher,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into account , our [[ averaged LAS score ]] over all remaining languages is [[ 1.1 % absolute higher ]] than Stack - propagation 's .",0
2062,averaged LAS score,Stack - propagation 's,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into account , our [[ averaged LAS score ]] over all remaining languages is 1.1 % absolute higher than [[ Stack - propagation 's ]] .",0
2063,all remaining languages,1.1 % absolute higher,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into account , our averaged LAS score over [[ all remaining languages ]] is [[ 1.1 % absolute higher ]] than Stack - propagation 's .",0
2064,all remaining languages,Stack - propagation 's,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into account , our averaged LAS score over [[ all remaining languages ]] is 1.1 % absolute higher than [[ Stack - propagation 's ]] .",0
2065,1.1 % absolute higher,Stack - propagation 's,"[['nl', 'has', 'averaged LAS score']]","Without taking "" nl "" into account , our averaged LAS score over all remaining languages is [[ 1.1 % absolute higher ]] than [[ Stack - propagation 's ]] .",0
2066,several methods,constructing,[],"In this paper , we investigate [[ several methods ]] of [[ constructing ]] an auxiliary sentence and transform ( T ) ABSA into a sentence - pair classification task .",0
2067,several methods,auxiliary sentence,[],"In this paper , we investigate [[ several methods ]] of constructing an [[ auxiliary sentence ]] and transform ( T ) ABSA into a sentence - pair classification task .",0
2068,several methods,( T ) ABSA,[],"In this paper , we investigate [[ several methods ]] of constructing an auxiliary sentence and transform [[ ( T ) ABSA ]] into a sentence - pair classification task .",0
2069,several methods,sentence - pair classification task,[],"In this paper , we investigate [[ several methods ]] of constructing an auxiliary sentence and transform ( T ) ABSA into a [[ sentence - pair classification task ]] .",0
2070,constructing,auxiliary sentence,[],"In this paper , we investigate several methods of [[ constructing ]] an [[ auxiliary sentence ]] and transform ( T ) ABSA into a sentence - pair classification task .",0
2071,constructing,( T ) ABSA,[],"In this paper , we investigate several methods of [[ constructing ]] an auxiliary sentence and transform [[ ( T ) ABSA ]] into a sentence - pair classification task .",0
2072,constructing,sentence - pair classification task,[],"In this paper , we investigate several methods of [[ constructing ]] an auxiliary sentence and transform ( T ) ABSA into a [[ sentence - pair classification task ]] .",0
2073,auxiliary sentence,( T ) ABSA,[],"In this paper , we investigate several methods of constructing an [[ auxiliary sentence ]] and transform [[ ( T ) ABSA ]] into a sentence - pair classification task .",0
2074,auxiliary sentence,sentence - pair classification task,[],"In this paper , we investigate several methods of constructing an [[ auxiliary sentence ]] and transform ( T ) ABSA into a [[ sentence - pair classification task ]] .",0
2075,( T ) ABSA,sentence - pair classification task,[],"In this paper , we investigate several methods of constructing an auxiliary sentence and transform [[ ( T ) ABSA ]] into a [[ sentence - pair classification task ]] .",0
2076,weights,encoder embedding,[],"For all experiments , we tie the [[ weights ]] of the [[ encoder embedding ]] , the decoder embedding , and the decoder output layers .",0
2077,weights,decoder embedding,[],"For all experiments , we tie the [[ weights ]] of the encoder embedding , the [[ decoder embedding ]] , and the decoder output layers .",0
2078,weights,decoder output layers,[],"For all experiments , we tie the [[ weights ]] of the encoder embedding , the decoder embedding , and the [[ decoder output layers ]] .",0
2079,encoder embedding,decoder embedding,[],"For all experiments , we tie the weights of the [[ encoder embedding ]] , the [[ decoder embedding ]] , and the decoder output layers .",0
2080,encoder embedding,decoder output layers,[],"For all experiments , we tie the weights of the [[ encoder embedding ]] , the decoder embedding , and the [[ decoder output layers ]] .",0
2081,decoder embedding,decoder output layers,[],"For all experiments , we tie the weights of the encoder embedding , the [[ decoder embedding ]] , and the [[ decoder output layers ]] .",0
2082,200 arrays,"columns ( 165 for faces , 18 for hands , and 6 for rotation )",[],"We then average each of the [[ 200 arrays ]] along the [[ columns ( 165 for faces , 18 for hands , and 6 for rotation ) ]] , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each utterance .",0
2083,200 arrays,"( 200,189 ) dimension vector",[],"We then average each of the [[ 200 arrays ]] along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain [[ ( 200,189 ) dimension vector ]] for each utterance .",0
2084,200 arrays,utterance,[],"We then average each of the [[ 200 arrays ]] along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each [[ utterance ]] .",0
2085,"columns ( 165 for faces , 18 for hands , and 6 for rotation )","( 200,189 ) dimension vector",[],"We then average each of the 200 arrays along the [[ columns ( 165 for faces , 18 for hands , and 6 for rotation ) ]] , and finally concatenate all of them to obtain [[ ( 200,189 ) dimension vector ]] for each utterance .",0
2086,"columns ( 165 for faces , 18 for hands , and 6 for rotation )",utterance,[],"We then average each of the 200 arrays along the [[ columns ( 165 for faces , 18 for hands , and 6 for rotation ) ]] , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each [[ utterance ]] .",0
2087,"( 200,189 ) dimension vector",utterance,[],"We then average each of the 200 arrays along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain [[ ( 200,189 ) dimension vector ]] for each [[ utterance ]] .",0
2088,sequential inference models,chain models,[],"While some previous top - performing models use rather complicated network architectures to achieve the state - of - the - art results , we demonstrate in this paper that enhancing [[ sequential inference models ]] based on [[ chain models ]] can outperform all previous results , suggesting that the potentials of such sequential inference approaches have not been fully exploited yet .",0
2089,gains,1.1 - 2.0 %,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe [[ gains ]] of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .",0
2090,gains,sentiment classification tasks,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe [[ gains ]] of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",0
2091,gains,MR,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
2092,gains,CR,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
2093,gains,SUBJ,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
2094,gains,MPQA,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
2095,gains,Infersent,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
2096,1.1 - 2.0 %,sentiment classification tasks,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of [[ 1.1 - 2.0 % ]] on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",0
2097,1.1 - 2.0 %,MR,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
2098,1.1 - 2.0 %,CR,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
2099,1.1 - 2.0 %,SUBJ,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
2100,1.1 - 2.0 %,MPQA,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
2101,1.1 - 2.0 %,Infersent,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
2102,sentiment classification tasks,MR,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
2103,sentiment classification tasks,CR,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
2104,sentiment classification tasks,SUBJ,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
2105,sentiment classification tasks,MPQA,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
2106,sentiment classification tasks,Infersent,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
2107,MR,CR,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
2108,MR,SUBJ,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
2109,MR,MPQA,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
2110,MR,Infersent,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
2111,CR,SUBJ,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , [[ SUBJ ]] & MPQA ) over Infersent .",0
2112,CR,MPQA,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & [[ MPQA ]] ) over Infersent .",0
2113,CR,Infersent,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over [[ Infersent ]] .",0
2114,SUBJ,MPQA,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & [[ MPQA ]] ) over Infersent .",0
2115,SUBJ,Infersent,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over [[ Infersent ]] .",0
2116,MPQA,Infersent,"[['sentiment classification tasks', 'name', 'MR'], ['sentiment classification tasks', 'name', 'CR'], ['sentiment classification tasks', 'name', 'SUBJ'], ['sentiment classification tasks', 'name', 'MPQA']]","We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over [[ Infersent ]] .",0
2117,20 % of the training set,validation set,[],[[ 20 % of the training set ]] is used as [[ validation set ]] for hyper - parameter tuning .,0
2118,20 % of the training set,hyper - parameter tuning,[],[[ 20 % of the training set ]] is used as validation set for [[ hyper - parameter tuning ]] .,0
2119,validation set,hyper - parameter tuning,[],20 % of the training set is used as [[ validation set ]] for [[ hyper - parameter tuning ]] .,0
2120,differences,F- score,[],"The [[ differences ]] in [[ F- score ]] between the three versions of LSTM are not significant , all scores are close to 0.81 .",0
2121,differences,three versions of LSTM,[],"The [[ differences ]] in F- score between the [[ three versions of LSTM ]] are not significant , all scores are close to 0.81 .",0
2122,differences,not significant,[],"The [[ differences ]] in F- score between the three versions of LSTM are [[ not significant ]] , all scores are close to 0.81 .",0
2123,differences,scores,[],"The [[ differences ]] in F- score between the three versions of LSTM are not significant , all [[ scores ]] are close to 0.81 .",0
2124,differences,0.81,[],"The [[ differences ]] in F- score between the three versions of LSTM are not significant , all scores are close to [[ 0.81 ]] .",0
2125,F- score,three versions of LSTM,[],"The differences in [[ F- score ]] between the [[ three versions of LSTM ]] are not significant , all scores are close to 0.81 .",0
2126,F- score,not significant,[],"The differences in [[ F- score ]] between the three versions of LSTM are [[ not significant ]] , all scores are close to 0.81 .",0
2127,F- score,scores,[],"The differences in [[ F- score ]] between the three versions of LSTM are not significant , all [[ scores ]] are close to 0.81 .",0
2128,F- score,0.81,[],"The differences in [[ F- score ]] between the three versions of LSTM are not significant , all scores are close to [[ 0.81 ]] .",0
2129,three versions of LSTM,not significant,[],"The differences in F- score between the [[ three versions of LSTM ]] are [[ not significant ]] , all scores are close to 0.81 .",0
2130,three versions of LSTM,scores,[],"The differences in F- score between the [[ three versions of LSTM ]] are not significant , all [[ scores ]] are close to 0.81 .",0
2131,three versions of LSTM,0.81,[],"The differences in F- score between the [[ three versions of LSTM ]] are not significant , all scores are close to [[ 0.81 ]] .",0
2132,not significant,scores,[],"The differences in F- score between the three versions of LSTM are [[ not significant ]] , all [[ scores ]] are close to 0.81 .",0
2133,not significant,0.81,[],"The differences in F- score between the three versions of LSTM are [[ not significant ]] , all scores are close to [[ 0.81 ]] .",0
2134,scores,0.81,[],"The differences in F- score between the three versions of LSTM are not significant , all [[ scores ]] are close to [[ 0.81 ]] .",0
2135,RACE,key competitors,"[['RACE', 'has', 'key competitors']]","[[ RACE ]] - the [[ key competitors ]] are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .",1
2136,RACE,Stanford Attention Reader ( Stanford AR ),"[['RACE', 'has', 'key competitors']]","[[ RACE ]] - the key competitors are the [[ Stanford Attention Reader ( Stanford AR ) ]] , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .",0
2137,RACE,Gated Attention Reader ( GA ),"[['RACE', 'has', 'key competitors']]","[[ RACE ]] - the key competitors are the Stanford Attention Reader ( Stanford AR ) , [[ Gated Attention Reader ( GA ) ]] , and Dynamic Fusion Networks ( DFN ) .",0
2138,RACE,Dynamic Fusion Networks ( DFN ),"[['RACE', 'has', 'key competitors']]","[[ RACE ]] - the key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and [[ Dynamic Fusion Networks ( DFN ) ]] .",0
2139,key competitors,Stanford Attention Reader ( Stanford AR ),"[['RACE', 'has', 'key competitors']]","RACE - the [[ key competitors ]] are the [[ Stanford Attention Reader ( Stanford AR ) ]] , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .",0
2140,key competitors,Gated Attention Reader ( GA ),"[['RACE', 'has', 'key competitors']]","RACE - the [[ key competitors ]] are the Stanford Attention Reader ( Stanford AR ) , [[ Gated Attention Reader ( GA ) ]] , and Dynamic Fusion Networks ( DFN ) .",0
2141,key competitors,Dynamic Fusion Networks ( DFN ),"[['RACE', 'has', 'key competitors']]","RACE - the [[ key competitors ]] are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and [[ Dynamic Fusion Networks ( DFN ) ]] .",0
2142,Stanford Attention Reader ( Stanford AR ),Gated Attention Reader ( GA ),"[['RACE', 'has', 'key competitors']]","RACE - the key competitors are the [[ Stanford Attention Reader ( Stanford AR ) ]] , [[ Gated Attention Reader ( GA ) ]] , and Dynamic Fusion Networks ( DFN ) .",0
2143,Stanford Attention Reader ( Stanford AR ),Dynamic Fusion Networks ( DFN ),"[['RACE', 'has', 'key competitors']]","RACE - the key competitors are the [[ Stanford Attention Reader ( Stanford AR ) ]] , Gated Attention Reader ( GA ) , and [[ Dynamic Fusion Networks ( DFN ) ]] .",0
2144,Gated Attention Reader ( GA ),Dynamic Fusion Networks ( DFN ),"[['RACE', 'has', 'key competitors']]","RACE - the key competitors are the Stanford Attention Reader ( Stanford AR ) , [[ Gated Attention Reader ( GA ) ]] , and [[ Dynamic Fusion Networks ( DFN ) ]] .",0
2145,sentiment lexicon,VADER,[],We consider a recent [[ sentiment lexicon ]] called [[ VADER ]] .,0
2146,two bidirectional GRUs,80 hidden units,[],We use [[ two bidirectional GRUs ]] with [[ 80 hidden units ]] and 25 dimensional character embeddings for the token character encoder .,0
2147,two bidirectional GRUs,25 dimensional character embeddings,[],We use [[ two bidirectional GRUs ]] with 80 hidden units and [[ 25 dimensional character embeddings ]] for the token character encoder .,0
2148,two bidirectional GRUs,token character encoder,[],We use [[ two bidirectional GRUs ]] with 80 hidden units and 25 dimensional character embeddings for the [[ token character encoder ]] .,0
2149,80 hidden units,25 dimensional character embeddings,[],We use two bidirectional GRUs with [[ 80 hidden units ]] and [[ 25 dimensional character embeddings ]] for the token character encoder .,0
2150,80 hidden units,token character encoder,[],We use two bidirectional GRUs with [[ 80 hidden units ]] and 25 dimensional character embeddings for the [[ token character encoder ]] .,0
2151,25 dimensional character embeddings,token character encoder,[],We use two bidirectional GRUs with 80 hidden units and [[ 25 dimensional character embeddings ]] for the [[ token character encoder ]] .,0
2152,contextual utterances,correlations,[],"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the [[ contextual utterances ]] by computing [[ correlations ]] among the modalities of the target utterance and the context utterances .",0
2153,contextual utterances,modalities,[],"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the [[ contextual utterances ]] by computing correlations among the [[ modalities ]] of the target utterance and the context utterances .",0
2154,contextual utterances,target utterance and the context utterances,[],"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the [[ contextual utterances ]] by computing correlations among the modalities of the [[ target utterance and the context utterances ]] .",0
2155,correlations,modalities,[],"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing [[ correlations ]] among the [[ modalities ]] of the target utterance and the context utterances .",0
2156,correlations,target utterance and the context utterances,[],"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing [[ correlations ]] among the modalities of the [[ target utterance and the context utterances ]] .",0
2157,modalities,target utterance and the context utterances,[],"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the [[ modalities ]] of the [[ target utterance and the context utterances ]] .",0
2158,twelve methods,Kullback - Leibler importance estimation procedure ( KLIEP ),[],"In addition , for comparison purpose , we use [[ twelve methods ]] including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2159,twelve methods,unconstrained least - squares importance fitting ( ULSIF ),[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2160,twelve methods,selective transfer machine ( STM ),[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2161,twelve methods,"linear SVM , transfer component analysis ( TCA )",[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2162,twelve methods,transfer component analysis ( TCA ),[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2163,twelve methods,geodesic flow kernel ( GFK ),[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2164,twelve methods,DANN,[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2165,twelve methods,DGCNN,[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2166,twelve methods,deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2167,twelve methods,BiDANN,[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2168,twelve methods,A - LSTM,[],"In addition , for comparison purpose , we use [[ twelve methods ]] including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2169,Kullback - Leibler importance estimation procedure ( KLIEP ),unconstrained least - squares importance fitting ( ULSIF ),[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2170,Kullback - Leibler importance estimation procedure ( KLIEP ),selective transfer machine ( STM ),[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2171,Kullback - Leibler importance estimation procedure ( KLIEP ),"linear SVM , transfer component analysis ( TCA )",[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2172,Kullback - Leibler importance estimation procedure ( KLIEP ),transfer component analysis ( TCA ),[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2173,Kullback - Leibler importance estimation procedure ( KLIEP ),geodesic flow kernel ( GFK ),[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2174,Kullback - Leibler importance estimation procedure ( KLIEP ),DANN,[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2175,Kullback - Leibler importance estimation procedure ( KLIEP ),DGCNN,[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2176,Kullback - Leibler importance estimation procedure ( KLIEP ),deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2177,Kullback - Leibler importance estimation procedure ( KLIEP ),BiDANN,[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2178,Kullback - Leibler importance estimation procedure ( KLIEP ),A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including [[ Kullback - Leibler importance estimation procedure ( KLIEP ) ]] , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2179,unconstrained least - squares importance fitting ( ULSIF ),selective transfer machine ( STM ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2180,unconstrained least - squares importance fitting ( ULSIF ),"linear SVM , transfer component analysis ( TCA )",[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2181,unconstrained least - squares importance fitting ( ULSIF ),transfer component analysis ( TCA ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2182,unconstrained least - squares importance fitting ( ULSIF ),geodesic flow kernel ( GFK ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2183,unconstrained least - squares importance fitting ( ULSIF ),DANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2184,unconstrained least - squares importance fitting ( ULSIF ),DGCNN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2185,unconstrained least - squares importance fitting ( ULSIF ),deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2186,unconstrained least - squares importance fitting ( ULSIF ),BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2187,unconstrained least - squares importance fitting ( ULSIF ),A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , [[ unconstrained least - squares importance fitting ( ULSIF ) ]] , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2188,selective transfer machine ( STM ),"linear SVM , transfer component analysis ( TCA )",[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2189,selective transfer machine ( STM ),transfer component analysis ( TCA ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2190,selective transfer machine ( STM ),geodesic flow kernel ( GFK ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2191,selective transfer machine ( STM ),DANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2192,selective transfer machine ( STM ),DGCNN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2193,selective transfer machine ( STM ),deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2194,selective transfer machine ( STM ),BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2195,selective transfer machine ( STM ),A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , [[ selective transfer machine ( STM ) ]] , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2196,"linear SVM , transfer component analysis ( TCA )",transfer component analysis ( TCA ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2197,"linear SVM , transfer component analysis ( TCA )",geodesic flow kernel ( GFK ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2198,"linear SVM , transfer component analysis ( TCA )",DANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2199,"linear SVM , transfer component analysis ( TCA )",DGCNN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2200,"linear SVM , transfer component analysis ( TCA )",deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2201,"linear SVM , transfer component analysis ( TCA )",BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2202,"linear SVM , transfer component analysis ( TCA )",A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , [[ linear SVM , transfer component analysis ( TCA ) ]] , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2203,transfer component analysis ( TCA ),geodesic flow kernel ( GFK ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2204,transfer component analysis ( TCA ),DANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2205,transfer component analysis ( TCA ),DGCNN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2206,transfer component analysis ( TCA ),deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2207,transfer component analysis ( TCA ),BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2208,transfer component analysis ( TCA ),A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , [[ transfer component analysis ( TCA ) ]] , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2209,geodesic flow kernel ( GFK ),DANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2210,geodesic flow kernel ( GFK ),DGCNN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2211,geodesic flow kernel ( GFK ),deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2212,geodesic flow kernel ( GFK ),BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2213,geodesic flow kernel ( GFK ),A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , [[ geodesic flow kernel ( GFK ) ]] , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2214,DANN,DGCNN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",0
2215,DANN,deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2216,DANN,BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2217,DANN,A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , [[ DANN ]] , DGCNN , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2218,DGCNN,deep adaptation network ( DAN ),[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , [[ deep adaptation network ( DAN ) ]] , BiDANN , and A - LSTM , to conduct the same experiments .",0
2219,DGCNN,BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2220,DGCNN,A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , [[ DGCNN ]] , deep adaptation network ( DAN ) , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2221,deep adaptation network ( DAN ),BiDANN,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , [[ BiDANN ]] , and A - LSTM , to conduct the same experiments .",0
2222,deep adaptation network ( DAN ),A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , [[ deep adaptation network ( DAN ) ]] , BiDANN , and [[ A - LSTM ]] , to conduct the same experiments .",0
2223,BiDANN,A - LSTM,[],"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , [[ BiDANN ]] , and [[ A - LSTM ]] , to conduct the same experiments .",0
2224,model with GLoVe embeddings ( G ),still ahead,[],"The [[ model with GLoVe embeddings ( G ) ]] is [[ still ahead ]] with a 1.1 point margin , but the gap has been shrunk .",0
2225,model with GLoVe embeddings ( G ),1.1 point margin,[],"The [[ model with GLoVe embeddings ( G ) ]] is still ahead with a [[ 1.1 point margin ]] , but the gap has been shrunk .",0
2226,still ahead,1.1 point margin,[],"The model with GLoVe embeddings ( G ) is [[ still ahead ]] with a [[ 1.1 point margin ]] , but the gap has been shrunk .",0
2227,reinforcement learning,reward,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ [[ reinforcement learning ]] with a [[ reward ]] defined by the uniformity extent of the original labels to train the model .",0
2228,reinforcement learning,uniformity extent,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ [[ reinforcement learning ]] with a reward defined by the [[ uniformity extent ]] of the original labels to train the model .",0
2229,reinforcement learning,original labels,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ [[ reinforcement learning ]] with a reward defined by the uniformity extent of the [[ original labels ]] to train the model .",0
2230,reinforcement learning,model,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ [[ reinforcement learning ]] with a reward defined by the uniformity extent of the original labels to train the [[ model ]] .",0
2231,reward,uniformity extent,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a [[ reward ]] defined by the [[ uniformity extent ]] of the original labels to train the model .",0
2232,reward,original labels,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a [[ reward ]] defined by the uniformity extent of the [[ original labels ]] to train the model .",0
2233,reward,model,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a [[ reward ]] defined by the uniformity extent of the original labels to train the [[ model ]] .",0
2234,uniformity extent,original labels,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the [[ uniformity extent ]] of the [[ original labels ]] to train the model .",0
2235,uniformity extent,model,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the [[ uniformity extent ]] of the original labels to train the [[ model ]] .",0
2236,original labels,model,[],"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the uniformity extent of the [[ original labels ]] to train the [[ model ]] .",0
2237,aspects and texts simultaneously,LSTMs,[],"Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models [[ aspects and texts simultaneously ]] using [[ LSTMs ]] .",0
2238,Dropout,keep rate 0.75,[],[[ Dropout ]] with the [[ keep rate 0.75 ]] and L2 regularization are performed to avoid overfitting .,0
2239,Dropout,L2 regularization,[],[[ Dropout ]] with the keep rate 0.75 and [[ L2 regularization ]] are performed to avoid overfitting .,0
2240,Dropout,overfitting,[],[[ Dropout ]] with the keep rate 0.75 and L2 regularization are performed to avoid [[ overfitting ]] .,0
2241,keep rate 0.75,L2 regularization,[],Dropout with the [[ keep rate 0.75 ]] and [[ L2 regularization ]] are performed to avoid overfitting .,0
2242,keep rate 0.75,overfitting,[],Dropout with the [[ keep rate 0.75 ]] and L2 regularization are performed to avoid [[ overfitting ]] .,0
2243,L2 regularization,overfitting,[],Dropout with the keep rate 0.75 and [[ L2 regularization ]] are performed to avoid [[ overfitting ]] .,0
2244,n-gram functionality,almost 5 % accuracy improvement,[],"Not surprisingly , the [[ n-gram functionality ]] is important , contributing [[ almost 5 % accuracy improvement ]] .",0
2245,impressive new state of the art,AS2,[],"Wiki QA establish an [[ impressive new state of the art ]] for [[ AS2 ]] on WikiQA of 0.920 and 0.933 in MAP and MRR , respectively .",0
2246,impressive new state of the art,WikiQA,[],"Wiki QA establish an [[ impressive new state of the art ]] for AS2 on [[ WikiQA ]] of 0.920 and 0.933 in MAP and MRR , respectively .",0
2247,impressive new state of the art,0.920 and 0.933,[],"Wiki QA establish an [[ impressive new state of the art ]] for AS2 on WikiQA of [[ 0.920 and 0.933 ]] in MAP and MRR , respectively .",0
2248,impressive new state of the art,MAP and MRR,[],"Wiki QA establish an [[ impressive new state of the art ]] for AS2 on WikiQA of 0.920 and 0.933 in [[ MAP and MRR ]] , respectively .",0
2249,AS2,WikiQA,[],"Wiki QA establish an impressive new state of the art for [[ AS2 ]] on [[ WikiQA ]] of 0.920 and 0.933 in MAP and MRR , respectively .",0
2250,AS2,0.920 and 0.933,[],"Wiki QA establish an impressive new state of the art for [[ AS2 ]] on WikiQA of [[ 0.920 and 0.933 ]] in MAP and MRR , respectively .",0
2251,AS2,MAP and MRR,[],"Wiki QA establish an impressive new state of the art for [[ AS2 ]] on WikiQA of 0.920 and 0.933 in [[ MAP and MRR ]] , respectively .",0
2252,WikiQA,0.920 and 0.933,[],"Wiki QA establish an impressive new state of the art for AS2 on [[ WikiQA ]] of [[ 0.920 and 0.933 ]] in MAP and MRR , respectively .",0
2253,WikiQA,MAP and MRR,[],"Wiki QA establish an impressive new state of the art for AS2 on [[ WikiQA ]] of 0.920 and 0.933 in [[ MAP and MRR ]] , respectively .",0
2254,0.920 and 0.933,MAP and MRR,[],"Wiki QA establish an impressive new state of the art for AS2 on WikiQA of [[ 0.920 and 0.933 ]] in [[ MAP and MRR ]] , respectively .",0
2255,BLSTM,sequential CRF,[],"On top of [[ BLSTM ]] , we use a [[ sequential CRF ]] to jointly decode labels for the whole sentence .",0
2256,BLSTM,labels,[],"On top of [[ BLSTM ]] , we use a sequential CRF to jointly decode [[ labels ]] for the whole sentence .",0
2257,BLSTM,whole sentence,[],"On top of [[ BLSTM ]] , we use a sequential CRF to jointly decode labels for the [[ whole sentence ]] .",0
2258,sequential CRF,labels,[],"On top of BLSTM , we use a [[ sequential CRF ]] to jointly decode [[ labels ]] for the whole sentence .",0
2259,sequential CRF,whole sentence,[],"On top of BLSTM , we use a [[ sequential CRF ]] to jointly decode labels for the [[ whole sentence ]] .",0
2260,labels,whole sentence,[],"On top of BLSTM , we use a sequential CRF to jointly decode [[ labels ]] for the [[ whole sentence ]] .",0
2261,generative model,stochastic parametrized policy,[],"To solve the problem that the gradient can not pass back to the [[ generative model ]] when the output is discrete , we regard the generative model as a [[ stochastic parametrized policy ]] .",0
2262,dictionary - enabled models,significantly outperform,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , [[ dictionary - enabled models ]] [[ significantly outperform ]] baseline models for sentences containing rare words .",1
2263,dictionary - enabled models,baseline models,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , [[ dictionary - enabled models ]] significantly outperform [[ baseline models ]] for sentences containing rare words .",0
2264,dictionary - enabled models,sentences,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , [[ dictionary - enabled models ]] significantly outperform baseline models for [[ sentences ]] containing rare words .",0
2265,dictionary - enabled models,rare words,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , [[ dictionary - enabled models ]] significantly outperform baseline models for sentences containing [[ rare words ]] .",0
2266,significantly outperform,baseline models,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , dictionary - enabled models [[ significantly outperform ]] [[ baseline models ]] for sentences containing rare words .",1
2267,significantly outperform,sentences,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , dictionary - enabled models [[ significantly outperform ]] baseline models for [[ sentences ]] containing rare words .",0
2268,significantly outperform,rare words,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , dictionary - enabled models [[ significantly outperform ]] baseline models for sentences containing [[ rare words ]] .",0
2269,baseline models,sentences,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , dictionary - enabled models significantly outperform [[ baseline models ]] for [[ sentences ]] containing rare words .",0
2270,baseline models,rare words,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , dictionary - enabled models significantly outperform [[ baseline models ]] for sentences containing [[ rare words ]] .",0
2271,sentences,rare words,"[['dictionary - enabled models', 'has', 'significantly outperform'], ['significantly outperform', 'has', 'baseline models']]","shows that , as expected , dictionary - enabled models significantly outperform baseline models for [[ sentences ]] containing [[ rare words ]] .",0
2272,bi-directional LSTM,each modality,[],Then a [[ bi-directional LSTM ]] is used for [[ each modality ]] to obtain contextual representations .,0
2273,bi-directional LSTM,contextual representations,[],Then a [[ bi-directional LSTM ]] is used for each modality to obtain [[ contextual representations ]] .,0
2274,each modality,contextual representations,[],Then a bi-directional LSTM is used for [[ each modality ]] to obtain [[ contextual representations ]] .,0
2275,best model,ending,[],"The [[ best model ]] that only uses the [[ ending ]] is the LSTM sequence model with ELMo embeddings , which obtains 43.6 % .",0
2276,best model,LSTM sequence model,[],"The [[ best model ]] that only uses the ending is the [[ LSTM sequence model ]] with ELMo embeddings , which obtains 43.6 % .",0
2277,best model,ELMo embeddings,[],"The [[ best model ]] that only uses the ending is the LSTM sequence model with [[ ELMo embeddings ]] , which obtains 43.6 % .",0
2278,best model,43.6 %,[],"The [[ best model ]] that only uses the ending is the LSTM sequence model with ELMo embeddings , which obtains [[ 43.6 % ]] .",0
2279,ending,LSTM sequence model,[],"The best model that only uses the [[ ending ]] is the [[ LSTM sequence model ]] with ELMo embeddings , which obtains 43.6 % .",0
2280,ending,ELMo embeddings,[],"The best model that only uses the [[ ending ]] is the LSTM sequence model with [[ ELMo embeddings ]] , which obtains 43.6 % .",0
2281,ending,43.6 %,[],"The best model that only uses the [[ ending ]] is the LSTM sequence model with ELMo embeddings , which obtains [[ 43.6 % ]] .",0
2282,LSTM sequence model,ELMo embeddings,[],"The best model that only uses the ending is the [[ LSTM sequence model ]] with [[ ELMo embeddings ]] , which obtains 43.6 % .",0
2283,LSTM sequence model,43.6 %,[],"The best model that only uses the ending is the [[ LSTM sequence model ]] with ELMo embeddings , which obtains [[ 43.6 % ]] .",0
2284,ELMo embeddings,43.6 %,[],"The best model that only uses the ending is the LSTM sequence model with [[ ELMo embeddings ]] , which obtains [[ 43.6 % ]] .",0
2285,vocabulary size,20 k,[],We use a [[ vocabulary size ]] of [[ 20 k ]] for both data sets and set the word embedding dimension to be 512 .,0
2286,vocabulary size,both data sets,[],We use a [[ vocabulary size ]] of 20 k for [[ both data sets ]] and set the word embedding dimension to be 512 .,0
2287,vocabulary size,word embedding dimension,[],We use a [[ vocabulary size ]] of 20 k for both data sets and set the [[ word embedding dimension ]] to be 512 .,0
2288,vocabulary size,512,[],We use a [[ vocabulary size ]] of 20 k for both data sets and set the word embedding dimension to be [[ 512 ]] .,0
2289,20 k,both data sets,[],We use a vocabulary size of [[ 20 k ]] for [[ both data sets ]] and set the word embedding dimension to be 512 .,0
2290,20 k,word embedding dimension,[],We use a vocabulary size of [[ 20 k ]] for both data sets and set the [[ word embedding dimension ]] to be 512 .,0
2291,20 k,512,[],We use a vocabulary size of [[ 20 k ]] for both data sets and set the word embedding dimension to be [[ 512 ]] .,0
2292,both data sets,word embedding dimension,[],We use a vocabulary size of 20 k for [[ both data sets ]] and set the [[ word embedding dimension ]] to be 512 .,0
2293,both data sets,512,[],We use a vocabulary size of 20 k for [[ both data sets ]] and set the word embedding dimension to be [[ 512 ]] .,0
2294,word embedding dimension,512,[],We use a vocabulary size of 20 k for both data sets and set the [[ word embedding dimension ]] to be [[ 512 ]] .,0
2295,novel neural network model BiHDM,bi-hemispheric discrepancy,[],"Thus , in this paper , we propose a [[ novel neural network model BiHDM ]] to learn the [[ bi-hemispheric discrepancy ]] for EEG emotion recognition .",0
2296,novel neural network model BiHDM,EEG emotion recognition,[],"Thus , in this paper , we propose a [[ novel neural network model BiHDM ]] to learn the bi-hemispheric discrepancy for [[ EEG emotion recognition ]] .",0
2297,bi-hemispheric discrepancy,EEG emotion recognition,[],"Thus , in this paper , we propose a novel neural network model BiHDM to learn the [[ bi-hemispheric discrepancy ]] for [[ EEG emotion recognition ]] .",0
2298,residual connection,two sub-layers,[],"We employ a [[ residual connection ]] around each of the [[ two sub-layers ]] , followed by layer normalization .",0
2299,residual connection,layer normalization,[],"We employ a [[ residual connection ]] around each of the two sub-layers , followed by [[ layer normalization ]] .",0
2300,two sub-layers,layer normalization,[],"We employ a residual connection around each of the [[ two sub-layers ]] , followed by [[ layer normalization ]] .",0
2301,improvement,C - AGGCN with pruned trees,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the [[ improvement ]] achieved by [[ C - AGGCN with pruned trees ]] decays when the sentence length increases .",0
2302,improvement,decays,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the [[ improvement ]] achieved by C - AGGCN with pruned trees [[ decays ]] when the sentence length increases .",0
2303,improvement,sentence length,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the [[ improvement ]] achieved by C - AGGCN with pruned trees decays when the [[ sentence length ]] increases .",0
2304,improvement,increases,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the [[ improvement ]] achieved by C - AGGCN with pruned trees decays when the sentence length [[ increases ]] .",0
2305,C - AGGCN with pruned trees,decays,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the improvement achieved by [[ C - AGGCN with pruned trees ]] [[ decays ]] when the sentence length increases .",1
2306,C - AGGCN with pruned trees,sentence length,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the improvement achieved by [[ C - AGGCN with pruned trees ]] decays when the [[ sentence length ]] increases .",0
2307,C - AGGCN with pruned trees,increases,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the improvement achieved by [[ C - AGGCN with pruned trees ]] decays when the sentence length [[ increases ]] .",0
2308,decays,sentence length,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the improvement achieved by C - AGGCN with pruned trees [[ decays ]] when the [[ sentence length ]] increases .",0
2309,decays,increases,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the improvement achieved by C - AGGCN with pruned trees [[ decays ]] when the sentence length [[ increases ]] .",0
2310,sentence length,increases,"[['C - AGGCN with pruned trees', 'has', 'decays'], ['sentence length', 'has', 'increases']]","Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the [[ sentence length ]] [[ increases ]] .",1
2311,AE - LSTM and ATAE - LSTM,modeling,[],"We can also see that [[ AE - LSTM and ATAE - LSTM ]] further emphasize the [[ modeling ]] of targets via the addition of the aspect embedding , which is also the reason of performance improvement .",0
2312,AE - LSTM and ATAE - LSTM,targets,[],"We can also see that [[ AE - LSTM and ATAE - LSTM ]] further emphasize the modeling of [[ targets ]] via the addition of the aspect embedding , which is also the reason of performance improvement .",0
2313,AE - LSTM and ATAE - LSTM,addition of the aspect embedding,[],"We can also see that [[ AE - LSTM and ATAE - LSTM ]] further emphasize the modeling of targets via the [[ addition of the aspect embedding ]] , which is also the reason of performance improvement .",0
2314,modeling,targets,[],"We can also see that AE - LSTM and ATAE - LSTM further emphasize the [[ modeling ]] of [[ targets ]] via the addition of the aspect embedding , which is also the reason of performance improvement .",0
2315,modeling,addition of the aspect embedding,[],"We can also see that AE - LSTM and ATAE - LSTM further emphasize the [[ modeling ]] of targets via the [[ addition of the aspect embedding ]] , which is also the reason of performance improvement .",0
2316,targets,addition of the aspect embedding,[],"We can also see that AE - LSTM and ATAE - LSTM further emphasize the modeling of [[ targets ]] via the [[ addition of the aspect embedding ]] , which is also the reason of performance improvement .",0
2317,arranged in sets of five,dilation rates,[],"As in the ByteNet Decoder , the residual blocks are [[ arranged in sets of five ]] with corresponding [[ dilation rates ]] of 1 , 2 , 4 , 8 and 16 .",0
2318,arranged in sets of five,"1 , 2 , 4 , 8 and 16",[],"As in the ByteNet Decoder , the residual blocks are [[ arranged in sets of five ]] with corresponding dilation rates of [[ 1 , 2 , 4 , 8 and 16 ]] .",0
2319,dilation rates,"1 , 2 , 4 , 8 and 16",[],"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding [[ dilation rates ]] of [[ 1 , 2 , 4 , 8 and 16 ]] .",0
2320,momentum,= 0.9,[],"The same considerations apply to the [[ momentum ]] , which is fixed to [[ = 0.9 ]] .",0
2321,Each tree node,tree - LSTM block,[],[[ Each tree node ]] is implemented with a [[ tree - LSTM block ]] same as in model .,0
2322,Hyper QA,Tensor - Flow,[],[[ Hyper QA ]] is implemented in [[ Tensor - Flow ]] .,0
2323,Evaluator - SLM,only a tiny improvement,[],"When further applying [[ Evaluator - SLM ]] , [[ only a tiny improvement ]] is observed ( &3 vs & 4 ) , not comparable to the improvement between # 3 and # 5 .",0
2324,all deep architectures,three - layer LSTMN,"[['all deep architectures', 'has', 'three - layer LSTMN']]","Amongst [[ all deep architectures ]] , the [[ three - layer LSTMN ]] also performs best .",1
2325,all deep architectures,best,"[['all deep architectures', 'has', 'three - layer LSTMN']]","Amongst [[ all deep architectures ]] , the three - layer LSTMN also performs [[ best ]] .",0
2326,three - layer LSTMN,best,"[['all deep architectures', 'has', 'three - layer LSTMN']]","Amongst all deep architectures , the [[ three - layer LSTMN ]] also performs [[ best ]] .",0
2327,dropouts,l 2 weight decay,[],The models were regularized by using [[ dropouts ]] and an [[ l 2 weight decay ]] .,0
2328,proposed model,domainindependent general - purpose GloVe embeddings,[],"It is impressive that the [[ proposed model ]] , equipped only with [[ domainindependent general - purpose GloVe embeddings ]] , outperforms Sentic LSTM , an approach heavily reliant on external knowledge bases and domainspecific embeddings .",0
2329,proposed model,Sentic LSTM,[],"It is impressive that the [[ proposed model ]] , equipped only with domainindependent general - purpose GloVe embeddings , outperforms [[ Sentic LSTM ]] , an approach heavily reliant on external knowledge bases and domainspecific embeddings .",0
2330,proposed model,external knowledge bases and domainspecific embeddings,[],"It is impressive that the [[ proposed model ]] , equipped only with domainindependent general - purpose GloVe embeddings , outperforms Sentic LSTM , an approach heavily reliant on [[ external knowledge bases and domainspecific embeddings ]] .",0
2331,domainindependent general - purpose GloVe embeddings,Sentic LSTM,[],"It is impressive that the proposed model , equipped only with [[ domainindependent general - purpose GloVe embeddings ]] , outperforms [[ Sentic LSTM ]] , an approach heavily reliant on external knowledge bases and domainspecific embeddings .",0
2332,domainindependent general - purpose GloVe embeddings,external knowledge bases and domainspecific embeddings,[],"It is impressive that the proposed model , equipped only with [[ domainindependent general - purpose GloVe embeddings ]] , outperforms Sentic LSTM , an approach heavily reliant on [[ external knowledge bases and domainspecific embeddings ]] .",0
2333,Sentic LSTM,external knowledge bases and domainspecific embeddings,[],"It is impressive that the proposed model , equipped only with domainindependent general - purpose GloVe embeddings , outperforms [[ Sentic LSTM ]] , an approach heavily reliant on [[ external knowledge bases and domainspecific embeddings ]] .",0
2334,two state - of - the - art methods,sequence labeling,[],"We adopt [[ two state - of - the - art methods ]] in [[ sequence labeling ]] , denoted as char - LSTM and char - CNN .",0
2335,two state - of - the - art methods,char - LSTM,[],"We adopt [[ two state - of - the - art methods ]] in sequence labeling , denoted as [[ char - LSTM ]] and char - CNN .",0
2336,two state - of - the - art methods,char - CNN,[],"We adopt [[ two state - of - the - art methods ]] in sequence labeling , denoted as char - LSTM and [[ char - CNN ]] .",0
2337,sequence labeling,char - LSTM,[],"We adopt two state - of - the - art methods in [[ sequence labeling ]] , denoted as [[ char - LSTM ]] and char - CNN .",0
2338,sequence labeling,char - CNN,[],"We adopt two state - of - the - art methods in [[ sequence labeling ]] , denoted as char - LSTM and [[ char - CNN ]] .",0
2339,char - LSTM,char - CNN,[],"We adopt two state - of - the - art methods in sequence labeling , denoted as [[ char - LSTM ]] and [[ char - CNN ]] .",0
2340,AT - LSTM,sentence hidden states,[],[[ AT - LSTM ]] combines the [[ sentence hidden states ]] from a LSTM with the aspect term embedding to generate the attention vector .,0
2341,AT - LSTM,LSTM,[],[[ AT - LSTM ]] combines the sentence hidden states from a [[ LSTM ]] with the aspect term embedding to generate the attention vector .,0
2342,AT - LSTM,aspect term embedding,[],[[ AT - LSTM ]] combines the sentence hidden states from a LSTM with the [[ aspect term embedding ]] to generate the attention vector .,0
2343,AT - LSTM,attention vector,[],[[ AT - LSTM ]] combines the sentence hidden states from a LSTM with the aspect term embedding to generate the [[ attention vector ]] .,0
2344,sentence hidden states,LSTM,[],AT - LSTM combines the [[ sentence hidden states ]] from a [[ LSTM ]] with the aspect term embedding to generate the attention vector .,0
2345,sentence hidden states,aspect term embedding,[],AT - LSTM combines the [[ sentence hidden states ]] from a LSTM with the [[ aspect term embedding ]] to generate the attention vector .,0
2346,sentence hidden states,attention vector,[],AT - LSTM combines the [[ sentence hidden states ]] from a LSTM with the aspect term embedding to generate the [[ attention vector ]] .,0
2347,LSTM,aspect term embedding,[],AT - LSTM combines the sentence hidden states from a [[ LSTM ]] with the [[ aspect term embedding ]] to generate the attention vector .,0
2348,LSTM,attention vector,[],AT - LSTM combines the sentence hidden states from a [[ LSTM ]] with the aspect term embedding to generate the [[ attention vector ]] .,0
2349,aspect term embedding,attention vector,[],AT - LSTM combines the sentence hidden states from a LSTM with the [[ aspect term embedding ]] to generate the [[ attention vector ]] .,0
2350,fully - aware attention,multi -level attention mechanism,[],"With this [[ fully - aware attention ]] , we put forward a [[ multi -level attention mechanism ]] to understand the information in the question , and exploit it layer by layer on the context side .",0
2351,fully - aware attention,information,[],"With this [[ fully - aware attention ]] , we put forward a multi -level attention mechanism to understand the [[ information ]] in the question , and exploit it layer by layer on the context side .",0
2352,fully - aware attention,question,[],"With this [[ fully - aware attention ]] , we put forward a multi -level attention mechanism to understand the information in the [[ question ]] , and exploit it layer by layer on the context side .",0
2353,fully - aware attention,layer by layer,[],"With this [[ fully - aware attention ]] , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it [[ layer by layer ]] on the context side .",0
2354,fully - aware attention,context side,[],"With this [[ fully - aware attention ]] , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it layer by layer on the [[ context side ]] .",0
2355,multi -level attention mechanism,information,[],"With this fully - aware attention , we put forward a [[ multi -level attention mechanism ]] to understand the [[ information ]] in the question , and exploit it layer by layer on the context side .",0
2356,multi -level attention mechanism,question,[],"With this fully - aware attention , we put forward a [[ multi -level attention mechanism ]] to understand the information in the [[ question ]] , and exploit it layer by layer on the context side .",0
2357,multi -level attention mechanism,layer by layer,[],"With this fully - aware attention , we put forward a [[ multi -level attention mechanism ]] to understand the information in the question , and exploit it [[ layer by layer ]] on the context side .",0
2358,multi -level attention mechanism,context side,[],"With this fully - aware attention , we put forward a [[ multi -level attention mechanism ]] to understand the information in the question , and exploit it layer by layer on the [[ context side ]] .",0
2359,information,question,[],"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the [[ information ]] in the [[ question ]] , and exploit it layer by layer on the context side .",0
2360,information,layer by layer,[],"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the [[ information ]] in the question , and exploit it [[ layer by layer ]] on the context side .",0
2361,information,context side,[],"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the [[ information ]] in the question , and exploit it layer by layer on the [[ context side ]] .",0
2362,question,layer by layer,[],"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the [[ question ]] , and exploit it [[ layer by layer ]] on the context side .",0
2363,question,context side,[],"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the [[ question ]] , and exploit it layer by layer on the [[ context side ]] .",0
2364,layer by layer,context side,[],"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it [[ layer by layer ]] on the [[ context side ]] .",0
2365,our best model,competitive results,[],"1 ) Feature - based SVM is still a strong baseline , [[ our best model ]] achieves [[ competitive results ]] on D1 and D2 without relying on so many manually - designed features and external resources .",0
2366,our best model,D1 and D2,[],"1 ) Feature - based SVM is still a strong baseline , [[ our best model ]] achieves competitive results on [[ D1 and D2 ]] without relying on so many manually - designed features and external resources .",0
2367,our best model,so many manually - designed features and external resources,[],"1 ) Feature - based SVM is still a strong baseline , [[ our best model ]] achieves competitive results on D1 and D2 without relying on [[ so many manually - designed features and external resources ]] .",0
2368,competitive results,D1 and D2,[],"1 ) Feature - based SVM is still a strong baseline , our best model achieves [[ competitive results ]] on [[ D1 and D2 ]] without relying on so many manually - designed features and external resources .",0
2369,competitive results,so many manually - designed features and external resources,[],"1 ) Feature - based SVM is still a strong baseline , our best model achieves [[ competitive results ]] on D1 and D2 without relying on [[ so many manually - designed features and external resources ]] .",0
2370,D1 and D2,so many manually - designed features and external resources,[],"1 ) Feature - based SVM is still a strong baseline , our best model achieves competitive results on [[ D1 and D2 ]] without relying on [[ so many manually - designed features and external resources ]] .",0
2371,AEN,embedding layer,[],"[[ AEN ]] mainly consists of an [[ embedding layer ]] , an attentional encoder layer , an aspect - specific attention layer , and an output layer .",0
2372,AEN,attentional encoder layer,[],"[[ AEN ]] mainly consists of an embedding layer , an [[ attentional encoder layer ]] , an aspect - specific attention layer , and an output layer .",0
2373,AEN,aspect - specific attention layer,[],"[[ AEN ]] mainly consists of an embedding layer , an attentional encoder layer , an [[ aspect - specific attention layer ]] , and an output layer .",0
2374,AEN,output layer,[],"[[ AEN ]] mainly consists of an embedding layer , an attentional encoder layer , an aspect - specific attention layer , and an [[ output layer ]] .",0
2375,embedding layer,attentional encoder layer,[],"AEN mainly consists of an [[ embedding layer ]] , an [[ attentional encoder layer ]] , an aspect - specific attention layer , and an output layer .",0
2376,embedding layer,aspect - specific attention layer,[],"AEN mainly consists of an [[ embedding layer ]] , an attentional encoder layer , an [[ aspect - specific attention layer ]] , and an output layer .",0
2377,embedding layer,output layer,[],"AEN mainly consists of an [[ embedding layer ]] , an attentional encoder layer , an aspect - specific attention layer , and an [[ output layer ]] .",0
2378,attentional encoder layer,aspect - specific attention layer,[],"AEN mainly consists of an embedding layer , an [[ attentional encoder layer ]] , an [[ aspect - specific attention layer ]] , and an output layer .",0
2379,attentional encoder layer,output layer,[],"AEN mainly consists of an embedding layer , an [[ attentional encoder layer ]] , an aspect - specific attention layer , and an [[ output layer ]] .",0
2380,aspect - specific attention layer,output layer,[],"AEN mainly consists of an embedding layer , an attentional encoder layer , an [[ aspect - specific attention layer ]] , and an [[ output layer ]] .",0
2381,superior performance,contextual one,[],"In we compare these two variants over the development set and observe [[ superior performance ]] by the [[ contextual one ]] , illustrating the benefit of contextualization and specifically per-sequence contextualization which is done separately for the question and for the passage .",0
2382,superior performance,benefit,[],"In we compare these two variants over the development set and observe [[ superior performance ]] by the contextual one , illustrating the [[ benefit ]] of contextualization and specifically per-sequence contextualization which is done separately for the question and for the passage .",0
2383,superior performance,contextualization,[],"In we compare these two variants over the development set and observe [[ superior performance ]] by the contextual one , illustrating the benefit of [[ contextualization ]] and specifically per-sequence contextualization which is done separately for the question and for the passage .",0
2384,contextual one,benefit,[],"In we compare these two variants over the development set and observe superior performance by the [[ contextual one ]] , illustrating the [[ benefit ]] of contextualization and specifically per-sequence contextualization which is done separately for the question and for the passage .",0
2385,contextual one,contextualization,[],"In we compare these two variants over the development set and observe superior performance by the [[ contextual one ]] , illustrating the benefit of [[ contextualization ]] and specifically per-sequence contextualization which is done separately for the question and for the passage .",0
2386,benefit,contextualization,[],"In we compare these two variants over the development set and observe superior performance by the contextual one , illustrating the [[ benefit ]] of [[ contextualization ]] and specifically per-sequence contextualization which is done separately for the question and for the passage .",0
2387,documents and summaries,100 and 50,[],The maximum length of [[ documents and summaries ]] is [[ 100 and 50 ]] respectively .,0
2388,encoding model,general purpose,[],The [[ encoding model ]] is designed to be as [[ general purpose ]] as possible .,0
2389,Rewrite,summary,[],"In [[ Rewrite ]] , the [[ summary ]] is generated according to the hidden states of both the sentence and template .",0
2390,Rewrite,hidden states,[],"In [[ Rewrite ]] , the summary is generated according to the [[ hidden states ]] of both the sentence and template .",0
2391,Rewrite,sentence and template,[],"In [[ Rewrite ]] , the summary is generated according to the hidden states of both the [[ sentence and template ]] .",0
2392,summary,hidden states,[],"In Rewrite , the [[ summary ]] is generated according to the [[ hidden states ]] of both the sentence and template .",0
2393,summary,sentence and template,[],"In Rewrite , the [[ summary ]] is generated according to the hidden states of both the [[ sentence and template ]] .",0
2394,hidden states,sentence and template,[],"In Rewrite , the summary is generated according to the [[ hidden states ]] of both the [[ sentence and template ]] .",0
2395,answer verification,each answer candidate,[],"Third , we conduct the [[ answer verification ]] by enabling [[ each answer candidate ]] to attend to the other candidates based on their representations .",0
2396,answer verification,other candidates,[],"Third , we conduct the [[ answer verification ]] by enabling each answer candidate to attend to the [[ other candidates ]] based on their representations .",0
2397,answer verification,their representations,[],"Third , we conduct the [[ answer verification ]] by enabling each answer candidate to attend to the other candidates based on [[ their representations ]] .",0
2398,each answer candidate,other candidates,[],"Third , we conduct the answer verification by enabling [[ each answer candidate ]] to attend to the [[ other candidates ]] based on their representations .",0
2399,each answer candidate,their representations,[],"Third , we conduct the answer verification by enabling [[ each answer candidate ]] to attend to the other candidates based on [[ their representations ]] .",0
2400,other candidates,their representations,[],"Third , we conduct the answer verification by enabling each answer candidate to attend to the [[ other candidates ]] based on [[ their representations ]] .",0
2401,reset gate,helps,"[['reset gate', 'has', 'helps']]",( b ) Adding the [[ reset gate ]] [[ helps ]] .,1
2402,Document - cue,effect,"[['Document - cue', 'has', 'effect']]","( 1 ) [[ Document - cue ]] During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the [[ effect ]] that the correct candidate is often indicated solely by the presence of certain documents in Sq .",1
2403,Document - cue,correct candidate,"[['Document - cue', 'has', 'effect']]","( 1 ) [[ Document - cue ]] During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the effect that the [[ correct candidate ]] is often indicated solely by the presence of certain documents in Sq .",0
2404,Document - cue,presence of certain documents,"[['Document - cue', 'has', 'effect']]","( 1 ) [[ Document - cue ]] During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the effect that the correct candidate is often indicated solely by the [[ presence of certain documents ]] in Sq .",0
2405,effect,correct candidate,"[['Document - cue', 'has', 'effect']]","( 1 ) Document - cue During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the [[ effect ]] that the [[ correct candidate ]] is often indicated solely by the presence of certain documents in Sq .",0
2406,effect,presence of certain documents,"[['Document - cue', 'has', 'effect']]","( 1 ) Document - cue During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the [[ effect ]] that the correct candidate is often indicated solely by the [[ presence of certain documents ]] in Sq .",0
2407,correct candidate,presence of certain documents,"[['Document - cue', 'has', 'effect']]","( 1 ) Document - cue During dataset construction we observed that certain document - answer pairs appear more frequently than others , to the effect that the [[ correct candidate ]] is often indicated solely by the [[ presence of certain documents ]] in Sq .",0
2408,official test set,76.2/84.6,"[['significantly outperforms', 'has', 'best documented result 73.2/81.8']]","Finally , our result on the [[ official test set ]] is [[ 76.2/84.6 ]] , which significantly outperforms the best documented result 73.2/81.8 .",0
2409,official test set,significantly outperforms,"[['significantly outperforms', 'has', 'best documented result 73.2/81.8']]","Finally , our result on the [[ official test set ]] is 76.2/84.6 , which [[ significantly outperforms ]] the best documented result 73.2/81.8 .",0
2410,official test set,best documented result 73.2/81.8,"[['significantly outperforms', 'has', 'best documented result 73.2/81.8']]","Finally , our result on the [[ official test set ]] is 76.2/84.6 , which significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
2411,76.2/84.6,significantly outperforms,"[['significantly outperforms', 'has', 'best documented result 73.2/81.8']]","Finally , our result on the official test set is [[ 76.2/84.6 ]] , which [[ significantly outperforms ]] the best documented result 73.2/81.8 .",0
2412,76.2/84.6,best documented result 73.2/81.8,"[['significantly outperforms', 'has', 'best documented result 73.2/81.8']]","Finally , our result on the official test set is [[ 76.2/84.6 ]] , which significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
2413,significantly outperforms,best documented result 73.2/81.8,"[['significantly outperforms', 'has', 'best documented result 73.2/81.8']]","Finally , our result on the official test set is 76.2/84.6 , which [[ significantly outperforms ]] the [[ best documented result 73.2/81.8 ]] .",1
2414,deep highway bidirectional LSTMs,constrained decoding,[],"In this paper , we show that this result can be pushed further using [[ deep highway bidirectional LSTMs ]] with [[ constrained decoding ]] , again significantly moving the state of the art ( another 2 points on CoNLL 2005 ) .",0
2415,parallel blocks,worse,[],"In the last ablation study , we can see that [[ parallel blocks ]] perform [[ worse ]] than stacked blocks , which supports the preference for deeper models over wider ones .",0
2416,parallel blocks,stacked blocks,[],"In the last ablation study , we can see that [[ parallel blocks ]] perform worse than [[ stacked blocks ]] , which supports the preference for deeper models over wider ones .",0
2417,parallel blocks,preference,[],"In the last ablation study , we can see that [[ parallel blocks ]] perform worse than stacked blocks , which supports the [[ preference ]] for deeper models over wider ones .",0
2418,parallel blocks,deeper models,[],"In the last ablation study , we can see that [[ parallel blocks ]] perform worse than stacked blocks , which supports the preference for [[ deeper models ]] over wider ones .",0
2419,parallel blocks,wider ones,[],"In the last ablation study , we can see that [[ parallel blocks ]] perform worse than stacked blocks , which supports the preference for deeper models over [[ wider ones ]] .",0
2420,worse,stacked blocks,[],"In the last ablation study , we can see that parallel blocks perform [[ worse ]] than [[ stacked blocks ]] , which supports the preference for deeper models over wider ones .",0
2421,worse,preference,[],"In the last ablation study , we can see that parallel blocks perform [[ worse ]] than stacked blocks , which supports the [[ preference ]] for deeper models over wider ones .",0
2422,worse,deeper models,[],"In the last ablation study , we can see that parallel blocks perform [[ worse ]] than stacked blocks , which supports the preference for [[ deeper models ]] over wider ones .",0
2423,worse,wider ones,[],"In the last ablation study , we can see that parallel blocks perform [[ worse ]] than stacked blocks , which supports the preference for deeper models over [[ wider ones ]] .",0
2424,stacked blocks,preference,[],"In the last ablation study , we can see that parallel blocks perform worse than [[ stacked blocks ]] , which supports the [[ preference ]] for deeper models over wider ones .",0
2425,stacked blocks,deeper models,[],"In the last ablation study , we can see that parallel blocks perform worse than [[ stacked blocks ]] , which supports the preference for [[ deeper models ]] over wider ones .",0
2426,stacked blocks,wider ones,[],"In the last ablation study , we can see that parallel blocks perform worse than [[ stacked blocks ]] , which supports the preference for deeper models over [[ wider ones ]] .",0
2427,preference,deeper models,[],"In the last ablation study , we can see that parallel blocks perform worse than stacked blocks , which supports the [[ preference ]] for [[ deeper models ]] over wider ones .",0
2428,preference,wider ones,[],"In the last ablation study , we can see that parallel blocks perform worse than stacked blocks , which supports the [[ preference ]] for deeper models over [[ wider ones ]] .",0
2429,deeper models,wider ones,[],"In the last ablation study , we can see that parallel blocks perform worse than stacked blocks , which supports the preference for [[ deeper models ]] over [[ wider ones ]] .",0
2430,50 - dimension,character - level embedding vectors,"[['50 - dimension', 'has', 'character - level embedding vectors']]",We use [[ 50 - dimension ]] [[ character - level embedding vectors ]] .,1
2431,number of hidden units and the number of layers,RNN,[],"The [[ number of hidden units and the number of layers ]] in the [[ RNN ]] for each model ( ARE , TRE , MDRE and MDREA ) are selected based on extensive hyperparameter search experiments .",0
2432,number of hidden units and the number of layers,"each model ( ARE , TRE , MDRE and MDREA )",[],"The [[ number of hidden units and the number of layers ]] in the RNN for [[ each model ( ARE , TRE , MDRE and MDREA ) ]] are selected based on extensive hyperparameter search experiments .",0
2433,number of hidden units and the number of layers,extensive hyperparameter search experiments,[],"The [[ number of hidden units and the number of layers ]] in the RNN for each model ( ARE , TRE , MDRE and MDREA ) are selected based on [[ extensive hyperparameter search experiments ]] .",0
2434,RNN,"each model ( ARE , TRE , MDRE and MDREA )",[],"The number of hidden units and the number of layers in the [[ RNN ]] for [[ each model ( ARE , TRE , MDRE and MDREA ) ]] are selected based on extensive hyperparameter search experiments .",0
2435,RNN,extensive hyperparameter search experiments,[],"The number of hidden units and the number of layers in the [[ RNN ]] for each model ( ARE , TRE , MDRE and MDREA ) are selected based on [[ extensive hyperparameter search experiments ]] .",0
2436,"each model ( ARE , TRE , MDRE and MDREA )",extensive hyperparameter search experiments,[],"The number of hidden units and the number of layers in the RNN for [[ each model ( ARE , TRE , MDRE and MDREA ) ]] are selected based on [[ extensive hyperparameter search experiments ]] .",0
2437,untranscribed speech,reverberation and background noise,[],"In contrast to the subsequent TTS model , this network is trained on [[ untranscribed speech ]] containing [[ reverberation and background noise ]] from a large number of speakers .",0
2438,untranscribed speech,large number of speakers,[],"In contrast to the subsequent TTS model , this network is trained on [[ untranscribed speech ]] containing reverberation and background noise from a [[ large number of speakers ]] .",0
2439,reverberation and background noise,large number of speakers,[],"In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing [[ reverberation and background noise ]] from a [[ large number of speakers ]] .",0
2440,mixed objective,traditional cross entropy loss,[],"To address this problem , we propose a [[ mixed objective ]] that combines [[ traditional cross entropy loss ]] over positions with a measure of word overlap trained with reinforcement learning .",0
2441,mixed objective,positions,[],"To address this problem , we propose a [[ mixed objective ]] that combines traditional cross entropy loss over [[ positions ]] with a measure of word overlap trained with reinforcement learning .",0
2442,mixed objective,measure of word overlap,[],"To address this problem , we propose a [[ mixed objective ]] that combines traditional cross entropy loss over positions with a [[ measure of word overlap ]] trained with reinforcement learning .",0
2443,mixed objective,reinforcement learning,[],"To address this problem , we propose a [[ mixed objective ]] that combines traditional cross entropy loss over positions with a measure of word overlap trained with [[ reinforcement learning ]] .",0
2444,traditional cross entropy loss,positions,[],"To address this problem , we propose a mixed objective that combines [[ traditional cross entropy loss ]] over [[ positions ]] with a measure of word overlap trained with reinforcement learning .",0
2445,traditional cross entropy loss,measure of word overlap,[],"To address this problem , we propose a mixed objective that combines [[ traditional cross entropy loss ]] over positions with a [[ measure of word overlap ]] trained with reinforcement learning .",0
2446,traditional cross entropy loss,reinforcement learning,[],"To address this problem , we propose a mixed objective that combines [[ traditional cross entropy loss ]] over positions with a measure of word overlap trained with [[ reinforcement learning ]] .",0
2447,positions,measure of word overlap,[],"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over [[ positions ]] with a [[ measure of word overlap ]] trained with reinforcement learning .",0
2448,positions,reinforcement learning,[],"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over [[ positions ]] with a measure of word overlap trained with [[ reinforcement learning ]] .",0
2449,measure of word overlap,reinforcement learning,[],"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a [[ measure of word overlap ]] trained with [[ reinforcement learning ]] .",0
2450,topic,ending option,[],Our model accounts for that by analyzing if the [[ topic ]] of an [[ ending option ]] is consistent with the preceding context .,0
2451,topic,preceding context,[],Our model accounts for that by analyzing if the [[ topic ]] of an ending option is consistent with the [[ preceding context ]] .,0
2452,ending option,preceding context,[],Our model accounts for that by analyzing if the topic of an [[ ending option ]] is consistent with the [[ preceding context ]] .,0
2453,special tokens,target entities,[],"We insert [[ special tokens ]] before and after the [[ target entities ]] before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2454,special tokens,text,[],"We insert [[ special tokens ]] before and after the target entities before feeding the [[ text ]] to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2455,special tokens,BERT,[],"We insert [[ special tokens ]] before and after the target entities before feeding the text to [[ BERT ]] for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2456,special tokens,fine - tuning,[],"We insert [[ special tokens ]] before and after the target entities before feeding the text to BERT for [[ fine - tuning ]] , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2457,special tokens,locations,[],"We insert [[ special tokens ]] before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the [[ locations ]] of the two target entities and transfer the information into the BERT model .",0
2458,special tokens,two target entities,[],"We insert [[ special tokens ]] before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the [[ two target entities ]] and transfer the information into the BERT model .",0
2459,special tokens,information,[],"We insert [[ special tokens ]] before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the [[ information ]] into the BERT model .",0
2460,special tokens,BERT model,[],"We insert [[ special tokens ]] before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the [[ BERT model ]] .",0
2461,target entities,text,[],"We insert special tokens before and after the [[ target entities ]] before feeding the [[ text ]] to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2462,target entities,BERT,[],"We insert special tokens before and after the [[ target entities ]] before feeding the text to [[ BERT ]] for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2463,target entities,fine - tuning,[],"We insert special tokens before and after the [[ target entities ]] before feeding the text to BERT for [[ fine - tuning ]] , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2464,target entities,locations,[],"We insert special tokens before and after the [[ target entities ]] before feeding the text to BERT for fine - tuning , in order to identify the [[ locations ]] of the two target entities and transfer the information into the BERT model .",0
2465,target entities,two target entities,[],"We insert special tokens before and after the [[ target entities ]] before feeding the text to BERT for fine - tuning , in order to identify the locations of the [[ two target entities ]] and transfer the information into the BERT model .",0
2466,target entities,information,[],"We insert special tokens before and after the [[ target entities ]] before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the [[ information ]] into the BERT model .",0
2467,target entities,BERT model,[],"We insert special tokens before and after the [[ target entities ]] before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the [[ BERT model ]] .",0
2468,text,BERT,[],"We insert special tokens before and after the target entities before feeding the [[ text ]] to [[ BERT ]] for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2469,text,fine - tuning,[],"We insert special tokens before and after the target entities before feeding the [[ text ]] to BERT for [[ fine - tuning ]] , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2470,text,locations,[],"We insert special tokens before and after the target entities before feeding the [[ text ]] to BERT for fine - tuning , in order to identify the [[ locations ]] of the two target entities and transfer the information into the BERT model .",0
2471,text,two target entities,[],"We insert special tokens before and after the target entities before feeding the [[ text ]] to BERT for fine - tuning , in order to identify the locations of the [[ two target entities ]] and transfer the information into the BERT model .",0
2472,text,information,[],"We insert special tokens before and after the target entities before feeding the [[ text ]] to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the [[ information ]] into the BERT model .",0
2473,text,BERT model,[],"We insert special tokens before and after the target entities before feeding the [[ text ]] to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the [[ BERT model ]] .",0
2474,BERT,fine - tuning,[],"We insert special tokens before and after the target entities before feeding the text to [[ BERT ]] for [[ fine - tuning ]] , in order to identify the locations of the two target entities and transfer the information into the BERT model .",0
2475,BERT,locations,[],"We insert special tokens before and after the target entities before feeding the text to [[ BERT ]] for fine - tuning , in order to identify the [[ locations ]] of the two target entities and transfer the information into the BERT model .",0
2476,BERT,two target entities,[],"We insert special tokens before and after the target entities before feeding the text to [[ BERT ]] for fine - tuning , in order to identify the locations of the [[ two target entities ]] and transfer the information into the BERT model .",0
2477,BERT,information,[],"We insert special tokens before and after the target entities before feeding the text to [[ BERT ]] for fine - tuning , in order to identify the locations of the two target entities and transfer the [[ information ]] into the BERT model .",0
2478,BERT,BERT model,[],"We insert special tokens before and after the target entities before feeding the text to [[ BERT ]] for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the [[ BERT model ]] .",0
2479,fine - tuning,locations,[],"We insert special tokens before and after the target entities before feeding the text to BERT for [[ fine - tuning ]] , in order to identify the [[ locations ]] of the two target entities and transfer the information into the BERT model .",0
2480,fine - tuning,two target entities,[],"We insert special tokens before and after the target entities before feeding the text to BERT for [[ fine - tuning ]] , in order to identify the locations of the [[ two target entities ]] and transfer the information into the BERT model .",0
2481,fine - tuning,information,[],"We insert special tokens before and after the target entities before feeding the text to BERT for [[ fine - tuning ]] , in order to identify the locations of the two target entities and transfer the [[ information ]] into the BERT model .",0
2482,fine - tuning,BERT model,[],"We insert special tokens before and after the target entities before feeding the text to BERT for [[ fine - tuning ]] , in order to identify the locations of the two target entities and transfer the information into the [[ BERT model ]] .",0
2483,locations,two target entities,[],"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the [[ locations ]] of the [[ two target entities ]] and transfer the information into the BERT model .",0
2484,locations,information,[],"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the [[ locations ]] of the two target entities and transfer the [[ information ]] into the BERT model .",0
2485,locations,BERT model,[],"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the [[ locations ]] of the two target entities and transfer the information into the [[ BERT model ]] .",0
2486,two target entities,information,[],"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the [[ two target entities ]] and transfer the [[ information ]] into the BERT model .",0
2487,two target entities,BERT model,[],"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the [[ two target entities ]] and transfer the information into the [[ BERT model ]] .",0
2488,information,BERT model,[],"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the [[ information ]] into the [[ BERT model ]] .",0
2489,all models,TensorFlow,[],We implement [[ all models ]] in [[ TensorFlow ]] .,0
2490,DMN +,improved dynamic memory networks,[],"[[ DMN + ]] is the [[ improved dynamic memory networks ]] , which is one of the representative architectures that achieve good performance on the VQA Task .",0
2491,DMN +,representative architectures,[],"[[ DMN + ]] is the improved dynamic memory networks , which is one of the [[ representative architectures ]] that achieve good performance on the VQA Task .",0
2492,DMN +,good performance,[],"[[ DMN + ]] is the improved dynamic memory networks , which is one of the representative architectures that achieve [[ good performance ]] on the VQA Task .",0
2493,DMN +,VQA Task,[],"[[ DMN + ]] is the improved dynamic memory networks , which is one of the representative architectures that achieve good performance on the [[ VQA Task ]] .",0
2494,improved dynamic memory networks,representative architectures,[],"DMN + is the [[ improved dynamic memory networks ]] , which is one of the [[ representative architectures ]] that achieve good performance on the VQA Task .",0
2495,improved dynamic memory networks,good performance,[],"DMN + is the [[ improved dynamic memory networks ]] , which is one of the representative architectures that achieve [[ good performance ]] on the VQA Task .",0
2496,improved dynamic memory networks,VQA Task,[],"DMN + is the [[ improved dynamic memory networks ]] , which is one of the representative architectures that achieve good performance on the [[ VQA Task ]] .",0
2497,representative architectures,good performance,[],"DMN + is the improved dynamic memory networks , which is one of the [[ representative architectures ]] that achieve [[ good performance ]] on the VQA Task .",0
2498,representative architectures,VQA Task,[],"DMN + is the improved dynamic memory networks , which is one of the [[ representative architectures ]] that achieve good performance on the [[ VQA Task ]] .",0
2499,good performance,VQA Task,[],"DMN + is the improved dynamic memory networks , which is one of the representative architectures that achieve [[ good performance ]] on the [[ VQA Task ]] .",0
2500,three intent categories,BACK - GROUND,"[['three intent categories', 'name', 'BACK - GROUND'], ['three intent categories', 'name', 'METHOD'], ['three intent categories', 'name', 'RESULTCOMPARISON']]","We consider [[ three intent categories ]] outlined in : [[ BACK - GROUND ]] , METHOD and RESULTCOMPARISON .",0
2501,three intent categories,METHOD,"[['three intent categories', 'name', 'BACK - GROUND'], ['three intent categories', 'name', 'METHOD'], ['three intent categories', 'name', 'RESULTCOMPARISON']]","We consider [[ three intent categories ]] outlined in : BACK - GROUND , [[ METHOD ]] and RESULTCOMPARISON .",0
2502,three intent categories,RESULTCOMPARISON,"[['three intent categories', 'name', 'BACK - GROUND'], ['three intent categories', 'name', 'METHOD'], ['three intent categories', 'name', 'RESULTCOMPARISON']]","We consider [[ three intent categories ]] outlined in : BACK - GROUND , METHOD and [[ RESULTCOMPARISON ]] .",0
2503,BACK - GROUND,METHOD,"[['three intent categories', 'name', 'BACK - GROUND'], ['three intent categories', 'name', 'METHOD'], ['three intent categories', 'name', 'RESULTCOMPARISON']]","We consider three intent categories outlined in : [[ BACK - GROUND ]] , [[ METHOD ]] and RESULTCOMPARISON .",0
2504,BACK - GROUND,RESULTCOMPARISON,"[['three intent categories', 'name', 'BACK - GROUND'], ['three intent categories', 'name', 'METHOD'], ['three intent categories', 'name', 'RESULTCOMPARISON']]","We consider three intent categories outlined in : [[ BACK - GROUND ]] , METHOD and [[ RESULTCOMPARISON ]] .",0
2505,METHOD,RESULTCOMPARISON,"[['three intent categories', 'name', 'BACK - GROUND'], ['three intent categories', 'name', 'METHOD'], ['three intent categories', 'name', 'RESULTCOMPARISON']]","We consider three intent categories outlined in : BACK - GROUND , [[ METHOD ]] and [[ RESULTCOMPARISON ]] .",0
2506,learning rate,0.0001,[],We set the [[ learning rate ]] as [[ 0.0001 ]] .,0
2507,sentence matching tasks,word embeddings,[],"For the [[ sentence matching tasks ]] , we initialized the [[ word embeddings ]] with 50 - dimensional Glove word vectors pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .",0
2508,sentence matching tasks,50 - dimensional Glove word vectors,[],"For the [[ sentence matching tasks ]] , we initialized the word embeddings with [[ 50 - dimensional Glove word vectors ]] pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .",0
2509,sentence matching tasks,Wikipedia 2014 and Gigaword 5,[],"For the [[ sentence matching tasks ]] , we initialized the word embeddings with 50 - dimensional Glove word vectors pretrained from [[ Wikipedia 2014 and Gigaword 5 ]] for all model variants .",0
2510,word embeddings,50 - dimensional Glove word vectors,[],"For the sentence matching tasks , we initialized the [[ word embeddings ]] with [[ 50 - dimensional Glove word vectors ]] pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .",0
2511,word embeddings,Wikipedia 2014 and Gigaword 5,[],"For the sentence matching tasks , we initialized the [[ word embeddings ]] with 50 - dimensional Glove word vectors pretrained from [[ Wikipedia 2014 and Gigaword 5 ]] for all model variants .",0
2512,50 - dimensional Glove word vectors,Wikipedia 2014 and Gigaword 5,[],"For the sentence matching tasks , we initialized the word embeddings with [[ 50 - dimensional Glove word vectors ]] pretrained from [[ Wikipedia 2014 and Gigaword 5 ]] for all model variants .",0
2513,L2 regularized dot product,"[ 1 , 0.1 , 0.01 ]",[],"In the case of using [[ L2 regularized dot product ]] , ? ( regularization strength ) was chosen from [[ [ 1 , 0.1 , 0.01 ] ]] .",0
2514,Adam optimizer,learning rate,[],We adopt the [[ Adam optimizer ]] with a [[ learning rate ]] of 0.0003/ 0.001/0.001 for RACE / SearchQA / NarrativeQA respectively .,0
2515,Adam optimizer,0.0003/ 0.001/0.001,[],We adopt the [[ Adam optimizer ]] with a learning rate of [[ 0.0003/ 0.001/0.001 ]] for RACE / SearchQA / NarrativeQA respectively .,0
2516,Adam optimizer,RACE / SearchQA / NarrativeQA,[],We adopt the [[ Adam optimizer ]] with a learning rate of 0.0003/ 0.001/0.001 for [[ RACE / SearchQA / NarrativeQA ]] respectively .,0
2517,learning rate,0.0003/ 0.001/0.001,[],We adopt the Adam optimizer with a [[ learning rate ]] of [[ 0.0003/ 0.001/0.001 ]] for RACE / SearchQA / NarrativeQA respectively .,0
2518,learning rate,RACE / SearchQA / NarrativeQA,[],We adopt the Adam optimizer with a [[ learning rate ]] of 0.0003/ 0.001/0.001 for [[ RACE / SearchQA / NarrativeQA ]] respectively .,0
2519,0.0003/ 0.001/0.001,RACE / SearchQA / NarrativeQA,[],We adopt the Adam optimizer with a learning rate of [[ 0.0003/ 0.001/0.001 ]] for [[ RACE / SearchQA / NarrativeQA ]] respectively .,0
2520,training,exponential moving average,[],"During the [[ training ]] , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these averages at test time .",0
2521,training,weights,[],"During the [[ training ]] , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these averages at test time .",0
2522,training,0.001 decay,[],"During the [[ training ]] , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these averages at test time .",0
2523,training,averages,[],"During the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and use these [[ averages ]] at test time .",0
2524,training,test time,[],"During the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and use these averages at [[ test time ]] .",0
2525,exponential moving average,weights,[],"During the training , we keep the [[ exponential moving average ]] of [[ weights ]] with 0.001 decay and use these averages at test time .",0
2526,exponential moving average,0.001 decay,[],"During the training , we keep the [[ exponential moving average ]] of weights with [[ 0.001 decay ]] and use these averages at test time .",0
2527,exponential moving average,averages,[],"During the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these [[ averages ]] at test time .",0
2528,exponential moving average,test time,[],"During the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these averages at [[ test time ]] .",0
2529,weights,0.001 decay,[],"During the training , we keep the exponential moving average of [[ weights ]] with [[ 0.001 decay ]] and use these averages at test time .",0
2530,weights,averages,[],"During the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these [[ averages ]] at test time .",0
2531,weights,test time,[],"During the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these averages at [[ test time ]] .",0
2532,0.001 decay,averages,[],"During the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these [[ averages ]] at test time .",0
2533,0.001 decay,test time,[],"During the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these averages at [[ test time ]] .",0
2534,averages,test time,[],"During the training , we keep the exponential moving average of weights with 0.001 decay and use these [[ averages ]] at [[ test time ]] .",0
2535,combination ( PRET + MULT ),better results,[],The [[ combination ( PRET + MULT ) ]] over all yields [[ better results ]] .,0
2536,question decoder,appropriate question,[],This embedding is used by a [[ question decoder ]] to decode the [[ appropriate question ]] .,0
2537,stacked bi - LSTMs,3 layers,"[['stacked bi - LSTMs', 'has', '3 layers']]",Our [[ stacked bi - LSTMs ]] ( Section 3.1 ) has [[ 3 layers ]] with 200 - dimensional hidden states and highway connections .,1
2538,stacked bi - LSTMs,200 - dimensional hidden states,"[['stacked bi - LSTMs', 'has', '3 layers']]",Our [[ stacked bi - LSTMs ]] ( Section 3.1 ) has 3 layers with [[ 200 - dimensional hidden states ]] and highway connections .,0
2539,stacked bi - LSTMs,highway connections,"[['stacked bi - LSTMs', 'has', '3 layers']]",Our [[ stacked bi - LSTMs ]] ( Section 3.1 ) has 3 layers with 200 - dimensional hidden states and [[ highway connections ]] .,0
2540,3 layers,200 - dimensional hidden states,"[['stacked bi - LSTMs', 'has', '3 layers']]",Our stacked bi - LSTMs ( Section 3.1 ) has [[ 3 layers ]] with [[ 200 - dimensional hidden states ]] and highway connections .,0
2541,3 layers,highway connections,"[['stacked bi - LSTMs', 'has', '3 layers']]",Our stacked bi - LSTMs ( Section 3.1 ) has [[ 3 layers ]] with 200 - dimensional hidden states and [[ highway connections ]] .,0
2542,200 - dimensional hidden states,highway connections,"[['stacked bi - LSTMs', 'has', '3 layers']]",Our stacked bi - LSTMs ( Section 3.1 ) has 3 layers with [[ 200 - dimensional hidden states ]] and [[ highway connections ]] .,0
2543,interactions between the endpoints,spanlevel FFNN,[],"Second , we observe the importance of allowing [[ interactions between the endpoints ]] using the [[ spanlevel FFNN ]] .",0
2544,graph models,all other models,"[['graph models', 'has', 'GGNN']]","As can be seen , the [[ graph models ]] outperform [[ all other models ]] across precision , recall and F-score , with GGNN showing the best over all result .",0
2545,graph models,"precision , recall and F-score","[['graph models', 'has', 'GGNN']]","As can be seen , the [[ graph models ]] outperform all other models across [[ precision , recall and F-score ]] , with GGNN showing the best over all result .",0
2546,graph models,GGNN,"[['graph models', 'has', 'GGNN']]","As can be seen , the [[ graph models ]] outperform all other models across precision , recall and F-score , with [[ GGNN ]] showing the best over all result .",1
2547,graph models,best over all result,"[['graph models', 'has', 'GGNN']]","As can be seen , the [[ graph models ]] outperform all other models across precision , recall and F-score , with GGNN showing the [[ best over all result ]] .",0
2548,all other models,"precision , recall and F-score","[['graph models', 'has', 'GGNN']]","As can be seen , the graph models outperform [[ all other models ]] across [[ precision , recall and F-score ]] , with GGNN showing the best over all result .",0
2549,all other models,GGNN,"[['graph models', 'has', 'GGNN']]","As can be seen , the graph models outperform [[ all other models ]] across precision , recall and F-score , with [[ GGNN ]] showing the best over all result .",0
2550,all other models,best over all result,"[['graph models', 'has', 'GGNN']]","As can be seen , the graph models outperform [[ all other models ]] across precision , recall and F-score , with GGNN showing the [[ best over all result ]] .",0
2551,"precision , recall and F-score",GGNN,"[['graph models', 'has', 'GGNN']]","As can be seen , the graph models outperform all other models across [[ precision , recall and F-score ]] , with [[ GGNN ]] showing the best over all result .",0
2552,"precision , recall and F-score",best over all result,"[['graph models', 'has', 'GGNN']]","As can be seen , the graph models outperform all other models across [[ precision , recall and F-score ]] , with GGNN showing the [[ best over all result ]] .",0
2553,GGNN,best over all result,"[['graph models', 'has', 'GGNN']]","As can be seen , the graph models outperform all other models across precision , recall and F-score , with [[ GGNN ]] showing the [[ best over all result ]] .",0
2554,timex - timex and event - timex relations,CAEVO,[],"If we consider the different entity pairs , CATENA performs best on [[ timex - timex and event - timex relations ]] , while [[ CAEVO ]] still achieves the best results on event - DCT and event - event pairs .",0
2555,timex - timex and event - timex relations,best results,[],"If we consider the different entity pairs , CATENA performs best on [[ timex - timex and event - timex relations ]] , while CAEVO still achieves the [[ best results ]] on event - DCT and event - event pairs .",0
2556,timex - timex and event - timex relations,event - DCT and event - event pairs,[],"If we consider the different entity pairs , CATENA performs best on [[ timex - timex and event - timex relations ]] , while CAEVO still achieves the best results on [[ event - DCT and event - event pairs ]] .",0
2557,CAEVO,best results,[],"If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while [[ CAEVO ]] still achieves the [[ best results ]] on event - DCT and event - event pairs .",0
2558,CAEVO,event - DCT and event - event pairs,[],"If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while [[ CAEVO ]] still achieves the best results on [[ event - DCT and event - event pairs ]] .",0
2559,best results,event - DCT and event - event pairs,[],"If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while CAEVO still achieves the [[ best results ]] on [[ event - DCT and event - event pairs ]] .",0
2560,"Adam ( Kingma and Ba , 2014 )",optimization,[],"We use the [[ Adam ( Kingma and Ba , 2014 ) ]] for [[ optimization ]] .",0
2561,multi - layer GA architecture,coreference edges,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the [[ multi - layer GA architecture ]] , the [[ coreference edges ]] again lead to an improvement of 2 % , setting a new state - of - theart on this dataset .",1
2562,multi - layer GA architecture,improvement,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the [[ multi - layer GA architecture ]] , the coreference edges again lead to an [[ improvement ]] of 2 % , setting a new state - of - theart on this dataset .",0
2563,multi - layer GA architecture,2 %,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the [[ multi - layer GA architecture ]] , the coreference edges again lead to an improvement of [[ 2 % ]] , setting a new state - of - theart on this dataset .",0
2564,multi - layer GA architecture,new state - of - theart,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the [[ multi - layer GA architecture ]] , the coreference edges again lead to an improvement of 2 % , setting a [[ new state - of - theart ]] on this dataset .",0
2565,coreference edges,improvement,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the multi - layer GA architecture , the [[ coreference edges ]] again lead to an [[ improvement ]] of 2 % , setting a new state - of - theart on this dataset .",0
2566,coreference edges,2 %,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the multi - layer GA architecture , the [[ coreference edges ]] again lead to an improvement of [[ 2 % ]] , setting a new state - of - theart on this dataset .",0
2567,coreference edges,new state - of - theart,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the multi - layer GA architecture , the [[ coreference edges ]] again lead to an improvement of 2 % , setting a [[ new state - of - theart ]] on this dataset .",0
2568,improvement,2 %,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the multi - layer GA architecture , the coreference edges again lead to an [[ improvement ]] of [[ 2 % ]] , setting a new state - of - theart on this dataset .",0
2569,improvement,new state - of - theart,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the multi - layer GA architecture , the coreference edges again lead to an [[ improvement ]] of 2 % , setting a [[ new state - of - theart ]] on this dataset .",0
2570,2 %,new state - of - theart,"[['multi - layer GA architecture', 'has', 'coreference edges']]","On the multi - layer GA architecture , the coreference edges again lead to an improvement of [[ 2 % ]] , setting a [[ new state - of - theart ]] on this dataset .",0
2571,AOA,mutual attentions,[],[[ AOA ]] automatically generates [[ mutual attentions ]] not only from aspect - to - text but also text - to - aspect .,0
2572,AOA,aspect - to - text,[],[[ AOA ]] automatically generates mutual attentions not only from [[ aspect - to - text ]] but also text - to - aspect .,0
2573,AOA,text - to - aspect,[],[[ AOA ]] automatically generates mutual attentions not only from aspect - to - text but also [[ text - to - aspect ]] .,0
2574,mutual attentions,aspect - to - text,[],AOA automatically generates [[ mutual attentions ]] not only from [[ aspect - to - text ]] but also text - to - aspect .,0
2575,mutual attentions,text - to - aspect,[],AOA automatically generates [[ mutual attentions ]] not only from aspect - to - text but also [[ text - to - aspect ]] .,0
2576,aspect - to - text,text - to - aspect,[],AOA automatically generates mutual attentions not only from [[ aspect - to - text ]] but also [[ text - to - aspect ]] .,0
2577,word embedding vectors,unlabeled corpus,[],The [[ word embedding vectors ]] are pre-trained on an [[ unlabeled corpus ]] whose size is about 840 billion .,0
2578,word embedding vectors,size,[],The [[ word embedding vectors ]] are pre-trained on an unlabeled corpus whose [[ size ]] is about 840 billion .,0
2579,word embedding vectors,about 840 billion,[],The [[ word embedding vectors ]] are pre-trained on an unlabeled corpus whose size is [[ about 840 billion ]] .,0
2580,unlabeled corpus,size,[],The word embedding vectors are pre-trained on an [[ unlabeled corpus ]] whose [[ size ]] is about 840 billion .,0
2581,unlabeled corpus,about 840 billion,[],The word embedding vectors are pre-trained on an [[ unlabeled corpus ]] whose size is [[ about 840 billion ]] .,0
2582,size,about 840 billion,[],The word embedding vectors are pre-trained on an unlabeled corpus whose [[ size ]] is [[ about 840 billion ]] .,0
2583,LSTM,basic LSTM - based deletion method,[],[[ LSTM ]] : This is the [[ basic LSTM - based deletion method ]] proposed by .,0
2584,attention mechanism,interactions,[],"Specifically , at the beginning we employ an [[ attention mechanism ]] to model [[ interactions ]] between a question and reviews .",0
2585,attention mechanism,question and reviews,[],"Specifically , at the beginning we employ an [[ attention mechanism ]] to model interactions between a [[ question and reviews ]] .",0
2586,interactions,question and reviews,[],"Specifically , at the beginning we employ an attention mechanism to model [[ interactions ]] between a [[ question and reviews ]] .",0
2587,SQLNet and TypeSQL,SQL structure information,[],"In contrast , [[ SQLNet and TypeSQL ]] that utilize [[ SQL structure information ]] to guide the SQL generation process significantly outperform other Seq2Seq models .",0
2588,SQLNet and TypeSQL,SQL generation process,[],"In contrast , [[ SQLNet and TypeSQL ]] that utilize SQL structure information to guide the [[ SQL generation process ]] significantly outperform other Seq2Seq models .",0
2589,SQLNet and TypeSQL,other Seq2Seq models,[],"In contrast , [[ SQLNet and TypeSQL ]] that utilize SQL structure information to guide the SQL generation process significantly outperform [[ other Seq2Seq models ]] .",0
2590,SQL structure information,SQL generation process,[],"In contrast , SQLNet and TypeSQL that utilize [[ SQL structure information ]] to guide the [[ SQL generation process ]] significantly outperform other Seq2Seq models .",0
2591,SQL structure information,other Seq2Seq models,[],"In contrast , SQLNet and TypeSQL that utilize [[ SQL structure information ]] to guide the SQL generation process significantly outperform [[ other Seq2Seq models ]] .",0
2592,SQL generation process,other Seq2Seq models,[],"In contrast , SQLNet and TypeSQL that utilize SQL structure information to guide the [[ SQL generation process ]] significantly outperform [[ other Seq2Seq models ]] .",0
2593,FTSum,facts,[],[[ FTSum ]] encoded the [[ facts ]] extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries .,0
2594,FTSum,source sentence,[],[[ FTSum ]] encoded the facts extracted from the [[ source sentence ]] to improve both the faithfulness and informativeness of generated summaries .,0
2595,FTSum,faithfulness,[],[[ FTSum ]] encoded the facts extracted from the source sentence to improve both the [[ faithfulness ]] and informativeness of generated summaries .,0
2596,FTSum,informativeness,[],[[ FTSum ]] encoded the facts extracted from the source sentence to improve both the faithfulness and [[ informativeness ]] of generated summaries .,0
2597,FTSum,generated summaries,[],[[ FTSum ]] encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of [[ generated summaries ]] .,0
2598,facts,source sentence,[],FTSum encoded the [[ facts ]] extracted from the [[ source sentence ]] to improve both the faithfulness and informativeness of generated summaries .,0
2599,facts,faithfulness,[],FTSum encoded the [[ facts ]] extracted from the source sentence to improve both the [[ faithfulness ]] and informativeness of generated summaries .,0
2600,facts,informativeness,[],FTSum encoded the [[ facts ]] extracted from the source sentence to improve both the faithfulness and [[ informativeness ]] of generated summaries .,0
2601,facts,generated summaries,[],FTSum encoded the [[ facts ]] extracted from the source sentence to improve both the faithfulness and informativeness of [[ generated summaries ]] .,0
2602,source sentence,faithfulness,[],FTSum encoded the facts extracted from the [[ source sentence ]] to improve both the [[ faithfulness ]] and informativeness of generated summaries .,0
2603,source sentence,informativeness,[],FTSum encoded the facts extracted from the [[ source sentence ]] to improve both the faithfulness and [[ informativeness ]] of generated summaries .,0
2604,source sentence,generated summaries,[],FTSum encoded the facts extracted from the [[ source sentence ]] to improve both the faithfulness and informativeness of [[ generated summaries ]] .,0
2605,faithfulness,informativeness,[],FTSum encoded the facts extracted from the source sentence to improve both the [[ faithfulness ]] and [[ informativeness ]] of generated summaries .,0
2606,faithfulness,generated summaries,[],FTSum encoded the facts extracted from the source sentence to improve both the [[ faithfulness ]] and informativeness of [[ generated summaries ]] .,0
2607,informativeness,generated summaries,[],FTSum encoded the facts extracted from the source sentence to improve both the faithfulness and [[ informativeness ]] of [[ generated summaries ]] .,0
2608,hidden state dimensions d 2,300,[],All the [[ hidden state dimensions d 2 ]] are set to [[ 300 ]] .,0
2609,postfiltering method,better performances,[],"By applying the [[ postfiltering method ]] proposed in Sec. 4 , we were able to achieve [[ better performances ]] using SP + ILP ( line 5 ) , which shows the effectiveness of this strategy .",0
2610,postfiltering method,SP + ILP,[],"By applying the [[ postfiltering method ]] proposed in Sec. 4 , we were able to achieve better performances using [[ SP + ILP ]] ( line 5 ) , which shows the effectiveness of this strategy .",0
2611,better performances,SP + ILP,[],"By applying the postfiltering method proposed in Sec. 4 , we were able to achieve [[ better performances ]] using [[ SP + ILP ]] ( line 5 ) , which shows the effectiveness of this strategy .",0
2612,models,single QA dataset,[],"We first notice that [[ models ]] trained on a [[ single QA dataset ]] perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",0
2613,models,poorly,[],"We first notice that [[ models ]] trained on a single QA dataset perform [[ poorly ]] on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",0
2614,models,other datasets,[],"We first notice that [[ models ]] trained on a single QA dataset perform poorly on the [[ other datasets ]] ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",0
2615,single QA dataset,poorly,[],"We first notice that models trained on a [[ single QA dataset ]] perform [[ poorly ]] on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",0
2616,single QA dataset,other datasets,[],"We first notice that models trained on a [[ single QA dataset ]] perform poorly on the [[ other datasets ]] ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",0
2617,poorly,other datasets,[],"We first notice that models trained on a single QA dataset perform [[ poorly ]] on the [[ other datasets ]] ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",0
2618,early stopping,5 - fold cross validation,[],We only fine tune [[ early stopping ]] with [[ 5 - fold cross validation ]] on training datasets .,0
2619,early stopping,training datasets,[],We only fine tune [[ early stopping ]] with 5 - fold cross validation on [[ training datasets ]] .,0
2620,5 - fold cross validation,training datasets,[],We only fine tune early stopping with [[ 5 - fold cross validation ]] on [[ training datasets ]] .,0
2621,accurate syntactic information,deep models,[],"While the continuing trend of improving SRL without syntax seems to suggest that neural end - to - end systems no longer needs parsers , our analysis in Section 4.4 will show that [[ accurate syntactic information ]] can improve these [[ deep models ]] .",0
2622,RAM,recurrent attention network,[],"[[ RAM ]] is a [[ recurrent attention network ]] for ATSA task , which uses LSTM and multiple attention mechanisms .",0
2623,RAM,ATSA task,[],"[[ RAM ]] is a recurrent attention network for [[ ATSA task ]] , which uses LSTM and multiple attention mechanisms .",0
2624,RAM,LSTM and multiple attention mechanisms,[],"[[ RAM ]] is a recurrent attention network for ATSA task , which uses [[ LSTM and multiple attention mechanisms ]] .",0
2625,recurrent attention network,ATSA task,[],"RAM is a [[ recurrent attention network ]] for [[ ATSA task ]] , which uses LSTM and multiple attention mechanisms .",0
2626,recurrent attention network,LSTM and multiple attention mechanisms,[],"RAM is a [[ recurrent attention network ]] for ATSA task , which uses [[ LSTM and multiple attention mechanisms ]] .",0
2627,ATSA task,LSTM and multiple attention mechanisms,[],"RAM is a recurrent attention network for [[ ATSA task ]] , which uses [[ LSTM and multiple attention mechanisms ]] .",0
2628,connections,layers,[],The result of ( 10 ) shows that the [[ connections ]] among the [[ layers ]] are important to help gradient flow .,0
2629,connections,gradient flow,[],The result of ( 10 ) shows that the [[ connections ]] among the layers are important to help [[ gradient flow ]] .,0
2630,layers,gradient flow,[],The result of ( 10 ) shows that the connections among the [[ layers ]] are important to help [[ gradient flow ]] .,0
2631,encoder,projection,[],"Replacing the [[ encoder ]] with a [[ projection ]] over word embeddings result in significant performance drop , which suggests that contextual encodings that capture positional information is crucial to this task .",0
2632,encoder,word embeddings,[],"Replacing the [[ encoder ]] with a projection over [[ word embeddings ]] result in significant performance drop , which suggests that contextual encodings that capture positional information is crucial to this task .",0
2633,encoder,significant performance drop,[],"Replacing the [[ encoder ]] with a projection over word embeddings result in [[ significant performance drop ]] , which suggests that contextual encodings that capture positional information is crucial to this task .",0
2634,projection,word embeddings,[],"Replacing the encoder with a [[ projection ]] over [[ word embeddings ]] result in significant performance drop , which suggests that contextual encodings that capture positional information is crucial to this task .",0
2635,projection,significant performance drop,[],"Replacing the encoder with a [[ projection ]] over word embeddings result in [[ significant performance drop ]] , which suggests that contextual encodings that capture positional information is crucial to this task .",0
2636,word embeddings,significant performance drop,[],"Replacing the encoder with a projection over [[ word embeddings ]] result in [[ significant performance drop ]] , which suggests that contextual encodings that capture positional information is crucial to this task .",0
2637,out - of - vocabulary words,withinvocabulary word embeddings,[],"These representations can then be used for [[ out - of - vocabulary words ]] , or combined with [[ withinvocabulary word embeddings ]] directly trained on the task of interest or pretrained from an external data source .",0
2638,out - of - vocabulary words,task of interest,[],"These representations can then be used for [[ out - of - vocabulary words ]] , or combined with withinvocabulary word embeddings directly trained on the [[ task of interest ]] or pretrained from an external data source .",0
2639,out - of - vocabulary words,external data source,[],"These representations can then be used for [[ out - of - vocabulary words ]] , or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an [[ external data source ]] .",0
2640,withinvocabulary word embeddings,task of interest,[],"These representations can then be used for out - of - vocabulary words , or combined with [[ withinvocabulary word embeddings ]] directly trained on the [[ task of interest ]] or pretrained from an external data source .",0
2641,withinvocabulary word embeddings,external data source,[],"These representations can then be used for out - of - vocabulary words , or combined with [[ withinvocabulary word embeddings ]] directly trained on the task of interest or pretrained from an [[ external data source ]] .",0
2642,task of interest,external data source,[],"These representations can then be used for out - of - vocabulary words , or combined with withinvocabulary word embeddings directly trained on the [[ task of interest ]] or pretrained from an [[ external data source ]] .",0
2643,lightweight adaptation,87.7 %,[],"Moreover , our [[ lightweight adaptation ]] achieves [[ 87.7 % ]] with only 750K parameters , which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model ( 86.8 % ) .",0
2644,pre-trained uncased BERT - base model,fine - tuning,[],We use the [[ pre-trained uncased BERT - base model ]] 5 for [[ fine - tuning ]] .,0
2645,mini - batch size,32,[],The [[ mini - batch size ]] is set to [[ 32 ]] .,0
2646,neural multitask learning framework,knowledge,[],"To this end , we propose a [[ neural multitask learning framework ]] to incorporate [[ knowledge ]] into citations from the structure of scientific papers .",0
2647,neural multitask learning framework,citations,[],"To this end , we propose a [[ neural multitask learning framework ]] to incorporate knowledge into [[ citations ]] from the structure of scientific papers .",0
2648,neural multitask learning framework,structure of scientific papers,[],"To this end , we propose a [[ neural multitask learning framework ]] to incorporate knowledge into citations from the [[ structure of scientific papers ]] .",0
2649,knowledge,citations,[],"To this end , we propose a neural multitask learning framework to incorporate [[ knowledge ]] into [[ citations ]] from the structure of scientific papers .",0
2650,knowledge,structure of scientific papers,[],"To this end , we propose a neural multitask learning framework to incorporate [[ knowledge ]] into citations from the [[ structure of scientific papers ]] .",0
2651,citations,structure of scientific papers,[],"To this end , we propose a neural multitask learning framework to incorporate knowledge into [[ citations ]] from the [[ structure of scientific papers ]] .",0
2652,sentences,treebank,[],"The [[ sentences ]] in the [[ treebank ]] were split into a train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) and these splits are made available with the data release .",0
2653,sentences,"train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 )",[],"The [[ sentences ]] in the treebank were split into a [[ train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) ]] and these splits are made available with the data release .",0
2654,treebank,"train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 )",[],"The sentences in the [[ treebank ]] were split into a [[ train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) ]] and these splits are made available with the data release .",0
2655,non-linear map,iterative manner,[],The basic idea of dynamic routing is to construct a [[ non-linear map ]] in an [[ iterative manner ]] ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,0
2656,non-linear map,output of each capsule,[],The basic idea of dynamic routing is to construct a [[ non-linear map ]] in an iterative manner ensuring that the [[ output of each capsule ]] gets sent to an appropriate parent in the subsequent layer :,0
2657,non-linear map,appropriate parent in the subsequent layer,[],The basic idea of dynamic routing is to construct a [[ non-linear map ]] in an iterative manner ensuring that the output of each capsule gets sent to an [[ appropriate parent in the subsequent layer ]] :,0
2658,iterative manner,output of each capsule,[],The basic idea of dynamic routing is to construct a non-linear map in an [[ iterative manner ]] ensuring that the [[ output of each capsule ]] gets sent to an appropriate parent in the subsequent layer :,0
2659,iterative manner,appropriate parent in the subsequent layer,[],The basic idea of dynamic routing is to construct a non-linear map in an [[ iterative manner ]] ensuring that the output of each capsule gets sent to an [[ appropriate parent in the subsequent layer ]] :,0
2660,output of each capsule,appropriate parent in the subsequent layer,[],The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the [[ output of each capsule ]] gets sent to an [[ appropriate parent in the subsequent layer ]] :,0
2661,coverage penalty parameter,10,[],"The [[ coverage penalty parameter ]] ? is set to [[ 10 ]] , and the copy attention normalization parameter ? to 2 for both approaches .",0
2662,coverage penalty parameter,copy attention normalization parameter,[],"The [[ coverage penalty parameter ]] ? is set to 10 , and the [[ copy attention normalization parameter ]] ? to 2 for both approaches .",0
2663,coverage penalty parameter,2,[],"The [[ coverage penalty parameter ]] ? is set to 10 , and the copy attention normalization parameter ? to [[ 2 ]] for both approaches .",0
2664,10,copy attention normalization parameter,[],"The coverage penalty parameter ? is set to [[ 10 ]] , and the [[ copy attention normalization parameter ]] ? to 2 for both approaches .",0
2665,10,2,[],"The coverage penalty parameter ? is set to [[ 10 ]] , and the copy attention normalization parameter ? to [[ 2 ]] for both approaches .",0
2666,copy attention normalization parameter,2,[],"The coverage penalty parameter ? is set to 10 , and the [[ copy attention normalization parameter ]] ? to [[ 2 ]] for both approaches .",0
2667,observation,data size is generally much larger than feature length,[],"Then , based on the [[ observation ]] that [[ data size is generally much larger than feature length ]] , i.e. N L , we propose a speedup solver .",0
2668,observation,speedup solver,[],"Then , based on the [[ observation ]] that data size is generally much larger than feature length , i.e. N L , we propose a [[ speedup solver ]] .",0
2669,data size is generally much larger than feature length,speedup solver,[],"Then , based on the observation that [[ data size is generally much larger than feature length ]] , i.e. N L , we propose a [[ speedup solver ]] .",0
2670,overfitting,inject supervision on intermediate activations of the network,[],This parameter sharing prevents [[ overfitting ]] and also provides opportunities to [[ inject supervision on intermediate activations of the network ]] .,0
2671,Our network,solve,"[['solve', 'has', '94.1 %']]",[[ Our network ]] learns to [[ solve ]] 94.1 % of even the hardest 17 - givens Sudokus after 32 steps .,0
2672,Our network,94.1 %,"[['solve', 'has', '94.1 %']]",[[ Our network ]] learns to solve [[ 94.1 % ]] of even the hardest 17 - givens Sudokus after 32 steps .,0
2673,Our network,even the hardest 17 - givens Sudokus,"[['solve', 'has', '94.1 %']]",[[ Our network ]] learns to solve 94.1 % of [[ even the hardest 17 - givens Sudokus ]] after 32 steps .,0
2674,Our network,32 steps,"[['solve', 'has', '94.1 %']]",[[ Our network ]] learns to solve 94.1 % of even the hardest 17 - givens Sudokus after [[ 32 steps ]] .,0
2675,solve,94.1 %,"[['solve', 'has', '94.1 %']]",Our network learns to [[ solve ]] [[ 94.1 % ]] of even the hardest 17 - givens Sudokus after 32 steps .,1
2676,solve,even the hardest 17 - givens Sudokus,"[['solve', 'has', '94.1 %']]",Our network learns to [[ solve ]] 94.1 % of [[ even the hardest 17 - givens Sudokus ]] after 32 steps .,0
2677,solve,32 steps,"[['solve', 'has', '94.1 %']]",Our network learns to [[ solve ]] 94.1 % of even the hardest 17 - givens Sudokus after [[ 32 steps ]] .,0
2678,94.1 %,even the hardest 17 - givens Sudokus,"[['solve', 'has', '94.1 %']]",Our network learns to solve [[ 94.1 % ]] of [[ even the hardest 17 - givens Sudokus ]] after 32 steps .,0
2679,94.1 %,32 steps,"[['solve', 'has', '94.1 %']]",Our network learns to solve [[ 94.1 % ]] of even the hardest 17 - givens Sudokus after [[ 32 steps ]] .,0
2680,even the hardest 17 - givens Sudokus,32 steps,"[['solve', 'has', '94.1 %']]",Our network learns to solve 94.1 % of [[ even the hardest 17 - givens Sudokus ]] after [[ 32 steps ]] .,0
2681,Bi - LSTM and Bi - GRU,Bi - LSTM and a Bi - GRU network,[],[[ Bi - LSTM and Bi - GRU ]] adopt a [[ Bi - LSTM and a Bi - GRU network ]] to model the sentence and use the hidden state of the final word for prediction respectively .,0
2682,Bi - LSTM and Bi - GRU,sentence,[],[[ Bi - LSTM and Bi - GRU ]] adopt a Bi - LSTM and a Bi - GRU network to model the [[ sentence ]] and use the hidden state of the final word for prediction respectively .,0
2683,Bi - LSTM and Bi - GRU,hidden state,[],[[ Bi - LSTM and Bi - GRU ]] adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the [[ hidden state ]] of the final word for prediction respectively .,0
2684,Bi - LSTM and Bi - GRU,final word,[],[[ Bi - LSTM and Bi - GRU ]] adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the [[ final word ]] for prediction respectively .,0
2685,Bi - LSTM and Bi - GRU,prediction,[],[[ Bi - LSTM and Bi - GRU ]] adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the final word for [[ prediction ]] respectively .,0
2686,Bi - LSTM and a Bi - GRU network,sentence,[],Bi - LSTM and Bi - GRU adopt a [[ Bi - LSTM and a Bi - GRU network ]] to model the [[ sentence ]] and use the hidden state of the final word for prediction respectively .,0
2687,Bi - LSTM and a Bi - GRU network,hidden state,[],Bi - LSTM and Bi - GRU adopt a [[ Bi - LSTM and a Bi - GRU network ]] to model the sentence and use the [[ hidden state ]] of the final word for prediction respectively .,0
2688,Bi - LSTM and a Bi - GRU network,final word,[],Bi - LSTM and Bi - GRU adopt a [[ Bi - LSTM and a Bi - GRU network ]] to model the sentence and use the hidden state of the [[ final word ]] for prediction respectively .,0
2689,Bi - LSTM and a Bi - GRU network,prediction,[],Bi - LSTM and Bi - GRU adopt a [[ Bi - LSTM and a Bi - GRU network ]] to model the sentence and use the hidden state of the final word for [[ prediction ]] respectively .,0
2690,sentence,hidden state,[],Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the [[ sentence ]] and use the [[ hidden state ]] of the final word for prediction respectively .,0
2691,sentence,final word,[],Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the [[ sentence ]] and use the hidden state of the [[ final word ]] for prediction respectively .,0
2692,sentence,prediction,[],Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the [[ sentence ]] and use the hidden state of the final word for [[ prediction ]] respectively .,0
2693,hidden state,final word,[],Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the [[ hidden state ]] of the [[ final word ]] for prediction respectively .,0
2694,hidden state,prediction,[],Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the [[ hidden state ]] of the final word for [[ prediction ]] respectively .,0
2695,final word,prediction,[],Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the [[ final word ]] for [[ prediction ]] respectively .,0
2696,Adam optimizer,"? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9",[],"We use the [[ Adam optimizer ]] with [[ ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 ]] and follow the same learning rate schedule in .",0
2697,Adam optimizer,learning rate,[],We use [[ Adam optimizer ]] and set the [[ learning rate ]] to be 3e - 5 .,0
2698,Adam optimizer,3e - 5,[],We use [[ Adam optimizer ]] and set the learning rate to be [[ 3e - 5 ]] .,0
2699,learning rate,3e - 5,[],We use Adam optimizer and set the [[ learning rate ]] to be [[ 3e - 5 ]] .,0
2700,justifications,most help,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the [[ justifications ]] that provide the [[ most help ]] towards ranking the correct answers higher than incorrect ones .",0
2701,justifications,ranking,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the [[ justifications ]] that provide the most help towards [[ ranking ]] the correct answers higher than incorrect ones .",0
2702,justifications,correct answers,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the [[ justifications ]] that provide the most help towards ranking the [[ correct answers ]] higher than incorrect ones .",0
2703,justifications,higher,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the [[ justifications ]] that provide the most help towards ranking the correct answers [[ higher ]] than incorrect ones .",0
2704,justifications,incorrect ones,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the [[ justifications ]] that provide the most help towards ranking the correct answers higher than [[ incorrect ones ]] .",0
2705,most help,ranking,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the [[ most help ]] towards [[ ranking ]] the correct answers higher than incorrect ones .",0
2706,most help,correct answers,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the [[ most help ]] towards ranking the [[ correct answers ]] higher than incorrect ones .",0
2707,most help,higher,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the [[ most help ]] towards ranking the correct answers [[ higher ]] than incorrect ones .",0
2708,most help,incorrect ones,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the [[ most help ]] towards ranking the correct answers higher than [[ incorrect ones ]] .",0
2709,ranking,correct answers,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the most help towards [[ ranking ]] the [[ correct answers ]] higher than incorrect ones .",1
2710,ranking,higher,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the most help towards [[ ranking ]] the correct answers [[ higher ]] than incorrect ones .",0
2711,ranking,incorrect ones,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the most help towards [[ ranking ]] the correct answers higher than [[ incorrect ones ]] .",0
2712,correct answers,higher,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the most help towards ranking the [[ correct answers ]] [[ higher ]] than incorrect ones .",1
2713,correct answers,incorrect ones,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the most help towards ranking the [[ correct answers ]] higher than [[ incorrect ones ]] .",0
2714,higher,incorrect ones,"[['ranking', 'has', 'correct answers'], ['correct answers', 'has', 'higher']]","Intuitively , our approach chooses the justifications that provide the most help towards ranking the correct answers [[ higher ]] than [[ incorrect ones ]] .",0
2715,17.51 %,RASG w / o GTD to RASG w / o GT,[],"Consequently , there is an increment of [[ 17.51 % ]] from [[ RASG w / o GTD to RASG w / o GT ]] in terms of ROUGE - L , which demonstrates the effectiveness of discriminator .",0
2716,17.51 %,ROUGE - L,[],"Consequently , there is an increment of [[ 17.51 % ]] from RASG w / o GTD to RASG w / o GT in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of discriminator .",0
2717,RASG w / o GTD to RASG w / o GT,ROUGE - L,[],"Consequently , there is an increment of 17.51 % from [[ RASG w / o GTD to RASG w / o GT ]] in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of discriminator .",0
2718,Document - cue baseline,more than a third of the samples,"[['more than a third of the samples', 'has', 'correctly']]","The [[ Document - cue baseline ]] can predict [[ more than a third of the samples ]] correctly , for both datasets , even after sub - sampling frequent document - answer pairs for WIKIHOP .",0
2719,Document - cue baseline,correctly,"[['more than a third of the samples', 'has', 'correctly']]","The [[ Document - cue baseline ]] can predict more than a third of the samples [[ correctly ]] , for both datasets , even after sub - sampling frequent document - answer pairs for WIKIHOP .",0
2720,Document - cue baseline,frequent document - answer pairs,"[['more than a third of the samples', 'has', 'correctly']]","The [[ Document - cue baseline ]] can predict more than a third of the samples correctly , for both datasets , even after sub - sampling [[ frequent document - answer pairs ]] for WIKIHOP .",0
2721,Document - cue baseline,WIKIHOP,"[['more than a third of the samples', 'has', 'correctly']]","The [[ Document - cue baseline ]] can predict more than a third of the samples correctly , for both datasets , even after sub - sampling frequent document - answer pairs for [[ WIKIHOP ]] .",0
2722,more than a third of the samples,correctly,"[['more than a third of the samples', 'has', 'correctly']]","The Document - cue baseline can predict [[ more than a third of the samples ]] [[ correctly ]] , for both datasets , even after sub - sampling frequent document - answer pairs for WIKIHOP .",1
2723,more than a third of the samples,frequent document - answer pairs,"[['more than a third of the samples', 'has', 'correctly']]","The Document - cue baseline can predict [[ more than a third of the samples ]] correctly , for both datasets , even after sub - sampling [[ frequent document - answer pairs ]] for WIKIHOP .",0
2724,more than a third of the samples,WIKIHOP,"[['more than a third of the samples', 'has', 'correctly']]","The Document - cue baseline can predict [[ more than a third of the samples ]] correctly , for both datasets , even after sub - sampling frequent document - answer pairs for [[ WIKIHOP ]] .",0
2725,correctly,frequent document - answer pairs,"[['more than a third of the samples', 'has', 'correctly']]","The Document - cue baseline can predict more than a third of the samples [[ correctly ]] , for both datasets , even after sub - sampling [[ frequent document - answer pairs ]] for WIKIHOP .",0
2726,correctly,WIKIHOP,"[['more than a third of the samples', 'has', 'correctly']]","The Document - cue baseline can predict more than a third of the samples [[ correctly ]] , for both datasets , even after sub - sampling frequent document - answer pairs for [[ WIKIHOP ]] .",0
2727,frequent document - answer pairs,WIKIHOP,"[['more than a third of the samples', 'has', 'correctly']]","The Document - cue baseline can predict more than a third of the samples correctly , for both datasets , even after sub - sampling [[ frequent document - answer pairs ]] for [[ WIKIHOP ]] .",0
2728,Gigaword dataset,texts are short,"[['Gigaword dataset', 'has', 'our best model']]","In [[ Gigaword dataset ]] where the [[ texts are short ]] , our best model achieves a comparable performance with the current state - of - the - art .",0
2729,Gigaword dataset,our best model,"[['Gigaword dataset', 'has', 'our best model']]","In [[ Gigaword dataset ]] where the texts are short , [[ our best model ]] achieves a comparable performance with the current state - of - the - art .",1
2730,Gigaword dataset,comparable performance,"[['Gigaword dataset', 'has', 'our best model']]","In [[ Gigaword dataset ]] where the texts are short , our best model achieves a [[ comparable performance ]] with the current state - of - the - art .",0
2731,Gigaword dataset,current state - of - the - art,"[['Gigaword dataset', 'has', 'our best model']]","In [[ Gigaword dataset ]] where the texts are short , our best model achieves a comparable performance with the [[ current state - of - the - art ]] .",0
2732,texts are short,our best model,"[['Gigaword dataset', 'has', 'our best model']]","In Gigaword dataset where the [[ texts are short ]] , [[ our best model ]] achieves a comparable performance with the current state - of - the - art .",0
2733,texts are short,comparable performance,"[['Gigaword dataset', 'has', 'our best model']]","In Gigaword dataset where the [[ texts are short ]] , our best model achieves a [[ comparable performance ]] with the current state - of - the - art .",0
2734,texts are short,current state - of - the - art,"[['Gigaword dataset', 'has', 'our best model']]","In Gigaword dataset where the [[ texts are short ]] , our best model achieves a comparable performance with the [[ current state - of - the - art ]] .",0
2735,our best model,comparable performance,"[['Gigaword dataset', 'has', 'our best model']]","In Gigaword dataset where the texts are short , [[ our best model ]] achieves a [[ comparable performance ]] with the current state - of - the - art .",0
2736,our best model,current state - of - the - art,"[['Gigaword dataset', 'has', 'our best model']]","In Gigaword dataset where the texts are short , [[ our best model ]] achieves a comparable performance with the [[ current state - of - the - art ]] .",0
2737,comparable performance,current state - of - the - art,"[['Gigaword dataset', 'has', 'our best model']]","In Gigaword dataset where the texts are short , our best model achieves a [[ comparable performance ]] with the [[ current state - of - the - art ]] .",0
2738,feature engineering,significant improvements,[],"Interestingly , we observe that [[ feature engineering ]] leads to [[ significant improvements ]] for WDW and CBT datasets , but not for CNN and Daily Mail datasets .",0
2739,feature engineering,WDW and CBT datasets,[],"Interestingly , we observe that [[ feature engineering ]] leads to significant improvements for [[ WDW and CBT datasets ]] , but not for CNN and Daily Mail datasets .",0
2740,feature engineering,CNN and Daily Mail datasets,[],"Interestingly , we observe that [[ feature engineering ]] leads to significant improvements for WDW and CBT datasets , but not for [[ CNN and Daily Mail datasets ]] .",0
2741,significant improvements,WDW and CBT datasets,[],"Interestingly , we observe that feature engineering leads to [[ significant improvements ]] for [[ WDW and CBT datasets ]] , but not for CNN and Daily Mail datasets .",0
2742,significant improvements,CNN and Daily Mail datasets,[],"Interestingly , we observe that feature engineering leads to [[ significant improvements ]] for WDW and CBT datasets , but not for [[ CNN and Daily Mail datasets ]] .",0
2743,WDW and CBT datasets,CNN and Daily Mail datasets,[],"Interestingly , we observe that feature engineering leads to significant improvements for [[ WDW and CBT datasets ]] , but not for [[ CNN and Daily Mail datasets ]] .",0
2744,LSTM,word - by - word matching,[],"Instead , we use an [[ LSTM ]] to perform [[ word - by - word matching ]] of the hypothesis with the premise .",0
2745,LSTM,hypothesis,[],"Instead , we use an [[ LSTM ]] to perform word - by - word matching of the [[ hypothesis ]] with the premise .",0
2746,LSTM,premise,[],"Instead , we use an [[ LSTM ]] to perform word - by - word matching of the hypothesis with the [[ premise ]] .",0
2747,word - by - word matching,hypothesis,[],"Instead , we use an LSTM to perform [[ word - by - word matching ]] of the [[ hypothesis ]] with the premise .",0
2748,word - by - word matching,premise,[],"Instead , we use an LSTM to perform [[ word - by - word matching ]] of the hypothesis with the [[ premise ]] .",0
2749,hypothesis,premise,[],"Instead , we use an LSTM to perform word - by - word matching of the [[ hypothesis ]] with the [[ premise ]] .",0
