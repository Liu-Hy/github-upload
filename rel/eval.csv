,idx,text,labels
0,4772,"[[ Conventional RNN models ]] clearly << benefit from >> [[ max pooling ]] , especially on the task of answer sentence selection .",1
1,4772,"[[ Conventional RNN models ]] clearly << benefit from >> max pooling , especially on the task of [[ answer sentence selection ]] .",0
2,4772,"Conventional RNN models clearly << benefit from >> [[ max pooling ]] , especially on the task of [[ answer sentence selection ]] .",0
3,4772,"[[ Conventional RNN models ]] clearly benefit from [[ max pooling ]] , << especially on >> the task of answer sentence selection .",0
4,4772,"[[ Conventional RNN models ]] clearly benefit from max pooling , << especially on >> the task of [[ answer sentence selection ]] .",0
5,4772,"Conventional RNN models clearly benefit from [[ max pooling ]] , << especially on >> the task of [[ answer sentence selection ]] .",1
6,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] << explores >> [[ deeper and wider architecture ]] with no down - sampling for learning character - to - word representations from limited supervised training corpora .",1
7,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] << explores >> deeper and wider architecture with no [[ down - sampling ]] for learning character - to - word representations from limited supervised training corpora .",0
8,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] << explores >> deeper and wider architecture with no down - sampling for learning [[ character - to - word representations ]] from limited supervised training corpora .",0
9,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] << explores >> deeper and wider architecture with no down - sampling for learning character - to - word representations from [[ limited supervised training corpora ]] .",0
10,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net << explores >> [[ deeper and wider architecture ]] with no [[ down - sampling ]] for learning character - to - word representations from limited supervised training corpora .",0
11,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net << explores >> [[ deeper and wider architecture ]] with no down - sampling for learning [[ character - to - word representations ]] from limited supervised training corpora .",0
12,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net << explores >> [[ deeper and wider architecture ]] with no down - sampling for learning character - to - word representations from [[ limited supervised training corpora ]] .",0
13,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net << explores >> deeper and wider architecture with no [[ down - sampling ]] for learning [[ character - to - word representations ]] from limited supervised training corpora .",0
14,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net << explores >> deeper and wider architecture with no [[ down - sampling ]] for learning character - to - word representations from [[ limited supervised training corpora ]] .",0
15,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net << explores >> deeper and wider architecture with no down - sampling for learning [[ character - to - word representations ]] from [[ limited supervised training corpora ]] .",0
16,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores [[ deeper and wider architecture ]] << with no >> down - sampling for learning character - to - word representations from limited supervised training corpora .",0
17,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture << with no >> [[ down - sampling ]] for learning character - to - word representations from limited supervised training corpora .",0
18,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture << with no >> down - sampling for learning [[ character - to - word representations ]] from limited supervised training corpora .",0
19,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture << with no >> down - sampling for learning character - to - word representations from [[ limited supervised training corpora ]] .",0
20,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] << with no >> [[ down - sampling ]] for learning character - to - word representations from limited supervised training corpora .",1
21,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] << with no >> down - sampling for learning [[ character - to - word representations ]] from limited supervised training corpora .",0
22,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] << with no >> down - sampling for learning character - to - word representations from [[ limited supervised training corpora ]] .",0
23,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture << with no >> [[ down - sampling ]] for learning [[ character - to - word representations ]] from limited supervised training corpora .",0
24,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture << with no >> [[ down - sampling ]] for learning character - to - word representations from [[ limited supervised training corpora ]] .",0
25,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture << with no >> down - sampling for learning [[ character - to - word representations ]] from [[ limited supervised training corpora ]] .",0
26,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores [[ deeper and wider architecture ]] with no down - sampling << for learning >> character - to - word representations from limited supervised training corpora .",0
27,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture with no [[ down - sampling ]] << for learning >> character - to - word representations from limited supervised training corpora .",0
28,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture with no down - sampling << for learning >> [[ character - to - word representations ]] from limited supervised training corpora .",0
29,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture with no down - sampling << for learning >> character - to - word representations from [[ limited supervised training corpora ]] .",0
30,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] with no [[ down - sampling ]] << for learning >> character - to - word representations from limited supervised training corpora .",0
31,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] with no down - sampling << for learning >> [[ character - to - word representations ]] from limited supervised training corpora .",0
32,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] with no down - sampling << for learning >> character - to - word representations from [[ limited supervised training corpora ]] .",0
33,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no [[ down - sampling ]] << for learning >> [[ character - to - word representations ]] from limited supervised training corpora .",1
34,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no [[ down - sampling ]] << for learning >> character - to - word representations from [[ limited supervised training corpora ]] .",0
35,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no down - sampling << for learning >> [[ character - to - word representations ]] from [[ limited supervised training corpora ]] .",0
36,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores [[ deeper and wider architecture ]] with no down - sampling for learning character - to - word representations << from >> limited supervised training corpora .",0
37,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture with no [[ down - sampling ]] for learning character - to - word representations << from >> limited supervised training corpora .",0
38,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture with no down - sampling for learning [[ character - to - word representations ]] << from >> limited supervised training corpora .",0
39,2548,"Unlike previous CNN - based approaches , our [[ funnel - shaped Int - Net ]] explores deeper and wider architecture with no down - sampling for learning character - to - word representations << from >> [[ limited supervised training corpora ]] .",0
40,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] with no [[ down - sampling ]] for learning character - to - word representations << from >> limited supervised training corpora .",0
41,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] with no down - sampling for learning [[ character - to - word representations ]] << from >> limited supervised training corpora .",0
42,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores [[ deeper and wider architecture ]] with no down - sampling for learning character - to - word representations << from >> [[ limited supervised training corpora ]] .",0
43,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no [[ down - sampling ]] for learning [[ character - to - word representations ]] << from >> limited supervised training corpora .",0
44,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no [[ down - sampling ]] for learning character - to - word representations << from >> [[ limited supervised training corpora ]] .",0
45,2548,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no down - sampling for learning [[ character - to - word representations ]] << from >> [[ limited supervised training corpora ]] .",1
46,967,"First , we can << see that >> [[ multitasking ]] with [[ paraphrase data ]] is essential since it improves F1 from 0.60 to 0.68 .",0
47,967,"First , we can << see that >> [[ multitasking ]] with paraphrase data is [[ essential ]] since it improves F1 from 0.60 to 0.68 .",0
48,967,"First , we can << see that >> [[ multitasking ]] with paraphrase data is essential since it improves [[ F1 ]] from 0.60 to 0.68 .",0
49,967,"First , we can << see that >> [[ multitasking ]] with paraphrase data is essential since it improves F1 from [[ 0.60 ]] to 0.68 .",0
50,967,"First , we can << see that >> [[ multitasking ]] with paraphrase data is essential since it improves F1 from 0.60 to [[ 0.68 ]] .",0
51,967,"First , we can << see that >> multitasking with [[ paraphrase data ]] is [[ essential ]] since it improves F1 from 0.60 to 0.68 .",0
52,967,"First , we can << see that >> multitasking with [[ paraphrase data ]] is essential since it improves [[ F1 ]] from 0.60 to 0.68 .",0
53,967,"First , we can << see that >> multitasking with [[ paraphrase data ]] is essential since it improves F1 from [[ 0.60 ]] to 0.68 .",0
54,967,"First , we can << see that >> multitasking with [[ paraphrase data ]] is essential since it improves F1 from 0.60 to [[ 0.68 ]] .",0
55,967,"First , we can << see that >> multitasking with paraphrase data is [[ essential ]] since it improves [[ F1 ]] from 0.60 to 0.68 .",0
56,967,"First , we can << see that >> multitasking with paraphrase data is [[ essential ]] since it improves F1 from [[ 0.60 ]] to 0.68 .",0
57,967,"First , we can << see that >> multitasking with paraphrase data is [[ essential ]] since it improves F1 from 0.60 to [[ 0.68 ]] .",0
58,967,"First , we can << see that >> multitasking with paraphrase data is essential since it improves [[ F1 ]] from [[ 0.60 ]] to 0.68 .",0
59,967,"First , we can << see that >> multitasking with paraphrase data is essential since it improves [[ F1 ]] from 0.60 to [[ 0.68 ]] .",0
60,967,"First , we can << see that >> multitasking with paraphrase data is essential since it improves F1 from [[ 0.60 ]] to [[ 0.68 ]] .",0
61,967,"First , we can see that [[ multitasking ]] << with >> [[ paraphrase data ]] is essential since it improves F1 from 0.60 to 0.68 .",1
62,967,"First , we can see that [[ multitasking ]] << with >> paraphrase data is [[ essential ]] since it improves F1 from 0.60 to 0.68 .",0
63,967,"First , we can see that [[ multitasking ]] << with >> paraphrase data is essential since it improves [[ F1 ]] from 0.60 to 0.68 .",0
64,967,"First , we can see that [[ multitasking ]] << with >> paraphrase data is essential since it improves F1 from [[ 0.60 ]] to 0.68 .",0
65,967,"First , we can see that [[ multitasking ]] << with >> paraphrase data is essential since it improves F1 from 0.60 to [[ 0.68 ]] .",0
66,967,"First , we can see that multitasking << with >> [[ paraphrase data ]] is [[ essential ]] since it improves F1 from 0.60 to 0.68 .",0
67,967,"First , we can see that multitasking << with >> [[ paraphrase data ]] is essential since it improves [[ F1 ]] from 0.60 to 0.68 .",0
68,967,"First , we can see that multitasking << with >> [[ paraphrase data ]] is essential since it improves F1 from [[ 0.60 ]] to 0.68 .",0
69,967,"First , we can see that multitasking << with >> [[ paraphrase data ]] is essential since it improves F1 from 0.60 to [[ 0.68 ]] .",0
70,967,"First , we can see that multitasking << with >> paraphrase data is [[ essential ]] since it improves [[ F1 ]] from 0.60 to 0.68 .",0
71,967,"First , we can see that multitasking << with >> paraphrase data is [[ essential ]] since it improves F1 from [[ 0.60 ]] to 0.68 .",0
72,967,"First , we can see that multitasking << with >> paraphrase data is [[ essential ]] since it improves F1 from 0.60 to [[ 0.68 ]] .",0
73,967,"First , we can see that multitasking << with >> paraphrase data is essential since it improves [[ F1 ]] from [[ 0.60 ]] to 0.68 .",0
74,967,"First , we can see that multitasking << with >> paraphrase data is essential since it improves [[ F1 ]] from 0.60 to [[ 0.68 ]] .",0
75,967,"First , we can see that multitasking << with >> paraphrase data is essential since it improves F1 from [[ 0.60 ]] to [[ 0.68 ]] .",0
76,967,"First , we can see that [[ multitasking ]] with [[ paraphrase data ]] << is >> essential since it improves F1 from 0.60 to 0.68 .",0
77,967,"First , we can see that [[ multitasking ]] with paraphrase data << is >> [[ essential ]] since it improves F1 from 0.60 to 0.68 .",0
78,967,"First , we can see that [[ multitasking ]] with paraphrase data << is >> essential since it improves [[ F1 ]] from 0.60 to 0.68 .",0
79,967,"First , we can see that [[ multitasking ]] with paraphrase data << is >> essential since it improves F1 from [[ 0.60 ]] to 0.68 .",0
80,967,"First , we can see that [[ multitasking ]] with paraphrase data << is >> essential since it improves F1 from 0.60 to [[ 0.68 ]] .",0
81,967,"First , we can see that multitasking with [[ paraphrase data ]] << is >> [[ essential ]] since it improves F1 from 0.60 to 0.68 .",1
82,967,"First , we can see that multitasking with [[ paraphrase data ]] << is >> essential since it improves [[ F1 ]] from 0.60 to 0.68 .",0
83,967,"First , we can see that multitasking with [[ paraphrase data ]] << is >> essential since it improves F1 from [[ 0.60 ]] to 0.68 .",0
84,967,"First , we can see that multitasking with [[ paraphrase data ]] << is >> essential since it improves F1 from 0.60 to [[ 0.68 ]] .",0
85,967,"First , we can see that multitasking with paraphrase data << is >> [[ essential ]] since it improves [[ F1 ]] from 0.60 to 0.68 .",0
86,967,"First , we can see that multitasking with paraphrase data << is >> [[ essential ]] since it improves F1 from [[ 0.60 ]] to 0.68 .",0
87,967,"First , we can see that multitasking with paraphrase data << is >> [[ essential ]] since it improves F1 from 0.60 to [[ 0.68 ]] .",0
88,967,"First , we can see that multitasking with paraphrase data << is >> essential since it improves [[ F1 ]] from [[ 0.60 ]] to 0.68 .",0
89,967,"First , we can see that multitasking with paraphrase data << is >> essential since it improves [[ F1 ]] from 0.60 to [[ 0.68 ]] .",0
90,967,"First , we can see that multitasking with paraphrase data << is >> essential since it improves F1 from [[ 0.60 ]] to [[ 0.68 ]] .",0
91,967,"First , we can see that [[ multitasking ]] with [[ paraphrase data ]] is essential since it << improves >> F1 from 0.60 to 0.68 .",0
92,967,"First , we can see that [[ multitasking ]] with paraphrase data is [[ essential ]] since it << improves >> F1 from 0.60 to 0.68 .",0
93,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it << improves >> [[ F1 ]] from 0.60 to 0.68 .",0
94,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it << improves >> F1 from [[ 0.60 ]] to 0.68 .",0
95,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it << improves >> F1 from 0.60 to [[ 0.68 ]] .",0
96,967,"First , we can see that multitasking with [[ paraphrase data ]] is [[ essential ]] since it << improves >> F1 from 0.60 to 0.68 .",0
97,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it << improves >> [[ F1 ]] from 0.60 to 0.68 .",1
98,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it << improves >> F1 from [[ 0.60 ]] to 0.68 .",0
99,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it << improves >> F1 from 0.60 to [[ 0.68 ]] .",0
100,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it << improves >> [[ F1 ]] from 0.60 to 0.68 .",0
101,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it << improves >> F1 from [[ 0.60 ]] to 0.68 .",0
102,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it << improves >> F1 from 0.60 to [[ 0.68 ]] .",0
103,967,"First , we can see that multitasking with paraphrase data is essential since it << improves >> [[ F1 ]] from [[ 0.60 ]] to 0.68 .",0
104,967,"First , we can see that multitasking with paraphrase data is essential since it << improves >> [[ F1 ]] from 0.60 to [[ 0.68 ]] .",0
105,967,"First , we can see that multitasking with paraphrase data is essential since it << improves >> F1 from [[ 0.60 ]] to [[ 0.68 ]] .",0
106,967,"First , we can see that [[ multitasking ]] with [[ paraphrase data ]] is essential since it improves F1 << from >> 0.60 to 0.68 .",0
107,967,"First , we can see that [[ multitasking ]] with paraphrase data is [[ essential ]] since it improves F1 << from >> 0.60 to 0.68 .",0
108,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it improves [[ F1 ]] << from >> 0.60 to 0.68 .",0
109,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it improves F1 << from >> [[ 0.60 ]] to 0.68 .",0
110,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it improves F1 << from >> 0.60 to [[ 0.68 ]] .",0
111,967,"First , we can see that multitasking with [[ paraphrase data ]] is [[ essential ]] since it improves F1 << from >> 0.60 to 0.68 .",0
112,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it improves [[ F1 ]] << from >> 0.60 to 0.68 .",0
113,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it improves F1 << from >> [[ 0.60 ]] to 0.68 .",0
114,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it improves F1 << from >> 0.60 to [[ 0.68 ]] .",0
115,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it improves [[ F1 ]] << from >> 0.60 to 0.68 .",0
116,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it improves F1 << from >> [[ 0.60 ]] to 0.68 .",0
117,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it improves F1 << from >> 0.60 to [[ 0.68 ]] .",0
118,967,"First , we can see that multitasking with paraphrase data is essential since it improves [[ F1 ]] << from >> [[ 0.60 ]] to 0.68 .",1
119,967,"First , we can see that multitasking with paraphrase data is essential since it improves [[ F1 ]] << from >> 0.60 to [[ 0.68 ]] .",0
120,967,"First , we can see that multitasking with paraphrase data is essential since it improves F1 << from >> [[ 0.60 ]] to [[ 0.68 ]] .",0
121,967,"First , we can see that [[ multitasking ]] with [[ paraphrase data ]] is essential since it improves F1 from 0.60 << to >> 0.68 .",0
122,967,"First , we can see that [[ multitasking ]] with paraphrase data is [[ essential ]] since it improves F1 from 0.60 << to >> 0.68 .",0
123,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it improves [[ F1 ]] from 0.60 << to >> 0.68 .",0
124,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it improves F1 from [[ 0.60 ]] << to >> 0.68 .",0
125,967,"First , we can see that [[ multitasking ]] with paraphrase data is essential since it improves F1 from 0.60 << to >> [[ 0.68 ]] .",0
126,967,"First , we can see that multitasking with [[ paraphrase data ]] is [[ essential ]] since it improves F1 from 0.60 << to >> 0.68 .",0
127,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it improves [[ F1 ]] from 0.60 << to >> 0.68 .",0
128,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it improves F1 from [[ 0.60 ]] << to >> 0.68 .",0
129,967,"First , we can see that multitasking with [[ paraphrase data ]] is essential since it improves F1 from 0.60 << to >> [[ 0.68 ]] .",0
130,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it improves [[ F1 ]] from 0.60 << to >> 0.68 .",0
131,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it improves F1 from [[ 0.60 ]] << to >> 0.68 .",0
132,967,"First , we can see that multitasking with paraphrase data is [[ essential ]] since it improves F1 from 0.60 << to >> [[ 0.68 ]] .",0
133,967,"First , we can see that multitasking with paraphrase data is essential since it improves [[ F1 ]] from [[ 0.60 ]] << to >> 0.68 .",0
134,967,"First , we can see that multitasking with paraphrase data is essential since it improves [[ F1 ]] from 0.60 << to >> [[ 0.68 ]] .",0
135,967,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from [[ 0.60 ]] << to >> [[ 0.68 ]] .",1
136,1106,"To handle the noise this creates , we << use >> a [[ summed objective function ]] that marginalizes the [[ model 's output ]] over all locations the answer text occurs .",0
137,1106,"To handle the noise this creates , we << use >> a [[ summed objective function ]] that marginalizes the model 's output over [[ all locations ]] the answer text occurs .",0
138,1106,"To handle the noise this creates , we << use >> a [[ summed objective function ]] that marginalizes the model 's output over all locations the [[ answer text ]] occurs .",0
139,1106,"To handle the noise this creates , we << use >> a summed objective function that marginalizes the [[ model 's output ]] over [[ all locations ]] the answer text occurs .",0
140,1106,"To handle the noise this creates , we << use >> a summed objective function that marginalizes the [[ model 's output ]] over all locations the [[ answer text ]] occurs .",0
141,1106,"To handle the noise this creates , we << use >> a summed objective function that marginalizes the model 's output over [[ all locations ]] the [[ answer text ]] occurs .",0
142,1106,"To handle the noise this creates , we use a [[ summed objective function ]] << that marginalizes >> the [[ model 's output ]] over all locations the answer text occurs .",1
143,1106,"To handle the noise this creates , we use a [[ summed objective function ]] << that marginalizes >> the model 's output over [[ all locations ]] the answer text occurs .",0
144,1106,"To handle the noise this creates , we use a [[ summed objective function ]] << that marginalizes >> the model 's output over all locations the [[ answer text ]] occurs .",0
145,1106,"To handle the noise this creates , we use a summed objective function << that marginalizes >> the [[ model 's output ]] over [[ all locations ]] the answer text occurs .",0
146,1106,"To handle the noise this creates , we use a summed objective function << that marginalizes >> the [[ model 's output ]] over all locations the [[ answer text ]] occurs .",0
147,1106,"To handle the noise this creates , we use a summed objective function << that marginalizes >> the model 's output over [[ all locations ]] the [[ answer text ]] occurs .",0
148,1106,"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the [[ model 's output ]] << over >> all locations the answer text occurs .",0
149,1106,"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the model 's output << over >> [[ all locations ]] the answer text occurs .",0
150,1106,"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the model 's output << over >> all locations the [[ answer text ]] occurs .",0
151,1106,"To handle the noise this creates , we use a summed objective function that marginalizes the [[ model 's output ]] << over >> [[ all locations ]] the answer text occurs .",1
152,1106,"To handle the noise this creates , we use a summed objective function that marginalizes the [[ model 's output ]] << over >> all locations the [[ answer text ]] occurs .",0
153,1106,"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output << over >> [[ all locations ]] the [[ answer text ]] occurs .",0
154,1106,"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the [[ model 's output ]] over all locations the answer text << occurs >> .",0
155,1106,"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the model 's output over [[ all locations ]] the answer text << occurs >> .",0
156,1106,"To handle the noise this creates , we use a [[ summed objective function ]] that marginalizes the model 's output over all locations the [[ answer text ]] << occurs >> .",0
157,1106,"To handle the noise this creates , we use a summed objective function that marginalizes the [[ model 's output ]] over [[ all locations ]] the answer text << occurs >> .",0
158,1106,"To handle the noise this creates , we use a summed objective function that marginalizes the [[ model 's output ]] over all locations the [[ answer text ]] << occurs >> .",0
159,1106,"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output over [[ all locations ]] the [[ answer text ]] << occurs >> .",1
160,5612,"The [[ proposed model ]] << achieved >> [[ about 4.0 MOS ]] in all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .",1
161,5612,"The [[ proposed model ]] << achieved >> about 4.0 MOS in [[ all datasets ]] , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .",0
162,5612,"The proposed model << achieved >> [[ about 4.0 MOS ]] in [[ all datasets ]] , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .",0
163,5612,"The [[ proposed model ]] achieved [[ about 4.0 MOS ]] << in >> all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .",0
164,5612,"The [[ proposed model ]] achieved about 4.0 MOS << in >> [[ all datasets ]] , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .",0
165,5612,"The proposed model achieved [[ about 4.0 MOS ]] << in >> [[ all datasets ]] , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .",1
166,4336,"2 ) << Compared with >> [[ all other neural baselines ]] , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
167,4336,"2 ) << Compared with >> [[ all other neural baselines ]] , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
168,4336,"2 ) << Compared with >> [[ all other neural baselines ]] , our full model achieves statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
169,4336,"2 ) << Compared with >> [[ all other neural baselines ]] , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
170,4336,"2 ) << Compared with >> all other neural baselines , [[ our full model ]] achieves [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
171,4336,"2 ) << Compared with >> all other neural baselines , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
172,4336,"2 ) << Compared with >> all other neural baselines , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
173,4336,"2 ) << Compared with >> all other neural baselines , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] on [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
174,4336,"2 ) << Compared with >> all other neural baselines , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
175,4336,"2 ) << Compared with >> all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] for [[ D1 , D3 , D4 ]] .",0
176,4336,"2 ) Compared with [[ all other neural baselines ]] , [[ our full model ]] << achieves >> statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
177,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model << achieves >> [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
178,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model << achieves >> statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
179,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model << achieves >> statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
180,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] << achieves >> [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores for D1 , D3 , D4 .",1
181,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] << achieves >> statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
182,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] << achieves >> statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
183,4336,"2 ) Compared with all other neural baselines , our full model << achieves >> [[ statistically significant improvements ( p < 0.05 ) ]] on [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
184,4336,"2 ) Compared with all other neural baselines , our full model << achieves >> [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
185,4336,"2 ) Compared with all other neural baselines , our full model << achieves >> statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] for [[ D1 , D3 , D4 ]] .",0
186,4336,"2 ) Compared with [[ all other neural baselines ]] , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) << on >> both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
187,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] << on >> both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
188,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model achieves statistically significant improvements ( p < 0.05 ) << on >> [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
189,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model achieves statistically significant improvements ( p < 0.05 ) << on >> both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
190,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] achieves [[ statistically significant improvements ( p < 0.05 ) ]] << on >> both accuracies and macro - F1 scores for D1 , D3 , D4 .",0
191,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) << on >> [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",0
192,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) << on >> both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
193,4336,"2 ) Compared with all other neural baselines , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] << on >> [[ both accuracies and macro - F1 scores ]] for D1 , D3 , D4 .",1
194,4336,"2 ) Compared with all other neural baselines , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] << on >> both accuracies and macro - F1 scores for [[ D1 , D3 , D4 ]] .",0
195,4336,"2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) << on >> [[ both accuracies and macro - F1 scores ]] for [[ D1 , D3 , D4 ]] .",0
196,4336,"2 ) Compared with [[ all other neural baselines ]] , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores << for >> D1 , D3 , D4 .",0
197,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores << for >> D1 , D3 , D4 .",0
198,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model achieves statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] << for >> D1 , D3 , D4 .",0
199,4336,"2 ) Compared with [[ all other neural baselines ]] , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores << for >> [[ D1 , D3 , D4 ]] .",0
200,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] achieves [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores << for >> D1 , D3 , D4 .",0
201,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] << for >> D1 , D3 , D4 .",0
202,4336,"2 ) Compared with all other neural baselines , [[ our full model ]] achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores << for >> [[ D1 , D3 , D4 ]] .",0
203,4336,"2 ) Compared with all other neural baselines , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] on [[ both accuracies and macro - F1 scores ]] << for >> D1 , D3 , D4 .",0
204,4336,"2 ) Compared with all other neural baselines , our full model achieves [[ statistically significant improvements ( p < 0.05 ) ]] on both accuracies and macro - F1 scores << for >> [[ D1 , D3 , D4 ]] .",0
205,4336,"2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on [[ both accuracies and macro - F1 scores ]] << for >> [[ D1 , D3 , D4 ]] .",1
206,1254,"We compare our model , WEBQA , to [[ STAGG and COMPQ ]] , << which are >> to the best of our knowledge the [[ highest performing semantic parsing models ]] on both COMPLEXQUESTIONS and WEBQUES - TIONS .",1
207,1254,"We compare our model , WEBQA , to [[ STAGG and COMPQ ]] , << which are >> to the best of our knowledge the highest performing semantic parsing models on both [[ COMPLEXQUESTIONS and WEBQUES - TIONS ]] .",0
208,1254,"We compare our model , WEBQA , to STAGG and COMPQ , << which are >> to the best of our knowledge the [[ highest performing semantic parsing models ]] on both [[ COMPLEXQUESTIONS and WEBQUES - TIONS ]] .",0
209,1254,"We compare our model , WEBQA , to [[ STAGG and COMPQ ]] , which are to the best of our knowledge the [[ highest performing semantic parsing models ]] << on >> both COMPLEXQUESTIONS and WEBQUES - TIONS .",0
210,1254,"We compare our model , WEBQA , to [[ STAGG and COMPQ ]] , which are to the best of our knowledge the highest performing semantic parsing models << on >> both [[ COMPLEXQUESTIONS and WEBQUES - TIONS ]] .",0
211,1254,"We compare our model , WEBQA , to STAGG and COMPQ , which are to the best of our knowledge the [[ highest performing semantic parsing models ]] << on >> both [[ COMPLEXQUESTIONS and WEBQUES - TIONS ]] .",1
212,4780,The [[ best performing system ]] << in >> the [[ SemEval 2017 task ]] is the one described in which achieved an F P N of 0.61 .,1
213,1639,[[ Out - of - vocabulary ( OOV ) words ]] << are >> [[ initialized randomly ]] with Gaussian samples .,1
214,1639,[[ Out - of - vocabulary ( OOV ) words ]] << are >> initialized randomly with [[ Gaussian samples ]] .,0
215,1639,Out - of - vocabulary ( OOV ) words << are >> [[ initialized randomly ]] with [[ Gaussian samples ]] .,0
216,1639,[[ Out - of - vocabulary ( OOV ) words ]] are [[ initialized randomly ]] << with >> Gaussian samples .,0
217,1639,[[ Out - of - vocabulary ( OOV ) words ]] are initialized randomly << with >> [[ Gaussian samples ]] .,0
218,1639,Out - of - vocabulary ( OOV ) words are [[ initialized randomly ]] << with >> [[ Gaussian samples ]] .,1
219,1133,"<< For >> the [[ skip - gram model ]] , [[ our model ]] refers to the word2 vec module in open source software library , Tensorflow , the skip window is set as 2 .",0
220,1133,"<< For >> the [[ skip - gram model ]] , our model refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is set as 2 .",0
221,1133,"<< For >> the [[ skip - gram model ]] , our model refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
222,1133,"<< For >> the [[ skip - gram model ]] , our model refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
223,1133,"<< For >> the [[ skip - gram model ]] , our model refers to the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
224,1133,"<< For >> the [[ skip - gram model ]] , our model refers to the word2 vec module in open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
225,1133,"<< For >> the skip - gram model , [[ our model ]] refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is set as 2 .",0
226,1133,"<< For >> the skip - gram model , [[ our model ]] refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
227,1133,"<< For >> the skip - gram model , [[ our model ]] refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
228,1133,"<< For >> the skip - gram model , [[ our model ]] refers to the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
229,1133,"<< For >> the skip - gram model , [[ our model ]] refers to the word2 vec module in open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
230,1133,"<< For >> the skip - gram model , our model refers to the [[ word2 vec module ]] in [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
231,1133,"<< For >> the skip - gram model , our model refers to the [[ word2 vec module ]] in open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
232,1133,"<< For >> the skip - gram model , our model refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
233,1133,"<< For >> the skip - gram model , our model refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
234,1133,"<< For >> the skip - gram model , our model refers to the word2 vec module in [[ open source software library ]] , [[ Tensorflow ]] , the skip window is set as 2 .",0
235,1133,"<< For >> the skip - gram model , our model refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the [[ skip window ]] is set as 2 .",0
236,1133,"<< For >> the skip - gram model , our model refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is set as [[ 2 ]] .",0
237,1133,"<< For >> the skip - gram model , our model refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the [[ skip window ]] is set as 2 .",0
238,1133,"<< For >> the skip - gram model , our model refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is set as [[ 2 ]] .",0
239,1133,"<< For >> the skip - gram model , our model refers to the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is set as [[ 2 ]] .",0
240,1133,"For the [[ skip - gram model ]] , [[ our model ]] << refers to >> the word2 vec module in open source software library , Tensorflow , the skip window is set as 2 .",0
241,1133,"For the [[ skip - gram model ]] , our model << refers to >> the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is set as 2 .",0
242,1133,"For the [[ skip - gram model ]] , our model << refers to >> the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
243,1133,"For the [[ skip - gram model ]] , our model << refers to >> the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
244,1133,"For the [[ skip - gram model ]] , our model << refers to >> the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
245,1133,"For the [[ skip - gram model ]] , our model << refers to >> the word2 vec module in open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
246,1133,"For the skip - gram model , [[ our model ]] << refers to >> the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is set as 2 .",1
247,1133,"For the skip - gram model , [[ our model ]] << refers to >> the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
248,1133,"For the skip - gram model , [[ our model ]] << refers to >> the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
249,1133,"For the skip - gram model , [[ our model ]] << refers to >> the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
250,1133,"For the skip - gram model , [[ our model ]] << refers to >> the word2 vec module in open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
251,1133,"For the skip - gram model , our model << refers to >> the [[ word2 vec module ]] in [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
252,1133,"For the skip - gram model , our model << refers to >> the [[ word2 vec module ]] in open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
253,1133,"For the skip - gram model , our model << refers to >> the [[ word2 vec module ]] in open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
254,1133,"For the skip - gram model , our model << refers to >> the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
255,1133,"For the skip - gram model , our model << refers to >> the word2 vec module in [[ open source software library ]] , [[ Tensorflow ]] , the skip window is set as 2 .",0
256,1133,"For the skip - gram model , our model << refers to >> the word2 vec module in [[ open source software library ]] , Tensorflow , the [[ skip window ]] is set as 2 .",0
257,1133,"For the skip - gram model , our model << refers to >> the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is set as [[ 2 ]] .",0
258,1133,"For the skip - gram model , our model << refers to >> the word2 vec module in open source software library , [[ Tensorflow ]] , the [[ skip window ]] is set as 2 .",0
259,1133,"For the skip - gram model , our model << refers to >> the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is set as [[ 2 ]] .",0
260,1133,"For the skip - gram model , our model << refers to >> the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is set as [[ 2 ]] .",0
261,1133,"For the [[ skip - gram model ]] , [[ our model ]] refers to the word2 vec module << in >> open source software library , Tensorflow , the skip window is set as 2 .",0
262,1133,"For the [[ skip - gram model ]] , our model refers to the [[ word2 vec module ]] << in >> open source software library , Tensorflow , the skip window is set as 2 .",0
263,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module << in >> [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
264,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module << in >> open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
265,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module << in >> open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
266,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module << in >> open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
267,1133,"For the skip - gram model , [[ our model ]] refers to the [[ word2 vec module ]] << in >> open source software library , Tensorflow , the skip window is set as 2 .",0
268,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module << in >> [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",0
269,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module << in >> open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
270,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module << in >> open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
271,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module << in >> open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
272,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] << in >> [[ open source software library ]] , Tensorflow , the skip window is set as 2 .",1
273,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] << in >> open source software library , [[ Tensorflow ]] , the skip window is set as 2 .",0
274,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] << in >> open source software library , Tensorflow , the [[ skip window ]] is set as 2 .",0
275,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] << in >> open source software library , Tensorflow , the skip window is set as [[ 2 ]] .",0
276,1133,"For the skip - gram model , our model refers to the word2 vec module << in >> [[ open source software library ]] , [[ Tensorflow ]] , the skip window is set as 2 .",0
277,1133,"For the skip - gram model , our model refers to the word2 vec module << in >> [[ open source software library ]] , Tensorflow , the [[ skip window ]] is set as 2 .",0
278,1133,"For the skip - gram model , our model refers to the word2 vec module << in >> [[ open source software library ]] , Tensorflow , the skip window is set as [[ 2 ]] .",0
279,1133,"For the skip - gram model , our model refers to the word2 vec module << in >> open source software library , [[ Tensorflow ]] , the [[ skip window ]] is set as 2 .",0
280,1133,"For the skip - gram model , our model refers to the word2 vec module << in >> open source software library , [[ Tensorflow ]] , the skip window is set as [[ 2 ]] .",0
281,1133,"For the skip - gram model , our model refers to the word2 vec module << in >> open source software library , Tensorflow , the [[ skip window ]] is set as [[ 2 ]] .",0
282,1133,"For the [[ skip - gram model ]] , [[ our model ]] refers to the word2 vec module in open source software library , Tensorflow , the skip window is << set as >> 2 .",0
283,1133,"For the [[ skip - gram model ]] , our model refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is << set as >> 2 .",0
284,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is << set as >> 2 .",0
285,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is << set as >> 2 .",0
286,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is << set as >> 2 .",0
287,1133,"For the [[ skip - gram model ]] , our model refers to the word2 vec module in open source software library , Tensorflow , the skip window is << set as >> [[ 2 ]] .",0
288,1133,"For the skip - gram model , [[ our model ]] refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is << set as >> 2 .",0
289,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is << set as >> 2 .",0
290,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is << set as >> 2 .",0
291,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is << set as >> 2 .",0
292,1133,"For the skip - gram model , [[ our model ]] refers to the word2 vec module in open source software library , Tensorflow , the skip window is << set as >> [[ 2 ]] .",0
293,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] in [[ open source software library ]] , Tensorflow , the skip window is << set as >> 2 .",0
294,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] in open source software library , [[ Tensorflow ]] , the skip window is << set as >> 2 .",0
295,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the [[ skip window ]] is << set as >> 2 .",0
296,1133,"For the skip - gram model , our model refers to the [[ word2 vec module ]] in open source software library , Tensorflow , the skip window is << set as >> [[ 2 ]] .",0
297,1133,"For the skip - gram model , our model refers to the word2 vec module in [[ open source software library ]] , [[ Tensorflow ]] , the skip window is << set as >> 2 .",0
298,1133,"For the skip - gram model , our model refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the [[ skip window ]] is << set as >> 2 .",0
299,1133,"For the skip - gram model , our model refers to the word2 vec module in [[ open source software library ]] , Tensorflow , the skip window is << set as >> [[ 2 ]] .",0
300,1133,"For the skip - gram model , our model refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the [[ skip window ]] is << set as >> 2 .",0
301,1133,"For the skip - gram model , our model refers to the word2 vec module in open source software library , [[ Tensorflow ]] , the skip window is << set as >> [[ 2 ]] .",0
302,1133,"For the skip - gram model , our model refers to the word2 vec module in open source software library , Tensorflow , the [[ skip window ]] is << set as >> [[ 2 ]] .",1
303,5719,We << find >> that the [[ AEM model ]] achieves [[ significant improvement ]] on the diversity of generated text .,0
304,5719,We << find >> that the [[ AEM model ]] achieves significant improvement on the [[ diversity of generated text ]] .,0
305,5719,We << find >> that the AEM model achieves [[ significant improvement ]] on the [[ diversity of generated text ]] .,0
306,5719,We find that the [[ AEM model ]] << achieves >> [[ significant improvement ]] on the diversity of generated text .,1
307,5719,We find that the [[ AEM model ]] << achieves >> significant improvement on the [[ diversity of generated text ]] .,0
308,5719,We find that the AEM model << achieves >> [[ significant improvement ]] on the [[ diversity of generated text ]] .,0
309,5719,We find that the [[ AEM model ]] achieves [[ significant improvement ]] << on >> the diversity of generated text .,0
310,5719,We find that the [[ AEM model ]] achieves significant improvement << on >> the [[ diversity of generated text ]] .,0
311,5719,We find that the AEM model achieves [[ significant improvement ]] << on >> the [[ diversity of generated text ]] .,1
312,3519,"However , [[ our AGGCN model ]] still << obtains >> [[ 8.0 and 5.7 points ]] higher than the GS GLSTM model for ternary and binary relations , respectively .",1
313,3519,"However , [[ our AGGCN model ]] still << obtains >> 8.0 and 5.7 points higher than the [[ GS GLSTM model ]] for ternary and binary relations , respectively .",0
314,3519,"However , [[ our AGGCN model ]] still << obtains >> 8.0 and 5.7 points higher than the GS GLSTM model for [[ ternary and binary relations ]] , respectively .",0
315,3519,"However , our AGGCN model still << obtains >> [[ 8.0 and 5.7 points ]] higher than the [[ GS GLSTM model ]] for ternary and binary relations , respectively .",0
316,3519,"However , our AGGCN model still << obtains >> [[ 8.0 and 5.7 points ]] higher than the GS GLSTM model for [[ ternary and binary relations ]] , respectively .",0
317,3519,"However , our AGGCN model still << obtains >> 8.0 and 5.7 points higher than the [[ GS GLSTM model ]] for [[ ternary and binary relations ]] , respectively .",0
318,3519,"However , [[ our AGGCN model ]] still obtains [[ 8.0 and 5.7 points ]] << higher than >> the GS GLSTM model for ternary and binary relations , respectively .",0
319,3519,"However , [[ our AGGCN model ]] still obtains 8.0 and 5.7 points << higher than >> the [[ GS GLSTM model ]] for ternary and binary relations , respectively .",0
320,3519,"However , [[ our AGGCN model ]] still obtains 8.0 and 5.7 points << higher than >> the GS GLSTM model for [[ ternary and binary relations ]] , respectively .",0
321,3519,"However , our AGGCN model still obtains [[ 8.0 and 5.7 points ]] << higher than >> the [[ GS GLSTM model ]] for ternary and binary relations , respectively .",1
322,3519,"However , our AGGCN model still obtains [[ 8.0 and 5.7 points ]] << higher than >> the GS GLSTM model for [[ ternary and binary relations ]] , respectively .",0
323,3519,"However , our AGGCN model still obtains 8.0 and 5.7 points << higher than >> the [[ GS GLSTM model ]] for [[ ternary and binary relations ]] , respectively .",0
324,3519,"However , [[ our AGGCN model ]] still obtains [[ 8.0 and 5.7 points ]] higher than the GS GLSTM model << for >> ternary and binary relations , respectively .",0
325,3519,"However , [[ our AGGCN model ]] still obtains 8.0 and 5.7 points higher than the [[ GS GLSTM model ]] << for >> ternary and binary relations , respectively .",0
326,3519,"However , [[ our AGGCN model ]] still obtains 8.0 and 5.7 points higher than the GS GLSTM model << for >> [[ ternary and binary relations ]] , respectively .",0
327,3519,"However , our AGGCN model still obtains [[ 8.0 and 5.7 points ]] higher than the [[ GS GLSTM model ]] << for >> ternary and binary relations , respectively .",0
328,3519,"However , our AGGCN model still obtains [[ 8.0 and 5.7 points ]] higher than the GS GLSTM model << for >> [[ ternary and binary relations ]] , respectively .",0
329,3519,"However , our AGGCN model still obtains 8.0 and 5.7 points higher than the [[ GS GLSTM model ]] << for >> [[ ternary and binary relations ]] , respectively .",1
330,271,Our over all [[ iterated dilated CNN architecture ( ID - CNN ) ]] repeatedly << applies >> the [[ same block of dilated convolutions ]] to token - wise representations .,1
331,271,Our over all [[ iterated dilated CNN architecture ( ID - CNN ) ]] repeatedly << applies >> the same block of dilated convolutions to [[ token - wise representations ]] .,0
332,271,Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly << applies >> the [[ same block of dilated convolutions ]] to [[ token - wise representations ]] .,0
333,271,Our over all [[ iterated dilated CNN architecture ( ID - CNN ) ]] repeatedly applies the [[ same block of dilated convolutions ]] << to >> token - wise representations .,0
334,271,Our over all [[ iterated dilated CNN architecture ( ID - CNN ) ]] repeatedly applies the same block of dilated convolutions << to >> [[ token - wise representations ]] .,0
335,271,Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the [[ same block of dilated convolutions ]] << to >> [[ token - wise representations ]] .,1
336,1473,[[ Our transfer learning approach ]] << obtains >> [[ better results ]] than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,1
337,1473,[[ Our transfer learning approach ]] << obtains >> better results than [[ previous state - of - the - art ]] on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,0
338,1473,Our transfer learning approach << obtains >> [[ better results ]] than [[ previous state - of - the - art ]] on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,0
339,1473,[[ Our transfer learning approach ]] obtains [[ better results ]] << than >> previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,0
340,1473,[[ Our transfer learning approach ]] obtains better results << than >> [[ previous state - of - the - art ]] on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,0
341,1473,Our transfer learning approach obtains [[ better results ]] << than >> [[ previous state - of - the - art ]] on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,1
342,5349,"[[ SP + ILP ]] << further improved >> the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .",1
343,5349,"[[ SP + ILP ]] << further improved >> the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .",0
344,5349,"[[ SP + ILP ]] << further improved >> the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of 67.2 % .",0
345,5349,"[[ SP + ILP ]] << further improved >> the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of [[ 67.2 % ]] .",0
346,5349,"SP + ILP << further improved >> the [[ performance ]] in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .",0
347,5349,"SP + ILP << further improved >> the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of 67.2 % .",0
348,5349,"SP + ILP << further improved >> the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of [[ 67.2 % ]] .",0
349,5349,"SP + ILP << further improved >> the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of 67.2 % .",0
350,5349,"SP + ILP << further improved >> the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of [[ 67.2 % ]] .",0
351,5349,"SP + ILP << further improved >> the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of [[ 67.2 % ]] .",0
352,5349,"[[ SP + ILP ]] further improved the [[ performance ]] << in >> precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .",0
353,5349,"[[ SP + ILP ]] further improved the performance << in >> [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .",0
354,5349,"[[ SP + ILP ]] further improved the performance << in >> precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of 67.2 % .",0
355,5349,"[[ SP + ILP ]] further improved the performance << in >> precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of [[ 67.2 % ]] .",0
356,5349,"SP + ILP further improved the [[ performance ]] << in >> [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .",1
357,5349,"SP + ILP further improved the [[ performance ]] << in >> precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of 67.2 % .",0
358,5349,"SP + ILP further improved the [[ performance ]] << in >> precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of [[ 67.2 % ]] .",0
359,5349,"SP + ILP further improved the performance << in >> [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of 67.2 % .",0
360,5349,"SP + ILP further improved the performance << in >> [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of [[ 67.2 % ]] .",0
361,5349,"SP + ILP further improved the performance << in >> precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] of [[ 67.2 % ]] .",0
362,5349,"[[ SP + ILP ]] further improved the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an F 1 score of 67.2 % .",0
363,5349,"[[ SP + ILP ]] further improved the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an F 1 score of 67.2 % .",0
364,5349,"[[ SP + ILP ]] further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an [[ F 1 score ]] of 67.2 % .",1
365,5349,"[[ SP + ILP ]] further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an F 1 score of [[ 67.2 % ]] .",0
366,5349,"SP + ILP further improved the [[ performance ]] in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an F 1 score of 67.2 % .",0
367,5349,"SP + ILP further improved the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an [[ F 1 score ]] of 67.2 % .",0
368,5349,"SP + ILP further improved the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an F 1 score of [[ 67.2 % ]] .",0
369,5349,"SP + ILP further improved the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an [[ F 1 score ]] of 67.2 % .",0
370,5349,"SP + ILP further improved the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an F 1 score of [[ 67.2 % ]] .",0
371,5349,"SP + ILP further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , << reaching >> an [[ F 1 score ]] of [[ 67.2 % ]] .",0
372,5349,"[[ SP + ILP ]] further improved the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score << of >> 67.2 % .",0
373,5349,"[[ SP + ILP ]] further improved the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score << of >> 67.2 % .",0
374,5349,"[[ SP + ILP ]] further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] << of >> 67.2 % .",0
375,5349,"[[ SP + ILP ]] further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score << of >> [[ 67.2 % ]] .",0
376,5349,"SP + ILP further improved the [[ performance ]] in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score << of >> 67.2 % .",0
377,5349,"SP + ILP further improved the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] << of >> 67.2 % .",0
378,5349,"SP + ILP further improved the [[ performance ]] in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score << of >> [[ 67.2 % ]] .",0
379,5349,"SP + ILP further improved the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] << of >> 67.2 % .",0
380,5349,"SP + ILP further improved the performance in [[ precision , recall , and F 1 ]] significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score << of >> [[ 67.2 % ]] .",0
381,5349,"SP + ILP further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an [[ F 1 score ]] << of >> [[ 67.2 % ]] .",1
382,4310,"[[ IT , NP , and CNP ]] can << achieve >> [[ very good scores ]] in some cases but are less stable .",1
383,4310,"[[ IT , NP , and CNP ]] can << achieve >> very good scores in [[ some cases ]] but are less stable .",0
384,4310,"[[ IT , NP , and CNP ]] can << achieve >> very good scores in some cases but are [[ less stable ]] .",0
385,4310,"IT , NP , and CNP can << achieve >> [[ very good scores ]] in [[ some cases ]] but are less stable .",0
386,4310,"IT , NP , and CNP can << achieve >> [[ very good scores ]] in some cases but are [[ less stable ]] .",0
387,4310,"IT , NP , and CNP can << achieve >> very good scores in [[ some cases ]] but are [[ less stable ]] .",0
388,4310,"[[ IT , NP , and CNP ]] can achieve [[ very good scores ]] << in >> some cases but are less stable .",0
389,4310,"[[ IT , NP , and CNP ]] can achieve very good scores << in >> [[ some cases ]] but are less stable .",0
390,4310,"[[ IT , NP , and CNP ]] can achieve very good scores << in >> some cases but are [[ less stable ]] .",0
391,4310,"IT , NP , and CNP can achieve [[ very good scores ]] << in >> [[ some cases ]] but are less stable .",1
392,4310,"IT , NP , and CNP can achieve [[ very good scores ]] << in >> some cases but are [[ less stable ]] .",0
393,4310,"IT , NP , and CNP can achieve very good scores << in >> [[ some cases ]] but are [[ less stable ]] .",0
394,4310,"[[ IT , NP , and CNP ]] can achieve [[ very good scores ]] in some cases but << are >> less stable .",0
395,4310,"[[ IT , NP , and CNP ]] can achieve very good scores in [[ some cases ]] but << are >> less stable .",0
396,4310,"[[ IT , NP , and CNP ]] can achieve very good scores in some cases but << are >> [[ less stable ]] .",0
397,4310,"IT , NP , and CNP can achieve [[ very good scores ]] in [[ some cases ]] but << are >> less stable .",0
398,4310,"IT , NP , and CNP can achieve [[ very good scores ]] in some cases but << are >> [[ less stable ]] .",1
399,4310,"IT , NP , and CNP can achieve very good scores in [[ some cases ]] but << are >> [[ less stable ]] .",0
400,5699,"<< In >> [[ all measured metrics ]] , [[ LeakGAN ]] shows significant performance gain compared to baseline models .",0
401,5699,"<< In >> [[ all measured metrics ]] , LeakGAN shows [[ significant performance gain ]] compared to baseline models .",0
402,5699,"<< In >> [[ all measured metrics ]] , LeakGAN shows significant performance gain compared to [[ baseline models ]] .",0
403,5699,"<< In >> all measured metrics , [[ LeakGAN ]] shows [[ significant performance gain ]] compared to baseline models .",0
404,5699,"<< In >> all measured metrics , [[ LeakGAN ]] shows significant performance gain compared to [[ baseline models ]] .",0
405,5699,"<< In >> all measured metrics , LeakGAN shows [[ significant performance gain ]] compared to [[ baseline models ]] .",0
406,5699,"In [[ all measured metrics ]] , [[ LeakGAN ]] << shows >> significant performance gain compared to baseline models .",0
407,5699,"In [[ all measured metrics ]] , LeakGAN << shows >> [[ significant performance gain ]] compared to baseline models .",0
408,5699,"In [[ all measured metrics ]] , LeakGAN << shows >> significant performance gain compared to [[ baseline models ]] .",0
409,5699,"In all measured metrics , [[ LeakGAN ]] << shows >> [[ significant performance gain ]] compared to baseline models .",1
410,5699,"In all measured metrics , [[ LeakGAN ]] << shows >> significant performance gain compared to [[ baseline models ]] .",0
411,5699,"In all measured metrics , LeakGAN << shows >> [[ significant performance gain ]] compared to [[ baseline models ]] .",0
412,5699,"In [[ all measured metrics ]] , [[ LeakGAN ]] shows significant performance gain << compared to >> baseline models .",0
413,5699,"In [[ all measured metrics ]] , LeakGAN shows [[ significant performance gain ]] << compared to >> baseline models .",0
414,5699,"In [[ all measured metrics ]] , LeakGAN shows significant performance gain << compared to >> [[ baseline models ]] .",0
415,5699,"In all measured metrics , [[ LeakGAN ]] shows [[ significant performance gain ]] << compared to >> baseline models .",0
416,5699,"In all measured metrics , [[ LeakGAN ]] shows significant performance gain << compared to >> [[ baseline models ]] .",0
417,5699,"In all measured metrics , LeakGAN shows [[ significant performance gain ]] << compared to >> [[ baseline models ]] .",1
418,4440,"In this paper , we << propose >> a [[ novel neural architecture ]] for [[ discriminative sentence modeling ]] , called the Tree - Based Convolutional Neural Network ( TBCNN ) .",0
419,4440,"In this paper , we << propose >> a [[ novel neural architecture ]] for discriminative sentence modeling , called the [[ Tree - Based Convolutional Neural Network ( TBCNN ) ]] .",0
420,4440,"In this paper , we << propose >> a novel neural architecture for [[ discriminative sentence modeling ]] , called the [[ Tree - Based Convolutional Neural Network ( TBCNN ) ]] .",0
421,4440,"In this paper , we propose a [[ novel neural architecture ]] << for >> [[ discriminative sentence modeling ]] , called the Tree - Based Convolutional Neural Network ( TBCNN ) .",1
422,4440,"In this paper , we propose a [[ novel neural architecture ]] << for >> discriminative sentence modeling , called the [[ Tree - Based Convolutional Neural Network ( TBCNN ) ]] .",0
423,4440,"In this paper , we propose a novel neural architecture << for >> [[ discriminative sentence modeling ]] , called the [[ Tree - Based Convolutional Neural Network ( TBCNN ) ]] .",0
424,4440,"In this paper , we propose a [[ novel neural architecture ]] for [[ discriminative sentence modeling ]] , << called >> the Tree - Based Convolutional Neural Network ( TBCNN ) .",0
425,4440,"In this paper , we propose a [[ novel neural architecture ]] for discriminative sentence modeling , << called >> the [[ Tree - Based Convolutional Neural Network ( TBCNN ) ]] .",0
426,4440,"In this paper , we propose a novel neural architecture for [[ discriminative sentence modeling ]] , << called >> the [[ Tree - Based Convolutional Neural Network ( TBCNN ) ]] .",0
427,838,"To illustrate this idea , we << take >> a [[ model ]] that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
428,838,"To illustrate this idea , we << take >> a [[ model ]] that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
429,838,"To illustrate this idea , we << take >> a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
430,838,"To illustrate this idea , we << take >> a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
431,838,"To illustrate this idea , we << take >> a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
432,838,"To illustrate this idea , we << take >> a model that carries out [[ only basic question - document interaction ]] and prepend to it a [[ module ]] that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
433,838,"To illustrate this idea , we << take >> a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
434,838,"To illustrate this idea , we << take >> a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
435,838,"To illustrate this idea , we << take >> a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
436,838,"To illustrate this idea , we << take >> a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
437,838,"To illustrate this idea , we << take >> a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
438,838,"To illustrate this idea , we << take >> a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
439,838,"To illustrate this idea , we << take >> a model that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
440,838,"To illustrate this idea , we << take >> a model that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
441,838,"To illustrate this idea , we << take >> a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by [[ explicitly gating ]] between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
442,838,"To illustrate this idea , we take a [[ model ]] that << carries out >> [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",1
443,838,"To illustrate this idea , we take a [[ model ]] that << carries out >> only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
444,838,"To illustrate this idea , we take a [[ model ]] that << carries out >> only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
445,838,"To illustrate this idea , we take a [[ model ]] that << carries out >> only basic question - document interaction and prepend to it a module that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
446,838,"To illustrate this idea , we take a [[ model ]] that << carries out >> only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
447,838,"To illustrate this idea , we take a model that << carries out >> [[ only basic question - document interaction ]] and prepend to it a [[ module ]] that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
448,838,"To illustrate this idea , we take a model that << carries out >> [[ only basic question - document interaction ]] and prepend to it a module that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
449,838,"To illustrate this idea , we take a model that << carries out >> [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
450,838,"To illustrate this idea , we take a model that << carries out >> [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
451,838,"To illustrate this idea , we take a model that << carries out >> only basic question - document interaction and prepend to it a [[ module ]] that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
452,838,"To illustrate this idea , we take a model that << carries out >> only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
453,838,"To illustrate this idea , we take a model that << carries out >> only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
454,838,"To illustrate this idea , we take a model that << carries out >> only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
455,838,"To illustrate this idea , we take a model that << carries out >> only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
456,838,"To illustrate this idea , we take a model that << carries out >> only basic question - document interaction and prepend to it a module that produces token embeddings by [[ explicitly gating ]] between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
457,838,"To illustrate this idea , we take a [[ model ]] that carries out [[ only basic question - document interaction ]] and << prepend to >> it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
458,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and << prepend to >> it a [[ module ]] that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
459,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and << prepend to >> it a module that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
460,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and << prepend to >> it a module that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
461,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and << prepend to >> it a module that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
462,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and << prepend to >> it a [[ module ]] that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",1
463,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and << prepend to >> it a module that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
464,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and << prepend to >> it a module that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
465,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and << prepend to >> it a module that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
466,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and << prepend to >> it a [[ module ]] that produces [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
467,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and << prepend to >> it a [[ module ]] that produces token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
468,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and << prepend to >> it a [[ module ]] that produces token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
469,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and << prepend to >> it a module that produces [[ token embeddings ]] by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
470,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and << prepend to >> it a module that produces [[ token embeddings ]] by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
471,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and << prepend to >> it a module that produces token embeddings by [[ explicitly gating ]] between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
472,838,"To illustrate this idea , we take a [[ model ]] that carries out [[ only basic question - document interaction ]] and prepend to it a module << that produces >> token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
473,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a [[ module ]] << that produces >> token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
474,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module << that produces >> [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
475,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module << that produces >> token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
476,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module << that produces >> token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
477,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a [[ module ]] << that produces >> token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
478,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module << that produces >> [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
479,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module << that produces >> token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
480,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module << that produces >> token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
481,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] << that produces >> [[ token embeddings ]] by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",1
482,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] << that produces >> token embeddings by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
483,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] << that produces >> token embeddings by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
484,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module << that produces >> [[ token embeddings ]] by [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
485,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module << that produces >> [[ token embeddings ]] by explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
486,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module << that produces >> token embeddings by [[ explicitly gating ]] between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
487,838,"To illustrate this idea , we take a [[ model ]] that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings << by >> explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
488,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings << by >> explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
489,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] << by >> explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
490,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces token embeddings << by >> [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
491,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces token embeddings << by >> explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
492,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a [[ module ]] that produces token embeddings << by >> explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
493,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces [[ token embeddings ]] << by >> explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
494,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings << by >> [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
495,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings << by >> explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
496,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces [[ token embeddings ]] << by >> explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",0
497,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings << by >> [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",0
498,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings << by >> explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
499,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] << by >> [[ explicitly gating ]] between contextual and non-contextual representations ( for both the document and question ) .",1
500,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] << by >> explicitly gating between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
501,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings << by >> [[ explicitly gating ]] between [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
502,838,"To illustrate this idea , we take a [[ model ]] that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by explicitly gating << between >> contextual and non-contextual representations ( for both the document and question ) .",0
503,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by explicitly gating << between >> contextual and non-contextual representations ( for both the document and question ) .",0
504,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by explicitly gating << between >> contextual and non-contextual representations ( for both the document and question ) .",0
505,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by [[ explicitly gating ]] << between >> contextual and non-contextual representations ( for both the document and question ) .",0
506,838,"To illustrate this idea , we take a [[ model ]] that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating << between >> [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
507,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a [[ module ]] that produces token embeddings by explicitly gating << between >> contextual and non-contextual representations ( for both the document and question ) .",0
508,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces [[ token embeddings ]] by explicitly gating << between >> contextual and non-contextual representations ( for both the document and question ) .",0
509,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by [[ explicitly gating ]] << between >> contextual and non-contextual representations ( for both the document and question ) .",0
510,838,"To illustrate this idea , we take a model that carries out [[ only basic question - document interaction ]] and prepend to it a module that produces token embeddings by explicitly gating << between >> [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
511,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces [[ token embeddings ]] by explicitly gating << between >> contextual and non-contextual representations ( for both the document and question ) .",0
512,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by [[ explicitly gating ]] << between >> contextual and non-contextual representations ( for both the document and question ) .",0
513,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a [[ module ]] that produces token embeddings by explicitly gating << between >> [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
514,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by [[ explicitly gating ]] << between >> contextual and non-contextual representations ( for both the document and question ) .",0
515,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces [[ token embeddings ]] by explicitly gating << between >> [[ contextual and non-contextual representations ]] ( for both the document and question ) .",0
516,838,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by [[ explicitly gating ]] << between >> [[ contextual and non-contextual representations ]] ( for both the document and question ) .",1
517,2195,We [[ renormalize ]] the [[ gradient ]] << if >> its norm is greater than 5 .,0
518,2195,We [[ renormalize ]] the gradient << if >> its [[ norm ]] is greater than 5 .,0
519,2195,We [[ renormalize ]] the gradient << if >> its norm is greater than [[ 5 ]] .,0
520,2195,We renormalize the [[ gradient ]] << if >> its [[ norm ]] is greater than 5 .,1
521,2195,We renormalize the [[ gradient ]] << if >> its norm is greater than [[ 5 ]] .,0
522,2195,We renormalize the gradient << if >> its [[ norm ]] is greater than [[ 5 ]] .,0
523,2195,We [[ renormalize ]] the [[ gradient ]] if its norm is << greater than >> 5 .,0
524,2195,We [[ renormalize ]] the gradient if its [[ norm ]] is << greater than >> 5 .,0
525,2195,We [[ renormalize ]] the gradient if its norm is << greater than >> [[ 5 ]] .,0
526,2195,We renormalize the [[ gradient ]] if its [[ norm ]] is << greater than >> 5 .,0
527,2195,We renormalize the [[ gradient ]] if its norm is << greater than >> [[ 5 ]] .,0
528,2195,We renormalize the gradient if its [[ norm ]] is << greater than >> [[ 5 ]] .,1
529,4891,"[[ Location entity names ]] are << masked by >> [[ location1 and location 2 ]] in the whole dataset , so the task does not involve identification and segmentation of the named entities .",1
530,4891,"[[ Location entity names ]] are << masked by >> location1 and location 2 in the [[ whole dataset ]] , so the task does not involve identification and segmentation of the named entities .",0
531,4891,"Location entity names are << masked by >> [[ location1 and location 2 ]] in the [[ whole dataset ]] , so the task does not involve identification and segmentation of the named entities .",0
532,4891,"[[ Location entity names ]] are masked by [[ location1 and location 2 ]] << in >> the whole dataset , so the task does not involve identification and segmentation of the named entities .",0
533,4891,"[[ Location entity names ]] are masked by location1 and location 2 << in >> the [[ whole dataset ]] , so the task does not involve identification and segmentation of the named entities .",0
534,4891,"Location entity names are masked by [[ location1 and location 2 ]] << in >> the [[ whole dataset ]] , so the task does not involve identification and segmentation of the named entities .",1
535,412,"On the other hand , [[ BioBERT ]] << achieves >> [[ higher scores ]] than BERT on all the datasets .",1
536,412,"On the other hand , [[ BioBERT ]] << achieves >> higher scores than [[ BERT ]] on all the datasets .",0
537,412,"On the other hand , BioBERT << achieves >> [[ higher scores ]] than [[ BERT ]] on all the datasets .",0
538,412,"On the other hand , [[ BioBERT ]] achieves [[ higher scores ]] << than >> BERT on all the datasets .",0
539,412,"On the other hand , [[ BioBERT ]] achieves higher scores << than >> [[ BERT ]] on all the datasets .",0
540,412,"On the other hand , BioBERT achieves [[ higher scores ]] << than >> [[ BERT ]] on all the datasets .",1
541,1712,"[[ AP - biLSTM ]] [[ outperforms ]] the QA - biLSTM , but its performance is not as good << as >> the of AP - CNN .",0
542,1712,"[[ AP - biLSTM ]] outperforms the [[ QA - biLSTM ]] , but its performance is not as good << as >> the of AP - CNN .",0
543,1712,"[[ AP - biLSTM ]] outperforms the QA - biLSTM , but its [[ performance ]] is not as good << as >> the of AP - CNN .",0
544,1712,"[[ AP - biLSTM ]] outperforms the QA - biLSTM , but its performance is [[ not as good ]] << as >> the of AP - CNN .",0
545,1712,"[[ AP - biLSTM ]] outperforms the QA - biLSTM , but its performance is not as good << as >> the of [[ AP - CNN ]] .",0
546,1712,"AP - biLSTM [[ outperforms ]] the [[ QA - biLSTM ]] , but its performance is not as good << as >> the of AP - CNN .",0
547,1712,"AP - biLSTM [[ outperforms ]] the QA - biLSTM , but its [[ performance ]] is not as good << as >> the of AP - CNN .",0
548,1712,"AP - biLSTM [[ outperforms ]] the QA - biLSTM , but its performance is [[ not as good ]] << as >> the of AP - CNN .",0
549,1712,"AP - biLSTM [[ outperforms ]] the QA - biLSTM , but its performance is not as good << as >> the of [[ AP - CNN ]] .",0
550,1712,"AP - biLSTM outperforms the [[ QA - biLSTM ]] , but its [[ performance ]] is not as good << as >> the of AP - CNN .",0
551,1712,"AP - biLSTM outperforms the [[ QA - biLSTM ]] , but its performance is [[ not as good ]] << as >> the of AP - CNN .",0
552,1712,"AP - biLSTM outperforms the [[ QA - biLSTM ]] , but its performance is not as good << as >> the of [[ AP - CNN ]] .",0
553,1712,"AP - biLSTM outperforms the QA - biLSTM , but its [[ performance ]] is [[ not as good ]] << as >> the of AP - CNN .",0
554,1712,"AP - biLSTM outperforms the QA - biLSTM , but its [[ performance ]] is not as good << as >> the of [[ AP - CNN ]] .",0
555,1712,"AP - biLSTM outperforms the QA - biLSTM , but its performance is [[ not as good ]] << as >> the of [[ AP - CNN ]] .",1
556,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , << achieving >> the [[ accuracies ]] of 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",1
557,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , << achieving >> the accuracies of [[ 80.89 % and 76.02 % ]] on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
558,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , << achieving >> the accuracies of 80.89 % and 76.02 % on the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
559,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , << achieving >> the [[ accuracies ]] of [[ 80.89 % and 76.02 % ]] on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
560,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , << achieving >> the [[ accuracies ]] of 80.89 % and 76.02 % on the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
561,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , << achieving >> the accuracies of [[ 80.89 % and 76.02 % ]] on the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
562,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , achieving the [[ accuracies ]] << of >> 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
563,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , achieving the accuracies << of >> [[ 80.89 % and 76.02 % ]] on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
564,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , achieving the accuracies << of >> 80.89 % and 76.02 % on the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
565,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the [[ accuracies ]] << of >> [[ 80.89 % and 76.02 % ]] on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",1
566,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the [[ accuracies ]] << of >> 80.89 % and 76.02 % on the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
567,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the accuracies << of >> [[ 80.89 % and 76.02 % ]] on the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
568,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , achieving the [[ accuracies ]] of 80.89 % and 76.02 % << on >> the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
569,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , achieving the accuracies of [[ 80.89 % and 76.02 % ]] << on >> the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
570,4438,"And the achieved model is "" [[ Bi - GRU - PE ]] "" reported in the , achieving the accuracies of 80.89 % and 76.02 % << on >> the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
571,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the [[ accuracies ]] of [[ 80.89 % and 76.02 % ]] << on >> the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
572,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the [[ accuracies ]] of 80.89 % and 76.02 % << on >> the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",0
573,4438,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the accuracies of [[ 80.89 % and 76.02 % ]] << on >> the [[ two datasets ]] respectively , which are 1.34 % and 1.25 % lower than the proposed model .",1
574,5043,"( 3 ) [[ IAN ]] << achieves >> [[ slightly better results ]] with the previous LSTM - based methods , which interactively learns the attended aspect and context vector as final representation .",1
575,5043,"( 3 ) [[ IAN ]] << achieves >> slightly better results with the [[ previous LSTM - based methods ]] , which interactively learns the attended aspect and context vector as final representation .",0
576,5043,"( 3 ) IAN << achieves >> [[ slightly better results ]] with the [[ previous LSTM - based methods ]] , which interactively learns the attended aspect and context vector as final representation .",0
577,5043,"( 3 ) [[ IAN ]] achieves [[ slightly better results ]] << with >> the previous LSTM - based methods , which interactively learns the attended aspect and context vector as final representation .",0
578,5043,"( 3 ) [[ IAN ]] achieves slightly better results << with >> the [[ previous LSTM - based methods ]] , which interactively learns the attended aspect and context vector as final representation .",0
579,5043,"( 3 ) IAN achieves [[ slightly better results ]] << with >> the [[ previous LSTM - based methods ]] , which interactively learns the attended aspect and context vector as final representation .",1
580,2795,[[ WORDEMBED ]] : We first << represent >> [[ each short - text ]] as the sum of the embedding of the words it contains .,1
581,2795,[[ WORDEMBED ]] : We first << represent >> each short - text as the [[ sum of the embedding of the words it contains ]] .,0
582,2795,WORDEMBED : We first << represent >> [[ each short - text ]] as the [[ sum of the embedding of the words it contains ]] .,0
583,2795,[[ WORDEMBED ]] : We first represent [[ each short - text ]] << as >> the sum of the embedding of the words it contains .,0
584,2795,[[ WORDEMBED ]] : We first represent each short - text << as >> the [[ sum of the embedding of the words it contains ]] .,0
585,2795,WORDEMBED : We first represent [[ each short - text ]] << as >> the [[ sum of the embedding of the words it contains ]] .,1
586,377,"Additionally , << for >> [[ BERT LARGE ]] we found that finetuning was sometimes unstable on small datasets , so we ran [[ several random restarts ]] and selected the best model on the Dev set .",0
587,377,"Additionally , << for >> [[ BERT LARGE ]] we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the [[ best model on the Dev set ]] .",0
588,377,"Additionally , << for >> BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran [[ several random restarts ]] and selected the [[ best model on the Dev set ]] .",0
589,377,"Additionally , for [[ BERT LARGE ]] we found that finetuning was sometimes unstable on small datasets , so we << ran >> [[ several random restarts ]] and selected the best model on the Dev set .",1
590,377,"Additionally , for [[ BERT LARGE ]] we found that finetuning was sometimes unstable on small datasets , so we << ran >> several random restarts and selected the [[ best model on the Dev set ]] .",0
591,377,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we << ran >> [[ several random restarts ]] and selected the [[ best model on the Dev set ]] .",0
592,377,"Additionally , for [[ BERT LARGE ]] we found that finetuning was sometimes unstable on small datasets , so we ran [[ several random restarts ]] and << selected >> the best model on the Dev set .",0
593,377,"Additionally , for [[ BERT LARGE ]] we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and << selected >> the [[ best model on the Dev set ]] .",0
594,377,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran [[ several random restarts ]] and << selected >> the [[ best model on the Dev set ]] .",1
595,1491,The [[ POS model ]] << contains >> [[ syntactic information ]] with 39 different POS tags that serve as both input and output .,1
596,1491,The [[ POS model ]] << contains >> syntactic information with [[ 39 different POS tags ]] that serve as both input and output .,0
597,1491,The [[ POS model ]] << contains >> syntactic information with 39 different POS tags that serve as [[ both input and output ]] .,0
598,1491,The POS model << contains >> [[ syntactic information ]] with [[ 39 different POS tags ]] that serve as both input and output .,0
599,1491,The POS model << contains >> [[ syntactic information ]] with 39 different POS tags that serve as [[ both input and output ]] .,0
600,1491,The POS model << contains >> syntactic information with [[ 39 different POS tags ]] that serve as [[ both input and output ]] .,0
601,1491,The [[ POS model ]] contains [[ syntactic information ]] << with >> 39 different POS tags that serve as both input and output .,0
602,1491,The [[ POS model ]] contains syntactic information << with >> [[ 39 different POS tags ]] that serve as both input and output .,0
603,1491,The [[ POS model ]] contains syntactic information << with >> 39 different POS tags that serve as [[ both input and output ]] .,0
604,1491,The POS model contains [[ syntactic information ]] << with >> [[ 39 different POS tags ]] that serve as both input and output .,1
605,1491,The POS model contains [[ syntactic information ]] << with >> 39 different POS tags that serve as [[ both input and output ]] .,0
606,1491,The POS model contains syntactic information << with >> [[ 39 different POS tags ]] that serve as [[ both input and output ]] .,0
607,1491,The [[ POS model ]] contains [[ syntactic information ]] with 39 different POS tags that << serve >> as both input and output .,0
608,1491,The [[ POS model ]] contains syntactic information with [[ 39 different POS tags ]] that << serve >> as both input and output .,0
609,1491,The [[ POS model ]] contains syntactic information with 39 different POS tags that << serve >> as [[ both input and output ]] .,0
610,1491,The POS model contains [[ syntactic information ]] with [[ 39 different POS tags ]] that << serve >> as both input and output .,0
611,1491,The POS model contains [[ syntactic information ]] with 39 different POS tags that << serve >> as [[ both input and output ]] .,1
612,1491,The POS model contains syntactic information with [[ 39 different POS tags ]] that << serve >> as [[ both input and output ]] .,0
613,738,"For all of our models we << used >> a [[ gradient descent optimization algorithm ]] based on the [[ Adam update rule ]] , which is pre-implemented in PyTorch .",0
614,738,"For all of our models we << used >> a [[ gradient descent optimization algorithm ]] based on the Adam update rule , which is pre-implemented in [[ PyTorch ]] .",0
615,738,"For all of our models we << used >> a gradient descent optimization algorithm based on the [[ Adam update rule ]] , which is pre-implemented in [[ PyTorch ]] .",0
616,738,"For all of our models we used a [[ gradient descent optimization algorithm ]] << based on >> the [[ Adam update rule ]] , which is pre-implemented in PyTorch .",1
617,738,"For all of our models we used a [[ gradient descent optimization algorithm ]] << based on >> the Adam update rule , which is pre-implemented in [[ PyTorch ]] .",0
618,738,"For all of our models we used a gradient descent optimization algorithm << based on >> the [[ Adam update rule ]] , which is pre-implemented in [[ PyTorch ]] .",0
619,738,"For all of our models we used a [[ gradient descent optimization algorithm ]] based on the [[ Adam update rule ]] , which is << pre-implemented in >> PyTorch .",0
620,738,"For all of our models we used a [[ gradient descent optimization algorithm ]] based on the Adam update rule , which is << pre-implemented in >> [[ PyTorch ]] .",1
621,738,"For all of our models we used a gradient descent optimization algorithm based on the [[ Adam update rule ]] , which is << pre-implemented in >> [[ PyTorch ]] .",0
622,3615,<< presents >> the [[ results of an ablation test ]] of [[ our position - aware attention model ]] on the development set of TACRED .,0
623,3615,<< presents >> the [[ results of an ablation test ]] of our position - aware attention model on the [[ development set of TACRED ]] .,0
624,3615,<< presents >> the results of an ablation test of [[ our position - aware attention model ]] on the [[ development set of TACRED ]] .,0
625,3615,presents the [[ results of an ablation test ]] << of >> [[ our position - aware attention model ]] on the development set of TACRED .,1
626,3615,presents the [[ results of an ablation test ]] << of >> our position - aware attention model on the [[ development set of TACRED ]] .,0
627,3615,presents the results of an ablation test << of >> [[ our position - aware attention model ]] on the [[ development set of TACRED ]] .,0
628,3615,presents the [[ results of an ablation test ]] of [[ our position - aware attention model ]] << on >> the development set of TACRED .,0
629,3615,presents the [[ results of an ablation test ]] of our position - aware attention model << on >> the [[ development set of TACRED ]] .,0
630,3615,presents the results of an ablation test of [[ our position - aware attention model ]] << on >> the [[ development set of TACRED ]] .,1
631,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we << employ >> a [[ discriminator ]] to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .",0
632,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we << employ >> a discriminator to evaluate the sequence and feedback the [[ evaluation ]] to guide the learning of the generative model .",0
633,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we << employ >> a discriminator to evaluate the sequence and feedback the evaluation to guide the [[ learning ]] of the generative model .",0
634,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we << employ >> a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the [[ generative model ]] .",0
635,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we << employ >> a [[ discriminator ]] to evaluate the sequence and feedback the [[ evaluation ]] to guide the learning of the generative model .",0
636,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we << employ >> a [[ discriminator ]] to evaluate the sequence and feedback the evaluation to guide the [[ learning ]] of the generative model .",0
637,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we << employ >> a [[ discriminator ]] to evaluate the sequence and feedback the evaluation to guide the learning of the [[ generative model ]] .",0
638,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we << employ >> a discriminator to evaluate the sequence and feedback the [[ evaluation ]] to guide the [[ learning ]] of the generative model .",0
639,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we << employ >> a discriminator to evaluate the sequence and feedback the [[ evaluation ]] to guide the learning of the [[ generative model ]] .",0
640,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we << employ >> a discriminator to evaluate the sequence and feedback the evaluation to guide the [[ learning ]] of the [[ generative model ]] .",0
641,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] << to evaluate >> the sequence and feedback the evaluation to guide the learning of the generative model .",0
642,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator << to evaluate >> the sequence and feedback the [[ evaluation ]] to guide the learning of the generative model .",0
643,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator << to evaluate >> the sequence and feedback the evaluation to guide the [[ learning ]] of the generative model .",0
644,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator << to evaluate >> the sequence and feedback the evaluation to guide the learning of the [[ generative model ]] .",0
645,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] << to evaluate >> the sequence and feedback the [[ evaluation ]] to guide the learning of the generative model .",0
646,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] << to evaluate >> the sequence and feedback the evaluation to guide the [[ learning ]] of the generative model .",0
647,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] << to evaluate >> the sequence and feedback the evaluation to guide the learning of the [[ generative model ]] .",0
648,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator << to evaluate >> the sequence and feedback the [[ evaluation ]] to guide the [[ learning ]] of the generative model .",0
649,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator << to evaluate >> the sequence and feedback the [[ evaluation ]] to guide the learning of the [[ generative model ]] .",0
650,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator << to evaluate >> the sequence and feedback the evaluation to guide the [[ learning ]] of the [[ generative model ]] .",0
651,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and << feedback >> the evaluation to guide the learning of the generative model .",0
652,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and << feedback >> the [[ evaluation ]] to guide the learning of the generative model .",0
653,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and << feedback >> the evaluation to guide the [[ learning ]] of the generative model .",0
654,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and << feedback >> the evaluation to guide the learning of the [[ generative model ]] .",0
655,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and << feedback >> the [[ evaluation ]] to guide the learning of the generative model .",1
656,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and << feedback >> the evaluation to guide the [[ learning ]] of the generative model .",0
657,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and << feedback >> the evaluation to guide the learning of the [[ generative model ]] .",0
658,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and << feedback >> the [[ evaluation ]] to guide the [[ learning ]] of the generative model .",0
659,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and << feedback >> the [[ evaluation ]] to guide the learning of the [[ generative model ]] .",0
660,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and << feedback >> the evaluation to guide the [[ learning ]] of the [[ generative model ]] .",0
661,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the evaluation << to guide >> the learning of the generative model .",0
662,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the [[ evaluation ]] << to guide >> the learning of the generative model .",0
663,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation << to guide >> the [[ learning ]] of the generative model .",0
664,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation << to guide >> the learning of the [[ generative model ]] .",0
665,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the [[ evaluation ]] << to guide >> the learning of the generative model .",0
666,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the evaluation << to guide >> the [[ learning ]] of the generative model .",0
667,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the evaluation << to guide >> the learning of the [[ generative model ]] .",0
668,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the [[ evaluation ]] << to guide >> the [[ learning ]] of the generative model .",1
669,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the [[ evaluation ]] << to guide >> the learning of the [[ generative model ]] .",0
670,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation << to guide >> the [[ learning ]] of the [[ generative model ]] .",0
671,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the evaluation to guide the learning << of >> the generative model .",0
672,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the [[ evaluation ]] to guide the learning << of >> the generative model .",0
673,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the [[ learning ]] << of >> the generative model .",0
674,5633,"Unlike the work in ) that requires a task - specific [[ sequence ]] score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning << of >> the [[ generative model ]] .",0
675,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the [[ evaluation ]] to guide the learning << of >> the generative model .",0
676,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the evaluation to guide the [[ learning ]] << of >> the generative model .",0
677,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a [[ discriminator ]] to evaluate the sequence and feedback the evaluation to guide the learning << of >> the [[ generative model ]] .",0
678,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the [[ evaluation ]] to guide the [[ learning ]] << of >> the generative model .",0
679,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the [[ evaluation ]] to guide the learning << of >> the [[ generative model ]] .",0
680,5633,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the [[ learning ]] << of >> the [[ generative model ]] .",1
681,52,"In addition to the two sub-layers in each encoder layer , the decoder << inserts >> a [[ third sub - layer ]] , which performs [[ multi-head attention ]] over the output of the encoder stack .",0
682,52,"In addition to the two sub-layers in each encoder layer , the decoder << inserts >> a [[ third sub - layer ]] , which performs multi-head attention over the [[ output of the encoder stack ]] .",0
683,52,"In addition to the two sub-layers in each encoder layer , the decoder << inserts >> a third sub - layer , which performs [[ multi-head attention ]] over the [[ output of the encoder stack ]] .",0
684,52,"In addition to the two sub-layers in each encoder layer , the decoder inserts a [[ third sub - layer ]] , which << performs >> [[ multi-head attention ]] over the output of the encoder stack .",1
685,52,"In addition to the two sub-layers in each encoder layer , the decoder inserts a [[ third sub - layer ]] , which << performs >> multi-head attention over the [[ output of the encoder stack ]] .",0
686,52,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which << performs >> [[ multi-head attention ]] over the [[ output of the encoder stack ]] .",0
687,52,"In addition to the two sub-layers in each encoder layer , the decoder inserts a [[ third sub - layer ]] , which performs [[ multi-head attention ]] << over >> the output of the encoder stack .",0
688,52,"In addition to the two sub-layers in each encoder layer , the decoder inserts a [[ third sub - layer ]] , which performs multi-head attention << over >> the [[ output of the encoder stack ]] .",0
689,52,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs [[ multi-head attention ]] << over >> the [[ output of the encoder stack ]] .",1
690,4114,The [[ MDRE model ]] ) << compensates for >> the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .,1
691,4114,The [[ MDRE model ]] ) << compensates for >> the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths to a surprising degree .,0
692,4114,The [[ MDRE model ]] ) << compensates for >> the weaknesses of the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] to a surprising degree .,0
693,4114,The [[ MDRE model ]] ) << compensates for >> the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths to a [[ surprising degree ]] .,0
694,4114,The MDRE model ) << compensates for >> the [[ weaknesses ]] of the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths to a surprising degree .,0
695,4114,The MDRE model ) << compensates for >> the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] to a surprising degree .,0
696,4114,The MDRE model ) << compensates for >> the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and benefits from their strengths to a [[ surprising degree ]] .,0
697,4114,The MDRE model ) << compensates for >> the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and benefits from their [[ strengths ]] to a surprising degree .,0
698,4114,The MDRE model ) << compensates for >> the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths to a [[ surprising degree ]] .,0
699,4114,The MDRE model ) << compensates for >> the weaknesses of the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] to a [[ surprising degree ]] .,0
700,4114,The [[ MDRE model ]] ) compensates for the [[ weaknesses ]] << of >> the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .,0
701,4114,The [[ MDRE model ]] ) compensates for the weaknesses << of >> the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths to a surprising degree .,0
702,4114,The [[ MDRE model ]] ) compensates for the weaknesses << of >> the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] to a surprising degree .,0
703,4114,The [[ MDRE model ]] ) compensates for the weaknesses << of >> the previous two models ( ARE and TRE ) and benefits from their strengths to a [[ surprising degree ]] .,0
704,4114,The MDRE model ) compensates for the [[ weaknesses ]] << of >> the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths to a surprising degree .,1
705,4114,The MDRE model ) compensates for the [[ weaknesses ]] << of >> the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] to a surprising degree .,0
706,4114,The MDRE model ) compensates for the [[ weaknesses ]] << of >> the previous two models ( ARE and TRE ) and benefits from their strengths to a [[ surprising degree ]] .,0
707,4114,The MDRE model ) compensates for the weaknesses << of >> the [[ previous two models ( ARE and TRE ) ]] and benefits from their [[ strengths ]] to a surprising degree .,0
708,4114,The MDRE model ) compensates for the weaknesses << of >> the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths to a [[ surprising degree ]] .,0
709,4114,The MDRE model ) compensates for the weaknesses << of >> the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] to a [[ surprising degree ]] .,0
710,4114,The [[ MDRE model ]] ) compensates for the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and << benefits from >> their strengths to a surprising degree .,0
711,4114,The [[ MDRE model ]] ) compensates for the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and << benefits from >> their strengths to a surprising degree .,0
712,4114,The [[ MDRE model ]] ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and << benefits from >> their [[ strengths ]] to a surprising degree .,1
713,4114,The [[ MDRE model ]] ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and << benefits from >> their strengths to a [[ surprising degree ]] .,0
714,4114,The MDRE model ) compensates for the [[ weaknesses ]] of the [[ previous two models ( ARE and TRE ) ]] and << benefits from >> their strengths to a surprising degree .,0
715,4114,The MDRE model ) compensates for the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and << benefits from >> their [[ strengths ]] to a surprising degree .,0
716,4114,The MDRE model ) compensates for the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and << benefits from >> their strengths to a [[ surprising degree ]] .,0
717,4114,The MDRE model ) compensates for the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and << benefits from >> their [[ strengths ]] to a surprising degree .,0
718,4114,The MDRE model ) compensates for the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and << benefits from >> their strengths to a [[ surprising degree ]] .,0
719,4114,The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and << benefits from >> their [[ strengths ]] to a [[ surprising degree ]] .,0
720,4114,The [[ MDRE model ]] ) compensates for the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and benefits from their strengths << to >> a surprising degree .,0
721,4114,The [[ MDRE model ]] ) compensates for the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths << to >> a surprising degree .,0
722,4114,The [[ MDRE model ]] ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] << to >> a surprising degree .,0
723,4114,The [[ MDRE model ]] ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths << to >> a [[ surprising degree ]] .,0
724,4114,The MDRE model ) compensates for the [[ weaknesses ]] of the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths << to >> a surprising degree .,0
725,4114,The MDRE model ) compensates for the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] << to >> a surprising degree .,0
726,4114,The MDRE model ) compensates for the [[ weaknesses ]] of the previous two models ( ARE and TRE ) and benefits from their strengths << to >> a [[ surprising degree ]] .,0
727,4114,The MDRE model ) compensates for the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and benefits from their [[ strengths ]] << to >> a surprising degree .,0
728,4114,The MDRE model ) compensates for the weaknesses of the [[ previous two models ( ARE and TRE ) ]] and benefits from their strengths << to >> a [[ surprising degree ]] .,0
729,4114,The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their [[ strengths ]] << to >> a [[ surprising degree ]] .,1
730,5405,"Interestingly , << using >> [[ multiword - wise embeddings ]] on the [[ vanilla setting ]] leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
731,5405,"Interestingly , << using >> [[ multiword - wise embeddings ]] on the vanilla setting leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
732,5405,"Interestingly , << using >> [[ multiword - wise embeddings ]] on the vanilla setting leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
733,5405,"Interestingly , << using >> [[ multiword - wise embeddings ]] on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
734,5405,"Interestingly , << using >> multiword - wise embeddings on the [[ vanilla setting ]] leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
735,5405,"Interestingly , << using >> multiword - wise embeddings on the [[ vanilla setting ]] leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
736,5405,"Interestingly , << using >> multiword - wise embeddings on the [[ vanilla setting ]] leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
737,5405,"Interestingly , << using >> multiword - wise embeddings on the vanilla setting leads to [[ consistently better results ]] than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
738,5405,"Interestingly , << using >> multiword - wise embeddings on the vanilla setting leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
739,5405,"Interestingly , << using >> multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in [[ eight of the nine datasets ]] .",0
740,5405,"Interestingly , using [[ multiword - wise embeddings ]] << on >> the [[ vanilla setting ]] leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",1
741,5405,"Interestingly , using [[ multiword - wise embeddings ]] << on >> the vanilla setting leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
742,5405,"Interestingly , using [[ multiword - wise embeddings ]] << on >> the vanilla setting leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
743,5405,"Interestingly , using [[ multiword - wise embeddings ]] << on >> the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
744,5405,"Interestingly , using multiword - wise embeddings << on >> the [[ vanilla setting ]] leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
745,5405,"Interestingly , using multiword - wise embeddings << on >> the [[ vanilla setting ]] leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
746,5405,"Interestingly , using multiword - wise embeddings << on >> the [[ vanilla setting ]] leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
747,5405,"Interestingly , using multiword - wise embeddings << on >> the vanilla setting leads to [[ consistently better results ]] than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
748,5405,"Interestingly , using multiword - wise embeddings << on >> the vanilla setting leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
749,5405,"Interestingly , using multiword - wise embeddings << on >> the vanilla setting leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in [[ eight of the nine datasets ]] .",0
750,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the [[ vanilla setting ]] << leads to >> consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
751,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting << leads to >> [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
752,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting << leads to >> consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
753,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting << leads to >> consistently better results than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
754,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] << leads to >> [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",1
755,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] << leads to >> consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
756,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] << leads to >> consistently better results than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
757,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting << leads to >> [[ consistently better results ]] than using them on the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
758,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting << leads to >> [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
759,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting << leads to >> consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] in [[ eight of the nine datasets ]] .",0
760,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the [[ vanilla setting ]] leads to consistently better results << than using them on >> the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
761,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting leads to [[ consistently better results ]] << than using them on >> the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
762,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting leads to consistently better results << than using them on >> the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
763,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting leads to consistently better results << than using them on >> the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
764,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] leads to [[ consistently better results ]] << than using them on >> the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",0
765,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] leads to consistently better results << than using them on >> the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",0
766,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] leads to consistently better results << than using them on >> the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
767,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to [[ consistently better results ]] << than using them on >> the [[ same multiwordgrouped preprocessed dataset ]] in eight of the nine datasets .",1
768,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to [[ consistently better results ]] << than using them on >> the same multiwordgrouped preprocessed dataset in [[ eight of the nine datasets ]] .",0
769,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results << than using them on >> the [[ same multiwordgrouped preprocessed dataset ]] in [[ eight of the nine datasets ]] .",0
770,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the [[ vanilla setting ]] leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset << in >> eight of the nine datasets .",0
771,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset << in >> eight of the nine datasets .",0
772,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] << in >> eight of the nine datasets .",0
773,5405,"Interestingly , using [[ multiword - wise embeddings ]] on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset << in >> [[ eight of the nine datasets ]] .",0
774,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset << in >> eight of the nine datasets .",0
775,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] << in >> eight of the nine datasets .",0
776,5405,"Interestingly , using multiword - wise embeddings on the [[ vanilla setting ]] leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset << in >> [[ eight of the nine datasets ]] .",0
777,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to [[ consistently better results ]] than using them on the [[ same multiwordgrouped preprocessed dataset ]] << in >> eight of the nine datasets .",0
778,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to [[ consistently better results ]] than using them on the same multiwordgrouped preprocessed dataset << in >> [[ eight of the nine datasets ]] .",0
779,5405,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the [[ same multiwordgrouped preprocessed dataset ]] << in >> [[ eight of the nine datasets ]] .",1
780,2269,"According to the ablation results , we infer that : ( a ) << When >> the [[ number of layers ]] is [[ only one ]] , the model lacks reasoning capability .",0
781,2269,"According to the ablation results , we infer that : ( a ) << When >> the [[ number of layers ]] is only one , the [[ model ]] lacks reasoning capability .",0
782,2269,"According to the ablation results , we infer that : ( a ) << When >> the [[ number of layers ]] is only one , the model lacks [[ reasoning capability ]] .",0
783,2269,"According to the ablation results , we infer that : ( a ) << When >> the number of layers is [[ only one ]] , the [[ model ]] lacks reasoning capability .",0
784,2269,"According to the ablation results , we infer that : ( a ) << When >> the number of layers is [[ only one ]] , the model lacks [[ reasoning capability ]] .",0
785,2269,"According to the ablation results , we infer that : ( a ) << When >> the number of layers is only one , the [[ model ]] lacks [[ reasoning capability ]] .",0
786,2269,"According to the ablation results , we infer that : ( a ) When the [[ number of layers ]] << is >> [[ only one ]] , the model lacks reasoning capability .",1
787,2269,"According to the ablation results , we infer that : ( a ) When the [[ number of layers ]] << is >> only one , the [[ model ]] lacks reasoning capability .",0
788,2269,"According to the ablation results , we infer that : ( a ) When the [[ number of layers ]] << is >> only one , the model lacks [[ reasoning capability ]] .",0
789,2269,"According to the ablation results , we infer that : ( a ) When the number of layers << is >> [[ only one ]] , the [[ model ]] lacks reasoning capability .",0
790,2269,"According to the ablation results , we infer that : ( a ) When the number of layers << is >> [[ only one ]] , the model lacks [[ reasoning capability ]] .",0
791,2269,"According to the ablation results , we infer that : ( a ) When the number of layers << is >> only one , the [[ model ]] lacks [[ reasoning capability ]] .",0
792,2269,"According to the ablation results , we infer that : ( a ) When the [[ number of layers ]] is [[ only one ]] , the model << lacks >> reasoning capability .",0
793,2269,"According to the ablation results , we infer that : ( a ) When the [[ number of layers ]] is only one , the [[ model ]] << lacks >> reasoning capability .",0
794,2269,"According to the ablation results , we infer that : ( a ) When the [[ number of layers ]] is only one , the model << lacks >> [[ reasoning capability ]] .",0
795,2269,"According to the ablation results , we infer that : ( a ) When the number of layers is [[ only one ]] , the [[ model ]] << lacks >> reasoning capability .",0
796,2269,"According to the ablation results , we infer that : ( a ) When the number of layers is [[ only one ]] , the model << lacks >> [[ reasoning capability ]] .",0
797,2269,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the [[ model ]] << lacks >> [[ reasoning capability ]] .",1
798,5094,"We << optimize >> the [[ proposed model ]] with [[ RMSprop algorithm ]] , using mini-batch training .",0
799,5094,"We << optimize >> the [[ proposed model ]] with RMSprop algorithm , using [[ mini-batch training ]] .",0
800,5094,"We << optimize >> the proposed model with [[ RMSprop algorithm ]] , using [[ mini-batch training ]] .",0
801,5094,"We optimize the [[ proposed model ]] << with >> [[ RMSprop algorithm ]] , using mini-batch training .",1
802,5094,"We optimize the [[ proposed model ]] << with >> RMSprop algorithm , using [[ mini-batch training ]] .",0
803,5094,"We optimize the proposed model << with >> [[ RMSprop algorithm ]] , using [[ mini-batch training ]] .",0
804,5094,"We optimize the [[ proposed model ]] with [[ RMSprop algorithm ]] , << using >> mini-batch training .",0
805,5094,"We optimize the [[ proposed model ]] with RMSprop algorithm , << using >> [[ mini-batch training ]] .",1
806,5094,"We optimize the proposed model with [[ RMSprop algorithm ]] , << using >> [[ mini-batch training ]] .",0
807,1389,The [[ pointwise learning approach ]] also << shows >> [[ excellent performance ]] with the TREC - QA dataset .,1
808,1389,The [[ pointwise learning approach ]] also << shows >> excellent performance with the [[ TREC - QA dataset ]] .,0
809,1389,The pointwise learning approach also << shows >> [[ excellent performance ]] with the [[ TREC - QA dataset ]] .,0
810,1389,The [[ pointwise learning approach ]] also shows [[ excellent performance ]] << with >> the TREC - QA dataset .,0
811,1389,The [[ pointwise learning approach ]] also shows excellent performance << with >> the [[ TREC - QA dataset ]] .,0
812,1389,The pointwise learning approach also shows [[ excellent performance ]] << with >> the [[ TREC - QA dataset ]] .,0
813,3433,We also << add >> an [[ additional layer ]] to [[ our network ]] to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
814,3433,We also << add >> an [[ additional layer ]] to our network to encode the [[ output sequence ]] from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
815,3433,We also << add >> an [[ additional layer ]] to our network to encode the output sequence from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
816,3433,We also << add >> an additional layer to [[ our network ]] to encode the [[ output sequence ]] from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
817,3433,We also << add >> an additional layer to [[ our network ]] to encode the output sequence from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
818,3433,We also << add >> an additional layer to our network to encode the [[ output sequence ]] from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
819,3433,We also add an [[ additional layer ]] << to >> [[ our network ]] to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,1
820,3433,We also add an [[ additional layer ]] << to >> our network to encode the [[ output sequence ]] from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
821,3433,We also add an [[ additional layer ]] << to >> our network to encode the output sequence from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
822,3433,We also add an additional layer << to >> [[ our network ]] to encode the [[ output sequence ]] from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
823,3433,We also add an additional layer << to >> [[ our network ]] to encode the output sequence from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
824,3433,We also add an additional layer << to >> our network to encode the [[ output sequence ]] from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
825,3433,We also add an [[ additional layer ]] to [[ our network ]] << to encode >> the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
826,3433,We also add an [[ additional layer ]] to our network << to encode >> the [[ output sequence ]] from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,1
827,3433,We also add an [[ additional layer ]] to our network << to encode >> the output sequence from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
828,3433,We also add an additional layer to [[ our network ]] << to encode >> the [[ output sequence ]] from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
829,3433,We also add an additional layer to [[ our network ]] << to encode >> the output sequence from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
830,3433,We also add an additional layer to our network << to encode >> the [[ output sequence ]] from [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
831,3433,We also add an [[ additional layer ]] to [[ our network ]] to encode the output sequence << from >> right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
832,3433,We also add an [[ additional layer ]] to our network to encode the [[ output sequence ]] << from >> right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
833,3433,We also add an [[ additional layer ]] to our network to encode the output sequence << from >> [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
834,3433,We also add an additional layer to [[ our network ]] to encode the [[ output sequence ]] << from >> right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,0
835,3433,We also add an additional layer to [[ our network ]] to encode the output sequence << from >> [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,0
836,3433,We also add an additional layer to our network to encode the [[ output sequence ]] << from >> [[ right - to - left ]] and find significant improvement on the performance of relation identification using bi-directional encoding .,1
837,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model << generates >> the [[ memory representations ]] for each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .",1
838,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model << generates >> the memory representations for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without sequential modeling .",0
839,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model << generates >> the memory representations for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
840,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model << generates >> the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
841,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model << generates >> the [[ memory representations ]] for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without sequential modeling .",0
842,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model << generates >> the [[ memory representations ]] for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
843,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model << generates >> the [[ memory representations ]] for each historical utterance using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
844,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model << generates >> the memory representations for [[ each historical utterance ]] using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
845,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model << generates >> the memory representations for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
846,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model << generates >> the memory representations for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without [[ sequential modeling ]] .",0
847,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] << for >> each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .",0
848,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations << for >> [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without sequential modeling .",0
849,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations << for >> each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
850,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations << for >> each historical utterance using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
851,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] << for >> [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without sequential modeling .",1
852,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] << for >> each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
853,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] << for >> each historical utterance using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
854,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations << for >> [[ each historical utterance ]] using an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
855,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations << for >> [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
856,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations << for >> each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , without [[ sequential modeling ]] .",0
857,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance << using >> an embedding matrix B as used in equation 7 , without sequential modeling .",0
858,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] << using >> an embedding matrix B as used in equation 7 , without sequential modeling .",0
859,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance << using >> an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
860,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance << using >> an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
861,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for [[ each historical utterance ]] << using >> an embedding matrix B as used in equation 7 , without sequential modeling .",0
862,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance << using >> an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",0
863,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance << using >> an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
864,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] << using >> an [[ embedding matrix B ]] as used in equation 7 , without sequential modeling .",1
865,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] << using >> an embedding matrix B as used in equation 7 , without [[ sequential modeling ]] .",0
866,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance << using >> an [[ embedding matrix B ]] as used in equation 7 , without [[ sequential modeling ]] .",0
867,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance using an embedding matrix B as used in equation 7 , << without >> sequential modeling .",0
868,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , << without >> sequential modeling .",0
869,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , << without >> sequential modeling .",0
870,4191,"[[ Memn2n ]] : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , << without >> [[ sequential modeling ]] .",0
871,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , << without >> sequential modeling .",0
872,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , << without >> sequential modeling .",0
873,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the [[ memory representations ]] for each historical utterance using an embedding matrix B as used in equation 7 , << without >> [[ sequential modeling ]] .",0
874,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] using an [[ embedding matrix B ]] as used in equation 7 , << without >> sequential modeling .",0
875,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for [[ each historical utterance ]] using an embedding matrix B as used in equation 7 , << without >> [[ sequential modeling ]] .",0
876,4191,"Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an [[ embedding matrix B ]] as used in equation 7 , << without >> [[ sequential modeling ]] .",1
877,2309,"Most notably , these models [[ outperform ]] the [[ tree - based CNN ]] of , which also << uses >> tree - structured composition for local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
878,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also << uses >> [[ tree - structured composition ]] for local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
879,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also << uses >> tree - structured composition for [[ local feature extraction ]] , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
880,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also << uses >> tree - structured composition for local feature extraction , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",0
881,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also << uses >> tree - structured composition for local feature extraction , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
882,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also << uses >> [[ tree - structured composition ]] for local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",1
883,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also << uses >> tree - structured composition for [[ local feature extraction ]] , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
884,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also << uses >> tree - structured composition for local feature extraction , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",1
885,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also << uses >> tree - structured composition for local feature extraction , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
886,2309,"Most notably , these models outperform the tree - based CNN of , which also << uses >> [[ tree - structured composition ]] for [[ local feature extraction ]] , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
887,2309,"Most notably , these models outperform the tree - based CNN of , which also << uses >> [[ tree - structured composition ]] for local feature extraction , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",0
888,2309,"Most notably , these models outperform the tree - based CNN of , which also << uses >> [[ tree - structured composition ]] for local feature extraction , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
889,2309,"Most notably , these models outperform the tree - based CNN of , which also << uses >> tree - structured composition for [[ local feature extraction ]] , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",0
890,2309,"Most notably , these models outperform the tree - based CNN of , which also << uses >> tree - structured composition for [[ local feature extraction ]] , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
891,2309,"Most notably , these models outperform the tree - based CNN of , which also << uses >> tree - structured composition for local feature extraction , but uses [[ simpler pooling techniques ]] to build [[ sentence features ]] in the interest of efficiency .",0
892,2309,"Most notably , these models [[ outperform ]] the [[ tree - based CNN ]] of , which also uses tree - structured composition << for >> local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
893,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses [[ tree - structured composition ]] << for >> local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
894,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses tree - structured composition << for >> [[ local feature extraction ]] , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
895,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses tree - structured composition << for >> local feature extraction , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",0
896,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses tree - structured composition << for >> local feature extraction , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
897,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses [[ tree - structured composition ]] << for >> local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
898,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses tree - structured composition << for >> [[ local feature extraction ]] , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",0
899,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses tree - structured composition << for >> local feature extraction , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",0
900,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses tree - structured composition << for >> local feature extraction , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
901,2309,"Most notably , these models outperform the tree - based CNN of , which also uses [[ tree - structured composition ]] << for >> [[ local feature extraction ]] , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",1
902,2309,"Most notably , these models outperform the tree - based CNN of , which also uses [[ tree - structured composition ]] << for >> local feature extraction , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",0
903,2309,"Most notably , these models outperform the tree - based CNN of , which also uses [[ tree - structured composition ]] << for >> local feature extraction , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
904,2309,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition << for >> [[ local feature extraction ]] , but uses [[ simpler pooling techniques ]] to build sentence features in the interest of efficiency .",0
905,2309,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition << for >> [[ local feature extraction ]] , but uses simpler pooling techniques to build [[ sentence features ]] in the interest of efficiency .",0
906,2309,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition << for >> local feature extraction , but uses [[ simpler pooling techniques ]] to build [[ sentence features ]] in the interest of efficiency .",0
907,2309,"Most notably , these models [[ outperform ]] the [[ tree - based CNN ]] of , which also uses tree - structured composition for local feature extraction , but uses simpler pooling techniques << to build >> sentence features in the interest of efficiency .",0
908,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses [[ tree - structured composition ]] for local feature extraction , but uses simpler pooling techniques << to build >> sentence features in the interest of efficiency .",0
909,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses tree - structured composition for [[ local feature extraction ]] , but uses simpler pooling techniques << to build >> sentence features in the interest of efficiency .",0
910,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses tree - structured composition for local feature extraction , but uses [[ simpler pooling techniques ]] << to build >> sentence features in the interest of efficiency .",0
911,2309,"Most notably , these models [[ outperform ]] the tree - based CNN of , which also uses tree - structured composition for local feature extraction , but uses simpler pooling techniques << to build >> [[ sentence features ]] in the interest of efficiency .",0
912,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses [[ tree - structured composition ]] for local feature extraction , but uses simpler pooling techniques << to build >> sentence features in the interest of efficiency .",0
913,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses tree - structured composition for [[ local feature extraction ]] , but uses simpler pooling techniques << to build >> sentence features in the interest of efficiency .",0
914,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses tree - structured composition for local feature extraction , but uses [[ simpler pooling techniques ]] << to build >> sentence features in the interest of efficiency .",0
915,2309,"Most notably , these models outperform the [[ tree - based CNN ]] of , which also uses tree - structured composition for local feature extraction , but uses simpler pooling techniques << to build >> [[ sentence features ]] in the interest of efficiency .",0
916,2309,"Most notably , these models outperform the tree - based CNN of , which also uses [[ tree - structured composition ]] for [[ local feature extraction ]] , but uses simpler pooling techniques << to build >> sentence features in the interest of efficiency .",0
917,2309,"Most notably , these models outperform the tree - based CNN of , which also uses [[ tree - structured composition ]] for local feature extraction , but uses [[ simpler pooling techniques ]] << to build >> sentence features in the interest of efficiency .",0
918,2309,"Most notably , these models outperform the tree - based CNN of , which also uses [[ tree - structured composition ]] for local feature extraction , but uses simpler pooling techniques << to build >> [[ sentence features ]] in the interest of efficiency .",0
919,2309,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition for [[ local feature extraction ]] , but uses [[ simpler pooling techniques ]] << to build >> sentence features in the interest of efficiency .",0
920,2309,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition for [[ local feature extraction ]] , but uses simpler pooling techniques << to build >> [[ sentence features ]] in the interest of efficiency .",0
921,2309,"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition for local feature extraction , but uses [[ simpler pooling techniques ]] << to build >> [[ sentence features ]] in the interest of efficiency .",1
922,2590,"<< For >> [[ Spanish and Dutch ]] , we use the [[ 64 - dimensional Polyglot embeddings ]] .",0
923,2590,"For [[ Spanish and Dutch ]] , we << use >> the [[ 64 - dimensional Polyglot embeddings ]] .",1
924,3174,Our [[ NQG framework ]] << outperforms >> the [[ PCFG - Trans and s 2s + att baselines ]] by a large margin .,1
925,3174,Our [[ NQG framework ]] << outperforms >> the PCFG - Trans and s 2s + att baselines by a [[ large margin ]] .,0
926,3174,Our NQG framework << outperforms >> the [[ PCFG - Trans and s 2s + att baselines ]] by a [[ large margin ]] .,0
927,3174,Our [[ NQG framework ]] outperforms the [[ PCFG - Trans and s 2s + att baselines ]] << by >> a large margin .,0
928,3174,Our [[ NQG framework ]] outperforms the PCFG - Trans and s 2s + att baselines << by >> a [[ large margin ]] .,0
929,3174,Our NQG framework outperforms the [[ PCFG - Trans and s 2s + att baselines ]] << by >> a [[ large margin ]] .,1
930,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a [[ model variant ]] that does not use the [[ gating mechanism ]] and directly computes the hidden state as a combination of the activations ( Eq 1 ) and the previous state .",0
931,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and directly computes the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",0
932,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and directly computes the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
933,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and directly computes the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
934,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and directly computes the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",0
935,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and directly computes the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
936,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and directly computes the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
937,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the [[ hidden state ]] as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
938,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the [[ hidden state ]] as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
939,2976,"To judge the effect << of >> the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the hidden state as a [[ combination ]] of the [[ activations ( Eq 1 ) and the previous state ]] .",1
940,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a [[ model variant ]] that does not use the [[ gating mechanism ]] and directly computes the hidden state as a combination of the activations ( Eq 1 ) and the previous state .",0
941,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a [[ model variant ]] that does not use the gating mechanism and directly computes the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",0
942,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a [[ model variant ]] that does not use the gating mechanism and directly computes the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
943,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a [[ model variant ]] that does not use the gating mechanism and directly computes the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
944,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a model variant that does not use the [[ gating mechanism ]] and directly computes the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",0
945,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a model variant that does not use the [[ gating mechanism ]] and directly computes the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
946,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a model variant that does not use the [[ gating mechanism ]] and directly computes the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
947,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a model variant that does not use the gating mechanism and directly computes the [[ hidden state ]] as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
948,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a model variant that does not use the gating mechanism and directly computes the [[ hidden state ]] as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
949,2976,"To judge the effect of the gated graph neural architecture , we also << include >> a model variant that does not use the gating mechanism and directly computes the hidden state as a [[ combination ]] of the [[ activations ( Eq 1 ) and the previous state ]] .",0
950,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] << that does not use >> the [[ gating mechanism ]] and directly computes the hidden state as a combination of the activations ( Eq 1 ) and the previous state .",1
951,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] << that does not use >> the gating mechanism and directly computes the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",0
952,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] << that does not use >> the gating mechanism and directly computes the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
953,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] << that does not use >> the gating mechanism and directly computes the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
954,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant << that does not use >> the [[ gating mechanism ]] and directly computes the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",0
955,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant << that does not use >> the [[ gating mechanism ]] and directly computes the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
956,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant << that does not use >> the [[ gating mechanism ]] and directly computes the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
957,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant << that does not use >> the gating mechanism and directly computes the [[ hidden state ]] as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
958,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant << that does not use >> the gating mechanism and directly computes the [[ hidden state ]] as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
959,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant << that does not use >> the gating mechanism and directly computes the hidden state as a [[ combination ]] of the [[ activations ( Eq 1 ) and the previous state ]] .",0
960,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the [[ gating mechanism ]] and << directly computes >> the hidden state as a combination of the activations ( Eq 1 ) and the previous state .",0
961,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and << directly computes >> the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",1
962,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and << directly computes >> the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
963,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and << directly computes >> the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
964,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and << directly computes >> the [[ hidden state ]] as a combination of the activations ( Eq 1 ) and the previous state .",0
965,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and << directly computes >> the hidden state as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
966,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and << directly computes >> the hidden state as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
967,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and << directly computes >> the [[ hidden state ]] as a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
968,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and << directly computes >> the [[ hidden state ]] as a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
969,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and << directly computes >> the hidden state as a [[ combination ]] of the [[ activations ( Eq 1 ) and the previous state ]] .",0
970,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the [[ gating mechanism ]] and directly computes the hidden state << as >> a combination of the activations ( Eq 1 ) and the previous state .",0
971,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and directly computes the [[ hidden state ]] << as >> a combination of the activations ( Eq 1 ) and the previous state .",0
972,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and directly computes the hidden state << as >> a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
973,2976,"To judge the effect of the gated graph neural architecture , we also include a [[ model variant ]] that does not use the gating mechanism and directly computes the hidden state << as >> a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
974,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and directly computes the [[ hidden state ]] << as >> a combination of the activations ( Eq 1 ) and the previous state .",0
975,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and directly computes the hidden state << as >> a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",0
976,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the [[ gating mechanism ]] and directly computes the hidden state << as >> a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
977,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the [[ hidden state ]] << as >> a [[ combination ]] of the activations ( Eq 1 ) and the previous state .",1
978,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the [[ hidden state ]] << as >> a combination of the [[ activations ( Eq 1 ) and the previous state ]] .",0
979,2976,"To judge the effect of the gated graph neural architecture , we also include a model variant that does not use the gating mechanism and directly computes the hidden state << as >> a [[ combination ]] of the [[ activations ( Eq 1 ) and the previous state ]] .",0
980,4280,"The [[ recursive models ]] << work >> [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",1
981,4280,"The [[ recursive models ]] << work >> very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",0
982,4280,"The [[ recursive models ]] << work >> very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
983,4280,"The [[ recursive models ]] << work >> very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
984,4280,"The [[ recursive models ]] << work >> very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
985,4280,"The [[ recursive models ]] << work >> very well on shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
986,4280,"The [[ recursive models ]] << work >> very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
987,4280,"The recursive models << work >> [[ very well ]] on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",0
988,4280,"The recursive models << work >> [[ very well ]] on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
989,4280,"The recursive models << work >> [[ very well ]] on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
990,4280,"The recursive models << work >> [[ very well ]] on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
991,4280,"The recursive models << work >> [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
992,4280,"The recursive models << work >> [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
993,4280,"The recursive models << work >> very well on [[ shorter phrases ]] , where [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
994,4280,"The recursive models << work >> very well on [[ shorter phrases ]] , where negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
995,4280,"The recursive models << work >> very well on [[ shorter phrases ]] , where negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
996,4280,"The recursive models << work >> very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
997,4280,"The recursive models << work >> very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
998,4280,"The recursive models << work >> very well on shorter phrases , where [[ negation and composition ]] are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
999,4280,"The recursive models << work >> very well on shorter phrases , where [[ negation and composition ]] are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1000,4280,"The recursive models << work >> very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1001,4280,"The recursive models << work >> very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1002,4280,"The recursive models << work >> very well on shorter phrases , where negation and composition are [[ important ]] , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1003,4280,"The recursive models << work >> very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1004,4280,"The recursive models << work >> very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1005,4280,"The recursive models << work >> very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform [[ well ]] only with longer sentences .",0
1006,4280,"The recursive models << work >> very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only with [[ longer sentences ]] .",0
1007,4280,"The recursive models << work >> very well on shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only with [[ longer sentences ]] .",0
1008,4280,"The [[ recursive models ]] work [[ very well ]] << on >> shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",0
1009,4280,"The [[ recursive models ]] work very well << on >> [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",0
1010,4280,"The [[ recursive models ]] work very well << on >> shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
1011,4280,"The [[ recursive models ]] work very well << on >> shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1012,4280,"The [[ recursive models ]] work very well << on >> shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1013,4280,"The [[ recursive models ]] work very well << on >> shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1014,4280,"The [[ recursive models ]] work very well << on >> shorter phrases , where negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1015,4280,"The recursive models work [[ very well ]] << on >> [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",1
1016,4280,"The recursive models work [[ very well ]] << on >> shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
1017,4280,"The recursive models work [[ very well ]] << on >> shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1018,4280,"The recursive models work [[ very well ]] << on >> shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1019,4280,"The recursive models work [[ very well ]] << on >> shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1020,4280,"The recursive models work [[ very well ]] << on >> shorter phrases , where negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1021,4280,"The recursive models work very well << on >> [[ shorter phrases ]] , where [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
1022,4280,"The recursive models work very well << on >> [[ shorter phrases ]] , where negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1023,4280,"The recursive models work very well << on >> [[ shorter phrases ]] , where negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1024,4280,"The recursive models work very well << on >> [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1025,4280,"The recursive models work very well << on >> [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1026,4280,"The recursive models work very well << on >> shorter phrases , where [[ negation and composition ]] are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1027,4280,"The recursive models work very well << on >> shorter phrases , where [[ negation and composition ]] are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1028,4280,"The recursive models work very well << on >> shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1029,4280,"The recursive models work very well << on >> shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1030,4280,"The recursive models work very well << on >> shorter phrases , where negation and composition are [[ important ]] , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1031,4280,"The recursive models work very well << on >> shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1032,4280,"The recursive models work very well << on >> shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1033,4280,"The recursive models work very well << on >> shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform [[ well ]] only with longer sentences .",0
1034,4280,"The recursive models work very well << on >> shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only with [[ longer sentences ]] .",0
1035,4280,"The recursive models work very well << on >> shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only with [[ longer sentences ]] .",0
1036,4280,"The [[ recursive models ]] work [[ very well ]] on shorter phrases , << where >> negation and composition are important , while bag of features baselines perform well only with longer sentences .",0
1037,4280,"The [[ recursive models ]] work very well on [[ shorter phrases ]] , << where >> negation and composition are important , while bag of features baselines perform well only with longer sentences .",0
1038,4280,"The [[ recursive models ]] work very well on shorter phrases , << where >> [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
1039,4280,"The [[ recursive models ]] work very well on shorter phrases , << where >> negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1040,4280,"The [[ recursive models ]] work very well on shorter phrases , << where >> negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1041,4280,"The [[ recursive models ]] work very well on shorter phrases , << where >> negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1042,4280,"The [[ recursive models ]] work very well on shorter phrases , << where >> negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1043,4280,"The recursive models work [[ very well ]] on [[ shorter phrases ]] , << where >> negation and composition are important , while bag of features baselines perform well only with longer sentences .",0
1044,4280,"The recursive models work [[ very well ]] on shorter phrases , << where >> [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",0
1045,4280,"The recursive models work [[ very well ]] on shorter phrases , << where >> negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1046,4280,"The recursive models work [[ very well ]] on shorter phrases , << where >> negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1047,4280,"The recursive models work [[ very well ]] on shorter phrases , << where >> negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1048,4280,"The recursive models work [[ very well ]] on shorter phrases , << where >> negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1049,4280,"The recursive models work very well on [[ shorter phrases ]] , << where >> [[ negation and composition ]] are important , while bag of features baselines perform well only with longer sentences .",1
1050,4280,"The recursive models work very well on [[ shorter phrases ]] , << where >> negation and composition are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1051,4280,"The recursive models work very well on [[ shorter phrases ]] , << where >> negation and composition are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1052,4280,"The recursive models work very well on [[ shorter phrases ]] , << where >> negation and composition are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1053,4280,"The recursive models work very well on [[ shorter phrases ]] , << where >> negation and composition are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1054,4280,"The recursive models work very well on shorter phrases , << where >> [[ negation and composition ]] are [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1055,4280,"The recursive models work very well on shorter phrases , << where >> [[ negation and composition ]] are important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1056,4280,"The recursive models work very well on shorter phrases , << where >> [[ negation and composition ]] are important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1057,4280,"The recursive models work very well on shorter phrases , << where >> [[ negation and composition ]] are important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1058,4280,"The recursive models work very well on shorter phrases , << where >> negation and composition are [[ important ]] , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1059,4280,"The recursive models work very well on shorter phrases , << where >> negation and composition are [[ important ]] , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1060,4280,"The recursive models work very well on shorter phrases , << where >> negation and composition are [[ important ]] , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1061,4280,"The recursive models work very well on shorter phrases , << where >> negation and composition are important , while [[ bag of features baselines ]] perform [[ well ]] only with longer sentences .",0
1062,4280,"The recursive models work very well on shorter phrases , << where >> negation and composition are important , while [[ bag of features baselines ]] perform well only with [[ longer sentences ]] .",0
1063,4280,"The recursive models work very well on shorter phrases , << where >> negation and composition are important , while bag of features baselines perform [[ well ]] only with [[ longer sentences ]] .",0
1064,4280,"The [[ recursive models ]] work [[ very well ]] on shorter phrases , where negation and composition << are >> important , while bag of features baselines perform well only with longer sentences .",0
1065,4280,"The [[ recursive models ]] work very well on [[ shorter phrases ]] , where negation and composition << are >> important , while bag of features baselines perform well only with longer sentences .",0
1066,4280,"The [[ recursive models ]] work very well on shorter phrases , where [[ negation and composition ]] << are >> important , while bag of features baselines perform well only with longer sentences .",0
1067,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition << are >> [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1068,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition << are >> important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1069,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition << are >> important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1070,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition << are >> important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1071,4280,"The recursive models work [[ very well ]] on [[ shorter phrases ]] , where negation and composition << are >> important , while bag of features baselines perform well only with longer sentences .",0
1072,4280,"The recursive models work [[ very well ]] on shorter phrases , where [[ negation and composition ]] << are >> important , while bag of features baselines perform well only with longer sentences .",0
1073,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition << are >> [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1074,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition << are >> important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1075,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition << are >> important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1076,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition << are >> important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1077,4280,"The recursive models work very well on [[ shorter phrases ]] , where [[ negation and composition ]] << are >> important , while bag of features baselines perform well only with longer sentences .",0
1078,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition << are >> [[ important ]] , while bag of features baselines perform well only with longer sentences .",0
1079,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition << are >> important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1080,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition << are >> important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1081,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition << are >> important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1082,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] << are >> [[ important ]] , while bag of features baselines perform well only with longer sentences .",1
1083,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] << are >> important , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1084,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] << are >> important , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1085,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] << are >> important , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1086,4280,"The recursive models work very well on shorter phrases , where negation and composition << are >> [[ important ]] , while [[ bag of features baselines ]] perform well only with longer sentences .",0
1087,4280,"The recursive models work very well on shorter phrases , where negation and composition << are >> [[ important ]] , while bag of features baselines perform [[ well ]] only with longer sentences .",0
1088,4280,"The recursive models work very well on shorter phrases , where negation and composition << are >> [[ important ]] , while bag of features baselines perform well only with [[ longer sentences ]] .",0
1089,4280,"The recursive models work very well on shorter phrases , where negation and composition << are >> important , while [[ bag of features baselines ]] perform [[ well ]] only with longer sentences .",0
1090,4280,"The recursive models work very well on shorter phrases , where negation and composition << are >> important , while [[ bag of features baselines ]] perform well only with [[ longer sentences ]] .",0
1091,4280,"The recursive models work very well on shorter phrases , where negation and composition << are >> important , while bag of features baselines perform [[ well ]] only with [[ longer sentences ]] .",0
1092,4280,"The [[ recursive models ]] work [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines << perform >> well only with longer sentences .",0
1093,4280,"The [[ recursive models ]] work very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines << perform >> well only with longer sentences .",0
1094,4280,"The [[ recursive models ]] work very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines << perform >> well only with longer sentences .",0
1095,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines << perform >> well only with longer sentences .",0
1096,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] << perform >> well only with longer sentences .",0
1097,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are important , while bag of features baselines << perform >> [[ well ]] only with longer sentences .",0
1098,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are important , while bag of features baselines << perform >> well only with [[ longer sentences ]] .",0
1099,4280,"The recursive models work [[ very well ]] on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines << perform >> well only with longer sentences .",0
1100,4280,"The recursive models work [[ very well ]] on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines << perform >> well only with longer sentences .",0
1101,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines << perform >> well only with longer sentences .",0
1102,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] << perform >> well only with longer sentences .",0
1103,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines << perform >> [[ well ]] only with longer sentences .",0
1104,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines << perform >> well only with [[ longer sentences ]] .",0
1105,4280,"The recursive models work very well on [[ shorter phrases ]] , where [[ negation and composition ]] are important , while bag of features baselines << perform >> well only with longer sentences .",0
1106,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are [[ important ]] , while bag of features baselines << perform >> well only with longer sentences .",0
1107,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are important , while [[ bag of features baselines ]] << perform >> well only with longer sentences .",0
1108,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines << perform >> [[ well ]] only with longer sentences .",0
1109,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines << perform >> well only with [[ longer sentences ]] .",0
1110,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are [[ important ]] , while bag of features baselines << perform >> well only with longer sentences .",0
1111,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are important , while [[ bag of features baselines ]] << perform >> well only with longer sentences .",0
1112,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines << perform >> [[ well ]] only with longer sentences .",0
1113,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines << perform >> well only with [[ longer sentences ]] .",0
1114,4280,"The recursive models work very well on shorter phrases , where negation and composition are [[ important ]] , while [[ bag of features baselines ]] << perform >> well only with longer sentences .",0
1115,4280,"The recursive models work very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines << perform >> [[ well ]] only with longer sentences .",0
1116,4280,"The recursive models work very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines << perform >> well only with [[ longer sentences ]] .",0
1117,4280,"The recursive models work very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] << perform >> [[ well ]] only with longer sentences .",1
1118,4280,"The recursive models work very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] << perform >> well only with [[ longer sentences ]] .",0
1119,4280,"The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines << perform >> [[ well ]] only with [[ longer sentences ]] .",0
1120,4280,"The [[ recursive models ]] work [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines perform well only << with >> longer sentences .",0
1121,4280,"The [[ recursive models ]] work very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only << with >> longer sentences .",0
1122,4280,"The [[ recursive models ]] work very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only << with >> longer sentences .",0
1123,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only << with >> longer sentences .",0
1124,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only << with >> longer sentences .",0
1125,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only << with >> longer sentences .",0
1126,4280,"The [[ recursive models ]] work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only << with >> [[ longer sentences ]] .",0
1127,4280,"The recursive models work [[ very well ]] on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only << with >> longer sentences .",0
1128,4280,"The recursive models work [[ very well ]] on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only << with >> longer sentences .",0
1129,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only << with >> longer sentences .",0
1130,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only << with >> longer sentences .",0
1131,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only << with >> longer sentences .",0
1132,4280,"The recursive models work [[ very well ]] on shorter phrases , where negation and composition are important , while bag of features baselines perform well only << with >> [[ longer sentences ]] .",0
1133,4280,"The recursive models work very well on [[ shorter phrases ]] , where [[ negation and composition ]] are important , while bag of features baselines perform well only << with >> longer sentences .",0
1134,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are [[ important ]] , while bag of features baselines perform well only << with >> longer sentences .",0
1135,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are important , while [[ bag of features baselines ]] perform well only << with >> longer sentences .",0
1136,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform [[ well ]] only << with >> longer sentences .",0
1137,4280,"The recursive models work very well on [[ shorter phrases ]] , where negation and composition are important , while bag of features baselines perform well only << with >> [[ longer sentences ]] .",0
1138,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are [[ important ]] , while bag of features baselines perform well only << with >> longer sentences .",0
1139,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are important , while [[ bag of features baselines ]] perform well only << with >> longer sentences .",0
1140,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform [[ well ]] only << with >> longer sentences .",0
1141,4280,"The recursive models work very well on shorter phrases , where [[ negation and composition ]] are important , while bag of features baselines perform well only << with >> [[ longer sentences ]] .",0
1142,4280,"The recursive models work very well on shorter phrases , where negation and composition are [[ important ]] , while [[ bag of features baselines ]] perform well only << with >> longer sentences .",0
1143,4280,"The recursive models work very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform [[ well ]] only << with >> longer sentences .",0
1144,4280,"The recursive models work very well on shorter phrases , where negation and composition are [[ important ]] , while bag of features baselines perform well only << with >> [[ longer sentences ]] .",0
1145,4280,"The recursive models work very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform [[ well ]] only << with >> longer sentences .",0
1146,4280,"The recursive models work very well on shorter phrases , where negation and composition are important , while [[ bag of features baselines ]] perform well only << with >> [[ longer sentences ]] .",0
1147,4280,"The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform [[ well ]] only << with >> [[ longer sentences ]] .",1
1148,467,"Secondly , comparing ours with other models , we << find that >> [[ our 100D and 300D model ]] outperform [[ all other models ]] of similar numbers of parameters .",0
1149,467,"Secondly , comparing ours with other models , we << find that >> [[ our 100D and 300D model ]] outperform all other models of [[ similar numbers of parameters ]] .",0
1150,467,"Secondly , comparing ours with other models , we << find that >> our 100D and 300D model outperform [[ all other models ]] of [[ similar numbers of parameters ]] .",0
1151,467,"Secondly , comparing ours with other models , we find that [[ our 100D and 300D model ]] << outperform >> [[ all other models ]] of similar numbers of parameters .",1
1152,467,"Secondly , comparing ours with other models , we find that [[ our 100D and 300D model ]] << outperform >> all other models of [[ similar numbers of parameters ]] .",0
1153,467,"Secondly , comparing ours with other models , we find that our 100D and 300D model << outperform >> [[ all other models ]] of [[ similar numbers of parameters ]] .",0
1154,467,"Secondly , comparing ours with other models , we find that [[ our 100D and 300D model ]] outperform [[ all other models ]] << of >> similar numbers of parameters .",0
1155,467,"Secondly , comparing ours with other models , we find that [[ our 100D and 300D model ]] outperform all other models << of >> [[ similar numbers of parameters ]] .",0
1156,467,"Secondly , comparing ours with other models , we find that our 100D and 300D model outperform [[ all other models ]] << of >> [[ similar numbers of parameters ]] .",1
1157,5842,[[ Copy - Net ]] << is >> the [[ attention - based seq2seq model ]] with the copy mechanism .,1
1158,5842,[[ Copy - Net ]] << is >> the attention - based seq2seq model with the [[ copy mechanism ]] .,0
1159,5842,Copy - Net << is >> the [[ attention - based seq2seq model ]] with the [[ copy mechanism ]] .,0
1160,5842,[[ Copy - Net ]] is the [[ attention - based seq2seq model ]] << with >> the copy mechanism .,0
1161,5842,[[ Copy - Net ]] is the attention - based seq2seq model << with >> the [[ copy mechanism ]] .,0
1162,5842,Copy - Net is the [[ attention - based seq2seq model ]] << with >> the [[ copy mechanism ]] .,1
1163,71,We << trained >> our [[ models ]] on [[ one machine ]] with 8 NVIDIA P100 GPUs .,0
1164,71,We << trained >> our [[ models ]] on one machine with [[ 8 NVIDIA P100 GPUs ]] .,0
1165,71,We << trained >> our models on [[ one machine ]] with [[ 8 NVIDIA P100 GPUs ]] .,0
1166,71,We trained our [[ models ]] << on >> [[ one machine ]] with 8 NVIDIA P100 GPUs .,1
1167,71,We trained our [[ models ]] << on >> one machine with [[ 8 NVIDIA P100 GPUs ]] .,0
1168,71,We trained our models << on >> [[ one machine ]] with [[ 8 NVIDIA P100 GPUs ]] .,0
1169,71,We trained our [[ models ]] on [[ one machine ]] << with >> 8 NVIDIA P100 GPUs .,0
1170,71,We trained our [[ models ]] on one machine << with >> [[ 8 NVIDIA P100 GPUs ]] .,0
1171,71,We trained our models on [[ one machine ]] << with >> [[ 8 NVIDIA P100 GPUs ]] .,1
1172,3217,We << initialized >> [[ word vectors ]] via [[ word2 vec ]] trained on Wikipedia 8 and randomly initialized all other parameters .,0
1173,3217,We initialized [[ word vectors ]] << via >> [[ word2 vec ]] trained on Wikipedia 8 and randomly initialized all other parameters .,1
1174,5943,We << present >> [[ structure - infused copy mechanisms ]] to facilitate copying [[ source words and relations ]] to the summary based on their semantic and structural importance in the source sentences .,0
1175,5943,We << present >> [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the [[ summary ]] based on their semantic and structural importance in the source sentences .,0
1176,5943,We << present >> [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the summary based on their [[ semantic and structural importance ]] in the source sentences .,0
1177,5943,We << present >> [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the summary based on their semantic and structural importance in the [[ source sentences ]] .,0
1178,5943,We << present >> structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the [[ summary ]] based on their semantic and structural importance in the source sentences .,0
1179,5943,We << present >> structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the summary based on their [[ semantic and structural importance ]] in the source sentences .,0
1180,5943,We << present >> structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the summary based on their semantic and structural importance in the [[ source sentences ]] .,0
1181,5943,We << present >> structure - infused copy mechanisms to facilitate copying source words and relations to the [[ summary ]] based on their [[ semantic and structural importance ]] in the source sentences .,0
1182,5943,We << present >> structure - infused copy mechanisms to facilitate copying source words and relations to the [[ summary ]] based on their semantic and structural importance in the [[ source sentences ]] .,0
1183,5943,We << present >> structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their [[ semantic and structural importance ]] in the [[ source sentences ]] .,0
1184,5943,We present [[ structure - infused copy mechanisms ]] << to facilitate copying >> [[ source words and relations ]] to the summary based on their semantic and structural importance in the source sentences .,1
1185,5943,We present [[ structure - infused copy mechanisms ]] << to facilitate copying >> source words and relations to the [[ summary ]] based on their semantic and structural importance in the source sentences .,0
1186,5943,We present [[ structure - infused copy mechanisms ]] << to facilitate copying >> source words and relations to the summary based on their [[ semantic and structural importance ]] in the source sentences .,0
1187,5943,We present [[ structure - infused copy mechanisms ]] << to facilitate copying >> source words and relations to the summary based on their semantic and structural importance in the [[ source sentences ]] .,0
1188,5943,We present structure - infused copy mechanisms << to facilitate copying >> [[ source words and relations ]] to the [[ summary ]] based on their semantic and structural importance in the source sentences .,0
1189,5943,We present structure - infused copy mechanisms << to facilitate copying >> [[ source words and relations ]] to the summary based on their [[ semantic and structural importance ]] in the source sentences .,0
1190,5943,We present structure - infused copy mechanisms << to facilitate copying >> [[ source words and relations ]] to the summary based on their semantic and structural importance in the [[ source sentences ]] .,0
1191,5943,We present structure - infused copy mechanisms << to facilitate copying >> source words and relations to the [[ summary ]] based on their [[ semantic and structural importance ]] in the source sentences .,0
1192,5943,We present structure - infused copy mechanisms << to facilitate copying >> source words and relations to the [[ summary ]] based on their semantic and structural importance in the [[ source sentences ]] .,0
1193,5943,We present structure - infused copy mechanisms << to facilitate copying >> source words and relations to the summary based on their [[ semantic and structural importance ]] in the [[ source sentences ]] .,0
1194,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying [[ source words and relations ]] << to >> the summary based on their semantic and structural importance in the source sentences .,0
1195,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations << to >> the [[ summary ]] based on their semantic and structural importance in the source sentences .,0
1196,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations << to >> the summary based on their [[ semantic and structural importance ]] in the source sentences .,0
1197,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations << to >> the summary based on their semantic and structural importance in the [[ source sentences ]] .,0
1198,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] << to >> the [[ summary ]] based on their semantic and structural importance in the source sentences .,1
1199,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] << to >> the summary based on their [[ semantic and structural importance ]] in the source sentences .,0
1200,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] << to >> the summary based on their semantic and structural importance in the [[ source sentences ]] .,0
1201,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations << to >> the [[ summary ]] based on their [[ semantic and structural importance ]] in the source sentences .,0
1202,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations << to >> the [[ summary ]] based on their semantic and structural importance in the [[ source sentences ]] .,0
1203,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations << to >> the summary based on their [[ semantic and structural importance ]] in the [[ source sentences ]] .,0
1204,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying [[ source words and relations ]] to the summary << based on >> their semantic and structural importance in the source sentences .,0
1205,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the [[ summary ]] << based on >> their semantic and structural importance in the source sentences .,0
1206,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the summary << based on >> their [[ semantic and structural importance ]] in the source sentences .,0
1207,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the summary << based on >> their semantic and structural importance in the [[ source sentences ]] .,0
1208,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the [[ summary ]] << based on >> their semantic and structural importance in the source sentences .,0
1209,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the summary << based on >> their [[ semantic and structural importance ]] in the source sentences .,0
1210,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the summary << based on >> their semantic and structural importance in the [[ source sentences ]] .,0
1211,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations to the [[ summary ]] << based on >> their [[ semantic and structural importance ]] in the source sentences .,1
1212,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations to the [[ summary ]] << based on >> their semantic and structural importance in the [[ source sentences ]] .,0
1213,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary << based on >> their [[ semantic and structural importance ]] in the [[ source sentences ]] .,0
1214,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying [[ source words and relations ]] to the summary based on their semantic and structural importance << in >> the source sentences .,0
1215,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the [[ summary ]] based on their semantic and structural importance << in >> the source sentences .,0
1216,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the summary based on their [[ semantic and structural importance ]] << in >> the source sentences .,0
1217,5943,We present [[ structure - infused copy mechanisms ]] to facilitate copying source words and relations to the summary based on their semantic and structural importance << in >> the [[ source sentences ]] .,0
1218,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the [[ summary ]] based on their semantic and structural importance << in >> the source sentences .,0
1219,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the summary based on their [[ semantic and structural importance ]] << in >> the source sentences .,0
1220,5943,We present structure - infused copy mechanisms to facilitate copying [[ source words and relations ]] to the summary based on their semantic and structural importance << in >> the [[ source sentences ]] .,0
1221,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations to the [[ summary ]] based on their [[ semantic and structural importance ]] << in >> the source sentences .,0
1222,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations to the [[ summary ]] based on their semantic and structural importance << in >> the [[ source sentences ]] .,0
1223,5943,We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their [[ semantic and structural importance ]] << in >> the [[ source sentences ]] .,1
1224,1898,"The [[ number of epochs ]] << is >> set to be [[ 10 ]] , and the feedforward dropout rate is 0.2 .",0
1225,1898,"The [[ number of epochs ]] << is >> set to be 10 , and the [[ feedforward dropout rate ]] is 0.2 .",0
1226,1898,"The [[ number of epochs ]] << is >> set to be 10 , and the feedforward dropout rate is [[ 0.2 ]] .",0
1227,1898,"The number of epochs << is >> set to be [[ 10 ]] , and the [[ feedforward dropout rate ]] is 0.2 .",0
1228,1898,"The number of epochs << is >> set to be [[ 10 ]] , and the feedforward dropout rate is [[ 0.2 ]] .",0
1229,1898,"The number of epochs << is >> set to be 10 , and the [[ feedforward dropout rate ]] is [[ 0.2 ]] .",1
1230,1898,"The [[ number of epochs ]] is << set to >> be [[ 10 ]] , and the feedforward dropout rate is 0.2 .",1
1231,1898,"The [[ number of epochs ]] is << set to >> be 10 , and the [[ feedforward dropout rate ]] is 0.2 .",0
1232,1898,"The [[ number of epochs ]] is << set to >> be 10 , and the feedforward dropout rate is [[ 0.2 ]] .",0
1233,1898,"The number of epochs is << set to >> be [[ 10 ]] , and the [[ feedforward dropout rate ]] is 0.2 .",0
1234,1898,"The number of epochs is << set to >> be [[ 10 ]] , and the feedforward dropout rate is [[ 0.2 ]] .",0
1235,1898,"The number of epochs is << set to >> be 10 , and the [[ feedforward dropout rate ]] is [[ 0.2 ]] .",0
1236,5052,"Finally , we << join >> these [[ Neural Nets ]] in [[ two different ways ]] : Multitasking and Pre-training .",0
1237,5052,"Finally , we << join >> these [[ Neural Nets ]] in two different ways : [[ Multitasking ]] and Pre-training .",0
1238,5052,"Finally , we << join >> these [[ Neural Nets ]] in two different ways : Multitasking and [[ Pre-training ]] .",0
1239,5052,"Finally , we << join >> these Neural Nets in [[ two different ways ]] : [[ Multitasking ]] and Pre-training .",0
1240,5052,"Finally , we << join >> these Neural Nets in [[ two different ways ]] : Multitasking and [[ Pre-training ]] .",0
1241,5052,"Finally , we << join >> these Neural Nets in two different ways : [[ Multitasking ]] and [[ Pre-training ]] .",0
1242,5052,"Finally , we join these [[ Neural Nets ]] << in >> [[ two different ways ]] : Multitasking and Pre-training .",1
1243,5052,"Finally , we join these [[ Neural Nets ]] << in >> two different ways : [[ Multitasking ]] and Pre-training .",0
1244,5052,"Finally , we join these [[ Neural Nets ]] << in >> two different ways : Multitasking and [[ Pre-training ]] .",0
1245,5052,"Finally , we join these Neural Nets << in >> [[ two different ways ]] : [[ Multitasking ]] and Pre-training .",0
1246,5052,"Finally , we join these Neural Nets << in >> [[ two different ways ]] : Multitasking and [[ Pre-training ]] .",0
1247,5052,"Finally , we join these Neural Nets << in >> two different ways : [[ Multitasking ]] and [[ Pre-training ]] .",0
1248,3995,"In this paper , we << propose >> the [[ usage of translations ]] as [[ compelling and effective domain - free contexts ]] , or contexts that are always available no matter what the task domain is .",0
1249,3995,"In this paper , we propose the [[ usage of translations ]] << as >> [[ compelling and effective domain - free contexts ]] , or contexts that are always available no matter what the task domain is .",1
1250,1207,[[ Documents ]] are << tokenized with >> the [[ same byte - pair encoding ]] as GPT - 2 .,1
1251,1207,[[ Documents ]] are << tokenized with >> the same byte - pair encoding as [[ GPT - 2 ]] .,0
1252,1207,Documents are << tokenized with >> the [[ same byte - pair encoding ]] as [[ GPT - 2 ]] .,0
1253,1207,[[ Documents ]] are tokenized with the [[ same byte - pair encoding ]] << as >> GPT - 2 .,0
1254,1207,[[ Documents ]] are tokenized with the same byte - pair encoding << as >> [[ GPT - 2 ]] .,0
1255,1207,Documents are tokenized with the [[ same byte - pair encoding ]] << as >> [[ GPT - 2 ]] .,1
1256,2844,"The [[ word - by - word perspective ]] << focuses on >> [[ similarity matches ]] between individual words from hypothesis and text , at various scales .",1
1257,2844,"The [[ word - by - word perspective ]] << focuses on >> similarity matches between [[ individual words from hypothesis and text , at various scales ]] .",0
1258,2844,"The word - by - word perspective << focuses on >> [[ similarity matches ]] between [[ individual words from hypothesis and text , at various scales ]] .",0
1259,2844,"The [[ word - by - word perspective ]] focuses on [[ similarity matches ]] << between >> individual words from hypothesis and text , at various scales .",0
1260,2844,"The [[ word - by - word perspective ]] focuses on similarity matches << between >> [[ individual words from hypothesis and text , at various scales ]] .",0
1261,2844,"The word - by - word perspective focuses on [[ similarity matches ]] << between >> [[ individual words from hypothesis and text , at various scales ]] .",1
1262,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] << for >> [[ optimization ]] with the two momentum parameters set to 0.9 and 0.999 respectively .",1
1263,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] << for >> optimization with the [[ two momentum parameters ]] set to 0.9 and 0.999 respectively .",0
1264,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] << for >> optimization with the two momentum parameters set to [[ 0.9 and 0.999 ]] respectively .",0
1265,2205,"We used Adam ( Kingma and Ba , 2015 ) << for >> [[ optimization ]] with the [[ two momentum parameters ]] set to 0.9 and 0.999 respectively .",0
1266,2205,"We used Adam ( Kingma and Ba , 2015 ) << for >> [[ optimization ]] with the two momentum parameters set to [[ 0.9 and 0.999 ]] respectively .",0
1267,2205,"We used Adam ( Kingma and Ba , 2015 ) << for >> optimization with the [[ two momentum parameters ]] set to [[ 0.9 and 0.999 ]] respectively .",0
1268,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] for [[ optimization ]] << with >> the two momentum parameters set to 0.9 and 0.999 respectively .",0
1269,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] for optimization << with >> the [[ two momentum parameters ]] set to 0.9 and 0.999 respectively .",0
1270,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] for optimization << with >> the two momentum parameters set to [[ 0.9 and 0.999 ]] respectively .",0
1271,2205,"We used Adam ( Kingma and Ba , 2015 ) for [[ optimization ]] << with >> the [[ two momentum parameters ]] set to 0.9 and 0.999 respectively .",1
1272,2205,"We used Adam ( Kingma and Ba , 2015 ) for [[ optimization ]] << with >> the two momentum parameters set to [[ 0.9 and 0.999 ]] respectively .",0
1273,2205,"We used Adam ( Kingma and Ba , 2015 ) for optimization << with >> the [[ two momentum parameters ]] set to [[ 0.9 and 0.999 ]] respectively .",0
1274,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] for [[ optimization ]] with the two momentum parameters << set to >> 0.9 and 0.999 respectively .",0
1275,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] for optimization with the [[ two momentum parameters ]] << set to >> 0.9 and 0.999 respectively .",0
1276,2205,"We used [[ Adam ( Kingma and Ba , 2015 ) ]] for optimization with the two momentum parameters << set to >> [[ 0.9 and 0.999 ]] respectively .",0
1277,2205,"We used Adam ( Kingma and Ba , 2015 ) for [[ optimization ]] with the [[ two momentum parameters ]] << set to >> 0.9 and 0.999 respectively .",0
1278,2205,"We used Adam ( Kingma and Ba , 2015 ) for [[ optimization ]] with the two momentum parameters << set to >> [[ 0.9 and 0.999 ]] respectively .",0
1279,2205,"We used Adam ( Kingma and Ba , 2015 ) for optimization with the [[ two momentum parameters ]] << set to >> [[ 0.9 and 0.999 ]] respectively .",1
1280,2980,"The [[ STAGG architecture ]] << delivers >> the [[ worst results ]] in our experiments , the main reason being supposedly that the model had to rely on manually defined features that are less flexible .",1
1281,4000,"( b ) [[ relative usability ]] ? r ( a , b ) << weighs >> the [[ confidence of ]] using sentence a in fixing sentence b.",1
1282,4000,"( b ) [[ relative usability ]] ? r ( a , b ) << weighs >> the confidence of using [[ sentence a ]] in fixing sentence b.",0
1283,4000,"( b ) relative usability ? r ( a , b ) << weighs >> the [[ confidence of ]] using [[ sentence a ]] in fixing sentence b.",0
1284,4000,"( b ) [[ relative usability ]] ? r ( a , b ) weighs the [[ confidence of ]] << using >> sentence a in fixing sentence b.",0
1285,4000,"( b ) [[ relative usability ]] ? r ( a , b ) weighs the confidence of << using >> [[ sentence a ]] in fixing sentence b.",0
1286,4000,"( b ) relative usability ? r ( a , b ) weighs the [[ confidence of ]] << using >> [[ sentence a ]] in fixing sentence b.",1
1287,4000,"( b ) [[ relative usability ]] ? r ( a , b ) weighs the [[ confidence of ]] using sentence a in << fixing >> sentence b.",0
1288,4000,"( b ) [[ relative usability ]] ? r ( a , b ) weighs the confidence of using [[ sentence a ]] in << fixing >> sentence b.",0
1289,4000,"( b ) relative usability ? r ( a , b ) weighs the [[ confidence of ]] using [[ sentence a ]] in << fixing >> sentence b.",0
1290,5172,"<< For >> [[ MOSEI dataset ]] , we obtain [[ better performance ]] with text .",0
1291,5172,"<< For >> [[ MOSEI dataset ]] , we obtain better performance with [[ text ]] .",0
1292,5172,"<< For >> MOSEI dataset , we obtain [[ better performance ]] with [[ text ]] .",0
1293,5172,"For [[ MOSEI dataset ]] , we << obtain >> [[ better performance ]] with text .",1
1294,5172,"For [[ MOSEI dataset ]] , we << obtain >> better performance with [[ text ]] .",0
1295,5172,"For MOSEI dataset , we << obtain >> [[ better performance ]] with [[ text ]] .",0
1296,5172,"For [[ MOSEI dataset ]] , we obtain [[ better performance ]] << with >> text .",0
1297,5172,"For [[ MOSEI dataset ]] , we obtain better performance << with >> [[ text ]] .",0
1298,5172,"For MOSEI dataset , we obtain [[ better performance ]] << with >> [[ text ]] .",1
1299,3036,We << reshape >> the [[ context tensor ]] into [[ H ? R 2 d T 6 ]] .,0
1300,3036,We reshape the [[ context tensor ]] << into >> [[ H ? R 2 d T 6 ]] .,1
1301,454,"In this paper , we << propose >> [[ Gumbel Tree - LSTM ]] , which is a [[ novel RvNN architecture ]] that does not require structured data and learns to compose task - specific tree structures without explicit guidance .",0
1302,454,"In this paper , we << propose >> [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require [[ structured data ]] and learns to compose task - specific tree structures without explicit guidance .",0
1303,454,"In this paper , we << propose >> [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require structured data and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1304,454,"In this paper , we << propose >> [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1305,454,"In this paper , we << propose >> Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require [[ structured data ]] and learns to compose task - specific tree structures without explicit guidance .",0
1306,454,"In this paper , we << propose >> Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require structured data and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1307,454,"In this paper , we << propose >> Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require structured data and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1308,454,"In this paper , we << propose >> Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require [[ structured data ]] and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1309,454,"In this paper , we << propose >> Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require [[ structured data ]] and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1310,454,"In this paper , we << propose >> Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose [[ task - specific tree structures ]] without [[ explicit guidance ]] .",0
1311,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which << is >> a [[ novel RvNN architecture ]] that does not require structured data and learns to compose task - specific tree structures without explicit guidance .",1
1312,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which << is >> a novel RvNN architecture that does not require [[ structured data ]] and learns to compose task - specific tree structures without explicit guidance .",0
1313,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which << is >> a novel RvNN architecture that does not require structured data and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1314,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which << is >> a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1315,454,"In this paper , we propose Gumbel Tree - LSTM , which << is >> a [[ novel RvNN architecture ]] that does not require [[ structured data ]] and learns to compose task - specific tree structures without explicit guidance .",0
1316,454,"In this paper , we propose Gumbel Tree - LSTM , which << is >> a [[ novel RvNN architecture ]] that does not require structured data and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1317,454,"In this paper , we propose Gumbel Tree - LSTM , which << is >> a [[ novel RvNN architecture ]] that does not require structured data and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1318,454,"In this paper , we propose Gumbel Tree - LSTM , which << is >> a novel RvNN architecture that does not require [[ structured data ]] and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1319,454,"In this paper , we propose Gumbel Tree - LSTM , which << is >> a novel RvNN architecture that does not require [[ structured data ]] and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1320,454,"In this paper , we propose Gumbel Tree - LSTM , which << is >> a novel RvNN architecture that does not require structured data and learns to compose [[ task - specific tree structures ]] without [[ explicit guidance ]] .",0
1321,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a [[ novel RvNN architecture ]] that does << not require >> structured data and learns to compose task - specific tree structures without explicit guidance .",0
1322,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does << not require >> [[ structured data ]] and learns to compose task - specific tree structures without explicit guidance .",0
1323,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does << not require >> structured data and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1324,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does << not require >> structured data and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1325,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does << not require >> [[ structured data ]] and learns to compose task - specific tree structures without explicit guidance .",1
1326,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does << not require >> structured data and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1327,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does << not require >> structured data and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1328,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does << not require >> [[ structured data ]] and learns to compose [[ task - specific tree structures ]] without explicit guidance .",0
1329,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does << not require >> [[ structured data ]] and learns to compose task - specific tree structures without [[ explicit guidance ]] .",0
1330,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does << not require >> structured data and learns to compose [[ task - specific tree structures ]] without [[ explicit guidance ]] .",0
1331,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a [[ novel RvNN architecture ]] that does not require structured data and << learns to compose >> task - specific tree structures without explicit guidance .",0
1332,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require [[ structured data ]] and << learns to compose >> task - specific tree structures without explicit guidance .",0
1333,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require structured data and << learns to compose >> [[ task - specific tree structures ]] without explicit guidance .",0
1334,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require structured data and << learns to compose >> task - specific tree structures without [[ explicit guidance ]] .",0
1335,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require [[ structured data ]] and << learns to compose >> task - specific tree structures without explicit guidance .",0
1336,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require structured data and << learns to compose >> [[ task - specific tree structures ]] without explicit guidance .",1
1337,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require structured data and << learns to compose >> task - specific tree structures without [[ explicit guidance ]] .",0
1338,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require [[ structured data ]] and << learns to compose >> [[ task - specific tree structures ]] without explicit guidance .",0
1339,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require [[ structured data ]] and << learns to compose >> task - specific tree structures without [[ explicit guidance ]] .",0
1340,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and << learns to compose >> [[ task - specific tree structures ]] without [[ explicit guidance ]] .",0
1341,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a [[ novel RvNN architecture ]] that does not require structured data and learns to compose task - specific tree structures << without >> explicit guidance .",0
1342,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require [[ structured data ]] and learns to compose task - specific tree structures << without >> explicit guidance .",0
1343,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require structured data and learns to compose [[ task - specific tree structures ]] << without >> explicit guidance .",0
1344,454,"In this paper , we propose [[ Gumbel Tree - LSTM ]] , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures << without >> [[ explicit guidance ]] .",0
1345,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require [[ structured data ]] and learns to compose task - specific tree structures << without >> explicit guidance .",0
1346,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require structured data and learns to compose [[ task - specific tree structures ]] << without >> explicit guidance .",0
1347,454,"In this paper , we propose Gumbel Tree - LSTM , which is a [[ novel RvNN architecture ]] that does not require structured data and learns to compose task - specific tree structures << without >> [[ explicit guidance ]] .",0
1348,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require [[ structured data ]] and learns to compose [[ task - specific tree structures ]] << without >> explicit guidance .",0
1349,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require [[ structured data ]] and learns to compose task - specific tree structures << without >> [[ explicit guidance ]] .",0
1350,454,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose [[ task - specific tree structures ]] << without >> [[ explicit guidance ]] .",1
1351,5479,[[ Additional baseline ]] [[ CNN and DAN models ]] << are trained without using >> any pretrained word or sentence embeddings .,0
1352,5479,[[ Additional baseline ]] CNN and DAN models << are trained without using >> any [[ pretrained word or sentence embeddings ]] .,0
1353,5479,Additional baseline [[ CNN and DAN models ]] << are trained without using >> any [[ pretrained word or sentence embeddings ]] .,1
1354,402,"In this article , we << introduce >> [[ BioBERT ]] , which is a [[ pre-trained language representation model ]] for the biomedical domain .",0
1355,402,"In this article , we << introduce >> [[ BioBERT ]] , which is a pre-trained language representation model for the [[ biomedical domain ]] .",0
1356,402,"In this article , we << introduce >> BioBERT , which is a [[ pre-trained language representation model ]] for the [[ biomedical domain ]] .",0
1357,402,"In this article , we introduce [[ BioBERT ]] , which << is >> a [[ pre-trained language representation model ]] for the biomedical domain .",1
1358,402,"In this article , we introduce [[ BioBERT ]] , which << is >> a pre-trained language representation model for the [[ biomedical domain ]] .",0
1359,402,"In this article , we introduce BioBERT , which << is >> a [[ pre-trained language representation model ]] for the [[ biomedical domain ]] .",0
1360,402,"In this article , we introduce [[ BioBERT ]] , which is a [[ pre-trained language representation model ]] << for >> the biomedical domain .",0
1361,402,"In this article , we introduce [[ BioBERT ]] , which is a pre-trained language representation model << for >> the [[ biomedical domain ]] .",0
1362,402,"In this article , we introduce BioBERT , which is a [[ pre-trained language representation model ]] << for >> the [[ biomedical domain ]] .",1
1363,5085,"[[ Tree-LSTM ]] : [[ Memory cells ]] was << introduced by >> Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1364,5085,"[[ Tree-LSTM ]] : Memory cells was << introduced by >> [[ Tree - Structured Long Short - Term Memory ]] and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1365,5085,"[[ Tree-LSTM ]] : Memory cells was << introduced by >> Tree - Structured Long Short - Term Memory and [[ gates ]] into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1366,5085,"[[ Tree-LSTM ]] : Memory cells was << introduced by >> Tree - Structured Long Short - Term Memory and gates into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1367,5085,"Tree-LSTM : [[ Memory cells ]] was << introduced by >> [[ Tree - Structured Long Short - Term Memory ]] and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",1
1368,5085,"Tree-LSTM : [[ Memory cells ]] was << introduced by >> Tree - Structured Long Short - Term Memory and [[ gates ]] into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1369,5085,"Tree-LSTM : [[ Memory cells ]] was << introduced by >> Tree - Structured Long Short - Term Memory and gates into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1370,5085,"Tree-LSTM : Memory cells was << introduced by >> [[ Tree - Structured Long Short - Term Memory ]] and [[ gates ]] into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1371,5085,"Tree-LSTM : Memory cells was << introduced by >> [[ Tree - Structured Long Short - Term Memory ]] and gates into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1372,5085,"Tree-LSTM : Memory cells was << introduced by >> Tree - Structured Long Short - Term Memory and [[ gates ]] into [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1373,5085,"[[ Tree-LSTM ]] : [[ Memory cells ]] was introduced by Tree - Structured Long Short - Term Memory and gates << into >> tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1374,5085,"[[ Tree-LSTM ]] : Memory cells was introduced by [[ Tree - Structured Long Short - Term Memory ]] and gates << into >> tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1375,5085,"[[ Tree-LSTM ]] : Memory cells was introduced by Tree - Structured Long Short - Term Memory and [[ gates ]] << into >> tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1376,5085,"[[ Tree-LSTM ]] : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates << into >> [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1377,5085,"Tree-LSTM : [[ Memory cells ]] was introduced by [[ Tree - Structured Long Short - Term Memory ]] and gates << into >> tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1378,5085,"Tree-LSTM : [[ Memory cells ]] was introduced by Tree - Structured Long Short - Term Memory and [[ gates ]] << into >> tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1379,5085,"Tree-LSTM : [[ Memory cells ]] was introduced by Tree - Structured Long Short - Term Memory and gates << into >> [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1380,5085,"Tree-LSTM : Memory cells was introduced by [[ Tree - Structured Long Short - Term Memory ]] and [[ gates ]] << into >> tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1381,5085,"Tree-LSTM : Memory cells was introduced by [[ Tree - Structured Long Short - Term Memory ]] and gates << into >> [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",0
1382,5085,"Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and [[ gates ]] << into >> [[ tree - structured neural network ]] , which is beneficial to capture semantic relatedness by parsing syntax trees .",1
1383,677,The [[ NASM ]] ( << is >> a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,1
1384,677,The [[ NASM ]] ( << is >> a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,0
1385,677,The [[ NASM ]] ( << is >> a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1386,677,The [[ NASM ]] ( << is >> a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1387,677,The [[ NASM ]] ( << is >> a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1388,677,The NASM ( << is >> a [[ supervised conditional model ]] which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,0
1389,677,The NASM ( << is >> a [[ supervised conditional model ]] which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1390,677,The NASM ( << is >> a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1391,677,The NASM ( << is >> a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1392,677,The NASM ( << is >> a supervised conditional model which imbues [[ LSTMs ]] with a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1393,677,The NASM ( << is >> a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1394,677,The NASM ( << is >> a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1395,677,The NASM ( << is >> a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1396,677,The NASM ( << is >> a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1397,677,The NASM ( << is >> a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] of [[ question - answer pairs ]] and predict their relatedness .,0
1398,677,The [[ NASM ]] ( is a [[ supervised conditional model ]] which << imbues >> LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,0
1399,677,The [[ NASM ]] ( is a supervised conditional model which << imbues >> [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,0
1400,677,The [[ NASM ]] ( is a supervised conditional model which << imbues >> LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1401,677,The [[ NASM ]] ( is a supervised conditional model which << imbues >> LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1402,677,The [[ NASM ]] ( is a supervised conditional model which << imbues >> LSTMs with a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1403,677,The NASM ( is a [[ supervised conditional model ]] which << imbues >> [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,1
1404,677,The NASM ( is a [[ supervised conditional model ]] which << imbues >> LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1405,677,The NASM ( is a [[ supervised conditional model ]] which << imbues >> LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1406,677,The NASM ( is a [[ supervised conditional model ]] which << imbues >> LSTMs with a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1407,677,The NASM ( is a supervised conditional model which << imbues >> [[ LSTMs ]] with a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1408,677,The NASM ( is a supervised conditional model which << imbues >> [[ LSTMs ]] with a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1409,677,The NASM ( is a supervised conditional model which << imbues >> [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1410,677,The NASM ( is a supervised conditional model which << imbues >> LSTMs with a [[ latent stochastic attention mechanism ]] to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1411,677,The NASM ( is a supervised conditional model which << imbues >> LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1412,677,The NASM ( is a supervised conditional model which << imbues >> LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] of [[ question - answer pairs ]] and predict their relatedness .,0
1413,677,The [[ NASM ]] ( is a [[ supervised conditional model ]] which imbues LSTMs << with >> a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,0
1414,677,The [[ NASM ]] ( is a supervised conditional model which imbues [[ LSTMs ]] << with >> a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,0
1415,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs << with >> a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1416,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs << with >> a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1417,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs << with >> a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1418,677,The NASM ( is a [[ supervised conditional model ]] which imbues [[ LSTMs ]] << with >> a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,0
1419,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs << with >> a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,0
1420,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs << with >> a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1421,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs << with >> a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1422,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] << with >> a [[ latent stochastic attention mechanism ]] to model the semantics of question - answer pairs and predict their relatedness .,1
1423,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] << with >> a latent stochastic attention mechanism to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1424,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] << with >> a latent stochastic attention mechanism to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1425,677,The NASM ( is a supervised conditional model which imbues LSTMs << with >> a [[ latent stochastic attention mechanism ]] to model the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1426,677,The NASM ( is a supervised conditional model which imbues LSTMs << with >> a [[ latent stochastic attention mechanism ]] to model the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1427,677,The NASM ( is a supervised conditional model which imbues LSTMs << with >> a latent stochastic attention mechanism to model the [[ semantics ]] of [[ question - answer pairs ]] and predict their relatedness .,0
1428,677,The [[ NASM ]] ( is a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism << to model >> the semantics of question - answer pairs and predict their relatedness .,0
1429,677,The [[ NASM ]] ( is a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism << to model >> the semantics of question - answer pairs and predict their relatedness .,0
1430,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] << to model >> the semantics of question - answer pairs and predict their relatedness .,0
1431,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism << to model >> the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1432,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism << to model >> the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1433,677,The NASM ( is a [[ supervised conditional model ]] which imbues [[ LSTMs ]] with a latent stochastic attention mechanism << to model >> the semantics of question - answer pairs and predict their relatedness .,0
1434,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs with a [[ latent stochastic attention mechanism ]] << to model >> the semantics of question - answer pairs and predict their relatedness .,0
1435,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism << to model >> the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1436,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism << to model >> the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1437,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] with a [[ latent stochastic attention mechanism ]] << to model >> the semantics of question - answer pairs and predict their relatedness .,0
1438,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism << to model >> the [[ semantics ]] of question - answer pairs and predict their relatedness .,1
1439,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism << to model >> the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1440,677,The NASM ( is a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] << to model >> the [[ semantics ]] of question - answer pairs and predict their relatedness .,0
1441,677,The NASM ( is a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] << to model >> the semantics of [[ question - answer pairs ]] and predict their relatedness .,0
1442,677,The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism << to model >> the [[ semantics ]] of [[ question - answer pairs ]] and predict their relatedness .,0
1443,677,The [[ NASM ]] ( is a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism to model the semantics << of >> question - answer pairs and predict their relatedness .,0
1444,677,The [[ NASM ]] ( is a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics << of >> question - answer pairs and predict their relatedness .,0
1445,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics << of >> question - answer pairs and predict their relatedness .,0
1446,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] << of >> question - answer pairs and predict their relatedness .,0
1447,677,The [[ NASM ]] ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics << of >> [[ question - answer pairs ]] and predict their relatedness .,0
1448,677,The NASM ( is a [[ supervised conditional model ]] which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics << of >> question - answer pairs and predict their relatedness .,0
1449,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics << of >> question - answer pairs and predict their relatedness .,0
1450,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] << of >> question - answer pairs and predict their relatedness .,0
1451,677,The NASM ( is a [[ supervised conditional model ]] which imbues LSTMs with a latent stochastic attention mechanism to model the semantics << of >> [[ question - answer pairs ]] and predict their relatedness .,0
1452,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] with a [[ latent stochastic attention mechanism ]] to model the semantics << of >> question - answer pairs and predict their relatedness .,0
1453,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the [[ semantics ]] << of >> question - answer pairs and predict their relatedness .,0
1454,677,The NASM ( is a supervised conditional model which imbues [[ LSTMs ]] with a latent stochastic attention mechanism to model the semantics << of >> [[ question - answer pairs ]] and predict their relatedness .,0
1455,677,The NASM ( is a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the [[ semantics ]] << of >> question - answer pairs and predict their relatedness .,0
1456,677,The NASM ( is a supervised conditional model which imbues LSTMs with a [[ latent stochastic attention mechanism ]] to model the semantics << of >> [[ question - answer pairs ]] and predict their relatedness .,0
1457,677,The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the [[ semantics ]] << of >> [[ question - answer pairs ]] and predict their relatedness .,1
1458,1983,The [[ performance gain ]] << over >> [[ strong baselines ]] such as DecompAtt and ESIM are ?,1
1459,1983,The [[ performance gain ]] << over >> strong baselines such as [[ DecompAtt ]] and ESIM are ?,0
1460,1983,The [[ performance gain ]] << over >> strong baselines such as DecompAtt and [[ ESIM ]] are ?,0
1461,1983,The performance gain << over >> [[ strong baselines ]] such as [[ DecompAtt ]] and ESIM are ?,0
1462,1983,The performance gain << over >> [[ strong baselines ]] such as DecompAtt and [[ ESIM ]] are ?,0
1463,1983,The performance gain << over >> strong baselines such as [[ DecompAtt ]] and [[ ESIM ]] are ?,0
1464,1983,The [[ performance gain ]] over [[ strong baselines ]] << such as >> DecompAtt and ESIM are ?,0
1465,1983,The [[ performance gain ]] over strong baselines << such as >> [[ DecompAtt ]] and ESIM are ?,0
1466,1983,The [[ performance gain ]] over strong baselines << such as >> DecompAtt and [[ ESIM ]] are ?,0
1467,1983,The performance gain over [[ strong baselines ]] << such as >> [[ DecompAtt ]] and ESIM are ?,1
1468,1983,The performance gain over [[ strong baselines ]] << such as >> DecompAtt and [[ ESIM ]] are ?,1
1469,1983,The performance gain over strong baselines << such as >> [[ DecompAtt ]] and [[ ESIM ]] are ?,0
1470,1983,The [[ performance gain ]] over [[ strong baselines ]] such as DecompAtt and ESIM << are >> ?,0
1471,1983,The [[ performance gain ]] over strong baselines such as [[ DecompAtt ]] and ESIM << are >> ?,0
1472,1983,The [[ performance gain ]] over strong baselines such as DecompAtt and [[ ESIM ]] << are >> ?,0
1473,1983,The performance gain over [[ strong baselines ]] such as [[ DecompAtt ]] and ESIM << are >> ?,0
1474,1983,The performance gain over [[ strong baselines ]] such as DecompAtt and [[ ESIM ]] << are >> ?,0
1475,1983,The performance gain over strong baselines such as [[ DecompAtt ]] and [[ ESIM ]] << are >> ?,0
1476,4918,"<< At >> [[ each iteration ]] , the [[ representation of the test utterance ]] is improved with this summary representation and finally used for prediction .",0
1477,4918,"<< At >> [[ each iteration ]] , the representation of the test utterance is improved with this [[ summary representation ]] and finally used for prediction .",0
1478,4918,"<< At >> [[ each iteration ]] , the representation of the test utterance is improved with this summary representation and finally used for [[ prediction ]] .",0
1479,4918,"<< At >> each iteration , the [[ representation of the test utterance ]] is improved with this [[ summary representation ]] and finally used for prediction .",0
1480,4918,"<< At >> each iteration , the [[ representation of the test utterance ]] is improved with this summary representation and finally used for [[ prediction ]] .",0
1481,4918,"<< At >> each iteration , the representation of the test utterance is improved with this [[ summary representation ]] and finally used for [[ prediction ]] .",0
1482,4918,"At [[ each iteration ]] , the [[ representation of the test utterance ]] is << improved with >> this summary representation and finally used for prediction .",0
1483,4918,"At [[ each iteration ]] , the representation of the test utterance is << improved with >> this [[ summary representation ]] and finally used for prediction .",0
1484,4918,"At [[ each iteration ]] , the representation of the test utterance is << improved with >> this summary representation and finally used for [[ prediction ]] .",0
1485,4918,"At each iteration , the [[ representation of the test utterance ]] is << improved with >> this [[ summary representation ]] and finally used for prediction .",1
1486,4918,"At each iteration , the [[ representation of the test utterance ]] is << improved with >> this summary representation and finally used for [[ prediction ]] .",0
1487,4918,"At each iteration , the representation of the test utterance is << improved with >> this [[ summary representation ]] and finally used for [[ prediction ]] .",0
1488,4918,"At [[ each iteration ]] , the [[ representation of the test utterance ]] is improved with this summary representation and finally << used for >> prediction .",0
1489,4918,"At [[ each iteration ]] , the representation of the test utterance is improved with this [[ summary representation ]] and finally << used for >> prediction .",0
1490,4918,"At [[ each iteration ]] , the representation of the test utterance is improved with this summary representation and finally << used for >> [[ prediction ]] .",0
1491,4918,"At each iteration , the [[ representation of the test utterance ]] is improved with this [[ summary representation ]] and finally << used for >> prediction .",0
1492,4918,"At each iteration , the [[ representation of the test utterance ]] is improved with this summary representation and finally << used for >> [[ prediction ]] .",1
1493,4918,"At each iteration , the representation of the test utterance is improved with this [[ summary representation ]] and finally << used for >> [[ prediction ]] .",0
1494,3843,[[ Sketches ]] << produced by >> [[ COARSE2FINE ]] are more accurate across the board .,1
1495,3843,[[ Sketches ]] << produced by >> COARSE2FINE are [[ more accurate ]] across the board .,0
1496,3843,Sketches << produced by >> [[ COARSE2FINE ]] are [[ more accurate ]] across the board .,0
1497,3843,[[ Sketches ]] produced by [[ COARSE2FINE ]] << are >> more accurate across the board .,0
1498,3843,[[ Sketches ]] produced by COARSE2FINE << are >> [[ more accurate ]] across the board .,0
1499,3843,Sketches produced by [[ COARSE2FINE ]] << are >> [[ more accurate ]] across the board .,1
1500,4931,"[[ MFN ]] << performs >> [[ multi-view learning ]] by using Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .",1
1501,4931,"[[ MFN ]] << performs >> multi-view learning by using [[ Delta - memory Attention Network ]] , a fusion mechanism to learn cross - view interactions .",0
1502,4931,"[[ MFN ]] << performs >> multi-view learning by using Delta - memory Attention Network , a [[ fusion mechanism ]] to learn cross - view interactions .",0
1503,4931,"[[ MFN ]] << performs >> multi-view learning by using Delta - memory Attention Network , a fusion mechanism to learn [[ cross - view interactions ]] .",0
1504,4931,"MFN << performs >> [[ multi-view learning ]] by using [[ Delta - memory Attention Network ]] , a fusion mechanism to learn cross - view interactions .",0
1505,4931,"MFN << performs >> [[ multi-view learning ]] by using Delta - memory Attention Network , a [[ fusion mechanism ]] to learn cross - view interactions .",0
1506,4931,"MFN << performs >> [[ multi-view learning ]] by using Delta - memory Attention Network , a fusion mechanism to learn [[ cross - view interactions ]] .",0
1507,4931,"MFN << performs >> multi-view learning by using [[ Delta - memory Attention Network ]] , a [[ fusion mechanism ]] to learn cross - view interactions .",0
1508,4931,"MFN << performs >> multi-view learning by using [[ Delta - memory Attention Network ]] , a fusion mechanism to learn [[ cross - view interactions ]] .",0
1509,4931,"MFN << performs >> multi-view learning by using Delta - memory Attention Network , a [[ fusion mechanism ]] to learn [[ cross - view interactions ]] .",0
1510,4931,"[[ MFN ]] performs [[ multi-view learning ]] << by using >> Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .",0
1511,4931,"[[ MFN ]] performs multi-view learning << by using >> [[ Delta - memory Attention Network ]] , a fusion mechanism to learn cross - view interactions .",0
1512,4931,"[[ MFN ]] performs multi-view learning << by using >> Delta - memory Attention Network , a [[ fusion mechanism ]] to learn cross - view interactions .",0
1513,4931,"[[ MFN ]] performs multi-view learning << by using >> Delta - memory Attention Network , a fusion mechanism to learn [[ cross - view interactions ]] .",0
1514,4931,"MFN performs [[ multi-view learning ]] << by using >> [[ Delta - memory Attention Network ]] , a fusion mechanism to learn cross - view interactions .",1
1515,4931,"MFN performs [[ multi-view learning ]] << by using >> Delta - memory Attention Network , a [[ fusion mechanism ]] to learn cross - view interactions .",0
1516,4931,"MFN performs [[ multi-view learning ]] << by using >> Delta - memory Attention Network , a fusion mechanism to learn [[ cross - view interactions ]] .",0
1517,4931,"MFN performs multi-view learning << by using >> [[ Delta - memory Attention Network ]] , a [[ fusion mechanism ]] to learn cross - view interactions .",0
1518,4931,"MFN performs multi-view learning << by using >> [[ Delta - memory Attention Network ]] , a fusion mechanism to learn [[ cross - view interactions ]] .",0
1519,4931,"MFN performs multi-view learning << by using >> Delta - memory Attention Network , a [[ fusion mechanism ]] to learn [[ cross - view interactions ]] .",0
1520,4931,"[[ MFN ]] performs [[ multi-view learning ]] by using Delta - memory Attention Network , a fusion mechanism << to learn >> cross - view interactions .",0
1521,4931,"[[ MFN ]] performs multi-view learning by using [[ Delta - memory Attention Network ]] , a fusion mechanism << to learn >> cross - view interactions .",0
1522,4931,"[[ MFN ]] performs multi-view learning by using Delta - memory Attention Network , a [[ fusion mechanism ]] << to learn >> cross - view interactions .",0
1523,4931,"[[ MFN ]] performs multi-view learning by using Delta - memory Attention Network , a fusion mechanism << to learn >> [[ cross - view interactions ]] .",0
1524,4931,"MFN performs [[ multi-view learning ]] by using [[ Delta - memory Attention Network ]] , a fusion mechanism << to learn >> cross - view interactions .",0
1525,4931,"MFN performs [[ multi-view learning ]] by using Delta - memory Attention Network , a [[ fusion mechanism ]] << to learn >> cross - view interactions .",0
1526,4931,"MFN performs [[ multi-view learning ]] by using Delta - memory Attention Network , a fusion mechanism << to learn >> [[ cross - view interactions ]] .",0
1527,4931,"MFN performs multi-view learning by using [[ Delta - memory Attention Network ]] , a [[ fusion mechanism ]] << to learn >> cross - view interactions .",0
1528,4931,"MFN performs multi-view learning by using [[ Delta - memory Attention Network ]] , a fusion mechanism << to learn >> [[ cross - view interactions ]] .",0
1529,4931,"MFN performs multi-view learning by using Delta - memory Attention Network , a [[ fusion mechanism ]] << to learn >> [[ cross - view interactions ]] .",1
1530,174,"The MoE << consists of >> a [[ number of experts ]] , each a [[ simple feed - forward neural network ]] , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",0
1531,174,"The MoE << consists of >> a [[ number of experts ]] , each a simple feed - forward neural network , and a [[ trainable gating network ]] which selects a sparse combination of the experts to process each input ( see ) .",0
1532,174,"The MoE << consists of >> a [[ number of experts ]] , each a simple feed - forward neural network , and a trainable gating network which selects a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1533,174,"The MoE << consists of >> a number of experts , each a [[ simple feed - forward neural network ]] , and a [[ trainable gating network ]] which selects a sparse combination of the experts to process each input ( see ) .",0
1534,174,"The MoE << consists of >> a number of experts , each a [[ simple feed - forward neural network ]] , and a trainable gating network which selects a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1535,174,"The MoE << consists of >> a number of experts , each a simple feed - forward neural network , and a [[ trainable gating network ]] which selects a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1536,174,"The MoE consists of a [[ number of experts ]] , << each >> a [[ simple feed - forward neural network ]] , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",1
1537,174,"The MoE consists of a [[ number of experts ]] , << each >> a simple feed - forward neural network , and a [[ trainable gating network ]] which selects a sparse combination of the experts to process each input ( see ) .",0
1538,174,"The MoE consists of a [[ number of experts ]] , << each >> a simple feed - forward neural network , and a trainable gating network which selects a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1539,174,"The MoE consists of a number of experts , << each >> a [[ simple feed - forward neural network ]] , and a [[ trainable gating network ]] which selects a sparse combination of the experts to process each input ( see ) .",0
1540,174,"The MoE consists of a number of experts , << each >> a [[ simple feed - forward neural network ]] , and a trainable gating network which selects a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1541,174,"The MoE consists of a number of experts , << each >> a simple feed - forward neural network , and a [[ trainable gating network ]] which selects a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1542,174,"The MoE consists of a [[ number of experts ]] , each a [[ simple feed - forward neural network ]] , and a trainable gating network which << selects >> a sparse combination of the experts to process each input ( see ) .",0
1543,174,"The MoE consists of a [[ number of experts ]] , each a simple feed - forward neural network , and a [[ trainable gating network ]] which << selects >> a sparse combination of the experts to process each input ( see ) .",0
1544,174,"The MoE consists of a [[ number of experts ]] , each a simple feed - forward neural network , and a trainable gating network which << selects >> a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1545,174,"The MoE consists of a number of experts , each a [[ simple feed - forward neural network ]] , and a [[ trainable gating network ]] which << selects >> a sparse combination of the experts to process each input ( see ) .",0
1546,174,"The MoE consists of a number of experts , each a [[ simple feed - forward neural network ]] , and a trainable gating network which << selects >> a [[ sparse combination of the experts to process each input ]] ( see ) .",0
1547,174,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a [[ trainable gating network ]] which << selects >> a [[ sparse combination of the experts to process each input ]] ( see ) .",1
1548,3102,"The [[ competitor baselines ]] on this dataset << are >> [[ Attention Sum Reader ( ASR ) ]] , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .",1
1549,3102,"The [[ competitor baselines ]] on this dataset << are >> Attention Sum Reader ( ASR ) , [[ Focused Hierarchical RNNs ( FH - RNN ) ]] , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .",1
1550,3102,"The [[ competitor baselines ]] on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , [[ AMANDA ]] , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .",1
1551,3102,"The [[ competitor baselines ]] on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , [[ BiDAF ]] , AQA and the Reinforced Ranker - Reader ( R 3 ) .",1
1552,3102,"The [[ competitor baselines ]] on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , [[ AQA ]] and the Reinforced Ranker - Reader ( R 3 ) .",1
1553,3102,"The [[ competitor baselines ]] on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the [[ Reinforced Ranker - Reader ( R 3 ) ]] .",1
1554,3102,"The competitor baselines on this dataset << are >> [[ Attention Sum Reader ( ASR ) ]] , [[ Focused Hierarchical RNNs ( FH - RNN ) ]] , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .",0
1555,3102,"The competitor baselines on this dataset << are >> [[ Attention Sum Reader ( ASR ) ]] , Focused Hierarchical RNNs ( FH - RNN ) , [[ AMANDA ]] , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .",0
1556,3102,"The competitor baselines on this dataset << are >> [[ Attention Sum Reader ( ASR ) ]] , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , [[ BiDAF ]] , AQA and the Reinforced Ranker - Reader ( R 3 ) .",0
1557,3102,"The competitor baselines on this dataset << are >> [[ Attention Sum Reader ( ASR ) ]] , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , [[ AQA ]] and the Reinforced Ranker - Reader ( R 3 ) .",0
1558,3102,"The competitor baselines on this dataset << are >> [[ Attention Sum Reader ( ASR ) ]] , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the [[ Reinforced Ranker - Reader ( R 3 ) ]] .",0
1559,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , [[ Focused Hierarchical RNNs ( FH - RNN ) ]] , [[ AMANDA ]] , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .",0
1560,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , [[ Focused Hierarchical RNNs ( FH - RNN ) ]] , AMANDA , [[ BiDAF ]] , AQA and the Reinforced Ranker - Reader ( R 3 ) .",0
1561,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , [[ Focused Hierarchical RNNs ( FH - RNN ) ]] , AMANDA , BiDAF , [[ AQA ]] and the Reinforced Ranker - Reader ( R 3 ) .",0
1562,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , [[ Focused Hierarchical RNNs ( FH - RNN ) ]] , AMANDA , BiDAF , AQA and the [[ Reinforced Ranker - Reader ( R 3 ) ]] .",0
1563,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , [[ AMANDA ]] , [[ BiDAF ]] , AQA and the Reinforced Ranker - Reader ( R 3 ) .",0
1564,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , [[ AMANDA ]] , BiDAF , [[ AQA ]] and the Reinforced Ranker - Reader ( R 3 ) .",0
1565,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , [[ AMANDA ]] , BiDAF , AQA and the [[ Reinforced Ranker - Reader ( R 3 ) ]] .",0
1566,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , [[ BiDAF ]] , [[ AQA ]] and the Reinforced Ranker - Reader ( R 3 ) .",0
1567,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , [[ BiDAF ]] , AQA and the [[ Reinforced Ranker - Reader ( R 3 ) ]] .",0
1568,3102,"The competitor baselines on this dataset << are >> Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , [[ AQA ]] and the [[ Reinforced Ranker - Reader ( R 3 ) ]] .",0
1569,873,"We additionally use [[ dropout ]] << on >> [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",1
1570,873,"We additionally use [[ dropout ]] << on >> word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",1
1571,873,"We additionally use [[ dropout ]] << on >> word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",1
1572,873,"We additionally use [[ dropout ]] << on >> word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1573,873,"We additionally use [[ dropout ]] << on >> word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1574,873,"We additionally use [[ dropout ]] << on >> word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1575,873,"We additionally use [[ dropout ]] << on >> word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1576,873,"We additionally use [[ dropout ]] << on >> word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1577,873,"We additionally use dropout << on >> [[ word ]] , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1578,873,"We additionally use dropout << on >> [[ word ]] , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1579,873,"We additionally use dropout << on >> [[ word ]] , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1580,873,"We additionally use dropout << on >> [[ word ]] , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1581,873,"We additionally use dropout << on >> [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1582,873,"We additionally use dropout << on >> [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1583,873,"We additionally use dropout << on >> [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1584,873,"We additionally use dropout << on >> word , [[ character embeddings ]] and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1585,873,"We additionally use dropout << on >> word , [[ character embeddings ]] and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1586,873,"We additionally use dropout << on >> word , [[ character embeddings ]] and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1587,873,"We additionally use dropout << on >> word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1588,873,"We additionally use dropout << on >> word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1589,873,"We additionally use dropout << on >> word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1590,873,"We additionally use dropout << on >> word , character embeddings and [[ between layers ]] , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1591,873,"We additionally use dropout << on >> word , character embeddings and [[ between layers ]] , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1592,873,"We additionally use dropout << on >> word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1593,873,"We additionally use dropout << on >> word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1594,873,"We additionally use dropout << on >> word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1595,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the [[ word and character dropout rates ]] are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1596,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1597,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1598,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1599,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1600,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1601,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1602,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between [[ every two layers ]] is 0.1 .",0
1603,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is [[ 0.1 ]] .",0
1604,873,"We additionally use dropout << on >> word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is [[ 0.1 ]] .",0
1605,873,"We additionally use [[ dropout ]] on [[ word ]] , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1606,873,"We additionally use [[ dropout ]] on word , [[ character embeddings ]] and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1607,873,"We additionally use [[ dropout ]] on word , character embeddings and [[ between layers ]] , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1608,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , << where >> the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",1
1609,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , << where >> the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1610,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",1
1611,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1612,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1613,873,"We additionally use dropout on [[ word ]] , [[ character embeddings ]] and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1614,873,"We additionally use dropout on [[ word ]] , character embeddings and [[ between layers ]] , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1615,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , << where >> the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1616,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , << where >> the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1617,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1618,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1619,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1620,873,"We additionally use dropout on word , [[ character embeddings ]] and [[ between layers ]] , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1621,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , << where >> the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1622,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , << where >> the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1623,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1624,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1625,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1626,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , << where >> the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1627,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , << where >> the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1628,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1629,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1630,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1631,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the [[ word and character dropout rates ]] are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1632,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1633,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1634,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1635,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1636,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1637,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1638,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between [[ every two layers ]] is 0.1 .",0
1639,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is [[ 0.1 ]] .",0
1640,873,"We additionally use dropout on word , character embeddings and between layers , << where >> the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is [[ 0.1 ]] .",0
1641,873,"We additionally use [[ dropout ]] on [[ word ]] , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1642,873,"We additionally use [[ dropout ]] on word , [[ character embeddings ]] and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1643,873,"We additionally use [[ dropout ]] on word , character embeddings and [[ between layers ]] , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1644,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the [[ word and character dropout rates ]] << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1645,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates << are >> [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1646,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1647,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1648,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1649,873,"We additionally use dropout on [[ word ]] , [[ character embeddings ]] and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1650,873,"We additionally use dropout on [[ word ]] , character embeddings and [[ between layers ]] , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1651,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the [[ word and character dropout rates ]] << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1652,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates << are >> [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1653,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1654,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1655,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1656,873,"We additionally use dropout on word , [[ character embeddings ]] and [[ between layers ]] , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1657,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the [[ word and character dropout rates ]] << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1658,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates << are >> [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1659,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1660,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1661,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1662,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the [[ word and character dropout rates ]] << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",0
1663,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates << are >> [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",0
1664,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1665,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1666,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1667,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] << are >> [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is 0.1 .",1
1668,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] << are >> 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1669,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] << are >> 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1670,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] << are >> 0.1 and 0.05 respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1671,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates << are >> [[ 0.1 and 0.05 ]] respectively , and the [[ dropout rate ]] between every two layers is 0.1 .",0
1672,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates << are >> [[ 0.1 and 0.05 ]] respectively , and the dropout rate between [[ every two layers ]] is 0.1 .",0
1673,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates << are >> [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers is [[ 0.1 ]] .",0
1674,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the [[ dropout rate ]] between [[ every two layers ]] is 0.1 .",0
1675,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers is [[ 0.1 ]] .",0
1676,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates << are >> 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] is [[ 0.1 ]] .",0
1677,873,"We additionally use [[ dropout ]] on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1678,873,"We additionally use [[ dropout ]] on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1679,873,"We additionally use [[ dropout ]] on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1680,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1681,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1682,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] << between >> every two layers is 0.1 .",0
1683,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> [[ every two layers ]] is 0.1 .",0
1684,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is [[ 0.1 ]] .",0
1685,873,"We additionally use dropout on [[ word ]] , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1686,873,"We additionally use dropout on [[ word ]] , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1687,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1688,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1689,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] << between >> every two layers is 0.1 .",0
1690,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> [[ every two layers ]] is 0.1 .",0
1691,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is [[ 0.1 ]] .",0
1692,873,"We additionally use dropout on word , [[ character embeddings ]] and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1693,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1694,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1695,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] << between >> every two layers is 0.1 .",0
1696,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> [[ every two layers ]] is 0.1 .",0
1697,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is [[ 0.1 ]] .",0
1698,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1699,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1700,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] << between >> every two layers is 0.1 .",0
1701,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> [[ every two layers ]] is 0.1 .",0
1702,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is [[ 0.1 ]] .",0
1703,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are [[ 0.1 and 0.05 ]] respectively , and the dropout rate << between >> every two layers is 0.1 .",0
1704,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the [[ dropout rate ]] << between >> every two layers is 0.1 .",0
1705,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate << between >> [[ every two layers ]] is 0.1 .",0
1706,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate << between >> every two layers is [[ 0.1 ]] .",0
1707,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the [[ dropout rate ]] << between >> every two layers is 0.1 .",0
1708,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate << between >> [[ every two layers ]] is 0.1 .",0
1709,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate << between >> every two layers is [[ 0.1 ]] .",0
1710,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] << between >> [[ every two layers ]] is 0.1 .",1
1711,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] << between >> every two layers is [[ 0.1 ]] .",0
1712,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate << between >> [[ every two layers ]] is [[ 0.1 ]] .",0
1713,873,"We additionally use [[ dropout ]] on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1714,873,"We additionally use [[ dropout ]] on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1715,873,"We additionally use [[ dropout ]] on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1716,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1717,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1718,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers << is >> 0.1 .",0
1719,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] << is >> 0.1 .",0
1720,873,"We additionally use [[ dropout ]] on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> [[ 0.1 ]] .",0
1721,873,"We additionally use dropout on [[ word ]] , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1722,873,"We additionally use dropout on [[ word ]] , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1723,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1724,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1725,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers << is >> 0.1 .",0
1726,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] << is >> 0.1 .",0
1727,873,"We additionally use dropout on [[ word ]] , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> [[ 0.1 ]] .",0
1728,873,"We additionally use dropout on word , [[ character embeddings ]] and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1729,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1730,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1731,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers << is >> 0.1 .",0
1732,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] << is >> 0.1 .",0
1733,873,"We additionally use dropout on word , [[ character embeddings ]] and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> [[ 0.1 ]] .",0
1734,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1735,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1736,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers << is >> 0.1 .",0
1737,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] << is >> 0.1 .",0
1738,873,"We additionally use dropout on word , character embeddings and [[ between layers ]] , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> [[ 0.1 ]] .",0
1739,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers << is >> 0.1 .",0
1740,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers << is >> 0.1 .",0
1741,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] << is >> 0.1 .",0
1742,873,"We additionally use dropout on word , character embeddings and between layers , where the [[ word and character dropout rates ]] are 0.1 and 0.05 respectively , and the dropout rate between every two layers << is >> [[ 0.1 ]] .",0
1743,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the [[ dropout rate ]] between every two layers << is >> 0.1 .",0
1744,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between [[ every two layers ]] << is >> 0.1 .",0
1745,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are [[ 0.1 and 0.05 ]] respectively , and the dropout rate between every two layers << is >> [[ 0.1 ]] .",0
1746,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between [[ every two layers ]] << is >> 0.1 .",0
1747,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the [[ dropout rate ]] between every two layers << is >> [[ 0.1 ]] .",0
1748,873,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between [[ every two layers ]] << is >> [[ 0.1 ]] .",1
1749,2853,"The [[ top N function ]] << contributes very little >> to the over all performance , suggesting that [[ most multi questions have their evidence distributed across contiguous sentences ]] .",0
1750,2853,"The [[ top N function ]] contributes very little to the over all performance , << suggesting >> that [[ most multi questions have their evidence distributed across contiguous sentences ]] .",1
1751,1486,"As introduced in Section 2 , we << use >> a [[ variable character embedding ]] with a [[ fixed pre-trained word embedding ]] to serve as part of the input into the model .",0
1752,1486,"As introduced in Section 2 , we << use >> a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as [[ part ]] of the input into the model .",0
1753,1486,"As introduced in Section 2 , we << use >> a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as part of the [[ input ]] into the model .",0
1754,1486,"As introduced in Section 2 , we << use >> a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as part of the input into the [[ model ]] .",0
1755,1486,"As introduced in Section 2 , we << use >> a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as [[ part ]] of the input into the model .",0
1756,1486,"As introduced in Section 2 , we << use >> a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as part of the [[ input ]] into the model .",0
1757,1486,"As introduced in Section 2 , we << use >> a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as part of the input into the [[ model ]] .",0
1758,1486,"As introduced in Section 2 , we << use >> a variable character embedding with a fixed pre-trained word embedding to serve as [[ part ]] of the [[ input ]] into the model .",0
1759,1486,"As introduced in Section 2 , we << use >> a variable character embedding with a fixed pre-trained word embedding to serve as [[ part ]] of the input into the [[ model ]] .",0
1760,1486,"As introduced in Section 2 , we << use >> a variable character embedding with a fixed pre-trained word embedding to serve as part of the [[ input ]] into the [[ model ]] .",0
1761,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] << with >> a [[ fixed pre-trained word embedding ]] to serve as part of the input into the model .",1
1762,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] << with >> a fixed pre-trained word embedding to serve as [[ part ]] of the input into the model .",0
1763,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] << with >> a fixed pre-trained word embedding to serve as part of the [[ input ]] into the model .",0
1764,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] << with >> a fixed pre-trained word embedding to serve as part of the input into the [[ model ]] .",0
1765,1486,"As introduced in Section 2 , we use a variable character embedding << with >> a [[ fixed pre-trained word embedding ]] to serve as [[ part ]] of the input into the model .",0
1766,1486,"As introduced in Section 2 , we use a variable character embedding << with >> a [[ fixed pre-trained word embedding ]] to serve as part of the [[ input ]] into the model .",0
1767,1486,"As introduced in Section 2 , we use a variable character embedding << with >> a [[ fixed pre-trained word embedding ]] to serve as part of the input into the [[ model ]] .",0
1768,1486,"As introduced in Section 2 , we use a variable character embedding << with >> a fixed pre-trained word embedding to serve as [[ part ]] of the [[ input ]] into the model .",0
1769,1486,"As introduced in Section 2 , we use a variable character embedding << with >> a fixed pre-trained word embedding to serve as [[ part ]] of the input into the [[ model ]] .",0
1770,1486,"As introduced in Section 2 , we use a variable character embedding << with >> a fixed pre-trained word embedding to serve as part of the [[ input ]] into the [[ model ]] .",0
1771,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a [[ fixed pre-trained word embedding ]] << to serve >> as part of the input into the model .",0
1772,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding << to serve >> as [[ part ]] of the input into the model .",1
1773,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding << to serve >> as part of the [[ input ]] into the model .",0
1774,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding << to serve >> as part of the input into the [[ model ]] .",0
1775,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] << to serve >> as [[ part ]] of the input into the model .",0
1776,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] << to serve >> as part of the [[ input ]] into the model .",0
1777,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] << to serve >> as part of the input into the [[ model ]] .",0
1778,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding << to serve >> as [[ part ]] of the [[ input ]] into the model .",0
1779,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding << to serve >> as [[ part ]] of the input into the [[ model ]] .",0
1780,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding << to serve >> as part of the [[ input ]] into the [[ model ]] .",0
1781,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a [[ fixed pre-trained word embedding ]] to serve as part << of >> the input into the model .",0
1782,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as [[ part ]] << of >> the input into the model .",0
1783,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as part << of >> the [[ input ]] into the model .",0
1784,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as part << of >> the input into the [[ model ]] .",0
1785,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as [[ part ]] << of >> the input into the model .",0
1786,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as part << of >> the [[ input ]] into the model .",0
1787,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as part << of >> the input into the [[ model ]] .",0
1788,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as [[ part ]] << of >> the [[ input ]] into the model .",1
1789,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as [[ part ]] << of >> the input into the [[ model ]] .",0
1790,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as part << of >> the [[ input ]] into the [[ model ]] .",0
1791,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a [[ fixed pre-trained word embedding ]] to serve as part of the input << into >> the model .",0
1792,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as [[ part ]] of the input << into >> the model .",0
1793,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as part of the [[ input ]] << into >> the model .",0
1794,1486,"As introduced in Section 2 , we use a [[ variable character embedding ]] with a fixed pre-trained word embedding to serve as part of the input << into >> the [[ model ]] .",0
1795,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as [[ part ]] of the input << into >> the model .",0
1796,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as part of the [[ input ]] << into >> the model .",0
1797,1486,"As introduced in Section 2 , we use a variable character embedding with a [[ fixed pre-trained word embedding ]] to serve as part of the input << into >> the [[ model ]] .",0
1798,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as [[ part ]] of the [[ input ]] << into >> the model .",0
1799,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as [[ part ]] of the input << into >> the [[ model ]] .",0
1800,1486,"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as part of the [[ input ]] << into >> the [[ model ]] .",1
1801,296,"All experiments << use >> the [[ Adam optimizer ]] ( Kingma and Ba , 2015 ) with [[ gradient norms ]] clipped at 5.0 .",0
1802,296,"All experiments << use >> the [[ Adam optimizer ]] ( Kingma and Ba , 2015 ) with gradient norms clipped at [[ 5.0 ]] .",0
1803,296,"All experiments << use >> the Adam optimizer ( Kingma and Ba , 2015 ) with [[ gradient norms ]] clipped at [[ 5.0 ]] .",0
1804,296,"All experiments use the [[ Adam optimizer ]] ( Kingma and Ba , 2015 ) << with >> [[ gradient norms ]] clipped at 5.0 .",1
1805,296,"All experiments use the [[ Adam optimizer ]] ( Kingma and Ba , 2015 ) << with >> gradient norms clipped at [[ 5.0 ]] .",0
1806,296,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) << with >> [[ gradient norms ]] clipped at [[ 5.0 ]] .",0
1807,296,"All experiments use the [[ Adam optimizer ]] ( Kingma and Ba , 2015 ) with [[ gradient norms ]] << clipped at >> 5.0 .",0
1808,296,"All experiments use the [[ Adam optimizer ]] ( Kingma and Ba , 2015 ) with gradient norms << clipped at >> [[ 5.0 ]] .",0
1809,296,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with [[ gradient norms ]] << clipped at >> [[ 5.0 ]] .",1
1810,4218,"This work << first builds >> an [[ RRC dataset ]] called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1811,4218,"This work << first builds >> an [[ RRC dataset ]] called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1812,4218,"This work << first builds >> an [[ RRC dataset ]] called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1813,4218,"This work << first builds >> an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1814,4218,"This work << first builds >> an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1815,4218,"This work << first builds >> an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1816,4218,"This work << first builds >> an RRC dataset called [[ ReviewRC ]] , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1817,4218,"This work << first builds >> an RRC dataset called [[ ReviewRC ]] , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1818,4218,"This work << first builds >> an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1819,4218,"This work << first builds >> an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1820,4218,"This work << first builds >> an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1821,4218,"This work << first builds >> an RRC dataset called ReviewRC , using [[ reviews ]] from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1822,4218,"This work << first builds >> an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1823,4218,"This work << first builds >> an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1824,4218,"This work << first builds >> an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1825,4218,"This work << first builds >> an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1826,4218,"This work << first builds >> an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1827,4218,"This work << first builds >> an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1828,4218,"This work << first builds >> an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1829,4218,"This work << first builds >> an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1830,4218,"This work << first builds >> an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the [[ domains of laptop and restaurant ]] .",0
1831,4218,"This work first builds an [[ RRC dataset ]] << called >> [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",1
1832,4218,"This work first builds an [[ RRC dataset ]] << called >> ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1833,4218,"This work first builds an [[ RRC dataset ]] << called >> ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1834,4218,"This work first builds an [[ RRC dataset ]] << called >> ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1835,4218,"This work first builds an [[ RRC dataset ]] << called >> ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1836,4218,"This work first builds an [[ RRC dataset ]] << called >> ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1837,4218,"This work first builds an RRC dataset << called >> [[ ReviewRC ]] , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1838,4218,"This work first builds an RRC dataset << called >> [[ ReviewRC ]] , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1839,4218,"This work first builds an RRC dataset << called >> [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1840,4218,"This work first builds an RRC dataset << called >> [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1841,4218,"This work first builds an RRC dataset << called >> [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1842,4218,"This work first builds an RRC dataset << called >> ReviewRC , using [[ reviews ]] from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1843,4218,"This work first builds an RRC dataset << called >> ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1844,4218,"This work first builds an RRC dataset << called >> ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1845,4218,"This work first builds an RRC dataset << called >> ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1846,4218,"This work first builds an RRC dataset << called >> ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1847,4218,"This work first builds an RRC dataset << called >> ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1848,4218,"This work first builds an RRC dataset << called >> ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1849,4218,"This work first builds an RRC dataset << called >> ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1850,4218,"This work first builds an RRC dataset << called >> ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1851,4218,"This work first builds an RRC dataset << called >> ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the [[ domains of laptop and restaurant ]] .",0
1852,4218,"This work first builds an [[ RRC dataset ]] called [[ ReviewRC ]] , << using >> reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1853,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , << using >> [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1854,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , << using >> reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1855,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , << using >> reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1856,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , << using >> reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1857,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , << using >> reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1858,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , << using >> [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",1
1859,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , << using >> reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1860,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , << using >> reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1861,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , << using >> reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1862,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , << using >> reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1863,4218,"This work first builds an RRC dataset called ReviewRC , << using >> [[ reviews ]] from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1864,4218,"This work first builds an RRC dataset called ReviewRC , << using >> [[ reviews ]] from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1865,4218,"This work first builds an RRC dataset called ReviewRC , << using >> [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1866,4218,"This work first builds an RRC dataset called ReviewRC , << using >> [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1867,4218,"This work first builds an RRC dataset called ReviewRC , << using >> reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1868,4218,"This work first builds an RRC dataset called ReviewRC , << using >> reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1869,4218,"This work first builds an RRC dataset called ReviewRC , << using >> reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1870,4218,"This work first builds an RRC dataset called ReviewRC , << using >> reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1871,4218,"This work first builds an RRC dataset called ReviewRC , << using >> reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1872,4218,"This work first builds an RRC dataset called ReviewRC , << using >> reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the [[ domains of laptop and restaurant ]] .",0
1873,4218,"This work first builds an [[ RRC dataset ]] called [[ ReviewRC ]] , using reviews << from >> SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1874,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using [[ reviews ]] << from >> SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1875,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews << from >> [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1876,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews << from >> SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1877,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews << from >> SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1878,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews << from >> SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1879,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using [[ reviews ]] << from >> SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1880,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews << from >> [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1881,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews << from >> SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1882,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews << from >> SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1883,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews << from >> SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1884,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] << from >> [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",1
1885,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] << from >> SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1886,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] << from >> SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1887,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] << from >> SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1888,4218,"This work first builds an RRC dataset called ReviewRC , using reviews << from >> [[ SemEval 2016 Task 5 ]] 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1889,4218,"This work first builds an RRC dataset called ReviewRC , using reviews << from >> [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1890,4218,"This work first builds an RRC dataset called ReviewRC , using reviews << from >> [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1891,4218,"This work first builds an RRC dataset called ReviewRC , using reviews << from >> SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1892,4218,"This work first builds an RRC dataset called ReviewRC , using reviews << from >> SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1893,4218,"This work first builds an RRC dataset called ReviewRC , using reviews << from >> SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the [[ domains of laptop and restaurant ]] .",0
1894,4218,"This work first builds an [[ RRC dataset ]] called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1895,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1896,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1897,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which << is a >> [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1898,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which << is a >> popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1899,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1900,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using [[ reviews ]] from SemEval 2016 Task 5 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1901,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1902,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which << is a >> [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1903,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which << is a >> popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1904,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1905,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from [[ SemEval 2016 Task 5 ]] 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1906,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which << is a >> [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1907,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which << is a >> popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1908,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1909,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which << is a >> [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",1
1910,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which << is a >> popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1911,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which << is a >> popular dataset for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1912,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which << is a >> [[ popular dataset ]] for [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1913,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which << is a >> [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1914,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which << is a >> popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] in the [[ domains of laptop and restaurant ]] .",0
1915,4218,"This work first builds an [[ RRC dataset ]] called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1916,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1917,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1918,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1919,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset << for >> [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1920,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1921,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1922,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1923,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1924,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset << for >> [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1925,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1926,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1927,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1928,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset << for >> [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1929,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1930,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a [[ popular dataset ]] << for >> aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",0
1931,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset << for >> [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",0
1932,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset << for >> aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1933,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] << for >> [[ aspect - based sentiment analysis ( ABSA ) ]] in the domains of laptop and restaurant .",1
1934,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] << for >> aspect - based sentiment analysis ( ABSA ) in the [[ domains of laptop and restaurant ]] .",0
1935,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset << for >> [[ aspect - based sentiment analysis ( ABSA ) ]] in the [[ domains of laptop and restaurant ]] .",0
1936,4218,"This work first builds an [[ RRC dataset ]] called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1937,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1938,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1939,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1940,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] << in >> the domains of laptop and restaurant .",0
1941,4218,"This work first builds an [[ RRC dataset ]] called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the [[ domains of laptop and restaurant ]] .",0
1942,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1943,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1944,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1945,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] << in >> the domains of laptop and restaurant .",0
1946,4218,"This work first builds an RRC dataset called [[ ReviewRC ]] , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the [[ domains of laptop and restaurant ]] .",0
1947,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1948,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1949,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] << in >> the domains of laptop and restaurant .",0
1950,4218,"This work first builds an RRC dataset called ReviewRC , using [[ reviews ]] from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the [[ domains of laptop and restaurant ]] .",0
1951,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) << in >> the domains of laptop and restaurant .",0
1952,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] << in >> the domains of laptop and restaurant .",0
1953,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from [[ SemEval 2016 Task 5 ]] 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) << in >> the [[ domains of laptop and restaurant ]] .",0
1954,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for [[ aspect - based sentiment analysis ( ABSA ) ]] << in >> the domains of laptop and restaurant .",0
1955,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a [[ popular dataset ]] for aspect - based sentiment analysis ( ABSA ) << in >> the [[ domains of laptop and restaurant ]] .",0
1956,4218,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for [[ aspect - based sentiment analysis ( ABSA ) ]] << in >> the [[ domains of laptop and restaurant ]] .",1
1957,3760,This is << done by >> [[ document modeling ]] of these [[ consolidated comments ]] belonging to the same forum .,0
1958,3760,This is << done by >> [[ document modeling ]] of these consolidated comments belonging to the [[ same forum ]] .,0
1959,3760,This is << done by >> document modeling of these [[ consolidated comments ]] belonging to the [[ same forum ]] .,0
1960,3760,This is done by [[ document modeling ]] << of >> these [[ consolidated comments ]] belonging to the same forum .,1
1961,3760,This is done by [[ document modeling ]] << of >> these consolidated comments belonging to the [[ same forum ]] .,0
1962,3760,This is done by document modeling << of >> these [[ consolidated comments ]] belonging to the [[ same forum ]] .,0
1963,3760,This is done by [[ document modeling ]] of these [[ consolidated comments ]] << belonging to >> the same forum .,0
1964,3760,This is done by [[ document modeling ]] of these consolidated comments << belonging to >> the [[ same forum ]] .,0
1965,3760,This is done by document modeling of these [[ consolidated comments ]] << belonging to >> the [[ same forum ]] .,1
1966,378,"Both [[ BERT BASE and BERT LARGE ]] << outperform >> [[ all systems on all tasks ]] by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",1
1967,378,"Both [[ BERT BASE and BERT LARGE ]] << outperform >> all systems on all tasks by a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0
1968,378,"Both [[ BERT BASE and BERT LARGE ]] << outperform >> all systems on all tasks by a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1969,378,"Both [[ BERT BASE and BERT LARGE ]] << outperform >> all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1970,378,"Both BERT BASE and BERT LARGE << outperform >> [[ all systems on all tasks ]] by a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0
1971,378,"Both BERT BASE and BERT LARGE << outperform >> [[ all systems on all tasks ]] by a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1972,378,"Both BERT BASE and BERT LARGE << outperform >> [[ all systems on all tasks ]] by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1973,378,"Both BERT BASE and BERT LARGE << outperform >> all systems on all tasks by a [[ substantial margin ]] , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1974,378,"Both BERT BASE and BERT LARGE << outperform >> all systems on all tasks by a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1975,378,"Both BERT BASE and BERT LARGE << outperform >> all systems on all tasks by a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the [[ prior state of the art ]] .",0
1976,378,"Both [[ BERT BASE and BERT LARGE ]] outperform [[ all systems on all tasks ]] << by >> a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0
1977,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks << by >> a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0
1978,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks << by >> a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1979,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks << by >> a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1980,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] << by >> a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",1
1981,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] << by >> a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1982,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] << by >> a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1983,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks << by >> a [[ substantial margin ]] , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1984,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks << by >> a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1985,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks << by >> a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the [[ prior state of the art ]] .",0
1986,378,"Both [[ BERT BASE and BERT LARGE ]] outperform [[ all systems on all tasks ]] by a substantial margin , << obtaining >> 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0
1987,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks by a [[ substantial margin ]] , << obtaining >> 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0
1988,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks by a substantial margin , << obtaining >> [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1989,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks by a substantial margin , << obtaining >> 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1990,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] by a [[ substantial margin ]] , << obtaining >> 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",0
1991,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] by a substantial margin , << obtaining >> [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",0
1992,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] by a substantial margin , << obtaining >> 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1993,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a [[ substantial margin ]] , << obtaining >> [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the prior state of the art .",1
1994,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a [[ substantial margin ]] , << obtaining >> 4.5 % and 7.0 % respective average accuracy improvement over the [[ prior state of the art ]] .",0
1995,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , << obtaining >> [[ 4.5 % and 7.0 % respective average accuracy improvement ]] over the [[ prior state of the art ]] .",0
1996,378,"Both [[ BERT BASE and BERT LARGE ]] outperform [[ all systems on all tasks ]] by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement << over >> the prior state of the art .",0
1997,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks by a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement << over >> the prior state of the art .",0
1998,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks by a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] << over >> the prior state of the art .",0
1999,378,"Both [[ BERT BASE and BERT LARGE ]] outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement << over >> the [[ prior state of the art ]] .",0
2000,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] by a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement << over >> the prior state of the art .",0
2001,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] by a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] << over >> the prior state of the art .",0
2002,378,"Both BERT BASE and BERT LARGE outperform [[ all systems on all tasks ]] by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement << over >> the [[ prior state of the art ]] .",0
2003,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a [[ substantial margin ]] , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] << over >> the prior state of the art .",0
2004,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a [[ substantial margin ]] , obtaining 4.5 % and 7.0 % respective average accuracy improvement << over >> the [[ prior state of the art ]] .",0
2005,378,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining [[ 4.5 % and 7.0 % respective average accuracy improvement ]] << over >> the [[ prior state of the art ]] .",1
2006,3110,We use the [[ CUDNN implementation ]] << of >> the [[ RNN encoder ]] .,1
2007,2770,The [[ question word baseline ]] that << classifies >> [[ any query ]] starting with a question word word n-grams char n-grams POS n -grams pwf ( q ) :,1
2008,2770,The [[ question word baseline ]] that << classifies >> any query starting with a [[ question word ]] word n-grams char n-grams POS n -grams pwf ( q ) :,0
2009,2770,The question word baseline that << classifies >> [[ any query ]] starting with a [[ question word ]] word n-grams char n-grams POS n -grams pwf ( q ) :,0
2010,2770,The [[ question word baseline ]] that classifies [[ any query ]] << starting with >> a question word word n-grams char n-grams POS n -grams pwf ( q ) :,0
2011,2770,The [[ question word baseline ]] that classifies any query << starting with >> a [[ question word ]] word n-grams char n-grams POS n -grams pwf ( q ) :,0
2012,2770,The question word baseline that classifies [[ any query ]] << starting with >> a [[ question word ]] word n-grams char n-grams POS n -grams pwf ( q ) :,1
2013,3918,"<< For >> [[ DEEP - ATT ]] with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2014,3918,"<< For >> [[ DEEP - ATT ]] with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2015,3918,"<< For >> [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2016,3918,"<< For >> [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2017,3918,"<< For >> [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2018,3918,"<< For >> [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2019,3918,"<< For >> DEEP - ATT with [[ FFN sub - layers ]] , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2020,3918,"<< For >> DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2021,3918,"<< For >> DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2022,3918,"<< For >> DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2023,3918,"<< For >> DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2024,3918,"<< For >> DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2025,3918,"<< For >> DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2026,3918,"<< For >> DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2027,3918,"<< For >> DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2028,3918,"<< For >> DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2029,3918,"<< For >> DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2030,3918,"<< For >> DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2031,3918,"<< For >> DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2032,3918,"<< For >> DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2033,3918,"<< For >> DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2034,3918,"For [[ DEEP - ATT ]] << with >> [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",1
2035,3918,"For [[ DEEP - ATT ]] << with >> FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2036,3918,"For [[ DEEP - ATT ]] << with >> FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2037,3918,"For [[ DEEP - ATT ]] << with >> FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2038,3918,"For [[ DEEP - ATT ]] << with >> FFN sub - layers , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2039,3918,"For [[ DEEP - ATT ]] << with >> FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2040,3918,"For DEEP - ATT << with >> [[ FFN sub - layers ]] , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2041,3918,"For DEEP - ATT << with >> [[ FFN sub - layers ]] , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2042,3918,"For DEEP - ATT << with >> [[ FFN sub - layers ]] , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2043,3918,"For DEEP - ATT << with >> [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2044,3918,"For DEEP - ATT << with >> [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2045,3918,"For DEEP - ATT << with >> FFN sub - layers , the [[ whole training stage ]] takes about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2046,3918,"For DEEP - ATT << with >> FFN sub - layers , the [[ whole training stage ]] takes about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2047,3918,"For DEEP - ATT << with >> FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2048,3918,"For DEEP - ATT << with >> FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2049,3918,"For DEEP - ATT << with >> FFN sub - layers , the whole training stage takes about [[ two days ]] to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2050,3918,"For DEEP - ATT << with >> FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2051,3918,"For DEEP - ATT << with >> FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2052,3918,"For DEEP - ATT << with >> FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2053,3918,"For DEEP - ATT << with >> FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2054,3918,"For DEEP - ATT << with >> FFN sub - layers , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2055,3918,"For [[ DEEP - ATT ]] with [[ FFN sub - layers ]] , the whole training stage << takes >> about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2056,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the [[ whole training stage ]] << takes >> about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2057,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage << takes >> about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2058,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage << takes >> about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2059,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage << takes >> about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2060,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage << takes >> about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2061,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the [[ whole training stage ]] << takes >> about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2062,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage << takes >> about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2063,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage << takes >> about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2064,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage << takes >> about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2065,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage << takes >> about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2066,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] << takes >> about [[ two days ]] to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",1
2067,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] << takes >> about two days to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2068,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] << takes >> about two days to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2069,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] << takes >> about two days to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2070,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage << takes >> about [[ two days ]] to [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2071,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage << takes >> about [[ two days ]] to finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2072,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage << takes >> about [[ two days ]] to finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2073,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage << takes >> about two days to [[ finish ]] on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2074,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage << takes >> about two days to [[ finish ]] on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2075,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage << takes >> about two days to finish on a [[ single Titan X GPU ]] , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2076,3918,"For [[ DEEP - ATT ]] with [[ FFN sub - layers ]] , the whole training stage takes about two days << to >> finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2077,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the [[ whole training stage ]] takes about two days << to >> finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2078,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about [[ two days ]] << to >> finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2079,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days << to >> [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2080,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days << to >> finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2081,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days << to >> finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2082,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the [[ whole training stage ]] takes about two days << to >> finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2083,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about [[ two days ]] << to >> finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2084,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days << to >> [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2085,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days << to >> finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2086,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days << to >> finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2087,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about [[ two days ]] << to >> finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2088,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days << to >> [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2089,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days << to >> finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2090,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days << to >> finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2091,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] << to >> [[ finish ]] on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",1
2092,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] << to >> finish on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2093,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] << to >> finish on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2094,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days << to >> [[ finish ]] on a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2095,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days << to >> [[ finish ]] on a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2096,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days << to >> finish on a [[ single Titan X GPU ]] , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2097,3918,"For [[ DEEP - ATT ]] with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2098,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2099,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2100,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2101,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to finish << on >> a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2102,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to finish << on >> a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2103,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the [[ whole training stage ]] takes about two days to finish << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2104,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about [[ two days ]] to finish << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2105,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to [[ finish ]] << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2106,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish << on >> a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2107,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish << on >> a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2108,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about [[ two days ]] to finish << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2109,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to [[ finish ]] << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2110,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish << on >> a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2111,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish << on >> a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2112,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to [[ finish ]] << on >> a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",0
2113,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish << on >> a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",0
2114,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish << on >> a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2115,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] << on >> a [[ single Titan X GPU ]] , which is 2.5 times faster than the previous approach ) .",1
2116,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] << on >> a single Titan X GPU , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2117,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish << on >> a [[ single Titan X GPU ]] , which is [[ 2.5 times faster ]] than the previous approach ) .",0
2118,3918,"For [[ DEEP - ATT ]] with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2119,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2120,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2121,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2122,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , << which is >> 2.5 times faster than the previous approach ) .",0
2123,3918,"For [[ DEEP - ATT ]] with FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , << which is >> [[ 2.5 times faster ]] than the previous approach ) .",0
2124,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2125,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2126,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2127,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , << which is >> 2.5 times faster than the previous approach ) .",0
2128,3918,"For DEEP - ATT with [[ FFN sub - layers ]] , the whole training stage takes about two days to finish on a single Titan X GPU , << which is >> [[ 2.5 times faster ]] than the previous approach ) .",0
2129,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about [[ two days ]] to finish on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2130,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to [[ finish ]] on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2131,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a [[ single Titan X GPU ]] , << which is >> 2.5 times faster than the previous approach ) .",0
2132,3918,"For DEEP - ATT with FFN sub - layers , the [[ whole training stage ]] takes about two days to finish on a single Titan X GPU , << which is >> [[ 2.5 times faster ]] than the previous approach ) .",0
2133,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to [[ finish ]] on a single Titan X GPU , << which is >> 2.5 times faster than the previous approach ) .",0
2134,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a [[ single Titan X GPU ]] , << which is >> 2.5 times faster than the previous approach ) .",0
2135,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about [[ two days ]] to finish on a single Titan X GPU , << which is >> [[ 2.5 times faster ]] than the previous approach ) .",1
2136,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a [[ single Titan X GPU ]] , << which is >> 2.5 times faster than the previous approach ) .",0
2137,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to [[ finish ]] on a single Titan X GPU , << which is >> [[ 2.5 times faster ]] than the previous approach ) .",0
2138,3918,"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish on a [[ single Titan X GPU ]] , << which is >> [[ 2.5 times faster ]] than the previous approach ) .",0
2139,2210,"For comparison , we also << report >> the [[ performance ]] of the [[ paragraph vector model ]] ( PV ; ; see , second block ) which neither operates on trees nor sequences but learns distributed document representations parameterized directly .",0
2140,2210,"For comparison , we also report the [[ performance ]] << of >> the [[ paragraph vector model ]] ( PV ; ; see , second block ) which neither operates on trees nor sequences but learns distributed document representations parameterized directly .",1
2141,2599,"It << is >> a truly [[ endto - end model ]] requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",0
2142,2599,"It << is >> a truly [[ endto - end model ]] requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",0
2143,2599,"It << is >> a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings on unlabeled corpora .",0
2144,2599,"It << is >> a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2145,2599,"It << is >> a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2146,2599,"It << is >> a truly endto - end model requiring no [[ task - specific resources ]] , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",0
2147,2599,"It << is >> a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings on unlabeled corpora .",0
2148,2599,"It << is >> a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2149,2599,"It << is >> a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2150,2599,"It << is >> a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or [[ data pre-processing ]] beyond pre-trained word embeddings on unlabeled corpora .",0
2151,2599,"It << is >> a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2152,2599,"It << is >> a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2153,2599,"It << is >> a truly endto - end model requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2154,2599,"It << is >> a truly endto - end model requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2155,2599,"It << is >> a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] on [[ unlabeled corpora ]] .",0
2156,2599,"It is a truly [[ endto - end model ]] << requiring no >> [[ task - specific resources ]] , feature engineering , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",1
2157,2599,"It is a truly [[ endto - end model ]] << requiring no >> task - specific resources , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",1
2158,2599,"It is a truly [[ endto - end model ]] << requiring no >> task - specific resources , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings on unlabeled corpora .",1
2159,2599,"It is a truly [[ endto - end model ]] << requiring no >> task - specific resources , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2160,2599,"It is a truly [[ endto - end model ]] << requiring no >> task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2161,2599,"It is a truly endto - end model << requiring no >> [[ task - specific resources ]] , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",0
2162,2599,"It is a truly endto - end model << requiring no >> [[ task - specific resources ]] , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings on unlabeled corpora .",0
2163,2599,"It is a truly endto - end model << requiring no >> [[ task - specific resources ]] , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2164,2599,"It is a truly endto - end model << requiring no >> [[ task - specific resources ]] , feature engineering , or data pre-processing beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2165,2599,"It is a truly endto - end model << requiring no >> task - specific resources , [[ feature engineering ]] , or [[ data pre-processing ]] beyond pre-trained word embeddings on unlabeled corpora .",0
2166,2599,"It is a truly endto - end model << requiring no >> task - specific resources , [[ feature engineering ]] , or data pre-processing beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2167,2599,"It is a truly endto - end model << requiring no >> task - specific resources , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2168,2599,"It is a truly endto - end model << requiring no >> task - specific resources , feature engineering , or [[ data pre-processing ]] beyond [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2169,2599,"It is a truly endto - end model << requiring no >> task - specific resources , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2170,2599,"It is a truly endto - end model << requiring no >> task - specific resources , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] on [[ unlabeled corpora ]] .",0
2171,2599,"It is a truly [[ endto - end model ]] requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing << beyond >> pre-trained word embeddings on unlabeled corpora .",0
2172,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing << beyond >> pre-trained word embeddings on unlabeled corpora .",0
2173,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] << beyond >> pre-trained word embeddings on unlabeled corpora .",0
2174,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or data pre-processing << beyond >> [[ pre-trained word embeddings ]] on unlabeled corpora .",1
2175,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or data pre-processing << beyond >> pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2176,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , [[ feature engineering ]] , or data pre-processing << beyond >> pre-trained word embeddings on unlabeled corpora .",0
2177,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or [[ data pre-processing ]] << beyond >> pre-trained word embeddings on unlabeled corpora .",0
2178,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing << beyond >> [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2179,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing << beyond >> pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2180,2599,"It is a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or [[ data pre-processing ]] << beyond >> pre-trained word embeddings on unlabeled corpora .",0
2181,2599,"It is a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing << beyond >> [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2182,2599,"It is a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing << beyond >> pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2183,2599,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] << beyond >> [[ pre-trained word embeddings ]] on unlabeled corpora .",0
2184,2599,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] << beyond >> pre-trained word embeddings on [[ unlabeled corpora ]] .",0
2185,2599,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing << beyond >> [[ pre-trained word embeddings ]] on [[ unlabeled corpora ]] .",0
2186,2599,"It is a truly [[ endto - end model ]] requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing beyond pre-trained word embeddings << on >> unlabeled corpora .",0
2187,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings << on >> unlabeled corpora .",0
2188,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings << on >> unlabeled corpora .",0
2189,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] << on >> unlabeled corpora .",0
2190,2599,"It is a truly [[ endto - end model ]] requiring no task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings << on >> [[ unlabeled corpora ]] .",0
2191,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings << on >> unlabeled corpora .",0
2192,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings << on >> unlabeled corpora .",0
2193,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] << on >> unlabeled corpora .",0
2194,2599,"It is a truly endto - end model requiring no [[ task - specific resources ]] , feature engineering , or data pre-processing beyond pre-trained word embeddings << on >> [[ unlabeled corpora ]] .",0
2195,2599,"It is a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or [[ data pre-processing ]] beyond pre-trained word embeddings << on >> unlabeled corpora .",0
2196,2599,"It is a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing beyond [[ pre-trained word embeddings ]] << on >> unlabeled corpora .",0
2197,2599,"It is a truly endto - end model requiring no task - specific resources , [[ feature engineering ]] , or data pre-processing beyond pre-trained word embeddings << on >> [[ unlabeled corpora ]] .",0
2198,2599,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] beyond [[ pre-trained word embeddings ]] << on >> unlabeled corpora .",0
2199,2599,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or [[ data pre-processing ]] beyond pre-trained word embeddings << on >> [[ unlabeled corpora ]] .",0
2200,2599,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing beyond [[ pre-trained word embeddings ]] << on >> [[ unlabeled corpora ]] .",1
2201,915,"Eventually , we propose a [[ recurrent neural network ( RNN ) based decoder ]] , which << combines >> [[ product - aware review representation and attributes ]] to generate the answer .",1
2202,915,"Eventually , we propose a [[ recurrent neural network ( RNN ) based decoder ]] , which << combines >> product - aware review representation and attributes to generate the [[ answer ]] .",0
2203,915,"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which << combines >> [[ product - aware review representation and attributes ]] to generate the [[ answer ]] .",0
2204,915,"Eventually , we propose a [[ recurrent neural network ( RNN ) based decoder ]] , which combines [[ product - aware review representation and attributes ]] << to generate >> the answer .",0
2205,915,"Eventually , we propose a [[ recurrent neural network ( RNN ) based decoder ]] , which combines product - aware review representation and attributes << to generate >> the [[ answer ]] .",0
2206,915,"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which combines [[ product - aware review representation and attributes ]] << to generate >> the [[ answer ]] .",1
2207,6078,Our approach << first selects >> a [[ selection mask ]] for the [[ source document ]] and then constrains a standard neural model by this mask .,0
2208,6078,Our approach << first selects >> a [[ selection mask ]] for the source document and then [[ constrains a standard neural model ]] by this mask .,0
2209,6078,Our approach << first selects >> a selection mask for the [[ source document ]] and then [[ constrains a standard neural model ]] by this mask .,0
2210,6078,Our approach first selects a [[ selection mask ]] << for >> the [[ source document ]] and then constrains a standard neural model by this mask .,1
2211,6078,Our approach first selects a [[ selection mask ]] << for >> the source document and then [[ constrains a standard neural model ]] by this mask .,0
2212,6078,Our approach first selects a selection mask << for >> the [[ source document ]] and then [[ constrains a standard neural model ]] by this mask .,0
2213,6078,Our approach first selects a [[ selection mask ]] for the [[ source document ]] and then constrains a standard neural model << by this mask >> .,0
2214,6078,Our approach first selects a [[ selection mask ]] for the source document and then [[ constrains a standard neural model ]] << by this mask >> .,1
2215,6078,Our approach first selects a selection mask for the [[ source document ]] and then [[ constrains a standard neural model ]] << by this mask >> .,0
2216,5857,"<< First >> , the [[ sentence encoder ]] reads the [[ input words ]] through an RNN unit to construct the first level sentence representation .",0
2217,5857,"<< First >> , the [[ sentence encoder ]] reads the input words through an [[ RNN unit ]] to construct the first level sentence representation .",0
2218,5857,"<< First >> , the [[ sentence encoder ]] reads the input words through an RNN unit to construct the [[ first level sentence representation ]] .",0
2219,5857,"<< First >> , the sentence encoder reads the [[ input words ]] through an [[ RNN unit ]] to construct the first level sentence representation .",0
2220,5857,"<< First >> , the sentence encoder reads the [[ input words ]] through an RNN unit to construct the [[ first level sentence representation ]] .",0
2221,5857,"<< First >> , the sentence encoder reads the input words through an [[ RNN unit ]] to construct the [[ first level sentence representation ]] .",0
2222,5857,"First , the [[ sentence encoder ]] << reads >> the [[ input words ]] through an RNN unit to construct the first level sentence representation .",1
2223,5857,"First , the [[ sentence encoder ]] << reads >> the input words through an [[ RNN unit ]] to construct the first level sentence representation .",0
2224,5857,"First , the [[ sentence encoder ]] << reads >> the input words through an RNN unit to construct the [[ first level sentence representation ]] .",0
2225,5857,"First , the sentence encoder << reads >> the [[ input words ]] through an [[ RNN unit ]] to construct the first level sentence representation .",0
2226,5857,"First , the sentence encoder << reads >> the [[ input words ]] through an RNN unit to construct the [[ first level sentence representation ]] .",0
2227,5857,"First , the sentence encoder << reads >> the input words through an [[ RNN unit ]] to construct the [[ first level sentence representation ]] .",0
2228,5857,"First , the [[ sentence encoder ]] reads the [[ input words ]] << through >> an RNN unit to construct the first level sentence representation .",0
2229,5857,"First , the [[ sentence encoder ]] reads the input words << through >> an [[ RNN unit ]] to construct the first level sentence representation .",0
2230,5857,"First , the [[ sentence encoder ]] reads the input words << through >> an RNN unit to construct the [[ first level sentence representation ]] .",0
2231,5857,"First , the sentence encoder reads the [[ input words ]] << through >> an [[ RNN unit ]] to construct the first level sentence representation .",1
2232,5857,"First , the sentence encoder reads the [[ input words ]] << through >> an RNN unit to construct the [[ first level sentence representation ]] .",0
2233,5857,"First , the sentence encoder reads the input words << through >> an [[ RNN unit ]] to construct the [[ first level sentence representation ]] .",0
2234,5857,"First , the [[ sentence encoder ]] reads the [[ input words ]] through an RNN unit << to construct >> the first level sentence representation .",0
2235,5857,"First , the [[ sentence encoder ]] reads the input words through an [[ RNN unit ]] << to construct >> the first level sentence representation .",0
2236,5857,"First , the [[ sentence encoder ]] reads the input words through an RNN unit << to construct >> the [[ first level sentence representation ]] .",0
2237,5857,"First , the sentence encoder reads the [[ input words ]] through an [[ RNN unit ]] << to construct >> the first level sentence representation .",0
2238,5857,"First , the sentence encoder reads the [[ input words ]] through an RNN unit << to construct >> the [[ first level sentence representation ]] .",0
2239,5857,"First , the sentence encoder reads the input words through an [[ RNN unit ]] << to construct >> the [[ first level sentence representation ]] .",1
2240,2335,"1 ) the [[ recurrent network encoder ]] << to build >> [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",1
2241,2335,"1 ) the [[ recurrent network encoder ]] << to build >> representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2242,2335,"1 ) the [[ recurrent network encoder ]] << to build >> representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2243,2335,"1 ) the [[ recurrent network encoder ]] << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2244,2335,"1 ) the [[ recurrent network encoder ]] << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2245,2335,"1 ) the [[ recurrent network encoder ]] << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2246,2335,"1 ) the [[ recurrent network encoder ]] << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2247,2335,"1 ) the [[ recurrent network encoder ]] << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2248,2335,"1 ) the recurrent network encoder << to build >> [[ representation ]] for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2249,2335,"1 ) the recurrent network encoder << to build >> [[ representation ]] for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2250,2335,"1 ) the recurrent network encoder << to build >> [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2251,2335,"1 ) the recurrent network encoder << to build >> [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2252,2335,"1 ) the recurrent network encoder << to build >> [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2253,2335,"1 ) the recurrent network encoder << to build >> [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2254,2335,"1 ) the recurrent network encoder << to build >> [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2255,2335,"1 ) the recurrent network encoder << to build >> representation for [[ questions and passages ]] separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2256,2335,"1 ) the recurrent network encoder << to build >> representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2257,2335,"1 ) the recurrent network encoder << to build >> representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2258,2335,"1 ) the recurrent network encoder << to build >> representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2259,2335,"1 ) the recurrent network encoder << to build >> representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2260,2335,"1 ) the recurrent network encoder << to build >> representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2261,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2262,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2263,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2264,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2265,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2266,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2267,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2268,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2269,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2270,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2271,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2272,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2273,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2274,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2275,2335,"1 ) the recurrent network encoder << to build >> representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2276,2335,"1 ) the [[ recurrent network encoder ]] to build [[ representation ]] << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2277,2335,"1 ) the [[ recurrent network encoder ]] to build representation << for >> [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2278,2335,"1 ) the [[ recurrent network encoder ]] to build representation << for >> questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2279,2335,"1 ) the [[ recurrent network encoder ]] to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2280,2335,"1 ) the [[ recurrent network encoder ]] to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2281,2335,"1 ) the [[ recurrent network encoder ]] to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2282,2335,"1 ) the [[ recurrent network encoder ]] to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2283,2335,"1 ) the [[ recurrent network encoder ]] to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2284,2335,"1 ) the recurrent network encoder to build [[ representation ]] << for >> [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",1
2285,2335,"1 ) the recurrent network encoder to build [[ representation ]] << for >> questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2286,2335,"1 ) the recurrent network encoder to build [[ representation ]] << for >> questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2287,2335,"1 ) the recurrent network encoder to build [[ representation ]] << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2288,2335,"1 ) the recurrent network encoder to build [[ representation ]] << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2289,2335,"1 ) the recurrent network encoder to build [[ representation ]] << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2290,2335,"1 ) the recurrent network encoder to build [[ representation ]] << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2291,2335,"1 ) the recurrent network encoder to build representation << for >> [[ questions and passages ]] separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2292,2335,"1 ) the recurrent network encoder to build representation << for >> [[ questions and passages ]] separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2293,2335,"1 ) the recurrent network encoder to build representation << for >> [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2294,2335,"1 ) the recurrent network encoder to build representation << for >> [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2295,2335,"1 ) the recurrent network encoder to build representation << for >> [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2296,2335,"1 ) the recurrent network encoder to build representation << for >> [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2297,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the [[ gated matching layer ]] to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2298,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2299,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2300,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2301,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2302,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2303,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2304,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2305,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2306,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2307,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2308,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2309,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2310,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2311,2335,"1 ) the recurrent network encoder to build representation << for >> questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2312,2335,"1 ) the [[ recurrent network encoder ]] to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2313,2335,"1 ) the [[ recurrent network encoder ]] to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2314,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2315,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2316,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2317,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2318,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2319,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2320,2335,"1 ) the recurrent network encoder to build [[ representation ]] for [[ questions and passages ]] separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2321,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the [[ gated matching layer ]] << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2322,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer << to match >> the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2323,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2324,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2325,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2326,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2327,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the [[ gated matching layer ]] << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2328,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer << to match >> the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2329,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2330,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2331,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2332,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2333,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] << to match >> the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",1
2334,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] << to match >> the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2335,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] << to match >> the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2336,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2337,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2338,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the [[ question and passage ]] , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2339,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the [[ question and passage ]] , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2340,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2341,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2342,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the [[ self - matching layer ]] to aggregate [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2343,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2344,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2345,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2346,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2347,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer << to match >> the question and passage , 3 ) the self - matching layer to aggregate information from the [[ whole passage ]] , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2348,2335,"1 ) the [[ recurrent network encoder ]] to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2349,2335,"1 ) the [[ recurrent network encoder ]] to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2350,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2351,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2352,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2353,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2354,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2355,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2356,2335,"1 ) the recurrent network encoder to build [[ representation ]] for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2357,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2358,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2359,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2360,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2361,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2362,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2363,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2364,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2365,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2366,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2367,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2368,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2369,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the [[ question and passage ]] , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2370,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the [[ self - matching layer ]] << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2371,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer << to aggregate >> [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2372,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2373,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2374,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the [[ self - matching layer ]] << to aggregate >> information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2375,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer << to aggregate >> [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2376,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer << to aggregate >> information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2377,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer << to aggregate >> information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2378,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] << to aggregate >> [[ information ]] from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",1
2379,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] << to aggregate >> information from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2380,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] << to aggregate >> information from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2381,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> [[ information ]] from the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2382,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> [[ information ]] from the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2383,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer << to aggregate >> information from the [[ whole passage ]] , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2384,2335,"1 ) the [[ recurrent network encoder ]] to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2385,2335,"1 ) the [[ recurrent network encoder ]] to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2386,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2387,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2388,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2389,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2390,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2391,2335,"1 ) the [[ recurrent network encoder ]] to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2392,2335,"1 ) the recurrent network encoder to build [[ representation ]] for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2393,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2394,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2395,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2396,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2397,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2398,2335,"1 ) the recurrent network encoder to build [[ representation ]] for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2399,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2400,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2401,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2402,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2403,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2404,2335,"1 ) the recurrent network encoder to build representation for [[ questions and passages ]] separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2405,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2406,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2407,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2408,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2409,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the [[ gated matching layer ]] to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2410,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the [[ self - matching layer ]] to aggregate information << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2411,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate [[ information ]] << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2412,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information << from >> the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2413,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the [[ question and passage ]] , 3 ) the self - matching layer to aggregate information << from >> the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2414,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate [[ information ]] << from >> the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2415,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information << from >> the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",0
2416,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the [[ self - matching layer ]] to aggregate information << from >> the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2417,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] << from >> the [[ whole passage ]] , and 4 ) the pointernetwork based answer boundary prediction layer .",1
2418,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate [[ information ]] << from >> the whole passage , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2419,2335,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information << from >> the [[ whole passage ]] , and 4 ) the [[ pointernetwork based answer boundary prediction layer ]] .",0
2420,3820,"Firstly , the [[ decomposition ]] << disentangles >> [[ high - level from low - level semantic information ]] , which enables the decoders to model meaning at different levels of granularity .",1
2421,3820,"Firstly , the [[ decomposition ]] << disentangles >> high - level from low - level semantic information , which enables the [[ decoders ]] to model meaning at different levels of granularity .",0
2422,3820,"Firstly , the [[ decomposition ]] << disentangles >> high - level from low - level semantic information , which enables the decoders to model [[ meaning ]] at different levels of granularity .",0
2423,3820,"Firstly , the [[ decomposition ]] << disentangles >> high - level from low - level semantic information , which enables the decoders to model meaning at [[ different levels of granularity ]] .",0
2424,3820,"Firstly , the decomposition << disentangles >> [[ high - level from low - level semantic information ]] , which enables the [[ decoders ]] to model meaning at different levels of granularity .",0
2425,3820,"Firstly , the decomposition << disentangles >> [[ high - level from low - level semantic information ]] , which enables the decoders to model [[ meaning ]] at different levels of granularity .",0
2426,3820,"Firstly , the decomposition << disentangles >> [[ high - level from low - level semantic information ]] , which enables the decoders to model meaning at [[ different levels of granularity ]] .",0
2427,3820,"Firstly , the decomposition << disentangles >> high - level from low - level semantic information , which enables the [[ decoders ]] to model [[ meaning ]] at different levels of granularity .",0
2428,3820,"Firstly , the decomposition << disentangles >> high - level from low - level semantic information , which enables the [[ decoders ]] to model meaning at [[ different levels of granularity ]] .",0
2429,3820,"Firstly , the decomposition << disentangles >> high - level from low - level semantic information , which enables the decoders to model [[ meaning ]] at [[ different levels of granularity ]] .",0
2430,3820,"Firstly , the [[ decomposition ]] disentangles [[ high - level from low - level semantic information ]] , which << enables >> the decoders to model meaning at different levels of granularity .",0
2431,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which << enables >> the [[ decoders ]] to model meaning at different levels of granularity .",0
2432,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which << enables >> the decoders to model [[ meaning ]] at different levels of granularity .",0
2433,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which << enables >> the decoders to model meaning at [[ different levels of granularity ]] .",0
2434,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which << enables >> the [[ decoders ]] to model meaning at different levels of granularity .",1
2435,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which << enables >> the decoders to model [[ meaning ]] at different levels of granularity .",0
2436,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which << enables >> the decoders to model meaning at [[ different levels of granularity ]] .",0
2437,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which << enables >> the [[ decoders ]] to model [[ meaning ]] at different levels of granularity .",0
2438,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which << enables >> the [[ decoders ]] to model meaning at [[ different levels of granularity ]] .",0
2439,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which << enables >> the decoders to model [[ meaning ]] at [[ different levels of granularity ]] .",0
2440,3820,"Firstly , the [[ decomposition ]] disentangles [[ high - level from low - level semantic information ]] , which enables the decoders << to model >> meaning at different levels of granularity .",0
2441,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which enables the [[ decoders ]] << to model >> meaning at different levels of granularity .",0
2442,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which enables the decoders << to model >> [[ meaning ]] at different levels of granularity .",0
2443,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which enables the decoders << to model >> meaning at [[ different levels of granularity ]] .",0
2444,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which enables the [[ decoders ]] << to model >> meaning at different levels of granularity .",0
2445,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which enables the decoders << to model >> [[ meaning ]] at different levels of granularity .",0
2446,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which enables the decoders << to model >> meaning at [[ different levels of granularity ]] .",0
2447,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the [[ decoders ]] << to model >> [[ meaning ]] at different levels of granularity .",1
2448,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the [[ decoders ]] << to model >> meaning at [[ different levels of granularity ]] .",0
2449,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders << to model >> [[ meaning ]] at [[ different levels of granularity ]] .",0
2450,3820,"Firstly , the [[ decomposition ]] disentangles [[ high - level from low - level semantic information ]] , which enables the decoders to model meaning << at >> different levels of granularity .",0
2451,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which enables the [[ decoders ]] to model meaning << at >> different levels of granularity .",0
2452,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which enables the decoders to model [[ meaning ]] << at >> different levels of granularity .",0
2453,3820,"Firstly , the [[ decomposition ]] disentangles high - level from low - level semantic information , which enables the decoders to model meaning << at >> [[ different levels of granularity ]] .",0
2454,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which enables the [[ decoders ]] to model meaning << at >> different levels of granularity .",0
2455,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which enables the decoders to model [[ meaning ]] << at >> different levels of granularity .",0
2456,3820,"Firstly , the decomposition disentangles [[ high - level from low - level semantic information ]] , which enables the decoders to model meaning << at >> [[ different levels of granularity ]] .",0
2457,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the [[ decoders ]] to model [[ meaning ]] << at >> different levels of granularity .",0
2458,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the [[ decoders ]] to model meaning << at >> [[ different levels of granularity ]] .",0
2459,3820,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model [[ meaning ]] << at >> [[ different levels of granularity ]] .",1
2460,956,"<< Compared with >> [[ attention LSTMs ]] , [[ our two models ]] achieve comparable results to them using much fewer parameters ( nearly 1 / 5 ) .",0
2461,956,"<< Compared with >> [[ attention LSTMs ]] , our two models achieve [[ comparable results ]] to them using much fewer parameters ( nearly 1 / 5 ) .",0
2462,956,"<< Compared with >> [[ attention LSTMs ]] , our two models achieve comparable results to them using [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2463,956,"<< Compared with >> attention LSTMs , [[ our two models ]] achieve [[ comparable results ]] to them using much fewer parameters ( nearly 1 / 5 ) .",0
2464,956,"<< Compared with >> attention LSTMs , [[ our two models ]] achieve comparable results to them using [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2465,956,"<< Compared with >> attention LSTMs , our two models achieve [[ comparable results ]] to them using [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2466,956,"Compared with [[ attention LSTMs ]] , [[ our two models ]] << achieve >> comparable results to them using much fewer parameters ( nearly 1 / 5 ) .",0
2467,956,"Compared with [[ attention LSTMs ]] , our two models << achieve >> [[ comparable results ]] to them using much fewer parameters ( nearly 1 / 5 ) .",0
2468,956,"Compared with [[ attention LSTMs ]] , our two models << achieve >> comparable results to them using [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2469,956,"Compared with attention LSTMs , [[ our two models ]] << achieve >> [[ comparable results ]] to them using much fewer parameters ( nearly 1 / 5 ) .",1
2470,956,"Compared with attention LSTMs , [[ our two models ]] << achieve >> comparable results to them using [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2471,956,"Compared with attention LSTMs , our two models << achieve >> [[ comparable results ]] to them using [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2472,956,"Compared with [[ attention LSTMs ]] , [[ our two models ]] achieve comparable results to them << using >> much fewer parameters ( nearly 1 / 5 ) .",0
2473,956,"Compared with [[ attention LSTMs ]] , our two models achieve [[ comparable results ]] to them << using >> much fewer parameters ( nearly 1 / 5 ) .",0
2474,956,"Compared with [[ attention LSTMs ]] , our two models achieve comparable results to them << using >> [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2475,956,"Compared with attention LSTMs , [[ our two models ]] achieve [[ comparable results ]] to them << using >> much fewer parameters ( nearly 1 / 5 ) .",0
2476,956,"Compared with attention LSTMs , [[ our two models ]] achieve comparable results to them << using >> [[ much fewer parameters ( nearly 1 / 5 ) ]] .",1
2477,956,"Compared with attention LSTMs , our two models achieve [[ comparable results ]] to them << using >> [[ much fewer parameters ( nearly 1 / 5 ) ]] .",0
2478,6119,"2 ) [[ SMM ]] : Our second baseline system << is >> [[ non-Bayesian SMM with 1 regularization over the rows in T matrix ]] , i.e. , 1 SMM .",1
2479,3717,"Following , we << tune >> all of the [[ models ]] using [[ three - fold validation ]] on the training set .",0
2480,3717,"Following , we << tune >> all of the [[ models ]] using three - fold validation on the [[ training set ]] .",0
2481,3717,"Following , we << tune >> all of the models using [[ three - fold validation ]] on the [[ training set ]] .",0
2482,3717,"Following , we tune all of the [[ models ]] << using >> [[ three - fold validation ]] on the training set .",1
2483,3717,"Following , we tune all of the [[ models ]] << using >> three - fold validation on the [[ training set ]] .",0
2484,3717,"Following , we tune all of the models << using >> [[ three - fold validation ]] on the [[ training set ]] .",0
2485,3717,"Following , we tune all of the [[ models ]] using [[ three - fold validation ]] << on >> the training set .",0
2486,3717,"Following , we tune all of the [[ models ]] using three - fold validation << on >> the [[ training set ]] .",0
2487,3717,"Following , we tune all of the models using [[ three - fold validation ]] << on >> the [[ training set ]] .",1
2488,4027,"In the experiments , [[ our model ]] was << trained using >> the [[ Adam algorithm ]] with a learning rate initialized at 0.001 .",1
2489,4027,"In the experiments , [[ our model ]] was << trained using >> the Adam algorithm with a [[ learning rate ]] initialized at 0.001 .",0
2490,4027,"In the experiments , [[ our model ]] was << trained using >> the Adam algorithm with a learning rate initialized at [[ 0.001 ]] .",0
2491,4027,"In the experiments , our model was << trained using >> the [[ Adam algorithm ]] with a [[ learning rate ]] initialized at 0.001 .",0
2492,4027,"In the experiments , our model was << trained using >> the [[ Adam algorithm ]] with a learning rate initialized at [[ 0.001 ]] .",0
2493,4027,"In the experiments , our model was << trained using >> the Adam algorithm with a [[ learning rate ]] initialized at [[ 0.001 ]] .",0
2494,4027,"In the experiments , [[ our model ]] was trained using the [[ Adam algorithm ]] << with >> a learning rate initialized at 0.001 .",0
2495,4027,"In the experiments , [[ our model ]] was trained using the Adam algorithm << with >> a [[ learning rate ]] initialized at 0.001 .",0
2496,4027,"In the experiments , [[ our model ]] was trained using the Adam algorithm << with >> a learning rate initialized at [[ 0.001 ]] .",0
2497,4027,"In the experiments , our model was trained using the [[ Adam algorithm ]] << with >> a [[ learning rate ]] initialized at 0.001 .",1
2498,4027,"In the experiments , our model was trained using the [[ Adam algorithm ]] << with >> a learning rate initialized at [[ 0.001 ]] .",0
2499,4027,"In the experiments , our model was trained using the Adam algorithm << with >> a [[ learning rate ]] initialized at [[ 0.001 ]] .",0
2500,4027,"In the experiments , [[ our model ]] was trained using the [[ Adam algorithm ]] with a learning rate << initialized at >> 0.001 .",0
2501,4027,"In the experiments , [[ our model ]] was trained using the Adam algorithm with a [[ learning rate ]] << initialized at >> 0.001 .",0
2502,4027,"In the experiments , [[ our model ]] was trained using the Adam algorithm with a learning rate << initialized at >> [[ 0.001 ]] .",0
2503,4027,"In the experiments , our model was trained using the [[ Adam algorithm ]] with a [[ learning rate ]] << initialized at >> 0.001 .",0
2504,4027,"In the experiments , our model was trained using the [[ Adam algorithm ]] with a learning rate << initialized at >> [[ 0.001 ]] .",0
2505,4027,"In the experiments , our model was trained using the Adam algorithm with a [[ learning rate ]] << initialized at >> [[ 0.001 ]] .",1
2506,1182,"[[ MAGE - RNN ]] << learns >> [[ separate representations ]] for propagation along each edge type , which leads to superior performance empirically .",1
2507,1182,"[[ MAGE - RNN ]] << learns >> separate representations for [[ propagation ]] along each edge type , which leads to superior performance empirically .",0
2508,1182,"[[ MAGE - RNN ]] << learns >> separate representations for propagation along [[ each edge type ]] , which leads to superior performance empirically .",0
2509,1182,"[[ MAGE - RNN ]] << learns >> separate representations for propagation along each edge type , which leads to [[ superior performance empirically ]] .",0
2510,1182,"MAGE - RNN << learns >> [[ separate representations ]] for [[ propagation ]] along each edge type , which leads to superior performance empirically .",0
2511,1182,"MAGE - RNN << learns >> [[ separate representations ]] for propagation along [[ each edge type ]] , which leads to superior performance empirically .",0
2512,1182,"MAGE - RNN << learns >> [[ separate representations ]] for propagation along each edge type , which leads to [[ superior performance empirically ]] .",0
2513,1182,"MAGE - RNN << learns >> separate representations for [[ propagation ]] along [[ each edge type ]] , which leads to superior performance empirically .",0
2514,1182,"MAGE - RNN << learns >> separate representations for [[ propagation ]] along each edge type , which leads to [[ superior performance empirically ]] .",0
2515,1182,"MAGE - RNN << learns >> separate representations for propagation along [[ each edge type ]] , which leads to [[ superior performance empirically ]] .",0
2516,1182,"[[ MAGE - RNN ]] learns [[ separate representations ]] << for >> propagation along each edge type , which leads to superior performance empirically .",0
2517,1182,"[[ MAGE - RNN ]] learns separate representations << for >> [[ propagation ]] along each edge type , which leads to superior performance empirically .",0
2518,1182,"[[ MAGE - RNN ]] learns separate representations << for >> propagation along [[ each edge type ]] , which leads to superior performance empirically .",0
2519,1182,"[[ MAGE - RNN ]] learns separate representations << for >> propagation along each edge type , which leads to [[ superior performance empirically ]] .",0
2520,1182,"MAGE - RNN learns [[ separate representations ]] << for >> [[ propagation ]] along each edge type , which leads to superior performance empirically .",1
2521,1182,"MAGE - RNN learns [[ separate representations ]] << for >> propagation along [[ each edge type ]] , which leads to superior performance empirically .",0
2522,1182,"MAGE - RNN learns [[ separate representations ]] << for >> propagation along each edge type , which leads to [[ superior performance empirically ]] .",0
2523,1182,"MAGE - RNN learns separate representations << for >> [[ propagation ]] along [[ each edge type ]] , which leads to superior performance empirically .",0
2524,1182,"MAGE - RNN learns separate representations << for >> [[ propagation ]] along each edge type , which leads to [[ superior performance empirically ]] .",0
2525,1182,"MAGE - RNN learns separate representations << for >> propagation along [[ each edge type ]] , which leads to [[ superior performance empirically ]] .",0
2526,1182,"[[ MAGE - RNN ]] learns [[ separate representations ]] for propagation << along >> each edge type , which leads to superior performance empirically .",0
2527,1182,"[[ MAGE - RNN ]] learns separate representations for [[ propagation ]] << along >> each edge type , which leads to superior performance empirically .",0
2528,1182,"[[ MAGE - RNN ]] learns separate representations for propagation << along >> [[ each edge type ]] , which leads to superior performance empirically .",0
2529,1182,"[[ MAGE - RNN ]] learns separate representations for propagation << along >> each edge type , which leads to [[ superior performance empirically ]] .",0
2530,1182,"MAGE - RNN learns [[ separate representations ]] for [[ propagation ]] << along >> each edge type , which leads to superior performance empirically .",0
2531,1182,"MAGE - RNN learns [[ separate representations ]] for propagation << along >> [[ each edge type ]] , which leads to superior performance empirically .",0
2532,1182,"MAGE - RNN learns [[ separate representations ]] for propagation << along >> each edge type , which leads to [[ superior performance empirically ]] .",0
2533,1182,"MAGE - RNN learns separate representations for [[ propagation ]] << along >> [[ each edge type ]] , which leads to superior performance empirically .",1
2534,1182,"MAGE - RNN learns separate representations for [[ propagation ]] << along >> each edge type , which leads to [[ superior performance empirically ]] .",0
2535,1182,"MAGE - RNN learns separate representations for propagation << along >> [[ each edge type ]] , which leads to [[ superior performance empirically ]] .",0
2536,1182,"[[ MAGE - RNN ]] learns [[ separate representations ]] for propagation along each edge type , which << leads to >> superior performance empirically .",0
2537,1182,"[[ MAGE - RNN ]] learns separate representations for [[ propagation ]] along each edge type , which << leads to >> superior performance empirically .",0
2538,1182,"[[ MAGE - RNN ]] learns separate representations for propagation along [[ each edge type ]] , which << leads to >> superior performance empirically .",0
2539,1182,"[[ MAGE - RNN ]] learns separate representations for propagation along each edge type , which << leads to >> [[ superior performance empirically ]] .",0
2540,1182,"MAGE - RNN learns [[ separate representations ]] for [[ propagation ]] along each edge type , which << leads to >> superior performance empirically .",0
2541,1182,"MAGE - RNN learns [[ separate representations ]] for propagation along [[ each edge type ]] , which << leads to >> superior performance empirically .",0
2542,1182,"MAGE - RNN learns [[ separate representations ]] for propagation along each edge type , which << leads to >> [[ superior performance empirically ]] .",1
2543,1182,"MAGE - RNN learns separate representations for [[ propagation ]] along [[ each edge type ]] , which << leads to >> superior performance empirically .",0
2544,1182,"MAGE - RNN learns separate representations for [[ propagation ]] along each edge type , which << leads to >> [[ superior performance empirically ]] .",0
2545,1182,"MAGE - RNN learns separate representations for propagation along [[ each edge type ]] , which << leads to >> [[ superior performance empirically ]] .",0
2546,4602,"[[ Rec - NN ]] firstly << uses >> [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",1
2547,4602,"[[ Rec - NN ]] firstly << uses >> rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2548,4602,"[[ Rec - NN ]] firstly << uses >> rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2549,4602,"[[ Rec - NN ]] firstly << uses >> rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2550,4602,"[[ Rec - NN ]] firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2551,4602,"[[ Rec - NN ]] firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2552,4602,"[[ Rec - NN ]] firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2553,4602,"[[ Rec - NN ]] firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2554,4602,"Rec - NN firstly << uses >> [[ rules ]] to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2555,4602,"Rec - NN firstly << uses >> [[ rules ]] to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2556,4602,"Rec - NN firstly << uses >> [[ rules ]] to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2557,4602,"Rec - NN firstly << uses >> [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2558,4602,"Rec - NN firstly << uses >> [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2559,4602,"Rec - NN firstly << uses >> [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2560,4602,"Rec - NN firstly << uses >> [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2561,4602,"Rec - NN firstly << uses >> rules to transform the [[ dependency tree ]] and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2562,4602,"Rec - NN firstly << uses >> rules to transform the [[ dependency tree ]] and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2563,4602,"Rec - NN firstly << uses >> rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2564,4602,"Rec - NN firstly << uses >> rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2565,4602,"Rec - NN firstly << uses >> rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2566,4602,"Rec - NN firstly << uses >> rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2567,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the [[ opinion target ]] at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2568,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2569,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2570,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2571,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2572,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2573,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2574,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2575,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2576,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward [[ target ]] via semantic composition using Recursive NNs .",0
2577,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via [[ semantic composition ]] using Recursive NNs .",0
2578,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using [[ Recursive NNs ]] .",0
2579,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via [[ semantic composition ]] using Recursive NNs .",0
2580,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using [[ Recursive NNs ]] .",0
2581,4602,"Rec - NN firstly << uses >> rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using [[ Recursive NNs ]] .",0
2582,4602,"[[ Rec - NN ]] firstly uses [[ rules ]] << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2583,4602,"[[ Rec - NN ]] firstly uses rules << to transform >> the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2584,4602,"[[ Rec - NN ]] firstly uses rules << to transform >> the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2585,4602,"[[ Rec - NN ]] firstly uses rules << to transform >> the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2586,4602,"[[ Rec - NN ]] firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2587,4602,"[[ Rec - NN ]] firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2588,4602,"[[ Rec - NN ]] firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2589,4602,"[[ Rec - NN ]] firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2590,4602,"Rec - NN firstly uses [[ rules ]] << to transform >> the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",1
2591,4602,"Rec - NN firstly uses [[ rules ]] << to transform >> the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2592,4602,"Rec - NN firstly uses [[ rules ]] << to transform >> the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2593,4602,"Rec - NN firstly uses [[ rules ]] << to transform >> the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2594,4602,"Rec - NN firstly uses [[ rules ]] << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2595,4602,"Rec - NN firstly uses [[ rules ]] << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2596,4602,"Rec - NN firstly uses [[ rules ]] << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2597,4602,"Rec - NN firstly uses rules << to transform >> the [[ dependency tree ]] and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2598,4602,"Rec - NN firstly uses rules << to transform >> the [[ dependency tree ]] and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2599,4602,"Rec - NN firstly uses rules << to transform >> the [[ dependency tree ]] and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2600,4602,"Rec - NN firstly uses rules << to transform >> the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2601,4602,"Rec - NN firstly uses rules << to transform >> the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2602,4602,"Rec - NN firstly uses rules << to transform >> the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2603,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the [[ opinion target ]] at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2604,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the [[ opinion target ]] at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2605,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2606,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2607,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2608,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the [[ root ]] , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2609,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2610,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2611,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2612,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward [[ target ]] via semantic composition using Recursive NNs .",0
2613,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via [[ semantic composition ]] using Recursive NNs .",0
2614,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using [[ Recursive NNs ]] .",0
2615,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via [[ semantic composition ]] using Recursive NNs .",0
2616,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using [[ Recursive NNs ]] .",0
2617,4602,"Rec - NN firstly uses rules << to transform >> the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using [[ Recursive NNs ]] .",0
2618,4602,"[[ Rec - NN ]] firstly uses [[ rules ]] to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2619,4602,"[[ Rec - NN ]] firstly uses rules to transform the [[ dependency tree ]] and << put >> the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2620,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and << put >> the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",1
2621,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and << put >> the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2622,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2623,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2624,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2625,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2626,4602,"Rec - NN firstly uses [[ rules ]] to transform the [[ dependency tree ]] and << put >> the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2627,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and << put >> the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2628,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and << put >> the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2629,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and << put >> the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2630,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2631,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2632,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2633,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and << put >> the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2634,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and << put >> the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2635,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and << put >> the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2636,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and << put >> the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2637,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and << put >> the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2638,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and << put >> the opinion target at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2639,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the [[ opinion target ]] at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2640,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the [[ opinion target ]] at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2641,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the [[ opinion target ]] at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2642,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the [[ opinion target ]] at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2643,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2644,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the [[ root ]] , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2645,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the [[ root ]] , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2646,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the [[ root ]] , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2647,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2648,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the [[ sentence representation ]] toward [[ target ]] via semantic composition using Recursive NNs .",0
2649,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the [[ sentence representation ]] toward target via [[ semantic composition ]] using Recursive NNs .",0
2650,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition using [[ Recursive NNs ]] .",0
2651,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward [[ target ]] via [[ semantic composition ]] using Recursive NNs .",0
2652,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition using [[ Recursive NNs ]] .",0
2653,4602,"Rec - NN firstly uses rules to transform the dependency tree and << put >> the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] using [[ Recursive NNs ]] .",0
2654,4602,"[[ Rec - NN ]] firstly uses [[ rules ]] to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2655,4602,"[[ Rec - NN ]] firstly uses rules to transform the [[ dependency tree ]] and put the opinion target << at >> the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2656,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the [[ opinion target ]] << at >> the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2657,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target << at >> the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2658,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2659,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2660,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2661,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2662,4602,"Rec - NN firstly uses [[ rules ]] to transform the [[ dependency tree ]] and put the opinion target << at >> the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2663,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the [[ opinion target ]] << at >> the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2664,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target << at >> the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2665,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target << at >> the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2666,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2667,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2668,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2669,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the [[ opinion target ]] << at >> the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2670,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target << at >> the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",0
2671,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target << at >> the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2672,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target << at >> the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2673,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target << at >> the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2674,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target << at >> the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2675,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] << at >> the [[ root ]] , and then learns the sentence representation toward target via semantic composition using Recursive NNs .",1
2676,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] << at >> the root , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2677,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] << at >> the root , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2678,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] << at >> the root , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2679,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] << at >> the root , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2680,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the [[ root ]] , and then learns the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2681,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the [[ root ]] , and then learns the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2682,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the [[ root ]] , and then learns the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2683,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the [[ root ]] , and then learns the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2684,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the [[ sentence representation ]] toward [[ target ]] via semantic composition using Recursive NNs .",0
2685,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the [[ sentence representation ]] toward target via [[ semantic composition ]] using Recursive NNs .",0
2686,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the [[ sentence representation ]] toward target via semantic composition using [[ Recursive NNs ]] .",0
2687,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward [[ target ]] via [[ semantic composition ]] using Recursive NNs .",0
2688,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward [[ target ]] via semantic composition using [[ Recursive NNs ]] .",0
2689,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target << at >> the root , and then learns the sentence representation toward target via [[ semantic composition ]] using [[ Recursive NNs ]] .",0
2690,4602,"[[ Rec - NN ]] firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2691,4602,"[[ Rec - NN ]] firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2692,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2693,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2694,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",1
2695,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2696,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2697,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2698,4602,"Rec - NN firstly uses [[ rules ]] to transform the [[ dependency tree ]] and put the opinion target at the root , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2699,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the [[ opinion target ]] at the root , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2700,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the [[ root ]] , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2701,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then << learns >> the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2702,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2703,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2704,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2705,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the [[ opinion target ]] at the root , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2706,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the [[ root ]] , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2707,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then << learns >> the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2708,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then << learns >> the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2709,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then << learns >> the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2710,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then << learns >> the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2711,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the [[ root ]] , and then << learns >> the sentence representation toward target via semantic composition using Recursive NNs .",0
2712,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then << learns >> the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2713,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then << learns >> the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2714,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then << learns >> the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2715,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then << learns >> the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2716,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then << learns >> the [[ sentence representation ]] toward target via semantic composition using Recursive NNs .",0
2717,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then << learns >> the sentence representation toward [[ target ]] via semantic composition using Recursive NNs .",0
2718,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then << learns >> the sentence representation toward target via [[ semantic composition ]] using Recursive NNs .",0
2719,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then << learns >> the sentence representation toward target via semantic composition using [[ Recursive NNs ]] .",0
2720,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the [[ sentence representation ]] toward [[ target ]] via semantic composition using Recursive NNs .",0
2721,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the [[ sentence representation ]] toward target via [[ semantic composition ]] using Recursive NNs .",0
2722,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the [[ sentence representation ]] toward target via semantic composition using [[ Recursive NNs ]] .",0
2723,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward [[ target ]] via [[ semantic composition ]] using Recursive NNs .",0
2724,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward [[ target ]] via semantic composition using [[ Recursive NNs ]] .",0
2725,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then << learns >> the sentence representation toward target via [[ semantic composition ]] using [[ Recursive NNs ]] .",0
2726,4602,"[[ Rec - NN ]] firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2727,4602,"[[ Rec - NN ]] firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2728,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2729,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2730,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] << toward >> target via semantic composition using Recursive NNs .",0
2731,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> [[ target ]] via semantic composition using Recursive NNs .",0
2732,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> target via [[ semantic composition ]] using Recursive NNs .",0
2733,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> target via semantic composition using [[ Recursive NNs ]] .",0
2734,4602,"Rec - NN firstly uses [[ rules ]] to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2735,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2736,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2737,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] << toward >> target via semantic composition using Recursive NNs .",0
2738,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> [[ target ]] via semantic composition using Recursive NNs .",0
2739,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> target via [[ semantic composition ]] using Recursive NNs .",0
2740,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> target via semantic composition using [[ Recursive NNs ]] .",0
2741,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the [[ opinion target ]] at the root , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2742,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the [[ root ]] , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2743,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the [[ sentence representation ]] << toward >> target via semantic composition using Recursive NNs .",0
2744,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation << toward >> [[ target ]] via semantic composition using Recursive NNs .",0
2745,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation << toward >> target via [[ semantic composition ]] using Recursive NNs .",0
2746,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation << toward >> target via semantic composition using [[ Recursive NNs ]] .",0
2747,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the [[ root ]] , and then learns the sentence representation << toward >> target via semantic composition using Recursive NNs .",0
2748,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the [[ sentence representation ]] << toward >> target via semantic composition using Recursive NNs .",0
2749,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation << toward >> [[ target ]] via semantic composition using Recursive NNs .",0
2750,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation << toward >> target via [[ semantic composition ]] using Recursive NNs .",0
2751,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation << toward >> target via semantic composition using [[ Recursive NNs ]] .",0
2752,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the [[ sentence representation ]] << toward >> target via semantic composition using Recursive NNs .",0
2753,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation << toward >> [[ target ]] via semantic composition using Recursive NNs .",0
2754,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation << toward >> target via [[ semantic composition ]] using Recursive NNs .",0
2755,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation << toward >> target via semantic composition using [[ Recursive NNs ]] .",0
2756,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] << toward >> [[ target ]] via semantic composition using Recursive NNs .",1
2757,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] << toward >> target via [[ semantic composition ]] using Recursive NNs .",0
2758,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] << toward >> target via semantic composition using [[ Recursive NNs ]] .",0
2759,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> [[ target ]] via [[ semantic composition ]] using Recursive NNs .",0
2760,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> [[ target ]] via semantic composition using [[ Recursive NNs ]] .",0
2761,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation << toward >> target via [[ semantic composition ]] using [[ Recursive NNs ]] .",0
2762,4602,"[[ Rec - NN ]] firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2763,4602,"[[ Rec - NN ]] firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2764,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2765,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2766,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target << via >> semantic composition using Recursive NNs .",0
2767,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] << via >> semantic composition using Recursive NNs .",0
2768,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target << via >> [[ semantic composition ]] using Recursive NNs .",0
2769,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target << via >> semantic composition using [[ Recursive NNs ]] .",0
2770,4602,"Rec - NN firstly uses [[ rules ]] to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2771,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2772,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2773,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target << via >> semantic composition using Recursive NNs .",0
2774,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] << via >> semantic composition using Recursive NNs .",0
2775,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target << via >> [[ semantic composition ]] using Recursive NNs .",0
2776,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target << via >> semantic composition using [[ Recursive NNs ]] .",0
2777,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2778,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2779,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target << via >> semantic composition using Recursive NNs .",0
2780,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] << via >> semantic composition using Recursive NNs .",0
2781,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target << via >> [[ semantic composition ]] using Recursive NNs .",0
2782,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target << via >> semantic composition using [[ Recursive NNs ]] .",0
2783,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the [[ root ]] , and then learns the sentence representation toward target << via >> semantic composition using Recursive NNs .",0
2784,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the [[ sentence representation ]] toward target << via >> semantic composition using Recursive NNs .",0
2785,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward [[ target ]] << via >> semantic composition using Recursive NNs .",0
2786,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target << via >> [[ semantic composition ]] using Recursive NNs .",0
2787,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target << via >> semantic composition using [[ Recursive NNs ]] .",0
2788,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the [[ sentence representation ]] toward target << via >> semantic composition using Recursive NNs .",0
2789,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward [[ target ]] << via >> semantic composition using Recursive NNs .",0
2790,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target << via >> [[ semantic composition ]] using Recursive NNs .",0
2791,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target << via >> semantic composition using [[ Recursive NNs ]] .",0
2792,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward [[ target ]] << via >> semantic composition using Recursive NNs .",0
2793,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target << via >> [[ semantic composition ]] using Recursive NNs .",1
2794,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target << via >> semantic composition using [[ Recursive NNs ]] .",0
2795,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] << via >> [[ semantic composition ]] using Recursive NNs .",0
2796,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] << via >> semantic composition using [[ Recursive NNs ]] .",0
2797,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target << via >> [[ semantic composition ]] using [[ Recursive NNs ]] .",0
2798,4602,"[[ Rec - NN ]] firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2799,4602,"[[ Rec - NN ]] firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2800,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2801,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2802,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition << using >> Recursive NNs .",0
2803,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition << using >> Recursive NNs .",0
2804,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] << using >> Recursive NNs .",0
2805,4602,"[[ Rec - NN ]] firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition << using >> [[ Recursive NNs ]] .",0
2806,4602,"Rec - NN firstly uses [[ rules ]] to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2807,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2808,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2809,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition << using >> Recursive NNs .",0
2810,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition << using >> Recursive NNs .",0
2811,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] << using >> Recursive NNs .",0
2812,4602,"Rec - NN firstly uses [[ rules ]] to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition << using >> [[ Recursive NNs ]] .",0
2813,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2814,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2815,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition << using >> Recursive NNs .",0
2816,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition << using >> Recursive NNs .",0
2817,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] << using >> Recursive NNs .",0
2818,4602,"Rec - NN firstly uses rules to transform the [[ dependency tree ]] and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition << using >> [[ Recursive NNs ]] .",0
2819,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the [[ root ]] , and then learns the sentence representation toward target via semantic composition << using >> Recursive NNs .",0
2820,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the [[ sentence representation ]] toward target via semantic composition << using >> Recursive NNs .",0
2821,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward [[ target ]] via semantic composition << using >> Recursive NNs .",0
2822,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via [[ semantic composition ]] << using >> Recursive NNs .",0
2823,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the [[ opinion target ]] at the root , and then learns the sentence representation toward target via semantic composition << using >> [[ Recursive NNs ]] .",0
2824,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the [[ sentence representation ]] toward target via semantic composition << using >> Recursive NNs .",0
2825,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward [[ target ]] via semantic composition << using >> Recursive NNs .",0
2826,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via [[ semantic composition ]] << using >> Recursive NNs .",0
2827,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the [[ root ]] , and then learns the sentence representation toward target via semantic composition << using >> [[ Recursive NNs ]] .",0
2828,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward [[ target ]] via semantic composition << using >> Recursive NNs .",0
2829,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via [[ semantic composition ]] << using >> Recursive NNs .",0
2830,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the [[ sentence representation ]] toward target via semantic composition << using >> [[ Recursive NNs ]] .",0
2831,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via [[ semantic composition ]] << using >> Recursive NNs .",0
2832,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward [[ target ]] via semantic composition << using >> [[ Recursive NNs ]] .",0
2833,4602,"Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via [[ semantic composition ]] << using >> [[ Recursive NNs ]] .",1
2834,4452,"<< In >> a [[ more controlled comparison ]] - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2835,4452,"<< In >> a [[ more controlled comparison ]] - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2836,4452,"<< In >> a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2837,4452,"<< In >> a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2838,4452,"<< In >> a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2839,4452,"<< In >> a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2840,4452,"<< In >> a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2841,4452,"<< In >> a more controlled comparison - with [[ shallow architectures ]] and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2842,4452,"<< In >> a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2843,4452,"<< In >> a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2844,4452,"<< In >> a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2845,4452,"<< In >> a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2846,4452,"<< In >> a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2847,4452,"<< In >> a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2848,4452,"<< In >> a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2849,4452,"<< In >> a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2850,4452,"<< In >> a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2851,4452,"<< In >> a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2852,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2853,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2854,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2855,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2856,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2857,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2858,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2859,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2860,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2861,4452,"<< In >> a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than [[ 10 % ]] .",0
2862,4452,"In a [[ more controlled comparison ]] - << with >> [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",1
2863,4452,"In a [[ more controlled comparison ]] - << with >> shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",1
2864,4452,"In a [[ more controlled comparison ]] - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2865,4452,"In a [[ more controlled comparison ]] - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2866,4452,"In a [[ more controlled comparison ]] - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2867,4452,"In a [[ more controlled comparison ]] - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2868,4452,"In a [[ more controlled comparison ]] - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2869,4452,"In a more controlled comparison - << with >> [[ shallow architectures ]] and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2870,4452,"In a more controlled comparison - << with >> [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2871,4452,"In a more controlled comparison - << with >> [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2872,4452,"In a more controlled comparison - << with >> [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2873,4452,"In a more controlled comparison - << with >> [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2874,4452,"In a more controlled comparison - << with >> [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2875,4452,"In a more controlled comparison - << with >> shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2876,4452,"In a more controlled comparison - << with >> shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2877,4452,"In a more controlled comparison - << with >> shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2878,4452,"In a more controlled comparison - << with >> shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2879,4452,"In a more controlled comparison - << with >> shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2880,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2881,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2882,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2883,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2884,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2885,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2886,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2887,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2888,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2889,4452,"In a more controlled comparison - << with >> shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than [[ 10 % ]] .",0
2890,4452,"In a [[ more controlled comparison ]] - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2891,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2892,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2893,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2894,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2895,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2896,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2897,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2898,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2899,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2900,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2901,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2902,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2903,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - [[ TBCNNs ]] , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2904,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , << consistently outperform >> [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2905,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , << consistently outperform >> RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2906,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2907,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2908,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , << consistently outperform >> [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",1
2909,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , << consistently outperform >> RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2910,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",1
2911,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2912,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> [[ RNNs ]] to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2913,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2914,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2915,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2916,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2917,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , << consistently outperform >> RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than [[ 10 % ]] .",0
2918,4452,"In a [[ more controlled comparison ]] - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2919,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2920,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2921,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2922,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2923,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2924,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2925,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2926,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2927,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2928,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2929,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2930,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2931,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - [[ TBCNNs ]] , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2932,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform [[ RNNs ]] << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2933,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs << to >> a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2934,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2935,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2936,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform [[ RNNs ]] << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2937,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs << to >> a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",0
2938,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2939,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2940,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] << to >> a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than 10 % .",1
2941,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2942,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2943,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform [[ "" flat "" CNNs ]] by more than 10 % .",0
2944,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs by more than [[ 10 % ]] .",0
2945,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs << to >> a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] by more than [[ 10 % ]] .",0
2946,4452,"In a [[ more controlled comparison ]] - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2947,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2948,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2949,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2950,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2951,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] << by more than >> 10 % .",0
2952,4452,"In a [[ more controlled comparison ]] - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> [[ 10 % ]] .",0
2953,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2954,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2955,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2956,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2957,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] << by more than >> 10 % .",0
2958,4452,"In a more controlled comparison - with [[ shallow architectures ]] and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> [[ 10 % ]] .",0
2959,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2960,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2961,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2962,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] << by more than >> 10 % .",0
2963,4452,"In a more controlled comparison - with shallow architectures and the [[ basic interaction ( linearly transformed and non-linearly squashed ) ]] - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> [[ 10 % ]] .",0
2964,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2965,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2966,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] << by more than >> 10 % .",0
2967,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - [[ TBCNNs ]] , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> [[ 10 % ]] .",0
2968,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs << by more than >> 10 % .",0
2969,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] << by more than >> 10 % .",0
2970,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform [[ RNNs ]] to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs << by more than >> [[ 10 % ]] .",0
2971,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform [[ "" flat "" CNNs ]] << by more than >> 10 % .",0
2972,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a [[ large extent ( 50.4 - 51.4 % versus 43.2 % ) ]] ; they also consistently outperform "" flat "" CNNs << by more than >> [[ 10 % ]] .",0
2973,4452,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform [[ "" flat "" CNNs ]] << by more than >> [[ 10 % ]] .",1
2974,6100,<< For >> the [[ decoder ]] we experimented with both the [[ Elman RNN ]] and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,0
2975,6100,<< For >> the [[ decoder ]] we experimented with both the Elman RNN and the [[ Long - Short Term Memory ( LSTM ) architecture ]] ( as discussed in 3.1 ) .,0
2976,6100,<< For >> the decoder we experimented with both the [[ Elman RNN ]] and the [[ Long - Short Term Memory ( LSTM ) architecture ]] ( as discussed in 3.1 ) .,0
2977,6100,For the [[ decoder ]] we << experimented with >> both the [[ Elman RNN ]] and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,1
2978,6100,For the [[ decoder ]] we << experimented with >> both the Elman RNN and the [[ Long - Short Term Memory ( LSTM ) architecture ]] ( as discussed in 3.1 ) .,1
2979,6100,For the decoder we << experimented with >> both the [[ Elman RNN ]] and the [[ Long - Short Term Memory ( LSTM ) architecture ]] ( as discussed in 3.1 ) .,0
2980,2400,We << present >> a [[ parallel - hierarchical approach ]] to [[ machine comprehension ]] designed to work well in a data - limited setting .,0
2981,2400,We << present >> a [[ parallel - hierarchical approach ]] to machine comprehension designed to [[ work well ]] in a data - limited setting .,0
2982,2400,We << present >> a [[ parallel - hierarchical approach ]] to machine comprehension designed to work well in a [[ data - limited setting ]] .,0
2983,2400,We << present >> a parallel - hierarchical approach to [[ machine comprehension ]] designed to [[ work well ]] in a data - limited setting .,0
2984,2400,We << present >> a parallel - hierarchical approach to [[ machine comprehension ]] designed to work well in a [[ data - limited setting ]] .,0
2985,2400,We << present >> a parallel - hierarchical approach to machine comprehension designed to [[ work well ]] in a [[ data - limited setting ]] .,0
2986,2400,We present a [[ parallel - hierarchical approach ]] << to >> [[ machine comprehension ]] designed to work well in a data - limited setting .,1
2987,2400,We present a [[ parallel - hierarchical approach ]] << to >> machine comprehension designed to [[ work well ]] in a data - limited setting .,0
2988,2400,We present a [[ parallel - hierarchical approach ]] << to >> machine comprehension designed to work well in a [[ data - limited setting ]] .,0
2989,2400,We present a parallel - hierarchical approach << to >> [[ machine comprehension ]] designed to [[ work well ]] in a data - limited setting .,0
2990,2400,We present a parallel - hierarchical approach << to >> [[ machine comprehension ]] designed to work well in a [[ data - limited setting ]] .,0
2991,2400,We present a parallel - hierarchical approach << to >> machine comprehension designed to [[ work well ]] in a [[ data - limited setting ]] .,0
2992,2400,We present a [[ parallel - hierarchical approach ]] to [[ machine comprehension ]] << designed to >> work well in a data - limited setting .,0
2993,2400,We present a [[ parallel - hierarchical approach ]] to machine comprehension << designed to >> [[ work well ]] in a data - limited setting .,1
2994,2400,We present a [[ parallel - hierarchical approach ]] to machine comprehension << designed to >> work well in a [[ data - limited setting ]] .,0
2995,2400,We present a parallel - hierarchical approach to [[ machine comprehension ]] << designed to >> [[ work well ]] in a data - limited setting .,0
2996,2400,We present a parallel - hierarchical approach to [[ machine comprehension ]] << designed to >> work well in a [[ data - limited setting ]] .,0
2997,2400,We present a parallel - hierarchical approach to machine comprehension << designed to >> [[ work well ]] in a [[ data - limited setting ]] .,0
2998,2400,We present a [[ parallel - hierarchical approach ]] to [[ machine comprehension ]] designed to work well << in >> a data - limited setting .,0
2999,2400,We present a [[ parallel - hierarchical approach ]] to machine comprehension designed to [[ work well ]] << in >> a data - limited setting .,0
3000,2400,We present a [[ parallel - hierarchical approach ]] to machine comprehension designed to work well << in >> a [[ data - limited setting ]] .,0
3001,2400,We present a parallel - hierarchical approach to [[ machine comprehension ]] designed to [[ work well ]] << in >> a data - limited setting .,0
3002,2400,We present a parallel - hierarchical approach to [[ machine comprehension ]] designed to work well << in >> a [[ data - limited setting ]] .,0
3003,2400,We present a parallel - hierarchical approach to machine comprehension designed to [[ work well ]] << in >> a [[ data - limited setting ]] .,1
3004,5250,"[[ Our ensemble ]] << consists of >> [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",1
3005,5250,"[[ Our ensemble ]] << consists of >> Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",1
3006,5250,"[[ Our ensemble ]] << consists of >> Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",1
3007,5250,"[[ Our ensemble ]] << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3008,5250,"[[ Our ensemble ]] << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3009,5250,"[[ Our ensemble ]] << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3010,5250,"[[ Our ensemble ]] << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3011,5250,"Our ensemble << consists of >> [[ Long Short Term Memory networks ]] , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3012,5250,"Our ensemble << consists of >> [[ Long Short Term Memory networks ]] , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3013,5250,"Our ensemble << consists of >> [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3014,5250,"Our ensemble << consists of >> [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3015,5250,"Our ensemble << consists of >> [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3016,5250,"Our ensemble << consists of >> [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3017,5250,"Our ensemble << consists of >> Long Short Term Memory networks , [[ Convolution Neural Networks ]] , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3018,5250,"Our ensemble << consists of >> Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3019,5250,"Our ensemble << consists of >> Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3020,5250,"Our ensemble << consists of >> Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3021,5250,"Our ensemble << consists of >> Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3022,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3023,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3024,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3025,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3026,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as [[ Dropout ]] , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3027,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as [[ Dropout ]] , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3028,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as [[ Dropout ]] , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3029,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , [[ adaptive optimizers ]] such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3030,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , [[ adaptive optimizers ]] such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3031,5250,"Our ensemble << consists of >> Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as [[ Adam ]] , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3032,5250,"[[ Our ensemble ]] consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3033,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3034,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3035,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",1
3036,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",1
3037,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3038,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",1
3039,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3040,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3041,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3042,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3043,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3044,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3045,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , [[ fully connected Multi - Layer Perceptrons ]] and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3046,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3047,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3048,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3049,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3050,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them << using techniques >> such as [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3051,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them << using techniques >> such as Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3052,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them << using techniques >> such as Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3053,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them << using techniques >> such as Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3054,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as [[ Dropout ]] , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3055,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as [[ Dropout ]] , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3056,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as [[ Dropout ]] , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3057,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , [[ adaptive optimizers ]] such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3058,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , [[ adaptive optimizers ]] such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3059,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them << using techniques >> such as Dropout , adaptive optimizers such as [[ Adam ]] , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3060,5250,"[[ Our ensemble ]] consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3061,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3062,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3063,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3064,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3065,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3066,5250,"[[ Our ensemble ]] consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3067,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3068,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3069,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3070,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3071,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3072,5250,"Our ensemble consists of [[ Long Short Term Memory networks ]] , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3073,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3074,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3075,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3076,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3077,5250,"Our ensemble consists of Long Short Term Memory networks , [[ Convolution Neural Networks ]] , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3078,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques << such as >> [[ Dropout ]] , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3079,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques << such as >> Dropout , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3080,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques << such as >> Dropout , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3081,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , [[ fully connected Multi - Layer Perceptrons ]] and we complement them using techniques << such as >> Dropout , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3082,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> [[ Dropout ]] , [[ adaptive optimizers ]] such as Adam , pretrained word - embedding models and Attention based RNN decoders .",0
3083,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> [[ Dropout ]] , adaptive optimizers such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",0
3084,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> [[ Dropout ]] , adaptive optimizers such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3085,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , [[ adaptive optimizers ]] such as [[ Adam ]] , pretrained word - embedding models and Attention based RNN decoders .",1
3086,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , [[ adaptive optimizers ]] such as Adam , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3087,5250,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques << such as >> Dropout , adaptive optimizers such as [[ Adam ]] , [[ pretrained word - embedding models and Attention based RNN decoders ]] .",0
3088,3596,Its [[ architecture ]] is << better customized for >> the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,1
3089,3596,Its [[ architecture ]] is << better customized for >> the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,0
3090,3596,Its [[ architecture ]] is << better customized for >> the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the subject and object of the putative relation .,0
3091,3596,Its [[ architecture ]] is << better customized for >> the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the [[ subject and object ]] of the putative relation .,0
3092,3596,Its [[ architecture ]] is << better customized for >> the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the [[ putative relation ]] .,0
3093,3596,Its architecture is << better customized for >> the [[ slot filling task ]] : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,0
3094,3596,Its architecture is << better customized for >> the [[ slot filling task ]] : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the subject and object of the putative relation .,0
3095,3596,Its architecture is << better customized for >> the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position relative to the [[ subject and object ]] of the putative relation .,0
3096,3596,Its architecture is << better customized for >> the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the [[ putative relation ]] .,0
3097,3596,Its architecture is << better customized for >> the slot filling task : the [[ word representations ]] are augmented by [[ extra distributed representations of word position ]] relative to the subject and object of the putative relation .,0
3098,3596,Its architecture is << better customized for >> the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the [[ subject and object ]] of the putative relation .,0
3099,3596,Its architecture is << better customized for >> the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the subject and object of the [[ putative relation ]] .,0
3100,3596,Its architecture is << better customized for >> the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the [[ subject and object ]] of the putative relation .,0
3101,3596,Its architecture is << better customized for >> the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the subject and object of the [[ putative relation ]] .,0
3102,3596,Its architecture is << better customized for >> the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the [[ subject and object ]] of the [[ putative relation ]] .,0
3103,3596,Its [[ architecture ]] is better customized for the [[ slot filling task ]] : the word representations are << augmented by >> extra distributed representations of word position relative to the subject and object of the putative relation .,0
3104,3596,Its [[ architecture ]] is better customized for the slot filling task : the [[ word representations ]] are << augmented by >> extra distributed representations of word position relative to the subject and object of the putative relation .,0
3105,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are << augmented by >> [[ extra distributed representations of word position ]] relative to the subject and object of the putative relation .,0
3106,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are << augmented by >> extra distributed representations of word position relative to the [[ subject and object ]] of the putative relation .,0
3107,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are << augmented by >> extra distributed representations of word position relative to the subject and object of the [[ putative relation ]] .,0
3108,3596,Its architecture is better customized for the [[ slot filling task ]] : the [[ word representations ]] are << augmented by >> extra distributed representations of word position relative to the subject and object of the putative relation .,0
3109,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are << augmented by >> [[ extra distributed representations of word position ]] relative to the subject and object of the putative relation .,0
3110,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are << augmented by >> extra distributed representations of word position relative to the [[ subject and object ]] of the putative relation .,0
3111,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are << augmented by >> extra distributed representations of word position relative to the subject and object of the [[ putative relation ]] .,0
3112,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are << augmented by >> [[ extra distributed representations of word position ]] relative to the subject and object of the putative relation .,1
3113,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are << augmented by >> extra distributed representations of word position relative to the [[ subject and object ]] of the putative relation .,0
3114,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are << augmented by >> extra distributed representations of word position relative to the subject and object of the [[ putative relation ]] .,0
3115,3596,Its architecture is better customized for the slot filling task : the word representations are << augmented by >> [[ extra distributed representations of word position ]] relative to the [[ subject and object ]] of the putative relation .,0
3116,3596,Its architecture is better customized for the slot filling task : the word representations are << augmented by >> [[ extra distributed representations of word position ]] relative to the subject and object of the [[ putative relation ]] .,0
3117,3596,Its architecture is better customized for the slot filling task : the word representations are << augmented by >> extra distributed representations of word position relative to the [[ subject and object ]] of the [[ putative relation ]] .,0
3118,3596,Its [[ architecture ]] is better customized for the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position << relative to >> the subject and object of the putative relation .,0
3119,3596,Its [[ architecture ]] is better customized for the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position << relative to >> the subject and object of the putative relation .,0
3120,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] << relative to >> the subject and object of the putative relation .,0
3121,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position << relative to >> the [[ subject and object ]] of the putative relation .,0
3122,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position << relative to >> the subject and object of the [[ putative relation ]] .,0
3123,3596,Its architecture is better customized for the [[ slot filling task ]] : the [[ word representations ]] are augmented by extra distributed representations of word position << relative to >> the subject and object of the putative relation .,0
3124,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are augmented by [[ extra distributed representations of word position ]] << relative to >> the subject and object of the putative relation .,0
3125,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position << relative to >> the [[ subject and object ]] of the putative relation .,0
3126,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position << relative to >> the subject and object of the [[ putative relation ]] .,0
3127,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are augmented by [[ extra distributed representations of word position ]] << relative to >> the subject and object of the putative relation .,0
3128,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position << relative to >> the [[ subject and object ]] of the putative relation .,0
3129,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position << relative to >> the subject and object of the [[ putative relation ]] .,0
3130,3596,Its architecture is better customized for the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] << relative to >> the [[ subject and object ]] of the putative relation .,1
3131,3596,Its architecture is better customized for the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] << relative to >> the subject and object of the [[ putative relation ]] .,0
3132,3596,Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position << relative to >> the [[ subject and object ]] of the [[ putative relation ]] .,0
3133,3596,Its [[ architecture ]] is better customized for the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position relative to the subject and object << of >> the putative relation .,0
3134,3596,Its [[ architecture ]] is better customized for the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the subject and object << of >> the putative relation .,0
3135,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the subject and object << of >> the putative relation .,0
3136,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the [[ subject and object ]] << of >> the putative relation .,0
3137,3596,Its [[ architecture ]] is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object << of >> the [[ putative relation ]] .,0
3138,3596,Its architecture is better customized for the [[ slot filling task ]] : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the subject and object << of >> the putative relation .,0
3139,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the subject and object << of >> the putative relation .,0
3140,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position relative to the [[ subject and object ]] << of >> the putative relation .,0
3141,3596,Its architecture is better customized for the [[ slot filling task ]] : the word representations are augmented by extra distributed representations of word position relative to the subject and object << of >> the [[ putative relation ]] .,0
3142,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are augmented by [[ extra distributed representations of word position ]] relative to the subject and object << of >> the putative relation .,0
3143,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the [[ subject and object ]] << of >> the putative relation .,0
3144,3596,Its architecture is better customized for the slot filling task : the [[ word representations ]] are augmented by extra distributed representations of word position relative to the subject and object << of >> the [[ putative relation ]] .,0
3145,3596,Its architecture is better customized for the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the [[ subject and object ]] << of >> the putative relation .,0
3146,3596,Its architecture is better customized for the slot filling task : the word representations are augmented by [[ extra distributed representations of word position ]] relative to the subject and object << of >> the [[ putative relation ]] .,0
3147,3596,Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the [[ subject and object ]] << of >> the [[ putative relation ]] .,1
3148,1667,"<< During >> the [[ training ]] , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these averages at test time .",0
3149,1667,"<< During >> the [[ training ]] , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these averages at test time .",0
3150,1667,"<< During >> the [[ training ]] , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these averages at test time .",0
3151,1667,"<< During >> the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and use these [[ averages ]] at test time .",0
3152,1667,"<< During >> the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and use these averages at [[ test time ]] .",0
3153,1667,"<< During >> the training , we keep the [[ exponential moving average ]] of [[ weights ]] with 0.001 decay and use these averages at test time .",0
3154,1667,"<< During >> the training , we keep the [[ exponential moving average ]] of weights with [[ 0.001 decay ]] and use these averages at test time .",0
3155,1667,"<< During >> the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these [[ averages ]] at test time .",0
3156,1667,"<< During >> the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these averages at [[ test time ]] .",0
3157,1667,"<< During >> the training , we keep the exponential moving average of [[ weights ]] with [[ 0.001 decay ]] and use these averages at test time .",0
3158,1667,"<< During >> the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these [[ averages ]] at test time .",0
3159,1667,"<< During >> the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these averages at [[ test time ]] .",0
3160,1667,"<< During >> the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these [[ averages ]] at test time .",0
3161,1667,"<< During >> the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these averages at [[ test time ]] .",0
3162,1667,"<< During >> the training , we keep the exponential moving average of weights with 0.001 decay and use these [[ averages ]] at [[ test time ]] .",0
3163,1667,"During the [[ training ]] , we << keep >> the [[ exponential moving average ]] of weights with 0.001 decay and use these averages at test time .",1
3164,1667,"During the [[ training ]] , we << keep >> the exponential moving average of [[ weights ]] with 0.001 decay and use these averages at test time .",0
3165,1667,"During the [[ training ]] , we << keep >> the exponential moving average of weights with [[ 0.001 decay ]] and use these averages at test time .",0
3166,1667,"During the [[ training ]] , we << keep >> the exponential moving average of weights with 0.001 decay and use these [[ averages ]] at test time .",0
3167,1667,"During the [[ training ]] , we << keep >> the exponential moving average of weights with 0.001 decay and use these averages at [[ test time ]] .",0
3168,1667,"During the training , we << keep >> the [[ exponential moving average ]] of [[ weights ]] with 0.001 decay and use these averages at test time .",0
3169,1667,"During the training , we << keep >> the [[ exponential moving average ]] of weights with [[ 0.001 decay ]] and use these averages at test time .",0
3170,1667,"During the training , we << keep >> the [[ exponential moving average ]] of weights with 0.001 decay and use these [[ averages ]] at test time .",0
3171,1667,"During the training , we << keep >> the [[ exponential moving average ]] of weights with 0.001 decay and use these averages at [[ test time ]] .",0
3172,1667,"During the training , we << keep >> the exponential moving average of [[ weights ]] with [[ 0.001 decay ]] and use these averages at test time .",0
3173,1667,"During the training , we << keep >> the exponential moving average of [[ weights ]] with 0.001 decay and use these [[ averages ]] at test time .",0
3174,1667,"During the training , we << keep >> the exponential moving average of [[ weights ]] with 0.001 decay and use these averages at [[ test time ]] .",0
3175,1667,"During the training , we << keep >> the exponential moving average of weights with [[ 0.001 decay ]] and use these [[ averages ]] at test time .",0
3176,1667,"During the training , we << keep >> the exponential moving average of weights with [[ 0.001 decay ]] and use these averages at [[ test time ]] .",0
3177,1667,"During the training , we << keep >> the exponential moving average of weights with 0.001 decay and use these [[ averages ]] at [[ test time ]] .",0
3178,1667,"During the [[ training ]] , we keep the [[ exponential moving average ]] << of >> weights with 0.001 decay and use these averages at test time .",0
3179,1667,"During the [[ training ]] , we keep the exponential moving average << of >> [[ weights ]] with 0.001 decay and use these averages at test time .",0
3180,1667,"During the [[ training ]] , we keep the exponential moving average << of >> weights with [[ 0.001 decay ]] and use these averages at test time .",0
3181,1667,"During the [[ training ]] , we keep the exponential moving average << of >> weights with 0.001 decay and use these [[ averages ]] at test time .",0
3182,1667,"During the [[ training ]] , we keep the exponential moving average << of >> weights with 0.001 decay and use these averages at [[ test time ]] .",0
3183,1667,"During the training , we keep the [[ exponential moving average ]] << of >> [[ weights ]] with 0.001 decay and use these averages at test time .",1
3184,1667,"During the training , we keep the [[ exponential moving average ]] << of >> weights with [[ 0.001 decay ]] and use these averages at test time .",0
3185,1667,"During the training , we keep the [[ exponential moving average ]] << of >> weights with 0.001 decay and use these [[ averages ]] at test time .",0
3186,1667,"During the training , we keep the [[ exponential moving average ]] << of >> weights with 0.001 decay and use these averages at [[ test time ]] .",0
3187,1667,"During the training , we keep the exponential moving average << of >> [[ weights ]] with [[ 0.001 decay ]] and use these averages at test time .",0
3188,1667,"During the training , we keep the exponential moving average << of >> [[ weights ]] with 0.001 decay and use these [[ averages ]] at test time .",0
3189,1667,"During the training , we keep the exponential moving average << of >> [[ weights ]] with 0.001 decay and use these averages at [[ test time ]] .",0
3190,1667,"During the training , we keep the exponential moving average << of >> weights with [[ 0.001 decay ]] and use these [[ averages ]] at test time .",0
3191,1667,"During the training , we keep the exponential moving average << of >> weights with [[ 0.001 decay ]] and use these averages at [[ test time ]] .",0
3192,1667,"During the training , we keep the exponential moving average << of >> weights with 0.001 decay and use these [[ averages ]] at [[ test time ]] .",0
3193,1667,"During the [[ training ]] , we keep the [[ exponential moving average ]] of weights << with >> 0.001 decay and use these averages at test time .",0
3194,1667,"During the [[ training ]] , we keep the exponential moving average of [[ weights ]] << with >> 0.001 decay and use these averages at test time .",0
3195,1667,"During the [[ training ]] , we keep the exponential moving average of weights << with >> [[ 0.001 decay ]] and use these averages at test time .",0
3196,1667,"During the [[ training ]] , we keep the exponential moving average of weights << with >> 0.001 decay and use these [[ averages ]] at test time .",0
3197,1667,"During the [[ training ]] , we keep the exponential moving average of weights << with >> 0.001 decay and use these averages at [[ test time ]] .",0
3198,1667,"During the training , we keep the [[ exponential moving average ]] of [[ weights ]] << with >> 0.001 decay and use these averages at test time .",0
3199,1667,"During the training , we keep the [[ exponential moving average ]] of weights << with >> [[ 0.001 decay ]] and use these averages at test time .",1
3200,1667,"During the training , we keep the [[ exponential moving average ]] of weights << with >> 0.001 decay and use these [[ averages ]] at test time .",0
3201,1667,"During the training , we keep the [[ exponential moving average ]] of weights << with >> 0.001 decay and use these averages at [[ test time ]] .",0
3202,1667,"During the training , we keep the exponential moving average of [[ weights ]] << with >> [[ 0.001 decay ]] and use these averages at test time .",0
3203,1667,"During the training , we keep the exponential moving average of [[ weights ]] << with >> 0.001 decay and use these [[ averages ]] at test time .",0
3204,1667,"During the training , we keep the exponential moving average of [[ weights ]] << with >> 0.001 decay and use these averages at [[ test time ]] .",0
3205,1667,"During the training , we keep the exponential moving average of weights << with >> [[ 0.001 decay ]] and use these [[ averages ]] at test time .",0
3206,1667,"During the training , we keep the exponential moving average of weights << with >> [[ 0.001 decay ]] and use these averages at [[ test time ]] .",0
3207,1667,"During the training , we keep the exponential moving average of weights << with >> 0.001 decay and use these [[ averages ]] at [[ test time ]] .",0
3208,1667,"During the [[ training ]] , we keep the [[ exponential moving average ]] of weights with 0.001 decay and << use >> these averages at test time .",0
3209,1667,"During the [[ training ]] , we keep the exponential moving average of [[ weights ]] with 0.001 decay and << use >> these averages at test time .",0
3210,1667,"During the [[ training ]] , we keep the exponential moving average of weights with [[ 0.001 decay ]] and << use >> these averages at test time .",0
3211,1667,"During the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and << use >> these [[ averages ]] at test time .",0
3212,1667,"During the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and << use >> these averages at [[ test time ]] .",0
3213,1667,"During the training , we keep the [[ exponential moving average ]] of [[ weights ]] with 0.001 decay and << use >> these averages at test time .",0
3214,1667,"During the training , we keep the [[ exponential moving average ]] of weights with [[ 0.001 decay ]] and << use >> these averages at test time .",0
3215,1667,"During the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and << use >> these [[ averages ]] at test time .",1
3216,1667,"During the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and << use >> these averages at [[ test time ]] .",0
3217,1667,"During the training , we keep the exponential moving average of [[ weights ]] with [[ 0.001 decay ]] and << use >> these averages at test time .",0
3218,1667,"During the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and << use >> these [[ averages ]] at test time .",0
3219,1667,"During the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and << use >> these averages at [[ test time ]] .",0
3220,1667,"During the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and << use >> these [[ averages ]] at test time .",0
3221,1667,"During the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and << use >> these averages at [[ test time ]] .",0
3222,1667,"During the training , we keep the exponential moving average of weights with 0.001 decay and << use >> these [[ averages ]] at [[ test time ]] .",0
3223,1667,"During the [[ training ]] , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these averages << at >> test time .",0
3224,1667,"During the [[ training ]] , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these averages << at >> test time .",0
3225,1667,"During the [[ training ]] , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these averages << at >> test time .",0
3226,1667,"During the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and use these [[ averages ]] << at >> test time .",0
3227,1667,"During the [[ training ]] , we keep the exponential moving average of weights with 0.001 decay and use these averages << at >> [[ test time ]] .",0
3228,1667,"During the training , we keep the [[ exponential moving average ]] of [[ weights ]] with 0.001 decay and use these averages << at >> test time .",0
3229,1667,"During the training , we keep the [[ exponential moving average ]] of weights with [[ 0.001 decay ]] and use these averages << at >> test time .",0
3230,1667,"During the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these [[ averages ]] << at >> test time .",0
3231,1667,"During the training , we keep the [[ exponential moving average ]] of weights with 0.001 decay and use these averages << at >> [[ test time ]] .",0
3232,1667,"During the training , we keep the exponential moving average of [[ weights ]] with [[ 0.001 decay ]] and use these averages << at >> test time .",0
3233,1667,"During the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these [[ averages ]] << at >> test time .",0
3234,1667,"During the training , we keep the exponential moving average of [[ weights ]] with 0.001 decay and use these averages << at >> [[ test time ]] .",0
3235,1667,"During the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these [[ averages ]] << at >> test time .",0
3236,1667,"During the training , we keep the exponential moving average of weights with [[ 0.001 decay ]] and use these averages << at >> [[ test time ]] .",0
3237,1667,"During the training , we keep the exponential moving average of weights with 0.001 decay and use these [[ averages ]] << at >> [[ test time ]] .",1
3238,886,[[ Self- attention ]] << in >> the [[ encoders ]] is also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,1
3239,886,[[ Self- attention ]] << in >> the encoders is also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,0
3240,886,[[ Self- attention ]] << in >> the encoders is also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3241,886,[[ Self- attention ]] << in >> the encoders is also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3242,886,Self- attention << in >> the [[ encoders ]] is also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,0
3243,886,Self- attention << in >> the [[ encoders ]] is also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3244,886,Self- attention << in >> the [[ encoders ]] is also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3245,886,Self- attention << in >> the encoders is also a [[ necessary component ]] that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3246,886,Self- attention << in >> the encoders is also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3247,886,Self- attention << in >> the encoders is also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the [[ ultimate performance ]] .,0
3248,886,[[ Self- attention ]] in the [[ encoders ]] << is >> also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,0
3249,886,[[ Self- attention ]] in the encoders << is >> also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,0
3250,886,[[ Self- attention ]] in the encoders << is >> also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3251,886,[[ Self- attention ]] in the encoders << is >> also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3252,886,Self- attention in the [[ encoders ]] << is >> also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,1
3253,886,Self- attention in the [[ encoders ]] << is >> also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3254,886,Self- attention in the [[ encoders ]] << is >> also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3255,886,Self- attention in the encoders << is >> also a [[ necessary component ]] that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3256,886,Self- attention in the encoders << is >> also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3257,886,Self- attention in the encoders << is >> also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] to the [[ ultimate performance ]] .,0
3258,886,[[ Self- attention ]] in the [[ encoders ]] is also a necessary component << that contributes >> 1.4/1.3 gain of EM / F1 to the ultimate performance .,0
3259,886,[[ Self- attention ]] in the encoders is also a [[ necessary component ]] << that contributes >> 1.4/1.3 gain of EM / F1 to the ultimate performance .,0
3260,886,[[ Self- attention ]] in the encoders is also a necessary component << that contributes >> [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3261,886,[[ Self- attention ]] in the encoders is also a necessary component << that contributes >> 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3262,886,Self- attention in the [[ encoders ]] is also a [[ necessary component ]] << that contributes >> 1.4/1.3 gain of EM / F1 to the ultimate performance .,0
3263,886,Self- attention in the [[ encoders ]] is also a necessary component << that contributes >> [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,0
3264,886,Self- attention in the [[ encoders ]] is also a necessary component << that contributes >> 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3265,886,Self- attention in the encoders is also a [[ necessary component ]] << that contributes >> [[ 1.4/1.3 gain of EM / F1 ]] to the ultimate performance .,1
3266,886,Self- attention in the encoders is also a [[ necessary component ]] << that contributes >> 1.4/1.3 gain of EM / F1 to the [[ ultimate performance ]] .,0
3267,886,Self- attention in the encoders is also a necessary component << that contributes >> [[ 1.4/1.3 gain of EM / F1 ]] to the [[ ultimate performance ]] .,0
3268,886,[[ Self- attention ]] in the [[ encoders ]] is also a necessary component that contributes 1.4/1.3 gain of EM / F1 << to >> the ultimate performance .,0
3269,886,[[ Self- attention ]] in the encoders is also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 << to >> the ultimate performance .,0
3270,886,[[ Self- attention ]] in the encoders is also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] << to >> the ultimate performance .,0
3271,886,[[ Self- attention ]] in the encoders is also a necessary component that contributes 1.4/1.3 gain of EM / F1 << to >> the [[ ultimate performance ]] .,0
3272,886,Self- attention in the [[ encoders ]] is also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 << to >> the ultimate performance .,0
3273,886,Self- attention in the [[ encoders ]] is also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] << to >> the ultimate performance .,0
3274,886,Self- attention in the [[ encoders ]] is also a necessary component that contributes 1.4/1.3 gain of EM / F1 << to >> the [[ ultimate performance ]] .,0
3275,886,Self- attention in the encoders is also a [[ necessary component ]] that contributes [[ 1.4/1.3 gain of EM / F1 ]] << to >> the ultimate performance .,0
3276,886,Self- attention in the encoders is also a [[ necessary component ]] that contributes 1.4/1.3 gain of EM / F1 << to >> the [[ ultimate performance ]] .,0
3277,886,Self- attention in the encoders is also a necessary component that contributes [[ 1.4/1.3 gain of EM / F1 ]] << to >> the [[ ultimate performance ]] .,1
3278,4056,"There is a [[ significant difference ]] << in >> [[ performance ]] of the MIRA baseline and the LSTM models , both in terms of F1 - score and in accuracy .",1
3279,4056,"There is a [[ significant difference ]] << in >> performance of the [[ MIRA baseline and the LSTM models ]] , both in terms of F1 - score and in accuracy .",0
3280,4056,"There is a [[ significant difference ]] << in >> performance of the MIRA baseline and the LSTM models , both in terms of [[ F1 - score and in accuracy ]] .",0
3281,4056,"There is a significant difference << in >> [[ performance ]] of the [[ MIRA baseline and the LSTM models ]] , both in terms of F1 - score and in accuracy .",0
3282,4056,"There is a significant difference << in >> [[ performance ]] of the MIRA baseline and the LSTM models , both in terms of [[ F1 - score and in accuracy ]] .",0
3283,4056,"There is a significant difference << in >> performance of the [[ MIRA baseline and the LSTM models ]] , both in terms of [[ F1 - score and in accuracy ]] .",0
3284,4056,"There is a [[ significant difference ]] in [[ performance ]] << of >> the MIRA baseline and the LSTM models , both in terms of F1 - score and in accuracy .",0
3285,4056,"There is a [[ significant difference ]] in performance << of >> the [[ MIRA baseline and the LSTM models ]] , both in terms of F1 - score and in accuracy .",0
3286,4056,"There is a [[ significant difference ]] in performance << of >> the MIRA baseline and the LSTM models , both in terms of [[ F1 - score and in accuracy ]] .",0
3287,4056,"There is a significant difference in [[ performance ]] << of >> the [[ MIRA baseline and the LSTM models ]] , both in terms of F1 - score and in accuracy .",1
3288,4056,"There is a significant difference in [[ performance ]] << of >> the MIRA baseline and the LSTM models , both in terms of [[ F1 - score and in accuracy ]] .",0
3289,4056,"There is a significant difference in performance << of >> the [[ MIRA baseline and the LSTM models ]] , both in terms of [[ F1 - score and in accuracy ]] .",0
3290,4056,"There is a [[ significant difference ]] in [[ performance ]] of the MIRA baseline and the LSTM models , both << in terms of >> F1 - score and in accuracy .",0
3291,4056,"There is a [[ significant difference ]] in performance of the [[ MIRA baseline and the LSTM models ]] , both << in terms of >> F1 - score and in accuracy .",0
3292,4056,"There is a [[ significant difference ]] in performance of the MIRA baseline and the LSTM models , both << in terms of >> [[ F1 - score and in accuracy ]] .",0
3293,4056,"There is a significant difference in [[ performance ]] of the [[ MIRA baseline and the LSTM models ]] , both << in terms of >> F1 - score and in accuracy .",0
3294,4056,"There is a significant difference in [[ performance ]] of the MIRA baseline and the LSTM models , both << in terms of >> [[ F1 - score and in accuracy ]] .",1
3295,4056,"There is a significant difference in performance of the [[ MIRA baseline and the LSTM models ]] , both << in terms of >> [[ F1 - score and in accuracy ]] .",0
3296,5380,We << use >> the [[ softmax function f ]] to compute the [[ probability distribution ]] over the predefined classes .,0
3297,5380,We << use >> the [[ softmax function f ]] to compute the probability distribution over the [[ predefined classes ]] .,0
3298,5380,We << use >> the softmax function f to compute the [[ probability distribution ]] over the [[ predefined classes ]] .,0
3299,5380,We use the [[ softmax function f ]] << to compute >> the [[ probability distribution ]] over the predefined classes .,1
3300,5380,We use the [[ softmax function f ]] << to compute >> the probability distribution over the [[ predefined classes ]] .,0
3301,5380,We use the softmax function f << to compute >> the [[ probability distribution ]] over the [[ predefined classes ]] .,0
3302,5380,We use the [[ softmax function f ]] to compute the [[ probability distribution ]] << over >> the predefined classes .,0
3303,5380,We use the [[ softmax function f ]] to compute the probability distribution << over >> the [[ predefined classes ]] .,0
3304,5380,We use the softmax function f to compute the [[ probability distribution ]] << over >> the [[ predefined classes ]] .,1
3305,333,[[ Training ]] is << carried out by >> [[ mini-batch stochastic gradient descent ( SGD ) ]] with a momentum of 0.9 and a gradient clipping of 5.0 .,1
3306,333,[[ Training ]] is << carried out by >> mini-batch stochastic gradient descent ( SGD ) with a [[ momentum of 0.9 ]] and a gradient clipping of 5.0 .,0
3307,333,[[ Training ]] is << carried out by >> mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a [[ gradient clipping of 5.0 ]] .,0
3308,333,Training is << carried out by >> [[ mini-batch stochastic gradient descent ( SGD ) ]] with a [[ momentum of 0.9 ]] and a gradient clipping of 5.0 .,0
3309,333,Training is << carried out by >> [[ mini-batch stochastic gradient descent ( SGD ) ]] with a momentum of 0.9 and a [[ gradient clipping of 5.0 ]] .,0
3310,333,Training is << carried out by >> mini-batch stochastic gradient descent ( SGD ) with a [[ momentum of 0.9 ]] and a [[ gradient clipping of 5.0 ]] .,0
3311,333,[[ Training ]] is carried out by [[ mini-batch stochastic gradient descent ( SGD ) ]] << with >> a momentum of 0.9 and a gradient clipping of 5.0 .,0
3312,333,[[ Training ]] is carried out by mini-batch stochastic gradient descent ( SGD ) << with >> a [[ momentum of 0.9 ]] and a gradient clipping of 5.0 .,0
3313,333,[[ Training ]] is carried out by mini-batch stochastic gradient descent ( SGD ) << with >> a momentum of 0.9 and a [[ gradient clipping of 5.0 ]] .,0
3314,333,Training is carried out by [[ mini-batch stochastic gradient descent ( SGD ) ]] << with >> a [[ momentum of 0.9 ]] and a gradient clipping of 5.0 .,1
3315,333,Training is carried out by [[ mini-batch stochastic gradient descent ( SGD ) ]] << with >> a momentum of 0.9 and a [[ gradient clipping of 5.0 ]] .,1
3316,333,Training is carried out by mini-batch stochastic gradient descent ( SGD ) << with >> a [[ momentum of 0.9 ]] and a [[ gradient clipping of 5.0 ]] .,0
3317,153,The << goal >> is to [[ predict the next word ]] conditioned on [[ previous words ]] and the task is evaluated by perplexity .,0
3318,153,The << goal >> is to [[ predict the next word ]] conditioned on previous words and the task is evaluated by [[ perplexity ]] .,0
3319,153,The << goal >> is to predict the next word conditioned on [[ previous words ]] and the task is evaluated by [[ perplexity ]] .,0
3320,153,The goal is to [[ predict the next word ]] << conditioned on >> [[ previous words ]] and the task is evaluated by perplexity .,1
3321,153,The goal is to [[ predict the next word ]] << conditioned on >> previous words and the task is evaluated by [[ perplexity ]] .,0
3322,153,The goal is to predict the next word << conditioned on >> [[ previous words ]] and the task is evaluated by [[ perplexity ]] .,0
3323,153,The goal is to [[ predict the next word ]] conditioned on [[ previous words ]] and the task is << evaluated by >> perplexity .,0
3324,153,The goal is to [[ predict the next word ]] conditioned on previous words and the task is << evaluated by >> [[ perplexity ]] .,0
3325,153,The goal is to predict the next word conditioned on [[ previous words ]] and the task is << evaluated by >> [[ perplexity ]] .,0
3326,5516,"<< To ac- count for >> the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare [[ SWEM - based models ]] with existing recurrent and convolutional networks in a pointby - point manner .",0
3327,5516,"<< To ac- count for >> the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare SWEM - based models with [[ existing recurrent and convolutional networks ]] in a pointby - point manner .",0
3328,5516,"<< To ac- count for >> the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare SWEM - based models with existing recurrent and convolutional networks in a [[ pointby - point manner ]] .",0
3329,5516,"<< To ac- count for >> the distinct nature of various NLP tasks that may require different semantic features , we compare [[ SWEM - based models ]] with [[ existing recurrent and convolutional networks ]] in a pointby - point manner .",0
3330,5516,"<< To ac- count for >> the distinct nature of various NLP tasks that may require different semantic features , we compare [[ SWEM - based models ]] with existing recurrent and convolutional networks in a [[ pointby - point manner ]] .",0
3331,5516,"<< To ac- count for >> the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with [[ existing recurrent and convolutional networks ]] in a [[ pointby - point manner ]] .",0
3332,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we << compare >> [[ SWEM - based models ]] with existing recurrent and convolutional networks in a pointby - point manner .",1
3333,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we << compare >> SWEM - based models with [[ existing recurrent and convolutional networks ]] in a pointby - point manner .",0
3334,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we << compare >> SWEM - based models with existing recurrent and convolutional networks in a [[ pointby - point manner ]] .",0
3335,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we << compare >> [[ SWEM - based models ]] with [[ existing recurrent and convolutional networks ]] in a pointby - point manner .",0
3336,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we << compare >> [[ SWEM - based models ]] with existing recurrent and convolutional networks in a [[ pointby - point manner ]] .",0
3337,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we << compare >> SWEM - based models with [[ existing recurrent and convolutional networks ]] in a [[ pointby - point manner ]] .",0
3338,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare [[ SWEM - based models ]] << with >> existing recurrent and convolutional networks in a pointby - point manner .",0
3339,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare SWEM - based models << with >> [[ existing recurrent and convolutional networks ]] in a pointby - point manner .",0
3340,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare SWEM - based models << with >> existing recurrent and convolutional networks in a [[ pointby - point manner ]] .",0
3341,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare [[ SWEM - based models ]] << with >> [[ existing recurrent and convolutional networks ]] in a pointby - point manner .",1
3342,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare [[ SWEM - based models ]] << with >> existing recurrent and convolutional networks in a [[ pointby - point manner ]] .",0
3343,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models << with >> [[ existing recurrent and convolutional networks ]] in a [[ pointby - point manner ]] .",0
3344,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare [[ SWEM - based models ]] with existing recurrent and convolutional networks << in a >> pointby - point manner .",0
3345,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare SWEM - based models with [[ existing recurrent and convolutional networks ]] << in a >> pointby - point manner .",0
3346,5516,"To ac- count for the [[ distinct nature of various NLP tasks that may require different semantic features ]] , we compare SWEM - based models with existing recurrent and convolutional networks << in a >> [[ pointby - point manner ]] .",0
3347,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare [[ SWEM - based models ]] with [[ existing recurrent and convolutional networks ]] << in a >> pointby - point manner .",0
3348,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare [[ SWEM - based models ]] with existing recurrent and convolutional networks << in a >> [[ pointby - point manner ]] .",0
3349,5516,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with [[ existing recurrent and convolutional networks ]] << in a >> [[ pointby - point manner ]] .",1
3350,721,The [[ number of hidden units ]] << in >> [[ all the LSTMs ]] is 150 .,1
3351,721,The [[ number of hidden units ]] << in >> all the LSTMs is [[ 150 ]] .,0
3352,721,The number of hidden units << in >> [[ all the LSTMs ]] is [[ 150 ]] .,0
3353,721,The [[ number of hidden units ]] in [[ all the LSTMs ]] << is >> 150 .,0
3354,721,The [[ number of hidden units ]] in all the LSTMs << is >> [[ 150 ]] .,1
3355,721,The number of hidden units in [[ all the LSTMs ]] << is >> [[ 150 ]] .,0
3356,1622,"It can be << seen that >> [[ all tasks ]] benefit from [[ deeper models ]] , in particular XNLI and Tatoeba , suggesting that a single layer BiLSTM has not enough capacity to encode so many languages .",0
3357,1622,"It can be seen that [[ all tasks ]] << benefit from >> [[ deeper models ]] , in particular XNLI and Tatoeba , suggesting that a single layer BiLSTM has not enough capacity to encode so many languages .",1
3358,2693,"We << achieve >> a [[ higher MRR@10 ]] of [[ 21.5 ]] when documents are expanded with both types of words , showing that they are complementary .",0
3359,2693,"We << achieve >> a [[ higher MRR@10 ]] of 21.5 when [[ documents ]] are expanded with both types of words , showing that they are complementary .",0
3360,2693,"We << achieve >> a [[ higher MRR@10 ]] of 21.5 when documents are expanded with [[ both types of words ]] , showing that they are complementary .",0
3361,2693,"We << achieve >> a higher MRR@10 of [[ 21.5 ]] when [[ documents ]] are expanded with both types of words , showing that they are complementary .",0
3362,2693,"We << achieve >> a higher MRR@10 of [[ 21.5 ]] when documents are expanded with [[ both types of words ]] , showing that they are complementary .",0
3363,2693,"We << achieve >> a higher MRR@10 of 21.5 when [[ documents ]] are expanded with [[ both types of words ]] , showing that they are complementary .",0
3364,2693,"We achieve a [[ higher MRR@10 ]] << of >> [[ 21.5 ]] when documents are expanded with both types of words , showing that they are complementary .",1
3365,2693,"We achieve a [[ higher MRR@10 ]] << of >> 21.5 when [[ documents ]] are expanded with both types of words , showing that they are complementary .",0
3366,2693,"We achieve a [[ higher MRR@10 ]] << of >> 21.5 when documents are expanded with [[ both types of words ]] , showing that they are complementary .",0
3367,2693,"We achieve a higher MRR@10 << of >> [[ 21.5 ]] when [[ documents ]] are expanded with both types of words , showing that they are complementary .",0
3368,2693,"We achieve a higher MRR@10 << of >> [[ 21.5 ]] when documents are expanded with [[ both types of words ]] , showing that they are complementary .",0
3369,2693,"We achieve a higher MRR@10 << of >> 21.5 when [[ documents ]] are expanded with [[ both types of words ]] , showing that they are complementary .",0
3370,2693,"We achieve a [[ higher MRR@10 ]] of [[ 21.5 ]] << when >> documents are expanded with both types of words , showing that they are complementary .",0
3371,2693,"We achieve a [[ higher MRR@10 ]] of 21.5 << when >> [[ documents ]] are expanded with both types of words , showing that they are complementary .",1
3372,2693,"We achieve a [[ higher MRR@10 ]] of 21.5 << when >> documents are expanded with [[ both types of words ]] , showing that they are complementary .",0
3373,2693,"We achieve a higher MRR@10 of [[ 21.5 ]] << when >> [[ documents ]] are expanded with both types of words , showing that they are complementary .",0
3374,2693,"We achieve a higher MRR@10 of [[ 21.5 ]] << when >> documents are expanded with [[ both types of words ]] , showing that they are complementary .",0
3375,2693,"We achieve a higher MRR@10 of 21.5 << when >> [[ documents ]] are expanded with [[ both types of words ]] , showing that they are complementary .",0
3376,2693,"We achieve a [[ higher MRR@10 ]] of [[ 21.5 ]] when documents are << expanded with >> both types of words , showing that they are complementary .",0
3377,2693,"We achieve a [[ higher MRR@10 ]] of 21.5 when [[ documents ]] are << expanded with >> both types of words , showing that they are complementary .",0
3378,2693,"We achieve a [[ higher MRR@10 ]] of 21.5 when documents are << expanded with >> [[ both types of words ]] , showing that they are complementary .",0
3379,2693,"We achieve a higher MRR@10 of [[ 21.5 ]] when [[ documents ]] are << expanded with >> both types of words , showing that they are complementary .",0
3380,2693,"We achieve a higher MRR@10 of [[ 21.5 ]] when documents are << expanded with >> [[ both types of words ]] , showing that they are complementary .",0
3381,2693,"We achieve a higher MRR@10 of 21.5 when [[ documents ]] are << expanded with >> [[ both types of words ]] , showing that they are complementary .",1
3382,5067,"the [[ dropout probability ]] << at >> [[ 0.1 ]] , set the number of epochs to 4 .",1
3383,5067,"the [[ dropout probability ]] << at >> 0.1 , set the [[ number of epochs ]] to 4 .",0
3384,5067,"the [[ dropout probability ]] << at >> 0.1 , set the number of epochs to [[ 4 ]] .",0
3385,5067,"the dropout probability << at >> [[ 0.1 ]] , set the [[ number of epochs ]] to 4 .",0
3386,5067,"the dropout probability << at >> [[ 0.1 ]] , set the number of epochs to [[ 4 ]] .",0
3387,5067,"the dropout probability << at >> 0.1 , set the [[ number of epochs ]] to [[ 4 ]] .",0
3388,5067,"the [[ dropout probability ]] at [[ 0.1 ]] , << set >> the number of epochs to 4 .",0
3389,5067,"the [[ dropout probability ]] at 0.1 , << set >> the [[ number of epochs ]] to 4 .",0
3390,5067,"the [[ dropout probability ]] at 0.1 , << set >> the number of epochs to [[ 4 ]] .",0
3391,5067,"the dropout probability at [[ 0.1 ]] , << set >> the [[ number of epochs ]] to 4 .",0
3392,5067,"the dropout probability at [[ 0.1 ]] , << set >> the number of epochs to [[ 4 ]] .",0
3393,5067,"the dropout probability at 0.1 , << set >> the [[ number of epochs ]] to [[ 4 ]] .",0
3394,5067,"the [[ dropout probability ]] at [[ 0.1 ]] , set the number of epochs << to >> 4 .",0
3395,5067,"the [[ dropout probability ]] at 0.1 , set the [[ number of epochs ]] << to >> 4 .",0
3396,5067,"the [[ dropout probability ]] at 0.1 , set the number of epochs << to >> [[ 4 ]] .",0
3397,5067,"the dropout probability at [[ 0.1 ]] , set the [[ number of epochs ]] << to >> 4 .",0
3398,5067,"the dropout probability at [[ 0.1 ]] , set the number of epochs << to >> [[ 4 ]] .",0
3399,5067,"the dropout probability at 0.1 , set the [[ number of epochs ]] << to >> [[ 4 ]] .",1
3400,4618,"[[ ATAE - LSTM , IAN and RAM ]] << are >> [[ attention based models ]] , they stably exceed the TD - LSTM method on Restaurant and Laptop datasets .",1
3401,4618,"[[ ATAE - LSTM , IAN and RAM ]] << are >> attention based models , they stably exceed the [[ TD - LSTM method ]] on Restaurant and Laptop datasets .",0
3402,4618,"[[ ATAE - LSTM , IAN and RAM ]] << are >> attention based models , they stably exceed the TD - LSTM method on [[ Restaurant and Laptop datasets ]] .",0
3403,4618,"ATAE - LSTM , IAN and RAM << are >> [[ attention based models ]] , they stably exceed the [[ TD - LSTM method ]] on Restaurant and Laptop datasets .",0
3404,4618,"ATAE - LSTM , IAN and RAM << are >> [[ attention based models ]] , they stably exceed the TD - LSTM method on [[ Restaurant and Laptop datasets ]] .",0
3405,4618,"ATAE - LSTM , IAN and RAM << are >> attention based models , they stably exceed the [[ TD - LSTM method ]] on [[ Restaurant and Laptop datasets ]] .",0
3406,4618,"[[ ATAE - LSTM , IAN and RAM ]] are [[ attention based models ]] , they << stably exceed >> the TD - LSTM method on Restaurant and Laptop datasets .",0
3407,4618,"[[ ATAE - LSTM , IAN and RAM ]] are attention based models , they << stably exceed >> the [[ TD - LSTM method ]] on Restaurant and Laptop datasets .",1
3408,4618,"[[ ATAE - LSTM , IAN and RAM ]] are attention based models , they << stably exceed >> the TD - LSTM method on [[ Restaurant and Laptop datasets ]] .",0
3409,4618,"ATAE - LSTM , IAN and RAM are [[ attention based models ]] , they << stably exceed >> the [[ TD - LSTM method ]] on Restaurant and Laptop datasets .",0
3410,4618,"ATAE - LSTM , IAN and RAM are [[ attention based models ]] , they << stably exceed >> the TD - LSTM method on [[ Restaurant and Laptop datasets ]] .",0
3411,4618,"ATAE - LSTM , IAN and RAM are attention based models , they << stably exceed >> the [[ TD - LSTM method ]] on [[ Restaurant and Laptop datasets ]] .",0
3412,4618,"[[ ATAE - LSTM , IAN and RAM ]] are [[ attention based models ]] , they stably exceed the TD - LSTM method << on >> Restaurant and Laptop datasets .",0
3413,4618,"[[ ATAE - LSTM , IAN and RAM ]] are attention based models , they stably exceed the [[ TD - LSTM method ]] << on >> Restaurant and Laptop datasets .",0
3414,4618,"[[ ATAE - LSTM , IAN and RAM ]] are attention based models , they stably exceed the TD - LSTM method << on >> [[ Restaurant and Laptop datasets ]] .",0
3415,4618,"ATAE - LSTM , IAN and RAM are [[ attention based models ]] , they stably exceed the [[ TD - LSTM method ]] << on >> Restaurant and Laptop datasets .",0
3416,4618,"ATAE - LSTM , IAN and RAM are [[ attention based models ]] , they stably exceed the TD - LSTM method << on >> [[ Restaurant and Laptop datasets ]] .",0
3417,4618,"ATAE - LSTM , IAN and RAM are attention based models , they stably exceed the [[ TD - LSTM method ]] << on >> [[ Restaurant and Laptop datasets ]] .",1
3418,4858,"Following , the [[ sentence ]] is << fed to >> along [[ short - term memory ( LSTM ) network ]] to propagate context among the constituent words .",1
3419,4858,"Following , the [[ sentence ]] is << fed to >> along short - term memory ( LSTM ) network to propagate [[ context ]] among the constituent words .",0
3420,4858,"Following , the [[ sentence ]] is << fed to >> along short - term memory ( LSTM ) network to propagate context among the [[ constituent words ]] .",0
3421,4858,"Following , the sentence is << fed to >> along [[ short - term memory ( LSTM ) network ]] to propagate [[ context ]] among the constituent words .",0
3422,4858,"Following , the sentence is << fed to >> along [[ short - term memory ( LSTM ) network ]] to propagate context among the [[ constituent words ]] .",0
3423,4858,"Following , the sentence is << fed to >> along short - term memory ( LSTM ) network to propagate [[ context ]] among the [[ constituent words ]] .",0
3424,4858,"Following , the [[ sentence ]] is fed to along [[ short - term memory ( LSTM ) network ]] << to propagate >> context among the constituent words .",0
3425,4858,"Following , the [[ sentence ]] is fed to along short - term memory ( LSTM ) network << to propagate >> [[ context ]] among the constituent words .",0
3426,4858,"Following , the [[ sentence ]] is fed to along short - term memory ( LSTM ) network << to propagate >> context among the [[ constituent words ]] .",0
3427,4858,"Following , the sentence is fed to along [[ short - term memory ( LSTM ) network ]] << to propagate >> [[ context ]] among the constituent words .",1
3428,4858,"Following , the sentence is fed to along [[ short - term memory ( LSTM ) network ]] << to propagate >> context among the [[ constituent words ]] .",0
3429,4858,"Following , the sentence is fed to along short - term memory ( LSTM ) network << to propagate >> [[ context ]] among the [[ constituent words ]] .",0
3430,4858,"Following , the [[ sentence ]] is fed to along [[ short - term memory ( LSTM ) network ]] to propagate context << among >> the constituent words .",0
3431,4858,"Following , the [[ sentence ]] is fed to along short - term memory ( LSTM ) network to propagate [[ context ]] << among >> the constituent words .",0
3432,4858,"Following , the [[ sentence ]] is fed to along short - term memory ( LSTM ) network to propagate context << among >> the [[ constituent words ]] .",0
3433,4858,"Following , the sentence is fed to along [[ short - term memory ( LSTM ) network ]] to propagate [[ context ]] << among >> the constituent words .",0
3434,4858,"Following , the sentence is fed to along [[ short - term memory ( LSTM ) network ]] to propagate context << among >> the [[ constituent words ]] .",0
3435,4858,"Following , the sentence is fed to along short - term memory ( LSTM ) network to propagate [[ context ]] << among >> the [[ constituent words ]] .",1
3436,4753,"<< For >> [[ AE - LSTM and ATAE - LSTM ]] , they capture [[ important information ]] in the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3437,4753,"<< For >> [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the [[ context ]] with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3438,4753,"<< For >> [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3439,4753,"<< For >> [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3440,4753,"<< For >> [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3441,4753,"<< For >> [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3442,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the [[ context ]] with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3443,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3444,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3445,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3446,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3447,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3448,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3449,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3450,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3451,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3452,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3453,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3454,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of [[ target ]] and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3455,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of [[ target ]] and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3456,4753,"<< For >> AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and generate [[ more reasonable representations ]] for [[ aspect - level sentiment classification ]] .",0
3457,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they << capture >> [[ important information ]] in the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",1
3458,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they << capture >> important information in the [[ context ]] with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3459,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they << capture >> important information in the context with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3460,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they << capture >> important information in the context with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3461,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they << capture >> important information in the context with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3462,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they << capture >> important information in the context with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3463,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> [[ important information ]] in the [[ context ]] with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3464,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> [[ important information ]] in the context with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3465,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> [[ important information ]] in the context with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3466,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> [[ important information ]] in the context with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3467,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> [[ important information ]] in the context with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3468,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the [[ context ]] with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3469,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the [[ context ]] with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3470,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the [[ context ]] with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3471,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the [[ context ]] with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3472,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the context with the [[ supervision ]] of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3473,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the context with the [[ supervision ]] of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3474,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the context with the [[ supervision ]] of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3475,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the context with the supervision of [[ target ]] and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3476,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the context with the supervision of [[ target ]] and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3477,4753,"For AE - LSTM and ATAE - LSTM , they << capture >> important information in the context with the supervision of target and generate [[ more reasonable representations ]] for [[ aspect - level sentiment classification ]] .",0
3478,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture [[ important information ]] << in >> the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3479,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information << in >> the [[ context ]] with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3480,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information << in >> the context with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3481,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information << in >> the context with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3482,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information << in >> the context with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3483,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information << in >> the context with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3484,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] << in >> the [[ context ]] with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",1
3485,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] << in >> the context with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3486,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] << in >> the context with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3487,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] << in >> the context with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3488,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] << in >> the context with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3489,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the [[ context ]] with the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3490,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the [[ context ]] with the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3491,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the [[ context ]] with the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3492,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the [[ context ]] with the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3493,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the context with the [[ supervision ]] of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3494,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the context with the [[ supervision ]] of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3495,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the context with the [[ supervision ]] of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3496,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the context with the supervision of [[ target ]] and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3497,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the context with the supervision of [[ target ]] and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3498,4753,"For AE - LSTM and ATAE - LSTM , they capture important information << in >> the context with the supervision of target and generate [[ more reasonable representations ]] for [[ aspect - level sentiment classification ]] .",0
3499,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture [[ important information ]] in the context << with >> the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3500,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the [[ context ]] << with >> the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3501,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context << with >> the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3502,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context << with >> the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3503,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context << with >> the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3504,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context << with >> the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3505,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the [[ context ]] << with >> the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",0
3506,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context << with >> the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",0
3507,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context << with >> the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3508,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context << with >> the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3509,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context << with >> the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3510,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] << with >> the [[ supervision ]] of target and generate more reasonable representations for aspect - level sentiment classification .",1
3511,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] << with >> the supervision of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3512,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] << with >> the supervision of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3513,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] << with >> the supervision of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3514,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context << with >> the [[ supervision ]] of [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3515,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context << with >> the [[ supervision ]] of target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3516,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context << with >> the [[ supervision ]] of target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3517,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context << with >> the supervision of [[ target ]] and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3518,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context << with >> the supervision of [[ target ]] and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3519,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context << with >> the supervision of target and generate [[ more reasonable representations ]] for [[ aspect - level sentiment classification ]] .",0
3520,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture [[ important information ]] in the context with the supervision << of >> target and generate more reasonable representations for aspect - level sentiment classification .",0
3521,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the [[ context ]] with the supervision << of >> target and generate more reasonable representations for aspect - level sentiment classification .",0
3522,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the [[ supervision ]] << of >> target and generate more reasonable representations for aspect - level sentiment classification .",0
3523,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision << of >> [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3524,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision << of >> target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3525,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision << of >> target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3526,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the [[ context ]] with the supervision << of >> target and generate more reasonable representations for aspect - level sentiment classification .",0
3527,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the [[ supervision ]] << of >> target and generate more reasonable representations for aspect - level sentiment classification .",0
3528,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision << of >> [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3529,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision << of >> target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3530,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision << of >> target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3531,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the [[ supervision ]] << of >> target and generate more reasonable representations for aspect - level sentiment classification .",0
3532,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision << of >> [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",0
3533,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision << of >> target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3534,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision << of >> target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3535,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] << of >> [[ target ]] and generate more reasonable representations for aspect - level sentiment classification .",1
3536,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] << of >> target and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3537,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] << of >> target and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3538,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision << of >> [[ target ]] and generate [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3539,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision << of >> [[ target ]] and generate more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3540,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision << of >> target and generate [[ more reasonable representations ]] for [[ aspect - level sentiment classification ]] .",0
3541,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture [[ important information ]] in the context with the supervision of target and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3542,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the [[ context ]] with the supervision of target and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3543,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the [[ supervision ]] of target and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3544,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of [[ target ]] and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3545,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of target and << generate >> [[ more reasonable representations ]] for aspect - level sentiment classification .",1
3546,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of target and << generate >> more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3547,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the [[ context ]] with the supervision of target and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3548,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the [[ supervision ]] of target and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3549,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of [[ target ]] and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3550,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of target and << generate >> [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3551,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of target and << generate >> more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3552,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the [[ supervision ]] of target and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3553,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of [[ target ]] and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3554,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of target and << generate >> [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3555,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of target and << generate >> more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3556,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of [[ target ]] and << generate >> more reasonable representations for aspect - level sentiment classification .",0
3557,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of target and << generate >> [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3558,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of target and << generate >> more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3559,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of [[ target ]] and << generate >> [[ more reasonable representations ]] for aspect - level sentiment classification .",0
3560,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of [[ target ]] and << generate >> more reasonable representations for [[ aspect - level sentiment classification ]] .",0
3561,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and << generate >> [[ more reasonable representations ]] for [[ aspect - level sentiment classification ]] .",0
3562,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture [[ important information ]] in the context with the supervision of target and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3563,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the [[ context ]] with the supervision of target and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3564,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the [[ supervision ]] of target and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3565,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of [[ target ]] and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3566,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of target and generate [[ more reasonable representations ]] << for >> aspect - level sentiment classification .",0
3567,4753,"For [[ AE - LSTM and ATAE - LSTM ]] , they capture important information in the context with the supervision of target and generate more reasonable representations << for >> [[ aspect - level sentiment classification ]] .",0
3568,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the [[ context ]] with the supervision of target and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3569,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the [[ supervision ]] of target and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3570,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of [[ target ]] and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3571,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of target and generate [[ more reasonable representations ]] << for >> aspect - level sentiment classification .",0
3572,4753,"For AE - LSTM and ATAE - LSTM , they capture [[ important information ]] in the context with the supervision of target and generate more reasonable representations << for >> [[ aspect - level sentiment classification ]] .",0
3573,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the [[ supervision ]] of target and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3574,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of [[ target ]] and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3575,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of target and generate [[ more reasonable representations ]] << for >> aspect - level sentiment classification .",0
3576,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the [[ context ]] with the supervision of target and generate more reasonable representations << for >> [[ aspect - level sentiment classification ]] .",0
3577,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of [[ target ]] and generate more reasonable representations << for >> aspect - level sentiment classification .",0
3578,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of target and generate [[ more reasonable representations ]] << for >> aspect - level sentiment classification .",0
3579,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the [[ supervision ]] of target and generate more reasonable representations << for >> [[ aspect - level sentiment classification ]] .",0
3580,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of [[ target ]] and generate [[ more reasonable representations ]] << for >> aspect - level sentiment classification .",0
3581,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of [[ target ]] and generate more reasonable representations << for >> [[ aspect - level sentiment classification ]] .",0
3582,4753,"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and generate [[ more reasonable representations ]] << for >> [[ aspect - level sentiment classification ]] .",1
3583,6002,"We << call >> our [[ summarization system ]] [[ Re 3 Sum ]] , which consists of three modules : Retrieve , Rerank and Rewrite .",0
3584,6002,"We << call >> our [[ summarization system ]] Re 3 Sum , which consists of [[ three modules ]] : Retrieve , Rerank and Rewrite .",0
3585,6002,"We << call >> our [[ summarization system ]] Re 3 Sum , which consists of three modules : [[ Retrieve ]] , Rerank and Rewrite .",0
3586,6002,"We << call >> our [[ summarization system ]] Re 3 Sum , which consists of three modules : Retrieve , [[ Rerank ]] and Rewrite .",0
3587,6002,"We << call >> our [[ summarization system ]] Re 3 Sum , which consists of three modules : Retrieve , Rerank and [[ Rewrite ]] .",0
3588,6002,"We << call >> our summarization system [[ Re 3 Sum ]] , which consists of [[ three modules ]] : Retrieve , Rerank and Rewrite .",0
3589,6002,"We << call >> our summarization system [[ Re 3 Sum ]] , which consists of three modules : [[ Retrieve ]] , Rerank and Rewrite .",0
3590,6002,"We << call >> our summarization system [[ Re 3 Sum ]] , which consists of three modules : Retrieve , [[ Rerank ]] and Rewrite .",0
3591,6002,"We << call >> our summarization system [[ Re 3 Sum ]] , which consists of three modules : Retrieve , Rerank and [[ Rewrite ]] .",0
3592,6002,"We << call >> our summarization system Re 3 Sum , which consists of [[ three modules ]] : [[ Retrieve ]] , Rerank and Rewrite .",0
3593,6002,"We << call >> our summarization system Re 3 Sum , which consists of [[ three modules ]] : Retrieve , [[ Rerank ]] and Rewrite .",0
3594,6002,"We << call >> our summarization system Re 3 Sum , which consists of [[ three modules ]] : Retrieve , Rerank and [[ Rewrite ]] .",0
3595,6002,"We << call >> our summarization system Re 3 Sum , which consists of three modules : [[ Retrieve ]] , [[ Rerank ]] and Rewrite .",0
3596,6002,"We << call >> our summarization system Re 3 Sum , which consists of three modules : [[ Retrieve ]] , Rerank and [[ Rewrite ]] .",0
3597,6002,"We << call >> our summarization system Re 3 Sum , which consists of three modules : Retrieve , [[ Rerank ]] and [[ Rewrite ]] .",0
3598,6002,"We call our [[ summarization system ]] [[ Re 3 Sum ]] , which << consists of >> three modules : Retrieve , Rerank and Rewrite .",0
3599,6002,"We call our [[ summarization system ]] Re 3 Sum , which << consists of >> [[ three modules ]] : Retrieve , Rerank and Rewrite .",0
3600,6002,"We call our [[ summarization system ]] Re 3 Sum , which << consists of >> three modules : [[ Retrieve ]] , Rerank and Rewrite .",0
3601,6002,"We call our [[ summarization system ]] Re 3 Sum , which << consists of >> three modules : Retrieve , [[ Rerank ]] and Rewrite .",0
3602,6002,"We call our [[ summarization system ]] Re 3 Sum , which << consists of >> three modules : Retrieve , Rerank and [[ Rewrite ]] .",0
3603,6002,"We call our summarization system [[ Re 3 Sum ]] , which << consists of >> [[ three modules ]] : Retrieve , Rerank and Rewrite .",1
3604,6002,"We call our summarization system [[ Re 3 Sum ]] , which << consists of >> three modules : [[ Retrieve ]] , Rerank and Rewrite .",0
3605,6002,"We call our summarization system [[ Re 3 Sum ]] , which << consists of >> three modules : Retrieve , [[ Rerank ]] and Rewrite .",0
3606,6002,"We call our summarization system [[ Re 3 Sum ]] , which << consists of >> three modules : Retrieve , Rerank and [[ Rewrite ]] .",0
3607,6002,"We call our summarization system Re 3 Sum , which << consists of >> [[ three modules ]] : [[ Retrieve ]] , Rerank and Rewrite .",0
3608,6002,"We call our summarization system Re 3 Sum , which << consists of >> [[ three modules ]] : Retrieve , [[ Rerank ]] and Rewrite .",0
3609,6002,"We call our summarization system Re 3 Sum , which << consists of >> [[ three modules ]] : Retrieve , Rerank and [[ Rewrite ]] .",0
3610,6002,"We call our summarization system Re 3 Sum , which << consists of >> three modules : [[ Retrieve ]] , [[ Rerank ]] and Rewrite .",0
3611,6002,"We call our summarization system Re 3 Sum , which << consists of >> three modules : [[ Retrieve ]] , Rerank and [[ Rewrite ]] .",0
3612,6002,"We call our summarization system Re 3 Sum , which << consists of >> three modules : Retrieve , [[ Rerank ]] and [[ Rewrite ]] .",0
3613,5703,The results on Chinese Poems << indicate >> that [[ LeakGAN ]] successfully handles the [[ short text generation tasks ]] .,0
3614,5703,The results on Chinese Poems indicate that [[ LeakGAN ]] << successfully handles >> the [[ short text generation tasks ]] .,1
3615,5845,"As to the baselines for [[ Gigaword ]] , [[ ABS and ABS + ]] << are >> the models with local attention and handcrafted features .",0
3616,5845,"As to the baselines for [[ Gigaword ]] , ABS and ABS + << are >> the [[ models ]] with local attention and handcrafted features .",0
3617,5845,"As to the baselines for [[ Gigaword ]] , ABS and ABS + << are >> the models with [[ local attention and handcrafted features ]] .",0
3618,5845,"As to the baselines for Gigaword , [[ ABS and ABS + ]] << are >> the [[ models ]] with local attention and handcrafted features .",1
3619,5845,"As to the baselines for Gigaword , [[ ABS and ABS + ]] << are >> the models with [[ local attention and handcrafted features ]] .",0
3620,5845,"As to the baselines for Gigaword , ABS and ABS + << are >> the [[ models ]] with [[ local attention and handcrafted features ]] .",0
3621,5845,"As to the baselines for [[ Gigaword ]] , [[ ABS and ABS + ]] are the models << with >> local attention and handcrafted features .",0
3622,5845,"As to the baselines for [[ Gigaword ]] , ABS and ABS + are the [[ models ]] << with >> local attention and handcrafted features .",0
3623,5845,"As to the baselines for [[ Gigaword ]] , ABS and ABS + are the models << with >> [[ local attention and handcrafted features ]] .",0
3624,5845,"As to the baselines for Gigaword , [[ ABS and ABS + ]] are the [[ models ]] << with >> local attention and handcrafted features .",0
3625,5845,"As to the baselines for Gigaword , [[ ABS and ABS + ]] are the models << with >> [[ local attention and handcrafted features ]] .",0
3626,5845,"As to the baselines for Gigaword , ABS and ABS + are the [[ models ]] << with >> [[ local attention and handcrafted features ]] .",1
3627,5919,We << apply >> [[ ROUGE - 2 RAML training ]] for [[ seq2seq model ]] .,0
3628,5919,We apply [[ ROUGE - 2 RAML training ]] << for >> [[ seq2seq model ]] .,1
3629,3614,"We << find >> that : ( 1 ) by only [[ training ]] [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3630,3614,"We << find >> that : ( 1 ) by only [[ training ]] our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3631,3614,"We << find >> that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3632,3614,"We << find >> that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3633,3614,"We << find >> that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3634,3614,"We << find >> that : ( 1 ) by only training [[ our logistic regression model ]] on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3635,3614,"We << find >> that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3636,3614,"We << find >> that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3637,3614,"We << find >> that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3638,3614,"We << find >> that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3639,3614,"We << find >> that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3640,3614,"We << find >> that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3641,3614,"We << find >> that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3642,3614,"We << find >> that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3643,3614,"We << find >> that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3644,3614,"We find that : ( 1 ) by only [[ training ]] [[ our logistic regression model ]] << on >> TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3645,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model << on >> [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3646,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model << on >> TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3647,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model << on >> TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3648,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model << on >> TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3649,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] << on >> [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",1
3650,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] << on >> TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",1
3651,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] << on >> TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3652,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] << on >> TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3653,3614,"We find that : ( 1 ) by only training our logistic regression model << on >> [[ TACRED ]] ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3654,3614,"We find that : ( 1 ) by only training our logistic regression model << on >> [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3655,3614,"We find that : ( 1 ) by only training our logistic regression model << on >> [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3656,3614,"We find that : ( 1 ) by only training our logistic regression model << on >> TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3657,3614,"We find that : ( 1 ) by only training our logistic regression model << on >> TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3658,3614,"We find that : ( 1 ) by only training our logistic regression model << on >> TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3659,3614,"We find that : ( 1 ) by only [[ training ]] [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples << used in >> the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3660,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples << used in >> the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3661,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] << used in >> the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3662,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples << used in >> the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3663,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples << used in >> the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3664,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples << used in >> the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3665,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] << used in >> the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3666,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples << used in >> the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3667,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples << used in >> the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3668,3614,"We find that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the [[ 2 million bootstrapped examples ]] << used in >> the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3669,3614,"We find that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples << used in >> the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3670,3614,"We find that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples << used in >> the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3671,3614,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] << used in >> the [[ 2015 Stanford system ]] ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",1
3672,3614,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] << used in >> the 2015 Stanford system ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3673,3614,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples << used in >> the [[ 2015 Stanford system ]] ) and combining it with [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3674,3614,"We find that : ( 1 ) by only [[ training ]] [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3675,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3676,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3677,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3678,3614,"We find that : ( 1 ) by only [[ training ]] our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and << combining it with >> [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3679,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3680,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3681,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3682,3614,"We find that : ( 1 ) by only training [[ our logistic regression model ]] on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and << combining it with >> [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3683,3614,"We find that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3684,3614,"We find that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3685,3614,"We find that : ( 1 ) by only training our logistic regression model on [[ TACRED ]] ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and << combining it with >> [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3686,3614,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the [[ 2015 Stanford system ]] ) and << combining it with >> patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3687,3614,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the [[ 2 million bootstrapped examples ]] used in the 2015 Stanford system ) and << combining it with >> [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",0
3688,3614,"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the [[ 2015 Stanford system ]] ) and << combining it with >> [[ patterns ]] , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",1
3689,1806,"First , we << observe >> [[ general improvements ]] when using [[ labels ]] that closely align with the task .",0
3690,1806,"First , we << observe >> [[ general improvements ]] when using labels that [[ closely align ]] with the task .",0
3691,1806,"First , we << observe >> [[ general improvements ]] when using labels that closely align with the [[ task ]] .",0
3692,1806,"First , we << observe >> general improvements when using [[ labels ]] that [[ closely align ]] with the task .",0
3693,1806,"First , we << observe >> general improvements when using [[ labels ]] that closely align with the [[ task ]] .",0
3694,1806,"First , we << observe >> general improvements when using labels that [[ closely align ]] with the [[ task ]] .",0
3695,1806,"First , we observe [[ general improvements ]] << when using >> [[ labels ]] that closely align with the task .",1
3696,1806,"First , we observe [[ general improvements ]] << when using >> labels that [[ closely align ]] with the task .",0
3697,1806,"First , we observe [[ general improvements ]] << when using >> labels that closely align with the [[ task ]] .",0
3698,1806,"First , we observe general improvements << when using >> [[ labels ]] that [[ closely align ]] with the task .",0
3699,1806,"First , we observe general improvements << when using >> [[ labels ]] that closely align with the [[ task ]] .",0
3700,1806,"First , we observe general improvements << when using >> labels that [[ closely align ]] with the [[ task ]] .",0
3701,1806,"First , we observe [[ general improvements ]] when using [[ labels ]] << that >> closely align with the task .",0
3702,1806,"First , we observe [[ general improvements ]] when using labels << that >> [[ closely align ]] with the task .",0
3703,1806,"First , we observe [[ general improvements ]] when using labels << that >> closely align with the [[ task ]] .",0
3704,1806,"First , we observe general improvements when using [[ labels ]] << that >> [[ closely align ]] with the task .",1
3705,1806,"First , we observe general improvements when using [[ labels ]] << that >> closely align with the [[ task ]] .",0
3706,1806,"First , we observe general improvements when using labels << that >> [[ closely align ]] with the [[ task ]] .",0
3707,1806,"First , we observe [[ general improvements ]] when using [[ labels ]] that closely align << with >> the task .",0
3708,1806,"First , we observe [[ general improvements ]] when using labels that [[ closely align ]] << with >> the task .",0
3709,1806,"First , we observe [[ general improvements ]] when using labels that closely align << with >> the [[ task ]] .",0
3710,1806,"First , we observe general improvements when using [[ labels ]] that [[ closely align ]] << with >> the task .",0
3711,1806,"First , we observe general improvements when using [[ labels ]] that closely align << with >> the [[ task ]] .",0
3712,1806,"First , we observe general improvements when using labels that [[ closely align ]] << with >> the [[ task ]] .",1
3713,1018,We << find that >> [[ FLOW ]] is a [[ critical component ]] .,0
3714,1018,We find that [[ FLOW ]] << is >> a [[ critical component ]] .,1
3715,1693,[[ RoBERTa - Large TANDA ]] << with >> [[ ASNQ ]] ?,1
3716,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM << extended >> [[ AE - LSTM ]] by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",1
3717,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM << extended >> AE - LSTM by appending the [[ aspect embedding ]] to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",0
3718,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM << extended >> AE - LSTM by appending the aspect embedding to [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3719,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM << extended >> AE - LSTM by appending the aspect embedding to each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3720,4368,"ATAE - LSTM : ATAE - LSTM << extended >> [[ AE - LSTM ]] by appending the [[ aspect embedding ]] to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",0
3721,4368,"ATAE - LSTM : ATAE - LSTM << extended >> [[ AE - LSTM ]] by appending the aspect embedding to [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3722,4368,"ATAE - LSTM : ATAE - LSTM << extended >> [[ AE - LSTM ]] by appending the aspect embedding to each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3723,4368,"ATAE - LSTM : ATAE - LSTM << extended >> AE - LSTM by appending the [[ aspect embedding ]] to [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3724,4368,"ATAE - LSTM : ATAE - LSTM << extended >> AE - LSTM by appending the [[ aspect embedding ]] to each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3725,4368,"ATAE - LSTM : ATAE - LSTM << extended >> AE - LSTM by appending the aspect embedding to [[ each word embedding ]] so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3726,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended [[ AE - LSTM ]] << by appending >> the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",0
3727,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM << by appending >> the [[ aspect embedding ]] to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",0
3728,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM << by appending >> the aspect embedding to [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3729,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM << by appending >> the aspect embedding to each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3730,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] << by appending >> the [[ aspect embedding ]] to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",1
3731,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] << by appending >> the aspect embedding to [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3732,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] << by appending >> the aspect embedding to each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3733,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM << by appending >> the [[ aspect embedding ]] to [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3734,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM << by appending >> the [[ aspect embedding ]] to each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3735,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM << by appending >> the aspect embedding to [[ each word embedding ]] so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3736,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended [[ AE - LSTM ]] by appending the aspect embedding << to >> each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",0
3737,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM by appending the [[ aspect embedding ]] << to >> each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",0
3738,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM by appending the aspect embedding << to >> [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3739,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM by appending the aspect embedding << to >> each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3740,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] by appending the [[ aspect embedding ]] << to >> each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .",0
3741,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] by appending the aspect embedding << to >> [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",0
3742,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] by appending the aspect embedding << to >> each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3743,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the [[ aspect embedding ]] << to >> [[ each word embedding ]] so as to represent the input sentence , which highlights the role of aspect embedding .",1
3744,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the [[ aspect embedding ]] << to >> each word embedding so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3745,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding << to >> [[ each word embedding ]] so as to represent the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3746,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended [[ AE - LSTM ]] by appending the aspect embedding to each word embedding so as << to represent >> the input sentence , which highlights the role of aspect embedding .",0
3747,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM by appending the [[ aspect embedding ]] to each word embedding so as << to represent >> the input sentence , which highlights the role of aspect embedding .",0
3748,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to [[ each word embedding ]] so as << to represent >> the input sentence , which highlights the role of aspect embedding .",0
3749,4368,"[[ ATAE - LSTM ]] : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as << to represent >> the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3750,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] by appending the [[ aspect embedding ]] to each word embedding so as << to represent >> the input sentence , which highlights the role of aspect embedding .",0
3751,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] by appending the aspect embedding to [[ each word embedding ]] so as << to represent >> the input sentence , which highlights the role of aspect embedding .",0
3752,4368,"ATAE - LSTM : ATAE - LSTM extended [[ AE - LSTM ]] by appending the aspect embedding to each word embedding so as << to represent >> the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3753,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the [[ aspect embedding ]] to [[ each word embedding ]] so as << to represent >> the input sentence , which highlights the role of aspect embedding .",0
3754,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the [[ aspect embedding ]] to each word embedding so as << to represent >> the [[ input sentence ]] , which highlights the role of aspect embedding .",0
3755,4368,"ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to [[ each word embedding ]] so as << to represent >> the [[ input sentence ]] , which highlights the role of aspect embedding .",1
3756,5589,"The regulator is << built on >> a [[ phoneme duration predictor ]] , which predicts the [[ duration of each phoneme ]] .",0
3757,5589,"The regulator is built on a [[ phoneme duration predictor ]] , which << predicts >> the [[ duration of each phoneme ]] .",1
3758,519,"[[ Dropout layer ]] is also << applied on >> the [[ output ]] of each layer of MLP , with dropout rate set to 0.1 .",1
3759,519,"[[ Dropout layer ]] is also << applied on >> the output of [[ each layer of MLP ]] , with dropout rate set to 0.1 .",0
3760,519,"[[ Dropout layer ]] is also << applied on >> the output of each layer of MLP , with [[ dropout rate ]] set to 0.1 .",0
3761,519,"[[ Dropout layer ]] is also << applied on >> the output of each layer of MLP , with dropout rate set to [[ 0.1 ]] .",0
3762,519,"Dropout layer is also << applied on >> the [[ output ]] of [[ each layer of MLP ]] , with dropout rate set to 0.1 .",0
3763,519,"Dropout layer is also << applied on >> the [[ output ]] of each layer of MLP , with [[ dropout rate ]] set to 0.1 .",0
3764,519,"Dropout layer is also << applied on >> the [[ output ]] of each layer of MLP , with dropout rate set to [[ 0.1 ]] .",0
3765,519,"Dropout layer is also << applied on >> the output of [[ each layer of MLP ]] , with [[ dropout rate ]] set to 0.1 .",0
3766,519,"Dropout layer is also << applied on >> the output of [[ each layer of MLP ]] , with dropout rate set to [[ 0.1 ]] .",0
3767,519,"Dropout layer is also << applied on >> the output of each layer of MLP , with [[ dropout rate ]] set to [[ 0.1 ]] .",0
3768,519,"[[ Dropout layer ]] is also applied on the [[ output ]] << of >> each layer of MLP , with dropout rate set to 0.1 .",0
3769,519,"[[ Dropout layer ]] is also applied on the output << of >> [[ each layer of MLP ]] , with dropout rate set to 0.1 .",0
3770,519,"[[ Dropout layer ]] is also applied on the output << of >> each layer of MLP , with [[ dropout rate ]] set to 0.1 .",0
3771,519,"[[ Dropout layer ]] is also applied on the output << of >> each layer of MLP , with dropout rate set to [[ 0.1 ]] .",0
3772,519,"Dropout layer is also applied on the [[ output ]] << of >> [[ each layer of MLP ]] , with dropout rate set to 0.1 .",1
3773,519,"Dropout layer is also applied on the [[ output ]] << of >> each layer of MLP , with [[ dropout rate ]] set to 0.1 .",0
3774,519,"Dropout layer is also applied on the [[ output ]] << of >> each layer of MLP , with dropout rate set to [[ 0.1 ]] .",0
3775,519,"Dropout layer is also applied on the output << of >> [[ each layer of MLP ]] , with [[ dropout rate ]] set to 0.1 .",0
3776,519,"Dropout layer is also applied on the output << of >> [[ each layer of MLP ]] , with dropout rate set to [[ 0.1 ]] .",0
3777,519,"Dropout layer is also applied on the output << of >> each layer of MLP , with [[ dropout rate ]] set to [[ 0.1 ]] .",0
3778,519,"[[ Dropout layer ]] is also applied on the [[ output ]] of each layer of MLP , << with >> dropout rate set to 0.1 .",0
3779,519,"[[ Dropout layer ]] is also applied on the output of [[ each layer of MLP ]] , << with >> dropout rate set to 0.1 .",0
3780,519,"[[ Dropout layer ]] is also applied on the output of each layer of MLP , << with >> [[ dropout rate ]] set to 0.1 .",0
3781,519,"[[ Dropout layer ]] is also applied on the output of each layer of MLP , << with >> dropout rate set to [[ 0.1 ]] .",0
3782,519,"Dropout layer is also applied on the [[ output ]] of [[ each layer of MLP ]] , << with >> dropout rate set to 0.1 .",0
3783,519,"Dropout layer is also applied on the [[ output ]] of each layer of MLP , << with >> [[ dropout rate ]] set to 0.1 .",1
3784,519,"Dropout layer is also applied on the [[ output ]] of each layer of MLP , << with >> dropout rate set to [[ 0.1 ]] .",0
3785,519,"Dropout layer is also applied on the output of [[ each layer of MLP ]] , << with >> [[ dropout rate ]] set to 0.1 .",0
3786,519,"Dropout layer is also applied on the output of [[ each layer of MLP ]] , << with >> dropout rate set to [[ 0.1 ]] .",0
3787,519,"Dropout layer is also applied on the output of each layer of MLP , << with >> [[ dropout rate ]] set to [[ 0.1 ]] .",0
3788,519,"[[ Dropout layer ]] is also applied on the [[ output ]] of each layer of MLP , with dropout rate << set to >> 0.1 .",0
3789,519,"[[ Dropout layer ]] is also applied on the output of [[ each layer of MLP ]] , with dropout rate << set to >> 0.1 .",0
3790,519,"[[ Dropout layer ]] is also applied on the output of each layer of MLP , with [[ dropout rate ]] << set to >> 0.1 .",0
3791,519,"[[ Dropout layer ]] is also applied on the output of each layer of MLP , with dropout rate << set to >> [[ 0.1 ]] .",0
3792,519,"Dropout layer is also applied on the [[ output ]] of [[ each layer of MLP ]] , with dropout rate << set to >> 0.1 .",0
3793,519,"Dropout layer is also applied on the [[ output ]] of each layer of MLP , with [[ dropout rate ]] << set to >> 0.1 .",0
3794,519,"Dropout layer is also applied on the [[ output ]] of each layer of MLP , with dropout rate << set to >> [[ 0.1 ]] .",0
3795,519,"Dropout layer is also applied on the output of [[ each layer of MLP ]] , with [[ dropout rate ]] << set to >> 0.1 .",0
3796,519,"Dropout layer is also applied on the output of [[ each layer of MLP ]] , with dropout rate << set to >> [[ 0.1 ]] .",0
3797,519,"Dropout layer is also applied on the output of each layer of MLP , with [[ dropout rate ]] << set to >> [[ 0.1 ]] .",1
3798,814,"All of these [[ innovations ]] are << integrated into >> a [[ new end - to - end structure ]] called FusionNet in , with details described in Section 3 .",1
3799,814,"All of these [[ innovations ]] are << integrated into >> a new end - to - end structure called [[ FusionNet ]] in , with details described in Section 3 .",0
3800,814,"All of these innovations are << integrated into >> a [[ new end - to - end structure ]] called [[ FusionNet ]] in , with details described in Section 3 .",0
3801,814,"All of these [[ innovations ]] are integrated into a [[ new end - to - end structure ]] << called >> FusionNet in , with details described in Section 3 .",0
3802,814,"All of these [[ innovations ]] are integrated into a new end - to - end structure << called >> [[ FusionNet ]] in , with details described in Section 3 .",0
3803,814,"All of these innovations are integrated into a [[ new end - to - end structure ]] << called >> [[ FusionNet ]] in , with details described in Section 3 .",1
3804,4830,"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we << design >> a [[ contextpreserving mechanism ]] to contextualize the [[ generated target - specific word representations ]] .",0
3805,4830,"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we design a [[ contextpreserving mechanism ]] << to contextualize >> the [[ generated target - specific word representations ]] .",1
3806,1522,"An important insight from the work of is to << decompose >> a [[ function ]] for [[ relational reasoning ]] into two components or "" modules "" :",0
3807,1522,"An important insight from the work of is to << decompose >> a [[ function ]] for relational reasoning into [[ two components ]] or "" modules "" :",0
3808,1522,"An important insight from the work of is to << decompose >> a function for [[ relational reasoning ]] into [[ two components ]] or "" modules "" :",0
3809,1522,"An important insight from the work of is to decompose a [[ function ]] << for >> [[ relational reasoning ]] into two components or "" modules "" :",1
3810,1522,"An important insight from the work of is to decompose a [[ function ]] << for >> relational reasoning into [[ two components ]] or "" modules "" :",0
3811,1522,"An important insight from the work of is to decompose a function << for >> [[ relational reasoning ]] into [[ two components ]] or "" modules "" :",0
3812,1522,"An important insight from the work of is to decompose a [[ function ]] for [[ relational reasoning ]] << into >> two components or "" modules "" :",0
3813,1522,"An important insight from the work of is to decompose a [[ function ]] for relational reasoning << into >> [[ two components ]] or "" modules "" :",1
3814,1522,"An important insight from the work of is to decompose a function for [[ relational reasoning ]] << into >> [[ two components ]] or "" modules "" :",0
3815,1638,We use [[ pre-trained 300 - D Glove 840B vectors ]] << to initialize >> [[ word embeddings ]] .,1
3816,6027,"Likewise , << comparing >> [[ Max and First ]] , we observe that the [[ improving capacity ]] of the Retrieve module is high .",0
3817,6027,"Likewise , << comparing >> [[ Max and First ]] , we observe that the improving capacity of the [[ Retrieve module ]] is high .",0
3818,6027,"Likewise , << comparing >> [[ Max and First ]] , we observe that the improving capacity of the Retrieve module is [[ high ]] .",0
3819,6027,"Likewise , << comparing >> Max and First , we observe that the [[ improving capacity ]] of the [[ Retrieve module ]] is high .",0
3820,6027,"Likewise , << comparing >> Max and First , we observe that the [[ improving capacity ]] of the Retrieve module is [[ high ]] .",0
3821,6027,"Likewise , << comparing >> Max and First , we observe that the improving capacity of the [[ Retrieve module ]] is [[ high ]] .",0
3822,6027,"Likewise , comparing [[ Max and First ]] , we << observe that >> the [[ improving capacity ]] of the Retrieve module is high .",1
3823,6027,"Likewise , comparing [[ Max and First ]] , we << observe that >> the improving capacity of the [[ Retrieve module ]] is high .",0
3824,6027,"Likewise , comparing [[ Max and First ]] , we << observe that >> the improving capacity of the Retrieve module is [[ high ]] .",0
3825,6027,"Likewise , comparing Max and First , we << observe that >> the [[ improving capacity ]] of the [[ Retrieve module ]] is high .",0
3826,6027,"Likewise , comparing Max and First , we << observe that >> the [[ improving capacity ]] of the Retrieve module is [[ high ]] .",0
3827,6027,"Likewise , comparing Max and First , we << observe that >> the improving capacity of the [[ Retrieve module ]] is [[ high ]] .",0
3828,6027,"Likewise , comparing [[ Max and First ]] , we observe that the [[ improving capacity ]] << of >> the Retrieve module is high .",0
3829,6027,"Likewise , comparing [[ Max and First ]] , we observe that the improving capacity << of >> the [[ Retrieve module ]] is high .",0
3830,6027,"Likewise , comparing [[ Max and First ]] , we observe that the improving capacity << of >> the Retrieve module is [[ high ]] .",0
3831,6027,"Likewise , comparing Max and First , we observe that the [[ improving capacity ]] << of >> the [[ Retrieve module ]] is high .",1
3832,6027,"Likewise , comparing Max and First , we observe that the [[ improving capacity ]] << of >> the Retrieve module is [[ high ]] .",0
3833,6027,"Likewise , comparing Max and First , we observe that the improving capacity << of >> the [[ Retrieve module ]] is [[ high ]] .",0
3834,6027,"Likewise , comparing [[ Max and First ]] , we observe that the [[ improving capacity ]] of the Retrieve module << is >> high .",0
3835,6027,"Likewise , comparing [[ Max and First ]] , we observe that the improving capacity of the [[ Retrieve module ]] << is >> high .",0
3836,6027,"Likewise , comparing [[ Max and First ]] , we observe that the improving capacity of the Retrieve module << is >> [[ high ]] .",0
3837,6027,"Likewise , comparing Max and First , we observe that the [[ improving capacity ]] of the [[ Retrieve module ]] << is >> high .",0
3838,6027,"Likewise , comparing Max and First , we observe that the [[ improving capacity ]] of the Retrieve module << is >> [[ high ]] .",0
3839,6027,"Likewise , comparing Max and First , we observe that the improving capacity of the [[ Retrieve module ]] << is >> [[ high ]] .",1
3840,3526,We can << observe that >> [[ adding ]] [[ either attention guided layers or densely connected layers ]] improves the performance of the model .,0
3841,3526,We can << observe that >> [[ adding ]] either attention guided layers or densely connected layers improves the [[ performance ]] of the model .,0
3842,3526,We can << observe that >> [[ adding ]] either attention guided layers or densely connected layers improves the performance of the [[ model ]] .,0
3843,3526,We can << observe that >> adding [[ either attention guided layers or densely connected layers ]] improves the [[ performance ]] of the model .,0
3844,3526,We can << observe that >> adding [[ either attention guided layers or densely connected layers ]] improves the performance of the [[ model ]] .,0
3845,3526,We can << observe that >> adding either attention guided layers or densely connected layers improves the [[ performance ]] of the [[ model ]] .,0
3846,3526,We can observe that [[ adding ]] [[ either attention guided layers or densely connected layers ]] << improves >> the performance of the model .,0
3847,3526,We can observe that [[ adding ]] either attention guided layers or densely connected layers << improves >> the [[ performance ]] of the model .,0
3848,3526,We can observe that [[ adding ]] either attention guided layers or densely connected layers << improves >> the performance of the [[ model ]] .,0
3849,3526,We can observe that adding [[ either attention guided layers or densely connected layers ]] << improves >> the [[ performance ]] of the model .,1
3850,3526,We can observe that adding [[ either attention guided layers or densely connected layers ]] << improves >> the performance of the [[ model ]] .,0
3851,3526,We can observe that adding either attention guided layers or densely connected layers << improves >> the [[ performance ]] of the [[ model ]] .,0
3852,3526,We can observe that [[ adding ]] [[ either attention guided layers or densely connected layers ]] improves the performance << of >> the model .,0
3853,3526,We can observe that [[ adding ]] either attention guided layers or densely connected layers improves the [[ performance ]] << of >> the model .,0
3854,3526,We can observe that [[ adding ]] either attention guided layers or densely connected layers improves the performance << of >> the [[ model ]] .,0
3855,3526,We can observe that adding [[ either attention guided layers or densely connected layers ]] improves the [[ performance ]] << of >> the model .,0
3856,3526,We can observe that adding [[ either attention guided layers or densely connected layers ]] improves the performance << of >> the [[ model ]] .,0
3857,3526,We can observe that adding either attention guided layers or densely connected layers improves the [[ performance ]] << of >> the [[ model ]] .,1
3858,5217,( 3 ) [[ Transfer of the embedding layer ]] << is >> [[ more helpful ]] on D3 and D4 .,1
3859,5217,( 3 ) [[ Transfer of the embedding layer ]] << is >> more helpful on [[ D3 and D4 ]] .,0
3860,5217,( 3 ) Transfer of the embedding layer << is >> [[ more helpful ]] on [[ D3 and D4 ]] .,0
3861,5217,( 3 ) [[ Transfer of the embedding layer ]] is [[ more helpful ]] << on >> D3 and D4 .,0
3862,5217,( 3 ) [[ Transfer of the embedding layer ]] is more helpful << on >> [[ D3 and D4 ]] .,0
3863,5217,( 3 ) Transfer of the embedding layer is [[ more helpful ]] << on >> [[ D3 and D4 ]] .,1
3864,3734,2 . We << introduce >> a [[ novel pair - wise margin - based objective function ]] that proves [[ superior ]] to standard loss functions .,0
3865,3734,2 . We << introduce >> a [[ novel pair - wise margin - based objective function ]] that proves superior to [[ standard loss functions ]] .,0
3866,3734,2 . We << introduce >> a novel pair - wise margin - based objective function that proves [[ superior ]] to [[ standard loss functions ]] .,0
3867,3734,2 . We introduce a [[ novel pair - wise margin - based objective function ]] that << proves >> [[ superior ]] to standard loss functions .,1
3868,3734,2 . We introduce a [[ novel pair - wise margin - based objective function ]] that << proves >> superior to [[ standard loss functions ]] .,0
3869,3734,2 . We introduce a novel pair - wise margin - based objective function that << proves >> [[ superior ]] to [[ standard loss functions ]] .,0
3870,3734,2 . We introduce a [[ novel pair - wise margin - based objective function ]] that proves [[ superior ]] << to >> standard loss functions .,0
3871,3734,2 . We introduce a [[ novel pair - wise margin - based objective function ]] that proves superior << to >> [[ standard loss functions ]] .,0
3872,3734,2 . We introduce a novel pair - wise margin - based objective function that proves [[ superior ]] << to >> [[ standard loss functions ]] .,1
3873,3046,It << shows >> the [[ question attention ]] provides [[ slight improvement ]] .,0
3874,3046,It shows the [[ question attention ]] << provides >> [[ slight improvement ]] .,1
3875,1883,"Compared to the [[ attention - based models ]] , [[ multi-head attention ]] and DiSAN , ReSAN << uses >> a similar number of parameters with better test performance and less time cost .",0
3876,1883,"Compared to the [[ attention - based models ]] , multi-head attention and [[ DiSAN ]] , ReSAN << uses >> a similar number of parameters with better test performance and less time cost .",0
3877,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , [[ ReSAN ]] << uses >> a similar number of parameters with better test performance and less time cost .",0
3878,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , ReSAN << uses >> a [[ similar number of parameters ]] with better test performance and less time cost .",0
3879,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , ReSAN << uses >> a similar number of parameters with [[ better test performance ]] and less time cost .",0
3880,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , ReSAN << uses >> a similar number of parameters with better test performance and [[ less time cost ]] .",0
3881,1883,"Compared to the attention - based models , [[ multi-head attention ]] and [[ DiSAN ]] , ReSAN << uses >> a similar number of parameters with better test performance and less time cost .",0
3882,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , [[ ReSAN ]] << uses >> a similar number of parameters with better test performance and less time cost .",0
3883,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , ReSAN << uses >> a [[ similar number of parameters ]] with better test performance and less time cost .",0
3884,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , ReSAN << uses >> a similar number of parameters with [[ better test performance ]] and less time cost .",0
3885,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , ReSAN << uses >> a similar number of parameters with better test performance and [[ less time cost ]] .",0
3886,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , [[ ReSAN ]] << uses >> a similar number of parameters with better test performance and less time cost .",0
3887,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , ReSAN << uses >> a [[ similar number of parameters ]] with better test performance and less time cost .",0
3888,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , ReSAN << uses >> a similar number of parameters with [[ better test performance ]] and less time cost .",0
3889,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , ReSAN << uses >> a similar number of parameters with better test performance and [[ less time cost ]] .",0
3890,1883,"Compared to the attention - based models , multi-head attention and DiSAN , [[ ReSAN ]] << uses >> a [[ similar number of parameters ]] with better test performance and less time cost .",1
3891,1883,"Compared to the attention - based models , multi-head attention and DiSAN , [[ ReSAN ]] << uses >> a similar number of parameters with [[ better test performance ]] and less time cost .",0
3892,1883,"Compared to the attention - based models , multi-head attention and DiSAN , [[ ReSAN ]] << uses >> a similar number of parameters with better test performance and [[ less time cost ]] .",0
3893,1883,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN << uses >> a [[ similar number of parameters ]] with [[ better test performance ]] and less time cost .",0
3894,1883,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN << uses >> a [[ similar number of parameters ]] with better test performance and [[ less time cost ]] .",0
3895,1883,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN << uses >> a similar number of parameters with [[ better test performance ]] and [[ less time cost ]] .",0
3896,1883,"Compared to the [[ attention - based models ]] , [[ multi-head attention ]] and DiSAN , ReSAN uses a similar number of parameters << with >> better test performance and less time cost .",0
3897,1883,"Compared to the [[ attention - based models ]] , multi-head attention and [[ DiSAN ]] , ReSAN uses a similar number of parameters << with >> better test performance and less time cost .",0
3898,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , [[ ReSAN ]] uses a similar number of parameters << with >> better test performance and less time cost .",0
3899,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , ReSAN uses a [[ similar number of parameters ]] << with >> better test performance and less time cost .",0
3900,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , ReSAN uses a similar number of parameters << with >> [[ better test performance ]] and less time cost .",0
3901,1883,"Compared to the [[ attention - based models ]] , multi-head attention and DiSAN , ReSAN uses a similar number of parameters << with >> better test performance and [[ less time cost ]] .",0
3902,1883,"Compared to the attention - based models , [[ multi-head attention ]] and [[ DiSAN ]] , ReSAN uses a similar number of parameters << with >> better test performance and less time cost .",0
3903,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , [[ ReSAN ]] uses a similar number of parameters << with >> better test performance and less time cost .",0
3904,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , ReSAN uses a [[ similar number of parameters ]] << with >> better test performance and less time cost .",0
3905,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , ReSAN uses a similar number of parameters << with >> [[ better test performance ]] and less time cost .",0
3906,1883,"Compared to the attention - based models , [[ multi-head attention ]] and DiSAN , ReSAN uses a similar number of parameters << with >> better test performance and [[ less time cost ]] .",0
3907,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , [[ ReSAN ]] uses a similar number of parameters << with >> better test performance and less time cost .",0
3908,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , ReSAN uses a [[ similar number of parameters ]] << with >> better test performance and less time cost .",0
3909,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , ReSAN uses a similar number of parameters << with >> [[ better test performance ]] and less time cost .",0
3910,1883,"Compared to the attention - based models , multi-head attention and [[ DiSAN ]] , ReSAN uses a similar number of parameters << with >> better test performance and [[ less time cost ]] .",0
3911,1883,"Compared to the attention - based models , multi-head attention and DiSAN , [[ ReSAN ]] uses a [[ similar number of parameters ]] << with >> better test performance and less time cost .",0
3912,1883,"Compared to the attention - based models , multi-head attention and DiSAN , [[ ReSAN ]] uses a similar number of parameters << with >> [[ better test performance ]] and less time cost .",1
3913,1883,"Compared to the attention - based models , multi-head attention and DiSAN , [[ ReSAN ]] uses a similar number of parameters << with >> better test performance and [[ less time cost ]] .",1
3914,1883,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN uses a [[ similar number of parameters ]] << with >> [[ better test performance ]] and less time cost .",0
3915,1883,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN uses a [[ similar number of parameters ]] << with >> better test performance and [[ less time cost ]] .",0
3916,1883,"Compared to the attention - based models , multi-head attention and DiSAN , ReSAN uses a similar number of parameters << with >> [[ better test performance ]] and [[ less time cost ]] .",0
3917,2592,The [[ initial learning rate ]] << for >> [[ AdaGrad ]] is fixed at 0.01 .,1
3918,2592,The [[ initial learning rate ]] << for >> AdaGrad is fixed at [[ 0.01 ]] .,0
3919,2592,The initial learning rate << for >> [[ AdaGrad ]] is fixed at [[ 0.01 ]] .,0
3920,2592,The [[ initial learning rate ]] for [[ AdaGrad ]] is << fixed at >> 0.01 .,0
3921,2592,The [[ initial learning rate ]] for AdaGrad is << fixed at >> [[ 0.01 ]] .,0
3922,2592,The initial learning rate for [[ AdaGrad ]] is << fixed at >> [[ 0.01 ]] .,1
3923,379,"We find that [[ BERT LARGE ]] << significantly outperforms >> [[ BERT BASE ]] across all tasks , especially those with very little training data .",1
3924,379,"We find that [[ BERT LARGE ]] << significantly outperforms >> BERT BASE across [[ all tasks ]] , especially those with very little training data .",0
3925,379,"We find that BERT LARGE << significantly outperforms >> [[ BERT BASE ]] across [[ all tasks ]] , especially those with very little training data .",0
3926,379,"We find that [[ BERT LARGE ]] significantly outperforms [[ BERT BASE ]] << across >> all tasks , especially those with very little training data .",0
3927,379,"We find that [[ BERT LARGE ]] significantly outperforms BERT BASE << across >> [[ all tasks ]] , especially those with very little training data .",0
3928,379,"We find that BERT LARGE significantly outperforms [[ BERT BASE ]] << across >> [[ all tasks ]] , especially those with very little training data .",1
3929,3775,[[ CASCADE ]] manages to << achieve >> [[ major improvement ]] across all datasets with statistical significance .,1
3930,3775,[[ CASCADE ]] manages to << achieve >> major improvement across [[ all datasets ]] with statistical significance .,0
3931,3775,[[ CASCADE ]] manages to << achieve >> major improvement across all datasets with [[ statistical significance ]] .,0
3932,3775,CASCADE manages to << achieve >> [[ major improvement ]] across [[ all datasets ]] with statistical significance .,0
3933,3775,CASCADE manages to << achieve >> [[ major improvement ]] across all datasets with [[ statistical significance ]] .,0
3934,3775,CASCADE manages to << achieve >> major improvement across [[ all datasets ]] with [[ statistical significance ]] .,0
3935,3775,[[ CASCADE ]] manages to achieve [[ major improvement ]] << across >> all datasets with statistical significance .,0
3936,3775,[[ CASCADE ]] manages to achieve major improvement << across >> [[ all datasets ]] with statistical significance .,0
3937,3775,[[ CASCADE ]] manages to achieve major improvement << across >> all datasets with [[ statistical significance ]] .,0
3938,3775,CASCADE manages to achieve [[ major improvement ]] << across >> [[ all datasets ]] with statistical significance .,1
3939,3775,CASCADE manages to achieve [[ major improvement ]] << across >> all datasets with [[ statistical significance ]] .,0
3940,3775,CASCADE manages to achieve major improvement << across >> [[ all datasets ]] with [[ statistical significance ]] .,0
3941,3775,[[ CASCADE ]] manages to achieve [[ major improvement ]] across all datasets << with >> statistical significance .,0
3942,3775,[[ CASCADE ]] manages to achieve major improvement across [[ all datasets ]] << with >> statistical significance .,0
3943,3775,[[ CASCADE ]] manages to achieve major improvement across all datasets << with >> [[ statistical significance ]] .,0
3944,3775,CASCADE manages to achieve [[ major improvement ]] across [[ all datasets ]] << with >> statistical significance .,0
3945,3775,CASCADE manages to achieve [[ major improvement ]] across all datasets << with >> [[ statistical significance ]] .,1
3946,3775,CASCADE manages to achieve major improvement across [[ all datasets ]] << with >> [[ statistical significance ]] .,0
3947,4003,[[ Tokenization ]] is << done using >> the [[ polyglot library ]] 7 .,1
3948,2148,[[ Each tree node ]] is << implemented with >> a [[ tree - LSTM block ]] same as in model .,1
3949,3726,"shows the precision - recall curves for each method , where [[ PCNNs + MIL ]] denotes our method , and << demonstrates >> that PCNNs + MIL achieves [[ higher precision ]] over the entire range of recall .",0
3950,3726,"shows the precision - recall curves for each method , where [[ PCNNs + MIL ]] denotes our method , and << demonstrates >> that PCNNs + MIL achieves higher precision over the [[ entire range of recall ]] .",0
3951,3726,"shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and << demonstrates >> that PCNNs + MIL achieves [[ higher precision ]] over the [[ entire range of recall ]] .",0
3952,3726,"shows the precision - recall curves for each method , where [[ PCNNs + MIL ]] denotes our method , and demonstrates that PCNNs + MIL << achieves >> [[ higher precision ]] over the entire range of recall .",1
3953,3726,"shows the precision - recall curves for each method , where [[ PCNNs + MIL ]] denotes our method , and demonstrates that PCNNs + MIL << achieves >> higher precision over the [[ entire range of recall ]] .",0
3954,3726,"shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and demonstrates that PCNNs + MIL << achieves >> [[ higher precision ]] over the [[ entire range of recall ]] .",0
3955,3726,"shows the precision - recall curves for each method , where [[ PCNNs + MIL ]] denotes our method , and demonstrates that PCNNs + MIL achieves [[ higher precision ]] << over >> the entire range of recall .",0
3956,3726,"shows the precision - recall curves for each method , where [[ PCNNs + MIL ]] denotes our method , and demonstrates that PCNNs + MIL achieves higher precision << over >> the [[ entire range of recall ]] .",0
3957,3726,"shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and demonstrates that PCNNs + MIL achieves [[ higher precision ]] << over >> the [[ entire range of recall ]] .",1
3958,2566,"The result shows that for most of the datasets , the [[ F1 score ]] << does not >> [[ improve much ]] when we directly add more layers .",1
3959,2566,"The result shows that for most of the datasets , the [[ F1 score ]] << does not >> improve much when we [[ directly add ]] more layers .",0
3960,2566,"The result shows that for most of the datasets , the [[ F1 score ]] << does not >> improve much when we directly add [[ more layers ]] .",0
3961,2566,"The result shows that for most of the datasets , the F1 score << does not >> [[ improve much ]] when we [[ directly add ]] more layers .",0
3962,2566,"The result shows that for most of the datasets , the F1 score << does not >> [[ improve much ]] when we directly add [[ more layers ]] .",0
3963,2566,"The result shows that for most of the datasets , the F1 score << does not >> improve much when we [[ directly add ]] [[ more layers ]] .",0
3964,2566,"The result shows that for most of the datasets , the [[ F1 score ]] does not [[ improve much ]] << when >> we directly add more layers .",0
3965,2566,"The result shows that for most of the datasets , the [[ F1 score ]] does not improve much << when >> we [[ directly add ]] more layers .",0
3966,2566,"The result shows that for most of the datasets , the [[ F1 score ]] does not improve much << when >> we directly add [[ more layers ]] .",0
3967,2566,"The result shows that for most of the datasets , the F1 score does not [[ improve much ]] << when >> we [[ directly add ]] more layers .",1
3968,2566,"The result shows that for most of the datasets , the F1 score does not [[ improve much ]] << when >> we directly add [[ more layers ]] .",0
3969,2566,"The result shows that for most of the datasets , the F1 score does not improve much << when >> we [[ directly add ]] [[ more layers ]] .",0
3970,1803,"Despite not having access to any external representation << of >> linguistic structure , [[ RASOR ]] achieves an [[ error reduction ]] of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",0
3971,1803,"Despite not having access to any external representation << of >> linguistic structure , [[ RASOR ]] achieves an error reduction of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",0
3972,1803,"Despite not having access to any external representation << of >> linguistic structure , [[ RASOR ]] achieves an error reduction of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3973,1803,"Despite not having access to any external representation << of >> linguistic structure , [[ RASOR ]] achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3974,1803,"Despite not having access to any external representation << of >> linguistic structure , RASOR achieves an [[ error reduction ]] of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",1
3975,1803,"Despite not having access to any external representation << of >> linguistic structure , RASOR achieves an [[ error reduction ]] of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3976,1803,"Despite not having access to any external representation << of >> linguistic structure , RASOR achieves an [[ error reduction ]] of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3977,1803,"Despite not having access to any external representation << of >> linguistic structure , RASOR achieves an error reduction of [[ more than 50 % ]] over this baseline , both in terms of [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3978,1803,"Despite not having access to any external representation << of >> linguistic structure , RASOR achieves an error reduction of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3979,1803,"Despite not having access to any external representation << of >> linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , relative to the [[ human performance upper bound ]] .",0
3980,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] << achieves >> an [[ error reduction ]] of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",1
3981,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] << achieves >> an error reduction of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",0
3982,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] << achieves >> an error reduction of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3983,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] << achieves >> an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3984,1803,"Despite not having access to any external representation of linguistic structure , RASOR << achieves >> an [[ error reduction ]] of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",0
3985,1803,"Despite not having access to any external representation of linguistic structure , RASOR << achieves >> an [[ error reduction ]] of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3986,1803,"Despite not having access to any external representation of linguistic structure , RASOR << achieves >> an [[ error reduction ]] of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3987,1803,"Despite not having access to any external representation of linguistic structure , RASOR << achieves >> an error reduction of [[ more than 50 % ]] over this baseline , both in terms of [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3988,1803,"Despite not having access to any external representation of linguistic structure , RASOR << achieves >> an error reduction of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3989,1803,"Despite not having access to any external representation of linguistic structure , RASOR << achieves >> an error reduction of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , relative to the [[ human performance upper bound ]] .",0
3990,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an [[ error reduction ]] of more than 50 % over this baseline , both << in terms of >> exact match and F1 , relative to the human performance upper bound .",0
3991,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an error reduction of [[ more than 50 % ]] over this baseline , both << in terms of >> exact match and F1 , relative to the human performance upper bound .",0
3992,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an error reduction of more than 50 % over this baseline , both << in terms of >> [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3993,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an error reduction of more than 50 % over this baseline , both << in terms of >> exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3994,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an [[ error reduction ]] of [[ more than 50 % ]] over this baseline , both << in terms of >> exact match and F1 , relative to the human performance upper bound .",0
3995,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an [[ error reduction ]] of more than 50 % over this baseline , both << in terms of >> [[ exact match and F1 ]] , relative to the human performance upper bound .",0
3996,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an [[ error reduction ]] of more than 50 % over this baseline , both << in terms of >> exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3997,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of [[ more than 50 % ]] over this baseline , both << in terms of >> [[ exact match and F1 ]] , relative to the human performance upper bound .",1
3998,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of [[ more than 50 % ]] over this baseline , both << in terms of >> exact match and F1 , relative to the [[ human performance upper bound ]] .",0
3999,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both << in terms of >> [[ exact match and F1 ]] , relative to the [[ human performance upper bound ]] .",0
4000,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an [[ error reduction ]] of more than 50 % over this baseline , both in terms of exact match and F1 , << relative to >> the human performance upper bound .",0
4001,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an error reduction of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , << relative to >> the human performance upper bound .",0
4002,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an error reduction of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , << relative to >> the human performance upper bound .",0
4003,1803,"Despite not having access to any external representation of linguistic structure , [[ RASOR ]] achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , << relative to >> the [[ human performance upper bound ]] .",0
4004,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an [[ error reduction ]] of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , << relative to >> the human performance upper bound .",0
4005,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an [[ error reduction ]] of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , << relative to >> the human performance upper bound .",0
4006,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an [[ error reduction ]] of more than 50 % over this baseline , both in terms of exact match and F1 , << relative to >> the [[ human performance upper bound ]] .",0
4007,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of [[ more than 50 % ]] over this baseline , both in terms of [[ exact match and F1 ]] , << relative to >> the human performance upper bound .",0
4008,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of [[ more than 50 % ]] over this baseline , both in terms of exact match and F1 , << relative to >> the [[ human performance upper bound ]] .",1
4009,1803,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of [[ exact match and F1 ]] , << relative to >> the [[ human performance upper bound ]] .",0
4010,5939,"As ? increases , the [[ accuracy ]] << of >> [[ entailment recognition ]] improves and finally exceeds that of the model without MTL , which reveals the advantage of MTL framework .",1
4011,5357,"In this article we << explore >> [[ treating text ]] as a kind of [[ raw signal ]] at character level , and applying temporal ( one-dimensional ) ConvNets to it .",0
4012,5357,"In this article we << explore >> [[ treating text ]] as a kind of raw signal at [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",0
4013,5357,"In this article we << explore >> [[ treating text ]] as a kind of raw signal at character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4014,5357,"In this article we << explore >> treating text as a kind of [[ raw signal ]] at [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",0
4015,5357,"In this article we << explore >> treating text as a kind of [[ raw signal ]] at character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4016,5357,"In this article we << explore >> treating text as a kind of raw signal at [[ character level ]] , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4017,5357,"In this article we explore [[ treating text ]] << as a kind of >> [[ raw signal ]] at character level , and applying temporal ( one-dimensional ) ConvNets to it .",1
4018,5357,"In this article we explore [[ treating text ]] << as a kind of >> raw signal at [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",0
4019,5357,"In this article we explore [[ treating text ]] << as a kind of >> raw signal at character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4020,5357,"In this article we explore treating text << as a kind of >> [[ raw signal ]] at [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",0
4021,5357,"In this article we explore treating text << as a kind of >> [[ raw signal ]] at character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4022,5357,"In this article we explore treating text << as a kind of >> raw signal at [[ character level ]] , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4023,5357,"In this article we explore [[ treating text ]] as a kind of [[ raw signal ]] << at >> character level , and applying temporal ( one-dimensional ) ConvNets to it .",0
4024,5357,"In this article we explore [[ treating text ]] as a kind of raw signal << at >> [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",0
4025,5357,"In this article we explore [[ treating text ]] as a kind of raw signal << at >> character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4026,5357,"In this article we explore treating text as a kind of [[ raw signal ]] << at >> [[ character level ]] , and applying temporal ( one-dimensional ) ConvNets to it .",1
4027,5357,"In this article we explore treating text as a kind of [[ raw signal ]] << at >> character level , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4028,5357,"In this article we explore treating text as a kind of raw signal << at >> [[ character level ]] , and applying [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4029,5357,"In this article we explore [[ treating text ]] as a kind of [[ raw signal ]] at character level , and << applying >> temporal ( one-dimensional ) ConvNets to it .",0
4030,5357,"In this article we explore [[ treating text ]] as a kind of raw signal at [[ character level ]] , and << applying >> temporal ( one-dimensional ) ConvNets to it .",0
4031,5357,"In this article we explore [[ treating text ]] as a kind of raw signal at character level , and << applying >> [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4032,5357,"In this article we explore treating text as a kind of [[ raw signal ]] at [[ character level ]] , and << applying >> temporal ( one-dimensional ) ConvNets to it .",0
4033,5357,"In this article we explore treating text as a kind of [[ raw signal ]] at character level , and << applying >> [[ temporal ( one-dimensional ) ConvNets ]] to it .",0
4034,5357,"In this article we explore treating text as a kind of raw signal at [[ character level ]] , and << applying >> [[ temporal ( one-dimensional ) ConvNets ]] to it .",1
4035,3621,[[ F 1 score ]] << keeps >> [[ increasing ]] .,1
4036,3967,"When [[ both scaffolds ]] are << used simultaneously in >> ' [[ BiLSTM - Attn + both scaffolds ]] ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",1
4037,3967,"When [[ both scaffolds ]] are << used simultaneously in >> ' BiLSTM - Attn + both scaffolds ' , the [[ F1 score ]] further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4038,3967,"When [[ both scaffolds ]] are << used simultaneously in >> ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to [[ 63.1 ( ?= 11.3 ) ]] , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4039,3967,"When both scaffolds are << used simultaneously in >> ' [[ BiLSTM - Attn + both scaffolds ]] ' , the [[ F1 score ]] further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4040,3967,"When both scaffolds are << used simultaneously in >> ' [[ BiLSTM - Attn + both scaffolds ]] ' , the F1 score further improves to [[ 63.1 ( ?= 11.3 ) ]] , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4041,3967,"When both scaffolds are << used simultaneously in >> ' BiLSTM - Attn + both scaffolds ' , the [[ F1 score ]] further improves to [[ 63.1 ( ?= 11.3 ) ]] , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4042,3967,"When [[ both scaffolds ]] are used simultaneously in ' [[ BiLSTM - Attn + both scaffolds ]] ' , the F1 score further << improves to >> 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4043,3967,"When [[ both scaffolds ]] are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the [[ F1 score ]] further << improves to >> 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4044,3967,"When [[ both scaffolds ]] are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further << improves to >> [[ 63.1 ( ?= 11.3 ) ]] , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4045,3967,"When both scaffolds are used simultaneously in ' [[ BiLSTM - Attn + both scaffolds ]] ' , the [[ F1 score ]] further << improves to >> 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4046,3967,"When both scaffolds are used simultaneously in ' [[ BiLSTM - Attn + both scaffolds ]] ' , the F1 score further << improves to >> [[ 63.1 ( ?= 11.3 ) ]] , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",0
4047,3967,"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the [[ F1 score ]] further << improves to >> [[ 63.1 ( ?= 11.3 ) ]] , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",1
4048,4328,We << use >> an [[ autoencoder structure ]] to learn [[ both the aspect embeddings as well as the representation of the target ]] as a weighted combination of the aspect embeddings .,0
4049,4328,We << use >> an [[ autoencoder structure ]] to learn both the aspect embeddings as well as the representation of the target as a [[ weighted combination ]] of the aspect embeddings .,0
4050,4328,We << use >> an [[ autoencoder structure ]] to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the [[ aspect embeddings ]] .,0
4051,4328,We << use >> an autoencoder structure to learn [[ both the aspect embeddings as well as the representation of the target ]] as a [[ weighted combination ]] of the aspect embeddings .,0
4052,4328,We << use >> an autoencoder structure to learn [[ both the aspect embeddings as well as the representation of the target ]] as a weighted combination of the [[ aspect embeddings ]] .,0
4053,4328,We << use >> an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a [[ weighted combination ]] of the [[ aspect embeddings ]] .,0
4054,4328,We use an [[ autoencoder structure ]] << to learn >> [[ both the aspect embeddings as well as the representation of the target ]] as a weighted combination of the aspect embeddings .,1
4055,4328,We use an [[ autoencoder structure ]] << to learn >> both the aspect embeddings as well as the representation of the target as a [[ weighted combination ]] of the aspect embeddings .,0
4056,4328,We use an [[ autoencoder structure ]] << to learn >> both the aspect embeddings as well as the representation of the target as a weighted combination of the [[ aspect embeddings ]] .,0
4057,4328,We use an autoencoder structure << to learn >> [[ both the aspect embeddings as well as the representation of the target ]] as a [[ weighted combination ]] of the aspect embeddings .,0
4058,4328,We use an autoencoder structure << to learn >> [[ both the aspect embeddings as well as the representation of the target ]] as a weighted combination of the [[ aspect embeddings ]] .,0
4059,4328,We use an autoencoder structure << to learn >> both the aspect embeddings as well as the representation of the target as a [[ weighted combination ]] of the [[ aspect embeddings ]] .,0
4060,4328,We use an [[ autoencoder structure ]] to learn [[ both the aspect embeddings as well as the representation of the target ]] << as >> a weighted combination of the aspect embeddings .,0
4061,4328,We use an [[ autoencoder structure ]] to learn both the aspect embeddings as well as the representation of the target << as >> a [[ weighted combination ]] of the aspect embeddings .,0
4062,4328,We use an [[ autoencoder structure ]] to learn both the aspect embeddings as well as the representation of the target << as >> a weighted combination of the [[ aspect embeddings ]] .,0
4063,4328,We use an autoencoder structure to learn [[ both the aspect embeddings as well as the representation of the target ]] << as >> a [[ weighted combination ]] of the aspect embeddings .,1
4064,4328,We use an autoencoder structure to learn [[ both the aspect embeddings as well as the representation of the target ]] << as >> a weighted combination of the [[ aspect embeddings ]] .,0
4065,4328,We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target << as >> a [[ weighted combination ]] of the [[ aspect embeddings ]] .,0
4066,4328,We use an [[ autoencoder structure ]] to learn [[ both the aspect embeddings as well as the representation of the target ]] as a weighted combination << of >> the aspect embeddings .,0
4067,4328,We use an [[ autoencoder structure ]] to learn both the aspect embeddings as well as the representation of the target as a [[ weighted combination ]] << of >> the aspect embeddings .,0
4068,4328,We use an [[ autoencoder structure ]] to learn both the aspect embeddings as well as the representation of the target as a weighted combination << of >> the [[ aspect embeddings ]] .,0
4069,4328,We use an autoencoder structure to learn [[ both the aspect embeddings as well as the representation of the target ]] as a [[ weighted combination ]] << of >> the aspect embeddings .,0
4070,4328,We use an autoencoder structure to learn [[ both the aspect embeddings as well as the representation of the target ]] as a weighted combination << of >> the [[ aspect embeddings ]] .,0
4071,4328,We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a [[ weighted combination ]] << of >> the [[ aspect embeddings ]] .,1
4072,4890,"Notice that << since >> [[ each sentence ]] can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .",0
4073,4890,"Notice that << since >> [[ each sentence ]] can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the dataset is higher than the number of sentences .",0
4074,4890,"Notice that << since >> [[ each sentence ]] can contain one or more opinions , the total number of opinions ( 5920 ) in the [[ dataset ]] is higher than the number of sentences .",0
4075,4890,"Notice that << since >> [[ each sentence ]] can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the [[ number of sentences ]] .",0
4076,4890,"Notice that << since >> each sentence can contain [[ one or more opinions ]] , the [[ total number of opinions ( 5920 ) ]] in the dataset is higher than the number of sentences .",0
4077,4890,"Notice that << since >> each sentence can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the [[ dataset ]] is higher than the number of sentences .",0
4078,4890,"Notice that << since >> each sentence can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the dataset is higher than the [[ number of sentences ]] .",0
4079,4890,"Notice that << since >> each sentence can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the [[ dataset ]] is higher than the number of sentences .",0
4080,4890,"Notice that << since >> each sentence can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the dataset is higher than the [[ number of sentences ]] .",0
4081,4890,"Notice that << since >> each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the [[ dataset ]] is higher than the [[ number of sentences ]] .",0
4082,4890,"Notice that since [[ each sentence ]] << can contain >> [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .",1
4083,4890,"Notice that since [[ each sentence ]] << can contain >> one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the dataset is higher than the number of sentences .",0
4084,4890,"Notice that since [[ each sentence ]] << can contain >> one or more opinions , the total number of opinions ( 5920 ) in the [[ dataset ]] is higher than the number of sentences .",0
4085,4890,"Notice that since [[ each sentence ]] << can contain >> one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the [[ number of sentences ]] .",0
4086,4890,"Notice that since each sentence << can contain >> [[ one or more opinions ]] , the [[ total number of opinions ( 5920 ) ]] in the dataset is higher than the number of sentences .",0
4087,4890,"Notice that since each sentence << can contain >> [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the [[ dataset ]] is higher than the number of sentences .",0
4088,4890,"Notice that since each sentence << can contain >> [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the dataset is higher than the [[ number of sentences ]] .",0
4089,4890,"Notice that since each sentence << can contain >> one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the [[ dataset ]] is higher than the number of sentences .",0
4090,4890,"Notice that since each sentence << can contain >> one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the dataset is higher than the [[ number of sentences ]] .",0
4091,4890,"Notice that since each sentence << can contain >> one or more opinions , the total number of opinions ( 5920 ) in the [[ dataset ]] is higher than the [[ number of sentences ]] .",0
4092,4890,"Notice that since [[ each sentence ]] can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) << in >> the dataset is higher than the number of sentences .",0
4093,4890,"Notice that since [[ each sentence ]] can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] << in >> the dataset is higher than the number of sentences .",0
4094,4890,"Notice that since [[ each sentence ]] can contain one or more opinions , the total number of opinions ( 5920 ) << in >> the [[ dataset ]] is higher than the number of sentences .",0
4095,4890,"Notice that since [[ each sentence ]] can contain one or more opinions , the total number of opinions ( 5920 ) << in >> the dataset is higher than the [[ number of sentences ]] .",0
4096,4890,"Notice that since each sentence can contain [[ one or more opinions ]] , the [[ total number of opinions ( 5920 ) ]] << in >> the dataset is higher than the number of sentences .",0
4097,4890,"Notice that since each sentence can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) << in >> the [[ dataset ]] is higher than the number of sentences .",0
4098,4890,"Notice that since each sentence can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) << in >> the dataset is higher than the [[ number of sentences ]] .",0
4099,4890,"Notice that since each sentence can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] << in >> the [[ dataset ]] is higher than the number of sentences .",1
4100,4890,"Notice that since each sentence can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] << in >> the dataset is higher than the [[ number of sentences ]] .",0
4101,4890,"Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) << in >> the [[ dataset ]] is higher than the [[ number of sentences ]] .",0
4102,4890,"Notice that since [[ each sentence ]] can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the dataset is << higher than >> the number of sentences .",0
4103,4890,"Notice that since [[ each sentence ]] can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the dataset is << higher than >> the number of sentences .",0
4104,4890,"Notice that since [[ each sentence ]] can contain one or more opinions , the total number of opinions ( 5920 ) in the [[ dataset ]] is << higher than >> the number of sentences .",0
4105,4890,"Notice that since [[ each sentence ]] can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is << higher than >> the [[ number of sentences ]] .",0
4106,4890,"Notice that since each sentence can contain [[ one or more opinions ]] , the [[ total number of opinions ( 5920 ) ]] in the dataset is << higher than >> the number of sentences .",0
4107,4890,"Notice that since each sentence can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the [[ dataset ]] is << higher than >> the number of sentences .",0
4108,4890,"Notice that since each sentence can contain [[ one or more opinions ]] , the total number of opinions ( 5920 ) in the dataset is << higher than >> the [[ number of sentences ]] .",0
4109,4890,"Notice that since each sentence can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the [[ dataset ]] is << higher than >> the number of sentences .",0
4110,4890,"Notice that since each sentence can contain one or more opinions , the [[ total number of opinions ( 5920 ) ]] in the dataset is << higher than >> the [[ number of sentences ]] .",1
4111,4890,"Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the [[ dataset ]] is << higher than >> the [[ number of sentences ]] .",0
4112,1820,The [[ primary system ]] << achieves >> the [[ highest F 1 and accuracy ]] on both tuning and test stages .,1
4113,1820,The [[ primary system ]] << achieves >> the highest F 1 and accuracy on [[ both tuning and test stages ]] .,0
4114,1820,The primary system << achieves >> the [[ highest F 1 and accuracy ]] on [[ both tuning and test stages ]] .,0
4115,1820,The [[ primary system ]] achieves the [[ highest F 1 and accuracy ]] << on >> both tuning and test stages .,0
4116,1820,The [[ primary system ]] achieves the highest F 1 and accuracy << on >> [[ both tuning and test stages ]] .,0
4117,1820,The primary system achieves the [[ highest F 1 and accuracy ]] << on >> [[ both tuning and test stages ]] .,1
4118,2562,"We << add >> [[ more layers ]] to the [[ char - CNN model ]] and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for 5 and 9 convolutional layers .",0
4119,2562,"We << add >> [[ more layers ]] to the char - CNN model and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for 5 and 9 convolutional layers .",0
4120,2562,"We << add >> [[ more layers ]] to the char - CNN model and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for [[ 5 and 9 convolutional layers ]] .",0
4121,2562,"We << add >> more layers to the [[ char - CNN model ]] and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for 5 and 9 convolutional layers .",0
4122,2562,"We << add >> more layers to the [[ char - CNN model ]] and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for [[ 5 and 9 convolutional layers ]] .",0
4123,2562,"We << add >> more layers to the char - CNN model and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for [[ 5 and 9 convolutional layers ]] .",0
4124,2562,"We add [[ more layers ]] << to >> the [[ char - CNN model ]] and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for 5 and 9 convolutional layers .",1
4125,2562,"We add [[ more layers ]] << to >> the char - CNN model and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for 5 and 9 convolutional layers .",0
4126,2562,"We add [[ more layers ]] << to >> the char - CNN model and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for [[ 5 and 9 convolutional layers ]] .",0
4127,2562,"We add more layers << to >> the [[ char - CNN model ]] and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for 5 and 9 convolutional layers .",0
4128,2562,"We add more layers << to >> the [[ char - CNN model ]] and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for [[ 5 and 9 convolutional layers ]] .",0
4129,2562,"We add more layers << to >> the char - CNN model and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for [[ 5 and 9 convolutional layers ]] .",0
4130,2562,"We add [[ more layers ]] to the [[ char - CNN model ]] and << refer to >> that as char - CNN - 5 and char - CNN - 9 , respectively for 5 and 9 convolutional layers .",0
4131,2562,"We add [[ more layers ]] to the char - CNN model and << refer to >> that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for 5 and 9 convolutional layers .",0
4132,2562,"We add [[ more layers ]] to the char - CNN model and << refer to >> that as char - CNN - 5 and char - CNN - 9 , respectively for [[ 5 and 9 convolutional layers ]] .",0
4133,2562,"We add more layers to the [[ char - CNN model ]] and << refer to >> that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for 5 and 9 convolutional layers .",1
4134,2562,"We add more layers to the [[ char - CNN model ]] and << refer to >> that as char - CNN - 5 and char - CNN - 9 , respectively for [[ 5 and 9 convolutional layers ]] .",0
4135,2562,"We add more layers to the char - CNN model and << refer to >> that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively for [[ 5 and 9 convolutional layers ]] .",0
4136,2562,"We add [[ more layers ]] to the [[ char - CNN model ]] and refer to that as char - CNN - 5 and char - CNN - 9 , respectively << for >> 5 and 9 convolutional layers .",0
4137,2562,"We add [[ more layers ]] to the char - CNN model and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively << for >> 5 and 9 convolutional layers .",0
4138,2562,"We add [[ more layers ]] to the char - CNN model and refer to that as char - CNN - 5 and char - CNN - 9 , respectively << for >> [[ 5 and 9 convolutional layers ]] .",0
4139,2562,"We add more layers to the [[ char - CNN model ]] and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively << for >> 5 and 9 convolutional layers .",0
4140,2562,"We add more layers to the [[ char - CNN model ]] and refer to that as char - CNN - 5 and char - CNN - 9 , respectively << for >> [[ 5 and 9 convolutional layers ]] .",0
4141,2562,"We add more layers to the char - CNN model and refer to that as [[ char - CNN - 5 and char - CNN - 9 ]] , respectively << for >> [[ 5 and 9 convolutional layers ]] .",1
4142,2559,"[[ Dropout ]] is << applied on >> the [[ input ]] of IntNet , LSTMs , and CRF , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .",1
4143,2559,"[[ Dropout ]] is << applied on >> the input of [[ IntNet , LSTMs , and CRF ]] , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .",0
4144,2559,"[[ Dropout ]] is << applied on >> the input of IntNet , LSTMs , and CRF , and its ratio [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4145,2559,"Dropout is << applied on >> the [[ input ]] of [[ IntNet , LSTMs , and CRF ]] , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .",0
4146,2559,"Dropout is << applied on >> the [[ input ]] of IntNet , LSTMs , and CRF , and its ratio [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4147,2559,"Dropout is << applied on >> the input of [[ IntNet , LSTMs , and CRF ]] , and its ratio [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4148,2559,"[[ Dropout ]] is applied on the [[ input ]] << of >> IntNet , LSTMs , and CRF , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .",0
4149,2559,"[[ Dropout ]] is applied on the input << of >> [[ IntNet , LSTMs , and CRF ]] , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .",0
4150,2559,"[[ Dropout ]] is applied on the input << of >> IntNet , LSTMs , and CRF , and its ratio [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4151,2559,"Dropout is applied on the [[ input ]] << of >> [[ IntNet , LSTMs , and CRF ]] , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .",1
4152,2559,"Dropout is applied on the [[ input ]] << of >> IntNet , LSTMs , and CRF , and its ratio [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4153,2559,"Dropout is applied on the input << of >> [[ IntNet , LSTMs , and CRF ]] , and its ratio [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4154,2559,"[[ Dropout ]] is applied on the [[ input ]] of IntNet , LSTMs , and CRF , and its << ratio >> 0.5 is fixed , but with no dropout inside of IntNet .",0
4155,2559,"[[ Dropout ]] is applied on the input of [[ IntNet , LSTMs , and CRF ]] , and its << ratio >> 0.5 is fixed , but with no dropout inside of IntNet .",0
4156,2559,"[[ Dropout ]] is applied on the input of IntNet , LSTMs , and CRF , and its << ratio >> [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",1
4157,2559,"Dropout is applied on the [[ input ]] of [[ IntNet , LSTMs , and CRF ]] , and its << ratio >> 0.5 is fixed , but with no dropout inside of IntNet .",0
4158,2559,"Dropout is applied on the [[ input ]] of IntNet , LSTMs , and CRF , and its << ratio >> [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4159,2559,"Dropout is applied on the input of [[ IntNet , LSTMs , and CRF ]] , and its << ratio >> [[ 0.5 ]] is fixed , but with no dropout inside of IntNet .",0
4160,4208,"<< To alleviate >> [[ overfitting ]] , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .",0
4161,4208,"<< To alleviate >> [[ overfitting ]] , we employed dropout strategy ( Hinton et al. , 2012 ) on the [[ input word embeddings ]] of the LSTM and the ultimate aspect - related sentence representation .",0
4162,4208,"<< To alleviate >> [[ overfitting ]] , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4163,4208,"<< To alleviate >> overfitting , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the [[ input word embeddings ]] of the LSTM and the ultimate aspect - related sentence representation .",0
4164,4208,"<< To alleviate >> overfitting , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the input word embeddings of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4165,4208,"<< To alleviate >> overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the [[ input word embeddings ]] of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4166,4208,"To alleviate [[ overfitting ]] , we << employed >> [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .",1
4167,4208,"To alleviate [[ overfitting ]] , we << employed >> dropout strategy ( Hinton et al. , 2012 ) on the [[ input word embeddings ]] of the LSTM and the ultimate aspect - related sentence representation .",0
4168,4208,"To alleviate [[ overfitting ]] , we << employed >> dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4169,4208,"To alleviate overfitting , we << employed >> [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the [[ input word embeddings ]] of the LSTM and the ultimate aspect - related sentence representation .",0
4170,4208,"To alleviate overfitting , we << employed >> [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the input word embeddings of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4171,4208,"To alleviate overfitting , we << employed >> dropout strategy ( Hinton et al. , 2012 ) on the [[ input word embeddings ]] of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4172,4208,"To alleviate [[ overfitting ]] , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] << on >> the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .",0
4173,4208,"To alleviate [[ overfitting ]] , we employed dropout strategy ( Hinton et al. , 2012 ) << on >> the [[ input word embeddings ]] of the LSTM and the ultimate aspect - related sentence representation .",0
4174,4208,"To alleviate [[ overfitting ]] , we employed dropout strategy ( Hinton et al. , 2012 ) << on >> the input word embeddings of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4175,4208,"To alleviate overfitting , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] << on >> the [[ input word embeddings ]] of the LSTM and the ultimate aspect - related sentence representation .",1
4176,4208,"To alleviate overfitting , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] << on >> the input word embeddings of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4177,4208,"To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) << on >> the [[ input word embeddings ]] of the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4178,4208,"To alleviate [[ overfitting ]] , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the input word embeddings << of >> the LSTM and the ultimate aspect - related sentence representation .",0
4179,4208,"To alleviate [[ overfitting ]] , we employed dropout strategy ( Hinton et al. , 2012 ) on the [[ input word embeddings ]] << of >> the LSTM and the ultimate aspect - related sentence representation .",0
4180,4208,"To alleviate [[ overfitting ]] , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings << of >> the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4181,4208,"To alleviate overfitting , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the [[ input word embeddings ]] << of >> the LSTM and the ultimate aspect - related sentence representation .",0
4182,4208,"To alleviate overfitting , we employed [[ dropout strategy ( Hinton et al. , 2012 ) ]] on the input word embeddings << of >> the [[ LSTM ]] and the ultimate aspect - related sentence representation .",0
4183,4208,"To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the [[ input word embeddings ]] << of >> the [[ LSTM ]] and the ultimate aspect - related sentence representation .",1
4184,5095,The [[ size ]] << of >> [[ mini-batch ]] is 60 .,1
4185,5095,The [[ size ]] << of >> mini-batch is [[ 60 ]] .,0
4186,5095,The size << of >> [[ mini-batch ]] is [[ 60 ]] .,0
4187,5095,The [[ size ]] of [[ mini-batch ]] << is >> 60 .,0
4188,5095,The [[ size ]] of mini-batch << is >> [[ 60 ]] .,0
4189,5095,The size of [[ mini-batch ]] << is >> [[ 60 ]] .,1
4190,1582,"While we << compare >> [[ ReasoNet ]] with [[ BiDAF ]] , ReasoNet exceeds BiDAF both in single model and ensemble model cases .",0
4191,1582,"While we << compare >> [[ ReasoNet ]] with BiDAF , ReasoNet exceeds BiDAF both in [[ single model and ensemble model cases ]] .",0
4192,1582,"While we << compare >> ReasoNet with [[ BiDAF ]] , ReasoNet exceeds BiDAF both in [[ single model and ensemble model cases ]] .",0
4193,1582,"While we compare [[ ReasoNet ]] with [[ BiDAF ]] , ReasoNet << exceeds >> BiDAF both in single model and ensemble model cases .",1
4194,1582,"While we compare [[ ReasoNet ]] with BiDAF , ReasoNet << exceeds >> BiDAF both in [[ single model and ensemble model cases ]] .",0
4195,1582,"While we compare ReasoNet with [[ BiDAF ]] , ReasoNet << exceeds >> BiDAF both in [[ single model and ensemble model cases ]] .",0
4196,1582,"While we compare [[ ReasoNet ]] with [[ BiDAF ]] , ReasoNet exceeds BiDAF << both in >> single model and ensemble model cases .",0
4197,1582,"While we compare [[ ReasoNet ]] with BiDAF , ReasoNet exceeds BiDAF << both in >> [[ single model and ensemble model cases ]] .",1
4198,1582,"While we compare ReasoNet with [[ BiDAF ]] , ReasoNet exceeds BiDAF << both in >> [[ single model and ensemble model cases ]] .",0
4199,2547,"Furthermore , we << propose >> [[ IntNet ]] , a [[ funnel - shaped wide convolutional neural network ]] for learning the internal structure of words by composing their characters .",0
4200,2547,"Furthermore , we << propose >> [[ IntNet ]] , a funnel - shaped wide convolutional neural network for learning the [[ internal structure of words ]] by composing their characters .",0
4201,2547,"Furthermore , we << propose >> [[ IntNet ]] , a funnel - shaped wide convolutional neural network for learning the internal structure of words by composing their [[ characters ]] .",0
4202,2547,"Furthermore , we << propose >> IntNet , a [[ funnel - shaped wide convolutional neural network ]] for learning the [[ internal structure of words ]] by composing their characters .",0
4203,2547,"Furthermore , we << propose >> IntNet , a [[ funnel - shaped wide convolutional neural network ]] for learning the internal structure of words by composing their [[ characters ]] .",0
4204,2547,"Furthermore , we << propose >> IntNet , a funnel - shaped wide convolutional neural network for learning the [[ internal structure of words ]] by composing their [[ characters ]] .",0
4205,2547,"Furthermore , we propose [[ IntNet ]] , a [[ funnel - shaped wide convolutional neural network ]] << for learning >> the internal structure of words by composing their characters .",0
4206,2547,"Furthermore , we propose [[ IntNet ]] , a funnel - shaped wide convolutional neural network << for learning >> the [[ internal structure of words ]] by composing their characters .",0
4207,2547,"Furthermore , we propose [[ IntNet ]] , a funnel - shaped wide convolutional neural network << for learning >> the internal structure of words by composing their [[ characters ]] .",0
4208,2547,"Furthermore , we propose IntNet , a [[ funnel - shaped wide convolutional neural network ]] << for learning >> the [[ internal structure of words ]] by composing their characters .",1
4209,2547,"Furthermore , we propose IntNet , a [[ funnel - shaped wide convolutional neural network ]] << for learning >> the internal structure of words by composing their [[ characters ]] .",0
4210,2547,"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural network << for learning >> the [[ internal structure of words ]] by composing their [[ characters ]] .",0
4211,2547,"Furthermore , we propose [[ IntNet ]] , a [[ funnel - shaped wide convolutional neural network ]] for learning the internal structure of words << by composing >> their characters .",0
4212,2547,"Furthermore , we propose [[ IntNet ]] , a funnel - shaped wide convolutional neural network for learning the [[ internal structure of words ]] << by composing >> their characters .",0
4213,2547,"Furthermore , we propose [[ IntNet ]] , a funnel - shaped wide convolutional neural network for learning the internal structure of words << by composing >> their [[ characters ]] .",0
4214,2547,"Furthermore , we propose IntNet , a [[ funnel - shaped wide convolutional neural network ]] for learning the [[ internal structure of words ]] << by composing >> their characters .",0
4215,2547,"Furthermore , we propose IntNet , a [[ funnel - shaped wide convolutional neural network ]] for learning the internal structure of words << by composing >> their [[ characters ]] .",0
4216,2547,"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural network for learning the [[ internal structure of words ]] << by composing >> their [[ characters ]] .",1
4217,319,"In particular , [[ each word ]] << receives >> [[ information ]] from its predecessor and successor simultaneously .",1
4218,319,"In particular , [[ each word ]] << receives >> information from its [[ predecessor and successor simultaneously ]] .",0
4219,319,"In particular , each word << receives >> [[ information ]] from its [[ predecessor and successor simultaneously ]] .",0
4220,319,"In particular , [[ each word ]] receives [[ information ]] << from >> its predecessor and successor simultaneously .",0
4221,319,"In particular , [[ each word ]] receives information << from >> its [[ predecessor and successor simultaneously ]] .",0
4222,319,"In particular , each word receives [[ information ]] << from >> its [[ predecessor and successor simultaneously ]] .",1
4223,4321,The [[ model parameters ]] were << regularized with >> a [[ per-minibatch L2 regularization strength ]] of 10 ?4 .,1
4224,4321,The [[ model parameters ]] were << regularized with >> a per-minibatch L2 regularization strength of [[ 10 ?4 ]] .,0
4225,4321,The model parameters were << regularized with >> a [[ per-minibatch L2 regularization strength ]] of [[ 10 ?4 ]] .,0
4226,4321,The [[ model parameters ]] were regularized with a [[ per-minibatch L2 regularization strength ]] << of >> 10 ?4 .,0
4227,4321,The [[ model parameters ]] were regularized with a per-minibatch L2 regularization strength << of >> [[ 10 ?4 ]] .,0
4228,4321,The model parameters were regularized with a [[ per-minibatch L2 regularization strength ]] << of >> [[ 10 ?4 ]] .,1
4229,5288,"The proposed model is << based on >> [[ multi-head self - attention ( MHSA ) ]] and integrates the [[ pre-trained and the local context focus mechanism ]] , namely LCF - ATEPC .",0
4230,5288,"The proposed model is << based on >> [[ multi-head self - attention ( MHSA ) ]] and integrates the pre-trained and the local context focus mechanism , namely [[ LCF - ATEPC ]] .",0
4231,5288,"The proposed model is << based on >> multi-head self - attention ( MHSA ) and integrates the [[ pre-trained and the local context focus mechanism ]] , namely [[ LCF - ATEPC ]] .",0
4232,5288,"The proposed model is based on [[ multi-head self - attention ( MHSA ) ]] and << integrates >> the [[ pre-trained and the local context focus mechanism ]] , namely LCF - ATEPC .",0
4233,5288,"The proposed model is based on [[ multi-head self - attention ( MHSA ) ]] and << integrates >> the pre-trained and the local context focus mechanism , namely [[ LCF - ATEPC ]] .",0
4234,5288,"The proposed model is based on multi-head self - attention ( MHSA ) and << integrates >> the [[ pre-trained and the local context focus mechanism ]] , namely [[ LCF - ATEPC ]] .",0
4235,5288,"The proposed model is based on [[ multi-head self - attention ( MHSA ) ]] and integrates the [[ pre-trained and the local context focus mechanism ]] , << namely >> LCF - ATEPC .",0
4236,5288,"The proposed model is based on [[ multi-head self - attention ( MHSA ) ]] and integrates the pre-trained and the local context focus mechanism , << namely >> [[ LCF - ATEPC ]] .",0
4237,5288,"The proposed model is based on multi-head self - attention ( MHSA ) and integrates the [[ pre-trained and the local context focus mechanism ]] , << namely >> [[ LCF - ATEPC ]] .",1
4238,3364,"Then , [[ BioBERT ]] is << pre-trained on >> [[ biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) ]] .",1
4239,5545,[[ Training ]] << is done via >> [[ stochastic gradient descent ]] over shuffled mini-batches with the Adadelta update rule .,1
4240,5545,[[ Training ]] << is done via >> stochastic gradient descent over [[ shuffled mini-batches ]] with the Adadelta update rule .,0
4241,5545,[[ Training ]] << is done via >> stochastic gradient descent over shuffled mini-batches with the [[ Adadelta update rule ]] .,0
4242,5545,Training << is done via >> [[ stochastic gradient descent ]] over [[ shuffled mini-batches ]] with the Adadelta update rule .,0
4243,5545,Training << is done via >> [[ stochastic gradient descent ]] over shuffled mini-batches with the [[ Adadelta update rule ]] .,0
4244,5545,Training << is done via >> stochastic gradient descent over [[ shuffled mini-batches ]] with the [[ Adadelta update rule ]] .,0
4245,5545,[[ Training ]] is done via [[ stochastic gradient descent ]] << over >> shuffled mini-batches with the Adadelta update rule .,0
4246,5545,[[ Training ]] is done via stochastic gradient descent << over >> [[ shuffled mini-batches ]] with the Adadelta update rule .,0
4247,5545,[[ Training ]] is done via stochastic gradient descent << over >> shuffled mini-batches with the [[ Adadelta update rule ]] .,0
4248,5545,Training is done via [[ stochastic gradient descent ]] << over >> [[ shuffled mini-batches ]] with the Adadelta update rule .,1
4249,5545,Training is done via [[ stochastic gradient descent ]] << over >> shuffled mini-batches with the [[ Adadelta update rule ]] .,0
4250,5545,Training is done via stochastic gradient descent << over >> [[ shuffled mini-batches ]] with the [[ Adadelta update rule ]] .,0
4251,5545,[[ Training ]] is done via [[ stochastic gradient descent ]] over shuffled mini-batches << with >> the Adadelta update rule .,0
4252,5545,[[ Training ]] is done via stochastic gradient descent over [[ shuffled mini-batches ]] << with >> the Adadelta update rule .,0
4253,5545,[[ Training ]] is done via stochastic gradient descent over shuffled mini-batches << with >> the [[ Adadelta update rule ]] .,0
4254,5545,Training is done via [[ stochastic gradient descent ]] over [[ shuffled mini-batches ]] << with >> the Adadelta update rule .,0
4255,5545,Training is done via [[ stochastic gradient descent ]] over shuffled mini-batches << with >> the [[ Adadelta update rule ]] .,0
4256,5545,Training is done via stochastic gradient descent over [[ shuffled mini-batches ]] << with >> the [[ Adadelta update rule ]] .,1
4257,1763,We also further << extend >> the [[ boundary model ]] with a [[ search mechanism ]] .,0
4258,1763,We also further extend the [[ boundary model ]] << with >> a [[ search mechanism ]] .,1
4259,2843,"The [[ semantic perspective ]] << compares >> the [[ hypothesis ]] to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",1
4260,2843,"The [[ semantic perspective ]] << compares >> the hypothesis to [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4261,2843,"The [[ semantic perspective ]] << compares >> the hypothesis to sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4262,2843,"The [[ semantic perspective ]] << compares >> the hypothesis to sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4263,2843,"The [[ semantic perspective ]] << compares >> the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4264,2843,"The semantic perspective << compares >> the [[ hypothesis ]] to [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4265,2843,"The semantic perspective << compares >> the [[ hypothesis ]] to sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4266,2843,"The semantic perspective << compares >> the [[ hypothesis ]] to sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4267,2843,"The semantic perspective << compares >> the [[ hypothesis ]] to sentences in the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4268,2843,"The semantic perspective << compares >> the hypothesis to [[ sentences ]] in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4269,2843,"The semantic perspective << compares >> the hypothesis to [[ sentences ]] in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4270,2843,"The semantic perspective << compares >> the hypothesis to [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4271,2843,"The semantic perspective << compares >> the hypothesis to sentences in the [[ text ]] viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4272,2843,"The semantic perspective << compares >> the hypothesis to sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4273,2843,"The semantic perspective << compares >> the hypothesis to sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4274,2843,"The [[ semantic perspective ]] compares the [[ hypothesis ]] << to >> sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4275,2843,"The [[ semantic perspective ]] compares the hypothesis << to >> [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4276,2843,"The [[ semantic perspective ]] compares the hypothesis << to >> sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4277,2843,"The [[ semantic perspective ]] compares the hypothesis << to >> sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4278,2843,"The [[ semantic perspective ]] compares the hypothesis << to >> sentences in the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4279,2843,"The semantic perspective compares the [[ hypothesis ]] << to >> [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",1
4280,2843,"The semantic perspective compares the [[ hypothesis ]] << to >> sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4281,2843,"The semantic perspective compares the [[ hypothesis ]] << to >> sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4282,2843,"The semantic perspective compares the [[ hypothesis ]] << to >> sentences in the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4283,2843,"The semantic perspective compares the hypothesis << to >> [[ sentences ]] in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4284,2843,"The semantic perspective compares the hypothesis << to >> [[ sentences ]] in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4285,2843,"The semantic perspective compares the hypothesis << to >> [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4286,2843,"The semantic perspective compares the hypothesis << to >> sentences in the [[ text ]] viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4287,2843,"The semantic perspective compares the hypothesis << to >> sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4288,2843,"The semantic perspective compares the hypothesis << to >> sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4289,2843,"The [[ semantic perspective ]] compares the [[ hypothesis ]] to sentences << in >> the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4290,2843,"The [[ semantic perspective ]] compares the hypothesis to [[ sentences ]] << in >> the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4291,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences << in >> the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4292,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences << in >> the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4293,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences << in >> the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4294,2843,"The semantic perspective compares the [[ hypothesis ]] to [[ sentences ]] << in >> the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4295,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences << in >> the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4296,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences << in >> the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4297,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences << in >> the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4298,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] << in >> the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",1
4299,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] << in >> the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4300,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] << in >> the text viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4301,2843,"The semantic perspective compares the hypothesis to sentences << in >> the [[ text ]] viewed as [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4302,2843,"The semantic perspective compares the hypothesis to sentences << in >> the [[ text ]] viewed as single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4303,2843,"The semantic perspective compares the hypothesis to sentences << in >> the text viewed as [[ single , self - contained thoughts ]] ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4304,2843,"The [[ semantic perspective ]] compares the [[ hypothesis ]] to sentences in the text << viewed as >> single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4305,2843,"The [[ semantic perspective ]] compares the hypothesis to [[ sentences ]] in the text << viewed as >> single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4306,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences in the [[ text ]] << viewed as >> single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4307,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences in the text << viewed as >> [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4308,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences in the text << viewed as >> single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4309,2843,"The semantic perspective compares the [[ hypothesis ]] to [[ sentences ]] in the text << viewed as >> single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4310,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences in the [[ text ]] << viewed as >> single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4311,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences in the text << viewed as >> [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4312,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences in the text << viewed as >> single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4313,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] in the [[ text ]] << viewed as >> single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4314,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] in the text << viewed as >> [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",0
4315,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] in the text << viewed as >> single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4316,2843,"The semantic perspective compares the hypothesis to sentences in the [[ text ]] << viewed as >> [[ single , self - contained thoughts ]] ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",1
4317,2843,"The semantic perspective compares the hypothesis to sentences in the [[ text ]] << viewed as >> single , self - contained thoughts ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4318,2843,"The semantic perspective compares the hypothesis to sentences in the text << viewed as >> [[ single , self - contained thoughts ]] ; these are represented using a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4319,2843,"The [[ semantic perspective ]] compares the [[ hypothesis ]] to sentences in the text viewed as single , self - contained thoughts ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4320,2843,"The [[ semantic perspective ]] compares the hypothesis to [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4321,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4322,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4323,2843,"The [[ semantic perspective ]] compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are << represented using >> a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4324,2843,"The semantic perspective compares the [[ hypothesis ]] to [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4325,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4326,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4327,2843,"The semantic perspective compares the [[ hypothesis ]] to sentences in the text viewed as single , self - contained thoughts ; these are << represented using >> a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4328,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] in the [[ text ]] viewed as single , self - contained thoughts ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4329,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] in the text viewed as [[ single , self - contained thoughts ]] ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4330,2843,"The semantic perspective compares the hypothesis to [[ sentences ]] in the text viewed as single , self - contained thoughts ; these are << represented using >> a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4331,2843,"The semantic perspective compares the hypothesis to sentences in the [[ text ]] viewed as [[ single , self - contained thoughts ]] ; these are << represented using >> a sum and transformation of word embedding vectors , similarly to in .",0
4332,2843,"The semantic perspective compares the hypothesis to sentences in the [[ text ]] viewed as single , self - contained thoughts ; these are << represented using >> a [[ sum and transformation of word embedding vectors ]] , similarly to in .",0
4333,2843,"The semantic perspective compares the hypothesis to sentences in the text viewed as [[ single , self - contained thoughts ]] ; these are << represented using >> a [[ sum and transformation of word embedding vectors ]] , similarly to in .",1
4334,4944,[[ DGIM ]] << prevents >> the [[ storage of dynamic influences ]] between speakers at each historical time step and leads to performance deterioration .,1
4335,4944,[[ DGIM ]] << prevents >> the storage of dynamic influences between [[ speakers ]] at each historical time step and leads to performance deterioration .,0
4336,4944,[[ DGIM ]] << prevents >> the storage of dynamic influences between speakers at each [[ historical time step ]] and leads to performance deterioration .,0
4337,4944,[[ DGIM ]] << prevents >> the storage of dynamic influences between speakers at each historical time step and leads to [[ performance deterioration ]] .,0
4338,4944,DGIM << prevents >> the [[ storage of dynamic influences ]] between [[ speakers ]] at each historical time step and leads to performance deterioration .,0
4339,4944,DGIM << prevents >> the [[ storage of dynamic influences ]] between speakers at each [[ historical time step ]] and leads to performance deterioration .,0
4340,4944,DGIM << prevents >> the [[ storage of dynamic influences ]] between speakers at each historical time step and leads to [[ performance deterioration ]] .,0
4341,4944,DGIM << prevents >> the storage of dynamic influences between [[ speakers ]] at each [[ historical time step ]] and leads to performance deterioration .,0
4342,4944,DGIM << prevents >> the storage of dynamic influences between [[ speakers ]] at each historical time step and leads to [[ performance deterioration ]] .,0
4343,4944,DGIM << prevents >> the storage of dynamic influences between speakers at each [[ historical time step ]] and leads to [[ performance deterioration ]] .,0
4344,4944,[[ DGIM ]] prevents the [[ storage of dynamic influences ]] << between >> speakers at each historical time step and leads to performance deterioration .,0
4345,4944,[[ DGIM ]] prevents the storage of dynamic influences << between >> [[ speakers ]] at each historical time step and leads to performance deterioration .,0
4346,4944,[[ DGIM ]] prevents the storage of dynamic influences << between >> speakers at each [[ historical time step ]] and leads to performance deterioration .,0
4347,4944,[[ DGIM ]] prevents the storage of dynamic influences << between >> speakers at each historical time step and leads to [[ performance deterioration ]] .,0
4348,4944,DGIM prevents the [[ storage of dynamic influences ]] << between >> [[ speakers ]] at each historical time step and leads to performance deterioration .,1
4349,4944,DGIM prevents the [[ storage of dynamic influences ]] << between >> speakers at each [[ historical time step ]] and leads to performance deterioration .,0
4350,4944,DGIM prevents the [[ storage of dynamic influences ]] << between >> speakers at each historical time step and leads to [[ performance deterioration ]] .,0
4351,4944,DGIM prevents the storage of dynamic influences << between >> [[ speakers ]] at each [[ historical time step ]] and leads to performance deterioration .,0
4352,4944,DGIM prevents the storage of dynamic influences << between >> [[ speakers ]] at each historical time step and leads to [[ performance deterioration ]] .,0
4353,4944,DGIM prevents the storage of dynamic influences << between >> speakers at each [[ historical time step ]] and leads to [[ performance deterioration ]] .,0
4354,4944,[[ DGIM ]] prevents the [[ storage of dynamic influences ]] between speakers << at each >> historical time step and leads to performance deterioration .,0
4355,4944,[[ DGIM ]] prevents the storage of dynamic influences between [[ speakers ]] << at each >> historical time step and leads to performance deterioration .,0
4356,4944,[[ DGIM ]] prevents the storage of dynamic influences between speakers << at each >> [[ historical time step ]] and leads to performance deterioration .,0
4357,4944,[[ DGIM ]] prevents the storage of dynamic influences between speakers << at each >> historical time step and leads to [[ performance deterioration ]] .,0
4358,4944,DGIM prevents the [[ storage of dynamic influences ]] between [[ speakers ]] << at each >> historical time step and leads to performance deterioration .,0
4359,4944,DGIM prevents the [[ storage of dynamic influences ]] between speakers << at each >> [[ historical time step ]] and leads to performance deterioration .,0
4360,4944,DGIM prevents the [[ storage of dynamic influences ]] between speakers << at each >> historical time step and leads to [[ performance deterioration ]] .,0
4361,4944,DGIM prevents the storage of dynamic influences between [[ speakers ]] << at each >> [[ historical time step ]] and leads to performance deterioration .,1
4362,4944,DGIM prevents the storage of dynamic influences between [[ speakers ]] << at each >> historical time step and leads to [[ performance deterioration ]] .,0
4363,4944,DGIM prevents the storage of dynamic influences between speakers << at each >> [[ historical time step ]] and leads to [[ performance deterioration ]] .,0
4364,4944,[[ DGIM ]] prevents the [[ storage of dynamic influences ]] between speakers at each historical time step and << leads to >> performance deterioration .,0
4365,4944,[[ DGIM ]] prevents the storage of dynamic influences between [[ speakers ]] at each historical time step and << leads to >> performance deterioration .,0
4366,4944,[[ DGIM ]] prevents the storage of dynamic influences between speakers at each [[ historical time step ]] and << leads to >> performance deterioration .,0
4367,4944,[[ DGIM ]] prevents the storage of dynamic influences between speakers at each historical time step and << leads to >> [[ performance deterioration ]] .,0
4368,4944,DGIM prevents the [[ storage of dynamic influences ]] between [[ speakers ]] at each historical time step and << leads to >> performance deterioration .,0
4369,4944,DGIM prevents the [[ storage of dynamic influences ]] between speakers at each [[ historical time step ]] and << leads to >> performance deterioration .,0
4370,4944,DGIM prevents the [[ storage of dynamic influences ]] between speakers at each historical time step and << leads to >> [[ performance deterioration ]] .,0
4371,4944,DGIM prevents the storage of dynamic influences between [[ speakers ]] at each [[ historical time step ]] and << leads to >> performance deterioration .,0
4372,4944,DGIM prevents the storage of dynamic influences between [[ speakers ]] at each historical time step and << leads to >> [[ performance deterioration ]] .,1
4373,4944,DGIM prevents the storage of dynamic influences between speakers at each [[ historical time step ]] and << leads to >> [[ performance deterioration ]] .,0
4374,4717,"<< For >> the [[ out of vocabulary words ]] we initialize them [[ randomly ]] from uniform distribution U ( ? 0.01 , 0.01 ) .",0
4375,4717,"<< For >> the [[ out of vocabulary words ]] we initialize them randomly from [[ uniform distribution U ( ? 0.01 , 0.01 ) ]] .",0
4376,4717,"<< For >> the out of vocabulary words we initialize them [[ randomly ]] from [[ uniform distribution U ( ? 0.01 , 0.01 ) ]] .",0
4377,4717,"For the [[ out of vocabulary words ]] we << initialize them >> [[ randomly ]] from uniform distribution U ( ? 0.01 , 0.01 ) .",1
4378,4717,"For the [[ out of vocabulary words ]] we << initialize them >> randomly from [[ uniform distribution U ( ? 0.01 , 0.01 ) ]] .",0
4379,4717,"For the out of vocabulary words we << initialize them >> [[ randomly ]] from [[ uniform distribution U ( ? 0.01 , 0.01 ) ]] .",0
4380,4717,"For the [[ out of vocabulary words ]] we initialize them [[ randomly ]] << from >> uniform distribution U ( ? 0.01 , 0.01 ) .",0
4381,4717,"For the [[ out of vocabulary words ]] we initialize them randomly << from >> [[ uniform distribution U ( ? 0.01 , 0.01 ) ]] .",1
4382,4717,"For the out of vocabulary words we initialize them [[ randomly ]] << from >> [[ uniform distribution U ( ? 0.01 , 0.01 ) ]] .",0
4383,2141,"We use [[ dropout ]] << with >> a [[ rate ]] of 0.5 , which is applied to all feedforward connections .",1
4384,2141,"We use [[ dropout ]] << with >> a rate of [[ 0.5 ]] , which is applied to all feedforward connections .",0
4385,2141,"We use [[ dropout ]] << with >> a rate of 0.5 , which is applied to [[ all feedforward connections ]] .",0
4386,2141,"We use dropout << with >> a [[ rate ]] of [[ 0.5 ]] , which is applied to all feedforward connections .",0
4387,2141,"We use dropout << with >> a [[ rate ]] of 0.5 , which is applied to [[ all feedforward connections ]] .",0
4388,2141,"We use dropout << with >> a rate of [[ 0.5 ]] , which is applied to [[ all feedforward connections ]] .",0
4389,2141,"We use [[ dropout ]] with a [[ rate ]] << of >> 0.5 , which is applied to all feedforward connections .",0
4390,2141,"We use [[ dropout ]] with a rate << of >> [[ 0.5 ]] , which is applied to all feedforward connections .",0
4391,2141,"We use [[ dropout ]] with a rate << of >> 0.5 , which is applied to [[ all feedforward connections ]] .",0
4392,2141,"We use dropout with a [[ rate ]] << of >> [[ 0.5 ]] , which is applied to all feedforward connections .",1
4393,2141,"We use dropout with a [[ rate ]] << of >> 0.5 , which is applied to [[ all feedforward connections ]] .",0
4394,2141,"We use dropout with a rate << of >> [[ 0.5 ]] , which is applied to [[ all feedforward connections ]] .",0
4395,2141,"We use [[ dropout ]] with a [[ rate ]] of 0.5 , which is << applied to >> all feedforward connections .",0
4396,2141,"We use [[ dropout ]] with a rate of [[ 0.5 ]] , which is << applied to >> all feedforward connections .",0
4397,2141,"We use [[ dropout ]] with a rate of 0.5 , which is << applied to >> [[ all feedforward connections ]] .",1
4398,2141,"We use dropout with a [[ rate ]] of [[ 0.5 ]] , which is << applied to >> all feedforward connections .",0
4399,2141,"We use dropout with a [[ rate ]] of 0.5 , which is << applied to >> [[ all feedforward connections ]] .",0
4400,2141,"We use dropout with a rate of [[ 0.5 ]] , which is << applied to >> [[ all feedforward connections ]] .",0
4401,4866,"As an approach , we << consider >> the popular [[ research benchmark of byte ( character ) level language modelling ]] due to [[ its further simplicity and generality ]] .",0
4402,4866,"As an approach , we consider the popular [[ research benchmark of byte ( character ) level language modelling ]] << due to >> [[ its further simplicity and generality ]] .",1
4403,4433,"In addition , another observation is that [[ Bi - GRU - PW ]] << performs >> [[ even worse ]] than Bi - GRU .",1
4404,4433,"In addition , another observation is that [[ Bi - GRU - PW ]] << performs >> even worse than [[ Bi - GRU ]] .",0
4405,4433,"In addition , another observation is that Bi - GRU - PW << performs >> [[ even worse ]] than [[ Bi - GRU ]] .",0
4406,4433,"In addition , another observation is that [[ Bi - GRU - PW ]] performs [[ even worse ]] << than >> Bi - GRU .",0
4407,4433,"In addition , another observation is that [[ Bi - GRU - PW ]] performs even worse << than >> [[ Bi - GRU ]] .",0
4408,4433,"In addition , another observation is that Bi - GRU - PW performs [[ even worse ]] << than >> [[ Bi - GRU ]] .",1
4409,3497,"Further , the improvement from [[ side information ]] shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that << inducing >> side information leads to [[ improved relation extraction ]] .",0
4410,3497,"Further , the improvement from [[ side information ]] shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that inducing side information << leads to >> [[ improved relation extraction ]] .",1
4411,803,"At the time of submission , our [[ model ]] << is >> [[ tied ]] in accuracy on the hidden test set with the bestperforming published single model .",1
4412,803,"At the time of submission , our [[ model ]] << is >> tied in [[ accuracy ]] on the hidden test set with the bestperforming published single model .",0
4413,803,"At the time of submission , our [[ model ]] << is >> tied in accuracy on the [[ hidden test set ]] with the bestperforming published single model .",0
4414,803,"At the time of submission , our [[ model ]] << is >> tied in accuracy on the hidden test set with the [[ bestperforming published single model ]] .",0
4415,803,"At the time of submission , our model << is >> [[ tied ]] in [[ accuracy ]] on the hidden test set with the bestperforming published single model .",0
4416,803,"At the time of submission , our model << is >> [[ tied ]] in accuracy on the [[ hidden test set ]] with the bestperforming published single model .",0
4417,803,"At the time of submission , our model << is >> [[ tied ]] in accuracy on the hidden test set with the [[ bestperforming published single model ]] .",0
4418,803,"At the time of submission , our model << is >> tied in [[ accuracy ]] on the [[ hidden test set ]] with the bestperforming published single model .",0
4419,803,"At the time of submission , our model << is >> tied in [[ accuracy ]] on the hidden test set with the [[ bestperforming published single model ]] .",0
4420,803,"At the time of submission , our model << is >> tied in accuracy on the [[ hidden test set ]] with the [[ bestperforming published single model ]] .",0
4421,803,"At the time of submission , our [[ model ]] is [[ tied ]] << in >> accuracy on the hidden test set with the bestperforming published single model .",0
4422,803,"At the time of submission , our [[ model ]] is tied << in >> [[ accuracy ]] on the hidden test set with the bestperforming published single model .",0
4423,803,"At the time of submission , our [[ model ]] is tied << in >> accuracy on the [[ hidden test set ]] with the bestperforming published single model .",0
4424,803,"At the time of submission , our [[ model ]] is tied << in >> accuracy on the hidden test set with the [[ bestperforming published single model ]] .",0
4425,803,"At the time of submission , our model is [[ tied ]] << in >> [[ accuracy ]] on the hidden test set with the bestperforming published single model .",1
4426,803,"At the time of submission , our model is [[ tied ]] << in >> accuracy on the [[ hidden test set ]] with the bestperforming published single model .",0
4427,803,"At the time of submission , our model is [[ tied ]] << in >> accuracy on the hidden test set with the [[ bestperforming published single model ]] .",0
4428,803,"At the time of submission , our model is tied << in >> [[ accuracy ]] on the [[ hidden test set ]] with the bestperforming published single model .",0
4429,803,"At the time of submission , our model is tied << in >> [[ accuracy ]] on the hidden test set with the [[ bestperforming published single model ]] .",0
4430,803,"At the time of submission , our model is tied << in >> accuracy on the [[ hidden test set ]] with the [[ bestperforming published single model ]] .",0
4431,803,"At the time of submission , our [[ model ]] is [[ tied ]] in accuracy << on >> the hidden test set with the bestperforming published single model .",0
4432,803,"At the time of submission , our [[ model ]] is tied in [[ accuracy ]] << on >> the hidden test set with the bestperforming published single model .",0
4433,803,"At the time of submission , our [[ model ]] is tied in accuracy << on >> the [[ hidden test set ]] with the bestperforming published single model .",0
4434,803,"At the time of submission , our [[ model ]] is tied in accuracy << on >> the hidden test set with the [[ bestperforming published single model ]] .",0
4435,803,"At the time of submission , our model is [[ tied ]] in [[ accuracy ]] << on >> the hidden test set with the bestperforming published single model .",0
4436,803,"At the time of submission , our model is [[ tied ]] in accuracy << on >> the [[ hidden test set ]] with the bestperforming published single model .",1
4437,803,"At the time of submission , our model is [[ tied ]] in accuracy << on >> the hidden test set with the [[ bestperforming published single model ]] .",0
4438,803,"At the time of submission , our model is tied in [[ accuracy ]] << on >> the [[ hidden test set ]] with the bestperforming published single model .",0
4439,803,"At the time of submission , our model is tied in [[ accuracy ]] << on >> the hidden test set with the [[ bestperforming published single model ]] .",0
4440,803,"At the time of submission , our model is tied in accuracy << on >> the [[ hidden test set ]] with the [[ bestperforming published single model ]] .",0
4441,803,"At the time of submission , our [[ model ]] is [[ tied ]] in accuracy on the hidden test set << with >> the bestperforming published single model .",0
4442,803,"At the time of submission , our [[ model ]] is tied in [[ accuracy ]] on the hidden test set << with >> the bestperforming published single model .",0
4443,803,"At the time of submission , our [[ model ]] is tied in accuracy on the [[ hidden test set ]] << with >> the bestperforming published single model .",0
4444,803,"At the time of submission , our [[ model ]] is tied in accuracy on the hidden test set << with >> the [[ bestperforming published single model ]] .",0
4445,803,"At the time of submission , our model is [[ tied ]] in [[ accuracy ]] on the hidden test set << with >> the bestperforming published single model .",0
4446,803,"At the time of submission , our model is [[ tied ]] in accuracy on the [[ hidden test set ]] << with >> the bestperforming published single model .",0
4447,803,"At the time of submission , our model is [[ tied ]] in accuracy on the hidden test set << with >> the [[ bestperforming published single model ]] .",1
4448,803,"At the time of submission , our model is tied in [[ accuracy ]] on the [[ hidden test set ]] << with >> the bestperforming published single model .",0
4449,803,"At the time of submission , our model is tied in [[ accuracy ]] on the hidden test set << with >> the [[ bestperforming published single model ]] .",0
4450,803,"At the time of submission , our model is tied in accuracy on the [[ hidden test set ]] << with >> the [[ bestperforming published single model ]] .",0
4451,5278,"Overall , we can << conclude that >> [[ our simple ML methods ]] are [[ very robust ]] to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .",0
4452,5278,"Overall , we can << conclude that >> [[ our simple ML methods ]] are very robust to have achieved [[ comparable performance ]] even though they are modeled to predict six - classes as opposed to four in previous works .",0
4453,5278,"Overall , we can << conclude that >> our simple ML methods are [[ very robust ]] to have achieved [[ comparable performance ]] even though they are modeled to predict six - classes as opposed to four in previous works .",0
4454,5278,"Overall , we can conclude that [[ our simple ML methods ]] << are >> [[ very robust ]] to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .",1
4455,5278,"Overall , we can conclude that [[ our simple ML methods ]] << are >> very robust to have achieved [[ comparable performance ]] even though they are modeled to predict six - classes as opposed to four in previous works .",0
4456,5278,"Overall , we can conclude that our simple ML methods << are >> [[ very robust ]] to have achieved [[ comparable performance ]] even though they are modeled to predict six - classes as opposed to four in previous works .",0
4457,5278,"Overall , we can conclude that [[ our simple ML methods ]] are [[ very robust ]] << to have achieved >> comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .",0
4458,5278,"Overall , we can conclude that [[ our simple ML methods ]] are very robust << to have achieved >> [[ comparable performance ]] even though they are modeled to predict six - classes as opposed to four in previous works .",0
4459,5278,"Overall , we can conclude that our simple ML methods are [[ very robust ]] << to have achieved >> [[ comparable performance ]] even though they are modeled to predict six - classes as opposed to four in previous works .",1
4460,5264,"We << use >> [[ librosa , a Python library ]] , to process the [[ audio files ]] and extract features from them .",0
4461,5264,"We use [[ librosa , a Python library ]] , << to process >> the [[ audio files ]] and extract features from them .",1
4462,4458,"Furthermore , [[ IMN ]] << allows >> [[ AE and AS ]] to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .",1
4463,4458,"Furthermore , [[ IMN ]] << allows >> AE and AS to be trained together with [[ related document - level tasks ]] , exploiting the knowledge from larger document - level corpora .",0
4464,4458,"Furthermore , [[ IMN ]] << allows >> AE and AS to be trained together with related document - level tasks , exploiting the [[ knowledge ]] from larger document - level corpora .",0
4465,4458,"Furthermore , [[ IMN ]] << allows >> AE and AS to be trained together with related document - level tasks , exploiting the knowledge from [[ larger document - level corpora ]] .",0
4466,4458,"Furthermore , IMN << allows >> [[ AE and AS ]] to be trained together with [[ related document - level tasks ]] , exploiting the knowledge from larger document - level corpora .",0
4467,4458,"Furthermore , IMN << allows >> [[ AE and AS ]] to be trained together with related document - level tasks , exploiting the [[ knowledge ]] from larger document - level corpora .",0
4468,4458,"Furthermore , IMN << allows >> [[ AE and AS ]] to be trained together with related document - level tasks , exploiting the knowledge from [[ larger document - level corpora ]] .",0
4469,4458,"Furthermore , IMN << allows >> AE and AS to be trained together with [[ related document - level tasks ]] , exploiting the [[ knowledge ]] from larger document - level corpora .",0
4470,4458,"Furthermore , IMN << allows >> AE and AS to be trained together with [[ related document - level tasks ]] , exploiting the knowledge from [[ larger document - level corpora ]] .",0
4471,4458,"Furthermore , IMN << allows >> AE and AS to be trained together with related document - level tasks , exploiting the [[ knowledge ]] from [[ larger document - level corpora ]] .",0
4472,4458,"Furthermore , [[ IMN ]] allows [[ AE and AS ]] << to be trained together with >> related document - level tasks , exploiting the knowledge from larger document - level corpora .",0
4473,4458,"Furthermore , [[ IMN ]] allows AE and AS << to be trained together with >> [[ related document - level tasks ]] , exploiting the knowledge from larger document - level corpora .",0
4474,4458,"Furthermore , [[ IMN ]] allows AE and AS << to be trained together with >> related document - level tasks , exploiting the [[ knowledge ]] from larger document - level corpora .",0
4475,4458,"Furthermore , [[ IMN ]] allows AE and AS << to be trained together with >> related document - level tasks , exploiting the knowledge from [[ larger document - level corpora ]] .",0
4476,4458,"Furthermore , IMN allows [[ AE and AS ]] << to be trained together with >> [[ related document - level tasks ]] , exploiting the knowledge from larger document - level corpora .",1
4477,4458,"Furthermore , IMN allows [[ AE and AS ]] << to be trained together with >> related document - level tasks , exploiting the [[ knowledge ]] from larger document - level corpora .",0
4478,4458,"Furthermore , IMN allows [[ AE and AS ]] << to be trained together with >> related document - level tasks , exploiting the knowledge from [[ larger document - level corpora ]] .",0
4479,4458,"Furthermore , IMN allows AE and AS << to be trained together with >> [[ related document - level tasks ]] , exploiting the [[ knowledge ]] from larger document - level corpora .",0
4480,4458,"Furthermore , IMN allows AE and AS << to be trained together with >> [[ related document - level tasks ]] , exploiting the knowledge from [[ larger document - level corpora ]] .",0
4481,4458,"Furthermore , IMN allows AE and AS << to be trained together with >> related document - level tasks , exploiting the [[ knowledge ]] from [[ larger document - level corpora ]] .",0
4482,4458,"Furthermore , [[ IMN ]] allows [[ AE and AS ]] to be trained together with related document - level tasks , << exploiting >> the knowledge from larger document - level corpora .",0
4483,4458,"Furthermore , [[ IMN ]] allows AE and AS to be trained together with [[ related document - level tasks ]] , << exploiting >> the knowledge from larger document - level corpora .",0
4484,4458,"Furthermore , [[ IMN ]] allows AE and AS to be trained together with related document - level tasks , << exploiting >> the [[ knowledge ]] from larger document - level corpora .",0
4485,4458,"Furthermore , [[ IMN ]] allows AE and AS to be trained together with related document - level tasks , << exploiting >> the knowledge from [[ larger document - level corpora ]] .",0
4486,4458,"Furthermore , IMN allows [[ AE and AS ]] to be trained together with [[ related document - level tasks ]] , << exploiting >> the knowledge from larger document - level corpora .",0
4487,4458,"Furthermore , IMN allows [[ AE and AS ]] to be trained together with related document - level tasks , << exploiting >> the [[ knowledge ]] from larger document - level corpora .",0
4488,4458,"Furthermore , IMN allows [[ AE and AS ]] to be trained together with related document - level tasks , << exploiting >> the knowledge from [[ larger document - level corpora ]] .",0
4489,4458,"Furthermore , IMN allows AE and AS to be trained together with [[ related document - level tasks ]] , << exploiting >> the [[ knowledge ]] from larger document - level corpora .",1
4490,4458,"Furthermore , IMN allows AE and AS to be trained together with [[ related document - level tasks ]] , << exploiting >> the knowledge from [[ larger document - level corpora ]] .",0
4491,4458,"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , << exploiting >> the [[ knowledge ]] from [[ larger document - level corpora ]] .",0
4492,4458,"Furthermore , [[ IMN ]] allows [[ AE and AS ]] to be trained together with related document - level tasks , exploiting the knowledge << from >> larger document - level corpora .",0
4493,4458,"Furthermore , [[ IMN ]] allows AE and AS to be trained together with [[ related document - level tasks ]] , exploiting the knowledge << from >> larger document - level corpora .",0
4494,4458,"Furthermore , [[ IMN ]] allows AE and AS to be trained together with related document - level tasks , exploiting the [[ knowledge ]] << from >> larger document - level corpora .",0
4495,4458,"Furthermore , [[ IMN ]] allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge << from >> [[ larger document - level corpora ]] .",0
4496,4458,"Furthermore , IMN allows [[ AE and AS ]] to be trained together with [[ related document - level tasks ]] , exploiting the knowledge << from >> larger document - level corpora .",0
4497,4458,"Furthermore , IMN allows [[ AE and AS ]] to be trained together with related document - level tasks , exploiting the [[ knowledge ]] << from >> larger document - level corpora .",0
4498,4458,"Furthermore , IMN allows [[ AE and AS ]] to be trained together with related document - level tasks , exploiting the knowledge << from >> [[ larger document - level corpora ]] .",0
4499,4458,"Furthermore , IMN allows AE and AS to be trained together with [[ related document - level tasks ]] , exploiting the [[ knowledge ]] << from >> larger document - level corpora .",0
4500,4458,"Furthermore , IMN allows AE and AS to be trained together with [[ related document - level tasks ]] , exploiting the knowledge << from >> [[ larger document - level corpora ]] .",0
4501,4458,"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the [[ knowledge ]] << from >> [[ larger document - level corpora ]] .",1
4502,4946,"Also , << removal of >> multi-hop leads to [[ worse performance ]] than the [[ removal of DGIM ]] .",0
4503,4946,"Also , removal of multi-hop << leads to >> [[ worse performance ]] than the [[ removal of DGIM ]] .",0
4504,3805,"We << test >> [[ TRANX ]] on four [[ semantic parsing ]] ( ATIS , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4505,3805,"We << test >> [[ TRANX ]] on four semantic parsing ( [[ ATIS ]] , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4506,3805,"We << test >> [[ TRANX ]] on four semantic parsing ( ATIS , [[ GEO ]] ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4507,3805,"We << test >> [[ TRANX ]] on four semantic parsing ( ATIS , GEO ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4508,3805,"We << test >> [[ TRANX ]] on four semantic parsing ( ATIS , GEO ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4509,3805,"We << test >> [[ TRANX ]] on four semantic parsing ( ATIS , GEO ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4510,3805,"We << test >> TRANX on four [[ semantic parsing ]] ( [[ ATIS ]] , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4511,3805,"We << test >> TRANX on four [[ semantic parsing ]] ( ATIS , [[ GEO ]] ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4512,3805,"We << test >> TRANX on four [[ semantic parsing ]] ( ATIS , GEO ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4513,3805,"We << test >> TRANX on four [[ semantic parsing ]] ( ATIS , GEO ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4514,3805,"We << test >> TRANX on four [[ semantic parsing ]] ( ATIS , GEO ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4515,3805,"We << test >> TRANX on four semantic parsing ( [[ ATIS ]] , [[ GEO ]] ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4516,3805,"We << test >> TRANX on four semantic parsing ( [[ ATIS ]] , GEO ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4517,3805,"We << test >> TRANX on four semantic parsing ( [[ ATIS ]] , GEO ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4518,3805,"We << test >> TRANX on four semantic parsing ( [[ ATIS ]] , GEO ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4519,3805,"We << test >> TRANX on four semantic parsing ( ATIS , [[ GEO ]] ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4520,3805,"We << test >> TRANX on four semantic parsing ( ATIS , [[ GEO ]] ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4521,3805,"We << test >> TRANX on four semantic parsing ( ATIS , [[ GEO ]] ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4522,3805,"We << test >> TRANX on four semantic parsing ( ATIS , GEO ) and [[ code generation ]] ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4523,3805,"We << test >> TRANX on four semantic parsing ( ATIS , GEO ) and [[ code generation ]] ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4524,3805,"We << test >> TRANX on four semantic parsing ( ATIS , GEO ) and code generation ( [[ DJANGO ]] , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4525,3805,"We test [[ TRANX ]] << on >> four [[ semantic parsing ]] ( ATIS , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",1
4526,3805,"We test [[ TRANX ]] << on >> four semantic parsing ( [[ ATIS ]] , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4527,3805,"We test [[ TRANX ]] << on >> four semantic parsing ( ATIS , [[ GEO ]] ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4528,3805,"We test [[ TRANX ]] << on >> four semantic parsing ( ATIS , GEO ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",1
4529,3805,"We test [[ TRANX ]] << on >> four semantic parsing ( ATIS , GEO ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4530,3805,"We test [[ TRANX ]] << on >> four semantic parsing ( ATIS , GEO ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4531,3805,"We test TRANX << on >> four [[ semantic parsing ]] ( [[ ATIS ]] , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4532,3805,"We test TRANX << on >> four [[ semantic parsing ]] ( ATIS , [[ GEO ]] ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4533,3805,"We test TRANX << on >> four [[ semantic parsing ]] ( ATIS , GEO ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4534,3805,"We test TRANX << on >> four [[ semantic parsing ]] ( ATIS , GEO ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4535,3805,"We test TRANX << on >> four [[ semantic parsing ]] ( ATIS , GEO ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4536,3805,"We test TRANX << on >> four semantic parsing ( [[ ATIS ]] , [[ GEO ]] ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4537,3805,"We test TRANX << on >> four semantic parsing ( [[ ATIS ]] , GEO ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4538,3805,"We test TRANX << on >> four semantic parsing ( [[ ATIS ]] , GEO ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4539,3805,"We test TRANX << on >> four semantic parsing ( [[ ATIS ]] , GEO ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4540,3805,"We test TRANX << on >> four semantic parsing ( ATIS , [[ GEO ]] ) and [[ code generation ]] ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4541,3805,"We test TRANX << on >> four semantic parsing ( ATIS , [[ GEO ]] ) and code generation ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4542,3805,"We test TRANX << on >> four semantic parsing ( ATIS , [[ GEO ]] ) and code generation ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4543,3805,"We test TRANX << on >> four semantic parsing ( ATIS , GEO ) and [[ code generation ]] ( [[ DJANGO ]] , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4544,3805,"We test TRANX << on >> four semantic parsing ( ATIS , GEO ) and [[ code generation ]] ( DJANGO , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4545,3805,"We test TRANX << on >> four semantic parsing ( ATIS , GEO ) and code generation ( [[ DJANGO ]] , [[ WIKISQL ]] ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",0
4546,4952,"After that , we << pay >> [[ multiple attentions ]] on the [[ position - weighted memory ]] and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .",0
4547,4952,"After that , we << pay >> [[ multiple attentions ]] on the position - weighted memory and nonlinearly combine the [[ attention results ]] with a recurrent network , i.e. GRUs .",0
4548,4952,"After that , we << pay >> [[ multiple attentions ]] on the position - weighted memory and nonlinearly combine the attention results with a [[ recurrent network , i.e. GRUs ]] .",0
4549,4952,"After that , we << pay >> multiple attentions on the [[ position - weighted memory ]] and nonlinearly combine the [[ attention results ]] with a recurrent network , i.e. GRUs .",0
4550,4952,"After that , we << pay >> multiple attentions on the [[ position - weighted memory ]] and nonlinearly combine the attention results with a [[ recurrent network , i.e. GRUs ]] .",0
4551,4952,"After that , we << pay >> multiple attentions on the position - weighted memory and nonlinearly combine the [[ attention results ]] with a [[ recurrent network , i.e. GRUs ]] .",0
4552,4952,"After that , we pay [[ multiple attentions ]] << on >> the [[ position - weighted memory ]] and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .",1
4553,4952,"After that , we pay [[ multiple attentions ]] << on >> the position - weighted memory and nonlinearly combine the [[ attention results ]] with a recurrent network , i.e. GRUs .",0
4554,4952,"After that , we pay [[ multiple attentions ]] << on >> the position - weighted memory and nonlinearly combine the attention results with a [[ recurrent network , i.e. GRUs ]] .",0
4555,4952,"After that , we pay multiple attentions << on >> the [[ position - weighted memory ]] and nonlinearly combine the [[ attention results ]] with a recurrent network , i.e. GRUs .",0
4556,4952,"After that , we pay multiple attentions << on >> the [[ position - weighted memory ]] and nonlinearly combine the attention results with a [[ recurrent network , i.e. GRUs ]] .",0
4557,4952,"After that , we pay multiple attentions << on >> the position - weighted memory and nonlinearly combine the [[ attention results ]] with a [[ recurrent network , i.e. GRUs ]] .",0
4558,4952,"After that , we pay [[ multiple attentions ]] on the [[ position - weighted memory ]] and << nonlinearly combine >> the attention results with a recurrent network , i.e. GRUs .",0
4559,4952,"After that , we pay [[ multiple attentions ]] on the position - weighted memory and << nonlinearly combine >> the [[ attention results ]] with a recurrent network , i.e. GRUs .",0
4560,4952,"After that , we pay [[ multiple attentions ]] on the position - weighted memory and << nonlinearly combine >> the attention results with a [[ recurrent network , i.e. GRUs ]] .",0
4561,4952,"After that , we pay multiple attentions on the [[ position - weighted memory ]] and << nonlinearly combine >> the [[ attention results ]] with a recurrent network , i.e. GRUs .",0
4562,4952,"After that , we pay multiple attentions on the [[ position - weighted memory ]] and << nonlinearly combine >> the attention results with a [[ recurrent network , i.e. GRUs ]] .",0
4563,4952,"After that , we pay multiple attentions on the position - weighted memory and << nonlinearly combine >> the [[ attention results ]] with a [[ recurrent network , i.e. GRUs ]] .",0
4564,4952,"After that , we pay [[ multiple attentions ]] on the [[ position - weighted memory ]] and nonlinearly combine the attention results << with >> a recurrent network , i.e. GRUs .",0
4565,4952,"After that , we pay [[ multiple attentions ]] on the position - weighted memory and nonlinearly combine the [[ attention results ]] << with >> a recurrent network , i.e. GRUs .",0
4566,4952,"After that , we pay [[ multiple attentions ]] on the position - weighted memory and nonlinearly combine the attention results << with >> a [[ recurrent network , i.e. GRUs ]] .",0
4567,4952,"After that , we pay multiple attentions on the [[ position - weighted memory ]] and nonlinearly combine the [[ attention results ]] << with >> a recurrent network , i.e. GRUs .",0
4568,4952,"After that , we pay multiple attentions on the [[ position - weighted memory ]] and nonlinearly combine the attention results << with >> a [[ recurrent network , i.e. GRUs ]] .",0
4569,4952,"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the [[ attention results ]] << with >> a [[ recurrent network , i.e. GRUs ]] .",1
4570,2953,"We << demonstrate >> that directly classifying [[ each of the competing spans ]] , and training with [[ global normalization ]] over all possible spans , leads to a significant increase in performance .",0
4571,2953,"We << demonstrate >> that directly classifying [[ each of the competing spans ]] , and training with global normalization over all [[ possible spans ]] , leads to a significant increase in performance .",0
4572,2953,"We << demonstrate >> that directly classifying [[ each of the competing spans ]] , and training with global normalization over all possible spans , leads to a [[ significant increase in performance ]] .",0
4573,2953,"We << demonstrate >> that directly classifying each of the competing spans , and training with [[ global normalization ]] over all [[ possible spans ]] , leads to a significant increase in performance .",0
4574,2953,"We << demonstrate >> that directly classifying each of the competing spans , and training with [[ global normalization ]] over all possible spans , leads to a [[ significant increase in performance ]] .",0
4575,2953,"We << demonstrate >> that directly classifying each of the competing spans , and training with global normalization over all [[ possible spans ]] , leads to a [[ significant increase in performance ]] .",0
4576,2953,"We demonstrate that << directly classifying >> [[ each of the competing spans ]] , and training with [[ global normalization ]] over all possible spans , leads to a significant increase in performance .",0
4577,2953,"We demonstrate that << directly classifying >> [[ each of the competing spans ]] , and training with global normalization over all [[ possible spans ]] , leads to a significant increase in performance .",0
4578,2953,"We demonstrate that << directly classifying >> [[ each of the competing spans ]] , and training with global normalization over all possible spans , leads to a [[ significant increase in performance ]] .",0
4579,2953,"We demonstrate that << directly classifying >> each of the competing spans , and training with [[ global normalization ]] over all [[ possible spans ]] , leads to a significant increase in performance .",0
4580,2953,"We demonstrate that << directly classifying >> each of the competing spans , and training with [[ global normalization ]] over all possible spans , leads to a [[ significant increase in performance ]] .",0
4581,2953,"We demonstrate that << directly classifying >> each of the competing spans , and training with global normalization over all [[ possible spans ]] , leads to a [[ significant increase in performance ]] .",0
4582,2953,"We demonstrate that directly classifying [[ each of the competing spans ]] , and << training with >> [[ global normalization ]] over all possible spans , leads to a significant increase in performance .",0
4583,2953,"We demonstrate that directly classifying [[ each of the competing spans ]] , and << training with >> global normalization over all [[ possible spans ]] , leads to a significant increase in performance .",0
4584,2953,"We demonstrate that directly classifying [[ each of the competing spans ]] , and << training with >> global normalization over all possible spans , leads to a [[ significant increase in performance ]] .",0
4585,2953,"We demonstrate that directly classifying each of the competing spans , and << training with >> [[ global normalization ]] over all [[ possible spans ]] , leads to a significant increase in performance .",0
4586,2953,"We demonstrate that directly classifying each of the competing spans , and << training with >> [[ global normalization ]] over all possible spans , leads to a [[ significant increase in performance ]] .",0
4587,2953,"We demonstrate that directly classifying each of the competing spans , and << training with >> global normalization over all [[ possible spans ]] , leads to a [[ significant increase in performance ]] .",0
4588,2953,"We demonstrate that directly classifying [[ each of the competing spans ]] , and training with [[ global normalization ]] << over all >> possible spans , leads to a significant increase in performance .",0
4589,2953,"We demonstrate that directly classifying [[ each of the competing spans ]] , and training with global normalization << over all >> [[ possible spans ]] , leads to a significant increase in performance .",0
4590,2953,"We demonstrate that directly classifying [[ each of the competing spans ]] , and training with global normalization << over all >> possible spans , leads to a [[ significant increase in performance ]] .",0
4591,2953,"We demonstrate that directly classifying each of the competing spans , and training with [[ global normalization ]] << over all >> [[ possible spans ]] , leads to a significant increase in performance .",1
4592,2953,"We demonstrate that directly classifying each of the competing spans , and training with [[ global normalization ]] << over all >> possible spans , leads to a [[ significant increase in performance ]] .",0
4593,2953,"We demonstrate that directly classifying each of the competing spans , and training with global normalization << over all >> [[ possible spans ]] , leads to a [[ significant increase in performance ]] .",0
4594,565,"We << propose >> a [[ novel method ]] for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4595,565,"We << propose >> a [[ novel method ]] for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4596,565,"We << propose >> a [[ novel method ]] for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4597,565,"We << propose >> a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4598,565,"We << propose >> a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4599,565,"We << propose >> a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4600,565,"We << propose >> a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4601,565,"We << propose >> a novel method for [[ question generation ]] , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4602,565,"We << propose >> a novel method for [[ question generation ]] , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4603,565,"We << propose >> a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4604,565,"We << propose >> a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4605,565,"We << propose >> a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4606,565,"We << propose >> a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4607,565,"We << propose >> a novel method for question generation , in which [[ human annotators ]] are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4608,565,"We << propose >> a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4609,565,"We << propose >> a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4610,565,"We << propose >> a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4611,565,"We << propose >> a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4612,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4613,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4614,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4615,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4616,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4617,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4618,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4619,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] [[ questions ]] that adversarially target the weaknesses .",0
4620,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the [[ weaknesses ]] .",0
4621,565,"We << propose >> a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the [[ weaknesses ]] .",0
4622,565,"We propose a [[ novel method ]] << for >> [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",1
4623,565,"We propose a [[ novel method ]] << for >> question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4624,565,"We propose a [[ novel method ]] << for >> question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4625,565,"We propose a [[ novel method ]] << for >> question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4626,565,"We propose a [[ novel method ]] << for >> question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4627,565,"We propose a [[ novel method ]] << for >> question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4628,565,"We propose a [[ novel method ]] << for >> question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4629,565,"We propose a novel method << for >> [[ question generation ]] , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4630,565,"We propose a novel method << for >> [[ question generation ]] , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4631,565,"We propose a novel method << for >> [[ question generation ]] , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4632,565,"We propose a novel method << for >> [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4633,565,"We propose a novel method << for >> [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4634,565,"We propose a novel method << for >> [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4635,565,"We propose a novel method << for >> question generation , in which [[ human annotators ]] are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4636,565,"We propose a novel method << for >> question generation , in which [[ human annotators ]] are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4637,565,"We propose a novel method << for >> question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4638,565,"We propose a novel method << for >> question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4639,565,"We propose a novel method << for >> question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4640,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the [[ workings ]] of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4641,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4642,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4643,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4644,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4645,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4646,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4647,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] [[ questions ]] that adversarially target the weaknesses .",0
4648,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the [[ weaknesses ]] .",0
4649,565,"We propose a novel method << for >> question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the [[ weaknesses ]] .",0
4650,565,"We propose a [[ novel method ]] for [[ question generation ]] , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4651,565,"We propose a [[ novel method ]] for question generation , << in which >> [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",1
4652,565,"We propose a [[ novel method ]] for question generation , << in which >> human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4653,565,"We propose a [[ novel method ]] for question generation , << in which >> human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4654,565,"We propose a [[ novel method ]] for question generation , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4655,565,"We propose a [[ novel method ]] for question generation , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4656,565,"We propose a [[ novel method ]] for question generation , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4657,565,"We propose a novel method for [[ question generation ]] , << in which >> [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4658,565,"We propose a novel method for [[ question generation ]] , << in which >> human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4659,565,"We propose a novel method for [[ question generation ]] , << in which >> human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4660,565,"We propose a novel method for [[ question generation ]] , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4661,565,"We propose a novel method for [[ question generation ]] , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4662,565,"We propose a novel method for [[ question generation ]] , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4663,565,"We propose a novel method for question generation , << in which >> [[ human annotators ]] are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4664,565,"We propose a novel method for question generation , << in which >> [[ human annotators ]] are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4665,565,"We propose a novel method for question generation , << in which >> [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4666,565,"We propose a novel method for question generation , << in which >> [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4667,565,"We propose a novel method for question generation , << in which >> [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4668,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the [[ workings ]] of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4669,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4670,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4671,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4672,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4673,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4674,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4675,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] [[ questions ]] that adversarially target the weaknesses .",0
4676,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the [[ weaknesses ]] .",0
4677,565,"We propose a novel method for question generation , << in which >> human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the [[ weaknesses ]] .",0
4678,565,"We propose a [[ novel method ]] for [[ question generation ]] , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4679,565,"We propose a [[ novel method ]] for question generation , in which [[ human annotators ]] are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4680,565,"We propose a [[ novel method ]] for question generation , in which human annotators are << educated on >> the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4681,565,"We propose a [[ novel method ]] for question generation , in which human annotators are << educated on >> the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4682,565,"We propose a [[ novel method ]] for question generation , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4683,565,"We propose a [[ novel method ]] for question generation , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4684,565,"We propose a [[ novel method ]] for question generation , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4685,565,"We propose a novel method for [[ question generation ]] , in which [[ human annotators ]] are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4686,565,"We propose a novel method for [[ question generation ]] , in which human annotators are << educated on >> the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4687,565,"We propose a novel method for [[ question generation ]] , in which human annotators are << educated on >> the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4688,565,"We propose a novel method for [[ question generation ]] , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4689,565,"We propose a novel method for [[ question generation ]] , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4690,565,"We propose a novel method for [[ question generation ]] , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4691,565,"We propose a novel method for question generation , in which [[ human annotators ]] are << educated on >> the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",1
4692,565,"We propose a novel method for question generation , in which [[ human annotators ]] are << educated on >> the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4693,565,"We propose a novel method for question generation , in which [[ human annotators ]] are << educated on >> the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4694,565,"We propose a novel method for question generation , in which [[ human annotators ]] are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4695,565,"We propose a novel method for question generation , in which [[ human annotators ]] are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4696,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the [[ workings ]] of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4697,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the [[ workings ]] of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4698,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4699,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4700,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the workings of a [[ state - of - the - art question answering model ]] , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4701,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4702,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4703,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] [[ questions ]] that adversarially target the weaknesses .",0
4704,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the [[ weaknesses ]] .",0
4705,565,"We propose a novel method for question generation , in which human annotators are << educated on >> the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the [[ weaknesses ]] .",0
4706,565,"We propose a [[ novel method ]] for [[ question generation ]] , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4707,565,"We propose a [[ novel method ]] for question generation , in which [[ human annotators ]] are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4708,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the [[ workings ]] << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4709,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings << of >> a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4710,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4711,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4712,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4713,565,"We propose a novel method for [[ question generation ]] , in which [[ human annotators ]] are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4714,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the [[ workings ]] << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4715,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings << of >> a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4716,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4717,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4718,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4719,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the [[ workings ]] << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",0
4720,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings << of >> a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",0
4721,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings << of >> a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4722,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4723,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4724,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] << of >> a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the weaknesses .",1
4725,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] << of >> a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4726,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] << of >> a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4727,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] << of >> a state - of - the - art question answering model , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4728,565,"We propose a novel method for question generation , in which human annotators are educated on the workings << of >> a [[ state - of - the - art question answering model ]] , and are asked to [[ submit ]] questions that adversarially target the weaknesses .",0
4729,565,"We propose a novel method for question generation , in which human annotators are educated on the workings << of >> a [[ state - of - the - art question answering model ]] , and are asked to submit [[ questions ]] that adversarially target the weaknesses .",0
4730,565,"We propose a novel method for question generation , in which human annotators are educated on the workings << of >> a [[ state - of - the - art question answering model ]] , and are asked to submit questions that adversarially target the [[ weaknesses ]] .",0
4731,565,"We propose a novel method for question generation , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to [[ submit ]] [[ questions ]] that adversarially target the weaknesses .",0
4732,565,"We propose a novel method for question generation , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to [[ submit ]] questions that adversarially target the [[ weaknesses ]] .",0
4733,565,"We propose a novel method for question generation , in which human annotators are educated on the workings << of >> a state - of - the - art question answering model , and are asked to submit [[ questions ]] that adversarially target the [[ weaknesses ]] .",0
4734,565,"We propose a [[ novel method ]] for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4735,565,"We propose a [[ novel method ]] for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4736,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4737,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4738,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> [[ submit ]] questions that adversarially target the weaknesses .",0
4739,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit [[ questions ]] that adversarially target the weaknesses .",0
4740,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the [[ weaknesses ]] .",0
4741,565,"We propose a novel method for [[ question generation ]] , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4742,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4743,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4744,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> [[ submit ]] questions that adversarially target the weaknesses .",0
4745,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit [[ questions ]] that adversarially target the weaknesses .",0
4746,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the [[ weaknesses ]] .",0
4747,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the [[ workings ]] of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4748,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a [[ state - of - the - art question answering model ]] , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4749,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are << asked to >> [[ submit ]] questions that adversarially target the weaknesses .",1
4750,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit [[ questions ]] that adversarially target the weaknesses .",0
4751,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the [[ weaknesses ]] .",0
4752,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a [[ state - of - the - art question answering model ]] , and are << asked to >> submit questions that adversarially target the weaknesses .",0
4753,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are << asked to >> [[ submit ]] questions that adversarially target the weaknesses .",0
4754,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are << asked to >> submit [[ questions ]] that adversarially target the weaknesses .",0
4755,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are << asked to >> submit questions that adversarially target the [[ weaknesses ]] .",0
4756,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are << asked to >> [[ submit ]] questions that adversarially target the weaknesses .",0
4757,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are << asked to >> submit [[ questions ]] that adversarially target the weaknesses .",0
4758,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are << asked to >> submit questions that adversarially target the [[ weaknesses ]] .",0
4759,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> [[ submit ]] [[ questions ]] that adversarially target the weaknesses .",0
4760,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> [[ submit ]] questions that adversarially target the [[ weaknesses ]] .",0
4761,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are << asked to >> submit [[ questions ]] that adversarially target the [[ weaknesses ]] .",0
4762,565,"We propose a [[ novel method ]] for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4763,565,"We propose a [[ novel method ]] for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4764,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4765,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4766,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that << adversarially target >> the weaknesses .",0
4767,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that << adversarially target >> the weaknesses .",0
4768,565,"We propose a [[ novel method ]] for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the [[ weaknesses ]] .",0
4769,565,"We propose a novel method for [[ question generation ]] , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4770,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4771,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4772,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that << adversarially target >> the weaknesses .",0
4773,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that << adversarially target >> the weaknesses .",0
4774,565,"We propose a novel method for [[ question generation ]] , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the [[ weaknesses ]] .",0
4775,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4776,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4777,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that << adversarially target >> the weaknesses .",0
4778,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that << adversarially target >> the weaknesses .",0
4779,565,"We propose a novel method for question generation , in which [[ human annotators ]] are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the [[ weaknesses ]] .",0
4780,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that << adversarially target >> the weaknesses .",0
4781,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that << adversarially target >> the weaknesses .",0
4782,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that << adversarially target >> the weaknesses .",0
4783,565,"We propose a novel method for question generation , in which human annotators are educated on the [[ workings ]] of a state - of - the - art question answering model , and are asked to submit questions that << adversarially target >> the [[ weaknesses ]] .",0
4784,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to [[ submit ]] questions that << adversarially target >> the weaknesses .",0
4785,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit [[ questions ]] that << adversarially target >> the weaknesses .",0
4786,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a [[ state - of - the - art question answering model ]] , and are asked to submit questions that << adversarially target >> the [[ weaknesses ]] .",0
4787,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] [[ questions ]] that << adversarially target >> the weaknesses .",0
4788,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to [[ submit ]] questions that << adversarially target >> the [[ weaknesses ]] .",0
4789,565,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit [[ questions ]] that << adversarially target >> the [[ weaknesses ]] .",1
4790,6108,The results ( Table 3 ) << show >> that [[ our models ]] are better than [[ ABS + ]] .,0
4791,6108,The results ( Table 3 ) show that [[ our models ]] are << better than >> [[ ABS + ]] .,1
4792,4345,"<< For >> [[ each token t ]] , we encode both its [[ prefix and suffix ]] in both the forward and reverse direction .",0
4793,4345,"<< For >> [[ each token t ]] , we encode both its prefix and suffix in both the [[ forward and reverse direction ]] .",0
4794,4345,"<< For >> each token t , we encode both its [[ prefix and suffix ]] in both the [[ forward and reverse direction ]] .",0
4795,4345,"For [[ each token t ]] , we << encode both >> its [[ prefix and suffix ]] in both the forward and reverse direction .",1
4796,4345,"For [[ each token t ]] , we << encode both >> its prefix and suffix in both the [[ forward and reverse direction ]] .",0
4797,4345,"For each token t , we << encode both >> its [[ prefix and suffix ]] in both the [[ forward and reverse direction ]] .",0
4798,4345,"For [[ each token t ]] , we encode both its [[ prefix and suffix ]] << in both >> the forward and reverse direction .",0
4799,4345,"For [[ each token t ]] , we encode both its prefix and suffix << in both >> the [[ forward and reverse direction ]] .",0
4800,4345,"For each token t , we encode both its [[ prefix and suffix ]] << in both >> the [[ forward and reverse direction ]] .",1
4801,2524,"Furthermore , the [[ paraphrases ]] << generated by >> [[ our system ]] are well - formed , semantically sensible , and grammatically correct for the most part .",1
4802,2524,"Furthermore , the [[ paraphrases ]] << generated by >> our system are [[ well - formed ]] , semantically sensible , and grammatically correct for the most part .",0
4803,2524,"Furthermore , the [[ paraphrases ]] << generated by >> our system are well - formed , [[ semantically sensible ]] , and grammatically correct for the most part .",0
4804,2524,"Furthermore , the [[ paraphrases ]] << generated by >> our system are well - formed , semantically sensible , and [[ grammatically correct ]] for the most part .",0
4805,2524,"Furthermore , the paraphrases << generated by >> [[ our system ]] are [[ well - formed ]] , semantically sensible , and grammatically correct for the most part .",0
4806,2524,"Furthermore , the paraphrases << generated by >> [[ our system ]] are well - formed , [[ semantically sensible ]] , and grammatically correct for the most part .",0
4807,2524,"Furthermore , the paraphrases << generated by >> [[ our system ]] are well - formed , semantically sensible , and [[ grammatically correct ]] for the most part .",0
4808,2524,"Furthermore , the paraphrases << generated by >> our system are [[ well - formed ]] , [[ semantically sensible ]] , and grammatically correct for the most part .",0
4809,2524,"Furthermore , the paraphrases << generated by >> our system are [[ well - formed ]] , semantically sensible , and [[ grammatically correct ]] for the most part .",0
4810,2524,"Furthermore , the paraphrases << generated by >> our system are well - formed , [[ semantically sensible ]] , and [[ grammatically correct ]] for the most part .",0
4811,2524,"Furthermore , the [[ paraphrases ]] generated by [[ our system ]] << are >> well - formed , semantically sensible , and grammatically correct for the most part .",0
4812,2524,"Furthermore , the [[ paraphrases ]] generated by our system << are >> [[ well - formed ]] , semantically sensible , and grammatically correct for the most part .",1
4813,2524,"Furthermore , the [[ paraphrases ]] generated by our system << are >> well - formed , [[ semantically sensible ]] , and grammatically correct for the most part .",1
4814,2524,"Furthermore , the [[ paraphrases ]] generated by our system << are >> well - formed , semantically sensible , and [[ grammatically correct ]] for the most part .",1
4815,2524,"Furthermore , the paraphrases generated by [[ our system ]] << are >> [[ well - formed ]] , semantically sensible , and grammatically correct for the most part .",0
4816,2524,"Furthermore , the paraphrases generated by [[ our system ]] << are >> well - formed , [[ semantically sensible ]] , and grammatically correct for the most part .",0
4817,2524,"Furthermore , the paraphrases generated by [[ our system ]] << are >> well - formed , semantically sensible , and [[ grammatically correct ]] for the most part .",0
4818,2524,"Furthermore , the paraphrases generated by our system << are >> [[ well - formed ]] , [[ semantically sensible ]] , and grammatically correct for the most part .",0
4819,2524,"Furthermore , the paraphrases generated by our system << are >> [[ well - formed ]] , semantically sensible , and [[ grammatically correct ]] for the most part .",0
4820,2524,"Furthermore , the paraphrases generated by our system << are >> well - formed , [[ semantically sensible ]] , and [[ grammatically correct ]] for the most part .",0
4821,2502,This model thus << provides >> a [[ ' global ' loss ]] that ensures the [[ sentence embedding ]] as a whole is close to other semantically related sentence embeddings .,0
4822,2502,This model thus << provides >> a [[ ' global ' loss ]] that ensures the sentence embedding as a whole is close to [[ other semantically related sentence embeddings ]] .,0
4823,2502,This model thus << provides >> a ' global ' loss that ensures the [[ sentence embedding ]] as a whole is close to [[ other semantically related sentence embeddings ]] .,0
4824,2502,This model thus provides a [[ ' global ' loss ]] that << ensures >> the [[ sentence embedding ]] as a whole is close to other semantically related sentence embeddings .,1
4825,2502,This model thus provides a [[ ' global ' loss ]] that << ensures >> the sentence embedding as a whole is close to [[ other semantically related sentence embeddings ]] .,0
4826,2502,This model thus provides a ' global ' loss that << ensures >> the [[ sentence embedding ]] as a whole is close to [[ other semantically related sentence embeddings ]] .,0
4827,2502,This model thus provides a [[ ' global ' loss ]] that ensures the [[ sentence embedding ]] as a whole is << close to >> other semantically related sentence embeddings .,0
4828,2502,This model thus provides a [[ ' global ' loss ]] that ensures the sentence embedding as a whole is << close to >> [[ other semantically related sentence embeddings ]] .,0
4829,2502,This model thus provides a ' global ' loss that ensures the [[ sentence embedding ]] as a whole is << close to >> [[ other semantically related sentence embeddings ]] .,1
4830,317,"<< To capture >> [[ local and non-local contexts ]] , [[ states ]] are updated recurrently by exchanging information between each other .",0
4831,317,"<< To capture >> [[ local and non-local contexts ]] , states are updated recurrently by [[ exchanging information between each other ]] .",0
4832,317,"<< To capture >> local and non-local contexts , [[ states ]] are updated recurrently by [[ exchanging information between each other ]] .",0
4833,317,"To capture [[ local and non-local contexts ]] , [[ states ]] are << updated recurrently >> by exchanging information between each other .",1
4834,317,"To capture [[ local and non-local contexts ]] , states are << updated recurrently >> by [[ exchanging information between each other ]] .",0
4835,317,"To capture local and non-local contexts , [[ states ]] are << updated recurrently >> by [[ exchanging information between each other ]] .",0
4836,317,"To capture [[ local and non-local contexts ]] , [[ states ]] are updated recurrently << by >> exchanging information between each other .",0
4837,317,"To capture [[ local and non-local contexts ]] , states are updated recurrently << by >> [[ exchanging information between each other ]] .",0
4838,317,"To capture local and non-local contexts , [[ states ]] are updated recurrently << by >> [[ exchanging information between each other ]] .",1
4839,5203,"Specifically , we << explore >> [[ two transfer methods ]] to incorporate this sort of [[ knowledge ]] - pretraining and multi-task learning .",0
4840,5203,"Specifically , we << explore >> [[ two transfer methods ]] to incorporate this sort of knowledge - [[ pretraining ]] and multi-task learning .",0
4841,5203,"Specifically , we << explore >> [[ two transfer methods ]] to incorporate this sort of knowledge - pretraining and [[ multi-task learning ]] .",0
4842,5203,"Specifically , we << explore >> two transfer methods to incorporate this sort of [[ knowledge ]] - [[ pretraining ]] and multi-task learning .",0
4843,5203,"Specifically , we << explore >> two transfer methods to incorporate this sort of [[ knowledge ]] - pretraining and [[ multi-task learning ]] .",0
4844,5203,"Specifically , we << explore >> two transfer methods to incorporate this sort of knowledge - [[ pretraining ]] and [[ multi-task learning ]] .",0
4845,5203,"Specifically , we explore [[ two transfer methods ]] << to incorporate >> this sort of [[ knowledge ]] - pretraining and multi-task learning .",1
4846,5203,"Specifically , we explore [[ two transfer methods ]] << to incorporate >> this sort of knowledge - [[ pretraining ]] and multi-task learning .",0
4847,5203,"Specifically , we explore [[ two transfer methods ]] << to incorporate >> this sort of knowledge - pretraining and [[ multi-task learning ]] .",0
4848,5203,"Specifically , we explore two transfer methods << to incorporate >> this sort of [[ knowledge ]] - [[ pretraining ]] and multi-task learning .",0
4849,5203,"Specifically , we explore two transfer methods << to incorporate >> this sort of [[ knowledge ]] - pretraining and [[ multi-task learning ]] .",0
4850,5203,"Specifically , we explore two transfer methods << to incorporate >> this sort of knowledge - [[ pretraining ]] and [[ multi-task learning ]] .",0
4851,652,"Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach << dynamically decides >> the [[ reward and the baseline ]] according to [[ two sampling strategies ]] , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .",0
4852,652,"Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach dynamically decides the [[ reward and the baseline ]] << according to >> [[ two sampling strategies ]] , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .",1
4853,5763,"Specifically , a [[ seq2seq architecture with attention mechanism ]] is << employed >> as the [[ basic summary generator ]] .",0
4854,5763,"Specifically , a [[ seq2seq architecture with attention mechanism ]] is employed << as >> the [[ basic summary generator ]] .",1
4855,2658,It is somewhat unexpected that our model << produces >> [[ about 7 % absolute lower LAS score ]] than [[ Stack - propagation ]] on Dutch ( nl ) .,0
4856,2658,It is somewhat unexpected that our model << produces >> [[ about 7 % absolute lower LAS score ]] than Stack - propagation on [[ Dutch ( nl ) ]] .,0
4857,2658,It is somewhat unexpected that our model << produces >> about 7 % absolute lower LAS score than [[ Stack - propagation ]] on [[ Dutch ( nl ) ]] .,0
4858,2658,It is somewhat unexpected that our model produces [[ about 7 % absolute lower LAS score ]] << than >> [[ Stack - propagation ]] on Dutch ( nl ) .,1
4859,2658,It is somewhat unexpected that our model produces [[ about 7 % absolute lower LAS score ]] << than >> Stack - propagation on [[ Dutch ( nl ) ]] .,0
4860,2658,It is somewhat unexpected that our model produces about 7 % absolute lower LAS score << than >> [[ Stack - propagation ]] on [[ Dutch ( nl ) ]] .,0
4861,2658,It is somewhat unexpected that our model produces [[ about 7 % absolute lower LAS score ]] than [[ Stack - propagation ]] << on >> Dutch ( nl ) .,0
4862,2658,It is somewhat unexpected that our model produces [[ about 7 % absolute lower LAS score ]] than Stack - propagation << on >> [[ Dutch ( nl ) ]] .,0
4863,2658,It is somewhat unexpected that our model produces about 7 % absolute lower LAS score than [[ Stack - propagation ]] << on >> [[ Dutch ( nl ) ]] .,1
4864,1174,"<< shows >> the [[ consistent performance gain ]] of [[ DCN + ]] over the baseline across question types , question lengths , and answer lengths .",0
4865,1174,"<< shows >> the [[ consistent performance gain ]] of DCN + over the [[ baseline ]] across question types , question lengths , and answer lengths .",0
4866,1174,"<< shows >> the [[ consistent performance gain ]] of DCN + over the baseline across [[ question types ]] , question lengths , and answer lengths .",0
4867,1174,"<< shows >> the [[ consistent performance gain ]] of DCN + over the baseline across question types , [[ question lengths ]] , and answer lengths .",0
4868,1174,"<< shows >> the [[ consistent performance gain ]] of DCN + over the baseline across question types , question lengths , and [[ answer lengths ]] .",0
4869,1174,"<< shows >> the consistent performance gain of [[ DCN + ]] over the [[ baseline ]] across question types , question lengths , and answer lengths .",0
4870,1174,"<< shows >> the consistent performance gain of [[ DCN + ]] over the baseline across [[ question types ]] , question lengths , and answer lengths .",0
4871,1174,"<< shows >> the consistent performance gain of [[ DCN + ]] over the baseline across question types , [[ question lengths ]] , and answer lengths .",0
4872,1174,"<< shows >> the consistent performance gain of [[ DCN + ]] over the baseline across question types , question lengths , and [[ answer lengths ]] .",0
4873,1174,"<< shows >> the consistent performance gain of DCN + over the [[ baseline ]] across [[ question types ]] , question lengths , and answer lengths .",0
4874,1174,"<< shows >> the consistent performance gain of DCN + over the [[ baseline ]] across question types , [[ question lengths ]] , and answer lengths .",0
4875,1174,"<< shows >> the consistent performance gain of DCN + over the [[ baseline ]] across question types , question lengths , and [[ answer lengths ]] .",0
4876,1174,"<< shows >> the consistent performance gain of DCN + over the baseline across [[ question types ]] , [[ question lengths ]] , and answer lengths .",0
4877,1174,"<< shows >> the consistent performance gain of DCN + over the baseline across [[ question types ]] , question lengths , and [[ answer lengths ]] .",0
4878,1174,"<< shows >> the consistent performance gain of DCN + over the baseline across question types , [[ question lengths ]] , and [[ answer lengths ]] .",0
4879,1174,"shows the [[ consistent performance gain ]] << of >> [[ DCN + ]] over the baseline across question types , question lengths , and answer lengths .",1
4880,1174,"shows the [[ consistent performance gain ]] << of >> DCN + over the [[ baseline ]] across question types , question lengths , and answer lengths .",0
4881,1174,"shows the [[ consistent performance gain ]] << of >> DCN + over the baseline across [[ question types ]] , question lengths , and answer lengths .",0
4882,1174,"shows the [[ consistent performance gain ]] << of >> DCN + over the baseline across question types , [[ question lengths ]] , and answer lengths .",0
4883,1174,"shows the [[ consistent performance gain ]] << of >> DCN + over the baseline across question types , question lengths , and [[ answer lengths ]] .",0
4884,1174,"shows the consistent performance gain << of >> [[ DCN + ]] over the [[ baseline ]] across question types , question lengths , and answer lengths .",0
4885,1174,"shows the consistent performance gain << of >> [[ DCN + ]] over the baseline across [[ question types ]] , question lengths , and answer lengths .",0
4886,1174,"shows the consistent performance gain << of >> [[ DCN + ]] over the baseline across question types , [[ question lengths ]] , and answer lengths .",0
4887,1174,"shows the consistent performance gain << of >> [[ DCN + ]] over the baseline across question types , question lengths , and [[ answer lengths ]] .",0
4888,1174,"shows the consistent performance gain << of >> DCN + over the [[ baseline ]] across [[ question types ]] , question lengths , and answer lengths .",0
4889,1174,"shows the consistent performance gain << of >> DCN + over the [[ baseline ]] across question types , [[ question lengths ]] , and answer lengths .",0
4890,1174,"shows the consistent performance gain << of >> DCN + over the [[ baseline ]] across question types , question lengths , and [[ answer lengths ]] .",0
4891,1174,"shows the consistent performance gain << of >> DCN + over the baseline across [[ question types ]] , [[ question lengths ]] , and answer lengths .",0
4892,1174,"shows the consistent performance gain << of >> DCN + over the baseline across [[ question types ]] , question lengths , and [[ answer lengths ]] .",0
4893,1174,"shows the consistent performance gain << of >> DCN + over the baseline across question types , [[ question lengths ]] , and [[ answer lengths ]] .",0
4894,1174,"shows the [[ consistent performance gain ]] of [[ DCN + ]] << over >> the baseline across question types , question lengths , and answer lengths .",0
4895,1174,"shows the [[ consistent performance gain ]] of DCN + << over >> the [[ baseline ]] across question types , question lengths , and answer lengths .",1
4896,1174,"shows the [[ consistent performance gain ]] of DCN + << over >> the baseline across [[ question types ]] , question lengths , and answer lengths .",0
4897,1174,"shows the [[ consistent performance gain ]] of DCN + << over >> the baseline across question types , [[ question lengths ]] , and answer lengths .",0
4898,1174,"shows the [[ consistent performance gain ]] of DCN + << over >> the baseline across question types , question lengths , and [[ answer lengths ]] .",0
4899,1174,"shows the consistent performance gain of [[ DCN + ]] << over >> the [[ baseline ]] across question types , question lengths , and answer lengths .",0
4900,1174,"shows the consistent performance gain of [[ DCN + ]] << over >> the baseline across [[ question types ]] , question lengths , and answer lengths .",0
4901,1174,"shows the consistent performance gain of [[ DCN + ]] << over >> the baseline across question types , [[ question lengths ]] , and answer lengths .",0
4902,1174,"shows the consistent performance gain of [[ DCN + ]] << over >> the baseline across question types , question lengths , and [[ answer lengths ]] .",0
4903,1174,"shows the consistent performance gain of DCN + << over >> the [[ baseline ]] across [[ question types ]] , question lengths , and answer lengths .",0
4904,1174,"shows the consistent performance gain of DCN + << over >> the [[ baseline ]] across question types , [[ question lengths ]] , and answer lengths .",0
4905,1174,"shows the consistent performance gain of DCN + << over >> the [[ baseline ]] across question types , question lengths , and [[ answer lengths ]] .",0
4906,1174,"shows the consistent performance gain of DCN + << over >> the baseline across [[ question types ]] , [[ question lengths ]] , and answer lengths .",0
4907,1174,"shows the consistent performance gain of DCN + << over >> the baseline across [[ question types ]] , question lengths , and [[ answer lengths ]] .",0
4908,1174,"shows the consistent performance gain of DCN + << over >> the baseline across question types , [[ question lengths ]] , and [[ answer lengths ]] .",0
4909,1174,"shows the [[ consistent performance gain ]] of [[ DCN + ]] over the baseline << across >> question types , question lengths , and answer lengths .",0
4910,1174,"shows the [[ consistent performance gain ]] of DCN + over the [[ baseline ]] << across >> question types , question lengths , and answer lengths .",0
4911,1174,"shows the [[ consistent performance gain ]] of DCN + over the baseline << across >> [[ question types ]] , question lengths , and answer lengths .",1
4912,1174,"shows the [[ consistent performance gain ]] of DCN + over the baseline << across >> question types , [[ question lengths ]] , and answer lengths .",1
4913,1174,"shows the [[ consistent performance gain ]] of DCN + over the baseline << across >> question types , question lengths , and [[ answer lengths ]] .",1
4914,1174,"shows the consistent performance gain of [[ DCN + ]] over the [[ baseline ]] << across >> question types , question lengths , and answer lengths .",0
4915,1174,"shows the consistent performance gain of [[ DCN + ]] over the baseline << across >> [[ question types ]] , question lengths , and answer lengths .",0
4916,1174,"shows the consistent performance gain of [[ DCN + ]] over the baseline << across >> question types , [[ question lengths ]] , and answer lengths .",0
4917,1174,"shows the consistent performance gain of [[ DCN + ]] over the baseline << across >> question types , question lengths , and [[ answer lengths ]] .",0
4918,1174,"shows the consistent performance gain of DCN + over the [[ baseline ]] << across >> [[ question types ]] , question lengths , and answer lengths .",0
4919,1174,"shows the consistent performance gain of DCN + over the [[ baseline ]] << across >> question types , [[ question lengths ]] , and answer lengths .",0
4920,1174,"shows the consistent performance gain of DCN + over the [[ baseline ]] << across >> question types , question lengths , and [[ answer lengths ]] .",0
4921,1174,"shows the consistent performance gain of DCN + over the baseline << across >> [[ question types ]] , [[ question lengths ]] , and answer lengths .",0
4922,1174,"shows the consistent performance gain of DCN + over the baseline << across >> [[ question types ]] , question lengths , and [[ answer lengths ]] .",0
4923,1174,"shows the consistent performance gain of DCN + over the baseline << across >> question types , [[ question lengths ]] , and [[ answer lengths ]] .",0
4924,6085,"The [[ coverage penalty parameter ]] ? is << set to >> [[ 10 ]] , and the copy attention normalization parameter ? to 2 for both approaches .",1
4925,6085,"The [[ coverage penalty parameter ]] ? is << set to >> 10 , and the [[ copy attention normalization parameter ]] ? to 2 for both approaches .",0
4926,6085,"The [[ coverage penalty parameter ]] ? is << set to >> 10 , and the copy attention normalization parameter ? to [[ 2 ]] for both approaches .",0
4927,6085,"The coverage penalty parameter ? is << set to >> [[ 10 ]] , and the [[ copy attention normalization parameter ]] ? to 2 for both approaches .",0
4928,6085,"The coverage penalty parameter ? is << set to >> [[ 10 ]] , and the copy attention normalization parameter ? to [[ 2 ]] for both approaches .",0
4929,6085,"The coverage penalty parameter ? is << set to >> 10 , and the [[ copy attention normalization parameter ]] ? to [[ 2 ]] for both approaches .",0
4930,6085,"The [[ coverage penalty parameter ]] ? is set to [[ 10 ]] , and the copy attention normalization parameter ? << to >> 2 for both approaches .",0
4931,6085,"The [[ coverage penalty parameter ]] ? is set to 10 , and the [[ copy attention normalization parameter ]] ? << to >> 2 for both approaches .",0
4932,6085,"The [[ coverage penalty parameter ]] ? is set to 10 , and the copy attention normalization parameter ? << to >> [[ 2 ]] for both approaches .",0
4933,6085,"The coverage penalty parameter ? is set to [[ 10 ]] , and the [[ copy attention normalization parameter ]] ? << to >> 2 for both approaches .",0
4934,6085,"The coverage penalty parameter ? is set to [[ 10 ]] , and the copy attention normalization parameter ? << to >> [[ 2 ]] for both approaches .",0
4935,6085,"The coverage penalty parameter ? is set to 10 , and the [[ copy attention normalization parameter ]] ? << to >> [[ 2 ]] for both approaches .",1
4936,410,We used a [[ single NVIDIA Titan Xp ( 12GB ) GPU ]] << to fine - tune >> [[ BioBERT ]] on each task .,1
4937,410,We used a [[ single NVIDIA Titan Xp ( 12GB ) GPU ]] << to fine - tune >> BioBERT on [[ each task ]] .,0
4938,410,We used a single NVIDIA Titan Xp ( 12GB ) GPU << to fine - tune >> [[ BioBERT ]] on [[ each task ]] .,0
4939,410,We used a [[ single NVIDIA Titan Xp ( 12GB ) GPU ]] to fine - tune [[ BioBERT ]] << on >> each task .,0
4940,410,We used a [[ single NVIDIA Titan Xp ( 12GB ) GPU ]] to fine - tune BioBERT << on >> [[ each task ]] .,0
4941,410,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune [[ BioBERT ]] << on >> [[ each task ]] .,1
4942,4064,"While not explored here , an [[ intriguing potential ]] << of >> [[ this work ]] is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .",1
4943,4064,"While not explored here , an [[ intriguing potential ]] << of >> this work is in deriving [[ sentence simplification models ]] that are personalized for individual users , based on their reading behavior .",0
4944,4064,"While not explored here , an [[ intriguing potential ]] << of >> this work is in deriving sentence simplification models that are [[ personalized ]] for individual users , based on their reading behavior .",0
4945,4064,"While not explored here , an [[ intriguing potential ]] << of >> this work is in deriving sentence simplification models that are personalized for [[ individual users ]] , based on their reading behavior .",0
4946,4064,"While not explored here , an [[ intriguing potential ]] << of >> this work is in deriving sentence simplification models that are personalized for individual users , based on [[ their reading behavior ]] .",0
4947,4064,"While not explored here , an intriguing potential << of >> [[ this work ]] is in deriving [[ sentence simplification models ]] that are personalized for individual users , based on their reading behavior .",0
4948,4064,"While not explored here , an intriguing potential << of >> [[ this work ]] is in deriving sentence simplification models that are [[ personalized ]] for individual users , based on their reading behavior .",0
4949,4064,"While not explored here , an intriguing potential << of >> [[ this work ]] is in deriving sentence simplification models that are personalized for [[ individual users ]] , based on their reading behavior .",0
4950,4064,"While not explored here , an intriguing potential << of >> [[ this work ]] is in deriving sentence simplification models that are personalized for individual users , based on [[ their reading behavior ]] .",0
4951,4064,"While not explored here , an intriguing potential << of >> this work is in deriving [[ sentence simplification models ]] that are [[ personalized ]] for individual users , based on their reading behavior .",0
4952,4064,"While not explored here , an intriguing potential << of >> this work is in deriving [[ sentence simplification models ]] that are personalized for [[ individual users ]] , based on their reading behavior .",0
4953,4064,"While not explored here , an intriguing potential << of >> this work is in deriving [[ sentence simplification models ]] that are personalized for individual users , based on [[ their reading behavior ]] .",0
4954,4064,"While not explored here , an intriguing potential << of >> this work is in deriving sentence simplification models that are [[ personalized ]] for [[ individual users ]] , based on their reading behavior .",0
4955,4064,"While not explored here , an intriguing potential << of >> this work is in deriving sentence simplification models that are [[ personalized ]] for individual users , based on [[ their reading behavior ]] .",0
4956,4064,"While not explored here , an intriguing potential << of >> this work is in deriving sentence simplification models that are personalized for [[ individual users ]] , based on [[ their reading behavior ]] .",0
4957,4064,"While not explored here , an [[ intriguing potential ]] of [[ this work ]] is << in deriving >> sentence simplification models that are personalized for individual users , based on their reading behavior .",0
4958,4064,"While not explored here , an [[ intriguing potential ]] of this work is << in deriving >> [[ sentence simplification models ]] that are personalized for individual users , based on their reading behavior .",0
4959,4064,"While not explored here , an [[ intriguing potential ]] of this work is << in deriving >> sentence simplification models that are [[ personalized ]] for individual users , based on their reading behavior .",0
4960,4064,"While not explored here , an [[ intriguing potential ]] of this work is << in deriving >> sentence simplification models that are personalized for [[ individual users ]] , based on their reading behavior .",0
4961,4064,"While not explored here , an [[ intriguing potential ]] of this work is << in deriving >> sentence simplification models that are personalized for individual users , based on [[ their reading behavior ]] .",0
4962,4064,"While not explored here , an intriguing potential of [[ this work ]] is << in deriving >> [[ sentence simplification models ]] that are personalized for individual users , based on their reading behavior .",1
4963,4064,"While not explored here , an intriguing potential of [[ this work ]] is << in deriving >> sentence simplification models that are [[ personalized ]] for individual users , based on their reading behavior .",0
4964,4064,"While not explored here , an intriguing potential of [[ this work ]] is << in deriving >> sentence simplification models that are personalized for [[ individual users ]] , based on their reading behavior .",0
4965,4064,"While not explored here , an intriguing potential of [[ this work ]] is << in deriving >> sentence simplification models that are personalized for individual users , based on [[ their reading behavior ]] .",0
4966,4064,"While not explored here , an intriguing potential of this work is << in deriving >> [[ sentence simplification models ]] that are [[ personalized ]] for individual users , based on their reading behavior .",0
4967,4064,"While not explored here , an intriguing potential of this work is << in deriving >> [[ sentence simplification models ]] that are personalized for [[ individual users ]] , based on their reading behavior .",0
4968,4064,"While not explored here , an intriguing potential of this work is << in deriving >> [[ sentence simplification models ]] that are personalized for individual users , based on [[ their reading behavior ]] .",0
4969,4064,"While not explored here , an intriguing potential of this work is << in deriving >> sentence simplification models that are [[ personalized ]] for [[ individual users ]] , based on their reading behavior .",0
4970,4064,"While not explored here , an intriguing potential of this work is << in deriving >> sentence simplification models that are [[ personalized ]] for individual users , based on [[ their reading behavior ]] .",0
4971,4064,"While not explored here , an intriguing potential of this work is << in deriving >> sentence simplification models that are personalized for [[ individual users ]] , based on [[ their reading behavior ]] .",0
4972,4064,"While not explored here , an [[ intriguing potential ]] of [[ this work ]] is in deriving sentence simplification models << that are >> personalized for individual users , based on their reading behavior .",0
4973,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving [[ sentence simplification models ]] << that are >> personalized for individual users , based on their reading behavior .",0
4974,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models << that are >> [[ personalized ]] for individual users , based on their reading behavior .",0
4975,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models << that are >> personalized for [[ individual users ]] , based on their reading behavior .",0
4976,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models << that are >> personalized for individual users , based on [[ their reading behavior ]] .",0
4977,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving [[ sentence simplification models ]] << that are >> personalized for individual users , based on their reading behavior .",0
4978,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models << that are >> [[ personalized ]] for individual users , based on their reading behavior .",0
4979,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models << that are >> personalized for [[ individual users ]] , based on their reading behavior .",0
4980,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models << that are >> personalized for individual users , based on [[ their reading behavior ]] .",0
4981,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] << that are >> [[ personalized ]] for individual users , based on their reading behavior .",1
4982,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] << that are >> personalized for [[ individual users ]] , based on their reading behavior .",0
4983,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] << that are >> personalized for individual users , based on [[ their reading behavior ]] .",0
4984,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models << that are >> [[ personalized ]] for [[ individual users ]] , based on their reading behavior .",0
4985,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models << that are >> [[ personalized ]] for individual users , based on [[ their reading behavior ]] .",0
4986,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models << that are >> personalized for [[ individual users ]] , based on [[ their reading behavior ]] .",0
4987,4064,"While not explored here , an [[ intriguing potential ]] of [[ this work ]] is in deriving sentence simplification models that are personalized << for >> individual users , based on their reading behavior .",0
4988,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving [[ sentence simplification models ]] that are personalized << for >> individual users , based on their reading behavior .",0
4989,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models that are [[ personalized ]] << for >> individual users , based on their reading behavior .",0
4990,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models that are personalized << for >> [[ individual users ]] , based on their reading behavior .",0
4991,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models that are personalized << for >> individual users , based on [[ their reading behavior ]] .",0
4992,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving [[ sentence simplification models ]] that are personalized << for >> individual users , based on their reading behavior .",0
4993,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models that are [[ personalized ]] << for >> individual users , based on their reading behavior .",0
4994,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models that are personalized << for >> [[ individual users ]] , based on their reading behavior .",0
4995,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models that are personalized << for >> individual users , based on [[ their reading behavior ]] .",0
4996,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] that are [[ personalized ]] << for >> individual users , based on their reading behavior .",0
4997,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] that are personalized << for >> [[ individual users ]] , based on their reading behavior .",0
4998,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] that are personalized << for >> individual users , based on [[ their reading behavior ]] .",0
4999,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are [[ personalized ]] << for >> [[ individual users ]] , based on their reading behavior .",1
5000,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are [[ personalized ]] << for >> individual users , based on [[ their reading behavior ]] .",0
5001,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized << for >> [[ individual users ]] , based on [[ their reading behavior ]] .",0
5002,4064,"While not explored here , an [[ intriguing potential ]] of [[ this work ]] is in deriving sentence simplification models that are personalized for individual users , << based on >> their reading behavior .",0
5003,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving [[ sentence simplification models ]] that are personalized for individual users , << based on >> their reading behavior .",0
5004,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models that are [[ personalized ]] for individual users , << based on >> their reading behavior .",0
5005,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models that are personalized for [[ individual users ]] , << based on >> their reading behavior .",0
5006,4064,"While not explored here , an [[ intriguing potential ]] of this work is in deriving sentence simplification models that are personalized for individual users , << based on >> [[ their reading behavior ]] .",0
5007,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving [[ sentence simplification models ]] that are personalized for individual users , << based on >> their reading behavior .",0
5008,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models that are [[ personalized ]] for individual users , << based on >> their reading behavior .",0
5009,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models that are personalized for [[ individual users ]] , << based on >> their reading behavior .",0
5010,4064,"While not explored here , an intriguing potential of [[ this work ]] is in deriving sentence simplification models that are personalized for individual users , << based on >> [[ their reading behavior ]] .",0
5011,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] that are [[ personalized ]] for individual users , << based on >> their reading behavior .",0
5012,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] that are personalized for [[ individual users ]] , << based on >> their reading behavior .",0
5013,4064,"While not explored here , an intriguing potential of this work is in deriving [[ sentence simplification models ]] that are personalized for individual users , << based on >> [[ their reading behavior ]] .",0
5014,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are [[ personalized ]] for [[ individual users ]] , << based on >> their reading behavior .",0
5015,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are [[ personalized ]] for individual users , << based on >> [[ their reading behavior ]] .",1
5016,4064,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for [[ individual users ]] , << based on >> [[ their reading behavior ]] .",0
5017,4635,"In experiments , we first << randomly select >> [[ 20 % ]] of [[ training data ]] as validation set to tune the hyperparameters .",0
5018,4635,"In experiments , we first << randomly select >> [[ 20 % ]] of training data as [[ validation set ]] to tune the hyperparameters .",0
5019,4635,"In experiments , we first << randomly select >> [[ 20 % ]] of training data as validation set to tune the [[ hyperparameters ]] .",0
5020,4635,"In experiments , we first << randomly select >> 20 % of [[ training data ]] as [[ validation set ]] to tune the hyperparameters .",0
5021,4635,"In experiments , we first << randomly select >> 20 % of [[ training data ]] as validation set to tune the [[ hyperparameters ]] .",0
5022,4635,"In experiments , we first << randomly select >> 20 % of training data as [[ validation set ]] to tune the [[ hyperparameters ]] .",0
5023,4635,"In experiments , we first randomly select [[ 20 % ]] << of >> [[ training data ]] as validation set to tune the hyperparameters .",1
5024,4635,"In experiments , we first randomly select [[ 20 % ]] << of >> training data as [[ validation set ]] to tune the hyperparameters .",0
5025,4635,"In experiments , we first randomly select [[ 20 % ]] << of >> training data as validation set to tune the [[ hyperparameters ]] .",0
5026,4635,"In experiments , we first randomly select 20 % << of >> [[ training data ]] as [[ validation set ]] to tune the hyperparameters .",0
5027,4635,"In experiments , we first randomly select 20 % << of >> [[ training data ]] as validation set to tune the [[ hyperparameters ]] .",0
5028,4635,"In experiments , we first randomly select 20 % << of >> training data as [[ validation set ]] to tune the [[ hyperparameters ]] .",0
5029,4635,"In experiments , we first randomly select [[ 20 % ]] of [[ training data ]] << as >> validation set to tune the hyperparameters .",0
5030,4635,"In experiments , we first randomly select [[ 20 % ]] of training data << as >> [[ validation set ]] to tune the hyperparameters .",0
5031,4635,"In experiments , we first randomly select [[ 20 % ]] of training data << as >> validation set to tune the [[ hyperparameters ]] .",0
5032,4635,"In experiments , we first randomly select 20 % of [[ training data ]] << as >> [[ validation set ]] to tune the hyperparameters .",1
5033,4635,"In experiments , we first randomly select 20 % of [[ training data ]] << as >> validation set to tune the [[ hyperparameters ]] .",0
5034,4635,"In experiments , we first randomly select 20 % of training data << as >> [[ validation set ]] to tune the [[ hyperparameters ]] .",0
5035,4635,"In experiments , we first randomly select [[ 20 % ]] of [[ training data ]] as validation set << to tune >> the hyperparameters .",0
5036,4635,"In experiments , we first randomly select [[ 20 % ]] of training data as [[ validation set ]] << to tune >> the hyperparameters .",0
5037,4635,"In experiments , we first randomly select [[ 20 % ]] of training data as validation set << to tune >> the [[ hyperparameters ]] .",0
5038,4635,"In experiments , we first randomly select 20 % of [[ training data ]] as [[ validation set ]] << to tune >> the hyperparameters .",0
5039,4635,"In experiments , we first randomly select 20 % of [[ training data ]] as validation set << to tune >> the [[ hyperparameters ]] .",1
5040,4635,"In experiments , we first randomly select 20 % of training data as [[ validation set ]] << to tune >> the [[ hyperparameters ]] .",0
5041,4792,"<< To prevent >> [[ false alignment ]] , we adopt the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) to semantically align aspect - specific representations .",0
5042,4792,"<< To prevent >> [[ false alignment ]] , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align [[ aspect - specific representations ]] .",0
5043,4792,"<< To prevent >> false alignment , we adopt the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) to semantically align [[ aspect - specific representations ]] .",0
5044,4792,"To prevent [[ false alignment ]] , we << adopt >> the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) to semantically align aspect - specific representations .",1
5045,4792,"To prevent [[ false alignment ]] , we << adopt >> the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align [[ aspect - specific representations ]] .",0
5046,4792,"To prevent false alignment , we << adopt >> the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) to semantically align [[ aspect - specific representations ]] .",0
5047,4792,"To prevent [[ false alignment ]] , we adopt the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) << to semantically align >> aspect - specific representations .",0
5048,4792,"To prevent [[ false alignment ]] , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) << to semantically align >> [[ aspect - specific representations ]] .",0
5049,4792,"To prevent false alignment , we adopt the [[ Contrastive Feature Alignment ( CFA ) ]] ( Motiian et al. 2017 ) << to semantically align >> [[ aspect - specific representations ]] .",1
5050,5530,"SWEM - hier << greatly outperforms >> the [[ other three SWEM variants ]] , and the [[ corresponding accuracies ]] are comparable to the results of CNN or LSTM ) .",0
5051,5530,"SWEM - hier << greatly outperforms >> the [[ other three SWEM variants ]] , and the corresponding accuracies are comparable to the [[ results of CNN or LSTM ]] ) .",0
5052,5530,"SWEM - hier << greatly outperforms >> the other three SWEM variants , and the [[ corresponding accuracies ]] are comparable to the [[ results of CNN or LSTM ]] ) .",0
5053,5530,"SWEM - hier greatly outperforms the [[ other three SWEM variants ]] , and the [[ corresponding accuracies ]] are << comparable to >> the results of CNN or LSTM ) .",0
5054,5530,"SWEM - hier greatly outperforms the [[ other three SWEM variants ]] , and the corresponding accuracies are << comparable to >> the [[ results of CNN or LSTM ]] ) .",0
5055,5530,"SWEM - hier greatly outperforms the other three SWEM variants , and the [[ corresponding accuracies ]] are << comparable to >> the [[ results of CNN or LSTM ]] ) .",1
5056,2219,The [[ mini- batch size ]] was << set to >> [[ 16 or 32 ]] .,1
5057,2465,We also compare our models << on >> the recently proposed dataset Trivia QA. shows the performance comparison on the [[ test set ]] of [[ Trivia QA ]] .,0
5058,2465,We also compare our models on the recently proposed dataset Trivia QA. shows the performance comparison on the [[ test set ]] << of >> [[ Trivia QA ]] .,1
5059,624,We << initialize >> [[ word embeddings ]] in the [[ word representation layer ]] with the 300 - dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus .,0
5060,624,We << initialize >> [[ word embeddings ]] in the word representation layer with the [[ 300 - dimensional GloVe word vectors ]] pretrained from the 840B Common Crawl corpus .,0
5061,624,We << initialize >> [[ word embeddings ]] in the word representation layer with the 300 - dimensional GloVe word vectors pretrained from the [[ 840B Common Crawl corpus ]] .,0
5062,624,We << initialize >> word embeddings in the [[ word representation layer ]] with the [[ 300 - dimensional GloVe word vectors ]] pretrained from the 840B Common Crawl corpus .,0
5063,624,We << initialize >> word embeddings in the [[ word representation layer ]] with the 300 - dimensional GloVe word vectors pretrained from the [[ 840B Common Crawl corpus ]] .,0
5064,624,We << initialize >> word embeddings in the word representation layer with the [[ 300 - dimensional GloVe word vectors ]] pretrained from the [[ 840B Common Crawl corpus ]] .,0
5065,624,We initialize [[ word embeddings ]] << in >> the [[ word representation layer ]] with the 300 - dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus .,1
5066,624,We initialize [[ word embeddings ]] << in >> the word representation layer with the [[ 300 - dimensional GloVe word vectors ]] pretrained from the 840B Common Crawl corpus .,0
5067,624,We initialize [[ word embeddings ]] << in >> the word representation layer with the 300 - dimensional GloVe word vectors pretrained from the [[ 840B Common Crawl corpus ]] .,0
5068,624,We initialize word embeddings << in >> the [[ word representation layer ]] with the [[ 300 - dimensional GloVe word vectors ]] pretrained from the 840B Common Crawl corpus .,0
5069,624,We initialize word embeddings << in >> the [[ word representation layer ]] with the 300 - dimensional GloVe word vectors pretrained from the [[ 840B Common Crawl corpus ]] .,0
5070,624,We initialize word embeddings << in >> the word representation layer with the [[ 300 - dimensional GloVe word vectors ]] pretrained from the [[ 840B Common Crawl corpus ]] .,0
5071,624,We initialize [[ word embeddings ]] in the [[ word representation layer ]] << with >> the 300 - dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus .,0
5072,624,We initialize [[ word embeddings ]] in the word representation layer << with >> the [[ 300 - dimensional GloVe word vectors ]] pretrained from the 840B Common Crawl corpus .,1
5073,624,We initialize [[ word embeddings ]] in the word representation layer << with >> the 300 - dimensional GloVe word vectors pretrained from the [[ 840B Common Crawl corpus ]] .,0
5074,624,We initialize word embeddings in the [[ word representation layer ]] << with >> the [[ 300 - dimensional GloVe word vectors ]] pretrained from the 840B Common Crawl corpus .,0
5075,624,We initialize word embeddings in the [[ word representation layer ]] << with >> the 300 - dimensional GloVe word vectors pretrained from the [[ 840B Common Crawl corpus ]] .,0
5076,624,We initialize word embeddings in the word representation layer << with >> the [[ 300 - dimensional GloVe word vectors ]] pretrained from the [[ 840B Common Crawl corpus ]] .,0
5077,624,We initialize [[ word embeddings ]] in the [[ word representation layer ]] with the 300 - dimensional GloVe word vectors << pretrained from >> the 840B Common Crawl corpus .,0
5078,624,We initialize [[ word embeddings ]] in the word representation layer with the [[ 300 - dimensional GloVe word vectors ]] << pretrained from >> the 840B Common Crawl corpus .,0
5079,624,We initialize [[ word embeddings ]] in the word representation layer with the 300 - dimensional GloVe word vectors << pretrained from >> the [[ 840B Common Crawl corpus ]] .,0
5080,624,We initialize word embeddings in the [[ word representation layer ]] with the [[ 300 - dimensional GloVe word vectors ]] << pretrained from >> the 840B Common Crawl corpus .,0
5081,624,We initialize word embeddings in the [[ word representation layer ]] with the 300 - dimensional GloVe word vectors << pretrained from >> the [[ 840B Common Crawl corpus ]] .,0
5082,624,We initialize word embeddings in the word representation layer with the [[ 300 - dimensional GloVe word vectors ]] << pretrained from >> the [[ 840B Common Crawl corpus ]] .,1
5083,806,It is << worth noting >> that the [[ BiLSTM ]] in the [[ context ruminate layer ]] contributes substantially to model performance .,0
5084,806,It is << worth noting >> that the [[ BiLSTM ]] in the context ruminate layer contributes [[ substantially ]] to model performance .,0
5085,806,It is << worth noting >> that the [[ BiLSTM ]] in the context ruminate layer contributes substantially to [[ model performance ]] .,0
5086,806,It is << worth noting >> that the BiLSTM in the [[ context ruminate layer ]] contributes [[ substantially ]] to model performance .,0
5087,806,It is << worth noting >> that the BiLSTM in the [[ context ruminate layer ]] contributes substantially to [[ model performance ]] .,0
5088,806,It is << worth noting >> that the BiLSTM in the context ruminate layer contributes [[ substantially ]] to [[ model performance ]] .,0
5089,806,It is worth noting that the [[ BiLSTM ]] << in >> the [[ context ruminate layer ]] contributes substantially to model performance .,1
5090,806,It is worth noting that the [[ BiLSTM ]] << in >> the context ruminate layer contributes [[ substantially ]] to model performance .,0
5091,806,It is worth noting that the [[ BiLSTM ]] << in >> the context ruminate layer contributes substantially to [[ model performance ]] .,0
5092,806,It is worth noting that the BiLSTM << in >> the [[ context ruminate layer ]] contributes [[ substantially ]] to model performance .,0
5093,806,It is worth noting that the BiLSTM << in >> the [[ context ruminate layer ]] contributes substantially to [[ model performance ]] .,0
5094,806,It is worth noting that the BiLSTM << in >> the context ruminate layer contributes [[ substantially ]] to [[ model performance ]] .,0
5095,806,It is worth noting that the [[ BiLSTM ]] in the [[ context ruminate layer ]] << contributes >> substantially to model performance .,0
5096,806,It is worth noting that the [[ BiLSTM ]] in the context ruminate layer << contributes >> [[ substantially ]] to model performance .,1
5097,806,It is worth noting that the [[ BiLSTM ]] in the context ruminate layer << contributes >> substantially to [[ model performance ]] .,0
5098,806,It is worth noting that the BiLSTM in the [[ context ruminate layer ]] << contributes >> [[ substantially ]] to model performance .,0
5099,806,It is worth noting that the BiLSTM in the [[ context ruminate layer ]] << contributes >> substantially to [[ model performance ]] .,0
5100,806,It is worth noting that the BiLSTM in the context ruminate layer << contributes >> [[ substantially ]] to [[ model performance ]] .,0
5101,806,It is worth noting that the [[ BiLSTM ]] in the [[ context ruminate layer ]] contributes substantially << to >> model performance .,0
5102,806,It is worth noting that the [[ BiLSTM ]] in the context ruminate layer contributes [[ substantially ]] << to >> model performance .,0
5103,806,It is worth noting that the [[ BiLSTM ]] in the context ruminate layer contributes substantially << to >> [[ model performance ]] .,0
5104,806,It is worth noting that the BiLSTM in the [[ context ruminate layer ]] contributes [[ substantially ]] << to >> model performance .,0
5105,806,It is worth noting that the BiLSTM in the [[ context ruminate layer ]] contributes substantially << to >> [[ model performance ]] .,0
5106,806,It is worth noting that the BiLSTM in the context ruminate layer contributes [[ substantially ]] << to >> [[ model performance ]] .,1
5107,4978,We << apply >> the [[ proposed model ]] to [[ aspect - level sentiment classification ]] .,0
5108,4978,We apply the [[ proposed model ]] << to >> [[ aspect - level sentiment classification ]] .,1
5109,5580,"We compare our method with the previous state - of - the - art [[ CNN with NSGD ]] ( which is reproduced << by >> ourself ) on our [[ internal dataset ]] , as shown in .",0
5110,637,"We can << see that >> [[ "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) ]] works [[ much better ]] than "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",0
5111,637,"We can << see that >> [[ "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) ]] works much better than [[ "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) ]] , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",0
5112,637,"We can << see that >> "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) works [[ much better ]] than [[ "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) ]] , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",0
5113,637,"We can see that [[ "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) ]] << works >> [[ much better ]] than "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",1
5114,637,"We can see that [[ "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) ]] << works >> much better than [[ "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) ]] , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",0
5115,637,"We can see that "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) << works >> [[ much better ]] than [[ "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) ]] , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",0
5116,637,"We can see that [[ "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) ]] works [[ much better ]] << than >> "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",0
5117,637,"We can see that [[ "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) ]] works much better << than >> [[ "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) ]] , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",0
5118,637,"We can see that "" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ) works [[ much better ]] << than >> [[ "" Siamese - CNN "" ( or "" Siamese - LSTM "" ) ]] , which further indicates that our multi-perspective cosine matching func - Models Accuracy 81.4 82.1 83.5 85.0 85.1 86.1 86.3 86.8 87.3 87.5 tion ( Eq. ) is very effective for matching vectors .",1
5119,423,"Such a [[ fine - grained attention ]] << enables >> our [[ model ]] to learn conditional token representations w.r.t. the given question , leading to accurate answer selections .",1
5120,423,"Such a [[ fine - grained attention ]] << enables >> our model to learn [[ conditional token representations ]] w.r.t. the given question , leading to accurate answer selections .",0
5121,423,"Such a [[ fine - grained attention ]] << enables >> our model to learn conditional token representations w.r.t. the [[ given question ]] , leading to accurate answer selections .",0
5122,423,"Such a fine - grained attention << enables >> our [[ model ]] to learn [[ conditional token representations ]] w.r.t. the given question , leading to accurate answer selections .",0
5123,423,"Such a fine - grained attention << enables >> our [[ model ]] to learn conditional token representations w.r.t. the [[ given question ]] , leading to accurate answer selections .",0
5124,423,"Such a fine - grained attention << enables >> our model to learn [[ conditional token representations ]] w.r.t. the [[ given question ]] , leading to accurate answer selections .",0
5125,423,"Such a [[ fine - grained attention ]] enables our [[ model ]] << to learn >> conditional token representations w.r.t. the given question , leading to accurate answer selections .",0
5126,423,"Such a [[ fine - grained attention ]] enables our model << to learn >> [[ conditional token representations ]] w.r.t. the given question , leading to accurate answer selections .",0
5127,423,"Such a [[ fine - grained attention ]] enables our model << to learn >> conditional token representations w.r.t. the [[ given question ]] , leading to accurate answer selections .",0
5128,423,"Such a fine - grained attention enables our [[ model ]] << to learn >> [[ conditional token representations ]] w.r.t. the given question , leading to accurate answer selections .",1
5129,423,"Such a fine - grained attention enables our [[ model ]] << to learn >> conditional token representations w.r.t. the [[ given question ]] , leading to accurate answer selections .",0
5130,423,"Such a fine - grained attention enables our model << to learn >> [[ conditional token representations ]] w.r.t. the [[ given question ]] , leading to accurate answer selections .",0
5131,423,"Such a [[ fine - grained attention ]] enables our [[ model ]] to learn conditional token representations << w.r.t. >> the given question , leading to accurate answer selections .",0
5132,423,"Such a [[ fine - grained attention ]] enables our model to learn [[ conditional token representations ]] << w.r.t. >> the given question , leading to accurate answer selections .",0
5133,423,"Such a [[ fine - grained attention ]] enables our model to learn conditional token representations << w.r.t. >> the [[ given question ]] , leading to accurate answer selections .",0
5134,423,"Such a fine - grained attention enables our [[ model ]] to learn [[ conditional token representations ]] << w.r.t. >> the given question , leading to accurate answer selections .",0
5135,423,"Such a fine - grained attention enables our [[ model ]] to learn conditional token representations << w.r.t. >> the [[ given question ]] , leading to accurate answer selections .",0
5136,423,"Such a fine - grained attention enables our model to learn [[ conditional token representations ]] << w.r.t. >> the [[ given question ]] , leading to accurate answer selections .",1
5137,4823,While [[ MGAN w / o C2F ]] << locates >> [[ wrong sentiment contexts ]] and fails in ( c ) .,1
5138,1996,"In our model , the [[ vector representation ]] is << trained to be >> [[ useful ]] for predicting words in a paragraph .",1
5139,1996,"In our model , the [[ vector representation ]] is << trained to be >> useful for predicting [[ words ]] in a paragraph .",0
5140,1996,"In our model , the [[ vector representation ]] is << trained to be >> useful for predicting words in a [[ paragraph ]] .",0
5141,1996,"In our model , the vector representation is << trained to be >> [[ useful ]] for predicting [[ words ]] in a paragraph .",0
5142,1996,"In our model , the vector representation is << trained to be >> [[ useful ]] for predicting words in a [[ paragraph ]] .",0
5143,1996,"In our model , the vector representation is << trained to be >> useful for predicting [[ words ]] in a [[ paragraph ]] .",0
5144,1996,"In our model , the [[ vector representation ]] is trained to be [[ useful ]] << for predicting >> words in a paragraph .",0
5145,1996,"In our model , the [[ vector representation ]] is trained to be useful << for predicting >> [[ words ]] in a paragraph .",0
5146,1996,"In our model , the [[ vector representation ]] is trained to be useful << for predicting >> words in a [[ paragraph ]] .",0
5147,1996,"In our model , the vector representation is trained to be [[ useful ]] << for predicting >> [[ words ]] in a paragraph .",1
5148,1996,"In our model , the vector representation is trained to be [[ useful ]] << for predicting >> words in a [[ paragraph ]] .",0
5149,1996,"In our model , the vector representation is trained to be useful << for predicting >> [[ words ]] in a [[ paragraph ]] .",0
5150,1996,"In our model , the [[ vector representation ]] is trained to be [[ useful ]] for predicting words << in >> a paragraph .",0
5151,1996,"In our model , the [[ vector representation ]] is trained to be useful for predicting [[ words ]] << in >> a paragraph .",0
5152,1996,"In our model , the [[ vector representation ]] is trained to be useful for predicting words << in >> a [[ paragraph ]] .",0
5153,1996,"In our model , the vector representation is trained to be [[ useful ]] for predicting [[ words ]] << in >> a paragraph .",0
5154,1996,"In our model , the vector representation is trained to be [[ useful ]] for predicting words << in >> a [[ paragraph ]] .",0
5155,1996,"In our model , the vector representation is trained to be useful for predicting [[ words ]] << in >> a [[ paragraph ]] .",1
5156,4472,[[ Learning rate and batch size ]] are << set to >> [[ conventional values ]] without specific tuning for our task .,1
5157,4472,[[ Learning rate and batch size ]] are << set to >> conventional values without specific tuning for [[ our task ]] .,0
5158,4472,Learning rate and batch size are << set to >> [[ conventional values ]] without specific tuning for [[ our task ]] .,0
5159,4472,[[ Learning rate and batch size ]] are set to [[ conventional values ]] << without specific tuning for >> our task .,0
5160,4472,[[ Learning rate and batch size ]] are set to conventional values << without specific tuning for >> [[ our task ]] .,0
5161,4472,Learning rate and batch size are set to [[ conventional values ]] << without specific tuning for >> [[ our task ]] .,1
5162,4620,"[[ Feature - based SVM ]] is << still >> a [[ competitive baseline ]] , but relying on manually - designed features .",1
5163,4620,"[[ Feature - based SVM ]] is << still >> a competitive baseline , but relying on [[ manually - designed features ]] .",0
5164,4620,"Feature - based SVM is << still >> a [[ competitive baseline ]] , but relying on [[ manually - designed features ]] .",0
5165,4620,"[[ Feature - based SVM ]] is still a [[ competitive baseline ]] , but << relying on >> manually - designed features .",0
5166,4620,"[[ Feature - based SVM ]] is still a competitive baseline , but << relying on >> [[ manually - designed features ]] .",1
5167,4620,"Feature - based SVM is still a [[ competitive baseline ]] , but << relying on >> [[ manually - designed features ]] .",0
5168,2486,"On the [[ BioScope Full papers ]] , we are able to << achieve >> [[ 90.43 F1 ]] when training on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",1
5169,2486,"On the [[ BioScope Full papers ]] , we are able to << achieve >> 90.43 F1 when [[ training ]] on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5170,2486,"On the [[ BioScope Full papers ]] , we are able to << achieve >> 90.43 F1 when training on the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5171,2486,"On the BioScope Full papers , we are able to << achieve >> [[ 90.43 F1 ]] when [[ training ]] on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5172,2486,"On the BioScope Full papers , we are able to << achieve >> [[ 90.43 F1 ]] when training on the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5173,2486,"On the BioScope Full papers , we are able to << achieve >> 90.43 F1 when [[ training ]] on the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5174,2486,"On the [[ BioScope Full papers ]] , we are able to achieve [[ 90.43 F1 ]] << when >> training on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5175,2486,"On the [[ BioScope Full papers ]] , we are able to achieve 90.43 F1 << when >> [[ training ]] on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5176,2486,"On the [[ BioScope Full papers ]] , we are able to achieve 90.43 F1 << when >> training on the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5177,2486,"On the BioScope Full papers , we are able to achieve [[ 90.43 F1 ]] << when >> [[ training ]] on the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",1
5178,2486,"On the BioScope Full papers , we are able to achieve [[ 90.43 F1 ]] << when >> training on the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5179,2486,"On the BioScope Full papers , we are able to achieve 90.43 F1 << when >> [[ training ]] on the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5180,2486,"On the [[ BioScope Full papers ]] , we are able to achieve [[ 90.43 F1 ]] when training << on >> the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5181,2486,"On the [[ BioScope Full papers ]] , we are able to achieve 90.43 F1 when [[ training ]] << on >> the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5182,2486,"On the [[ BioScope Full papers ]] , we are able to achieve 90.43 F1 when training << on >> the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5183,2486,"On the BioScope Full papers , we are able to achieve [[ 90.43 F1 ]] when [[ training ]] << on >> the same data , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5184,2486,"On the BioScope Full papers , we are able to achieve [[ 90.43 F1 ]] when training << on >> the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",0
5185,2486,"On the BioScope Full papers , we are able to achieve 90.43 F1 when [[ training ]] << on >> the [[ same data ]] , but we do note that the amount of training data available is significantly lower than for the other datasets , and while general Deep Learning based approaches can not perform well in such situations , we still manage to perform well .",1
5186,150,"We << use >> the [[ skip - gram model ]] as our [[ baseline model ]] , and train the embeddings using Enwik9 6 .",0
5187,150,"We use the [[ skip - gram model ]] << as >> our [[ baseline model ]] , and train the embeddings using Enwik9 6 .",1
5188,3109,"The << choice of >> the [[ RNN encoder ]] is tuned between [[ GRU and LSTM cells ]] and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .",0
5189,3109,"The << choice of >> the [[ RNN encoder ]] is tuned between GRU and LSTM cells and the [[ hidden size ]] is tuned amongst { 32 , 50 , 64 , 75 } .",0
5190,3109,"The << choice of >> the [[ RNN encoder ]] is tuned between GRU and LSTM cells and the hidden size is tuned amongst [[ { 32 , 50 , 64 , 75 } ]] .",0
5191,3109,"The << choice of >> the RNN encoder is tuned between [[ GRU and LSTM cells ]] and the [[ hidden size ]] is tuned amongst { 32 , 50 , 64 , 75 } .",0
5192,3109,"The << choice of >> the RNN encoder is tuned between [[ GRU and LSTM cells ]] and the hidden size is tuned amongst [[ { 32 , 50 , 64 , 75 } ]] .",0
5193,3109,"The << choice of >> the RNN encoder is tuned between GRU and LSTM cells and the [[ hidden size ]] is tuned amongst [[ { 32 , 50 , 64 , 75 } ]] .",0
5194,3109,"The choice of the [[ RNN encoder ]] is << tuned between >> [[ GRU and LSTM cells ]] and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .",1
5195,3109,"The choice of the [[ RNN encoder ]] is << tuned between >> GRU and LSTM cells and the [[ hidden size ]] is tuned amongst { 32 , 50 , 64 , 75 } .",0
5196,3109,"The choice of the [[ RNN encoder ]] is << tuned between >> GRU and LSTM cells and the hidden size is tuned amongst [[ { 32 , 50 , 64 , 75 } ]] .",0
5197,3109,"The choice of the RNN encoder is << tuned between >> [[ GRU and LSTM cells ]] and the [[ hidden size ]] is tuned amongst { 32 , 50 , 64 , 75 } .",0
5198,3109,"The choice of the RNN encoder is << tuned between >> [[ GRU and LSTM cells ]] and the hidden size is tuned amongst [[ { 32 , 50 , 64 , 75 } ]] .",0
5199,3109,"The choice of the RNN encoder is << tuned between >> GRU and LSTM cells and the [[ hidden size ]] is tuned amongst [[ { 32 , 50 , 64 , 75 } ]] .",0
5200,3109,"The choice of the [[ RNN encoder ]] is tuned between [[ GRU and LSTM cells ]] and the hidden size is << tuned amongst >> { 32 , 50 , 64 , 75 } .",0
5201,3109,"The choice of the [[ RNN encoder ]] is tuned between GRU and LSTM cells and the [[ hidden size ]] is << tuned amongst >> { 32 , 50 , 64 , 75 } .",0
5202,3109,"The choice of the [[ RNN encoder ]] is tuned between GRU and LSTM cells and the hidden size is << tuned amongst >> [[ { 32 , 50 , 64 , 75 } ]] .",0
5203,3109,"The choice of the RNN encoder is tuned between [[ GRU and LSTM cells ]] and the [[ hidden size ]] is << tuned amongst >> { 32 , 50 , 64 , 75 } .",0
5204,3109,"The choice of the RNN encoder is tuned between [[ GRU and LSTM cells ]] and the hidden size is << tuned amongst >> [[ { 32 , 50 , 64 , 75 } ]] .",0
5205,3109,"The choice of the RNN encoder is tuned between GRU and LSTM cells and the [[ hidden size ]] is << tuned amongst >> [[ { 32 , 50 , 64 , 75 } ]] .",1
5206,1580,We << use >> [[ AdaDelta optimizer ]] for [[ parameter optimization ]] with an initial learning rate of 0.5 and a batch size Results :,0
5207,1580,We << use >> [[ AdaDelta optimizer ]] for parameter optimization with an [[ initial learning rate ]] of 0.5 and a batch size Results :,0
5208,1580,We << use >> [[ AdaDelta optimizer ]] for parameter optimization with an initial learning rate of [[ 0.5 ]] and a batch size Results :,0
5209,1580,We << use >> AdaDelta optimizer for [[ parameter optimization ]] with an [[ initial learning rate ]] of 0.5 and a batch size Results :,0
5210,1580,We << use >> AdaDelta optimizer for [[ parameter optimization ]] with an initial learning rate of [[ 0.5 ]] and a batch size Results :,0
5211,1580,We << use >> AdaDelta optimizer for parameter optimization with an [[ initial learning rate ]] of [[ 0.5 ]] and a batch size Results :,0
5212,1580,We use [[ AdaDelta optimizer ]] << for >> [[ parameter optimization ]] with an initial learning rate of 0.5 and a batch size Results :,1
5213,1580,We use [[ AdaDelta optimizer ]] << for >> parameter optimization with an [[ initial learning rate ]] of 0.5 and a batch size Results :,0
5214,1580,We use [[ AdaDelta optimizer ]] << for >> parameter optimization with an initial learning rate of [[ 0.5 ]] and a batch size Results :,0
5215,1580,We use AdaDelta optimizer << for >> [[ parameter optimization ]] with an [[ initial learning rate ]] of 0.5 and a batch size Results :,0
5216,1580,We use AdaDelta optimizer << for >> [[ parameter optimization ]] with an initial learning rate of [[ 0.5 ]] and a batch size Results :,0
5217,1580,We use AdaDelta optimizer << for >> parameter optimization with an [[ initial learning rate ]] of [[ 0.5 ]] and a batch size Results :,0
5218,1580,We use [[ AdaDelta optimizer ]] for [[ parameter optimization ]] << with >> an initial learning rate of 0.5 and a batch size Results :,0
5219,1580,We use [[ AdaDelta optimizer ]] for parameter optimization << with >> an [[ initial learning rate ]] of 0.5 and a batch size Results :,0
5220,1580,We use [[ AdaDelta optimizer ]] for parameter optimization << with >> an initial learning rate of [[ 0.5 ]] and a batch size Results :,0
5221,1580,We use AdaDelta optimizer for [[ parameter optimization ]] << with >> an [[ initial learning rate ]] of 0.5 and a batch size Results :,1
5222,1580,We use AdaDelta optimizer for [[ parameter optimization ]] << with >> an initial learning rate of [[ 0.5 ]] and a batch size Results :,0
5223,1580,We use AdaDelta optimizer for parameter optimization << with >> an [[ initial learning rate ]] of [[ 0.5 ]] and a batch size Results :,0
5224,1580,We use [[ AdaDelta optimizer ]] for [[ parameter optimization ]] with an initial learning rate << of >> 0.5 and a batch size Results :,0
5225,1580,We use [[ AdaDelta optimizer ]] for parameter optimization with an [[ initial learning rate ]] << of >> 0.5 and a batch size Results :,0
5226,1580,We use [[ AdaDelta optimizer ]] for parameter optimization with an initial learning rate << of >> [[ 0.5 ]] and a batch size Results :,0
5227,1580,We use AdaDelta optimizer for [[ parameter optimization ]] with an [[ initial learning rate ]] << of >> 0.5 and a batch size Results :,0
5228,1580,We use AdaDelta optimizer for [[ parameter optimization ]] with an initial learning rate << of >> [[ 0.5 ]] and a batch size Results :,0
5229,1580,We use AdaDelta optimizer for parameter optimization with an [[ initial learning rate ]] << of >> [[ 0.5 ]] and a batch size Results :,1
5230,626,"For the [[ charactercomposed embeddings ]] , we << initialize >> [[ each character ]] as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a LSTM layer .",1
5231,626,"For the [[ charactercomposed embeddings ]] , we << initialize >> each character as a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector with a LSTM layer .",0
5232,626,"For the [[ charactercomposed embeddings ]] , we << initialize >> each character as a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5233,626,"For the [[ charactercomposed embeddings ]] , we << initialize >> each character as a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5234,626,"For the [[ charactercomposed embeddings ]] , we << initialize >> each character as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5235,626,"For the charactercomposed embeddings , we << initialize >> [[ each character ]] as a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector with a LSTM layer .",0
5236,626,"For the charactercomposed embeddings , we << initialize >> [[ each character ]] as a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5237,626,"For the charactercomposed embeddings , we << initialize >> [[ each character ]] as a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5238,626,"For the charactercomposed embeddings , we << initialize >> [[ each character ]] as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5239,626,"For the charactercomposed embeddings , we << initialize >> each character as a [[ 20 - dimensional vector ]] , and compose [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5240,626,"For the charactercomposed embeddings , we << initialize >> each character as a [[ 20 - dimensional vector ]] , and compose each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5241,626,"For the charactercomposed embeddings , we << initialize >> each character as a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5242,626,"For the charactercomposed embeddings , we << initialize >> each character as a 20 - dimensional vector , and compose [[ each word ]] into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5243,626,"For the charactercomposed embeddings , we << initialize >> each character as a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5244,626,"For the charactercomposed embeddings , we << initialize >> each character as a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] with a [[ LSTM layer ]] .",0
5245,626,"For the [[ charactercomposed embeddings ]] , we initialize [[ each character ]] << as >> a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a LSTM layer .",0
5246,626,"For the [[ charactercomposed embeddings ]] , we initialize each character << as >> a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector with a LSTM layer .",0
5247,626,"For the [[ charactercomposed embeddings ]] , we initialize each character << as >> a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5248,626,"For the [[ charactercomposed embeddings ]] , we initialize each character << as >> a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5249,626,"For the [[ charactercomposed embeddings ]] , we initialize each character << as >> a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5250,626,"For the charactercomposed embeddings , we initialize [[ each character ]] << as >> a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector with a LSTM layer .",1
5251,626,"For the charactercomposed embeddings , we initialize [[ each character ]] << as >> a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5252,626,"For the charactercomposed embeddings , we initialize [[ each character ]] << as >> a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5253,626,"For the charactercomposed embeddings , we initialize [[ each character ]] << as >> a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5254,626,"For the charactercomposed embeddings , we initialize each character << as >> a [[ 20 - dimensional vector ]] , and compose [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5255,626,"For the charactercomposed embeddings , we initialize each character << as >> a [[ 20 - dimensional vector ]] , and compose each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5256,626,"For the charactercomposed embeddings , we initialize each character << as >> a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5257,626,"For the charactercomposed embeddings , we initialize each character << as >> a 20 - dimensional vector , and compose [[ each word ]] into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5258,626,"For the charactercomposed embeddings , we initialize each character << as >> a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5259,626,"For the charactercomposed embeddings , we initialize each character << as >> a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] with a [[ LSTM layer ]] .",0
5260,626,"For the [[ charactercomposed embeddings ]] , we initialize [[ each character ]] as a 20 - dimensional vector , and << compose >> each word into a 50 dimensional vector with a LSTM layer .",0
5261,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a [[ 20 - dimensional vector ]] , and << compose >> each word into a 50 dimensional vector with a LSTM layer .",0
5262,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and << compose >> [[ each word ]] into a 50 dimensional vector with a LSTM layer .",1
5263,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and << compose >> each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5264,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and << compose >> each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5265,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a [[ 20 - dimensional vector ]] , and << compose >> each word into a 50 dimensional vector with a LSTM layer .",0
5266,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and << compose >> [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5267,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and << compose >> each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5268,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and << compose >> each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5269,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and << compose >> [[ each word ]] into a 50 dimensional vector with a LSTM layer .",0
5270,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and << compose >> each word into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5271,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and << compose >> each word into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5272,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and << compose >> [[ each word ]] into a [[ 50 dimensional vector ]] with a LSTM layer .",0
5273,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and << compose >> [[ each word ]] into a 50 dimensional vector with a [[ LSTM layer ]] .",0
5274,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and << compose >> each word into a [[ 50 dimensional vector ]] with a [[ LSTM layer ]] .",0
5275,626,"For the [[ charactercomposed embeddings ]] , we initialize [[ each character ]] as a 20 - dimensional vector , and compose each word << into >> a 50 dimensional vector with a LSTM layer .",0
5276,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a [[ 20 - dimensional vector ]] , and compose each word << into >> a 50 dimensional vector with a LSTM layer .",0
5277,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and compose [[ each word ]] << into >> a 50 dimensional vector with a LSTM layer .",0
5278,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and compose each word << into >> a [[ 50 dimensional vector ]] with a LSTM layer .",0
5279,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and compose each word << into >> a 50 dimensional vector with a [[ LSTM layer ]] .",0
5280,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a [[ 20 - dimensional vector ]] , and compose each word << into >> a 50 dimensional vector with a LSTM layer .",0
5281,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and compose [[ each word ]] << into >> a 50 dimensional vector with a LSTM layer .",0
5282,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and compose each word << into >> a [[ 50 dimensional vector ]] with a LSTM layer .",0
5283,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and compose each word << into >> a 50 dimensional vector with a [[ LSTM layer ]] .",0
5284,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and compose [[ each word ]] << into >> a 50 dimensional vector with a LSTM layer .",0
5285,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and compose each word << into >> a [[ 50 dimensional vector ]] with a LSTM layer .",0
5286,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and compose each word << into >> a 50 dimensional vector with a [[ LSTM layer ]] .",0
5287,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose [[ each word ]] << into >> a [[ 50 dimensional vector ]] with a LSTM layer .",1
5288,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose [[ each word ]] << into >> a 50 dimensional vector with a [[ LSTM layer ]] .",0
5289,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose each word << into >> a [[ 50 dimensional vector ]] with a [[ LSTM layer ]] .",0
5290,626,"For the [[ charactercomposed embeddings ]] , we initialize [[ each character ]] as a 20 - dimensional vector , and compose each word into a 50 dimensional vector << with >> a LSTM layer .",0
5291,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector << with >> a LSTM layer .",0
5292,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector << with >> a LSTM layer .",0
5293,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] << with >> a LSTM layer .",0
5294,626,"For the [[ charactercomposed embeddings ]] , we initialize each character as a 20 - dimensional vector , and compose each word into a 50 dimensional vector << with >> a [[ LSTM layer ]] .",0
5295,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector << with >> a LSTM layer .",0
5296,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector << with >> a LSTM layer .",0
5297,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] << with >> a LSTM layer .",0
5298,626,"For the charactercomposed embeddings , we initialize [[ each character ]] as a 20 - dimensional vector , and compose each word into a 50 dimensional vector << with >> a [[ LSTM layer ]] .",0
5299,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and compose [[ each word ]] into a 50 dimensional vector << with >> a LSTM layer .",0
5300,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and compose each word into a [[ 50 dimensional vector ]] << with >> a LSTM layer .",0
5301,626,"For the charactercomposed embeddings , we initialize each character as a [[ 20 - dimensional vector ]] , and compose each word into a 50 dimensional vector << with >> a [[ LSTM layer ]] .",0
5302,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose [[ each word ]] into a [[ 50 dimensional vector ]] << with >> a LSTM layer .",0
5303,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose [[ each word ]] into a 50 dimensional vector << with >> a [[ LSTM layer ]] .",0
5304,626,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose each word into a [[ 50 dimensional vector ]] << with >> a [[ LSTM layer ]] .",1
5305,3002,"A [[ dropout ) rate ]] << of >> [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the answers .",1
5306,3002,"A [[ dropout ) rate ]] << of >> 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax for the answers .",0
5307,3002,"A [[ dropout ) rate ]] << of >> 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax for the answers .",0
5308,3002,"A [[ dropout ) rate ]] << of >> 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax for the answers .",0
5309,3002,"A [[ dropout ) rate ]] << of >> 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] for the answers .",0
5310,3002,"A [[ dropout ) rate ]] << of >> 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the [[ answers ]] .",0
5311,3002,"A dropout ) rate << of >> [[ 0.2 ]] is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax for the answers .",0
5312,3002,"A dropout ) rate << of >> [[ 0.2 ]] is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax for the answers .",0
5313,3002,"A dropout ) rate << of >> [[ 0.2 ]] is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax for the answers .",0
5314,3002,"A dropout ) rate << of >> [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] for the answers .",0
5315,3002,"A dropout ) rate << of >> [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the [[ answers ]] .",0
5316,3002,"A dropout ) rate << of >> 0.2 is used for the [[ CNN ]] , [[ all LSTM layers ]] , and the linear transformation before the softmax for the answers .",0
5317,3002,"A dropout ) rate << of >> 0.2 is used for the [[ CNN ]] , all LSTM layers , and the [[ linear transformation ]] before the softmax for the answers .",0
5318,3002,"A dropout ) rate << of >> 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the [[ softmax ]] for the answers .",0
5319,3002,"A dropout ) rate << of >> 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax for the [[ answers ]] .",0
5320,3002,"A dropout ) rate << of >> 0.2 is used for the CNN , [[ all LSTM layers ]] , and the [[ linear transformation ]] before the softmax for the answers .",0
5321,3002,"A dropout ) rate << of >> 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the [[ softmax ]] for the answers .",0
5322,3002,"A dropout ) rate << of >> 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax for the [[ answers ]] .",0
5323,3002,"A dropout ) rate << of >> 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the [[ softmax ]] for the answers .",0
5324,3002,"A dropout ) rate << of >> 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax for the [[ answers ]] .",0
5325,3002,"A dropout ) rate << of >> 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] for the [[ answers ]] .",0
5326,3002,"A [[ dropout ) rate ]] of [[ 0.2 ]] is << used for >> the CNN , all LSTM layers , and the linear transformation before the softmax for the answers .",0
5327,3002,"A [[ dropout ) rate ]] of 0.2 is << used for >> the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax for the answers .",0
5328,3002,"A [[ dropout ) rate ]] of 0.2 is << used for >> the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax for the answers .",0
5329,3002,"A [[ dropout ) rate ]] of 0.2 is << used for >> the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax for the answers .",0
5330,3002,"A [[ dropout ) rate ]] of 0.2 is << used for >> the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] for the answers .",0
5331,3002,"A [[ dropout ) rate ]] of 0.2 is << used for >> the CNN , all LSTM layers , and the linear transformation before the softmax for the [[ answers ]] .",0
5332,3002,"A dropout ) rate of [[ 0.2 ]] is << used for >> the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax for the answers .",1
5333,3002,"A dropout ) rate of [[ 0.2 ]] is << used for >> the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax for the answers .",1
5334,3002,"A dropout ) rate of [[ 0.2 ]] is << used for >> the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax for the answers .",1
5335,3002,"A dropout ) rate of [[ 0.2 ]] is << used for >> the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] for the answers .",0
5336,3002,"A dropout ) rate of [[ 0.2 ]] is << used for >> the CNN , all LSTM layers , and the linear transformation before the softmax for the [[ answers ]] .",0
5337,3002,"A dropout ) rate of 0.2 is << used for >> the [[ CNN ]] , [[ all LSTM layers ]] , and the linear transformation before the softmax for the answers .",0
5338,3002,"A dropout ) rate of 0.2 is << used for >> the [[ CNN ]] , all LSTM layers , and the [[ linear transformation ]] before the softmax for the answers .",0
5339,3002,"A dropout ) rate of 0.2 is << used for >> the [[ CNN ]] , all LSTM layers , and the linear transformation before the [[ softmax ]] for the answers .",0
5340,3002,"A dropout ) rate of 0.2 is << used for >> the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax for the [[ answers ]] .",0
5341,3002,"A dropout ) rate of 0.2 is << used for >> the CNN , [[ all LSTM layers ]] , and the [[ linear transformation ]] before the softmax for the answers .",0
5342,3002,"A dropout ) rate of 0.2 is << used for >> the CNN , [[ all LSTM layers ]] , and the linear transformation before the [[ softmax ]] for the answers .",0
5343,3002,"A dropout ) rate of 0.2 is << used for >> the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax for the [[ answers ]] .",0
5344,3002,"A dropout ) rate of 0.2 is << used for >> the CNN , all LSTM layers , and the [[ linear transformation ]] before the [[ softmax ]] for the answers .",0
5345,3002,"A dropout ) rate of 0.2 is << used for >> the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax for the [[ answers ]] .",0
5346,3002,"A dropout ) rate of 0.2 is << used for >> the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] for the [[ answers ]] .",0
5347,3002,"A [[ dropout ) rate ]] of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation << before >> the softmax for the answers .",0
5348,3002,"A [[ dropout ) rate ]] of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation << before >> the softmax for the answers .",0
5349,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation << before >> the softmax for the answers .",0
5350,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] << before >> the softmax for the answers .",0
5351,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , all LSTM layers , and the linear transformation << before >> the [[ softmax ]] for the answers .",0
5352,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , all LSTM layers , and the linear transformation << before >> the softmax for the [[ answers ]] .",0
5353,3002,"A dropout ) rate of [[ 0.2 ]] is used for the [[ CNN ]] , all LSTM layers , and the linear transformation << before >> the softmax for the answers .",0
5354,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , [[ all LSTM layers ]] , and the linear transformation << before >> the softmax for the answers .",0
5355,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the [[ linear transformation ]] << before >> the softmax for the answers .",0
5356,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation << before >> the [[ softmax ]] for the answers .",0
5357,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation << before >> the softmax for the [[ answers ]] .",0
5358,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , [[ all LSTM layers ]] , and the linear transformation << before >> the softmax for the answers .",0
5359,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the [[ linear transformation ]] << before >> the softmax for the answers .",0
5360,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation << before >> the [[ softmax ]] for the answers .",0
5361,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation << before >> the softmax for the [[ answers ]] .",0
5362,3002,"A dropout ) rate of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the [[ linear transformation ]] << before >> the softmax for the answers .",0
5363,3002,"A dropout ) rate of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation << before >> the [[ softmax ]] for the answers .",0
5364,3002,"A dropout ) rate of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation << before >> the softmax for the [[ answers ]] .",0
5365,3002,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] << before >> the [[ softmax ]] for the answers .",1
5366,3002,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] << before >> the softmax for the [[ answers ]] .",0
5367,3002,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the linear transformation << before >> the [[ softmax ]] for the [[ answers ]] .",0
5368,3002,"A [[ dropout ) rate ]] of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation before the softmax << for >> the answers .",0
5369,3002,"A [[ dropout ) rate ]] of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax << for >> the answers .",0
5370,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax << for >> the answers .",0
5371,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax << for >> the answers .",0
5372,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] << for >> the answers .",0
5373,3002,"A [[ dropout ) rate ]] of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the softmax << for >> the [[ answers ]] .",0
5374,3002,"A dropout ) rate of [[ 0.2 ]] is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax << for >> the answers .",0
5375,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax << for >> the answers .",0
5376,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax << for >> the answers .",0
5377,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] << for >> the answers .",0
5378,3002,"A dropout ) rate of [[ 0.2 ]] is used for the CNN , all LSTM layers , and the linear transformation before the softmax << for >> the [[ answers ]] .",0
5379,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , [[ all LSTM layers ]] , and the linear transformation before the softmax << for >> the answers .",0
5380,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the [[ linear transformation ]] before the softmax << for >> the answers .",0
5381,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the [[ softmax ]] << for >> the answers .",0
5382,3002,"A dropout ) rate of 0.2 is used for the [[ CNN ]] , all LSTM layers , and the linear transformation before the softmax << for >> the [[ answers ]] .",0
5383,3002,"A dropout ) rate of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the [[ linear transformation ]] before the softmax << for >> the answers .",0
5384,3002,"A dropout ) rate of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the [[ softmax ]] << for >> the answers .",0
5385,3002,"A dropout ) rate of 0.2 is used for the CNN , [[ all LSTM layers ]] , and the linear transformation before the softmax << for >> the [[ answers ]] .",0
5386,3002,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the [[ softmax ]] << for >> the answers .",0
5387,3002,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the [[ linear transformation ]] before the softmax << for >> the [[ answers ]] .",0
5388,3002,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the [[ softmax ]] << for >> the [[ answers ]] .",1
5389,4601,[[ Feature - based SVM ]] << is >> a [[ traditional support vector machine based model ]] with extensive feature engineering .,1
5390,4601,[[ Feature - based SVM ]] << is >> a traditional support vector machine based model with [[ extensive feature engineering ]] .,0
5391,4601,Feature - based SVM << is >> a [[ traditional support vector machine based model ]] with [[ extensive feature engineering ]] .,0
5392,4601,[[ Feature - based SVM ]] is a [[ traditional support vector machine based model ]] << with >> extensive feature engineering .,0
5393,4601,[[ Feature - based SVM ]] is a traditional support vector machine based model << with >> [[ extensive feature engineering ]] .,0
5394,4601,Feature - based SVM is a [[ traditional support vector machine based model ]] << with >> [[ extensive feature engineering ]] .,1
5395,3796,"In contrast , [[ SQLNet and TypeSQL ]] that << utilize >> [[ SQL structure information ]] to guide the SQL generation process significantly outperform other Seq2Seq models .",1
5396,3796,"In contrast , [[ SQLNet and TypeSQL ]] that << utilize >> SQL structure information to guide the [[ SQL generation process ]] significantly outperform other Seq2Seq models .",0
5397,3796,"In contrast , [[ SQLNet and TypeSQL ]] that << utilize >> SQL structure information to guide the SQL generation process significantly outperform [[ other Seq2Seq models ]] .",0
5398,3796,"In contrast , SQLNet and TypeSQL that << utilize >> [[ SQL structure information ]] to guide the [[ SQL generation process ]] significantly outperform other Seq2Seq models .",0
5399,3796,"In contrast , SQLNet and TypeSQL that << utilize >> [[ SQL structure information ]] to guide the SQL generation process significantly outperform [[ other Seq2Seq models ]] .",0
5400,3796,"In contrast , SQLNet and TypeSQL that << utilize >> SQL structure information to guide the [[ SQL generation process ]] significantly outperform [[ other Seq2Seq models ]] .",0
5401,3796,"In contrast , [[ SQLNet and TypeSQL ]] that utilize [[ SQL structure information ]] << to guide >> the SQL generation process significantly outperform other Seq2Seq models .",0
5402,3796,"In contrast , [[ SQLNet and TypeSQL ]] that utilize SQL structure information << to guide >> the [[ SQL generation process ]] significantly outperform other Seq2Seq models .",0
5403,3796,"In contrast , [[ SQLNet and TypeSQL ]] that utilize SQL structure information << to guide >> the SQL generation process significantly outperform [[ other Seq2Seq models ]] .",0
5404,3796,"In contrast , SQLNet and TypeSQL that utilize [[ SQL structure information ]] << to guide >> the [[ SQL generation process ]] significantly outperform other Seq2Seq models .",1
5405,3796,"In contrast , SQLNet and TypeSQL that utilize [[ SQL structure information ]] << to guide >> the SQL generation process significantly outperform [[ other Seq2Seq models ]] .",0
5406,3796,"In contrast , SQLNet and TypeSQL that utilize SQL structure information << to guide >> the [[ SQL generation process ]] significantly outperform [[ other Seq2Seq models ]] .",0
5407,3796,"In contrast , [[ SQLNet and TypeSQL ]] that utilize [[ SQL structure information ]] to guide the SQL generation process << significantly outperform >> other Seq2Seq models .",0
5408,3796,"In contrast , [[ SQLNet and TypeSQL ]] that utilize SQL structure information to guide the [[ SQL generation process ]] << significantly outperform >> other Seq2Seq models .",0
5409,3796,"In contrast , [[ SQLNet and TypeSQL ]] that utilize SQL structure information to guide the SQL generation process << significantly outperform >> [[ other Seq2Seq models ]] .",1
5410,3796,"In contrast , SQLNet and TypeSQL that utilize [[ SQL structure information ]] to guide the [[ SQL generation process ]] << significantly outperform >> other Seq2Seq models .",0
5411,3796,"In contrast , SQLNet and TypeSQL that utilize [[ SQL structure information ]] to guide the SQL generation process << significantly outperform >> [[ other Seq2Seq models ]] .",0
5412,3796,"In contrast , SQLNet and TypeSQL that utilize SQL structure information to guide the [[ SQL generation process ]] << significantly outperform >> [[ other Seq2Seq models ]] .",0
5413,2574,These experiments << show that >> [[ our char - IntNet ]] generally improves [[ results ]] across different models and datasets .,0
5414,2574,These experiments << show that >> [[ our char - IntNet ]] generally improves results across [[ different models and datasets ]] .,0
5415,2574,These experiments << show that >> our char - IntNet generally improves [[ results ]] across [[ different models and datasets ]] .,0
5416,2574,These experiments show that [[ our char - IntNet ]] generally << improves >> [[ results ]] across different models and datasets .,1
5417,2574,These experiments show that [[ our char - IntNet ]] generally << improves >> results across [[ different models and datasets ]] .,0
5418,2574,These experiments show that our char - IntNet generally << improves >> [[ results ]] across [[ different models and datasets ]] .,0
5419,2574,These experiments show that [[ our char - IntNet ]] generally improves [[ results ]] << across >> different models and datasets .,0
5420,2574,These experiments show that [[ our char - IntNet ]] generally improves results << across >> [[ different models and datasets ]] .,0
5421,2574,These experiments show that our char - IntNet generally improves [[ results ]] << across >> [[ different models and datasets ]] .,1
5422,4014,"<< When >> [[ N = 1 ]] , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5423,4014,"<< When >> [[ N = 1 ]] , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5424,4014,"<< When >> [[ N = 1 ]] , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5425,4014,"<< When >> [[ N = 1 ]] , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5426,4014,"<< When >> [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5427,4014,"<< When >> [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5428,4014,"<< When >> [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5429,4014,"<< When >> N = 1 , [[ MCFA ]] increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5430,4014,"<< When >> N = 1 , [[ MCFA ]] increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5431,4014,"<< When >> N = 1 , [[ MCFA ]] increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5432,4014,"<< When >> N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5433,4014,"<< When >> N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5434,4014,"<< When >> N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5435,4014,"<< When >> N = 1 , MCFA increases the [[ performance ]] of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5436,4014,"<< When >> N = 1 , MCFA increases the [[ performance ]] of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5437,4014,"<< When >> N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5438,4014,"<< When >> N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5439,4014,"<< When >> N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5440,4014,"<< When >> N = 1 , MCFA increases the performance of a [[ normal CNN ]] from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5441,4014,"<< When >> N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5442,4014,"<< When >> N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5443,4014,"<< When >> N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5444,4014,"<< When >> N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5445,4014,"<< When >> N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5446,4014,"<< When >> N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5447,4014,"<< When >> N = 1 , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the [[ current state of the art ]] on the CR data set .",0
5448,4014,"<< When >> N = 1 , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the [[ CR data set ]] .",0
5449,4014,"<< When >> N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the [[ CR data set ]] .",0
5450,4014,"When [[ N = 1 ]] , [[ MCFA ]] << increases >> the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5451,4014,"When [[ N = 1 ]] , MCFA << increases >> the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5452,4014,"When [[ N = 1 ]] , MCFA << increases >> the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5453,4014,"When [[ N = 1 ]] , MCFA << increases >> the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5454,4014,"When [[ N = 1 ]] , MCFA << increases >> the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5455,4014,"When [[ N = 1 ]] , MCFA << increases >> the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5456,4014,"When [[ N = 1 ]] , MCFA << increases >> the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5457,4014,"When N = 1 , [[ MCFA ]] << increases >> the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",1
5458,4014,"When N = 1 , [[ MCFA ]] << increases >> the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5459,4014,"When N = 1 , [[ MCFA ]] << increases >> the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5460,4014,"When N = 1 , [[ MCFA ]] << increases >> the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5461,4014,"When N = 1 , [[ MCFA ]] << increases >> the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5462,4014,"When N = 1 , [[ MCFA ]] << increases >> the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5463,4014,"When N = 1 , MCFA << increases >> the [[ performance ]] of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5464,4014,"When N = 1 , MCFA << increases >> the [[ performance ]] of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5465,4014,"When N = 1 , MCFA << increases >> the [[ performance ]] of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5466,4014,"When N = 1 , MCFA << increases >> the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5467,4014,"When N = 1 , MCFA << increases >> the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5468,4014,"When N = 1 , MCFA << increases >> the performance of a [[ normal CNN ]] from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5469,4014,"When N = 1 , MCFA << increases >> the performance of a [[ normal CNN ]] from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5470,4014,"When N = 1 , MCFA << increases >> the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5471,4014,"When N = 1 , MCFA << increases >> the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5472,4014,"When N = 1 , MCFA << increases >> the performance of a normal CNN from [[ 85.0 ]] to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5473,4014,"When N = 1 , MCFA << increases >> the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5474,4014,"When N = 1 , MCFA << increases >> the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5475,4014,"When N = 1 , MCFA << increases >> the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the [[ current state of the art ]] on the CR data set .",0
5476,4014,"When N = 1 , MCFA << increases >> the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the [[ CR data set ]] .",0
5477,4014,"When N = 1 , MCFA << increases >> the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the [[ CR data set ]] .",0
5478,4014,"When [[ N = 1 ]] , [[ MCFA ]] increases the performance << of >> a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5479,4014,"When [[ N = 1 ]] , MCFA increases the [[ performance ]] << of >> a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5480,4014,"When [[ N = 1 ]] , MCFA increases the performance << of >> a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5481,4014,"When [[ N = 1 ]] , MCFA increases the performance << of >> a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5482,4014,"When [[ N = 1 ]] , MCFA increases the performance << of >> a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5483,4014,"When [[ N = 1 ]] , MCFA increases the performance << of >> a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5484,4014,"When [[ N = 1 ]] , MCFA increases the performance << of >> a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5485,4014,"When N = 1 , [[ MCFA ]] increases the [[ performance ]] << of >> a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5486,4014,"When N = 1 , [[ MCFA ]] increases the performance << of >> a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5487,4014,"When N = 1 , [[ MCFA ]] increases the performance << of >> a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5488,4014,"When N = 1 , [[ MCFA ]] increases the performance << of >> a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5489,4014,"When N = 1 , [[ MCFA ]] increases the performance << of >> a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5490,4014,"When N = 1 , [[ MCFA ]] increases the performance << of >> a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5491,4014,"When N = 1 , MCFA increases the [[ performance ]] << of >> a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the CR data set .",1
5492,4014,"When N = 1 , MCFA increases the [[ performance ]] << of >> a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5493,4014,"When N = 1 , MCFA increases the [[ performance ]] << of >> a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5494,4014,"When N = 1 , MCFA increases the [[ performance ]] << of >> a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5495,4014,"When N = 1 , MCFA increases the [[ performance ]] << of >> a normal CNN from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5496,4014,"When N = 1 , MCFA increases the performance << of >> a [[ normal CNN ]] from [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5497,4014,"When N = 1 , MCFA increases the performance << of >> a [[ normal CNN ]] from 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5498,4014,"When N = 1 , MCFA increases the performance << of >> a [[ normal CNN ]] from 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5499,4014,"When N = 1 , MCFA increases the performance << of >> a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5500,4014,"When N = 1 , MCFA increases the performance << of >> a normal CNN from [[ 85.0 ]] to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5501,4014,"When N = 1 , MCFA increases the performance << of >> a normal CNN from [[ 85.0 ]] to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5502,4014,"When N = 1 , MCFA increases the performance << of >> a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5503,4014,"When N = 1 , MCFA increases the performance << of >> a normal CNN from 85.0 to [[ 87.6 ]] , beating the [[ current state of the art ]] on the CR data set .",0
5504,4014,"When N = 1 , MCFA increases the performance << of >> a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art on the [[ CR data set ]] .",0
5505,4014,"When N = 1 , MCFA increases the performance << of >> a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] on the [[ CR data set ]] .",0
5506,4014,"When [[ N = 1 ]] , [[ MCFA ]] increases the performance of a normal CNN << from >> 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5507,4014,"When [[ N = 1 ]] , MCFA increases the [[ performance ]] of a normal CNN << from >> 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5508,4014,"When [[ N = 1 ]] , MCFA increases the performance of a [[ normal CNN ]] << from >> 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5509,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN << from >> [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5510,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN << from >> 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5511,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN << from >> 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5512,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN << from >> 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5513,4014,"When N = 1 , [[ MCFA ]] increases the [[ performance ]] of a normal CNN << from >> 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5514,4014,"When N = 1 , [[ MCFA ]] increases the performance of a [[ normal CNN ]] << from >> 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5515,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN << from >> [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5516,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN << from >> 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5517,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN << from >> 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5518,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN << from >> 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5519,4014,"When N = 1 , MCFA increases the [[ performance ]] of a [[ normal CNN ]] << from >> 85.0 to 87.6 , beating the current state of the art on the CR data set .",0
5520,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN << from >> [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",0
5521,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN << from >> 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5522,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN << from >> 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5523,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN << from >> 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5524,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] << from >> [[ 85.0 ]] to 87.6 , beating the current state of the art on the CR data set .",1
5525,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] << from >> 85.0 to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5526,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] << from >> 85.0 to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5527,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] << from >> 85.0 to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5528,4014,"When N = 1 , MCFA increases the performance of a normal CNN << from >> [[ 85.0 ]] to [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5529,4014,"When N = 1 , MCFA increases the performance of a normal CNN << from >> [[ 85.0 ]] to 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5530,4014,"When N = 1 , MCFA increases the performance of a normal CNN << from >> [[ 85.0 ]] to 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5531,4014,"When N = 1 , MCFA increases the performance of a normal CNN << from >> 85.0 to [[ 87.6 ]] , beating the [[ current state of the art ]] on the CR data set .",0
5532,4014,"When N = 1 , MCFA increases the performance of a normal CNN << from >> 85.0 to [[ 87.6 ]] , beating the current state of the art on the [[ CR data set ]] .",0
5533,4014,"When N = 1 , MCFA increases the performance of a normal CNN << from >> 85.0 to 87.6 , beating the [[ current state of the art ]] on the [[ CR data set ]] .",0
5534,4014,"When [[ N = 1 ]] , [[ MCFA ]] increases the performance of a normal CNN from 85.0 << to >> 87.6 , beating the current state of the art on the CR data set .",0
5535,4014,"When [[ N = 1 ]] , MCFA increases the [[ performance ]] of a normal CNN from 85.0 << to >> 87.6 , beating the current state of the art on the CR data set .",0
5536,4014,"When [[ N = 1 ]] , MCFA increases the performance of a [[ normal CNN ]] from 85.0 << to >> 87.6 , beating the current state of the art on the CR data set .",0
5537,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from [[ 85.0 ]] << to >> 87.6 , beating the current state of the art on the CR data set .",0
5538,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 << to >> [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5539,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 << to >> 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5540,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 << to >> 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5541,4014,"When N = 1 , [[ MCFA ]] increases the [[ performance ]] of a normal CNN from 85.0 << to >> 87.6 , beating the current state of the art on the CR data set .",0
5542,4014,"When N = 1 , [[ MCFA ]] increases the performance of a [[ normal CNN ]] from 85.0 << to >> 87.6 , beating the current state of the art on the CR data set .",0
5543,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from [[ 85.0 ]] << to >> 87.6 , beating the current state of the art on the CR data set .",0
5544,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 << to >> [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5545,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 << to >> 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5546,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 << to >> 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5547,4014,"When N = 1 , MCFA increases the [[ performance ]] of a [[ normal CNN ]] from 85.0 << to >> 87.6 , beating the current state of the art on the CR data set .",0
5548,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from [[ 85.0 ]] << to >> 87.6 , beating the current state of the art on the CR data set .",0
5549,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 << to >> [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5550,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 << to >> 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5551,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 << to >> 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5552,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from [[ 85.0 ]] << to >> 87.6 , beating the current state of the art on the CR data set .",0
5553,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 << to >> [[ 87.6 ]] , beating the current state of the art on the CR data set .",0
5554,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 << to >> 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5555,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 << to >> 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5556,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] << to >> [[ 87.6 ]] , beating the current state of the art on the CR data set .",1
5557,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] << to >> 87.6 , beating the [[ current state of the art ]] on the CR data set .",0
5558,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] << to >> 87.6 , beating the current state of the art on the [[ CR data set ]] .",0
5559,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 << to >> [[ 87.6 ]] , beating the [[ current state of the art ]] on the CR data set .",0
5560,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 << to >> [[ 87.6 ]] , beating the current state of the art on the [[ CR data set ]] .",0
5561,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 << to >> 87.6 , beating the [[ current state of the art ]] on the [[ CR data set ]] .",0
5562,4014,"When [[ N = 1 ]] , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , << beating >> the current state of the art on the CR data set .",0
5563,4014,"When [[ N = 1 ]] , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , << beating >> the current state of the art on the CR data set .",0
5564,4014,"When [[ N = 1 ]] , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , << beating >> the current state of the art on the CR data set .",0
5565,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , << beating >> the current state of the art on the CR data set .",0
5566,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , << beating >> the current state of the art on the CR data set .",0
5567,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , << beating >> the [[ current state of the art ]] on the CR data set .",0
5568,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , << beating >> the current state of the art on the [[ CR data set ]] .",0
5569,4014,"When N = 1 , [[ MCFA ]] increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , << beating >> the current state of the art on the CR data set .",0
5570,4014,"When N = 1 , [[ MCFA ]] increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , << beating >> the current state of the art on the CR data set .",0
5571,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , << beating >> the current state of the art on the CR data set .",0
5572,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , << beating >> the current state of the art on the CR data set .",0
5573,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , << beating >> the [[ current state of the art ]] on the CR data set .",0
5574,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , << beating >> the current state of the art on the [[ CR data set ]] .",0
5575,4014,"When N = 1 , MCFA increases the [[ performance ]] of a [[ normal CNN ]] from 85.0 to 87.6 , << beating >> the current state of the art on the CR data set .",0
5576,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from [[ 85.0 ]] to 87.6 , << beating >> the current state of the art on the CR data set .",0
5577,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to [[ 87.6 ]] , << beating >> the current state of the art on the CR data set .",0
5578,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , << beating >> the [[ current state of the art ]] on the CR data set .",0
5579,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , << beating >> the current state of the art on the [[ CR data set ]] .",0
5580,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from [[ 85.0 ]] to 87.6 , << beating >> the current state of the art on the CR data set .",0
5581,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to [[ 87.6 ]] , << beating >> the current state of the art on the CR data set .",0
5582,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , << beating >> the [[ current state of the art ]] on the CR data set .",1
5583,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , << beating >> the current state of the art on the [[ CR data set ]] .",0
5584,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to [[ 87.6 ]] , << beating >> the current state of the art on the CR data set .",0
5585,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , << beating >> the [[ current state of the art ]] on the CR data set .",0
5586,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , << beating >> the current state of the art on the [[ CR data set ]] .",0
5587,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , << beating >> the [[ current state of the art ]] on the CR data set .",0
5588,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , << beating >> the current state of the art on the [[ CR data set ]] .",0
5589,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , << beating >> the [[ current state of the art ]] on the [[ CR data set ]] .",0
5590,4014,"When [[ N = 1 ]] , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art << on >> the CR data set .",0
5591,4014,"When [[ N = 1 ]] , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art << on >> the CR data set .",0
5592,4014,"When [[ N = 1 ]] , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art << on >> the CR data set .",0
5593,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art << on >> the CR data set .",0
5594,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art << on >> the CR data set .",0
5595,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] << on >> the CR data set .",0
5596,4014,"When [[ N = 1 ]] , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art << on >> the [[ CR data set ]] .",0
5597,4014,"When N = 1 , [[ MCFA ]] increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art << on >> the CR data set .",0
5598,4014,"When N = 1 , [[ MCFA ]] increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art << on >> the CR data set .",0
5599,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art << on >> the CR data set .",0
5600,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art << on >> the CR data set .",0
5601,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] << on >> the CR data set .",0
5602,4014,"When N = 1 , [[ MCFA ]] increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art << on >> the [[ CR data set ]] .",0
5603,4014,"When N = 1 , MCFA increases the [[ performance ]] of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art << on >> the CR data set .",0
5604,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art << on >> the CR data set .",0
5605,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art << on >> the CR data set .",0
5606,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] << on >> the CR data set .",0
5607,4014,"When N = 1 , MCFA increases the [[ performance ]] of a normal CNN from 85.0 to 87.6 , beating the current state of the art << on >> the [[ CR data set ]] .",0
5608,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from [[ 85.0 ]] to 87.6 , beating the current state of the art << on >> the CR data set .",0
5609,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to [[ 87.6 ]] , beating the current state of the art << on >> the CR data set .",0
5610,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the [[ current state of the art ]] << on >> the CR data set .",0
5611,4014,"When N = 1 , MCFA increases the performance of a [[ normal CNN ]] from 85.0 to 87.6 , beating the current state of the art << on >> the [[ CR data set ]] .",0
5612,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to [[ 87.6 ]] , beating the current state of the art << on >> the CR data set .",0
5613,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the [[ current state of the art ]] << on >> the CR data set .",0
5614,4014,"When N = 1 , MCFA increases the performance of a normal CNN from [[ 85.0 ]] to 87.6 , beating the current state of the art << on >> the [[ CR data set ]] .",0
5615,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the [[ current state of the art ]] << on >> the CR data set .",0
5616,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to [[ 87.6 ]] , beating the current state of the art << on >> the [[ CR data set ]] .",0
5617,4014,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the [[ current state of the art ]] << on >> the [[ CR data set ]] .",1
5618,1880,"Furthermore , [[ ReSAN ]] even [[ outperforms ]] the 300D SPINN - PI encoders << by >> 3.1 %. , which is a recursive model and uses the result of an external semantic parsing tree as an extra input .",0
5619,1880,"Furthermore , [[ ReSAN ]] even outperforms the [[ 300D SPINN - PI encoders ]] << by >> 3.1 %. , which is a recursive model and uses the result of an external semantic parsing tree as an extra input .",0
5620,1880,"Furthermore , ReSAN even [[ outperforms ]] the [[ 300D SPINN - PI encoders ]] << by >> 3.1 %. , which is a recursive model and uses the result of an external semantic parsing tree as an extra input .",0
5621,5251,This << allows us to >> [[ individually target ]] each [[ modality ]] and only perform feature fusion at the final stage .,0
5622,5251,This << allows us to >> [[ individually target ]] each modality and only [[ perform feature fusion ]] at the final stage .,0
5623,5251,This << allows us to >> [[ individually target ]] each modality and only perform feature fusion at the [[ final stage ]] .,0
5624,5251,This << allows us to >> individually target each [[ modality ]] and only [[ perform feature fusion ]] at the final stage .,0
5625,5251,This << allows us to >> individually target each [[ modality ]] and only perform feature fusion at the [[ final stage ]] .,0
5626,5251,This << allows us to >> individually target each modality and only [[ perform feature fusion ]] at the [[ final stage ]] .,0
5627,5251,This allows us to [[ individually target ]] << each >> [[ modality ]] and only perform feature fusion at the final stage .,1
5628,5251,This allows us to [[ individually target ]] << each >> modality and only [[ perform feature fusion ]] at the final stage .,0
5629,5251,This allows us to [[ individually target ]] << each >> modality and only perform feature fusion at the [[ final stage ]] .,0
5630,5251,This allows us to individually target << each >> [[ modality ]] and only [[ perform feature fusion ]] at the final stage .,0
5631,5251,This allows us to individually target << each >> [[ modality ]] and only perform feature fusion at the [[ final stage ]] .,0
5632,5251,This allows us to individually target << each >> modality and only [[ perform feature fusion ]] at the [[ final stage ]] .,0
5633,5251,This allows us to [[ individually target ]] each [[ modality ]] and only perform feature fusion << at >> the final stage .,0
5634,5251,This allows us to [[ individually target ]] each modality and only [[ perform feature fusion ]] << at >> the final stage .,0
5635,5251,This allows us to [[ individually target ]] each modality and only perform feature fusion << at >> the [[ final stage ]] .,0
5636,5251,This allows us to individually target each [[ modality ]] and only [[ perform feature fusion ]] << at >> the final stage .,0
5637,5251,This allows us to individually target each [[ modality ]] and only perform feature fusion << at >> the [[ final stage ]] .,0
5638,5251,This allows us to individually target each modality and only [[ perform feature fusion ]] << at >> the [[ final stage ]] .,1
5639,3452,We << show >> that [[ simple neural architectures ]] built on top of [[ BERT ]] yields state - of - the - art performance on a variety of benchmark datasets for these two tasks .,0
5640,3452,We << show >> that [[ simple neural architectures ]] built on top of BERT yields [[ state - of - the - art performance ]] on a variety of benchmark datasets for these two tasks .,0
5641,3452,We << show >> that [[ simple neural architectures ]] built on top of BERT yields state - of - the - art performance on a [[ variety of benchmark datasets ]] for these two tasks .,0
5642,3452,We << show >> that simple neural architectures built on top of [[ BERT ]] yields [[ state - of - the - art performance ]] on a variety of benchmark datasets for these two tasks .,0
5643,3452,We << show >> that simple neural architectures built on top of [[ BERT ]] yields state - of - the - art performance on a [[ variety of benchmark datasets ]] for these two tasks .,0
5644,3452,We << show >> that simple neural architectures built on top of BERT yields [[ state - of - the - art performance ]] on a [[ variety of benchmark datasets ]] for these two tasks .,0
5645,3452,We show that [[ simple neural architectures ]] << built on top of >> [[ BERT ]] yields state - of - the - art performance on a variety of benchmark datasets for these two tasks .,1
5646,3452,We show that [[ simple neural architectures ]] << built on top of >> BERT yields [[ state - of - the - art performance ]] on a variety of benchmark datasets for these two tasks .,0
5647,3452,We show that [[ simple neural architectures ]] << built on top of >> BERT yields state - of - the - art performance on a [[ variety of benchmark datasets ]] for these two tasks .,0
5648,3452,We show that simple neural architectures << built on top of >> [[ BERT ]] yields [[ state - of - the - art performance ]] on a variety of benchmark datasets for these two tasks .,0
5649,3452,We show that simple neural architectures << built on top of >> [[ BERT ]] yields state - of - the - art performance on a [[ variety of benchmark datasets ]] for these two tasks .,0
5650,3452,We show that simple neural architectures << built on top of >> BERT yields [[ state - of - the - art performance ]] on a [[ variety of benchmark datasets ]] for these two tasks .,0
5651,3452,We show that [[ simple neural architectures ]] built on top of [[ BERT ]] << yields >> state - of - the - art performance on a variety of benchmark datasets for these two tasks .,0
5652,3452,We show that [[ simple neural architectures ]] built on top of BERT << yields >> [[ state - of - the - art performance ]] on a variety of benchmark datasets for these two tasks .,1
5653,3452,We show that [[ simple neural architectures ]] built on top of BERT << yields >> state - of - the - art performance on a [[ variety of benchmark datasets ]] for these two tasks .,0
5654,3452,We show that simple neural architectures built on top of [[ BERT ]] << yields >> [[ state - of - the - art performance ]] on a variety of benchmark datasets for these two tasks .,0
5655,3452,We show that simple neural architectures built on top of [[ BERT ]] << yields >> state - of - the - art performance on a [[ variety of benchmark datasets ]] for these two tasks .,0
5656,3452,We show that simple neural architectures built on top of BERT << yields >> [[ state - of - the - art performance ]] on a [[ variety of benchmark datasets ]] for these two tasks .,0
5657,3452,We show that [[ simple neural architectures ]] built on top of [[ BERT ]] yields state - of - the - art performance << on >> a variety of benchmark datasets for these two tasks .,0
5658,3452,We show that [[ simple neural architectures ]] built on top of BERT yields [[ state - of - the - art performance ]] << on >> a variety of benchmark datasets for these two tasks .,0
5659,3452,We show that [[ simple neural architectures ]] built on top of BERT yields state - of - the - art performance << on >> a [[ variety of benchmark datasets ]] for these two tasks .,0
5660,3452,We show that simple neural architectures built on top of [[ BERT ]] yields [[ state - of - the - art performance ]] << on >> a variety of benchmark datasets for these two tasks .,0
5661,3452,We show that simple neural architectures built on top of [[ BERT ]] yields state - of - the - art performance << on >> a [[ variety of benchmark datasets ]] for these two tasks .,0
5662,3452,We show that simple neural architectures built on top of BERT yields [[ state - of - the - art performance ]] << on >> a [[ variety of benchmark datasets ]] for these two tasks .,1
5663,3342,"Based on both the input words and the predicted NER labels , the [[ RC component ]] << uses >> [[ another BiLSTM ]] to learn latent features relevant for relation classification .",1
5664,3342,"Based on both the input words and the predicted NER labels , the [[ RC component ]] << uses >> another BiLSTM to learn [[ latent features relevant for relation classification ]] .",0
5665,3342,"Based on both the input words and the predicted NER labels , the RC component << uses >> [[ another BiLSTM ]] to learn [[ latent features relevant for relation classification ]] .",0
5666,3342,"Based on both the input words and the predicted NER labels , the [[ RC component ]] uses [[ another BiLSTM ]] << to learn >> latent features relevant for relation classification .",0
5667,3342,"Based on both the input words and the predicted NER labels , the [[ RC component ]] uses another BiLSTM << to learn >> [[ latent features relevant for relation classification ]] .",0
5668,3342,"Based on both the input words and the predicted NER labels , the RC component uses [[ another BiLSTM ]] << to learn >> [[ latent features relevant for relation classification ]] .",1
5669,5873,"<< For the popular >> [[ ROUGE - 2 metric ]] , our SEASS model achieves [[ 17.54 F1 score ]] and performs better than the previous works .",0
5670,5873,"<< For the popular >> [[ ROUGE - 2 metric ]] , our SEASS model achieves 17.54 F1 score and performs better than the [[ previous works ]] .",0
5671,5873,"<< For the popular >> ROUGE - 2 metric , our SEASS model achieves [[ 17.54 F1 score ]] and performs better than the [[ previous works ]] .",0
5672,5873,"For the popular [[ ROUGE - 2 metric ]] , our SEASS model << achieves >> [[ 17.54 F1 score ]] and performs better than the previous works .",1
5673,5873,"For the popular [[ ROUGE - 2 metric ]] , our SEASS model << achieves >> 17.54 F1 score and performs better than the [[ previous works ]] .",0
5674,5873,"For the popular ROUGE - 2 metric , our SEASS model << achieves >> [[ 17.54 F1 score ]] and performs better than the [[ previous works ]] .",0
5675,5873,"For the popular [[ ROUGE - 2 metric ]] , our SEASS model achieves [[ 17.54 F1 score ]] and << performs better than >> the previous works .",0
5676,5873,"For the popular [[ ROUGE - 2 metric ]] , our SEASS model achieves 17.54 F1 score and << performs better than >> the [[ previous works ]] .",1
5677,5873,"For the popular ROUGE - 2 metric , our SEASS model achieves [[ 17.54 F1 score ]] and << performs better than >> the [[ previous works ]] .",0
5678,4975,We << design >> an [[ aspect - tosentence attention mechanism ]] that can concentrate on the [[ key part ]] of a sentence given the aspect .,0
5679,4975,We << design >> an [[ aspect - tosentence attention mechanism ]] that can concentrate on the key part of a [[ sentence ]] given the aspect .,0
5680,4975,We << design >> an [[ aspect - tosentence attention mechanism ]] that can concentrate on the key part of a sentence given the [[ aspect ]] .,0
5681,4975,We << design >> an aspect - tosentence attention mechanism that can concentrate on the [[ key part ]] of a [[ sentence ]] given the aspect .,0
5682,4975,We << design >> an aspect - tosentence attention mechanism that can concentrate on the [[ key part ]] of a sentence given the [[ aspect ]] .,0
5683,4975,We << design >> an aspect - tosentence attention mechanism that can concentrate on the key part of a [[ sentence ]] given the [[ aspect ]] .,0
5684,4975,We design an [[ aspect - tosentence attention mechanism ]] that << can concentrate on >> the [[ key part ]] of a sentence given the aspect .,1
5685,4975,We design an [[ aspect - tosentence attention mechanism ]] that << can concentrate on >> the key part of a [[ sentence ]] given the aspect .,0
5686,4975,We design an [[ aspect - tosentence attention mechanism ]] that << can concentrate on >> the key part of a sentence given the [[ aspect ]] .,0
5687,4975,We design an aspect - tosentence attention mechanism that << can concentrate on >> the [[ key part ]] of a [[ sentence ]] given the aspect .,0
5688,4975,We design an aspect - tosentence attention mechanism that << can concentrate on >> the [[ key part ]] of a sentence given the [[ aspect ]] .,0
5689,4975,We design an aspect - tosentence attention mechanism that << can concentrate on >> the key part of a [[ sentence ]] given the [[ aspect ]] .,0
5690,4975,We design an [[ aspect - tosentence attention mechanism ]] that can concentrate on the [[ key part ]] << of >> a sentence given the aspect .,0
5691,4975,We design an [[ aspect - tosentence attention mechanism ]] that can concentrate on the key part << of >> a [[ sentence ]] given the aspect .,0
5692,4975,We design an [[ aspect - tosentence attention mechanism ]] that can concentrate on the key part << of >> a sentence given the [[ aspect ]] .,0
5693,4975,We design an aspect - tosentence attention mechanism that can concentrate on the [[ key part ]] << of >> a [[ sentence ]] given the aspect .,1
5694,4975,We design an aspect - tosentence attention mechanism that can concentrate on the [[ key part ]] << of >> a sentence given the [[ aspect ]] .,0
5695,4975,We design an aspect - tosentence attention mechanism that can concentrate on the key part << of >> a [[ sentence ]] given the [[ aspect ]] .,0
5696,4975,We design an [[ aspect - tosentence attention mechanism ]] that can concentrate on the [[ key part ]] of a sentence << given >> the aspect .,0
5697,4975,We design an [[ aspect - tosentence attention mechanism ]] that can concentrate on the key part of a [[ sentence ]] << given >> the aspect .,0
5698,4975,We design an [[ aspect - tosentence attention mechanism ]] that can concentrate on the key part of a sentence << given >> the [[ aspect ]] .,0
5699,4975,We design an aspect - tosentence attention mechanism that can concentrate on the [[ key part ]] of a [[ sentence ]] << given >> the aspect .,0
5700,4975,We design an aspect - tosentence attention mechanism that can concentrate on the [[ key part ]] of a sentence << given >> the [[ aspect ]] .,1
5701,4975,We design an aspect - tosentence attention mechanism that can concentrate on the key part of a [[ sentence ]] << given >> the [[ aspect ]] .,0
5702,5171,"<< For >> [[ training the network ]] we set the [[ batch size ]] = 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5703,5171,"<< For >> [[ training the network ]] we set the batch size = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5704,5171,"<< For >> [[ training the network ]] we set the batch size = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5705,5171,"<< For >> [[ training the network ]] we set the batch size = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5706,5171,"<< For >> [[ training the network ]] we set the batch size = 32 , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5707,5171,"<< For >> training the network we set the [[ batch size ]] = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5708,5171,"<< For >> training the network we set the [[ batch size ]] = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5709,5171,"<< For >> training the network we set the [[ batch size ]] = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5710,5171,"<< For >> training the network we set the [[ batch size ]] = 32 , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5711,5171,"<< For >> training the network we set the batch size = [[ 32 ]] , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5712,5171,"<< For >> training the network we set the batch size = [[ 32 ]] , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5713,5171,"<< For >> training the network we set the batch size = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5714,5171,"<< For >> training the network we set the batch size = 32 , use [[ Adam optimizer ]] with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5715,5171,"<< For >> training the network we set the batch size = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5716,5171,"<< For >> training the network we set the batch size = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for [[ 50 epochs ]] .",0
5717,5171,"For [[ training the network ]] we << set >> the [[ batch size ]] = 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",1
5718,5171,"For [[ training the network ]] we << set >> the batch size = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5719,5171,"For [[ training the network ]] we << set >> the batch size = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5720,5171,"For [[ training the network ]] we << set >> the batch size = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5721,5171,"For [[ training the network ]] we << set >> the batch size = 32 , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5722,5171,"For training the network we << set >> the [[ batch size ]] = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5723,5171,"For training the network we << set >> the [[ batch size ]] = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5724,5171,"For training the network we << set >> the [[ batch size ]] = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5725,5171,"For training the network we << set >> the [[ batch size ]] = 32 , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5726,5171,"For training the network we << set >> the batch size = [[ 32 ]] , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5727,5171,"For training the network we << set >> the batch size = [[ 32 ]] , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5728,5171,"For training the network we << set >> the batch size = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5729,5171,"For training the network we << set >> the batch size = 32 , use [[ Adam optimizer ]] with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5730,5171,"For training the network we << set >> the batch size = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5731,5171,"For training the network we << set >> the batch size = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for [[ 50 epochs ]] .",0
5732,5171,"For [[ training the network ]] we set the [[ batch size ]] << = >> 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5733,5171,"For [[ training the network ]] we set the batch size << = >> [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5734,5171,"For [[ training the network ]] we set the batch size << = >> 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5735,5171,"For [[ training the network ]] we set the batch size << = >> 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5736,5171,"For [[ training the network ]] we set the batch size << = >> 32 , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5737,5171,"For training the network we set the [[ batch size ]] << = >> [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",1
5738,5171,"For training the network we set the [[ batch size ]] << = >> 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5739,5171,"For training the network we set the [[ batch size ]] << = >> 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5740,5171,"For training the network we set the [[ batch size ]] << = >> 32 , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5741,5171,"For training the network we set the batch size << = >> [[ 32 ]] , use [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5742,5171,"For training the network we set the batch size << = >> [[ 32 ]] , use Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5743,5171,"For training the network we set the batch size << = >> [[ 32 ]] , use Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5744,5171,"For training the network we set the batch size << = >> 32 , use [[ Adam optimizer ]] with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5745,5171,"For training the network we set the batch size << = >> 32 , use [[ Adam optimizer ]] with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5746,5171,"For training the network we set the batch size << = >> 32 , use Adam optimizer with [[ cross - entropy loss function ]] and train for [[ 50 epochs ]] .",0
5747,5171,"For [[ training the network ]] we set the [[ batch size ]] = 32 , << use >> Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5748,5171,"For [[ training the network ]] we set the batch size = [[ 32 ]] , << use >> Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5749,5171,"For [[ training the network ]] we set the batch size = 32 , << use >> [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",1
5750,5171,"For [[ training the network ]] we set the batch size = 32 , << use >> Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5751,5171,"For [[ training the network ]] we set the batch size = 32 , << use >> Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5752,5171,"For training the network we set the [[ batch size ]] = [[ 32 ]] , << use >> Adam optimizer with cross - entropy loss function and train for 50 epochs .",0
5753,5171,"For training the network we set the [[ batch size ]] = 32 , << use >> [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5754,5171,"For training the network we set the [[ batch size ]] = 32 , << use >> Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5755,5171,"For training the network we set the [[ batch size ]] = 32 , << use >> Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5756,5171,"For training the network we set the batch size = [[ 32 ]] , << use >> [[ Adam optimizer ]] with cross - entropy loss function and train for 50 epochs .",0
5757,5171,"For training the network we set the batch size = [[ 32 ]] , << use >> Adam optimizer with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5758,5171,"For training the network we set the batch size = [[ 32 ]] , << use >> Adam optimizer with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5759,5171,"For training the network we set the batch size = 32 , << use >> [[ Adam optimizer ]] with [[ cross - entropy loss function ]] and train for 50 epochs .",0
5760,5171,"For training the network we set the batch size = 32 , << use >> [[ Adam optimizer ]] with cross - entropy loss function and train for [[ 50 epochs ]] .",0
5761,5171,"For training the network we set the batch size = 32 , << use >> Adam optimizer with [[ cross - entropy loss function ]] and train for [[ 50 epochs ]] .",0
5762,5171,"For [[ training the network ]] we set the [[ batch size ]] = 32 , use Adam optimizer << with >> cross - entropy loss function and train for 50 epochs .",0
5763,5171,"For [[ training the network ]] we set the batch size = [[ 32 ]] , use Adam optimizer << with >> cross - entropy loss function and train for 50 epochs .",0
5764,5171,"For [[ training the network ]] we set the batch size = 32 , use [[ Adam optimizer ]] << with >> cross - entropy loss function and train for 50 epochs .",0
5765,5171,"For [[ training the network ]] we set the batch size = 32 , use Adam optimizer << with >> [[ cross - entropy loss function ]] and train for 50 epochs .",0
5766,5171,"For [[ training the network ]] we set the batch size = 32 , use Adam optimizer << with >> cross - entropy loss function and train for [[ 50 epochs ]] .",0
5767,5171,"For training the network we set the [[ batch size ]] = [[ 32 ]] , use Adam optimizer << with >> cross - entropy loss function and train for 50 epochs .",0
5768,5171,"For training the network we set the [[ batch size ]] = 32 , use [[ Adam optimizer ]] << with >> cross - entropy loss function and train for 50 epochs .",0
5769,5171,"For training the network we set the [[ batch size ]] = 32 , use Adam optimizer << with >> [[ cross - entropy loss function ]] and train for 50 epochs .",0
5770,5171,"For training the network we set the [[ batch size ]] = 32 , use Adam optimizer << with >> cross - entropy loss function and train for [[ 50 epochs ]] .",0
5771,5171,"For training the network we set the batch size = [[ 32 ]] , use [[ Adam optimizer ]] << with >> cross - entropy loss function and train for 50 epochs .",0
5772,5171,"For training the network we set the batch size = [[ 32 ]] , use Adam optimizer << with >> [[ cross - entropy loss function ]] and train for 50 epochs .",0
5773,5171,"For training the network we set the batch size = [[ 32 ]] , use Adam optimizer << with >> cross - entropy loss function and train for [[ 50 epochs ]] .",0
5774,5171,"For training the network we set the batch size = 32 , use [[ Adam optimizer ]] << with >> [[ cross - entropy loss function ]] and train for 50 epochs .",1
5775,5171,"For training the network we set the batch size = 32 , use [[ Adam optimizer ]] << with >> cross - entropy loss function and train for [[ 50 epochs ]] .",0
5776,5171,"For training the network we set the batch size = 32 , use Adam optimizer << with >> [[ cross - entropy loss function ]] and train for [[ 50 epochs ]] .",0
5777,5171,"For [[ training the network ]] we set the [[ batch size ]] = 32 , use Adam optimizer with cross - entropy loss function and << train for >> 50 epochs .",0
5778,5171,"For [[ training the network ]] we set the batch size = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and << train for >> 50 epochs .",0
5779,5171,"For [[ training the network ]] we set the batch size = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and << train for >> 50 epochs .",0
5780,5171,"For [[ training the network ]] we set the batch size = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and << train for >> 50 epochs .",0
5781,5171,"For [[ training the network ]] we set the batch size = 32 , use Adam optimizer with cross - entropy loss function and << train for >> [[ 50 epochs ]] .",1
5782,5171,"For training the network we set the [[ batch size ]] = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and << train for >> 50 epochs .",0
5783,5171,"For training the network we set the [[ batch size ]] = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and << train for >> 50 epochs .",0
5784,5171,"For training the network we set the [[ batch size ]] = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and << train for >> 50 epochs .",0
5785,5171,"For training the network we set the [[ batch size ]] = 32 , use Adam optimizer with cross - entropy loss function and << train for >> [[ 50 epochs ]] .",0
5786,5171,"For training the network we set the batch size = [[ 32 ]] , use [[ Adam optimizer ]] with cross - entropy loss function and << train for >> 50 epochs .",0
5787,5171,"For training the network we set the batch size = [[ 32 ]] , use Adam optimizer with [[ cross - entropy loss function ]] and << train for >> 50 epochs .",0
5788,5171,"For training the network we set the batch size = [[ 32 ]] , use Adam optimizer with cross - entropy loss function and << train for >> [[ 50 epochs ]] .",0
5789,5171,"For training the network we set the batch size = 32 , use [[ Adam optimizer ]] with [[ cross - entropy loss function ]] and << train for >> 50 epochs .",0
5790,5171,"For training the network we set the batch size = 32 , use [[ Adam optimizer ]] with cross - entropy loss function and << train for >> [[ 50 epochs ]] .",0
5791,5171,"For training the network we set the batch size = 32 , use Adam optimizer with [[ cross - entropy loss function ]] and << train for >> [[ 50 epochs ]] .",0
5792,2780,"First , we can << see that >> [[ multitasking with paraphrase data ]] is essential since it improves F1 [[ from 0.60 to 0.68 ]] .",0
5793,2780,"First , we can see that [[ multitasking with paraphrase data ]] is essential since it << improves F1 >> [[ from 0.60 to 0.68 ]] .",1
5794,4720,[[ Training ]] is << done through >> [[ mini-batch stochastic gradient descent ]] with Adam update rule .,1
5795,4720,[[ Training ]] is << done through >> mini-batch stochastic gradient descent with [[ Adam update rule ]] .,0
5796,4720,Training is << done through >> [[ mini-batch stochastic gradient descent ]] with [[ Adam update rule ]] .,0
5797,4720,[[ Training ]] is done through [[ mini-batch stochastic gradient descent ]] << with >> Adam update rule .,0
5798,4720,[[ Training ]] is done through mini-batch stochastic gradient descent << with >> [[ Adam update rule ]] .,0
5799,4720,Training is done through [[ mini-batch stochastic gradient descent ]] << with >> [[ Adam update rule ]] .,1
5800,2832,"We can see that our model ( the last row of ) << got >> the [[ best MAP ]] among [[ all previous work ]] , and a comparable MRR than dos .",0
5801,2832,"We can see that our model ( the last row of ) << got >> the [[ best MAP ]] among all previous work , and a [[ comparable MRR ]] than dos .",0
5802,2832,"We can see that our model ( the last row of ) << got >> the [[ best MAP ]] among all previous work , and a comparable MRR than [[ dos ]] .",0
5803,2832,"We can see that our model ( the last row of ) << got >> the best MAP among [[ all previous work ]] , and a [[ comparable MRR ]] than dos .",0
5804,2832,"We can see that our model ( the last row of ) << got >> the best MAP among [[ all previous work ]] , and a comparable MRR than [[ dos ]] .",0
5805,2832,"We can see that our model ( the last row of ) << got >> the best MAP among all previous work , and a [[ comparable MRR ]] than [[ dos ]] .",0
5806,2832,"We can see that our model ( the last row of ) got the [[ best MAP ]] << among >> [[ all previous work ]] , and a comparable MRR than dos .",1
5807,2832,"We can see that our model ( the last row of ) got the [[ best MAP ]] << among >> all previous work , and a [[ comparable MRR ]] than dos .",0
5808,2832,"We can see that our model ( the last row of ) got the [[ best MAP ]] << among >> all previous work , and a comparable MRR than [[ dos ]] .",0
5809,2832,"We can see that our model ( the last row of ) got the best MAP << among >> [[ all previous work ]] , and a [[ comparable MRR ]] than dos .",0
5810,2832,"We can see that our model ( the last row of ) got the best MAP << among >> [[ all previous work ]] , and a comparable MRR than [[ dos ]] .",0
5811,2832,"We can see that our model ( the last row of ) got the best MAP << among >> all previous work , and a [[ comparable MRR ]] than [[ dos ]] .",0
5812,2832,"We can see that our model ( the last row of ) got the [[ best MAP ]] among [[ all previous work ]] , and a comparable MRR << than >> dos .",0
5813,2832,"We can see that our model ( the last row of ) got the [[ best MAP ]] among all previous work , and a [[ comparable MRR ]] << than >> dos .",0
5814,2832,"We can see that our model ( the last row of ) got the [[ best MAP ]] among all previous work , and a comparable MRR << than >> [[ dos ]] .",0
5815,2832,"We can see that our model ( the last row of ) got the best MAP among [[ all previous work ]] , and a [[ comparable MRR ]] << than >> dos .",0
5816,2832,"We can see that our model ( the last row of ) got the best MAP among [[ all previous work ]] , and a comparable MRR << than >> [[ dos ]] .",0
5817,2832,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a [[ comparable MRR ]] << than >> [[ dos ]] .",1
5818,5462,The [[ encoding model ]] is << designed to be >> as [[ general purpose ]] as possible .,1
5819,2230,"In order to not conflict << with >> no - answer detection , we leverage a [[ multi-head pointer network ]] to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5820,2230,"In order to not conflict << with >> no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5821,2230,"In order to not conflict << with >> no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5822,2230,"In order to not conflict << with >> no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5823,2230,"In order to not conflict << with >> no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5824,2230,"In order to not conflict << with >> no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5825,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5826,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5827,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5828,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5829,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5830,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5831,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5832,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5833,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5834,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the [[ no -answer score ]] and the other is used for our auxiliary loss .",1
5835,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5836,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5837,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the [[ other ]] is used for our auxiliary loss .",0
5838,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is used for our [[ auxiliary loss ]] .",0
5839,2230,"In order to not conflict << with >> no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is used for our [[ auxiliary loss ]] .",0
5840,2230,"In order to not conflict with no - answer detection , we << leverage >> a [[ multi-head pointer network ]] to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5841,2230,"In order to not conflict with no - answer detection , we << leverage >> a [[ multi-head pointer network ]] to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5842,2230,"In order to not conflict with no - answer detection , we << leverage >> a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5843,2230,"In order to not conflict with no - answer detection , we << leverage >> a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5844,2230,"In order to not conflict with no - answer detection , we << leverage >> a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5845,2230,"In order to not conflict with no - answer detection , we << leverage >> a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5846,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate [[ two pairs of span scores ]] , where [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5847,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5848,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5849,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5850,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5851,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5852,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5853,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5854,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5855,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5856,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5857,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5858,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the [[ other ]] is used for our auxiliary loss .",0
5859,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is used for our [[ auxiliary loss ]] .",0
5860,2230,"In order to not conflict with no - answer detection , we << leverage >> a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is used for our [[ auxiliary loss ]] .",0
5861,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] << to generate >> [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .",1
5862,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] << to generate >> two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5863,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] << to generate >> two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5864,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] << to generate >> two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5865,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] << to generate >> two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5866,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] << to generate >> two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5867,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> [[ two pairs of span scores ]] , where [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5868,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> [[ two pairs of span scores ]] , where one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5869,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> [[ two pairs of span scores ]] , where one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5870,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5871,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5872,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where [[ one pair ]] is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5873,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where [[ one pair ]] is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5874,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5875,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5876,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where one pair is [[ normalized ]] with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5877,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5878,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5879,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the [[ other ]] is used for our auxiliary loss .",0
5880,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is used for our [[ auxiliary loss ]] .",0
5881,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network << to generate >> two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is used for our [[ auxiliary loss ]] .",0
5882,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate [[ two pairs of span scores ]] , << where >> one pair is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5883,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , << where >> [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",0
5884,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , << where >> one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5885,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , << where >> one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5886,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , << where >> one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5887,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , << where >> one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5888,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , << where >> [[ one pair ]] is normalized with the no -answer score and the other is used for our auxiliary loss .",1
5889,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , << where >> one pair is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5890,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , << where >> one pair is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5891,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , << where >> one pair is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",1
5892,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , << where >> one pair is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5893,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> [[ one pair ]] is [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5894,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> [[ one pair ]] is normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5895,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> [[ one pair ]] is normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5896,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> [[ one pair ]] is normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5897,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> one pair is [[ normalized ]] with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5898,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> one pair is [[ normalized ]] with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5899,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> one pair is [[ normalized ]] with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5900,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> one pair is normalized with the [[ no -answer score ]] and the [[ other ]] is used for our auxiliary loss .",0
5901,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> one pair is normalized with the [[ no -answer score ]] and the other is used for our [[ auxiliary loss ]] .",0
5902,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , << where >> one pair is normalized with the no -answer score and the [[ other ]] is used for our [[ auxiliary loss ]] .",0
5903,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate [[ two pairs of span scores ]] , where one pair << is >> normalized with the no -answer score and the other is used for our auxiliary loss .",0
5904,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where [[ one pair ]] << is >> normalized with the no -answer score and the other is used for our auxiliary loss .",0
5905,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair << is >> [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5906,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair << is >> normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5907,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair << is >> normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5908,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair << is >> normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5909,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where [[ one pair ]] << is >> normalized with the no -answer score and the other is used for our auxiliary loss .",0
5910,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair << is >> [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",0
5911,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair << is >> normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5912,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair << is >> normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5913,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair << is >> normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5914,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] << is >> [[ normalized ]] with the no -answer score and the other is used for our auxiliary loss .",1
5915,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] << is >> normalized with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5916,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] << is >> normalized with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5917,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] << is >> normalized with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5918,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair << is >> [[ normalized ]] with the [[ no -answer score ]] and the other is used for our auxiliary loss .",0
5919,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair << is >> [[ normalized ]] with the no -answer score and the [[ other ]] is used for our auxiliary loss .",0
5920,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair << is >> [[ normalized ]] with the no -answer score and the other is used for our [[ auxiliary loss ]] .",0
5921,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair << is >> normalized with the [[ no -answer score ]] and the [[ other ]] is used for our auxiliary loss .",0
5922,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair << is >> normalized with the [[ no -answer score ]] and the other is used for our [[ auxiliary loss ]] .",0
5923,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair << is >> normalized with the no -answer score and the [[ other ]] is used for our [[ auxiliary loss ]] .",0
5924,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is << used for >> our auxiliary loss .",0
5925,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is << used for >> our auxiliary loss .",0
5926,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is << used for >> our auxiliary loss .",0
5927,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is << used for >> our auxiliary loss .",0
5928,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is << used for >> our auxiliary loss .",0
5929,2230,"In order to not conflict with no - answer detection , we leverage a [[ multi-head pointer network ]] to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is << used for >> our [[ auxiliary loss ]] .",0
5930,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where [[ one pair ]] is normalized with the no -answer score and the other is << used for >> our auxiliary loss .",0
5931,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is [[ normalized ]] with the no -answer score and the other is << used for >> our auxiliary loss .",0
5932,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the [[ no -answer score ]] and the other is << used for >> our auxiliary loss .",0
5933,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the [[ other ]] is << used for >> our auxiliary loss .",0
5934,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate [[ two pairs of span scores ]] , where one pair is normalized with the no -answer score and the other is << used for >> our [[ auxiliary loss ]] .",0
5935,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is [[ normalized ]] with the no -answer score and the other is << used for >> our auxiliary loss .",0
5936,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the [[ no -answer score ]] and the other is << used for >> our auxiliary loss .",0
5937,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the [[ other ]] is << used for >> our auxiliary loss .",0
5938,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where [[ one pair ]] is normalized with the no -answer score and the other is << used for >> our [[ auxiliary loss ]] .",0
5939,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the [[ no -answer score ]] and the other is << used for >> our auxiliary loss .",0
5940,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the [[ other ]] is << used for >> our auxiliary loss .",0
5941,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is [[ normalized ]] with the no -answer score and the other is << used for >> our [[ auxiliary loss ]] .",0
5942,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the [[ other ]] is << used for >> our auxiliary loss .",0
5943,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the [[ no -answer score ]] and the other is << used for >> our [[ auxiliary loss ]] .",0
5944,2230,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the [[ other ]] is << used for >> our [[ auxiliary loss ]] .",1
5945,617,"Finally , << based on >> the [[ matching vector ]] , a [[ decision ]] is made through a fully connected layer .",0
5946,617,"Finally , << based on >> the [[ matching vector ]] , a decision is made through a [[ fully connected layer ]] .",0
5947,617,"Finally , << based on >> the matching vector , a [[ decision ]] is made through a [[ fully connected layer ]] .",0
5948,617,"Finally , based on the [[ matching vector ]] , a [[ decision ]] is << made through >> a fully connected layer .",0
5949,617,"Finally , based on the [[ matching vector ]] , a decision is << made through >> a [[ fully connected layer ]] .",0
5950,617,"Finally , based on the matching vector , a [[ decision ]] is << made through >> a [[ fully connected layer ]] .",1
5951,1324,"<< After >> a [[ fixed number of iterations ]] , the [[ model ]] uses a summary of its inference process to predict the answer .",0
5952,1324,"<< After >> a [[ fixed number of iterations ]] , the model uses a [[ summary ]] of its inference process to predict the answer .",0
5953,1324,"<< After >> a [[ fixed number of iterations ]] , the model uses a summary of its [[ inference process ]] to predict the answer .",0
5954,1324,"<< After >> a [[ fixed number of iterations ]] , the model uses a summary of its inference process to predict the [[ answer ]] .",0
5955,1324,"<< After >> a fixed number of iterations , the [[ model ]] uses a [[ summary ]] of its inference process to predict the answer .",0
5956,1324,"<< After >> a fixed number of iterations , the [[ model ]] uses a summary of its [[ inference process ]] to predict the answer .",0
5957,1324,"<< After >> a fixed number of iterations , the [[ model ]] uses a summary of its inference process to predict the [[ answer ]] .",0
5958,1324,"<< After >> a fixed number of iterations , the model uses a [[ summary ]] of its [[ inference process ]] to predict the answer .",0
5959,1324,"<< After >> a fixed number of iterations , the model uses a [[ summary ]] of its inference process to predict the [[ answer ]] .",0
5960,1324,"<< After >> a fixed number of iterations , the model uses a summary of its [[ inference process ]] to predict the [[ answer ]] .",0
5961,1324,"After a [[ fixed number of iterations ]] , the [[ model ]] << uses >> a summary of its inference process to predict the answer .",0
5962,1324,"After a [[ fixed number of iterations ]] , the model << uses >> a [[ summary ]] of its inference process to predict the answer .",0
5963,1324,"After a [[ fixed number of iterations ]] , the model << uses >> a summary of its [[ inference process ]] to predict the answer .",0
5964,1324,"After a [[ fixed number of iterations ]] , the model << uses >> a summary of its inference process to predict the [[ answer ]] .",0
5965,1324,"After a fixed number of iterations , the [[ model ]] << uses >> a [[ summary ]] of its inference process to predict the answer .",1
5966,1324,"After a fixed number of iterations , the [[ model ]] << uses >> a summary of its [[ inference process ]] to predict the answer .",0
5967,1324,"After a fixed number of iterations , the [[ model ]] << uses >> a summary of its inference process to predict the [[ answer ]] .",0
5968,1324,"After a fixed number of iterations , the model << uses >> a [[ summary ]] of its [[ inference process ]] to predict the answer .",0
5969,1324,"After a fixed number of iterations , the model << uses >> a [[ summary ]] of its inference process to predict the [[ answer ]] .",0
5970,1324,"After a fixed number of iterations , the model << uses >> a summary of its [[ inference process ]] to predict the [[ answer ]] .",0
5971,1324,"After a [[ fixed number of iterations ]] , the [[ model ]] uses a summary << of >> its inference process to predict the answer .",0
5972,1324,"After a [[ fixed number of iterations ]] , the model uses a [[ summary ]] << of >> its inference process to predict the answer .",0
5973,1324,"After a [[ fixed number of iterations ]] , the model uses a summary << of >> its [[ inference process ]] to predict the answer .",0
5974,1324,"After a [[ fixed number of iterations ]] , the model uses a summary << of >> its inference process to predict the [[ answer ]] .",0
5975,1324,"After a fixed number of iterations , the [[ model ]] uses a [[ summary ]] << of >> its inference process to predict the answer .",0
5976,1324,"After a fixed number of iterations , the [[ model ]] uses a summary << of >> its [[ inference process ]] to predict the answer .",0
5977,1324,"After a fixed number of iterations , the [[ model ]] uses a summary << of >> its inference process to predict the [[ answer ]] .",0
5978,1324,"After a fixed number of iterations , the model uses a [[ summary ]] << of >> its [[ inference process ]] to predict the answer .",1
5979,1324,"After a fixed number of iterations , the model uses a [[ summary ]] << of >> its inference process to predict the [[ answer ]] .",0
5980,1324,"After a fixed number of iterations , the model uses a summary << of >> its [[ inference process ]] to predict the [[ answer ]] .",0
5981,1324,"After a [[ fixed number of iterations ]] , the [[ model ]] uses a summary of its inference process << to predict >> the answer .",0
5982,1324,"After a [[ fixed number of iterations ]] , the model uses a [[ summary ]] of its inference process << to predict >> the answer .",0
5983,1324,"After a [[ fixed number of iterations ]] , the model uses a summary of its [[ inference process ]] << to predict >> the answer .",0
5984,1324,"After a [[ fixed number of iterations ]] , the model uses a summary of its inference process << to predict >> the [[ answer ]] .",0
5985,1324,"After a fixed number of iterations , the [[ model ]] uses a [[ summary ]] of its inference process << to predict >> the answer .",0
5986,1324,"After a fixed number of iterations , the [[ model ]] uses a summary of its [[ inference process ]] << to predict >> the answer .",0
5987,1324,"After a fixed number of iterations , the [[ model ]] uses a summary of its inference process << to predict >> the [[ answer ]] .",0
5988,1324,"After a fixed number of iterations , the model uses a [[ summary ]] of its [[ inference process ]] << to predict >> the answer .",0
5989,1324,"After a fixed number of iterations , the model uses a [[ summary ]] of its inference process << to predict >> the [[ answer ]] .",1
5990,1324,"After a fixed number of iterations , the model uses a summary of its [[ inference process ]] << to predict >> the [[ answer ]] .",0
5991,690,"Similarly , << on >> the [[ Wik - iQA dataset ]] , [[ all of our models ]] outperform the previous distributional models by a large margin .",0
5992,690,"Similarly , << on >> the [[ Wik - iQA dataset ]] , all of our models outperform the [[ previous distributional models ]] by a large margin .",0
5993,690,"Similarly , << on >> the [[ Wik - iQA dataset ]] , all of our models outperform the previous distributional models by a [[ large margin ]] .",0
5994,690,"Similarly , << on >> the Wik - iQA dataset , [[ all of our models ]] outperform the [[ previous distributional models ]] by a large margin .",0
5995,690,"Similarly , << on >> the Wik - iQA dataset , [[ all of our models ]] outperform the previous distributional models by a [[ large margin ]] .",0
5996,690,"Similarly , << on >> the Wik - iQA dataset , all of our models outperform the [[ previous distributional models ]] by a [[ large margin ]] .",0
5997,690,"Similarly , on the [[ Wik - iQA dataset ]] , [[ all of our models ]] << outperform >> the previous distributional models by a large margin .",0
5998,690,"Similarly , on the [[ Wik - iQA dataset ]] , all of our models << outperform >> the [[ previous distributional models ]] by a large margin .",0
5999,690,"Similarly , on the [[ Wik - iQA dataset ]] , all of our models << outperform >> the previous distributional models by a [[ large margin ]] .",0
6000,690,"Similarly , on the Wik - iQA dataset , [[ all of our models ]] << outperform >> the [[ previous distributional models ]] by a large margin .",1
6001,690,"Similarly , on the Wik - iQA dataset , [[ all of our models ]] << outperform >> the previous distributional models by a [[ large margin ]] .",0
6002,690,"Similarly , on the Wik - iQA dataset , all of our models << outperform >> the [[ previous distributional models ]] by a [[ large margin ]] .",0
6003,690,"Similarly , on the [[ Wik - iQA dataset ]] , [[ all of our models ]] outperform the previous distributional models << by >> a large margin .",0
6004,690,"Similarly , on the [[ Wik - iQA dataset ]] , all of our models outperform the [[ previous distributional models ]] << by >> a large margin .",0
6005,690,"Similarly , on the [[ Wik - iQA dataset ]] , all of our models outperform the previous distributional models << by >> a [[ large margin ]] .",0
6006,690,"Similarly , on the Wik - iQA dataset , [[ all of our models ]] outperform the [[ previous distributional models ]] << by >> a large margin .",0
6007,690,"Similarly , on the Wik - iQA dataset , [[ all of our models ]] outperform the previous distributional models << by >> a [[ large margin ]] .",0
6008,690,"Similarly , on the Wik - iQA dataset , all of our models outperform the [[ previous distributional models ]] << by >> a [[ large margin ]] .",1
6009,5925,[[ Our model ]] << performs better than >> the [[ previous works ]] .,1
6010,2288,[[ Out - of - vocabulary ( OOV ) words ]] are << hashed to >> [[ one of 100 random embeddings ]] each initialized to mean 0 and standard deviation 1 .,1
6011,2288,[[ Out - of - vocabulary ( OOV ) words ]] are << hashed to >> one of 100 random embeddings each initialized to [[ mean 0 ]] and standard deviation 1 .,0
6012,2288,[[ Out - of - vocabulary ( OOV ) words ]] are << hashed to >> one of 100 random embeddings each initialized to mean 0 and [[ standard deviation 1 ]] .,0
6013,2288,Out - of - vocabulary ( OOV ) words are << hashed to >> [[ one of 100 random embeddings ]] each initialized to [[ mean 0 ]] and standard deviation 1 .,0
6014,2288,Out - of - vocabulary ( OOV ) words are << hashed to >> [[ one of 100 random embeddings ]] each initialized to mean 0 and [[ standard deviation 1 ]] .,0
6015,2288,Out - of - vocabulary ( OOV ) words are << hashed to >> one of 100 random embeddings each initialized to [[ mean 0 ]] and [[ standard deviation 1 ]] .,0
6016,2288,[[ Out - of - vocabulary ( OOV ) words ]] are hashed to [[ one of 100 random embeddings ]] each << initialized to >> mean 0 and standard deviation 1 .,0
6017,2288,[[ Out - of - vocabulary ( OOV ) words ]] are hashed to one of 100 random embeddings each << initialized to >> [[ mean 0 ]] and standard deviation 1 .,0
6018,2288,[[ Out - of - vocabulary ( OOV ) words ]] are hashed to one of 100 random embeddings each << initialized to >> mean 0 and [[ standard deviation 1 ]] .,0
6019,2288,Out - of - vocabulary ( OOV ) words are hashed to [[ one of 100 random embeddings ]] each << initialized to >> [[ mean 0 ]] and standard deviation 1 .,1
6020,2288,Out - of - vocabulary ( OOV ) words are hashed to [[ one of 100 random embeddings ]] each << initialized to >> mean 0 and [[ standard deviation 1 ]] .,1
6021,2288,Out - of - vocabulary ( OOV ) words are hashed to one of 100 random embeddings each << initialized to >> [[ mean 0 ]] and [[ standard deviation 1 ]] .,0
6022,3302,"For the [[ DREC dataset ]] , in both settings , << there is >> an [[ over all improvement of ? 1 % ]] .",1
6023,5837,"We << use >> [[ Adam optimizer ( Kingma and Ba , 2014 ) ]] with the [[ default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 ]] .",0
6024,5837,"We use [[ Adam optimizer ( Kingma and Ba , 2014 ) ]] << with >> the [[ default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 ]] .",1
6025,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> [[ non-linear mapping ]] in our [[ model ]] is really important , and replacing any mapping with a linear one significantly degrades the performance .",0
6026,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> [[ non-linear mapping ]] in our model is [[ really important ]] , and replacing any mapping with a linear one significantly degrades the performance .",0
6027,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> [[ non-linear mapping ]] in our model is really important , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6028,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> [[ non-linear mapping ]] in our model is really important , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6029,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> [[ non-linear mapping ]] in our model is really important , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6030,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our [[ model ]] is [[ really important ]] , and replacing any mapping with a linear one significantly degrades the performance .",0
6031,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our [[ model ]] is really important , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6032,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our [[ model ]] is really important , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6033,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our [[ model ]] is really important , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6034,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our model is [[ really important ]] , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6035,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our model is [[ really important ]] , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6036,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our model is [[ really important ]] , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6037,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our model is really important , and replacing [[ any mapping ]] with a [[ linear one ]] significantly degrades the performance .",0
6038,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our model is really important , and replacing [[ any mapping ]] with a linear one significantly degrades the [[ performance ]] .",0
6039,2729,"By comparing rows 2 , 4 , 5 , 7 , we << see that >> non-linear mapping in our model is really important , and replacing any mapping with a [[ linear one ]] significantly degrades the [[ performance ]] .",0
6040,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] << in >> our [[ model ]] is really important , and replacing any mapping with a linear one significantly degrades the performance .",1
6041,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] << in >> our model is [[ really important ]] , and replacing any mapping with a linear one significantly degrades the performance .",0
6042,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] << in >> our model is really important , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6043,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] << in >> our model is really important , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6044,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] << in >> our model is really important , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6045,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our [[ model ]] is [[ really important ]] , and replacing any mapping with a linear one significantly degrades the performance .",0
6046,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our [[ model ]] is really important , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6047,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our [[ model ]] is really important , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6048,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our [[ model ]] is really important , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6049,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our model is [[ really important ]] , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6050,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our model is [[ really important ]] , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6051,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our model is [[ really important ]] , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6052,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our model is really important , and replacing [[ any mapping ]] with a [[ linear one ]] significantly degrades the performance .",0
6053,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our model is really important , and replacing [[ any mapping ]] with a linear one significantly degrades the [[ performance ]] .",0
6054,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping << in >> our model is really important , and replacing any mapping with a [[ linear one ]] significantly degrades the [[ performance ]] .",0
6055,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our [[ model ]] << is >> really important , and replacing any mapping with a linear one significantly degrades the performance .",0
6056,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model << is >> [[ really important ]] , and replacing any mapping with a linear one significantly degrades the performance .",1
6057,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model << is >> really important , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6058,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model << is >> really important , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6059,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model << is >> really important , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6060,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] << is >> [[ really important ]] , and replacing any mapping with a linear one significantly degrades the performance .",0
6061,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] << is >> really important , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6062,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] << is >> really important , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6063,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] << is >> really important , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6064,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model << is >> [[ really important ]] , and replacing [[ any mapping ]] with a linear one significantly degrades the performance .",0
6065,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model << is >> [[ really important ]] , and replacing any mapping with a [[ linear one ]] significantly degrades the performance .",0
6066,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model << is >> [[ really important ]] , and replacing any mapping with a linear one significantly degrades the [[ performance ]] .",0
6067,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model << is >> really important , and replacing [[ any mapping ]] with a [[ linear one ]] significantly degrades the performance .",0
6068,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model << is >> really important , and replacing [[ any mapping ]] with a linear one significantly degrades the [[ performance ]] .",0
6069,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model << is >> really important , and replacing any mapping with a [[ linear one ]] significantly degrades the [[ performance ]] .",0
6070,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our [[ model ]] is really important , and << replacing >> any mapping with a linear one significantly degrades the performance .",0
6071,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is [[ really important ]] , and << replacing >> any mapping with a linear one significantly degrades the performance .",0
6072,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and << replacing >> [[ any mapping ]] with a linear one significantly degrades the performance .",0
6073,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and << replacing >> any mapping with a [[ linear one ]] significantly degrades the performance .",0
6074,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and << replacing >> any mapping with a linear one significantly degrades the [[ performance ]] .",0
6075,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is [[ really important ]] , and << replacing >> any mapping with a linear one significantly degrades the performance .",0
6076,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and << replacing >> [[ any mapping ]] with a linear one significantly degrades the performance .",0
6077,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and << replacing >> any mapping with a [[ linear one ]] significantly degrades the performance .",0
6078,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and << replacing >> any mapping with a linear one significantly degrades the [[ performance ]] .",0
6079,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and << replacing >> [[ any mapping ]] with a linear one significantly degrades the performance .",0
6080,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and << replacing >> any mapping with a [[ linear one ]] significantly degrades the performance .",0
6081,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and << replacing >> any mapping with a linear one significantly degrades the [[ performance ]] .",0
6082,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and << replacing >> [[ any mapping ]] with a [[ linear one ]] significantly degrades the performance .",0
6083,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and << replacing >> [[ any mapping ]] with a linear one significantly degrades the [[ performance ]] .",0
6084,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and << replacing >> any mapping with a [[ linear one ]] significantly degrades the [[ performance ]] .",0
6085,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our [[ model ]] is really important , and replacing any mapping << with >> a linear one significantly degrades the performance .",0
6086,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is [[ really important ]] , and replacing any mapping << with >> a linear one significantly degrades the performance .",0
6087,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and replacing [[ any mapping ]] << with >> a linear one significantly degrades the performance .",0
6088,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and replacing any mapping << with >> a [[ linear one ]] significantly degrades the performance .",0
6089,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and replacing any mapping << with >> a linear one significantly degrades the [[ performance ]] .",0
6090,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is [[ really important ]] , and replacing any mapping << with >> a linear one significantly degrades the performance .",0
6091,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and replacing [[ any mapping ]] << with >> a linear one significantly degrades the performance .",0
6092,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and replacing any mapping << with >> a [[ linear one ]] significantly degrades the performance .",0
6093,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and replacing any mapping << with >> a linear one significantly degrades the [[ performance ]] .",0
6094,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and replacing [[ any mapping ]] << with >> a linear one significantly degrades the performance .",0
6095,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and replacing any mapping << with >> a [[ linear one ]] significantly degrades the performance .",0
6096,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and replacing any mapping << with >> a linear one significantly degrades the [[ performance ]] .",0
6097,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing [[ any mapping ]] << with >> a [[ linear one ]] significantly degrades the performance .",1
6098,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing [[ any mapping ]] << with >> a linear one significantly degrades the [[ performance ]] .",0
6099,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing any mapping << with >> a [[ linear one ]] significantly degrades the [[ performance ]] .",0
6100,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our [[ model ]] is really important , and replacing any mapping with a linear one << significantly degrades >> the performance .",0
6101,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is [[ really important ]] , and replacing any mapping with a linear one << significantly degrades >> the performance .",0
6102,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and replacing [[ any mapping ]] with a linear one << significantly degrades >> the performance .",0
6103,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and replacing any mapping with a [[ linear one ]] << significantly degrades >> the performance .",0
6104,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that [[ non-linear mapping ]] in our model is really important , and replacing any mapping with a linear one << significantly degrades >> the [[ performance ]] .",0
6105,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is [[ really important ]] , and replacing any mapping with a linear one << significantly degrades >> the performance .",0
6106,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and replacing [[ any mapping ]] with a linear one << significantly degrades >> the performance .",0
6107,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and replacing any mapping with a [[ linear one ]] << significantly degrades >> the performance .",0
6108,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our [[ model ]] is really important , and replacing any mapping with a linear one << significantly degrades >> the [[ performance ]] .",0
6109,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and replacing [[ any mapping ]] with a linear one << significantly degrades >> the performance .",0
6110,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and replacing any mapping with a [[ linear one ]] << significantly degrades >> the performance .",0
6111,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is [[ really important ]] , and replacing any mapping with a linear one << significantly degrades >> the [[ performance ]] .",0
6112,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing [[ any mapping ]] with a [[ linear one ]] << significantly degrades >> the performance .",0
6113,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing [[ any mapping ]] with a linear one << significantly degrades >> the [[ performance ]] .",0
6114,2729,"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing any mapping with a [[ linear one ]] << significantly degrades >> the [[ performance ]] .",1
6115,5782,"Finally , [[ RASG w/o DM ]] << offers a decrease of >> [[ 10 . 22 % ]] compared with RASG in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .",1
6116,5782,"Finally , [[ RASG w/o DM ]] << offers a decrease of >> 10 . 22 % compared with [[ RASG ]] in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .",0
6117,5782,"Finally , [[ RASG w/o DM ]] << offers a decrease of >> 10 . 22 % compared with RASG in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6118,5782,"Finally , RASG w/o DM << offers a decrease of >> [[ 10 . 22 % ]] compared with [[ RASG ]] in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .",0
6119,5782,"Finally , RASG w/o DM << offers a decrease of >> [[ 10 . 22 % ]] compared with RASG in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6120,5782,"Finally , RASG w/o DM << offers a decrease of >> 10 . 22 % compared with [[ RASG ]] in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6121,5782,"Finally , [[ RASG w/o DM ]] offers a decrease of [[ 10 . 22 % ]] << compared with >> RASG in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .",0
6122,5782,"Finally , [[ RASG w/o DM ]] offers a decrease of 10 . 22 % << compared with >> [[ RASG ]] in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .",0
6123,5782,"Finally , [[ RASG w/o DM ]] offers a decrease of 10 . 22 % << compared with >> RASG in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6124,5782,"Finally , RASG w/o DM offers a decrease of [[ 10 . 22 % ]] << compared with >> [[ RASG ]] in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .",1
6125,5782,"Finally , RASG w/o DM offers a decrease of [[ 10 . 22 % ]] << compared with >> RASG in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6126,5782,"Finally , RASG w/o DM offers a decrease of 10 . 22 % << compared with >> [[ RASG ]] in terms of [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6127,5782,"Finally , [[ RASG w/o DM ]] offers a decrease of [[ 10 . 22 % ]] compared with RASG << in terms of >> ROUGE - L , which demonstrates the effectiveness of denoising module .",0
6128,5782,"Finally , [[ RASG w/o DM ]] offers a decrease of 10 . 22 % compared with [[ RASG ]] << in terms of >> ROUGE - L , which demonstrates the effectiveness of denoising module .",0
6129,5782,"Finally , [[ RASG w/o DM ]] offers a decrease of 10 . 22 % compared with RASG << in terms of >> [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6130,5782,"Finally , RASG w/o DM offers a decrease of [[ 10 . 22 % ]] compared with [[ RASG ]] << in terms of >> ROUGE - L , which demonstrates the effectiveness of denoising module .",0
6131,5782,"Finally , RASG w/o DM offers a decrease of [[ 10 . 22 % ]] compared with RASG << in terms of >> [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",0
6132,5782,"Finally , RASG w/o DM offers a decrease of 10 . 22 % compared with [[ RASG ]] << in terms of >> [[ ROUGE - L ]] , which demonstrates the effectiveness of denoising module .",1
6133,5788,"The [[ diversification stage ]] << leverages >> [[ content selection ]] to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",1
6134,5788,"The [[ diversification stage ]] << leverages >> content selection to map the [[ source to multiple sequences ]] , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",0
6135,5788,"The diversification stage << leverages >> [[ content selection ]] to map the [[ source to multiple sequences ]] , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",0
6136,5788,"The [[ diversification stage ]] leverages [[ content selection ]] to << map >> the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",0
6137,5788,"The [[ diversification stage ]] leverages content selection to << map >> the [[ source to multiple sequences ]] , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",0
6138,5788,"The diversification stage leverages [[ content selection ]] to << map >> the [[ source to multiple sequences ]] , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",1
6139,4561,"In addition , we << add >> [[ L2-regularization ]] to the [[ last fully connect layer ]] with a weight of 0.01 .",0
6140,4561,"In addition , we << add >> [[ L2-regularization ]] to the last fully connect layer with a [[ weight ]] of 0.01 .",0
6141,4561,"In addition , we << add >> [[ L2-regularization ]] to the last fully connect layer with a weight of [[ 0.01 ]] .",0
6142,4561,"In addition , we << add >> L2-regularization to the [[ last fully connect layer ]] with a [[ weight ]] of 0.01 .",0
6143,4561,"In addition , we << add >> L2-regularization to the [[ last fully connect layer ]] with a weight of [[ 0.01 ]] .",0
6144,4561,"In addition , we << add >> L2-regularization to the last fully connect layer with a [[ weight ]] of [[ 0.01 ]] .",0
6145,4561,"In addition , we add [[ L2-regularization ]] << to >> the [[ last fully connect layer ]] with a weight of 0.01 .",1
6146,4561,"In addition , we add [[ L2-regularization ]] << to >> the last fully connect layer with a [[ weight ]] of 0.01 .",0
6147,4561,"In addition , we add [[ L2-regularization ]] << to >> the last fully connect layer with a weight of [[ 0.01 ]] .",0
6148,4561,"In addition , we add L2-regularization << to >> the [[ last fully connect layer ]] with a [[ weight ]] of 0.01 .",0
6149,4561,"In addition , we add L2-regularization << to >> the [[ last fully connect layer ]] with a weight of [[ 0.01 ]] .",0
6150,4561,"In addition , we add L2-regularization << to >> the last fully connect layer with a [[ weight ]] of [[ 0.01 ]] .",0
6151,4561,"In addition , we add [[ L2-regularization ]] to the [[ last fully connect layer ]] << with >> a weight of 0.01 .",0
6152,4561,"In addition , we add [[ L2-regularization ]] to the last fully connect layer << with >> a [[ weight ]] of 0.01 .",0
6153,4561,"In addition , we add [[ L2-regularization ]] to the last fully connect layer << with >> a weight of [[ 0.01 ]] .",0
6154,4561,"In addition , we add L2-regularization to the [[ last fully connect layer ]] << with >> a [[ weight ]] of 0.01 .",1
6155,4561,"In addition , we add L2-regularization to the [[ last fully connect layer ]] << with >> a weight of [[ 0.01 ]] .",0
6156,4561,"In addition , we add L2-regularization to the last fully connect layer << with >> a [[ weight ]] of [[ 0.01 ]] .",0
6157,4561,"In addition , we add [[ L2-regularization ]] to the [[ last fully connect layer ]] with a weight << of >> 0.01 .",0
6158,4561,"In addition , we add [[ L2-regularization ]] to the last fully connect layer with a [[ weight ]] << of >> 0.01 .",0
6159,4561,"In addition , we add [[ L2-regularization ]] to the last fully connect layer with a weight << of >> [[ 0.01 ]] .",0
6160,4561,"In addition , we add L2-regularization to the [[ last fully connect layer ]] with a [[ weight ]] << of >> 0.01 .",0
6161,4561,"In addition , we add L2-regularization to the [[ last fully connect layer ]] with a weight << of >> [[ 0.01 ]] .",0
6162,4561,"In addition , we add L2-regularization to the last fully connect layer with a [[ weight ]] << of >> [[ 0.01 ]] .",1
6163,1187,"Moreover , we << observe >> that the [[ proposed MAGE architecture ]] can [[ substantially improve ]] the performance for both bi - GRUs and GAs .",0
6164,1187,"Moreover , we << observe >> that the [[ proposed MAGE architecture ]] can substantially improve the [[ performance ]] for both bi - GRUs and GAs .",0
6165,1187,"Moreover , we << observe >> that the [[ proposed MAGE architecture ]] can substantially improve the performance for [[ both bi - GRUs and GAs ]] .",0
6166,1187,"Moreover , we << observe >> that the proposed MAGE architecture can [[ substantially improve ]] the [[ performance ]] for both bi - GRUs and GAs .",0
6167,1187,"Moreover , we << observe >> that the proposed MAGE architecture can [[ substantially improve ]] the performance for [[ both bi - GRUs and GAs ]] .",0
6168,1187,"Moreover , we << observe >> that the proposed MAGE architecture can substantially improve the [[ performance ]] for [[ both bi - GRUs and GAs ]] .",0
6169,1187,"Moreover , we observe that the [[ proposed MAGE architecture ]] can [[ substantially improve ]] the performance << for >> both bi - GRUs and GAs .",0
6170,1187,"Moreover , we observe that the [[ proposed MAGE architecture ]] can substantially improve the [[ performance ]] << for >> both bi - GRUs and GAs .",0
6171,1187,"Moreover , we observe that the [[ proposed MAGE architecture ]] can substantially improve the performance << for >> [[ both bi - GRUs and GAs ]] .",0
6172,1187,"Moreover , we observe that the proposed MAGE architecture can [[ substantially improve ]] the [[ performance ]] << for >> both bi - GRUs and GAs .",0
6173,1187,"Moreover , we observe that the proposed MAGE architecture can [[ substantially improve ]] the performance << for >> [[ both bi - GRUs and GAs ]] .",0
6174,1187,"Moreover , we observe that the proposed MAGE architecture can substantially improve the [[ performance ]] << for >> [[ both bi - GRUs and GAs ]] .",1
6175,3572,"Specifically , the [[ proposed solution ]] is << built on top of >> the [[ existing transformer - based , pretrained general - purposed language encoders ]] .",1
6176,4420,The [[ paired t- test ]] is << used for >> the [[ significance testing ]] .,1
6177,3742,"To better quantify the contribution of the different components of our [[ model ]] , we also conduct an ablation study << evaluating >> [[ several simplified models ]] .",0
6178,1762,We << propose >> [[ two ways ]] to apply the [[ Ptr - Net model ]] for our task : a sequence model and a boundary model .,0
6179,1762,We << propose >> [[ two ways ]] to apply the Ptr - Net model for [[ our task ]] : a sequence model and a boundary model .,0
6180,1762,We << propose >> [[ two ways ]] to apply the Ptr - Net model for our task : a [[ sequence model ]] and a boundary model .,0
6181,1762,We << propose >> [[ two ways ]] to apply the Ptr - Net model for our task : a sequence model and a [[ boundary model ]] .,0
6182,1762,We << propose >> two ways to apply the [[ Ptr - Net model ]] for [[ our task ]] : a sequence model and a boundary model .,0
6183,1762,We << propose >> two ways to apply the [[ Ptr - Net model ]] for our task : a [[ sequence model ]] and a boundary model .,0
6184,1762,We << propose >> two ways to apply the [[ Ptr - Net model ]] for our task : a sequence model and a [[ boundary model ]] .,0
6185,1762,We << propose >> two ways to apply the Ptr - Net model for [[ our task ]] : a [[ sequence model ]] and a boundary model .,0
6186,1762,We << propose >> two ways to apply the Ptr - Net model for [[ our task ]] : a sequence model and a [[ boundary model ]] .,0
6187,1762,We << propose >> two ways to apply the Ptr - Net model for our task : a [[ sequence model ]] and a [[ boundary model ]] .,0
6188,1762,We propose [[ two ways ]] << to apply >> the [[ Ptr - Net model ]] for our task : a sequence model and a boundary model .,1
6189,1762,We propose [[ two ways ]] << to apply >> the Ptr - Net model for [[ our task ]] : a sequence model and a boundary model .,0
6190,1762,We propose [[ two ways ]] << to apply >> the Ptr - Net model for our task : a [[ sequence model ]] and a boundary model .,0
6191,1762,We propose [[ two ways ]] << to apply >> the Ptr - Net model for our task : a sequence model and a [[ boundary model ]] .,0
6192,1762,We propose two ways << to apply >> the [[ Ptr - Net model ]] for [[ our task ]] : a sequence model and a boundary model .,0
6193,1762,We propose two ways << to apply >> the [[ Ptr - Net model ]] for our task : a [[ sequence model ]] and a boundary model .,0
6194,1762,We propose two ways << to apply >> the [[ Ptr - Net model ]] for our task : a sequence model and a [[ boundary model ]] .,0
6195,1762,We propose two ways << to apply >> the Ptr - Net model for [[ our task ]] : a [[ sequence model ]] and a boundary model .,0
6196,1762,We propose two ways << to apply >> the Ptr - Net model for [[ our task ]] : a sequence model and a [[ boundary model ]] .,0
6197,1762,We propose two ways << to apply >> the Ptr - Net model for our task : a [[ sequence model ]] and a [[ boundary model ]] .,0
6198,1762,We propose [[ two ways ]] to apply the [[ Ptr - Net model ]] << for >> our task : a sequence model and a boundary model .,0
6199,1762,We propose [[ two ways ]] to apply the Ptr - Net model << for >> [[ our task ]] : a sequence model and a boundary model .,0
6200,1762,We propose [[ two ways ]] to apply the Ptr - Net model << for >> our task : a [[ sequence model ]] and a boundary model .,0
6201,1762,We propose [[ two ways ]] to apply the Ptr - Net model << for >> our task : a sequence model and a [[ boundary model ]] .,0
6202,1762,We propose two ways to apply the [[ Ptr - Net model ]] << for >> [[ our task ]] : a sequence model and a boundary model .,1
6203,1762,We propose two ways to apply the [[ Ptr - Net model ]] << for >> our task : a [[ sequence model ]] and a boundary model .,0
6204,1762,We propose two ways to apply the [[ Ptr - Net model ]] << for >> our task : a sequence model and a [[ boundary model ]] .,0
6205,1762,We propose two ways to apply the Ptr - Net model << for >> [[ our task ]] : a [[ sequence model ]] and a boundary model .,0
6206,1762,We propose two ways to apply the Ptr - Net model << for >> [[ our task ]] : a sequence model and a [[ boundary model ]] .,0
6207,1762,We propose two ways to apply the Ptr - Net model << for >> our task : a [[ sequence model ]] and a [[ boundary model ]] .,0
6208,3800,"Inspired by this existing research , we have << developed >> [[ TRANX ]] , a [[ TRANsition - based abstract syntaX parser ]] for semantic parsing and code generation .",0
6209,3800,"Inspired by this existing research , we have << developed >> [[ TRANX ]] , a TRANsition - based abstract syntaX parser for [[ semantic parsing and code generation ]] .",0
6210,3800,"Inspired by this existing research , we have << developed >> TRANX , a [[ TRANsition - based abstract syntaX parser ]] for [[ semantic parsing and code generation ]] .",0
6211,3800,"Inspired by this existing research , we have developed [[ TRANX ]] , a [[ TRANsition - based abstract syntaX parser ]] << for >> semantic parsing and code generation .",0
6212,3800,"Inspired by this existing research , we have developed [[ TRANX ]] , a TRANsition - based abstract syntaX parser << for >> [[ semantic parsing and code generation ]] .",1
6213,3800,"Inspired by this existing research , we have developed TRANX , a [[ TRANsition - based abstract syntaX parser ]] << for >> [[ semantic parsing and code generation ]] .",0
6214,4802,The hyperparameters are << tuned on >> [[ 10 % randomly held - out training data ]] of the [[ target domain ]] in R1?L task and are fixed to be used in all transfer pairs .,0
6215,4802,The hyperparameters are << tuned on >> [[ 10 % randomly held - out training data ]] of the target domain in [[ R1?L task ]] and are fixed to be used in all transfer pairs .,0
6216,4802,The hyperparameters are << tuned on >> [[ 10 % randomly held - out training data ]] of the target domain in R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
6217,4802,The hyperparameters are << tuned on >> 10 % randomly held - out training data of the [[ target domain ]] in [[ R1?L task ]] and are fixed to be used in all transfer pairs .,0
6218,4802,The hyperparameters are << tuned on >> 10 % randomly held - out training data of the [[ target domain ]] in R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
6219,4802,The hyperparameters are << tuned on >> 10 % randomly held - out training data of the target domain in [[ R1?L task ]] and are fixed to be used in all [[ transfer pairs ]] .,0
6220,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] << of >> the [[ target domain ]] in R1?L task and are fixed to be used in all transfer pairs .,1
6221,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] << of >> the target domain in [[ R1?L task ]] and are fixed to be used in all transfer pairs .,0
6222,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] << of >> the target domain in R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
6223,4802,The hyperparameters are tuned on 10 % randomly held - out training data << of >> the [[ target domain ]] in [[ R1?L task ]] and are fixed to be used in all transfer pairs .,0
6224,4802,The hyperparameters are tuned on 10 % randomly held - out training data << of >> the [[ target domain ]] in R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
6225,4802,The hyperparameters are tuned on 10 % randomly held - out training data << of >> the target domain in [[ R1?L task ]] and are fixed to be used in all [[ transfer pairs ]] .,0
6226,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the [[ target domain ]] << in >> R1?L task and are fixed to be used in all transfer pairs .,0
6227,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the target domain << in >> [[ R1?L task ]] and are fixed to be used in all transfer pairs .,0
6228,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the target domain << in >> R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
6229,4802,The hyperparameters are tuned on 10 % randomly held - out training data of the [[ target domain ]] << in >> [[ R1?L task ]] and are fixed to be used in all transfer pairs .,1
6230,4802,The hyperparameters are tuned on 10 % randomly held - out training data of the [[ target domain ]] << in >> R1?L task and are fixed to be used in all [[ transfer pairs ]] .,0
6231,4802,The hyperparameters are tuned on 10 % randomly held - out training data of the target domain << in >> [[ R1?L task ]] and are fixed to be used in all [[ transfer pairs ]] .,0
6232,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the [[ target domain ]] in R1?L task and are << fixed to be used in all >> transfer pairs .,0
6233,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the target domain in [[ R1?L task ]] and are << fixed to be used in all >> transfer pairs .,0
6234,4802,The hyperparameters are tuned on [[ 10 % randomly held - out training data ]] of the target domain in R1?L task and are << fixed to be used in all >> [[ transfer pairs ]] .,1
6235,4802,The hyperparameters are tuned on 10 % randomly held - out training data of the [[ target domain ]] in [[ R1?L task ]] and are << fixed to be used in all >> transfer pairs .,0
6236,4802,The hyperparameters are tuned on 10 % randomly held - out training data of the [[ target domain ]] in R1?L task and are << fixed to be used in all >> [[ transfer pairs ]] .,0
6237,4802,The hyperparameters are tuned on 10 % randomly held - out training data of the target domain in [[ R1?L task ]] and are << fixed to be used in all >> [[ transfer pairs ]] .,0
6238,4810,[[ M- DAN ]] : It << is >> a [[ multi-adversarial version ]] of Domain Adversarial Network ( DAN ) ) based on multiple domain discriminators .,1
6239,4810,[[ M- DAN ]] : It << is >> a multi-adversarial version of [[ Domain Adversarial Network ( DAN ) ]] ) based on multiple domain discriminators .,0
6240,4810,[[ M- DAN ]] : It << is >> a multi-adversarial version of Domain Adversarial Network ( DAN ) ) based on [[ multiple domain discriminators ]] .,0
6241,4810,M- DAN : It << is >> a [[ multi-adversarial version ]] of [[ Domain Adversarial Network ( DAN ) ]] ) based on multiple domain discriminators .,0
6242,4810,M- DAN : It << is >> a [[ multi-adversarial version ]] of Domain Adversarial Network ( DAN ) ) based on [[ multiple domain discriminators ]] .,0
6243,4810,M- DAN : It << is >> a multi-adversarial version of [[ Domain Adversarial Network ( DAN ) ]] ) based on [[ multiple domain discriminators ]] .,0
6244,4810,[[ M- DAN ]] : It is a [[ multi-adversarial version ]] << of >> Domain Adversarial Network ( DAN ) ) based on multiple domain discriminators .,0
6245,4810,[[ M- DAN ]] : It is a multi-adversarial version << of >> [[ Domain Adversarial Network ( DAN ) ]] ) based on multiple domain discriminators .,0
6246,4810,[[ M- DAN ]] : It is a multi-adversarial version << of >> Domain Adversarial Network ( DAN ) ) based on [[ multiple domain discriminators ]] .,0
6247,4810,M- DAN : It is a [[ multi-adversarial version ]] << of >> [[ Domain Adversarial Network ( DAN ) ]] ) based on multiple domain discriminators .,1
6248,4810,M- DAN : It is a [[ multi-adversarial version ]] << of >> Domain Adversarial Network ( DAN ) ) based on [[ multiple domain discriminators ]] .,0
6249,4810,M- DAN : It is a multi-adversarial version << of >> [[ Domain Adversarial Network ( DAN ) ]] ) based on [[ multiple domain discriminators ]] .,0
6250,4810,[[ M- DAN ]] : It is a [[ multi-adversarial version ]] of Domain Adversarial Network ( DAN ) ) << based on >> multiple domain discriminators .,0
6251,4810,[[ M- DAN ]] : It is a multi-adversarial version of [[ Domain Adversarial Network ( DAN ) ]] ) << based on >> multiple domain discriminators .,0
6252,4810,[[ M- DAN ]] : It is a multi-adversarial version of Domain Adversarial Network ( DAN ) ) << based on >> [[ multiple domain discriminators ]] .,0
6253,4810,M- DAN : It is a [[ multi-adversarial version ]] of [[ Domain Adversarial Network ( DAN ) ]] ) << based on >> multiple domain discriminators .,0
6254,4810,M- DAN : It is a [[ multi-adversarial version ]] of Domain Adversarial Network ( DAN ) ) << based on >> [[ multiple domain discriminators ]] .,1
6255,4810,M- DAN : It is a multi-adversarial version of [[ Domain Adversarial Network ( DAN ) ]] ) << based on >> [[ multiple domain discriminators ]] .,0
6256,6003,We << utilize >> a [[ widely - used Information Retrieval ( IR ) platform ]] to find out [[ candidate soft templates ]] from the training corpus .,0
6257,6003,We << utilize >> a [[ widely - used Information Retrieval ( IR ) platform ]] to find out candidate soft templates from the [[ training corpus ]] .,0
6258,6003,We << utilize >> a widely - used Information Retrieval ( IR ) platform to find out [[ candidate soft templates ]] from the [[ training corpus ]] .,0
6259,6003,We utilize a [[ widely - used Information Retrieval ( IR ) platform ]] << to find out >> [[ candidate soft templates ]] from the training corpus .,1
6260,6003,We utilize a [[ widely - used Information Retrieval ( IR ) platform ]] << to find out >> candidate soft templates from the [[ training corpus ]] .,0
6261,6003,We utilize a widely - used Information Retrieval ( IR ) platform << to find out >> [[ candidate soft templates ]] from the [[ training corpus ]] .,0
6262,6003,We utilize a [[ widely - used Information Retrieval ( IR ) platform ]] to find out [[ candidate soft templates ]] << from >> the training corpus .,0
6263,6003,We utilize a [[ widely - used Information Retrieval ( IR ) platform ]] to find out candidate soft templates << from >> the [[ training corpus ]] .,0
6264,6003,We utilize a widely - used Information Retrieval ( IR ) platform to find out [[ candidate soft templates ]] << from >> the [[ training corpus ]] .,1
6265,2127,"We << note that >> [[ lemma + lowercase ]] performs [[ worse ]] than any model with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6266,2127,"We << note that >> [[ lemma + lowercase ]] performs worse than [[ any model ]] with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6267,2127,"We << note that >> [[ lemma + lowercase ]] performs worse than any model with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6268,2127,"We << note that >> lemma + lowercase performs [[ worse ]] than [[ any model ]] with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6269,2127,"We << note that >> lemma + lowercase performs [[ worse ]] than any model with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6270,2127,"We << note that >> lemma + lowercase performs worse than [[ any model ]] with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6271,2127,"We note that [[ lemma + lowercase ]] << performs >> [[ worse ]] than any model with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",1
6272,2127,"We note that [[ lemma + lowercase ]] << performs >> worse than [[ any model ]] with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6273,2127,"We note that [[ lemma + lowercase ]] << performs >> worse than any model with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6274,2127,"We note that lemma + lowercase << performs >> [[ worse ]] than [[ any model ]] with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6275,2127,"We note that lemma + lowercase << performs >> [[ worse ]] than any model with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6276,2127,"We note that lemma + lowercase << performs >> worse than [[ any model ]] with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6277,2127,"We note that [[ lemma + lowercase ]] performs [[ worse ]] << than >> any model with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6278,2127,"We note that [[ lemma + lowercase ]] performs worse << than >> [[ any model ]] with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6279,2127,"We note that [[ lemma + lowercase ]] performs worse << than >> any model with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6280,2127,"We note that lemma + lowercase performs [[ worse ]] << than >> [[ any model ]] with the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",1
6281,2127,"We note that lemma + lowercase performs [[ worse ]] << than >> any model with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6282,2127,"We note that lemma + lowercase performs worse << than >> [[ any model ]] with the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6283,2127,"We note that [[ lemma + lowercase ]] performs [[ worse ]] than any model << with >> the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6284,2127,"We note that [[ lemma + lowercase ]] performs worse than [[ any model ]] << with >> the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6285,2127,"We note that [[ lemma + lowercase ]] performs worse than any model << with >> the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6286,2127,"We note that lemma + lowercase performs [[ worse ]] than [[ any model ]] << with >> the dictionary , which suggests that dictionary definitions are used in a non-trivial way .",0
6287,2127,"We note that lemma + lowercase performs [[ worse ]] than any model << with >> the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",0
6288,2127,"We note that lemma + lowercase performs worse than [[ any model ]] << with >> the [[ dictionary ]] , which suggests that dictionary definitions are used in a non-trivial way .",1
6289,4997,"[[ Each layer ]] << is >> a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then utilizes this information to calculate continuous text representation .",1
6290,4997,"[[ Each layer ]] << is >> a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then utilizes this information to calculate continuous text representation .",0
6291,4997,"[[ Each layer ]] << is >> a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6292,4997,"[[ Each layer ]] << is >> a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6293,4997,"[[ Each layer ]] << is >> a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6294,4997,"Each layer << is >> a [[ content - and location - based attention model ]] , which first learns the [[ importance / weight ]] of each context word and then utilizes this information to calculate continuous text representation .",0
6295,4997,"Each layer << is >> a [[ content - and location - based attention model ]] , which first learns the importance / weight of [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6296,4997,"Each layer << is >> a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6297,4997,"Each layer << is >> a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6298,4997,"Each layer << is >> a content - and location - based attention model , which first learns the [[ importance / weight ]] of [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6299,4997,"Each layer << is >> a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6300,4997,"Each layer << is >> a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6301,4997,"Each layer << is >> a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then utilizes this [[ information ]] to calculate continuous text representation .",0
6302,4997,"Each layer << is >> a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then utilizes this information to calculate [[ continuous text representation ]] .",0
6303,4997,"Each layer << is >> a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this [[ information ]] to calculate [[ continuous text representation ]] .",0
6304,4997,"[[ Each layer ]] is a [[ content - and location - based attention model ]] , which << first learns >> the importance / weight of each context word and then utilizes this information to calculate continuous text representation .",0
6305,4997,"[[ Each layer ]] is a content - and location - based attention model , which << first learns >> the [[ importance / weight ]] of each context word and then utilizes this information to calculate continuous text representation .",0
6306,4997,"[[ Each layer ]] is a content - and location - based attention model , which << first learns >> the importance / weight of [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6307,4997,"[[ Each layer ]] is a content - and location - based attention model , which << first learns >> the importance / weight of each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6308,4997,"[[ Each layer ]] is a content - and location - based attention model , which << first learns >> the importance / weight of each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6309,4997,"Each layer is a [[ content - and location - based attention model ]] , which << first learns >> the [[ importance / weight ]] of each context word and then utilizes this information to calculate continuous text representation .",1
6310,4997,"Each layer is a [[ content - and location - based attention model ]] , which << first learns >> the importance / weight of [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6311,4997,"Each layer is a [[ content - and location - based attention model ]] , which << first learns >> the importance / weight of each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6312,4997,"Each layer is a [[ content - and location - based attention model ]] , which << first learns >> the importance / weight of each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6313,4997,"Each layer is a content - and location - based attention model , which << first learns >> the [[ importance / weight ]] of [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6314,4997,"Each layer is a content - and location - based attention model , which << first learns >> the [[ importance / weight ]] of each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6315,4997,"Each layer is a content - and location - based attention model , which << first learns >> the [[ importance / weight ]] of each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6316,4997,"Each layer is a content - and location - based attention model , which << first learns >> the importance / weight of [[ each context word ]] and then utilizes this [[ information ]] to calculate continuous text representation .",0
6317,4997,"Each layer is a content - and location - based attention model , which << first learns >> the importance / weight of [[ each context word ]] and then utilizes this information to calculate [[ continuous text representation ]] .",0
6318,4997,"Each layer is a content - and location - based attention model , which << first learns >> the importance / weight of each context word and then utilizes this [[ information ]] to calculate [[ continuous text representation ]] .",0
6319,4997,"[[ Each layer ]] is a [[ content - and location - based attention model ]] , which first learns the importance / weight << of >> each context word and then utilizes this information to calculate continuous text representation .",0
6320,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the [[ importance / weight ]] << of >> each context word and then utilizes this information to calculate continuous text representation .",0
6321,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight << of >> [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6322,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight << of >> each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6323,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight << of >> each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6324,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the [[ importance / weight ]] << of >> each context word and then utilizes this information to calculate continuous text representation .",0
6325,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight << of >> [[ each context word ]] and then utilizes this information to calculate continuous text representation .",0
6326,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight << of >> each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6327,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight << of >> each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6328,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] << of >> [[ each context word ]] and then utilizes this information to calculate continuous text representation .",1
6329,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] << of >> each context word and then utilizes this [[ information ]] to calculate continuous text representation .",0
6330,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] << of >> each context word and then utilizes this information to calculate [[ continuous text representation ]] .",0
6331,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight << of >> [[ each context word ]] and then utilizes this [[ information ]] to calculate continuous text representation .",0
6332,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight << of >> [[ each context word ]] and then utilizes this information to calculate [[ continuous text representation ]] .",0
6333,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight << of >> each context word and then utilizes this [[ information ]] to calculate [[ continuous text representation ]] .",0
6334,4997,"[[ Each layer ]] is a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then << utilizes >> this information to calculate continuous text representation .",0
6335,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then << utilizes >> this information to calculate continuous text representation .",0
6336,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then << utilizes >> this information to calculate continuous text representation .",0
6337,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight of each context word and then << utilizes >> this [[ information ]] to calculate continuous text representation .",0
6338,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight of each context word and then << utilizes >> this information to calculate [[ continuous text representation ]] .",0
6339,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the [[ importance / weight ]] of each context word and then << utilizes >> this information to calculate continuous text representation .",0
6340,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight of [[ each context word ]] and then << utilizes >> this information to calculate continuous text representation .",0
6341,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then << utilizes >> this [[ information ]] to calculate continuous text representation .",1
6342,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then << utilizes >> this information to calculate [[ continuous text representation ]] .",0
6343,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] of [[ each context word ]] and then << utilizes >> this information to calculate continuous text representation .",0
6344,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then << utilizes >> this [[ information ]] to calculate continuous text representation .",0
6345,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then << utilizes >> this information to calculate [[ continuous text representation ]] .",0
6346,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then << utilizes >> this [[ information ]] to calculate continuous text representation .",0
6347,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then << utilizes >> this information to calculate [[ continuous text representation ]] .",0
6348,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then << utilizes >> this [[ information ]] to calculate [[ continuous text representation ]] .",0
6349,4997,"[[ Each layer ]] is a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then utilizes this information << to calculate >> continuous text representation .",0
6350,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then utilizes this information << to calculate >> continuous text representation .",0
6351,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then utilizes this information << to calculate >> continuous text representation .",0
6352,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this [[ information ]] << to calculate >> continuous text representation .",0
6353,4997,"[[ Each layer ]] is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information << to calculate >> [[ continuous text representation ]] .",0
6354,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the [[ importance / weight ]] of each context word and then utilizes this information << to calculate >> continuous text representation .",0
6355,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight of [[ each context word ]] and then utilizes this information << to calculate >> continuous text representation .",0
6356,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then utilizes this [[ information ]] << to calculate >> continuous text representation .",0
6357,4997,"Each layer is a [[ content - and location - based attention model ]] , which first learns the importance / weight of each context word and then utilizes this information << to calculate >> [[ continuous text representation ]] .",0
6358,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] of [[ each context word ]] and then utilizes this information << to calculate >> continuous text representation .",0
6359,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then utilizes this [[ information ]] << to calculate >> continuous text representation .",0
6360,4997,"Each layer is a content - and location - based attention model , which first learns the [[ importance / weight ]] of each context word and then utilizes this information << to calculate >> [[ continuous text representation ]] .",0
6361,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then utilizes this [[ information ]] << to calculate >> continuous text representation .",0
6362,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight of [[ each context word ]] and then utilizes this information << to calculate >> [[ continuous text representation ]] .",0
6363,4997,"Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this [[ information ]] << to calculate >> [[ continuous text representation ]] .",1
6364,4783,"In addition , it << achieved >> a [[ high performance ]] on the [[ ArSAS dataset ]] .",0
6365,4783,"In addition , it achieved a [[ high performance ]] << on >> the [[ ArSAS dataset ]] .",1
6366,5726,"Second , it is << interesting to note that with >> the [[ attention mechanism ]] , the [[ coherence ]] is decreased slightly in the Seq2Seq model but increased significantly in the AEM model .",0
6367,5726,"Second , it is << interesting to note that with >> the [[ attention mechanism ]] , the coherence is decreased slightly in the [[ Seq2Seq model ]] but increased significantly in the AEM model .",0
6368,5726,"Second , it is << interesting to note that with >> the [[ attention mechanism ]] , the coherence is decreased slightly in the Seq2Seq model but increased significantly in the [[ AEM model ]] .",0
6369,5726,"Second , it is << interesting to note that with >> the attention mechanism , the [[ coherence ]] is decreased slightly in the [[ Seq2Seq model ]] but increased significantly in the AEM model .",0
6370,5726,"Second , it is << interesting to note that with >> the attention mechanism , the [[ coherence ]] is decreased slightly in the Seq2Seq model but increased significantly in the [[ AEM model ]] .",0
6371,5726,"Second , it is << interesting to note that with >> the attention mechanism , the coherence is decreased slightly in the [[ Seq2Seq model ]] but increased significantly in the [[ AEM model ]] .",0
6372,5726,"Second , it is interesting to note that with the [[ attention mechanism ]] , the [[ coherence ]] is << decreased slightly in >> the Seq2Seq model but increased significantly in the AEM model .",0
6373,5726,"Second , it is interesting to note that with the [[ attention mechanism ]] , the coherence is << decreased slightly in >> the [[ Seq2Seq model ]] but increased significantly in the AEM model .",0
6374,5726,"Second , it is interesting to note that with the [[ attention mechanism ]] , the coherence is << decreased slightly in >> the Seq2Seq model but increased significantly in the [[ AEM model ]] .",0
6375,5726,"Second , it is interesting to note that with the attention mechanism , the [[ coherence ]] is << decreased slightly in >> the [[ Seq2Seq model ]] but increased significantly in the AEM model .",1
6376,5726,"Second , it is interesting to note that with the attention mechanism , the [[ coherence ]] is << decreased slightly in >> the Seq2Seq model but increased significantly in the [[ AEM model ]] .",0
6377,5726,"Second , it is interesting to note that with the attention mechanism , the coherence is << decreased slightly in >> the [[ Seq2Seq model ]] but increased significantly in the [[ AEM model ]] .",0
6378,5726,"Second , it is interesting to note that with the [[ attention mechanism ]] , the [[ coherence ]] is decreased slightly in the Seq2Seq model but << increased significantly in >> the AEM model .",0
6379,5726,"Second , it is interesting to note that with the [[ attention mechanism ]] , the coherence is decreased slightly in the [[ Seq2Seq model ]] but << increased significantly in >> the AEM model .",0
6380,5726,"Second , it is interesting to note that with the [[ attention mechanism ]] , the coherence is decreased slightly in the Seq2Seq model but << increased significantly in >> the [[ AEM model ]] .",0
6381,5726,"Second , it is interesting to note that with the attention mechanism , the [[ coherence ]] is decreased slightly in the [[ Seq2Seq model ]] but << increased significantly in >> the AEM model .",0
6382,5726,"Second , it is interesting to note that with the attention mechanism , the [[ coherence ]] is decreased slightly in the Seq2Seq model but << increased significantly in >> the [[ AEM model ]] .",0
6383,5726,"Second , it is interesting to note that with the attention mechanism , the coherence is decreased slightly in the [[ Seq2Seq model ]] but << increased significantly in >> the [[ AEM model ]] .",1
6384,3795,"The [[ performances ]] << of >> the [[ Seq2Seq - based basic models ]] including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are very low .",1
6385,3795,"The [[ performances ]] << of >> the Seq2Seq - based basic models including [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying are very low .",0
6386,3795,"The [[ performances ]] << of >> the Seq2Seq - based basic models including Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are very low .",0
6387,3795,"The [[ performances ]] << of >> the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are very low .",0
6388,3795,"The [[ performances ]] << of >> the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are [[ very low ]] .",0
6389,3795,"The performances << of >> the [[ Seq2Seq - based basic models ]] including [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying are very low .",0
6390,3795,"The performances << of >> the [[ Seq2Seq - based basic models ]] including Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are very low .",0
6391,3795,"The performances << of >> the [[ Seq2Seq - based basic models ]] including Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are very low .",0
6392,3795,"The performances << of >> the [[ Seq2Seq - based basic models ]] including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are [[ very low ]] .",0
6393,3795,"The performances << of >> the Seq2Seq - based basic models including [[ Seq2Seq ]] , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are very low .",0
6394,3795,"The performances << of >> the Seq2Seq - based basic models including [[ Seq2Seq ]] , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are very low .",0
6395,3795,"The performances << of >> the Seq2Seq - based basic models including [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying are [[ very low ]] .",0
6396,3795,"The performances << of >> the Seq2Seq - based basic models including Seq2Seq , [[ Seq2Seq + Attention ]] , and [[ Seq2Seq + Copying ]] are very low .",0
6397,3795,"The performances << of >> the Seq2Seq - based basic models including Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are [[ very low ]] .",0
6398,3795,"The performances << of >> the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are [[ very low ]] .",0
6399,3795,"The [[ performances ]] of the [[ Seq2Seq - based basic models ]] << including >> Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are very low .",0
6400,3795,"The [[ performances ]] of the Seq2Seq - based basic models << including >> [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying are very low .",0
6401,3795,"The [[ performances ]] of the Seq2Seq - based basic models << including >> Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are very low .",0
6402,3795,"The [[ performances ]] of the Seq2Seq - based basic models << including >> Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are very low .",0
6403,3795,"The [[ performances ]] of the Seq2Seq - based basic models << including >> Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are [[ very low ]] .",0
6404,3795,"The performances of the [[ Seq2Seq - based basic models ]] << including >> [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying are very low .",1
6405,3795,"The performances of the [[ Seq2Seq - based basic models ]] << including >> Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are very low .",1
6406,3795,"The performances of the [[ Seq2Seq - based basic models ]] << including >> Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are very low .",1
6407,3795,"The performances of the [[ Seq2Seq - based basic models ]] << including >> Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are [[ very low ]] .",0
6408,3795,"The performances of the Seq2Seq - based basic models << including >> [[ Seq2Seq ]] , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are very low .",0
6409,3795,"The performances of the Seq2Seq - based basic models << including >> [[ Seq2Seq ]] , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are very low .",0
6410,3795,"The performances of the Seq2Seq - based basic models << including >> [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying are [[ very low ]] .",0
6411,3795,"The performances of the Seq2Seq - based basic models << including >> Seq2Seq , [[ Seq2Seq + Attention ]] , and [[ Seq2Seq + Copying ]] are very low .",0
6412,3795,"The performances of the Seq2Seq - based basic models << including >> Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying are [[ very low ]] .",0
6413,3795,"The performances of the Seq2Seq - based basic models << including >> Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] are [[ very low ]] .",0
6414,3795,"The [[ performances ]] of the [[ Seq2Seq - based basic models ]] including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying << are >> very low .",0
6415,3795,"The [[ performances ]] of the Seq2Seq - based basic models including [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying << are >> very low .",0
6416,3795,"The [[ performances ]] of the Seq2Seq - based basic models including Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying << are >> very low .",0
6417,3795,"The [[ performances ]] of the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] << are >> very low .",0
6418,3795,"The [[ performances ]] of the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying << are >> [[ very low ]] .",0
6419,3795,"The performances of the [[ Seq2Seq - based basic models ]] including [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying << are >> very low .",0
6420,3795,"The performances of the [[ Seq2Seq - based basic models ]] including Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying << are >> very low .",0
6421,3795,"The performances of the [[ Seq2Seq - based basic models ]] including Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] << are >> very low .",0
6422,3795,"The performances of the [[ Seq2Seq - based basic models ]] including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying << are >> [[ very low ]] .",1
6423,3795,"The performances of the Seq2Seq - based basic models including [[ Seq2Seq ]] , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying << are >> very low .",0
6424,3795,"The performances of the Seq2Seq - based basic models including [[ Seq2Seq ]] , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] << are >> very low .",0
6425,3795,"The performances of the Seq2Seq - based basic models including [[ Seq2Seq ]] , Seq2Seq + Attention , and Seq2Seq + Copying << are >> [[ very low ]] .",0
6426,3795,"The performances of the Seq2Seq - based basic models including Seq2Seq , [[ Seq2Seq + Attention ]] , and [[ Seq2Seq + Copying ]] << are >> very low .",0
6427,3795,"The performances of the Seq2Seq - based basic models including Seq2Seq , [[ Seq2Seq + Attention ]] , and Seq2Seq + Copying << are >> [[ very low ]] .",0
6428,3795,"The performances of the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and [[ Seq2Seq + Copying ]] << are >> [[ very low ]] .",0
6429,2499,"In general , the model << ensures >> a [[ ' local ' loss ]] that is incurred for [[ each recurrent unit cell ]] .",0
6430,2499,"In general , the model ensures a [[ ' local ' loss ]] that is << incurred for >> [[ each recurrent unit cell ]] .",1
6431,2788,"<< To further explore >> the [[ relation ]] between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6432,2788,"<< To further explore >> the [[ relation ]] between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6433,2788,"<< To further explore >> the [[ relation ]] between representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6434,2788,"<< To further explore >> the [[ relation ]] between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6435,2788,"<< To further explore >> the [[ relation ]] between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6436,2788,"<< To further explore >> the relation between [[ representing sentences and matching them ]] , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6437,2788,"<< To further explore >> the relation between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6438,2788,"<< To further explore >> the relation between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6439,2788,"<< To further explore >> the relation between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6440,2788,"<< To further explore >> the relation between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6441,2788,"<< To further explore >> the relation between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6442,2788,"<< To further explore >> the relation between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6443,2788,"<< To further explore >> the relation between representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6444,2788,"<< To further explore >> the relation between representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6445,2788,"<< To further explore >> the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the [[ same convolutional architecture ]] .",0
6446,2788,"To further explore the [[ relation ]] << between >> [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",1
6447,2788,"To further explore the [[ relation ]] << between >> representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6448,2788,"To further explore the [[ relation ]] << between >> representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6449,2788,"To further explore the [[ relation ]] << between >> representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6450,2788,"To further explore the [[ relation ]] << between >> representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6451,2788,"To further explore the relation << between >> [[ representing sentences and matching them ]] , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6452,2788,"To further explore the relation << between >> [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6453,2788,"To further explore the relation << between >> [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6454,2788,"To further explore the relation << between >> [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6455,2788,"To further explore the relation << between >> representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6456,2788,"To further explore the relation << between >> representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6457,2788,"To further explore the relation << between >> representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6458,2788,"To further explore the relation << between >> representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6459,2788,"To further explore the relation << between >> representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6460,2788,"To further explore the relation << between >> representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the [[ same convolutional architecture ]] .",0
6461,2788,"To further explore the [[ relation ]] between [[ representing sentences and matching them ]] , we << devise >> a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6462,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we << devise >> a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6463,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we << devise >> a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6464,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we << devise >> a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6465,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we << devise >> a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6466,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we << devise >> a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",1
6467,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we << devise >> a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6468,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we << devise >> a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6469,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we << devise >> a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6470,2788,"To further explore the relation between representing sentences and matching them , we << devise >> a [[ novel model ]] that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6471,2788,"To further explore the relation between representing sentences and matching them , we << devise >> a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6472,2788,"To further explore the relation between representing sentences and matching them , we << devise >> a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6473,2788,"To further explore the relation between representing sentences and matching them , we << devise >> a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6474,2788,"To further explore the relation between representing sentences and matching them , we << devise >> a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6475,2788,"To further explore the relation between representing sentences and matching them , we << devise >> a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the [[ same convolutional architecture ]] .",0
6476,2788,"To further explore the [[ relation ]] between [[ representing sentences and matching them ]] , we devise a novel model that << can naturally host >> both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6477,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a [[ novel model ]] that << can naturally host >> both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6478,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a novel model that << can naturally host >> both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6479,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a novel model that << can naturally host >> both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6480,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a novel model that << can naturally host >> both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6481,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a [[ novel model ]] that << can naturally host >> both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6482,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a novel model that << can naturally host >> both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6483,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a novel model that << can naturally host >> both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6484,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a novel model that << can naturally host >> both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6485,2788,"To further explore the relation between representing sentences and matching them , we devise a [[ novel model ]] that << can naturally host >> both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",0
6486,2788,"To further explore the relation between representing sentences and matching them , we devise a [[ novel model ]] that << can naturally host >> both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6487,2788,"To further explore the relation between representing sentences and matching them , we devise a [[ novel model ]] that << can naturally host >> both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6488,2788,"To further explore the relation between representing sentences and matching them , we devise a novel model that << can naturally host >> both the [[ hierarchical composition for sentences ]] and the [[ simple - to - comprehensive fusion of matching patterns ]] with the same convolutional architecture .",0
6489,2788,"To further explore the relation between representing sentences and matching them , we devise a novel model that << can naturally host >> both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns with the [[ same convolutional architecture ]] .",0
6490,2788,"To further explore the relation between representing sentences and matching them , we devise a novel model that << can naturally host >> both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] with the [[ same convolutional architecture ]] .",0
6491,2788,"To further explore the [[ relation ]] between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns << with >> the same convolutional architecture .",0
6492,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns << with >> the same convolutional architecture .",0
6493,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns << with >> the same convolutional architecture .",0
6494,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] << with >> the same convolutional architecture .",0
6495,2788,"To further explore the [[ relation ]] between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns << with >> the [[ same convolutional architecture ]] .",0
6496,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns << with >> the same convolutional architecture .",0
6497,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns << with >> the same convolutional architecture .",0
6498,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] << with >> the same convolutional architecture .",0
6499,2788,"To further explore the relation between [[ representing sentences and matching them ]] , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns << with >> the [[ same convolutional architecture ]] .",0
6500,2788,"To further explore the relation between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns << with >> the same convolutional architecture .",0
6501,2788,"To further explore the relation between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] << with >> the same convolutional architecture .",0
6502,2788,"To further explore the relation between representing sentences and matching them , we devise a [[ novel model ]] that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns << with >> the [[ same convolutional architecture ]] .",1
6503,2788,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the [[ simple - to - comprehensive fusion of matching patterns ]] << with >> the same convolutional architecture .",0
6504,2788,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the [[ hierarchical composition for sentences ]] and the simple - to - comprehensive fusion of matching patterns << with >> the [[ same convolutional architecture ]] .",0
6505,2788,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the [[ simple - to - comprehensive fusion of matching patterns ]] << with >> the [[ same convolutional architecture ]] .",0
6506,4196,[[ CMN self ]] which << uses only >> [[ single history channel ]] also provides lesser performance when compared to CMN .,1
6507,4196,[[ CMN self ]] which << uses only >> single history channel also provides [[ lesser performance ]] when compared to CMN .,0
6508,4196,[[ CMN self ]] which << uses only >> single history channel also provides lesser performance when compared to [[ CMN ]] .,0
6509,4196,CMN self which << uses only >> [[ single history channel ]] also provides [[ lesser performance ]] when compared to CMN .,0
6510,4196,CMN self which << uses only >> [[ single history channel ]] also provides lesser performance when compared to [[ CMN ]] .,0
6511,4196,CMN self which << uses only >> single history channel also provides [[ lesser performance ]] when compared to [[ CMN ]] .,0
6512,4196,[[ CMN self ]] which uses only [[ single history channel ]] also << provides >> lesser performance when compared to CMN .,0
6513,4196,[[ CMN self ]] which uses only single history channel also << provides >> [[ lesser performance ]] when compared to CMN .,1
6514,4196,[[ CMN self ]] which uses only single history channel also << provides >> lesser performance when compared to [[ CMN ]] .,0
6515,4196,CMN self which uses only [[ single history channel ]] also << provides >> [[ lesser performance ]] when compared to CMN .,0
6516,4196,CMN self which uses only [[ single history channel ]] also << provides >> lesser performance when compared to [[ CMN ]] .,0
6517,4196,CMN self which uses only single history channel also << provides >> [[ lesser performance ]] when compared to [[ CMN ]] .,0
6518,4196,[[ CMN self ]] which uses only [[ single history channel ]] also provides lesser performance when << compared to >> CMN .,0
6519,4196,[[ CMN self ]] which uses only single history channel also provides [[ lesser performance ]] when << compared to >> CMN .,0
6520,4196,[[ CMN self ]] which uses only single history channel also provides lesser performance when << compared to >> [[ CMN ]] .,0
6521,4196,CMN self which uses only [[ single history channel ]] also provides [[ lesser performance ]] when << compared to >> CMN .,0
6522,4196,CMN self which uses only [[ single history channel ]] also provides lesser performance when << compared to >> [[ CMN ]] .,0
6523,4196,CMN self which uses only single history channel also provides [[ lesser performance ]] when << compared to >> [[ CMN ]] .,1
6524,3827,The [[ smoothing parameter ]] was << set to >> [[ 0.1 ]] .,1
6525,2492,"On the [[ SFU Review Corpus ]] , we << outperform >> the [[ best system ]] to date by 1.02 F1 .",1
6526,2492,"On the [[ SFU Review Corpus ]] , we << outperform >> the best system to date by [[ 1.02 F1 ]] .",0
6527,2492,"On the SFU Review Corpus , we << outperform >> the [[ best system ]] to date by [[ 1.02 F1 ]] .",0
6528,2492,"On the [[ SFU Review Corpus ]] , we outperform the [[ best system ]] to date << by >> 1.02 F1 .",0
6529,2492,"On the [[ SFU Review Corpus ]] , we outperform the best system to date << by >> [[ 1.02 F1 ]] .",0
6530,2492,"On the SFU Review Corpus , we outperform the [[ best system ]] to date << by >> [[ 1.02 F1 ]] .",1
6531,878,"We use a [[ learning rate warm - up scheme ]] << with >> an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",1
6532,878,"We use a [[ learning rate warm - up scheme ]] << with >> an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6533,878,"We use a [[ learning rate warm - up scheme ]] << with >> an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6534,878,"We use a [[ learning rate warm - up scheme ]] << with >> an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6535,878,"We use a [[ learning rate warm - up scheme ]] << with >> an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6536,878,"We use a [[ learning rate warm - up scheme ]] << with >> an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6537,878,"We use a [[ learning rate warm - up scheme ]] << with >> an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6538,878,"We use a [[ learning rate warm - up scheme ]] << with >> an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6539,878,"We use a learning rate warm - up scheme << with >> an [[ inverse exponential ]] [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6540,878,"We use a learning rate warm - up scheme << with >> an [[ inverse exponential ]] increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6541,878,"We use a learning rate warm - up scheme << with >> an [[ inverse exponential ]] increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6542,878,"We use a learning rate warm - up scheme << with >> an [[ inverse exponential ]] increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6543,878,"We use a learning rate warm - up scheme << with >> an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6544,878,"We use a learning rate warm - up scheme << with >> an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6545,878,"We use a learning rate warm - up scheme << with >> an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6546,878,"We use a learning rate warm - up scheme << with >> an inverse exponential [[ increase ]] from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6547,878,"We use a learning rate warm - up scheme << with >> an inverse exponential [[ increase ]] from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6548,878,"We use a learning rate warm - up scheme << with >> an inverse exponential [[ increase ]] from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6549,878,"We use a learning rate warm - up scheme << with >> an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6550,878,"We use a learning rate warm - up scheme << with >> an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6551,878,"We use a learning rate warm - up scheme << with >> an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6552,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from [[ 0.0 ]] to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6553,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from [[ 0.0 ]] to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6554,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6555,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6556,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6557,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to [[ 0.001 ]] in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6558,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6559,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6560,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6561,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6562,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6563,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6564,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a [[ constant learning rate ]] for the remainder of training .",0
6565,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the [[ remainder of training ]] .",0
6566,878,"We use a learning rate warm - up scheme << with >> an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the [[ remainder of training ]] .",0
6567,878,"We use a [[ learning rate warm - up scheme ]] with an [[ inverse exponential ]] increase << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6568,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential [[ increase ]] << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6569,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase << from >> [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6570,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase << from >> 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6571,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase << from >> 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6572,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase << from >> 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6573,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6574,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6575,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] [[ increase ]] << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6576,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase << from >> [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6577,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase << from >> 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6578,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase << from >> 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6579,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase << from >> 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6580,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6581,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6582,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] << from >> [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",1
6583,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] << from >> 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6584,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] << from >> 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6585,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] << from >> 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6586,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6587,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6588,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> [[ 0.0 ]] to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6589,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> [[ 0.0 ]] to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6590,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> [[ 0.0 ]] to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6591,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6592,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6593,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to [[ 0.001 ]] in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6594,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to [[ 0.001 ]] in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6595,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6596,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6597,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to 0.001 in the [[ first 1000 steps ]] , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6598,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6599,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6600,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a [[ constant learning rate ]] for the remainder of training .",0
6601,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the [[ remainder of training ]] .",0
6602,878,"We use a learning rate warm - up scheme with an inverse exponential increase << from >> 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the [[ remainder of training ]] .",0
6603,878,"We use a [[ learning rate warm - up scheme ]] with an [[ inverse exponential ]] increase from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6604,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential [[ increase ]] from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6605,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from [[ 0.0 ]] << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6606,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 << to >> [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6607,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 << to >> 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6608,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 << to >> 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6609,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6610,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6611,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] [[ increase ]] from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6612,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from [[ 0.0 ]] << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6613,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 << to >> [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6614,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 << to >> 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6615,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 << to >> 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6616,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6617,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6618,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from [[ 0.0 ]] << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6619,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 << to >> [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6620,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 << to >> 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6621,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 << to >> 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6622,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6623,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6624,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] << to >> [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",1
6625,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] << to >> 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6626,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] << to >> 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6627,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] << to >> 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6628,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] << to >> 0.001 in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6629,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> [[ 0.001 ]] in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6630,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> [[ 0.001 ]] in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6631,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> [[ 0.001 ]] in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6632,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6633,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> 0.001 in the [[ first 1000 steps ]] , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6634,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> 0.001 in the [[ first 1000 steps ]] , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6635,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6636,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> 0.001 in the first 1000 steps , and then [[ maintain ]] a [[ constant learning rate ]] for the remainder of training .",0
6637,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate for the [[ remainder of training ]] .",0
6638,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 << to >> 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] for the [[ remainder of training ]] .",0
6639,878,"We use a [[ learning rate warm - up scheme ]] with an [[ inverse exponential ]] increase from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6640,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential [[ increase ]] from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6641,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from [[ 0.0 ]] to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6642,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to [[ 0.001 ]] << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6643,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 << in >> the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6644,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 << in >> the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6645,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6646,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6647,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] [[ increase ]] from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6648,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from [[ 0.0 ]] to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6649,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to [[ 0.001 ]] << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6650,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 << in >> the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6651,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 << in >> the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6652,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6653,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6654,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from [[ 0.0 ]] to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6655,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to [[ 0.001 ]] << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6656,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 << in >> the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",1
6657,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 << in >> the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6658,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6659,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6660,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to [[ 0.001 ]] << in >> the first 1000 steps , and then maintain a constant learning rate for the remainder of training .",0
6661,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 << in >> the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6662,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 << in >> the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6663,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 << in >> the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6664,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 << in >> the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6665,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] << in >> the [[ first 1000 steps ]] , and then maintain a constant learning rate for the remainder of training .",0
6666,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] << in >> the first 1000 steps , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6667,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] << in >> the first 1000 steps , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6668,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] << in >> the first 1000 steps , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6669,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 << in >> the [[ first 1000 steps ]] , and then [[ maintain ]] a constant learning rate for the remainder of training .",0
6670,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 << in >> the [[ first 1000 steps ]] , and then maintain a [[ constant learning rate ]] for the remainder of training .",0
6671,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 << in >> the [[ first 1000 steps ]] , and then maintain a constant learning rate for the [[ remainder of training ]] .",0
6672,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 << in >> the first 1000 steps , and then [[ maintain ]] a [[ constant learning rate ]] for the remainder of training .",0
6673,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 << in >> the first 1000 steps , and then [[ maintain ]] a constant learning rate for the [[ remainder of training ]] .",0
6674,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 << in >> the first 1000 steps , and then maintain a [[ constant learning rate ]] for the [[ remainder of training ]] .",0
6675,878,"We use a [[ learning rate warm - up scheme ]] with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6676,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6677,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6678,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6679,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and << then >> maintain a constant learning rate for the remainder of training .",0
6680,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and << then >> [[ maintain ]] a constant learning rate for the remainder of training .",0
6681,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a [[ constant learning rate ]] for the remainder of training .",0
6682,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the [[ remainder of training ]] .",0
6683,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6684,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6685,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6686,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and << then >> maintain a constant learning rate for the remainder of training .",0
6687,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and << then >> [[ maintain ]] a constant learning rate for the remainder of training .",0
6688,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a [[ constant learning rate ]] for the remainder of training .",0
6689,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the [[ remainder of training ]] .",0
6690,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from [[ 0.0 ]] to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6691,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to [[ 0.001 ]] in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6692,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the [[ first 1000 steps ]] , and << then >> maintain a constant learning rate for the remainder of training .",0
6693,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and << then >> [[ maintain ]] a constant learning rate for the remainder of training .",1
6694,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a [[ constant learning rate ]] for the remainder of training .",0
6695,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the [[ remainder of training ]] .",0
6696,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to [[ 0.001 ]] in the first 1000 steps , and << then >> maintain a constant learning rate for the remainder of training .",0
6697,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the [[ first 1000 steps ]] , and << then >> maintain a constant learning rate for the remainder of training .",0
6698,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and << then >> [[ maintain ]] a constant learning rate for the remainder of training .",0
6699,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and << then >> maintain a [[ constant learning rate ]] for the remainder of training .",0
6700,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and << then >> maintain a constant learning rate for the [[ remainder of training ]] .",0
6701,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the [[ first 1000 steps ]] , and << then >> maintain a constant learning rate for the remainder of training .",0
6702,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and << then >> [[ maintain ]] a constant learning rate for the remainder of training .",0
6703,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and << then >> maintain a [[ constant learning rate ]] for the remainder of training .",0
6704,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and << then >> maintain a constant learning rate for the [[ remainder of training ]] .",0
6705,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and << then >> [[ maintain ]] a constant learning rate for the remainder of training .",0
6706,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and << then >> maintain a [[ constant learning rate ]] for the remainder of training .",0
6707,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and << then >> maintain a constant learning rate for the [[ remainder of training ]] .",0
6708,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and << then >> [[ maintain ]] a [[ constant learning rate ]] for the remainder of training .",0
6709,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and << then >> [[ maintain ]] a constant learning rate for the [[ remainder of training ]] .",0
6710,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and << then >> maintain a [[ constant learning rate ]] for the [[ remainder of training ]] .",0
6711,878,"We use a [[ learning rate warm - up scheme ]] with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6712,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6713,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6714,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6715,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate << for >> the remainder of training .",0
6716,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate << for >> the remainder of training .",0
6717,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] << for >> the remainder of training .",0
6718,878,"We use a [[ learning rate warm - up scheme ]] with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the [[ remainder of training ]] .",0
6719,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6720,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6721,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6722,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate << for >> the remainder of training .",0
6723,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate << for >> the remainder of training .",0
6724,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] << for >> the remainder of training .",0
6725,878,"We use a learning rate warm - up scheme with an [[ inverse exponential ]] increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the [[ remainder of training ]] .",0
6726,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6727,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6728,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate << for >> the remainder of training .",0
6729,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate << for >> the remainder of training .",0
6730,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] << for >> the remainder of training .",0
6731,878,"We use a learning rate warm - up scheme with an inverse exponential [[ increase ]] from 0.0 to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the [[ remainder of training ]] .",0
6732,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate << for >> the remainder of training .",0
6733,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate << for >> the remainder of training .",0
6734,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate << for >> the remainder of training .",0
6735,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] << for >> the remainder of training .",0
6736,878,"We use a learning rate warm - up scheme with an inverse exponential increase from [[ 0.0 ]] to 0.001 in the first 1000 steps , and then maintain a constant learning rate << for >> the [[ remainder of training ]] .",0
6737,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the [[ first 1000 steps ]] , and then maintain a constant learning rate << for >> the remainder of training .",0
6738,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then [[ maintain ]] a constant learning rate << for >> the remainder of training .",0
6739,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a [[ constant learning rate ]] << for >> the remainder of training .",0
6740,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to [[ 0.001 ]] in the first 1000 steps , and then maintain a constant learning rate << for >> the [[ remainder of training ]] .",0
6741,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then [[ maintain ]] a constant learning rate << for >> the remainder of training .",0
6742,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a [[ constant learning rate ]] << for >> the remainder of training .",0
6743,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the [[ first 1000 steps ]] , and then maintain a constant learning rate << for >> the [[ remainder of training ]] .",0
6744,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a [[ constant learning rate ]] << for >> the remainder of training .",0
6745,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then [[ maintain ]] a constant learning rate << for >> the [[ remainder of training ]] .",0
6746,878,"We use a learning rate warm - up scheme with an inverse exponential increase from 0.0 to 0.001 in the first 1000 steps , and then maintain a [[ constant learning rate ]] << for >> the [[ remainder of training ]] .",1
6747,1455,"<< At >> [[ each epoch ]] , we divide the [[ learning rate ]] by 5 if the dev accuracy decreases .",0
6748,1455,"<< At >> [[ each epoch ]] , we divide the learning rate by [[ 5 ]] if the dev accuracy decreases .",0
6749,1455,"<< At >> [[ each epoch ]] , we divide the learning rate by 5 if the [[ dev accuracy ]] decreases .",0
6750,1455,"<< At >> [[ each epoch ]] , we divide the learning rate by 5 if the dev accuracy [[ decreases ]] .",0
6751,1455,"<< At >> each epoch , we divide the [[ learning rate ]] by [[ 5 ]] if the dev accuracy decreases .",0
6752,1455,"<< At >> each epoch , we divide the [[ learning rate ]] by 5 if the [[ dev accuracy ]] decreases .",0
6753,1455,"<< At >> each epoch , we divide the [[ learning rate ]] by 5 if the dev accuracy [[ decreases ]] .",0
6754,1455,"<< At >> each epoch , we divide the learning rate by [[ 5 ]] if the [[ dev accuracy ]] decreases .",0
6755,1455,"<< At >> each epoch , we divide the learning rate by [[ 5 ]] if the dev accuracy [[ decreases ]] .",0
6756,1455,"<< At >> each epoch , we divide the learning rate by 5 if the [[ dev accuracy ]] [[ decreases ]] .",0
6757,1455,"At [[ each epoch ]] , we << divide >> the [[ learning rate ]] by 5 if the dev accuracy decreases .",1
6758,1455,"At [[ each epoch ]] , we << divide >> the learning rate by [[ 5 ]] if the dev accuracy decreases .",0
6759,1455,"At [[ each epoch ]] , we << divide >> the learning rate by 5 if the [[ dev accuracy ]] decreases .",0
6760,1455,"At [[ each epoch ]] , we << divide >> the learning rate by 5 if the dev accuracy [[ decreases ]] .",0
6761,1455,"At each epoch , we << divide >> the [[ learning rate ]] by [[ 5 ]] if the dev accuracy decreases .",0
6762,1455,"At each epoch , we << divide >> the [[ learning rate ]] by 5 if the [[ dev accuracy ]] decreases .",0
6763,1455,"At each epoch , we << divide >> the [[ learning rate ]] by 5 if the dev accuracy [[ decreases ]] .",0
6764,1455,"At each epoch , we << divide >> the learning rate by [[ 5 ]] if the [[ dev accuracy ]] decreases .",0
6765,1455,"At each epoch , we << divide >> the learning rate by [[ 5 ]] if the dev accuracy [[ decreases ]] .",0
6766,1455,"At each epoch , we << divide >> the learning rate by 5 if the [[ dev accuracy ]] [[ decreases ]] .",0
6767,1455,"At [[ each epoch ]] , we divide the [[ learning rate ]] << by >> 5 if the dev accuracy decreases .",0
6768,1455,"At [[ each epoch ]] , we divide the learning rate << by >> [[ 5 ]] if the dev accuracy decreases .",0
6769,1455,"At [[ each epoch ]] , we divide the learning rate << by >> 5 if the [[ dev accuracy ]] decreases .",0
6770,1455,"At [[ each epoch ]] , we divide the learning rate << by >> 5 if the dev accuracy [[ decreases ]] .",0
6771,1455,"At each epoch , we divide the [[ learning rate ]] << by >> [[ 5 ]] if the dev accuracy decreases .",1
6772,1455,"At each epoch , we divide the [[ learning rate ]] << by >> 5 if the [[ dev accuracy ]] decreases .",0
6773,1455,"At each epoch , we divide the [[ learning rate ]] << by >> 5 if the dev accuracy [[ decreases ]] .",0
6774,1455,"At each epoch , we divide the learning rate << by >> [[ 5 ]] if the [[ dev accuracy ]] decreases .",0
6775,1455,"At each epoch , we divide the learning rate << by >> [[ 5 ]] if the dev accuracy [[ decreases ]] .",0
6776,1455,"At each epoch , we divide the learning rate << by >> 5 if the [[ dev accuracy ]] [[ decreases ]] .",0
6777,1455,"At [[ each epoch ]] , we divide the [[ learning rate ]] by 5 << if >> the dev accuracy decreases .",0
6778,1455,"At [[ each epoch ]] , we divide the learning rate by [[ 5 ]] << if >> the dev accuracy decreases .",0
6779,1455,"At [[ each epoch ]] , we divide the learning rate by 5 << if >> the [[ dev accuracy ]] decreases .",0
6780,1455,"At [[ each epoch ]] , we divide the learning rate by 5 << if >> the dev accuracy [[ decreases ]] .",0
6781,1455,"At each epoch , we divide the [[ learning rate ]] by [[ 5 ]] << if >> the dev accuracy decreases .",0
6782,1455,"At each epoch , we divide the [[ learning rate ]] by 5 << if >> the [[ dev accuracy ]] decreases .",1
6783,1455,"At each epoch , we divide the [[ learning rate ]] by 5 << if >> the dev accuracy [[ decreases ]] .",0
6784,1455,"At each epoch , we divide the learning rate by [[ 5 ]] << if >> the [[ dev accuracy ]] decreases .",0
6785,1455,"At each epoch , we divide the learning rate by [[ 5 ]] << if >> the dev accuracy [[ decreases ]] .",0
6786,1455,"At each epoch , we divide the learning rate by 5 << if >> the [[ dev accuracy ]] [[ decreases ]] .",0
6787,4182,An [[ annealing approach ]] << halves >> the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,1
6788,4182,An [[ annealing approach ]] << halves >> the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6789,4182,An [[ annealing approach ]] << halves >> the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6790,4182,An [[ annealing approach ]] << halves >> the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6791,4182,An [[ annealing approach ]] << halves >> the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6792,4182,An [[ annealing approach ]] << halves >> the lr every 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6793,4182,An [[ annealing approach ]] << halves >> the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6794,4182,An annealing approach << halves >> the [[ lr ]] every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6795,4182,An annealing approach << halves >> the [[ lr ]] every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6796,4182,An annealing approach << halves >> the [[ lr ]] every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6797,4182,An annealing approach << halves >> the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6798,4182,An annealing approach << halves >> the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6799,4182,An annealing approach << halves >> the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6800,4182,An annealing approach << halves >> the lr every [[ 20 epochs ]] and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6801,4182,An annealing approach << halves >> the lr every [[ 20 epochs ]] and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6802,4182,An annealing approach << halves >> the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6803,4182,An annealing approach << halves >> the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6804,4182,An annealing approach << halves >> the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6805,4182,An annealing approach << halves >> the lr every 20 epochs and [[ termination ]] is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6806,4182,An annealing approach << halves >> the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6807,4182,An annealing approach << halves >> the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6808,4182,An annealing approach << halves >> the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6809,4182,An annealing approach << halves >> the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a [[ patience ]] of 12 by monitoring the validation loss .,0
6810,4182,An annealing approach << halves >> the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of [[ 12 ]] by monitoring the validation loss .,0
6811,4182,An annealing approach << halves >> the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the [[ validation loss ]] .,0
6812,4182,An annealing approach << halves >> the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of [[ 12 ]] by monitoring the validation loss .,0
6813,4182,An annealing approach << halves >> the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the [[ validation loss ]] .,0
6814,4182,An annealing approach << halves >> the lr every 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the [[ validation loss ]] .,0
6815,4182,An [[ annealing approach ]] halves the [[ lr ]] << every >> 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6816,4182,An [[ annealing approach ]] halves the lr << every >> [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6817,4182,An [[ annealing approach ]] halves the lr << every >> 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6818,4182,An [[ annealing approach ]] halves the lr << every >> 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6819,4182,An [[ annealing approach ]] halves the lr << every >> 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6820,4182,An [[ annealing approach ]] halves the lr << every >> 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6821,4182,An [[ annealing approach ]] halves the lr << every >> 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6822,4182,An annealing approach halves the [[ lr ]] << every >> [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,1
6823,4182,An annealing approach halves the [[ lr ]] << every >> 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6824,4182,An annealing approach halves the [[ lr ]] << every >> 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6825,4182,An annealing approach halves the [[ lr ]] << every >> 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6826,4182,An annealing approach halves the [[ lr ]] << every >> 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6827,4182,An annealing approach halves the [[ lr ]] << every >> 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6828,4182,An annealing approach halves the lr << every >> [[ 20 epochs ]] and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6829,4182,An annealing approach halves the lr << every >> [[ 20 epochs ]] and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6830,4182,An annealing approach halves the lr << every >> [[ 20 epochs ]] and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6831,4182,An annealing approach halves the lr << every >> [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6832,4182,An annealing approach halves the lr << every >> [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6833,4182,An annealing approach halves the lr << every >> 20 epochs and [[ termination ]] is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6834,4182,An annealing approach halves the lr << every >> 20 epochs and [[ termination ]] is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6835,4182,An annealing approach halves the lr << every >> 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6836,4182,An annealing approach halves the lr << every >> 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6837,4182,An annealing approach halves the lr << every >> 20 epochs and termination is decided using an [[ early - stop measure ]] with a [[ patience ]] of 12 by monitoring the validation loss .,0
6838,4182,An annealing approach halves the lr << every >> 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of [[ 12 ]] by monitoring the validation loss .,0
6839,4182,An annealing approach halves the lr << every >> 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 by monitoring the [[ validation loss ]] .,0
6840,4182,An annealing approach halves the lr << every >> 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of [[ 12 ]] by monitoring the validation loss .,0
6841,4182,An annealing approach halves the lr << every >> 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 by monitoring the [[ validation loss ]] .,0
6842,4182,An annealing approach halves the lr << every >> 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] by monitoring the [[ validation loss ]] .,0
6843,4182,An [[ annealing approach ]] halves the [[ lr ]] every 20 epochs and termination is << decided using >> an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6844,4182,An [[ annealing approach ]] halves the lr every [[ 20 epochs ]] and termination is << decided using >> an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6845,4182,An [[ annealing approach ]] halves the lr every 20 epochs and [[ termination ]] is << decided using >> an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6846,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is << decided using >> an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6847,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is << decided using >> an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6848,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is << decided using >> an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6849,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is << decided using >> an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6850,4182,An annealing approach halves the [[ lr ]] every [[ 20 epochs ]] and termination is << decided using >> an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6851,4182,An annealing approach halves the [[ lr ]] every 20 epochs and [[ termination ]] is << decided using >> an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6852,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is << decided using >> an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6853,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is << decided using >> an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6854,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is << decided using >> an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6855,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is << decided using >> an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6856,4182,An annealing approach halves the lr every [[ 20 epochs ]] and [[ termination ]] is << decided using >> an early - stop measure with a patience of 12 by monitoring the validation loss .,0
6857,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is << decided using >> an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,0
6858,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is << decided using >> an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6859,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is << decided using >> an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6860,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is << decided using >> an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6861,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is << decided using >> an [[ early - stop measure ]] with a patience of 12 by monitoring the validation loss .,1
6862,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is << decided using >> an early - stop measure with a [[ patience ]] of 12 by monitoring the validation loss .,0
6863,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is << decided using >> an early - stop measure with a patience of [[ 12 ]] by monitoring the validation loss .,0
6864,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is << decided using >> an early - stop measure with a patience of 12 by monitoring the [[ validation loss ]] .,0
6865,4182,An annealing approach halves the lr every 20 epochs and termination is << decided using >> an [[ early - stop measure ]] with a [[ patience ]] of 12 by monitoring the validation loss .,0
6866,4182,An annealing approach halves the lr every 20 epochs and termination is << decided using >> an [[ early - stop measure ]] with a patience of [[ 12 ]] by monitoring the validation loss .,0
6867,4182,An annealing approach halves the lr every 20 epochs and termination is << decided using >> an [[ early - stop measure ]] with a patience of 12 by monitoring the [[ validation loss ]] .,0
6868,4182,An annealing approach halves the lr every 20 epochs and termination is << decided using >> an early - stop measure with a [[ patience ]] of [[ 12 ]] by monitoring the validation loss .,0
6869,4182,An annealing approach halves the lr every 20 epochs and termination is << decided using >> an early - stop measure with a [[ patience ]] of 12 by monitoring the [[ validation loss ]] .,0
6870,4182,An annealing approach halves the lr every 20 epochs and termination is << decided using >> an early - stop measure with a patience of [[ 12 ]] by monitoring the [[ validation loss ]] .,0
6871,4182,An [[ annealing approach ]] halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure << with >> a patience of 12 by monitoring the validation loss .,0
6872,4182,An [[ annealing approach ]] halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure << with >> a patience of 12 by monitoring the validation loss .,0
6873,4182,An [[ annealing approach ]] halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure << with >> a patience of 12 by monitoring the validation loss .,0
6874,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] << with >> a patience of 12 by monitoring the validation loss .,0
6875,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure << with >> a [[ patience ]] of 12 by monitoring the validation loss .,0
6876,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure << with >> a patience of [[ 12 ]] by monitoring the validation loss .,0
6877,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure << with >> a patience of 12 by monitoring the [[ validation loss ]] .,0
6878,4182,An annealing approach halves the [[ lr ]] every [[ 20 epochs ]] and termination is decided using an early - stop measure << with >> a patience of 12 by monitoring the validation loss .,0
6879,4182,An annealing approach halves the [[ lr ]] every 20 epochs and [[ termination ]] is decided using an early - stop measure << with >> a patience of 12 by monitoring the validation loss .,0
6880,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an [[ early - stop measure ]] << with >> a patience of 12 by monitoring the validation loss .,0
6881,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure << with >> a [[ patience ]] of 12 by monitoring the validation loss .,0
6882,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure << with >> a patience of [[ 12 ]] by monitoring the validation loss .,0
6883,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure << with >> a patience of 12 by monitoring the [[ validation loss ]] .,0
6884,4182,An annealing approach halves the lr every [[ 20 epochs ]] and [[ termination ]] is decided using an early - stop measure << with >> a patience of 12 by monitoring the validation loss .,0
6885,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an [[ early - stop measure ]] << with >> a patience of 12 by monitoring the validation loss .,0
6886,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure << with >> a [[ patience ]] of 12 by monitoring the validation loss .,0
6887,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure << with >> a patience of [[ 12 ]] by monitoring the validation loss .,0
6888,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure << with >> a patience of 12 by monitoring the [[ validation loss ]] .,0
6889,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an [[ early - stop measure ]] << with >> a patience of 12 by monitoring the validation loss .,0
6890,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure << with >> a [[ patience ]] of 12 by monitoring the validation loss .,0
6891,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure << with >> a patience of [[ 12 ]] by monitoring the validation loss .,0
6892,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure << with >> a patience of 12 by monitoring the [[ validation loss ]] .,0
6893,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] << with >> a [[ patience ]] of 12 by monitoring the validation loss .,1
6894,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] << with >> a patience of [[ 12 ]] by monitoring the validation loss .,0
6895,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] << with >> a patience of 12 by monitoring the [[ validation loss ]] .,0
6896,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure << with >> a [[ patience ]] of [[ 12 ]] by monitoring the validation loss .,0
6897,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure << with >> a [[ patience ]] of 12 by monitoring the [[ validation loss ]] .,0
6898,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure << with >> a patience of [[ 12 ]] by monitoring the [[ validation loss ]] .,0
6899,4182,An [[ annealing approach ]] halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience << of >> 12 by monitoring the validation loss .,0
6900,4182,An [[ annealing approach ]] halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience << of >> 12 by monitoring the validation loss .,0
6901,4182,An [[ annealing approach ]] halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience << of >> 12 by monitoring the validation loss .,0
6902,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience << of >> 12 by monitoring the validation loss .,0
6903,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] << of >> 12 by monitoring the validation loss .,0
6904,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience << of >> [[ 12 ]] by monitoring the validation loss .,0
6905,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience << of >> 12 by monitoring the [[ validation loss ]] .,0
6906,4182,An annealing approach halves the [[ lr ]] every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience << of >> 12 by monitoring the validation loss .,0
6907,4182,An annealing approach halves the [[ lr ]] every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience << of >> 12 by monitoring the validation loss .,0
6908,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience << of >> 12 by monitoring the validation loss .,0
6909,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] << of >> 12 by monitoring the validation loss .,0
6910,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience << of >> [[ 12 ]] by monitoring the validation loss .,0
6911,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience << of >> 12 by monitoring the [[ validation loss ]] .,0
6912,4182,An annealing approach halves the lr every [[ 20 epochs ]] and [[ termination ]] is decided using an early - stop measure with a patience << of >> 12 by monitoring the validation loss .,0
6913,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an [[ early - stop measure ]] with a patience << of >> 12 by monitoring the validation loss .,0
6914,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a [[ patience ]] << of >> 12 by monitoring the validation loss .,0
6915,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience << of >> [[ 12 ]] by monitoring the validation loss .,0
6916,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience << of >> 12 by monitoring the [[ validation loss ]] .,0
6917,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an [[ early - stop measure ]] with a patience << of >> 12 by monitoring the validation loss .,0
6918,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a [[ patience ]] << of >> 12 by monitoring the validation loss .,0
6919,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience << of >> [[ 12 ]] by monitoring the validation loss .,0
6920,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience << of >> 12 by monitoring the [[ validation loss ]] .,0
6921,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a [[ patience ]] << of >> 12 by monitoring the validation loss .,0
6922,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience << of >> [[ 12 ]] by monitoring the validation loss .,0
6923,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience << of >> 12 by monitoring the [[ validation loss ]] .,0
6924,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] << of >> [[ 12 ]] by monitoring the validation loss .,1
6925,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] << of >> 12 by monitoring the [[ validation loss ]] .,0
6926,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience << of >> [[ 12 ]] by monitoring the [[ validation loss ]] .,0
6927,4182,An [[ annealing approach ]] halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience of 12 << by monitoring >> the validation loss .,0
6928,4182,An [[ annealing approach ]] halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 << by monitoring >> the validation loss .,0
6929,4182,An [[ annealing approach ]] halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 << by monitoring >> the validation loss .,0
6930,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 << by monitoring >> the validation loss .,0
6931,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 << by monitoring >> the validation loss .,0
6932,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] << by monitoring >> the validation loss .,0
6933,4182,An [[ annealing approach ]] halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 << by monitoring >> the [[ validation loss ]] .,0
6934,4182,An annealing approach halves the [[ lr ]] every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 << by monitoring >> the validation loss .,0
6935,4182,An annealing approach halves the [[ lr ]] every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 << by monitoring >> the validation loss .,0
6936,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 << by monitoring >> the validation loss .,0
6937,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 << by monitoring >> the validation loss .,0
6938,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] << by monitoring >> the validation loss .,0
6939,4182,An annealing approach halves the [[ lr ]] every 20 epochs and termination is decided using an early - stop measure with a patience of 12 << by monitoring >> the [[ validation loss ]] .,0
6940,4182,An annealing approach halves the lr every [[ 20 epochs ]] and [[ termination ]] is decided using an early - stop measure with a patience of 12 << by monitoring >> the validation loss .,0
6941,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an [[ early - stop measure ]] with a patience of 12 << by monitoring >> the validation loss .,0
6942,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a [[ patience ]] of 12 << by monitoring >> the validation loss .,0
6943,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of [[ 12 ]] << by monitoring >> the validation loss .,0
6944,4182,An annealing approach halves the lr every [[ 20 epochs ]] and termination is decided using an early - stop measure with a patience of 12 << by monitoring >> the [[ validation loss ]] .,0
6945,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an [[ early - stop measure ]] with a patience of 12 << by monitoring >> the validation loss .,0
6946,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a [[ patience ]] of 12 << by monitoring >> the validation loss .,0
6947,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of [[ 12 ]] << by monitoring >> the validation loss .,0
6948,4182,An annealing approach halves the lr every 20 epochs and [[ termination ]] is decided using an early - stop measure with a patience of 12 << by monitoring >> the [[ validation loss ]] .,0
6949,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a [[ patience ]] of 12 << by monitoring >> the validation loss .,0
6950,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of [[ 12 ]] << by monitoring >> the validation loss .,0
6951,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an [[ early - stop measure ]] with a patience of 12 << by monitoring >> the [[ validation loss ]] .,0
6952,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of [[ 12 ]] << by monitoring >> the validation loss .,0
6953,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a [[ patience ]] of 12 << by monitoring >> the [[ validation loss ]] .,0
6954,4182,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of [[ 12 ]] << by monitoring >> the [[ validation loss ]] .,1
6955,2283,"Finally , the [[ results ]] << of >> these [[ subproblems ]] are merged to produce the final classification .",1
6956,2283,"Finally , the [[ results ]] << of >> these subproblems are [[ merged ]] to produce the final classification .",0
6957,2283,"Finally , the [[ results ]] << of >> these subproblems are merged to produce the [[ final classification ]] .",0
6958,2283,"Finally , the results << of >> these [[ subproblems ]] are [[ merged ]] to produce the final classification .",0
6959,2283,"Finally , the results << of >> these [[ subproblems ]] are merged to produce the [[ final classification ]] .",0
6960,2283,"Finally , the results << of >> these subproblems are [[ merged ]] to produce the [[ final classification ]] .",0
6961,2283,"Finally , the [[ results ]] of these [[ subproblems ]] are merged << to produce >> the final classification .",0
6962,2283,"Finally , the [[ results ]] of these subproblems are [[ merged ]] << to produce >> the final classification .",0
6963,2283,"Finally , the [[ results ]] of these subproblems are merged << to produce >> the [[ final classification ]] .",0
6964,2283,"Finally , the results of these [[ subproblems ]] are [[ merged ]] << to produce >> the final classification .",0
6965,2283,"Finally , the results of these [[ subproblems ]] are merged << to produce >> the [[ final classification ]] .",0
6966,2283,"Finally , the results of these subproblems are [[ merged ]] << to produce >> the [[ final classification ]] .",1
6967,2714,"We << use >> a [[ batch size ]] of [[ B = 32 ]] , where for a batch of image - caption pairs each image ( caption ) is only related to one caption ( image ) .",0
6968,2714,"We use a [[ batch size ]] << of >> [[ B = 32 ]] , where for a batch of image - caption pairs each image ( caption ) is only related to one caption ( image ) .",1
6969,5371,"In , << on >> [[ three out of the four datasets ]] , [[ oh - 2 LSTMp ]] outperforms SVM and the CNN .",0
6970,5371,"In , << on >> [[ three out of the four datasets ]] , oh - 2 LSTMp outperforms [[ SVM and the CNN ]] .",0
6971,5371,"In , << on >> three out of the four datasets , [[ oh - 2 LSTMp ]] outperforms [[ SVM and the CNN ]] .",0
6972,5371,"In , on [[ three out of the four datasets ]] , [[ oh - 2 LSTMp ]] << outperforms >> SVM and the CNN .",0
6973,5371,"In , on [[ three out of the four datasets ]] , oh - 2 LSTMp << outperforms >> [[ SVM and the CNN ]] .",0
6974,5371,"In , on three out of the four datasets , [[ oh - 2 LSTMp ]] << outperforms >> [[ SVM and the CNN ]] .",1
6975,5619,"As shown in , as long as the synthesizer was << trained on >> a sufficiently large set of speakers , i.e. on [[ LibriSpeech ]] , the [[ synthesized speech ]] is typically most similar to the ground truth voices .",0
6976,5619,"As shown in , as long as the synthesizer was << trained on >> a sufficiently large set of speakers , i.e. on [[ LibriSpeech ]] , the synthesized speech is typically most similar to the [[ ground truth voices ]] .",0
6977,5619,"As shown in , as long as the synthesizer was << trained on >> a sufficiently large set of speakers , i.e. on LibriSpeech , the [[ synthesized speech ]] is typically most similar to the [[ ground truth voices ]] .",0
6978,5619,"As shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on [[ LibriSpeech ]] , the [[ synthesized speech ]] is typically << most similar to >> the ground truth voices .",0
6979,5619,"As shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on [[ LibriSpeech ]] , the synthesized speech is typically << most similar to >> the [[ ground truth voices ]] .",0
6980,5619,"As shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on LibriSpeech , the [[ synthesized speech ]] is typically << most similar to >> the [[ ground truth voices ]] .",1
6981,1295,"For the RC tasks , [[ questions ]] << maybe >> [[ answered ]] using just the summaries or the full story text .",1
6982,1295,"For the RC tasks , [[ questions ]] << maybe >> answered using just the [[ summaries ]] or the full story text .",0
6983,1295,"For the RC tasks , [[ questions ]] << maybe >> answered using just the summaries or the [[ full story text ]] .",0
6984,1295,"For the RC tasks , questions << maybe >> [[ answered ]] using just the [[ summaries ]] or the full story text .",0
6985,1295,"For the RC tasks , questions << maybe >> [[ answered ]] using just the summaries or the [[ full story text ]] .",0
6986,1295,"For the RC tasks , questions << maybe >> answered using just the [[ summaries ]] or the [[ full story text ]] .",0
6987,1295,"For the RC tasks , [[ questions ]] maybe [[ answered ]] << using >> just the summaries or the full story text .",0
6988,1295,"For the RC tasks , [[ questions ]] maybe answered << using >> just the [[ summaries ]] or the full story text .",0
6989,1295,"For the RC tasks , [[ questions ]] maybe answered << using >> just the summaries or the [[ full story text ]] .",0
6990,1295,"For the RC tasks , questions maybe [[ answered ]] << using >> just the [[ summaries ]] or the full story text .",1
6991,1295,"For the RC tasks , questions maybe [[ answered ]] << using >> just the summaries or the [[ full story text ]] .",1
6992,1295,"For the RC tasks , questions maybe answered << using >> just the [[ summaries ]] or the [[ full story text ]] .",0
6993,2092,Our model captures this by << evaluating if >> the [[ sentiment ]] described in an [[ ending option ]] makes sense considering the context of the story .,0
6994,2092,Our model captures this by << evaluating if >> the [[ sentiment ]] described in an ending option [[ makes sense ]] considering the context of the story .,0
6995,2092,Our model captures this by << evaluating if >> the [[ sentiment ]] described in an ending option makes sense considering the [[ context ]] of the story .,0
6996,2092,Our model captures this by << evaluating if >> the [[ sentiment ]] described in an ending option makes sense considering the context of the [[ story ]] .,0
6997,2092,Our model captures this by << evaluating if >> the sentiment described in an [[ ending option ]] [[ makes sense ]] considering the context of the story .,0
6998,2092,Our model captures this by << evaluating if >> the sentiment described in an [[ ending option ]] makes sense considering the [[ context ]] of the story .,0
6999,2092,Our model captures this by << evaluating if >> the sentiment described in an [[ ending option ]] makes sense considering the context of the [[ story ]] .,0
7000,2092,Our model captures this by << evaluating if >> the sentiment described in an ending option [[ makes sense ]] considering the [[ context ]] of the story .,0
7001,2092,Our model captures this by << evaluating if >> the sentiment described in an ending option [[ makes sense ]] considering the context of the [[ story ]] .,0
7002,2092,Our model captures this by << evaluating if >> the sentiment described in an ending option makes sense considering the [[ context ]] of the [[ story ]] .,0
7003,2092,Our model captures this by evaluating if the [[ sentiment ]] << described in >> an [[ ending option ]] makes sense considering the context of the story .,1
7004,2092,Our model captures this by evaluating if the [[ sentiment ]] << described in >> an ending option [[ makes sense ]] considering the context of the story .,0
7005,2092,Our model captures this by evaluating if the [[ sentiment ]] << described in >> an ending option makes sense considering the [[ context ]] of the story .,0
7006,2092,Our model captures this by evaluating if the [[ sentiment ]] << described in >> an ending option makes sense considering the context of the [[ story ]] .,0
7007,2092,Our model captures this by evaluating if the sentiment << described in >> an [[ ending option ]] [[ makes sense ]] considering the context of the story .,0
7008,2092,Our model captures this by evaluating if the sentiment << described in >> an [[ ending option ]] makes sense considering the [[ context ]] of the story .,0
7009,2092,Our model captures this by evaluating if the sentiment << described in >> an [[ ending option ]] makes sense considering the context of the [[ story ]] .,0
7010,2092,Our model captures this by evaluating if the sentiment << described in >> an ending option [[ makes sense ]] considering the [[ context ]] of the story .,0
7011,2092,Our model captures this by evaluating if the sentiment << described in >> an ending option [[ makes sense ]] considering the context of the [[ story ]] .,0
7012,2092,Our model captures this by evaluating if the sentiment << described in >> an ending option makes sense considering the [[ context ]] of the [[ story ]] .,0
7013,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an [[ ending option ]] makes sense << considering >> the context of the story .,0
7014,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an ending option [[ makes sense ]] << considering >> the context of the story .,0
7015,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an ending option makes sense << considering >> the [[ context ]] of the story .,0
7016,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an ending option makes sense << considering >> the context of the [[ story ]] .,0
7017,2092,Our model captures this by evaluating if the sentiment described in an [[ ending option ]] [[ makes sense ]] << considering >> the context of the story .,0
7018,2092,Our model captures this by evaluating if the sentiment described in an [[ ending option ]] makes sense << considering >> the [[ context ]] of the story .,0
7019,2092,Our model captures this by evaluating if the sentiment described in an [[ ending option ]] makes sense << considering >> the context of the [[ story ]] .,0
7020,2092,Our model captures this by evaluating if the sentiment described in an ending option [[ makes sense ]] << considering >> the [[ context ]] of the story .,1
7021,2092,Our model captures this by evaluating if the sentiment described in an ending option [[ makes sense ]] << considering >> the context of the [[ story ]] .,0
7022,2092,Our model captures this by evaluating if the sentiment described in an ending option makes sense << considering >> the [[ context ]] of the [[ story ]] .,0
7023,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an [[ ending option ]] makes sense considering the context << of >> the story .,0
7024,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an ending option [[ makes sense ]] considering the context << of >> the story .,0
7025,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an ending option makes sense considering the [[ context ]] << of >> the story .,0
7026,2092,Our model captures this by evaluating if the [[ sentiment ]] described in an ending option makes sense considering the context << of >> the [[ story ]] .,0
7027,2092,Our model captures this by evaluating if the sentiment described in an [[ ending option ]] [[ makes sense ]] considering the context << of >> the story .,0
7028,2092,Our model captures this by evaluating if the sentiment described in an [[ ending option ]] makes sense considering the [[ context ]] << of >> the story .,0
7029,2092,Our model captures this by evaluating if the sentiment described in an [[ ending option ]] makes sense considering the context << of >> the [[ story ]] .,0
7030,2092,Our model captures this by evaluating if the sentiment described in an ending option [[ makes sense ]] considering the [[ context ]] << of >> the story .,0
7031,2092,Our model captures this by evaluating if the sentiment described in an ending option [[ makes sense ]] considering the context << of >> the [[ story ]] .,0
7032,2092,Our model captures this by evaluating if the sentiment described in an ending option makes sense considering the [[ context ]] << of >> the [[ story ]] .,1
7033,3050,We << implement >> [[ FVTA network ]] for [[ Movie QA task ]] with modality number of 2 ( video & text ) .,0
7034,3050,We << implement >> [[ FVTA network ]] for Movie QA task with [[ modality number ]] of 2 ( video & text ) .,0
7035,3050,We << implement >> [[ FVTA network ]] for Movie QA task with modality number of [[ 2 ( video & text ) ]] .,0
7036,3050,We << implement >> FVTA network for [[ Movie QA task ]] with [[ modality number ]] of 2 ( video & text ) .,0
7037,3050,We << implement >> FVTA network for [[ Movie QA task ]] with modality number of [[ 2 ( video & text ) ]] .,0
7038,3050,We << implement >> FVTA network for Movie QA task with [[ modality number ]] of [[ 2 ( video & text ) ]] .,0
7039,3050,We implement [[ FVTA network ]] << for >> [[ Movie QA task ]] with modality number of 2 ( video & text ) .,1
7040,3050,We implement [[ FVTA network ]] << for >> Movie QA task with [[ modality number ]] of 2 ( video & text ) .,0
7041,3050,We implement [[ FVTA network ]] << for >> Movie QA task with modality number of [[ 2 ( video & text ) ]] .,0
7042,3050,We implement FVTA network << for >> [[ Movie QA task ]] with [[ modality number ]] of 2 ( video & text ) .,0
7043,3050,We implement FVTA network << for >> [[ Movie QA task ]] with modality number of [[ 2 ( video & text ) ]] .,0
7044,3050,We implement FVTA network << for >> Movie QA task with [[ modality number ]] of [[ 2 ( video & text ) ]] .,0
7045,1137,"We use the [[ AdaDelta ( Zeiler , 2012 ) optimizer ]] << with >> a [[ initial learning rate ]] as 0.001 .",1
7046,1137,"We use the [[ AdaDelta ( Zeiler , 2012 ) optimizer ]] << with >> a initial learning rate as [[ 0.001 ]] .",0
7047,1137,"We use the AdaDelta ( Zeiler , 2012 ) optimizer << with >> a [[ initial learning rate ]] as [[ 0.001 ]] .",0
7048,1137,"We use the [[ AdaDelta ( Zeiler , 2012 ) optimizer ]] with a [[ initial learning rate ]] << as >> 0.001 .",0
7049,1137,"We use the [[ AdaDelta ( Zeiler , 2012 ) optimizer ]] with a initial learning rate << as >> [[ 0.001 ]] .",0
7050,1137,"We use the AdaDelta ( Zeiler , 2012 ) optimizer with a [[ initial learning rate ]] << as >> [[ 0.001 ]] .",1
7051,2072,[[ Both the coarse - grain module and the fine - grain module ]] significantly << contribute to >> [[ model performance ]] .,1
7052,5331,The [[ document ]] is also << annotated with >> [[ causal relations ( CLINKs ) ]] between event pairs .,1
7053,5331,The [[ document ]] is also << annotated with >> causal relations ( CLINKs ) between [[ event pairs ]] .,0
7054,5331,The document is also << annotated with >> [[ causal relations ( CLINKs ) ]] between [[ event pairs ]] .,0
7055,5331,The [[ document ]] is also annotated with [[ causal relations ( CLINKs ) ]] << between >> event pairs .,0
7056,5331,The [[ document ]] is also annotated with causal relations ( CLINKs ) << between >> [[ event pairs ]] .,0
7057,5331,The document is also annotated with [[ causal relations ( CLINKs ) ]] << between >> [[ event pairs ]] .,1
7058,832,"[[ SAN ]] also [[ outperforms ]] a 5 - step memory net << with >> averaging , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .",0
7059,832,"[[ SAN ]] also outperforms a [[ 5 - step memory net ]] << with >> averaging , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .",0
7060,832,"[[ SAN ]] also outperforms a 5 - step memory net << with >> [[ averaging ]] , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .",0
7061,832,"SAN also [[ outperforms ]] a [[ 5 - step memory net ]] << with >> averaging , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .",0
7062,832,"SAN also [[ outperforms ]] a 5 - step memory net << with >> [[ averaging ]] , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .",0
7063,832,"SAN also outperforms a [[ 5 - step memory net ]] << with >> [[ averaging ]] , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .",1
7064,591,The [[ learning rate ]] was << decreased by >> a [[ factor ]] of 0.85 when the dev accuracy does not improve .,1
7065,591,The [[ learning rate ]] was << decreased by >> a factor of [[ 0.85 ]] when the dev accuracy does not improve .,0
7066,591,The [[ learning rate ]] was << decreased by >> a factor of 0.85 when the [[ dev accuracy ]] does not improve .,0
7067,591,The [[ learning rate ]] was << decreased by >> a factor of 0.85 when the dev accuracy [[ does not improve ]] .,0
7068,591,The learning rate was << decreased by >> a [[ factor ]] of [[ 0.85 ]] when the dev accuracy does not improve .,0
7069,591,The learning rate was << decreased by >> a [[ factor ]] of 0.85 when the [[ dev accuracy ]] does not improve .,0
7070,591,The learning rate was << decreased by >> a [[ factor ]] of 0.85 when the dev accuracy [[ does not improve ]] .,0
7071,591,The learning rate was << decreased by >> a factor of [[ 0.85 ]] when the [[ dev accuracy ]] does not improve .,0
7072,591,The learning rate was << decreased by >> a factor of [[ 0.85 ]] when the dev accuracy [[ does not improve ]] .,0
7073,591,The learning rate was << decreased by >> a factor of 0.85 when the [[ dev accuracy ]] [[ does not improve ]] .,0
7074,591,The [[ learning rate ]] was decreased by a [[ factor ]] << of >> 0.85 when the dev accuracy does not improve .,0
7075,591,The [[ learning rate ]] was decreased by a factor << of >> [[ 0.85 ]] when the dev accuracy does not improve .,0
7076,591,The [[ learning rate ]] was decreased by a factor << of >> 0.85 when the [[ dev accuracy ]] does not improve .,0
7077,591,The [[ learning rate ]] was decreased by a factor << of >> 0.85 when the dev accuracy [[ does not improve ]] .,0
7078,591,The learning rate was decreased by a [[ factor ]] << of >> [[ 0.85 ]] when the dev accuracy does not improve .,1
7079,591,The learning rate was decreased by a [[ factor ]] << of >> 0.85 when the [[ dev accuracy ]] does not improve .,0
7080,591,The learning rate was decreased by a [[ factor ]] << of >> 0.85 when the dev accuracy [[ does not improve ]] .,0
7081,591,The learning rate was decreased by a factor << of >> [[ 0.85 ]] when the [[ dev accuracy ]] does not improve .,0
7082,591,The learning rate was decreased by a factor << of >> [[ 0.85 ]] when the dev accuracy [[ does not improve ]] .,0
7083,591,The learning rate was decreased by a factor << of >> 0.85 when the [[ dev accuracy ]] [[ does not improve ]] .,0
7084,591,The [[ learning rate ]] was decreased by a [[ factor ]] of 0.85 << when >> the dev accuracy does not improve .,0
7085,591,The [[ learning rate ]] was decreased by a factor of [[ 0.85 ]] << when >> the dev accuracy does not improve .,0
7086,591,The [[ learning rate ]] was decreased by a factor of 0.85 << when >> the [[ dev accuracy ]] does not improve .,0
7087,591,The [[ learning rate ]] was decreased by a factor of 0.85 << when >> the dev accuracy [[ does not improve ]] .,0
7088,591,The learning rate was decreased by a [[ factor ]] of [[ 0.85 ]] << when >> the dev accuracy does not improve .,0
7089,591,The learning rate was decreased by a [[ factor ]] of 0.85 << when >> the [[ dev accuracy ]] does not improve .,1
7090,591,The learning rate was decreased by a [[ factor ]] of 0.85 << when >> the dev accuracy [[ does not improve ]] .,0
7091,591,The learning rate was decreased by a factor of [[ 0.85 ]] << when >> the [[ dev accuracy ]] does not improve .,0
7092,591,The learning rate was decreased by a factor of [[ 0.85 ]] << when >> the dev accuracy [[ does not improve ]] .,0
7093,591,The learning rate was decreased by a factor of 0.85 << when >> the [[ dev accuracy ]] [[ does not improve ]] .,0
7094,4658,"In addition , we propose a [[ hierarchical self - attention mechanism ]] << allowing >> [[ KET ]] to model the hierarchical structure of conversations .",1
7095,4658,"In addition , we propose a [[ hierarchical self - attention mechanism ]] << allowing >> KET to model the [[ hierarchical structure of conversations ]] .",0
7096,4658,"In addition , we propose a hierarchical self - attention mechanism << allowing >> [[ KET ]] to model the [[ hierarchical structure of conversations ]] .",0
7097,4658,"In addition , we propose a [[ hierarchical self - attention mechanism ]] allowing [[ KET ]] << to model >> the hierarchical structure of conversations .",0
7098,4658,"In addition , we propose a [[ hierarchical self - attention mechanism ]] allowing KET << to model >> the [[ hierarchical structure of conversations ]] .",0
7099,4658,"In addition , we propose a hierarchical self - attention mechanism allowing [[ KET ]] << to model >> the [[ hierarchical structure of conversations ]] .",1
7100,5716,The [[ proposed AEM model ]] << significantly outperforms >> the [[ Seq2Seq model ]] .,1
7101,109,"Additionally , [[ two different generative adversarial networks ( GAN ) ]] , << namely >> the [[ local and global GAN ]] , are proposed to further improve the cross - language translation .",1
7102,109,"Additionally , [[ two different generative adversarial networks ( GAN ) ]] , << namely >> the local and global GAN , are proposed to further improve the [[ cross - language translation ]] .",0
7103,109,"Additionally , two different generative adversarial networks ( GAN ) , << namely >> the [[ local and global GAN ]] , are proposed to further improve the [[ cross - language translation ]] .",0
7104,109,"Additionally , [[ two different generative adversarial networks ( GAN ) ]] , namely the [[ local and global GAN ]] , << are proposed >> to further improve the cross - language translation .",0
7105,109,"Additionally , [[ two different generative adversarial networks ( GAN ) ]] , namely the local and global GAN , << are proposed >> to further improve the [[ cross - language translation ]] .",0
7106,109,"Additionally , two different generative adversarial networks ( GAN ) , namely the [[ local and global GAN ]] , << are proposed >> to further improve the [[ cross - language translation ]] .",0
7107,109,"Additionally , [[ two different generative adversarial networks ( GAN ) ]] , namely the [[ local and global GAN ]] , are proposed << to further improve >> the cross - language translation .",0
7108,109,"Additionally , [[ two different generative adversarial networks ( GAN ) ]] , namely the local and global GAN , are proposed << to further improve >> the [[ cross - language translation ]] .",1
7109,109,"Additionally , two different generative adversarial networks ( GAN ) , namely the [[ local and global GAN ]] , are proposed << to further improve >> the [[ cross - language translation ]] .",0
7110,4318,"<< For >> the [[ sentiment classification task ]] , [[ word representations ]] were updated during training with a learning rate of 0.1 .",0
7111,4318,"<< For >> the [[ sentiment classification task ]] , word representations were updated [[ during training ]] with a learning rate of 0.1 .",0
7112,4318,"<< For >> the [[ sentiment classification task ]] , word representations were updated during training with a [[ learning rate ]] of 0.1 .",0
7113,4318,"<< For >> the [[ sentiment classification task ]] , word representations were updated during training with a learning rate of [[ 0.1 ]] .",0
7114,4318,"<< For >> the sentiment classification task , [[ word representations ]] were updated [[ during training ]] with a learning rate of 0.1 .",0
7115,4318,"<< For >> the sentiment classification task , [[ word representations ]] were updated during training with a [[ learning rate ]] of 0.1 .",0
7116,4318,"<< For >> the sentiment classification task , [[ word representations ]] were updated during training with a learning rate of [[ 0.1 ]] .",0
7117,4318,"<< For >> the sentiment classification task , word representations were updated [[ during training ]] with a [[ learning rate ]] of 0.1 .",0
7118,4318,"<< For >> the sentiment classification task , word representations were updated [[ during training ]] with a learning rate of [[ 0.1 ]] .",0
7119,4318,"<< For >> the sentiment classification task , word representations were updated during training with a [[ learning rate ]] of [[ 0.1 ]] .",0
7120,4318,"For the [[ sentiment classification task ]] , [[ word representations ]] were << updated >> during training with a learning rate of 0.1 .",0
7121,4318,"For the [[ sentiment classification task ]] , word representations were << updated >> [[ during training ]] with a learning rate of 0.1 .",0
7122,4318,"For the [[ sentiment classification task ]] , word representations were << updated >> during training with a [[ learning rate ]] of 0.1 .",0
7123,4318,"For the [[ sentiment classification task ]] , word representations were << updated >> during training with a learning rate of [[ 0.1 ]] .",0
7124,4318,"For the sentiment classification task , [[ word representations ]] were << updated >> [[ during training ]] with a learning rate of 0.1 .",1
7125,4318,"For the sentiment classification task , [[ word representations ]] were << updated >> during training with a [[ learning rate ]] of 0.1 .",0
7126,4318,"For the sentiment classification task , [[ word representations ]] were << updated >> during training with a learning rate of [[ 0.1 ]] .",0
7127,4318,"For the sentiment classification task , word representations were << updated >> [[ during training ]] with a [[ learning rate ]] of 0.1 .",0
7128,4318,"For the sentiment classification task , word representations were << updated >> [[ during training ]] with a learning rate of [[ 0.1 ]] .",0
7129,4318,"For the sentiment classification task , word representations were << updated >> during training with a [[ learning rate ]] of [[ 0.1 ]] .",0
7130,4318,"For the [[ sentiment classification task ]] , [[ word representations ]] were updated during training << with >> a learning rate of 0.1 .",0
7131,4318,"For the [[ sentiment classification task ]] , word representations were updated [[ during training ]] << with >> a learning rate of 0.1 .",0
7132,4318,"For the [[ sentiment classification task ]] , word representations were updated during training << with >> a [[ learning rate ]] of 0.1 .",0
7133,4318,"For the [[ sentiment classification task ]] , word representations were updated during training << with >> a learning rate of [[ 0.1 ]] .",0
7134,4318,"For the sentiment classification task , [[ word representations ]] were updated [[ during training ]] << with >> a learning rate of 0.1 .",0
7135,4318,"For the sentiment classification task , [[ word representations ]] were updated during training << with >> a [[ learning rate ]] of 0.1 .",0
7136,4318,"For the sentiment classification task , [[ word representations ]] were updated during training << with >> a learning rate of [[ 0.1 ]] .",0
7137,4318,"For the sentiment classification task , word representations were updated [[ during training ]] << with >> a [[ learning rate ]] of 0.1 .",1
7138,4318,"For the sentiment classification task , word representations were updated [[ during training ]] << with >> a learning rate of [[ 0.1 ]] .",0
7139,4318,"For the sentiment classification task , word representations were updated during training << with >> a [[ learning rate ]] of [[ 0.1 ]] .",0
7140,4318,"For the [[ sentiment classification task ]] , [[ word representations ]] were updated during training with a learning rate << of >> 0.1 .",0
7141,4318,"For the [[ sentiment classification task ]] , word representations were updated [[ during training ]] with a learning rate << of >> 0.1 .",0
7142,4318,"For the [[ sentiment classification task ]] , word representations were updated during training with a [[ learning rate ]] << of >> 0.1 .",0
7143,4318,"For the [[ sentiment classification task ]] , word representations were updated during training with a learning rate << of >> [[ 0.1 ]] .",0
7144,4318,"For the sentiment classification task , [[ word representations ]] were updated [[ during training ]] with a learning rate << of >> 0.1 .",0
7145,4318,"For the sentiment classification task , [[ word representations ]] were updated during training with a [[ learning rate ]] << of >> 0.1 .",0
7146,4318,"For the sentiment classification task , [[ word representations ]] were updated during training with a learning rate << of >> [[ 0.1 ]] .",0
7147,4318,"For the sentiment classification task , word representations were updated [[ during training ]] with a [[ learning rate ]] << of >> 0.1 .",0
7148,4318,"For the sentiment classification task , word representations were updated [[ during training ]] with a learning rate << of >> [[ 0.1 ]] .",0
7149,4318,"For the sentiment classification task , word representations were updated during training with a [[ learning rate ]] << of >> [[ 0.1 ]] .",1
7150,1052,"The [[ fusion process ]] << is >> [[ iteratively performed ]] at each hop through the document tokens and entities , and the final resulting answer is then obtained from document tokens .",1
7151,1052,"The [[ fusion process ]] << is >> iteratively performed at [[ each hop ]] through the document tokens and entities , and the final resulting answer is then obtained from document tokens .",0
7152,1052,"The [[ fusion process ]] << is >> iteratively performed at each hop through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7153,1052,"The [[ fusion process ]] << is >> iteratively performed at each hop through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7154,1052,"The [[ fusion process ]] << is >> iteratively performed at each hop through the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7155,1052,"The fusion process << is >> [[ iteratively performed ]] at [[ each hop ]] through the document tokens and entities , and the final resulting answer is then obtained from document tokens .",0
7156,1052,"The fusion process << is >> [[ iteratively performed ]] at each hop through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7157,1052,"The fusion process << is >> [[ iteratively performed ]] at each hop through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7158,1052,"The fusion process << is >> [[ iteratively performed ]] at each hop through the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7159,1052,"The fusion process << is >> iteratively performed at [[ each hop ]] through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7160,1052,"The fusion process << is >> iteratively performed at [[ each hop ]] through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7161,1052,"The fusion process << is >> iteratively performed at [[ each hop ]] through the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7162,1052,"The fusion process << is >> iteratively performed at each hop through the [[ document tokens and entities ]] , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7163,1052,"The fusion process << is >> iteratively performed at each hop through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7164,1052,"The fusion process << is >> iteratively performed at each hop through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from [[ document tokens ]] .",0
7165,1052,"The [[ fusion process ]] is [[ iteratively performed ]] << at >> each hop through the document tokens and entities , and the final resulting answer is then obtained from document tokens .",0
7166,1052,"The [[ fusion process ]] is iteratively performed << at >> [[ each hop ]] through the document tokens and entities , and the final resulting answer is then obtained from document tokens .",0
7167,1052,"The [[ fusion process ]] is iteratively performed << at >> each hop through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7168,1052,"The [[ fusion process ]] is iteratively performed << at >> each hop through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7169,1052,"The [[ fusion process ]] is iteratively performed << at >> each hop through the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7170,1052,"The fusion process is [[ iteratively performed ]] << at >> [[ each hop ]] through the document tokens and entities , and the final resulting answer is then obtained from document tokens .",1
7171,1052,"The fusion process is [[ iteratively performed ]] << at >> each hop through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7172,1052,"The fusion process is [[ iteratively performed ]] << at >> each hop through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7173,1052,"The fusion process is [[ iteratively performed ]] << at >> each hop through the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7174,1052,"The fusion process is iteratively performed << at >> [[ each hop ]] through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7175,1052,"The fusion process is iteratively performed << at >> [[ each hop ]] through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7176,1052,"The fusion process is iteratively performed << at >> [[ each hop ]] through the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7177,1052,"The fusion process is iteratively performed << at >> each hop through the [[ document tokens and entities ]] , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7178,1052,"The fusion process is iteratively performed << at >> each hop through the [[ document tokens and entities ]] , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7179,1052,"The fusion process is iteratively performed << at >> each hop through the document tokens and entities , and the [[ final resulting answer ]] is then obtained from [[ document tokens ]] .",0
7180,1052,"The [[ fusion process ]] is [[ iteratively performed ]] at each hop << through >> the document tokens and entities , and the final resulting answer is then obtained from document tokens .",0
7181,1052,"The [[ fusion process ]] is iteratively performed at [[ each hop ]] << through >> the document tokens and entities , and the final resulting answer is then obtained from document tokens .",0
7182,1052,"The [[ fusion process ]] is iteratively performed at each hop << through >> the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7183,1052,"The [[ fusion process ]] is iteratively performed at each hop << through >> the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7184,1052,"The [[ fusion process ]] is iteratively performed at each hop << through >> the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7185,1052,"The fusion process is [[ iteratively performed ]] at [[ each hop ]] << through >> the document tokens and entities , and the final resulting answer is then obtained from document tokens .",0
7186,1052,"The fusion process is [[ iteratively performed ]] at each hop << through >> the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",1
7187,1052,"The fusion process is [[ iteratively performed ]] at each hop << through >> the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7188,1052,"The fusion process is [[ iteratively performed ]] at each hop << through >> the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7189,1052,"The fusion process is iteratively performed at [[ each hop ]] << through >> the [[ document tokens and entities ]] , and the final resulting answer is then obtained from document tokens .",0
7190,1052,"The fusion process is iteratively performed at [[ each hop ]] << through >> the document tokens and entities , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7191,1052,"The fusion process is iteratively performed at [[ each hop ]] << through >> the document tokens and entities , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7192,1052,"The fusion process is iteratively performed at each hop << through >> the [[ document tokens and entities ]] , and the [[ final resulting answer ]] is then obtained from document tokens .",0
7193,1052,"The fusion process is iteratively performed at each hop << through >> the [[ document tokens and entities ]] , and the final resulting answer is then obtained from [[ document tokens ]] .",0
7194,1052,"The fusion process is iteratively performed at each hop << through >> the document tokens and entities , and the [[ final resulting answer ]] is then obtained from [[ document tokens ]] .",0
7195,1052,"The [[ fusion process ]] is [[ iteratively performed ]] at each hop through the document tokens and entities , and the final resulting answer is then << obtained from >> document tokens .",0
7196,1052,"The [[ fusion process ]] is iteratively performed at [[ each hop ]] through the document tokens and entities , and the final resulting answer is then << obtained from >> document tokens .",0
7197,1052,"The [[ fusion process ]] is iteratively performed at each hop through the [[ document tokens and entities ]] , and the final resulting answer is then << obtained from >> document tokens .",0
7198,1052,"The [[ fusion process ]] is iteratively performed at each hop through the document tokens and entities , and the [[ final resulting answer ]] is then << obtained from >> document tokens .",0
7199,1052,"The [[ fusion process ]] is iteratively performed at each hop through the document tokens and entities , and the final resulting answer is then << obtained from >> [[ document tokens ]] .",0
7200,1052,"The fusion process is [[ iteratively performed ]] at [[ each hop ]] through the document tokens and entities , and the final resulting answer is then << obtained from >> document tokens .",0
7201,1052,"The fusion process is [[ iteratively performed ]] at each hop through the [[ document tokens and entities ]] , and the final resulting answer is then << obtained from >> document tokens .",0
7202,1052,"The fusion process is [[ iteratively performed ]] at each hop through the document tokens and entities , and the [[ final resulting answer ]] is then << obtained from >> document tokens .",0
7203,1052,"The fusion process is [[ iteratively performed ]] at each hop through the document tokens and entities , and the final resulting answer is then << obtained from >> [[ document tokens ]] .",0
7204,1052,"The fusion process is iteratively performed at [[ each hop ]] through the [[ document tokens and entities ]] , and the final resulting answer is then << obtained from >> document tokens .",0
7205,1052,"The fusion process is iteratively performed at [[ each hop ]] through the document tokens and entities , and the [[ final resulting answer ]] is then << obtained from >> document tokens .",0
7206,1052,"The fusion process is iteratively performed at [[ each hop ]] through the document tokens and entities , and the final resulting answer is then << obtained from >> [[ document tokens ]] .",0
7207,1052,"The fusion process is iteratively performed at each hop through the [[ document tokens and entities ]] , and the [[ final resulting answer ]] is then << obtained from >> document tokens .",0
7208,1052,"The fusion process is iteratively performed at each hop through the [[ document tokens and entities ]] , and the final resulting answer is then << obtained from >> [[ document tokens ]] .",0
7209,1052,"The fusion process is iteratively performed at each hop through the document tokens and entities , and the [[ final resulting answer ]] is then << obtained from >> [[ document tokens ]] .",1
7210,668,"We notice that << using >> [[ 2 blocks ]] causes a [[ slight performance drop ]] , while increasing to 4 blocks barely affects the SoTA result .",0
7211,668,"We notice that << using >> [[ 2 blocks ]] causes a slight performance drop , while increasing to [[ 4 blocks ]] barely affects the SoTA result .",0
7212,668,"We notice that << using >> [[ 2 blocks ]] causes a slight performance drop , while increasing to 4 blocks barely affects the [[ SoTA result ]] .",0
7213,668,"We notice that << using >> 2 blocks causes a [[ slight performance drop ]] , while increasing to [[ 4 blocks ]] barely affects the SoTA result .",0
7214,668,"We notice that << using >> 2 blocks causes a [[ slight performance drop ]] , while increasing to 4 blocks barely affects the [[ SoTA result ]] .",0
7215,668,"We notice that << using >> 2 blocks causes a slight performance drop , while increasing to [[ 4 blocks ]] barely affects the [[ SoTA result ]] .",0
7216,668,"We notice that using [[ 2 blocks ]] << causes >> a [[ slight performance drop ]] , while increasing to 4 blocks barely affects the SoTA result .",1
7217,668,"We notice that using [[ 2 blocks ]] << causes >> a slight performance drop , while increasing to [[ 4 blocks ]] barely affects the SoTA result .",0
7218,668,"We notice that using [[ 2 blocks ]] << causes >> a slight performance drop , while increasing to 4 blocks barely affects the [[ SoTA result ]] .",0
7219,668,"We notice that using 2 blocks << causes >> a [[ slight performance drop ]] , while increasing to [[ 4 blocks ]] barely affects the SoTA result .",0
7220,668,"We notice that using 2 blocks << causes >> a [[ slight performance drop ]] , while increasing to 4 blocks barely affects the [[ SoTA result ]] .",0
7221,668,"We notice that using 2 blocks << causes >> a slight performance drop , while increasing to [[ 4 blocks ]] barely affects the [[ SoTA result ]] .",0
7222,668,"We notice that using [[ 2 blocks ]] causes a [[ slight performance drop ]] , while << increasing to >> 4 blocks barely affects the SoTA result .",0
7223,668,"We notice that using [[ 2 blocks ]] causes a slight performance drop , while << increasing to >> [[ 4 blocks ]] barely affects the SoTA result .",0
7224,668,"We notice that using [[ 2 blocks ]] causes a slight performance drop , while << increasing to >> 4 blocks barely affects the [[ SoTA result ]] .",0
7225,668,"We notice that using 2 blocks causes a [[ slight performance drop ]] , while << increasing to >> [[ 4 blocks ]] barely affects the SoTA result .",0
7226,668,"We notice that using 2 blocks causes a [[ slight performance drop ]] , while << increasing to >> 4 blocks barely affects the [[ SoTA result ]] .",0
7227,668,"We notice that using 2 blocks causes a slight performance drop , while << increasing to >> [[ 4 blocks ]] barely affects the [[ SoTA result ]] .",0
7228,668,"We notice that using [[ 2 blocks ]] causes a [[ slight performance drop ]] , while increasing to 4 blocks << barely affects >> the SoTA result .",0
7229,668,"We notice that using [[ 2 blocks ]] causes a slight performance drop , while increasing to [[ 4 blocks ]] << barely affects >> the SoTA result .",0
7230,668,"We notice that using [[ 2 blocks ]] causes a slight performance drop , while increasing to 4 blocks << barely affects >> the [[ SoTA result ]] .",0
7231,668,"We notice that using 2 blocks causes a [[ slight performance drop ]] , while increasing to [[ 4 blocks ]] << barely affects >> the SoTA result .",0
7232,668,"We notice that using 2 blocks causes a [[ slight performance drop ]] , while increasing to 4 blocks << barely affects >> the [[ SoTA result ]] .",0
7233,668,"We notice that using 2 blocks causes a slight performance drop , while increasing to [[ 4 blocks ]] << barely affects >> the [[ SoTA result ]] .",1
7234,253,"<< To capture >> [[ orthographic sensitivity ]] , we use [[ character - based word representation model ]] to capture distributional sensitivity , we combine these representations with distributional representations .",0
7235,253,"<< To capture >> [[ orthographic sensitivity ]] , we use character - based word representation model to capture distributional sensitivity , we combine these representations with [[ distributional representations ]] .",0
7236,253,"<< To capture >> orthographic sensitivity , we use [[ character - based word representation model ]] to capture distributional sensitivity , we combine these representations with [[ distributional representations ]] .",0
7237,253,"To capture [[ orthographic sensitivity ]] , we << use >> [[ character - based word representation model ]] to capture distributional sensitivity , we combine these representations with distributional representations .",1
7238,253,"To capture [[ orthographic sensitivity ]] , we << use >> character - based word representation model to capture distributional sensitivity , we combine these representations with [[ distributional representations ]] .",0
7239,253,"To capture orthographic sensitivity , we << use >> [[ character - based word representation model ]] to capture distributional sensitivity , we combine these representations with [[ distributional representations ]] .",0
7240,253,"To capture [[ orthographic sensitivity ]] , we use [[ character - based word representation model ]] to capture distributional sensitivity , we combine these representations << with >> distributional representations .",0
7241,253,"To capture [[ orthographic sensitivity ]] , we use character - based word representation model to capture distributional sensitivity , we combine these representations << with >> [[ distributional representations ]] .",0
7242,253,"To capture orthographic sensitivity , we use [[ character - based word representation model ]] to capture distributional sensitivity , we combine these representations << with >> [[ distributional representations ]] .",1
7243,5168,We << set >> [[ dropout ]] = [[ 0.5 ( MOSI ) ]] & 0.3 ( MOSEI ) as a measure of regularization .,0
7244,5168,We << set >> [[ dropout ]] = 0.5 ( MOSI ) & [[ 0.3 ( MOSEI ) ]] as a measure of regularization .,0
7245,5168,We << set >> [[ dropout ]] = 0.5 ( MOSI ) & 0.3 ( MOSEI ) as a measure of [[ regularization ]] .,0
7246,5168,We << set >> dropout = [[ 0.5 ( MOSI ) ]] & [[ 0.3 ( MOSEI ) ]] as a measure of regularization .,0
7247,5168,We << set >> dropout = [[ 0.5 ( MOSI ) ]] & 0.3 ( MOSEI ) as a measure of [[ regularization ]] .,0
7248,5168,We << set >> dropout = 0.5 ( MOSI ) & [[ 0.3 ( MOSEI ) ]] as a measure of [[ regularization ]] .,0
7249,5168,We set [[ dropout ]] << = >> [[ 0.5 ( MOSI ) ]] & 0.3 ( MOSEI ) as a measure of regularization .,1
7250,5168,We set [[ dropout ]] << = >> 0.5 ( MOSI ) & [[ 0.3 ( MOSEI ) ]] as a measure of regularization .,1
7251,5168,We set [[ dropout ]] << = >> 0.5 ( MOSI ) & 0.3 ( MOSEI ) as a measure of [[ regularization ]] .,0
7252,5168,We set dropout << = >> [[ 0.5 ( MOSI ) ]] & [[ 0.3 ( MOSEI ) ]] as a measure of regularization .,0
7253,5168,We set dropout << = >> [[ 0.5 ( MOSI ) ]] & 0.3 ( MOSEI ) as a measure of [[ regularization ]] .,0
7254,5168,We set dropout << = >> 0.5 ( MOSI ) & [[ 0.3 ( MOSEI ) ]] as a measure of [[ regularization ]] .,0
7255,5168,We set [[ dropout ]] = [[ 0.5 ( MOSI ) ]] & 0.3 ( MOSEI ) << as a measure of >> regularization .,0
7256,5168,We set [[ dropout ]] = 0.5 ( MOSI ) & [[ 0.3 ( MOSEI ) ]] << as a measure of >> regularization .,0
7257,5168,We set [[ dropout ]] = 0.5 ( MOSI ) & 0.3 ( MOSEI ) << as a measure of >> [[ regularization ]] .,0
7258,5168,We set dropout = [[ 0.5 ( MOSI ) ]] & [[ 0.3 ( MOSEI ) ]] << as a measure of >> regularization .,0
7259,5168,We set dropout = [[ 0.5 ( MOSI ) ]] & 0.3 ( MOSEI ) << as a measure of >> [[ regularization ]] .,0
7260,5168,We set dropout = 0.5 ( MOSI ) & [[ 0.3 ( MOSEI ) ]] << as a measure of >> [[ regularization ]] .,0
7261,5715,The [[ initial learning rate ]] is [[ 0.002 ]] and the model is << trained in >> minibatches with a batch size of 256 . ? 1 and ?,0
7262,5715,The [[ initial learning rate ]] is 0.002 and the [[ model ]] is << trained in >> minibatches with a batch size of 256 . ? 1 and ?,0
7263,5715,The [[ initial learning rate ]] is 0.002 and the model is << trained in >> [[ minibatches ]] with a batch size of 256 . ? 1 and ?,0
7264,5715,The [[ initial learning rate ]] is 0.002 and the model is << trained in >> minibatches with a [[ batch size ]] of 256 . ? 1 and ?,0
7265,5715,The [[ initial learning rate ]] is 0.002 and the model is << trained in >> minibatches with a batch size of [[ 256 ]] . ? 1 and ?,0
7266,5715,The initial learning rate is [[ 0.002 ]] and the [[ model ]] is << trained in >> minibatches with a batch size of 256 . ? 1 and ?,0
7267,5715,The initial learning rate is [[ 0.002 ]] and the model is << trained in >> [[ minibatches ]] with a batch size of 256 . ? 1 and ?,0
7268,5715,The initial learning rate is [[ 0.002 ]] and the model is << trained in >> minibatches with a [[ batch size ]] of 256 . ? 1 and ?,0
7269,5715,The initial learning rate is [[ 0.002 ]] and the model is << trained in >> minibatches with a batch size of [[ 256 ]] . ? 1 and ?,0
7270,5715,The initial learning rate is 0.002 and the [[ model ]] is << trained in >> [[ minibatches ]] with a batch size of 256 . ? 1 and ?,1
7271,5715,The initial learning rate is 0.002 and the [[ model ]] is << trained in >> minibatches with a [[ batch size ]] of 256 . ? 1 and ?,0
7272,5715,The initial learning rate is 0.002 and the [[ model ]] is << trained in >> minibatches with a batch size of [[ 256 ]] . ? 1 and ?,0
7273,5715,The initial learning rate is 0.002 and the model is << trained in >> [[ minibatches ]] with a [[ batch size ]] of 256 . ? 1 and ?,0
7274,5715,The initial learning rate is 0.002 and the model is << trained in >> [[ minibatches ]] with a batch size of [[ 256 ]] . ? 1 and ?,0
7275,5715,The initial learning rate is 0.002 and the model is << trained in >> minibatches with a [[ batch size ]] of [[ 256 ]] . ? 1 and ?,0
7276,5715,The [[ initial learning rate ]] is [[ 0.002 ]] and the model is trained in minibatches << with >> a batch size of 256 . ? 1 and ?,0
7277,5715,The [[ initial learning rate ]] is 0.002 and the [[ model ]] is trained in minibatches << with >> a batch size of 256 . ? 1 and ?,0
7278,5715,The [[ initial learning rate ]] is 0.002 and the model is trained in [[ minibatches ]] << with >> a batch size of 256 . ? 1 and ?,0
7279,5715,The [[ initial learning rate ]] is 0.002 and the model is trained in minibatches << with >> a [[ batch size ]] of 256 . ? 1 and ?,0
7280,5715,The [[ initial learning rate ]] is 0.002 and the model is trained in minibatches << with >> a batch size of [[ 256 ]] . ? 1 and ?,0
7281,5715,The initial learning rate is [[ 0.002 ]] and the [[ model ]] is trained in minibatches << with >> a batch size of 256 . ? 1 and ?,0
7282,5715,The initial learning rate is [[ 0.002 ]] and the model is trained in [[ minibatches ]] << with >> a batch size of 256 . ? 1 and ?,0
7283,5715,The initial learning rate is [[ 0.002 ]] and the model is trained in minibatches << with >> a [[ batch size ]] of 256 . ? 1 and ?,0
7284,5715,The initial learning rate is [[ 0.002 ]] and the model is trained in minibatches << with >> a batch size of [[ 256 ]] . ? 1 and ?,0
7285,5715,The initial learning rate is 0.002 and the [[ model ]] is trained in [[ minibatches ]] << with >> a batch size of 256 . ? 1 and ?,0
7286,5715,The initial learning rate is 0.002 and the [[ model ]] is trained in minibatches << with >> a [[ batch size ]] of 256 . ? 1 and ?,0
7287,5715,The initial learning rate is 0.002 and the [[ model ]] is trained in minibatches << with >> a batch size of [[ 256 ]] . ? 1 and ?,0
7288,5715,The initial learning rate is 0.002 and the model is trained in [[ minibatches ]] << with >> a [[ batch size ]] of 256 . ? 1 and ?,1
7289,5715,The initial learning rate is 0.002 and the model is trained in [[ minibatches ]] << with >> a batch size of [[ 256 ]] . ? 1 and ?,0
7290,5715,The initial learning rate is 0.002 and the model is trained in minibatches << with >> a [[ batch size ]] of [[ 256 ]] . ? 1 and ?,0
7291,5715,The [[ initial learning rate ]] is [[ 0.002 ]] and the model is trained in minibatches with a batch size << of >> 256 . ? 1 and ?,0
7292,5715,The [[ initial learning rate ]] is 0.002 and the [[ model ]] is trained in minibatches with a batch size << of >> 256 . ? 1 and ?,0
7293,5715,The [[ initial learning rate ]] is 0.002 and the model is trained in [[ minibatches ]] with a batch size << of >> 256 . ? 1 and ?,0
7294,5715,The [[ initial learning rate ]] is 0.002 and the model is trained in minibatches with a [[ batch size ]] << of >> 256 . ? 1 and ?,0
7295,5715,The [[ initial learning rate ]] is 0.002 and the model is trained in minibatches with a batch size << of >> [[ 256 ]] . ? 1 and ?,0
7296,5715,The initial learning rate is [[ 0.002 ]] and the [[ model ]] is trained in minibatches with a batch size << of >> 256 . ? 1 and ?,0
7297,5715,The initial learning rate is [[ 0.002 ]] and the model is trained in [[ minibatches ]] with a batch size << of >> 256 . ? 1 and ?,0
7298,5715,The initial learning rate is [[ 0.002 ]] and the model is trained in minibatches with a [[ batch size ]] << of >> 256 . ? 1 and ?,0
7299,5715,The initial learning rate is [[ 0.002 ]] and the model is trained in minibatches with a batch size << of >> [[ 256 ]] . ? 1 and ?,0
7300,5715,The initial learning rate is 0.002 and the [[ model ]] is trained in [[ minibatches ]] with a batch size << of >> 256 . ? 1 and ?,0
7301,5715,The initial learning rate is 0.002 and the [[ model ]] is trained in minibatches with a [[ batch size ]] << of >> 256 . ? 1 and ?,0
7302,5715,The initial learning rate is 0.002 and the [[ model ]] is trained in minibatches with a batch size << of >> [[ 256 ]] . ? 1 and ?,0
7303,5715,The initial learning rate is 0.002 and the model is trained in [[ minibatches ]] with a [[ batch size ]] << of >> 256 . ? 1 and ?,0
7304,5715,The initial learning rate is 0.002 and the model is trained in [[ minibatches ]] with a batch size << of >> [[ 256 ]] . ? 1 and ?,0
7305,5715,The initial learning rate is 0.002 and the model is trained in minibatches with a [[ batch size ]] << of >> [[ 256 ]] . ? 1 and ?,1
7306,5588,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a [[ length regulator ]] that << up - samples >> the [[ phoneme sequence according to the phoneme duration ]] ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .",1
7307,5588,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a [[ length regulator ]] that << up - samples >> the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the [[ length of the mel-spectrogram sequence ]] .",0
7308,5588,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that << up - samples >> the [[ phoneme sequence according to the phoneme duration ]] ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the [[ length of the mel-spectrogram sequence ]] .",0
7309,5588,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a [[ length regulator ]] that up - samples the [[ phoneme sequence according to the phoneme duration ]] ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) << to match >> the length of the mel-spectrogram sequence .",0
7310,5588,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a [[ length regulator ]] that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) << to match >> the [[ length of the mel-spectrogram sequence ]] .",0
7311,5588,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the [[ phoneme sequence according to the phoneme duration ]] ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) << to match >> the [[ length of the mel-spectrogram sequence ]] .",1
7312,2690,"Nevertheless , the model also << produces >> [[ words ]] not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as expansion by synonyms and other related terms .",0
7313,2690,"Nevertheless , the model also << produces >> [[ words ]] not present in the input document ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] by synonyms and other related terms .",0
7314,2690,"Nevertheless , the model also << produces >> [[ words ]] not present in the input document ( e.g. , weather , relationship ) , which can be characterized as expansion by [[ synonyms and other related terms ]] .",0
7315,2690,"Nevertheless , the model also << produces >> words not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] by synonyms and other related terms .",0
7316,2690,"Nevertheless , the model also << produces >> words not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as expansion by [[ synonyms and other related terms ]] .",0
7317,2690,"Nevertheless , the model also << produces >> words not present in the input document ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] by [[ synonyms and other related terms ]] .",0
7318,2690,"Nevertheless , the model also produces [[ words ]] << not present in >> the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as expansion by synonyms and other related terms .",1
7319,2690,"Nevertheless , the model also produces [[ words ]] << not present in >> the input document ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] by synonyms and other related terms .",0
7320,2690,"Nevertheless , the model also produces [[ words ]] << not present in >> the input document ( e.g. , weather , relationship ) , which can be characterized as expansion by [[ synonyms and other related terms ]] .",0
7321,2690,"Nevertheless , the model also produces words << not present in >> the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] by synonyms and other related terms .",0
7322,2690,"Nevertheless , the model also produces words << not present in >> the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as expansion by [[ synonyms and other related terms ]] .",0
7323,2690,"Nevertheless , the model also produces words << not present in >> the input document ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] by [[ synonyms and other related terms ]] .",0
7324,2690,"Nevertheless , the model also produces [[ words ]] not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be << characterized as >> expansion by synonyms and other related terms .",0
7325,2690,"Nevertheless , the model also produces [[ words ]] not present in the input document ( e.g. , weather , relationship ) , which can be << characterized as >> [[ expansion ]] by synonyms and other related terms .",1
7326,2690,"Nevertheless , the model also produces [[ words ]] not present in the input document ( e.g. , weather , relationship ) , which can be << characterized as >> expansion by [[ synonyms and other related terms ]] .",0
7327,2690,"Nevertheless , the model also produces words not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be << characterized as >> [[ expansion ]] by synonyms and other related terms .",0
7328,2690,"Nevertheless , the model also produces words not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be << characterized as >> expansion by [[ synonyms and other related terms ]] .",0
7329,2690,"Nevertheless , the model also produces words not present in the input document ( e.g. , weather , relationship ) , which can be << characterized as >> [[ expansion ]] by [[ synonyms and other related terms ]] .",0
7330,2690,"Nevertheless , the model also produces [[ words ]] not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as expansion << by >> synonyms and other related terms .",0
7331,2690,"Nevertheless , the model also produces [[ words ]] not present in the input document ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] << by >> synonyms and other related terms .",0
7332,2690,"Nevertheless , the model also produces [[ words ]] not present in the input document ( e.g. , weather , relationship ) , which can be characterized as expansion << by >> [[ synonyms and other related terms ]] .",0
7333,2690,"Nevertheless , the model also produces words not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] << by >> synonyms and other related terms .",0
7334,2690,"Nevertheless , the model also produces words not present in the [[ input document ]] ( e.g. , weather , relationship ) , which can be characterized as expansion << by >> [[ synonyms and other related terms ]] .",0
7335,2690,"Nevertheless , the model also produces words not present in the input document ( e.g. , weather , relationship ) , which can be characterized as [[ expansion ]] << by >> [[ synonyms and other related terms ]] .",1
7336,4641,The [[ initial learning rate ]] << is >> [[ 0.01 ]] for the Adam optimizer .,1
7337,4641,The [[ initial learning rate ]] << is >> 0.01 for the [[ Adam optimizer ]] .,0
7338,4641,The initial learning rate << is >> [[ 0.01 ]] for the [[ Adam optimizer ]] .,0
7339,4641,The [[ initial learning rate ]] is [[ 0.01 ]] << for >> the Adam optimizer .,0
7340,4641,The [[ initial learning rate ]] is 0.01 << for >> the [[ Adam optimizer ]] .,0
7341,4641,The initial learning rate is [[ 0.01 ]] << for >> the [[ Adam optimizer ]] .,1
7342,5045,"[[ BILSTM - ATT - G ]] << models >> [[ left context and right context ]] using attention - based LSTMs , which achieves better performance than MemNet .",1
7343,5045,"[[ BILSTM - ATT - G ]] << models >> left context and right context using [[ attention - based LSTMs ]] , which achieves better performance than MemNet .",0
7344,5045,"[[ BILSTM - ATT - G ]] << models >> left context and right context using attention - based LSTMs , which achieves [[ better performance ]] than MemNet .",0
7345,5045,"[[ BILSTM - ATT - G ]] << models >> left context and right context using attention - based LSTMs , which achieves better performance than [[ MemNet ]] .",0
7346,5045,"BILSTM - ATT - G << models >> [[ left context and right context ]] using [[ attention - based LSTMs ]] , which achieves better performance than MemNet .",0
7347,5045,"BILSTM - ATT - G << models >> [[ left context and right context ]] using attention - based LSTMs , which achieves [[ better performance ]] than MemNet .",0
7348,5045,"BILSTM - ATT - G << models >> [[ left context and right context ]] using attention - based LSTMs , which achieves better performance than [[ MemNet ]] .",0
7349,5045,"BILSTM - ATT - G << models >> left context and right context using [[ attention - based LSTMs ]] , which achieves [[ better performance ]] than MemNet .",0
7350,5045,"BILSTM - ATT - G << models >> left context and right context using [[ attention - based LSTMs ]] , which achieves better performance than [[ MemNet ]] .",0
7351,5045,"BILSTM - ATT - G << models >> left context and right context using attention - based LSTMs , which achieves [[ better performance ]] than [[ MemNet ]] .",0
7352,5045,"[[ BILSTM - ATT - G ]] models [[ left context and right context ]] << using >> attention - based LSTMs , which achieves better performance than MemNet .",0
7353,5045,"[[ BILSTM - ATT - G ]] models left context and right context << using >> [[ attention - based LSTMs ]] , which achieves better performance than MemNet .",0
7354,5045,"[[ BILSTM - ATT - G ]] models left context and right context << using >> attention - based LSTMs , which achieves [[ better performance ]] than MemNet .",0
7355,5045,"[[ BILSTM - ATT - G ]] models left context and right context << using >> attention - based LSTMs , which achieves better performance than [[ MemNet ]] .",0
7356,5045,"BILSTM - ATT - G models [[ left context and right context ]] << using >> [[ attention - based LSTMs ]] , which achieves better performance than MemNet .",1
7357,5045,"BILSTM - ATT - G models [[ left context and right context ]] << using >> attention - based LSTMs , which achieves [[ better performance ]] than MemNet .",0
7358,5045,"BILSTM - ATT - G models [[ left context and right context ]] << using >> attention - based LSTMs , which achieves better performance than [[ MemNet ]] .",0
7359,5045,"BILSTM - ATT - G models left context and right context << using >> [[ attention - based LSTMs ]] , which achieves [[ better performance ]] than MemNet .",0
7360,5045,"BILSTM - ATT - G models left context and right context << using >> [[ attention - based LSTMs ]] , which achieves better performance than [[ MemNet ]] .",0
7361,5045,"BILSTM - ATT - G models left context and right context << using >> attention - based LSTMs , which achieves [[ better performance ]] than [[ MemNet ]] .",0
7362,5045,"[[ BILSTM - ATT - G ]] models [[ left context and right context ]] using attention - based LSTMs , which << achieves >> better performance than MemNet .",0
7363,5045,"[[ BILSTM - ATT - G ]] models left context and right context using [[ attention - based LSTMs ]] , which << achieves >> better performance than MemNet .",0
7364,5045,"[[ BILSTM - ATT - G ]] models left context and right context using attention - based LSTMs , which << achieves >> [[ better performance ]] than MemNet .",1
7365,5045,"[[ BILSTM - ATT - G ]] models left context and right context using attention - based LSTMs , which << achieves >> better performance than [[ MemNet ]] .",0
7366,5045,"BILSTM - ATT - G models [[ left context and right context ]] using [[ attention - based LSTMs ]] , which << achieves >> better performance than MemNet .",0
7367,5045,"BILSTM - ATT - G models [[ left context and right context ]] using attention - based LSTMs , which << achieves >> [[ better performance ]] than MemNet .",0
7368,5045,"BILSTM - ATT - G models [[ left context and right context ]] using attention - based LSTMs , which << achieves >> better performance than [[ MemNet ]] .",0
7369,5045,"BILSTM - ATT - G models left context and right context using [[ attention - based LSTMs ]] , which << achieves >> [[ better performance ]] than MemNet .",0
7370,5045,"BILSTM - ATT - G models left context and right context using [[ attention - based LSTMs ]] , which << achieves >> better performance than [[ MemNet ]] .",0
7371,5045,"BILSTM - ATT - G models left context and right context using attention - based LSTMs , which << achieves >> [[ better performance ]] than [[ MemNet ]] .",0
7372,5045,"[[ BILSTM - ATT - G ]] models [[ left context and right context ]] using attention - based LSTMs , which achieves better performance << than >> MemNet .",0
7373,5045,"[[ BILSTM - ATT - G ]] models left context and right context using [[ attention - based LSTMs ]] , which achieves better performance << than >> MemNet .",0
7374,5045,"[[ BILSTM - ATT - G ]] models left context and right context using attention - based LSTMs , which achieves [[ better performance ]] << than >> MemNet .",0
7375,5045,"[[ BILSTM - ATT - G ]] models left context and right context using attention - based LSTMs , which achieves better performance << than >> [[ MemNet ]] .",0
7376,5045,"BILSTM - ATT - G models [[ left context and right context ]] using [[ attention - based LSTMs ]] , which achieves better performance << than >> MemNet .",0
7377,5045,"BILSTM - ATT - G models [[ left context and right context ]] using attention - based LSTMs , which achieves [[ better performance ]] << than >> MemNet .",0
7378,5045,"BILSTM - ATT - G models [[ left context and right context ]] using attention - based LSTMs , which achieves better performance << than >> [[ MemNet ]] .",0
7379,5045,"BILSTM - ATT - G models left context and right context using [[ attention - based LSTMs ]] , which achieves [[ better performance ]] << than >> MemNet .",0
7380,5045,"BILSTM - ATT - G models left context and right context using [[ attention - based LSTMs ]] , which achieves better performance << than >> [[ MemNet ]] .",0
7381,5045,"BILSTM - ATT - G models left context and right context using attention - based LSTMs , which achieves [[ better performance ]] << than >> [[ MemNet ]] .",1
7382,5667,"Following the evaluation protocol in , we compute the BLEU - 2 score and << estimate >> the [[ similarity ]] between the [[ human - written poem and the machine - created one ]] .",0
7383,5667,"Following the evaluation protocol in , we compute the BLEU - 2 score and estimate the [[ similarity ]] << between >> the [[ human - written poem and the machine - created one ]] .",1
7384,843,"Overall , we << observe >> a [[ significant improvement ]] with all three configurations , effectively showing the [[ benefit ]] of training a QA model in a semisupervised fashion with a large language model .",0
7385,843,"Overall , we << observe >> a [[ significant improvement ]] with all three configurations , effectively showing the benefit of [[ training ]] a QA model in a semisupervised fashion with a large language model .",0
7386,843,"Overall , we << observe >> a [[ significant improvement ]] with all three configurations , effectively showing the benefit of training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7387,843,"Overall , we << observe >> a [[ significant improvement ]] with all three configurations , effectively showing the benefit of training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7388,843,"Overall , we << observe >> a [[ significant improvement ]] with all three configurations , effectively showing the benefit of training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7389,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the [[ benefit ]] of [[ training ]] a QA model in a semisupervised fashion with a large language model .",0
7390,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the [[ benefit ]] of training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7391,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the [[ benefit ]] of training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7392,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the [[ benefit ]] of training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7393,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the benefit of [[ training ]] a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7394,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the benefit of [[ training ]] a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7395,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the benefit of [[ training ]] a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7396,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the benefit of training a [[ QA model ]] in a [[ semisupervised fashion ]] with a large language model .",0
7397,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the benefit of training a [[ QA model ]] in a semisupervised fashion with a [[ large language model ]] .",0
7398,843,"Overall , we << observe >> a significant improvement with all three configurations , effectively showing the benefit of training a QA model in a [[ semisupervised fashion ]] with a [[ large language model ]] .",0
7399,843,"Overall , we observe a [[ significant improvement ]] << with >> all three configurations , effectively showing the [[ benefit ]] of training a QA model in a semisupervised fashion with a large language model .",0
7400,843,"Overall , we observe a [[ significant improvement ]] << with >> all three configurations , effectively showing the benefit of [[ training ]] a QA model in a semisupervised fashion with a large language model .",0
7401,843,"Overall , we observe a [[ significant improvement ]] << with >> all three configurations , effectively showing the benefit of training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7402,843,"Overall , we observe a [[ significant improvement ]] << with >> all three configurations , effectively showing the benefit of training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7403,843,"Overall , we observe a [[ significant improvement ]] << with >> all three configurations , effectively showing the benefit of training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7404,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the [[ benefit ]] of [[ training ]] a QA model in a semisupervised fashion with a large language model .",0
7405,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the [[ benefit ]] of training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7406,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the [[ benefit ]] of training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7407,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the [[ benefit ]] of training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7408,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the benefit of [[ training ]] a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7409,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the benefit of [[ training ]] a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7410,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the benefit of [[ training ]] a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7411,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the benefit of training a [[ QA model ]] in a [[ semisupervised fashion ]] with a large language model .",0
7412,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the benefit of training a [[ QA model ]] in a semisupervised fashion with a [[ large language model ]] .",0
7413,843,"Overall , we observe a significant improvement << with >> all three configurations , effectively showing the benefit of training a QA model in a [[ semisupervised fashion ]] with a [[ large language model ]] .",1
7414,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively << showing >> the [[ benefit ]] of training a QA model in a semisupervised fashion with a large language model .",0
7415,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively << showing >> the benefit of [[ training ]] a QA model in a semisupervised fashion with a large language model .",0
7416,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively << showing >> the benefit of training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7417,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively << showing >> the benefit of training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7418,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively << showing >> the benefit of training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7419,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the [[ benefit ]] of [[ training ]] a QA model in a semisupervised fashion with a large language model .",0
7420,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the [[ benefit ]] of training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7421,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the [[ benefit ]] of training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7422,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the [[ benefit ]] of training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7423,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the benefit of [[ training ]] a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7424,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the benefit of [[ training ]] a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7425,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the benefit of [[ training ]] a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7426,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the benefit of training a [[ QA model ]] in a [[ semisupervised fashion ]] with a large language model .",0
7427,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the benefit of training a [[ QA model ]] in a semisupervised fashion with a [[ large language model ]] .",0
7428,843,"Overall , we observe a significant improvement with all three configurations , effectively << showing >> the benefit of training a QA model in a [[ semisupervised fashion ]] with a [[ large language model ]] .",0
7429,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the [[ benefit ]] << of >> training a QA model in a semisupervised fashion with a large language model .",0
7430,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit << of >> [[ training ]] a QA model in a semisupervised fashion with a large language model .",0
7431,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit << of >> training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7432,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit << of >> training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7433,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit << of >> training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7434,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] << of >> [[ training ]] a QA model in a semisupervised fashion with a large language model .",1
7435,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] << of >> training a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7436,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] << of >> training a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7437,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] << of >> training a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7438,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit << of >> [[ training ]] a [[ QA model ]] in a semisupervised fashion with a large language model .",0
7439,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit << of >> [[ training ]] a QA model in a [[ semisupervised fashion ]] with a large language model .",0
7440,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit << of >> [[ training ]] a QA model in a semisupervised fashion with a [[ large language model ]] .",0
7441,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit << of >> training a [[ QA model ]] in a [[ semisupervised fashion ]] with a large language model .",0
7442,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit << of >> training a [[ QA model ]] in a semisupervised fashion with a [[ large language model ]] .",0
7443,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit << of >> training a QA model in a [[ semisupervised fashion ]] with a [[ large language model ]] .",0
7444,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the [[ benefit ]] of training a QA model << in >> a semisupervised fashion with a large language model .",0
7445,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit of [[ training ]] a QA model << in >> a semisupervised fashion with a large language model .",0
7446,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit of training a [[ QA model ]] << in >> a semisupervised fashion with a large language model .",0
7447,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit of training a QA model << in >> a [[ semisupervised fashion ]] with a large language model .",0
7448,843,"Overall , we observe a [[ significant improvement ]] with all three configurations , effectively showing the benefit of training a QA model << in >> a semisupervised fashion with a [[ large language model ]] .",0
7449,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] of [[ training ]] a QA model << in >> a semisupervised fashion with a large language model .",0
7450,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] of training a [[ QA model ]] << in >> a semisupervised fashion with a large language model .",0
7451,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] of training a QA model << in >> a [[ semisupervised fashion ]] with a large language model .",0
7452,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the [[ benefit ]] of training a QA model << in >> a semisupervised fashion with a [[ large language model ]] .",0
7453,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of [[ training ]] a [[ QA model ]] << in >> a semisupervised fashion with a large language model .",0
7454,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of [[ training ]] a QA model << in >> a [[ semisupervised fashion ]] with a large language model .",1
7455,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of [[ training ]] a QA model << in >> a semisupervised fashion with a [[ large language model ]] .",0
7456,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a [[ QA model ]] << in >> a [[ semisupervised fashion ]] with a large language model .",0
7457,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a [[ QA model ]] << in >> a semisupervised fashion with a [[ large language model ]] .",0
7458,843,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a QA model << in >> a [[ semisupervised fashion ]] with a [[ large language model ]] .",0
7459,4159,"[[ Memnet ]] : As described in , the [[ current utterance ]] is << fed to >> a memory network , where the memories correspond to preceding utterances .",0
7460,4159,"[[ Memnet ]] : As described in , the current utterance is << fed to >> a [[ memory network ]] , where the memories correspond to preceding utterances .",0
7461,4159,"[[ Memnet ]] : As described in , the current utterance is << fed to >> a memory network , where the [[ memories ]] correspond to preceding utterances .",0
7462,4159,"[[ Memnet ]] : As described in , the current utterance is << fed to >> a memory network , where the memories correspond to [[ preceding utterances ]] .",0
7463,4159,"Memnet : As described in , the [[ current utterance ]] is << fed to >> a [[ memory network ]] , where the memories correspond to preceding utterances .",1
7464,4159,"Memnet : As described in , the [[ current utterance ]] is << fed to >> a memory network , where the [[ memories ]] correspond to preceding utterances .",0
7465,4159,"Memnet : As described in , the [[ current utterance ]] is << fed to >> a memory network , where the memories correspond to [[ preceding utterances ]] .",0
7466,4159,"Memnet : As described in , the current utterance is << fed to >> a [[ memory network ]] , where the [[ memories ]] correspond to preceding utterances .",0
7467,4159,"Memnet : As described in , the current utterance is << fed to >> a [[ memory network ]] , where the memories correspond to [[ preceding utterances ]] .",0
7468,4159,"Memnet : As described in , the current utterance is << fed to >> a memory network , where the [[ memories ]] correspond to [[ preceding utterances ]] .",0
7469,4159,"[[ Memnet ]] : As described in , the [[ current utterance ]] is fed to a memory network , << where >> the memories correspond to preceding utterances .",0
7470,4159,"[[ Memnet ]] : As described in , the current utterance is fed to a [[ memory network ]] , << where >> the memories correspond to preceding utterances .",0
7471,4159,"[[ Memnet ]] : As described in , the current utterance is fed to a memory network , << where >> the [[ memories ]] correspond to preceding utterances .",0
7472,4159,"[[ Memnet ]] : As described in , the current utterance is fed to a memory network , << where >> the memories correspond to [[ preceding utterances ]] .",0
7473,4159,"Memnet : As described in , the [[ current utterance ]] is fed to a [[ memory network ]] , << where >> the memories correspond to preceding utterances .",0
7474,4159,"Memnet : As described in , the [[ current utterance ]] is fed to a memory network , << where >> the [[ memories ]] correspond to preceding utterances .",0
7475,4159,"Memnet : As described in , the [[ current utterance ]] is fed to a memory network , << where >> the memories correspond to [[ preceding utterances ]] .",0
7476,4159,"Memnet : As described in , the current utterance is fed to a [[ memory network ]] , << where >> the [[ memories ]] correspond to preceding utterances .",1
7477,4159,"Memnet : As described in , the current utterance is fed to a [[ memory network ]] , << where >> the memories correspond to [[ preceding utterances ]] .",0
7478,4159,"Memnet : As described in , the current utterance is fed to a memory network , << where >> the [[ memories ]] correspond to [[ preceding utterances ]] .",0
7479,4159,"[[ Memnet ]] : As described in , the [[ current utterance ]] is fed to a memory network , where the memories << correspond to >> preceding utterances .",0
7480,4159,"[[ Memnet ]] : As described in , the current utterance is fed to a [[ memory network ]] , where the memories << correspond to >> preceding utterances .",0
7481,4159,"[[ Memnet ]] : As described in , the current utterance is fed to a memory network , where the [[ memories ]] << correspond to >> preceding utterances .",0
7482,4159,"[[ Memnet ]] : As described in , the current utterance is fed to a memory network , where the memories << correspond to >> [[ preceding utterances ]] .",0
7483,4159,"Memnet : As described in , the [[ current utterance ]] is fed to a [[ memory network ]] , where the memories << correspond to >> preceding utterances .",0
7484,4159,"Memnet : As described in , the [[ current utterance ]] is fed to a memory network , where the [[ memories ]] << correspond to >> preceding utterances .",0
7485,4159,"Memnet : As described in , the [[ current utterance ]] is fed to a memory network , where the memories << correspond to >> [[ preceding utterances ]] .",0
7486,4159,"Memnet : As described in , the current utterance is fed to a [[ memory network ]] , where the [[ memories ]] << correspond to >> preceding utterances .",0
7487,4159,"Memnet : As described in , the current utterance is fed to a [[ memory network ]] , where the memories << correspond to >> [[ preceding utterances ]] .",0
7488,4159,"Memnet : As described in , the current utterance is fed to a memory network , where the [[ memories ]] << correspond to >> [[ preceding utterances ]] .",1
7489,4509,"<< Without the large amount of >> [[ sentiment lexicons ]] , [[ SVM ]] perform worse than neural methods .",0
7490,4509,"<< Without the large amount of >> [[ sentiment lexicons ]] , SVM perform [[ worse ]] than neural methods .",0
7491,4509,"<< Without the large amount of >> [[ sentiment lexicons ]] , SVM perform worse than [[ neural methods ]] .",0
7492,4509,"<< Without the large amount of >> sentiment lexicons , [[ SVM ]] perform [[ worse ]] than neural methods .",0
7493,4509,"<< Without the large amount of >> sentiment lexicons , [[ SVM ]] perform worse than [[ neural methods ]] .",0
7494,4509,"<< Without the large amount of >> sentiment lexicons , SVM perform [[ worse ]] than [[ neural methods ]] .",0
7495,4509,"Without the large amount of [[ sentiment lexicons ]] , [[ SVM ]] << perform >> worse than neural methods .",0
7496,4509,"Without the large amount of [[ sentiment lexicons ]] , SVM << perform >> [[ worse ]] than neural methods .",0
7497,4509,"Without the large amount of [[ sentiment lexicons ]] , SVM << perform >> worse than [[ neural methods ]] .",0
7498,4509,"Without the large amount of sentiment lexicons , [[ SVM ]] << perform >> [[ worse ]] than neural methods .",1
7499,4509,"Without the large amount of sentiment lexicons , [[ SVM ]] << perform >> worse than [[ neural methods ]] .",0
7500,4509,"Without the large amount of sentiment lexicons , SVM << perform >> [[ worse ]] than [[ neural methods ]] .",0
7501,4509,"Without the large amount of [[ sentiment lexicons ]] , [[ SVM ]] perform worse << than >> neural methods .",0
7502,4509,"Without the large amount of [[ sentiment lexicons ]] , SVM perform [[ worse ]] << than >> neural methods .",0
7503,4509,"Without the large amount of [[ sentiment lexicons ]] , SVM perform worse << than >> [[ neural methods ]] .",0
7504,4509,"Without the large amount of sentiment lexicons , [[ SVM ]] perform [[ worse ]] << than >> neural methods .",0
7505,4509,"Without the large amount of sentiment lexicons , [[ SVM ]] perform worse << than >> [[ neural methods ]] .",0
7506,4509,"Without the large amount of sentiment lexicons , SVM perform [[ worse ]] << than >> [[ neural methods ]] .",1
7507,3394,The << first observation >> is that [[ our model architecture ]] achieves [[ much better results ]] compared to the previous state - of - the - art methods .,0
7508,3394,The << first observation >> is that [[ our model architecture ]] achieves much better results compared to the [[ previous state - of - the - art methods ]] .,0
7509,3394,The << first observation >> is that our model architecture achieves [[ much better results ]] compared to the [[ previous state - of - the - art methods ]] .,0
7510,3394,The first observation is that [[ our model architecture ]] << achieves >> [[ much better results ]] compared to the previous state - of - the - art methods .,1
7511,3394,The first observation is that [[ our model architecture ]] << achieves >> much better results compared to the [[ previous state - of - the - art methods ]] .,0
7512,3394,The first observation is that our model architecture << achieves >> [[ much better results ]] compared to the [[ previous state - of - the - art methods ]] .,0
7513,3394,The first observation is that [[ our model architecture ]] achieves [[ much better results ]] << compared to >> the previous state - of - the - art methods .,0
7514,3394,The first observation is that [[ our model architecture ]] achieves much better results << compared to >> the [[ previous state - of - the - art methods ]] .,0
7515,3394,The first observation is that our model architecture achieves [[ much better results ]] << compared to >> the [[ previous state - of - the - art methods ]] .,1
7516,884,"Finally , our result << on >> the [[ official test set ]] is [[ 76.2/84.6 ]] , which significantly outperforms the best documented result 73.2/81.8 .",0
7517,884,"Finally , our result << on >> the [[ official test set ]] is 76.2/84.6 , which [[ significantly outperforms ]] the best documented result 73.2/81.8 .",0
7518,884,"Finally , our result << on >> the [[ official test set ]] is 76.2/84.6 , which significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
7519,884,"Finally , our result << on >> the official test set is [[ 76.2/84.6 ]] , which [[ significantly outperforms ]] the best documented result 73.2/81.8 .",0
7520,884,"Finally , our result << on >> the official test set is [[ 76.2/84.6 ]] , which significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
7521,884,"Finally , our result << on >> the official test set is 76.2/84.6 , which [[ significantly outperforms ]] the [[ best documented result 73.2/81.8 ]] .",0
7522,884,"Finally , our result on the [[ official test set ]] << is >> [[ 76.2/84.6 ]] , which significantly outperforms the best documented result 73.2/81.8 .",1
7523,884,"Finally , our result on the [[ official test set ]] << is >> 76.2/84.6 , which [[ significantly outperforms ]] the best documented result 73.2/81.8 .",0
7524,884,"Finally , our result on the [[ official test set ]] << is >> 76.2/84.6 , which significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
7525,884,"Finally , our result on the official test set << is >> [[ 76.2/84.6 ]] , which [[ significantly outperforms ]] the best documented result 73.2/81.8 .",0
7526,884,"Finally , our result on the official test set << is >> [[ 76.2/84.6 ]] , which significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
7527,884,"Finally , our result on the official test set << is >> 76.2/84.6 , which [[ significantly outperforms ]] the [[ best documented result 73.2/81.8 ]] .",0
7528,884,"Finally , our result on the [[ official test set ]] is [[ 76.2/84.6 ]] , << which >> significantly outperforms the best documented result 73.2/81.8 .",0
7529,884,"Finally , our result on the [[ official test set ]] is 76.2/84.6 , << which >> [[ significantly outperforms ]] the best documented result 73.2/81.8 .",0
7530,884,"Finally , our result on the [[ official test set ]] is 76.2/84.6 , << which >> significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
7531,884,"Finally , our result on the official test set is [[ 76.2/84.6 ]] , << which >> [[ significantly outperforms ]] the best documented result 73.2/81.8 .",1
7532,884,"Finally , our result on the official test set is [[ 76.2/84.6 ]] , << which >> significantly outperforms the [[ best documented result 73.2/81.8 ]] .",0
7533,884,"Finally , our result on the official test set is 76.2/84.6 , << which >> [[ significantly outperforms ]] the [[ best documented result 73.2/81.8 ]] .",0
7534,5354,"[[ SP + ILP ]] << outperformed >> [[ CAEVO ]] and if additional unlabeled dataset TE3 - SV was used , CoDL + ILP achieved the best score with a relative improvement in F 1 score being 6.3 % .",1
7535,3996,"In this paper , we propose a [[ method ]] << to mitigate >> the [[ possible problems ]] when using translated sentences as context based on the following observations .",1
7536,3996,"In this paper , we propose a [[ method ]] << to mitigate >> the possible problems when using [[ translated sentences ]] as context based on the following observations .",0
7537,3996,"In this paper , we propose a [[ method ]] << to mitigate >> the possible problems when using translated sentences as [[ context ]] based on the following observations .",0
7538,3996,"In this paper , we propose a method << to mitigate >> the [[ possible problems ]] when using [[ translated sentences ]] as context based on the following observations .",0
7539,3996,"In this paper , we propose a method << to mitigate >> the [[ possible problems ]] when using translated sentences as [[ context ]] based on the following observations .",0
7540,3996,"In this paper , we propose a method << to mitigate >> the possible problems when using [[ translated sentences ]] as [[ context ]] based on the following observations .",0
7541,3996,"In this paper , we propose a [[ method ]] to mitigate the [[ possible problems ]] << when using >> translated sentences as context based on the following observations .",0
7542,3996,"In this paper , we propose a [[ method ]] to mitigate the possible problems << when using >> [[ translated sentences ]] as context based on the following observations .",0
7543,3996,"In this paper , we propose a [[ method ]] to mitigate the possible problems << when using >> translated sentences as [[ context ]] based on the following observations .",0
7544,3996,"In this paper , we propose a method to mitigate the [[ possible problems ]] << when using >> [[ translated sentences ]] as context based on the following observations .",1
7545,3996,"In this paper , we propose a method to mitigate the [[ possible problems ]] << when using >> translated sentences as [[ context ]] based on the following observations .",0
7546,3996,"In this paper , we propose a method to mitigate the possible problems << when using >> [[ translated sentences ]] as [[ context ]] based on the following observations .",0
7547,3996,"In this paper , we propose a [[ method ]] to mitigate the [[ possible problems ]] when using translated sentences << as >> context based on the following observations .",0
7548,3996,"In this paper , we propose a [[ method ]] to mitigate the possible problems when using [[ translated sentences ]] << as >> context based on the following observations .",0
7549,3996,"In this paper , we propose a [[ method ]] to mitigate the possible problems when using translated sentences << as >> [[ context ]] based on the following observations .",0
7550,3996,"In this paper , we propose a method to mitigate the [[ possible problems ]] when using [[ translated sentences ]] << as >> context based on the following observations .",0
7551,3996,"In this paper , we propose a method to mitigate the [[ possible problems ]] when using translated sentences << as >> [[ context ]] based on the following observations .",0
7552,3996,"In this paper , we propose a method to mitigate the possible problems when using [[ translated sentences ]] << as >> [[ context ]] based on the following observations .",1
7553,4752,"Further , [[ both AE - LSTM and ATAE - LSTM ]] << stably exceed >> the [[ TD - LSTM method ]] because of the introduction of attention mechanism .",1
7554,1879,"Specifically , << compared to >> the [[ last best models ]] , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses far fewer parameters with better performance .",0
7555,1879,"Specifically , << compared to >> the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters with better performance .",0
7556,1879,"Specifically , << compared to >> the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters with better performance .",0
7557,1879,"Specifically , << compared to >> the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] with better performance .",0
7558,1879,"Specifically , << compared to >> the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses far fewer parameters with [[ better performance ]] .",0
7559,1879,"Specifically , << compared to >> the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters with better performance .",0
7560,1879,"Specifically , << compared to >> the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters with better performance .",0
7561,1879,"Specifically , << compared to >> the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] with better performance .",0
7562,1879,"Specifically , << compared to >> the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses far fewer parameters with [[ better performance ]] .",0
7563,1879,"Specifically , << compared to >> the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , [[ ReSAN ]] uses far fewer parameters with better performance .",0
7564,1879,"Specifically , << compared to >> the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses [[ far fewer parameters ]] with better performance .",0
7565,1879,"Specifically , << compared to >> the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters with [[ better performance ]] .",0
7566,1879,"Specifically , << compared to >> the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses [[ far fewer parameters ]] with better performance .",0
7567,1879,"Specifically , << compared to >> the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters with [[ better performance ]] .",0
7568,1879,"Specifically , << compared to >> the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] with [[ better performance ]] .",0
7569,1879,"Specifically , compared to the [[ last best models ]] , << i.e. >> , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses far fewer parameters with better performance .",1
7570,1879,"Specifically , compared to the [[ last best models ]] , << i.e. >> , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters with better performance .",1
7571,1879,"Specifically , compared to the [[ last best models ]] , << i.e. >> , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters with better performance .",0
7572,1879,"Specifically , compared to the [[ last best models ]] , << i.e. >> , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] with better performance .",0
7573,1879,"Specifically , compared to the [[ last best models ]] , << i.e. >> , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses far fewer parameters with [[ better performance ]] .",0
7574,1879,"Specifically , compared to the last best models , << i.e. >> , [[ 600D Gumbel TreeLSTM encoders ]] and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters with better performance .",0
7575,1879,"Specifically , compared to the last best models , << i.e. >> , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters with better performance .",0
7576,1879,"Specifically , compared to the last best models , << i.e. >> , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] with better performance .",0
7577,1879,"Specifically , compared to the last best models , << i.e. >> , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses far fewer parameters with [[ better performance ]] .",0
7578,1879,"Specifically , compared to the last best models , << i.e. >> , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , [[ ReSAN ]] uses far fewer parameters with better performance .",0
7579,1879,"Specifically , compared to the last best models , << i.e. >> , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses [[ far fewer parameters ]] with better performance .",0
7580,1879,"Specifically , compared to the last best models , << i.e. >> , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters with [[ better performance ]] .",0
7581,1879,"Specifically , compared to the last best models , << i.e. >> , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses [[ far fewer parameters ]] with better performance .",0
7582,1879,"Specifically , compared to the last best models , << i.e. >> , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters with [[ better performance ]] .",0
7583,1879,"Specifically , compared to the last best models , << i.e. >> , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] with [[ better performance ]] .",0
7584,1879,"Specifically , compared to the [[ last best models ]] , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN << uses >> far fewer parameters with better performance .",0
7585,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN << uses >> far fewer parameters with better performance .",0
7586,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] << uses >> far fewer parameters with better performance .",0
7587,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN << uses >> [[ far fewer parameters ]] with better performance .",0
7588,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN << uses >> far fewer parameters with [[ better performance ]] .",0
7589,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and [[ 600D Residual stacked encoders ]] , ReSAN << uses >> far fewer parameters with better performance .",0
7590,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , [[ ReSAN ]] << uses >> far fewer parameters with better performance .",0
7591,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN << uses >> [[ far fewer parameters ]] with better performance .",0
7592,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN << uses >> far fewer parameters with [[ better performance ]] .",0
7593,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , [[ ReSAN ]] << uses >> far fewer parameters with better performance .",0
7594,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN << uses >> [[ far fewer parameters ]] with better performance .",0
7595,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN << uses >> far fewer parameters with [[ better performance ]] .",0
7596,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] << uses >> [[ far fewer parameters ]] with better performance .",1
7597,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] << uses >> far fewer parameters with [[ better performance ]] .",0
7598,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN << uses >> [[ far fewer parameters ]] with [[ better performance ]] .",0
7599,1879,"Specifically , compared to the [[ last best models ]] , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses far fewer parameters << with >> better performance .",0
7600,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters << with >> better performance .",0
7601,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters << with >> better performance .",0
7602,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] << with >> better performance .",0
7603,1879,"Specifically , compared to the [[ last best models ]] , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses far fewer parameters << with >> [[ better performance ]] .",0
7604,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters << with >> better performance .",0
7605,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters << with >> better performance .",0
7606,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] << with >> better performance .",0
7607,1879,"Specifically , compared to the last best models , i.e. , [[ 600D Gumbel TreeLSTM encoders ]] and 600D Residual stacked encoders , ReSAN uses far fewer parameters << with >> [[ better performance ]] .",0
7608,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , [[ ReSAN ]] uses far fewer parameters << with >> better performance .",0
7609,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses [[ far fewer parameters ]] << with >> better performance .",0
7610,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and [[ 600D Residual stacked encoders ]] , ReSAN uses far fewer parameters << with >> [[ better performance ]] .",0
7611,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses [[ far fewer parameters ]] << with >> better performance .",0
7612,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , [[ ReSAN ]] uses far fewer parameters << with >> [[ better performance ]] .",1
7613,1879,"Specifically , compared to the last best models , i.e. , 600D Gumbel TreeLSTM encoders and 600D Residual stacked encoders , ReSAN uses [[ far fewer parameters ]] << with >> [[ better performance ]] .",0
7614,5677,"The results indicate the [[ proposed RankGAN ]] << is able to capture >> the [[ transition pattern ]] among the words , even if the training sentences are novel , delicate and complicated .",1
7615,5677,"The results indicate the [[ proposed RankGAN ]] << is able to capture >> the transition pattern among the [[ words ]] , even if the training sentences are novel , delicate and complicated .",0
7616,5677,"The results indicate the proposed RankGAN << is able to capture >> the [[ transition pattern ]] among the [[ words ]] , even if the training sentences are novel , delicate and complicated .",0
7617,5677,"The results indicate the [[ proposed RankGAN ]] is able to capture the [[ transition pattern ]] << among >> the words , even if the training sentences are novel , delicate and complicated .",0
7618,5677,"The results indicate the [[ proposed RankGAN ]] is able to capture the transition pattern << among >> the [[ words ]] , even if the training sentences are novel , delicate and complicated .",0
7619,5677,"The results indicate the proposed RankGAN is able to capture the [[ transition pattern ]] << among >> the [[ words ]] , even if the training sentences are novel , delicate and complicated .",1
7620,1485,We << run >> [[ our experiments ]] on a [[ machine ]] that contains a single GTX 1080 GPU with 8 GB VRAM .,0
7621,1485,We << run >> [[ our experiments ]] on a machine that contains a [[ single GTX 1080 GPU ]] with 8 GB VRAM .,0
7622,1485,We << run >> [[ our experiments ]] on a machine that contains a single GTX 1080 GPU with [[ 8 GB VRAM ]] .,0
7623,1485,We << run >> our experiments on a [[ machine ]] that contains a [[ single GTX 1080 GPU ]] with 8 GB VRAM .,0
7624,1485,We << run >> our experiments on a [[ machine ]] that contains a single GTX 1080 GPU with [[ 8 GB VRAM ]] .,0
7625,1485,We << run >> our experiments on a machine that contains a [[ single GTX 1080 GPU ]] with [[ 8 GB VRAM ]] .,0
7626,1485,We run [[ our experiments ]] << on >> a [[ machine ]] that contains a single GTX 1080 GPU with 8 GB VRAM .,1
7627,1485,We run [[ our experiments ]] << on >> a machine that contains a [[ single GTX 1080 GPU ]] with 8 GB VRAM .,0
7628,1485,We run [[ our experiments ]] << on >> a machine that contains a single GTX 1080 GPU with [[ 8 GB VRAM ]] .,0
7629,1485,We run our experiments << on >> a [[ machine ]] that contains a [[ single GTX 1080 GPU ]] with 8 GB VRAM .,0
7630,1485,We run our experiments << on >> a [[ machine ]] that contains a single GTX 1080 GPU with [[ 8 GB VRAM ]] .,0
7631,1485,We run our experiments << on >> a machine that contains a [[ single GTX 1080 GPU ]] with [[ 8 GB VRAM ]] .,0
7632,1485,We run [[ our experiments ]] on a [[ machine ]] that << contains >> a single GTX 1080 GPU with 8 GB VRAM .,0
7633,1485,We run [[ our experiments ]] on a machine that << contains >> a [[ single GTX 1080 GPU ]] with 8 GB VRAM .,0
7634,1485,We run [[ our experiments ]] on a machine that << contains >> a single GTX 1080 GPU with [[ 8 GB VRAM ]] .,0
7635,1485,We run our experiments on a [[ machine ]] that << contains >> a [[ single GTX 1080 GPU ]] with 8 GB VRAM .,1
7636,1485,We run our experiments on a [[ machine ]] that << contains >> a single GTX 1080 GPU with [[ 8 GB VRAM ]] .,0
7637,1485,We run our experiments on a machine that << contains >> a [[ single GTX 1080 GPU ]] with [[ 8 GB VRAM ]] .,0
7638,1485,We run [[ our experiments ]] on a [[ machine ]] that contains a single GTX 1080 GPU << with >> 8 GB VRAM .,0
7639,1485,We run [[ our experiments ]] on a machine that contains a [[ single GTX 1080 GPU ]] << with >> 8 GB VRAM .,0
7640,1485,We run [[ our experiments ]] on a machine that contains a single GTX 1080 GPU << with >> [[ 8 GB VRAM ]] .,0
7641,1485,We run our experiments on a [[ machine ]] that contains a [[ single GTX 1080 GPU ]] << with >> 8 GB VRAM .,0
7642,1485,We run our experiments on a [[ machine ]] that contains a single GTX 1080 GPU << with >> [[ 8 GB VRAM ]] .,0
7643,1485,We run our experiments on a machine that contains a [[ single GTX 1080 GPU ]] << with >> [[ 8 GB VRAM ]] .,1
7644,2417,"Not surprisingly , the [[ n-gram functionality ]] << is >> [[ important ]] , contributing almost 5 % accuracy improvement .",1
7645,2417,"Not surprisingly , the [[ n-gram functionality ]] << is >> important , contributing [[ almost 5 % accuracy improvement ]] .",0
7646,2417,"Not surprisingly , the n-gram functionality << is >> [[ important ]] , contributing [[ almost 5 % accuracy improvement ]] .",0
7647,2417,"Not surprisingly , the [[ n-gram functionality ]] is [[ important ]] , << contributing >> almost 5 % accuracy improvement .",0
7648,2417,"Not surprisingly , the [[ n-gram functionality ]] is important , << contributing >> [[ almost 5 % accuracy improvement ]] .",1
7649,2417,"Not surprisingly , the n-gram functionality is [[ important ]] , << contributing >> [[ almost 5 % accuracy improvement ]] .",0
7650,3368,We used [[ eight NVIDIA V100 ( 32GB ) GPUs ]] << for >> the [[ pre-training ]] .,1
7651,2808,"Second , in sections 3 and 4 , we present an [[ embedding - based QA system ]] << developed under >> the [[ framework of Memory Networks ( Mem NNs ) ]] .",1
7652,2468,"<< Among >> [[ all the feature ablations ]] , the [[ Part - Of - Speech ]] , Exact Match , Qtype features drop much more than the other features , which shows the importance of these three features .",0
7653,2468,"<< Among >> [[ all the feature ablations ]] , the Part - Of - Speech , [[ Exact Match ]] , Qtype features drop much more than the other features , which shows the importance of these three features .",0
7654,2468,"<< Among >> [[ all the feature ablations ]] , the Part - Of - Speech , Exact Match , [[ Qtype features ]] drop much more than the other features , which shows the importance of these three features .",0
7655,2468,"<< Among >> [[ all the feature ablations ]] , the Part - Of - Speech , Exact Match , Qtype features [[ drop ]] much more than the other features , which shows the importance of these three features .",0
7656,2468,"<< Among >> [[ all the feature ablations ]] , the Part - Of - Speech , Exact Match , Qtype features drop much more than the [[ other features ]] , which shows the importance of these three features .",0
7657,2468,"<< Among >> all the feature ablations , the [[ Part - Of - Speech ]] , [[ Exact Match ]] , Qtype features drop much more than the other features , which shows the importance of these three features .",0
7658,2468,"<< Among >> all the feature ablations , the [[ Part - Of - Speech ]] , Exact Match , [[ Qtype features ]] drop much more than the other features , which shows the importance of these three features .",0
7659,2468,"<< Among >> all the feature ablations , the [[ Part - Of - Speech ]] , Exact Match , Qtype features [[ drop ]] much more than the other features , which shows the importance of these three features .",0
7660,2468,"<< Among >> all the feature ablations , the [[ Part - Of - Speech ]] , Exact Match , Qtype features drop much more than the [[ other features ]] , which shows the importance of these three features .",0
7661,2468,"<< Among >> all the feature ablations , the Part - Of - Speech , [[ Exact Match ]] , [[ Qtype features ]] drop much more than the other features , which shows the importance of these three features .",0
7662,2468,"<< Among >> all the feature ablations , the Part - Of - Speech , [[ Exact Match ]] , Qtype features [[ drop ]] much more than the other features , which shows the importance of these three features .",0
7663,2468,"<< Among >> all the feature ablations , the Part - Of - Speech , [[ Exact Match ]] , Qtype features drop much more than the [[ other features ]] , which shows the importance of these three features .",0
7664,2468,"<< Among >> all the feature ablations , the Part - Of - Speech , Exact Match , [[ Qtype features ]] [[ drop ]] much more than the other features , which shows the importance of these three features .",0
7665,2468,"<< Among >> all the feature ablations , the Part - Of - Speech , Exact Match , [[ Qtype features ]] drop much more than the [[ other features ]] , which shows the importance of these three features .",0
7666,2468,"<< Among >> all the feature ablations , the Part - Of - Speech , Exact Match , Qtype features [[ drop ]] much more than the [[ other features ]] , which shows the importance of these three features .",0
7667,2468,"Among [[ all the feature ablations ]] , the [[ Part - Of - Speech ]] , Exact Match , Qtype features drop << much more than >> the other features , which shows the importance of these three features .",0
7668,2468,"Among [[ all the feature ablations ]] , the Part - Of - Speech , [[ Exact Match ]] , Qtype features drop << much more than >> the other features , which shows the importance of these three features .",0
7669,2468,"Among [[ all the feature ablations ]] , the Part - Of - Speech , Exact Match , [[ Qtype features ]] drop << much more than >> the other features , which shows the importance of these three features .",0
7670,2468,"Among [[ all the feature ablations ]] , the Part - Of - Speech , Exact Match , Qtype features [[ drop ]] << much more than >> the other features , which shows the importance of these three features .",0
7671,2468,"Among [[ all the feature ablations ]] , the Part - Of - Speech , Exact Match , Qtype features drop << much more than >> the [[ other features ]] , which shows the importance of these three features .",0
7672,2468,"Among all the feature ablations , the [[ Part - Of - Speech ]] , [[ Exact Match ]] , Qtype features drop << much more than >> the other features , which shows the importance of these three features .",0
7673,2468,"Among all the feature ablations , the [[ Part - Of - Speech ]] , Exact Match , [[ Qtype features ]] drop << much more than >> the other features , which shows the importance of these three features .",0
7674,2468,"Among all the feature ablations , the [[ Part - Of - Speech ]] , Exact Match , Qtype features [[ drop ]] << much more than >> the other features , which shows the importance of these three features .",0
7675,2468,"Among all the feature ablations , the [[ Part - Of - Speech ]] , Exact Match , Qtype features drop << much more than >> the [[ other features ]] , which shows the importance of these three features .",0
7676,2468,"Among all the feature ablations , the Part - Of - Speech , [[ Exact Match ]] , [[ Qtype features ]] drop << much more than >> the other features , which shows the importance of these three features .",0
7677,2468,"Among all the feature ablations , the Part - Of - Speech , [[ Exact Match ]] , Qtype features [[ drop ]] << much more than >> the other features , which shows the importance of these three features .",0
7678,2468,"Among all the feature ablations , the Part - Of - Speech , [[ Exact Match ]] , Qtype features drop << much more than >> the [[ other features ]] , which shows the importance of these three features .",0
7679,2468,"Among all the feature ablations , the Part - Of - Speech , Exact Match , [[ Qtype features ]] [[ drop ]] << much more than >> the other features , which shows the importance of these three features .",0
7680,2468,"Among all the feature ablations , the Part - Of - Speech , Exact Match , [[ Qtype features ]] drop << much more than >> the [[ other features ]] , which shows the importance of these three features .",0
7681,2468,"Among all the feature ablations , the Part - Of - Speech , Exact Match , Qtype features [[ drop ]] << much more than >> the [[ other features ]] , which shows the importance of these three features .",1
7682,5115,"We << use >> [[ GloVe 2 vectors ]] with [[ 300 dimensions ]] to initialize the word embeddings , the same as .",0
7683,5115,"We << use >> [[ GloVe 2 vectors ]] with 300 dimensions to initialize the [[ word embeddings ]] , the same as .",0
7684,5115,"We << use >> GloVe 2 vectors with [[ 300 dimensions ]] to initialize the [[ word embeddings ]] , the same as .",0
7685,5115,"We use [[ GloVe 2 vectors ]] << with >> [[ 300 dimensions ]] to initialize the word embeddings , the same as .",1
7686,5115,"We use [[ GloVe 2 vectors ]] << with >> 300 dimensions to initialize the [[ word embeddings ]] , the same as .",0
7687,5115,"We use GloVe 2 vectors << with >> [[ 300 dimensions ]] to initialize the [[ word embeddings ]] , the same as .",0
7688,5115,"We use [[ GloVe 2 vectors ]] with [[ 300 dimensions ]] << to initialize >> the word embeddings , the same as .",0
7689,5115,"We use [[ GloVe 2 vectors ]] with 300 dimensions << to initialize >> the [[ word embeddings ]] , the same as .",0
7690,5115,"We use GloVe 2 vectors with [[ 300 dimensions ]] << to initialize >> the [[ word embeddings ]] , the same as .",1
7691,5656,"Specifically , the << proposed >> [[ new adversarial network ]] consists of [[ two neural network models ]] , a generator and a ranker .",0
7692,5656,"Specifically , the << proposed >> [[ new adversarial network ]] consists of two neural network models , [[ a generator and a ranker ]] .",0
7693,5656,"Specifically , the << proposed >> new adversarial network consists of [[ two neural network models ]] , [[ a generator and a ranker ]] .",0
7694,5656,"Specifically , the proposed [[ new adversarial network ]] << consists of >> [[ two neural network models ]] , a generator and a ranker .",1
7695,5656,"Specifically , the proposed [[ new adversarial network ]] << consists of >> two neural network models , [[ a generator and a ranker ]] .",0
7696,5656,"Specifically , the proposed new adversarial network << consists of >> [[ two neural network models ]] , [[ a generator and a ranker ]] .",0
7697,696,"<< In conjunction with >> a [[ directional mask ]] , the [[ distance mask ]] allows us to incorporate complete positional information of words in our model .",0
7698,696,"<< In conjunction with >> a [[ directional mask ]] , the distance mask allows us to incorporate [[ complete positional information of words ]] in our model .",0
7699,696,"<< In conjunction with >> a [[ directional mask ]] , the distance mask allows us to incorporate complete positional information of words in [[ our model ]] .",0
7700,696,"<< In conjunction with >> a directional mask , the [[ distance mask ]] allows us to incorporate [[ complete positional information of words ]] in our model .",0
7701,696,"<< In conjunction with >> a directional mask , the [[ distance mask ]] allows us to incorporate complete positional information of words in [[ our model ]] .",0
7702,696,"<< In conjunction with >> a directional mask , the distance mask allows us to incorporate [[ complete positional information of words ]] in [[ our model ]] .",0
7703,696,"In conjunction with a [[ directional mask ]] , the [[ distance mask ]] << allows us to incorporate >> complete positional information of words in our model .",0
7704,696,"In conjunction with a [[ directional mask ]] , the distance mask << allows us to incorporate >> [[ complete positional information of words ]] in our model .",0
7705,696,"In conjunction with a [[ directional mask ]] , the distance mask << allows us to incorporate >> complete positional information of words in [[ our model ]] .",0
7706,696,"In conjunction with a directional mask , the [[ distance mask ]] << allows us to incorporate >> [[ complete positional information of words ]] in our model .",1
7707,696,"In conjunction with a directional mask , the [[ distance mask ]] << allows us to incorporate >> complete positional information of words in [[ our model ]] .",0
7708,696,"In conjunction with a directional mask , the distance mask << allows us to incorporate >> [[ complete positional information of words ]] in [[ our model ]] .",0
7709,696,"In conjunction with a [[ directional mask ]] , the [[ distance mask ]] allows us to incorporate complete positional information of words << in >> our model .",0
7710,696,"In conjunction with a [[ directional mask ]] , the distance mask allows us to incorporate [[ complete positional information of words ]] << in >> our model .",0
7711,696,"In conjunction with a [[ directional mask ]] , the distance mask allows us to incorporate complete positional information of words << in >> [[ our model ]] .",0
7712,696,"In conjunction with a directional mask , the [[ distance mask ]] allows us to incorporate [[ complete positional information of words ]] << in >> our model .",0
7713,696,"In conjunction with a directional mask , the [[ distance mask ]] allows us to incorporate complete positional information of words << in >> [[ our model ]] .",0
7714,696,"In conjunction with a directional mask , the distance mask allows us to incorporate [[ complete positional information of words ]] << in >> [[ our model ]] .",1
7715,1741,"Second , it << represents >> [[ answer candidates ]] as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7716,1741,"Second , it << represents >> [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7717,1741,"Second , it << represents >> [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7718,1741,"Second , it << represents >> [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7719,1741,"Second , it << represents >> [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7720,1741,"Second , it << represents >> answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7721,1741,"Second , it << represents >> answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7722,1741,"Second , it << represents >> answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7723,1741,"Second , it << represents >> answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7724,1741,"Second , it << represents >> answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7725,1741,"Second , it << represents >> answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7726,1741,"Second , it << represents >> answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7727,1741,"Second , it << represents >> answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7728,1741,"Second , it << represents >> answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7729,1741,"Second , it << represents >> answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7730,1741,"Second , it represents [[ answer candidates ]] << as >> [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",1
7731,1741,"Second , it represents [[ answer candidates ]] << as >> chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7732,1741,"Second , it represents [[ answer candidates ]] << as >> chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7733,1741,"Second , it represents [[ answer candidates ]] << as >> chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7734,1741,"Second , it represents [[ answer candidates ]] << as >> chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7735,1741,"Second , it represents answer candidates << as >> [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7736,1741,"Second , it represents answer candidates << as >> [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7737,1741,"Second , it represents answer candidates << as >> [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7738,1741,"Second , it represents answer candidates << as >> [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7739,1741,"Second , it represents answer candidates << as >> chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7740,1741,"Second , it represents answer candidates << as >> chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7741,1741,"Second , it represents answer candidates << as >> chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7742,1741,"Second , it represents answer candidates << as >> chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7743,1741,"Second , it represents answer candidates << as >> chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7744,1741,"Second , it represents answer candidates << as >> chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7745,1741,"Second , it represents [[ answer candidates ]] as [[ chunks ]] , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7746,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , << instead of >> [[ word - level representations ]] , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7747,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7748,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7749,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7750,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , << instead of >> [[ word - level representations ]] , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",1
7751,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7752,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7753,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7754,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , << instead of >> [[ word - level representations ]] , to make the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7755,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , << instead of >> [[ word - level representations ]] , to make the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7756,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , << instead of >> [[ word - level representations ]] , to make the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7757,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the [[ model ]] aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7758,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the [[ model ]] aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7759,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , << instead of >> word - level representations , to make the model aware of the [[ subtle differences ]] among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7760,1741,"Second , it represents [[ answer candidates ]] as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7761,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , << to make >> the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7762,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7763,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7764,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7765,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , << to make >> the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7766,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",1
7767,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7768,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7769,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , << to make >> the [[ model ]] aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7770,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , << to make >> the model aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7771,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , << to make >> the model aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7772,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the [[ model ]] aware of the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7773,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the [[ model ]] aware of the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7774,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , << to make >> the model aware of the [[ subtle differences ]] among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7775,1741,"Second , it represents [[ answer candidates ]] as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model << aware of >> the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7776,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model << aware of >> the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7777,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] << aware of >> the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7778,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model << aware of >> the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7779,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model << aware of >> the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7780,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model << aware of >> the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7781,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] << aware of >> the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7782,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model << aware of >> the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7783,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model << aware of >> the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7784,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the [[ model ]] << aware of >> the subtle differences among candidates ( importantly , overlapping candidates ) .",0
7785,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model << aware of >> the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",0
7786,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model << aware of >> the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7787,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] << aware of >> the [[ subtle differences ]] among candidates ( importantly , overlapping candidates ) .",1
7788,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] << aware of >> the subtle differences among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7789,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model << aware of >> the [[ subtle differences ]] among [[ candidates ]] ( importantly , overlapping candidates ) .",0
7790,1741,"Second , it represents [[ answer candidates ]] as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences << among >> candidates ( importantly , overlapping candidates ) .",0
7791,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences << among >> candidates ( importantly , overlapping candidates ) .",0
7792,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences << among >> candidates ( importantly , overlapping candidates ) .",0
7793,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] << among >> candidates ( importantly , overlapping candidates ) .",0
7794,1741,"Second , it represents [[ answer candidates ]] as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences << among >> [[ candidates ]] ( importantly , overlapping candidates ) .",0
7795,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences << among >> candidates ( importantly , overlapping candidates ) .",0
7796,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences << among >> candidates ( importantly , overlapping candidates ) .",0
7797,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] << among >> candidates ( importantly , overlapping candidates ) .",0
7798,1741,"Second , it represents answer candidates as [[ chunks ]] , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences << among >> [[ candidates ]] ( importantly , overlapping candidates ) .",0
7799,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the [[ model ]] aware of the subtle differences << among >> candidates ( importantly , overlapping candidates ) .",0
7800,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the [[ subtle differences ]] << among >> candidates ( importantly , overlapping candidates ) .",0
7801,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of [[ word - level representations ]] , to make the model aware of the subtle differences << among >> [[ candidates ]] ( importantly , overlapping candidates ) .",0
7802,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the [[ subtle differences ]] << among >> candidates ( importantly , overlapping candidates ) .",0
7803,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the [[ model ]] aware of the subtle differences << among >> [[ candidates ]] ( importantly , overlapping candidates ) .",0
7804,1741,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the [[ subtle differences ]] << among >> [[ candidates ]] ( importantly , overlapping candidates ) .",1
7805,3718,"We use a [[ grid search ]] << to determine >> the [[ optimal parameters ]] and manually specify subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",1
7806,3718,"We use a [[ grid search ]] << to determine >> the optimal parameters and manually specify [[ subsets ]] of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7807,3718,"We use a [[ grid search ]] << to determine >> the optimal parameters and manually specify subsets of the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7808,3718,"We use a [[ grid search ]] << to determine >> the optimal parameters and manually specify subsets of the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7809,3718,"We use a grid search << to determine >> the [[ optimal parameters ]] and manually specify [[ subsets ]] of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7810,3718,"We use a grid search << to determine >> the [[ optimal parameters ]] and manually specify subsets of the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7811,3718,"We use a grid search << to determine >> the [[ optimal parameters ]] and manually specify subsets of the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7812,3718,"We use a grid search << to determine >> the optimal parameters and manually specify [[ subsets ]] of the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7813,3718,"We use a grid search << to determine >> the optimal parameters and manually specify [[ subsets ]] of the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7814,3718,"We use a grid search << to determine >> the optimal parameters and manually specify subsets of the [[ parameter spaces ]] : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7815,3718,"We use a [[ grid search ]] to determine the [[ optimal parameters ]] and << manually specify >> subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7816,3718,"We use a [[ grid search ]] to determine the optimal parameters and << manually specify >> [[ subsets ]] of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",1
7817,3718,"We use a [[ grid search ]] to determine the optimal parameters and << manually specify >> subsets of the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7818,3718,"We use a [[ grid search ]] to determine the optimal parameters and << manually specify >> subsets of the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7819,3718,"We use a grid search to determine the [[ optimal parameters ]] and << manually specify >> [[ subsets ]] of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7820,3718,"We use a grid search to determine the [[ optimal parameters ]] and << manually specify >> subsets of the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7821,3718,"We use a grid search to determine the [[ optimal parameters ]] and << manually specify >> subsets of the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7822,3718,"We use a grid search to determine the optimal parameters and << manually specify >> [[ subsets ]] of the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7823,3718,"We use a grid search to determine the optimal parameters and << manually specify >> [[ subsets ]] of the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7824,3718,"We use a grid search to determine the optimal parameters and << manually specify >> subsets of the [[ parameter spaces ]] : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7825,3718,"We use a [[ grid search ]] to determine the [[ optimal parameters ]] and manually specify subsets << of >> the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7826,3718,"We use a [[ grid search ]] to determine the optimal parameters and manually specify [[ subsets ]] << of >> the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7827,3718,"We use a [[ grid search ]] to determine the optimal parameters and manually specify subsets << of >> the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7828,3718,"We use a [[ grid search ]] to determine the optimal parameters and manually specify subsets << of >> the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7829,3718,"We use a grid search to determine the [[ optimal parameters ]] and manually specify [[ subsets ]] << of >> the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7830,3718,"We use a grid search to determine the [[ optimal parameters ]] and manually specify subsets << of >> the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7831,3718,"We use a grid search to determine the [[ optimal parameters ]] and manually specify subsets << of >> the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7832,3718,"We use a grid search to determine the optimal parameters and manually specify [[ subsets ]] << of >> the [[ parameter spaces ]] : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",1
7833,3718,"We use a grid search to determine the optimal parameters and manually specify [[ subsets ]] << of >> the parameter spaces : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7834,3718,"We use a grid search to determine the optimal parameters and manually specify subsets << of >> the [[ parameter spaces ]] : [[ w ? { 1 , 2 , 3 , , 7 } ]] and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",0
7835,811,"To alleviate this challenge , we << identify >> an [[ attention scoring function ]] utilizing [[ all layers ]] of representation with less training burden .",0
7836,811,"To alleviate this challenge , we << identify >> an [[ attention scoring function ]] utilizing all layers of [[ representation ]] with less training burden .",0
7837,811,"To alleviate this challenge , we << identify >> an [[ attention scoring function ]] utilizing all layers of representation with [[ less training burden ]] .",0
7838,811,"To alleviate this challenge , we << identify >> an attention scoring function utilizing [[ all layers ]] of [[ representation ]] with less training burden .",0
7839,811,"To alleviate this challenge , we << identify >> an attention scoring function utilizing [[ all layers ]] of representation with [[ less training burden ]] .",0
7840,811,"To alleviate this challenge , we << identify >> an attention scoring function utilizing all layers of [[ representation ]] with [[ less training burden ]] .",0
7841,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] << utilizing >> [[ all layers ]] of representation with less training burden .",1
7842,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] << utilizing >> all layers of [[ representation ]] with less training burden .",0
7843,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] << utilizing >> all layers of representation with [[ less training burden ]] .",0
7844,811,"To alleviate this challenge , we identify an attention scoring function << utilizing >> [[ all layers ]] of [[ representation ]] with less training burden .",0
7845,811,"To alleviate this challenge , we identify an attention scoring function << utilizing >> [[ all layers ]] of representation with [[ less training burden ]] .",0
7846,811,"To alleviate this challenge , we identify an attention scoring function << utilizing >> all layers of [[ representation ]] with [[ less training burden ]] .",0
7847,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] utilizing [[ all layers ]] << of >> representation with less training burden .",0
7848,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] utilizing all layers << of >> [[ representation ]] with less training burden .",0
7849,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] utilizing all layers << of >> representation with [[ less training burden ]] .",0
7850,811,"To alleviate this challenge , we identify an attention scoring function utilizing [[ all layers ]] << of >> [[ representation ]] with less training burden .",1
7851,811,"To alleviate this challenge , we identify an attention scoring function utilizing [[ all layers ]] << of >> representation with [[ less training burden ]] .",0
7852,811,"To alleviate this challenge , we identify an attention scoring function utilizing all layers << of >> [[ representation ]] with [[ less training burden ]] .",0
7853,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] utilizing [[ all layers ]] of representation << with >> less training burden .",0
7854,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] utilizing all layers of [[ representation ]] << with >> less training burden .",0
7855,811,"To alleviate this challenge , we identify an [[ attention scoring function ]] utilizing all layers of representation << with >> [[ less training burden ]] .",0
7856,811,"To alleviate this challenge , we identify an attention scoring function utilizing [[ all layers ]] of [[ representation ]] << with >> less training burden .",0
7857,811,"To alleviate this challenge , we identify an attention scoring function utilizing [[ all layers ]] of representation << with >> [[ less training burden ]] .",1
7858,811,"To alleviate this challenge , we identify an attention scoring function utilizing all layers of [[ representation ]] << with >> [[ less training burden ]] .",0
7859,3284,"<< For >> the [[ NER task ]] , we adopt the [[ BIO ( Beginning , Inside , Outside ) encoding scheme ]] .",0
7860,3284,"For the [[ NER task ]] , we << adopt >> the [[ BIO ( Beginning , Inside , Outside ) encoding scheme ]] .",1
7861,2189,"The resulting model , which we << term >> [[ Long Short - Term Memory - Network ( LSTMN ) ]] , is a [[ reading simulator ]] that can be used for sequence processing tasks .",0
7862,2189,"The resulting model , which we << term >> [[ Long Short - Term Memory - Network ( LSTMN ) ]] , is a reading simulator that can be used for [[ sequence processing tasks ]] .",0
7863,2189,"The resulting model , which we << term >> Long Short - Term Memory - Network ( LSTMN ) , is a [[ reading simulator ]] that can be used for [[ sequence processing tasks ]] .",0
7864,2189,"The resulting model , which we term [[ Long Short - Term Memory - Network ( LSTMN ) ]] , << is >> a [[ reading simulator ]] that can be used for sequence processing tasks .",1
7865,2189,"The resulting model , which we term [[ Long Short - Term Memory - Network ( LSTMN ) ]] , << is >> a reading simulator that can be used for [[ sequence processing tasks ]] .",0
7866,2189,"The resulting model , which we term Long Short - Term Memory - Network ( LSTMN ) , << is >> a [[ reading simulator ]] that can be used for [[ sequence processing tasks ]] .",0
7867,2189,"The resulting model , which we term [[ Long Short - Term Memory - Network ( LSTMN ) ]] , is a [[ reading simulator ]] that can be << used for >> sequence processing tasks .",0
7868,2189,"The resulting model , which we term [[ Long Short - Term Memory - Network ( LSTMN ) ]] , is a reading simulator that can be << used for >> [[ sequence processing tasks ]] .",0
7869,2189,"The resulting model , which we term Long Short - Term Memory - Network ( LSTMN ) , is a [[ reading simulator ]] that can be << used for >> [[ sequence processing tasks ]] .",1
7870,5006,We << use >> the [[ same Glove word vectors ]] for [[ fair comparison ]] .,0
7871,5006,We use the [[ same Glove word vectors ]] << for >> [[ fair comparison ]] .,1
7872,5617,"<< For >> [[ seen speakers on VCTK ]] , the [[ proposed model ]] performs about as well as the baseline which uses an embedding lookup table for speaker conditioning .",0
7873,5617,"<< For >> [[ seen speakers on VCTK ]] , the proposed model performs about as well as the [[ baseline ]] which uses an embedding lookup table for speaker conditioning .",0
7874,5617,"<< For >> [[ seen speakers on VCTK ]] , the proposed model performs about as well as the baseline which uses an [[ embedding lookup table for speaker conditioning ]] .",0
7875,5617,"<< For >> seen speakers on VCTK , the [[ proposed model ]] performs about as well as the [[ baseline ]] which uses an embedding lookup table for speaker conditioning .",0
7876,5617,"<< For >> seen speakers on VCTK , the [[ proposed model ]] performs about as well as the baseline which uses an [[ embedding lookup table for speaker conditioning ]] .",0
7877,5617,"<< For >> seen speakers on VCTK , the proposed model performs about as well as the [[ baseline ]] which uses an [[ embedding lookup table for speaker conditioning ]] .",0
7878,5617,"For [[ seen speakers on VCTK ]] , the [[ proposed model ]] << performs about as well as >> the baseline which uses an embedding lookup table for speaker conditioning .",0
7879,5617,"For [[ seen speakers on VCTK ]] , the proposed model << performs about as well as >> the [[ baseline ]] which uses an embedding lookup table for speaker conditioning .",0
7880,5617,"For [[ seen speakers on VCTK ]] , the proposed model << performs about as well as >> the baseline which uses an [[ embedding lookup table for speaker conditioning ]] .",0
7881,5617,"For seen speakers on VCTK , the [[ proposed model ]] << performs about as well as >> the [[ baseline ]] which uses an embedding lookup table for speaker conditioning .",1
7882,5617,"For seen speakers on VCTK , the [[ proposed model ]] << performs about as well as >> the baseline which uses an [[ embedding lookup table for speaker conditioning ]] .",0
7883,5617,"For seen speakers on VCTK , the proposed model << performs about as well as >> the [[ baseline ]] which uses an [[ embedding lookup table for speaker conditioning ]] .",0
7884,5617,"For [[ seen speakers on VCTK ]] , the [[ proposed model ]] performs about as well as the baseline which << uses >> an embedding lookup table for speaker conditioning .",0
7885,5617,"For [[ seen speakers on VCTK ]] , the proposed model performs about as well as the [[ baseline ]] which << uses >> an embedding lookup table for speaker conditioning .",0
7886,5617,"For [[ seen speakers on VCTK ]] , the proposed model performs about as well as the baseline which << uses >> an [[ embedding lookup table for speaker conditioning ]] .",0
7887,5617,"For seen speakers on VCTK , the [[ proposed model ]] performs about as well as the [[ baseline ]] which << uses >> an embedding lookup table for speaker conditioning .",0
7888,5617,"For seen speakers on VCTK , the [[ proposed model ]] performs about as well as the baseline which << uses >> an [[ embedding lookup table for speaker conditioning ]] .",0
7889,5617,"For seen speakers on VCTK , the proposed model performs about as well as the [[ baseline ]] which << uses >> an [[ embedding lookup table for speaker conditioning ]] .",1
7890,5145,"In addition , << by separating >> the [[ representation ]] of the [[ input sentence ]] , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7891,5145,"In addition , << by separating >> the [[ representation ]] of the input sentence , the [[ classifier ]] becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7892,5145,"In addition , << by separating >> the [[ representation ]] of the input sentence , the classifier becomes an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7893,5145,"In addition , << by separating >> the [[ representation ]] of the input sentence , the classifier becomes an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7894,5145,"In addition , << by separating >> the representation of the [[ input sentence ]] , the [[ classifier ]] becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7895,5145,"In addition , << by separating >> the representation of the [[ input sentence ]] , the classifier becomes an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7896,5145,"In addition , << by separating >> the representation of the [[ input sentence ]] , the classifier becomes an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7897,5145,"In addition , << by separating >> the representation of the input sentence , the [[ classifier ]] becomes an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7898,5145,"In addition , << by separating >> the representation of the input sentence , the [[ classifier ]] becomes an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7899,5145,"In addition , << by separating >> the representation of the input sentence , the classifier becomes an [[ independent module ]] in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7900,5145,"In addition , by separating the [[ representation ]] << of >> the [[ input sentence ]] , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",1
7901,5145,"In addition , by separating the [[ representation ]] << of >> the input sentence , the [[ classifier ]] becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7902,5145,"In addition , by separating the [[ representation ]] << of >> the input sentence , the classifier becomes an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7903,5145,"In addition , by separating the [[ representation ]] << of >> the input sentence , the classifier becomes an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7904,5145,"In addition , by separating the representation << of >> the [[ input sentence ]] , the [[ classifier ]] becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7905,5145,"In addition , by separating the representation << of >> the [[ input sentence ]] , the classifier becomes an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7906,5145,"In addition , by separating the representation << of >> the [[ input sentence ]] , the classifier becomes an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7907,5145,"In addition , by separating the representation << of >> the input sentence , the [[ classifier ]] becomes an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7908,5145,"In addition , by separating the representation << of >> the input sentence , the [[ classifier ]] becomes an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7909,5145,"In addition , by separating the representation << of >> the input sentence , the classifier becomes an [[ independent module ]] in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7910,5145,"In addition , by separating the [[ representation ]] of the [[ input sentence ]] , the classifier << becomes >> an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7911,5145,"In addition , by separating the [[ representation ]] of the input sentence , the [[ classifier ]] << becomes >> an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7912,5145,"In addition , by separating the [[ representation ]] of the input sentence , the classifier << becomes >> an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7913,5145,"In addition , by separating the [[ representation ]] of the input sentence , the classifier << becomes >> an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7914,5145,"In addition , by separating the representation of the [[ input sentence ]] , the [[ classifier ]] << becomes >> an independent module in our framework , which endows the method with the ability to integrate different classifiers .",0
7915,5145,"In addition , by separating the representation of the [[ input sentence ]] , the classifier << becomes >> an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",0
7916,5145,"In addition , by separating the representation of the [[ input sentence ]] , the classifier << becomes >> an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7917,5145,"In addition , by separating the representation of the input sentence , the [[ classifier ]] << becomes >> an [[ independent module ]] in our framework , which endows the method with the ability to integrate different classifiers .",1
7918,5145,"In addition , by separating the representation of the input sentence , the [[ classifier ]] << becomes >> an independent module in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7919,5145,"In addition , by separating the representation of the input sentence , the classifier << becomes >> an [[ independent module ]] in [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7920,5145,"In addition , by separating the [[ representation ]] of the [[ input sentence ]] , the classifier becomes an independent module << in >> our framework , which endows the method with the ability to integrate different classifiers .",0
7921,5145,"In addition , by separating the [[ representation ]] of the input sentence , the [[ classifier ]] becomes an independent module << in >> our framework , which endows the method with the ability to integrate different classifiers .",0
7922,5145,"In addition , by separating the [[ representation ]] of the input sentence , the classifier becomes an [[ independent module ]] << in >> our framework , which endows the method with the ability to integrate different classifiers .",0
7923,5145,"In addition , by separating the [[ representation ]] of the input sentence , the classifier becomes an independent module << in >> [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7924,5145,"In addition , by separating the representation of the [[ input sentence ]] , the [[ classifier ]] becomes an independent module << in >> our framework , which endows the method with the ability to integrate different classifiers .",0
7925,5145,"In addition , by separating the representation of the [[ input sentence ]] , the classifier becomes an [[ independent module ]] << in >> our framework , which endows the method with the ability to integrate different classifiers .",0
7926,5145,"In addition , by separating the representation of the [[ input sentence ]] , the classifier becomes an independent module << in >> [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7927,5145,"In addition , by separating the representation of the input sentence , the [[ classifier ]] becomes an [[ independent module ]] << in >> our framework , which endows the method with the ability to integrate different classifiers .",0
7928,5145,"In addition , by separating the representation of the input sentence , the [[ classifier ]] becomes an independent module << in >> [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",0
7929,5145,"In addition , by separating the representation of the input sentence , the classifier becomes an [[ independent module ]] << in >> [[ our framework ]] , which endows the method with the ability to integrate different classifiers .",1
7930,3303,"Finally , for [[ ADE ]] , our [[ AT model ]] << beats >> the baseline F 1 by 0.7 % .",0
7931,3303,"Finally , for [[ ADE ]] , our AT model << beats >> the [[ baseline F 1 ]] by 0.7 % .",0
7932,3303,"Finally , for [[ ADE ]] , our AT model << beats >> the baseline F 1 by [[ 0.7 % ]] .",0
7933,3303,"Finally , for ADE , our [[ AT model ]] << beats >> the [[ baseline F 1 ]] by 0.7 % .",1
7934,3303,"Finally , for ADE , our [[ AT model ]] << beats >> the baseline F 1 by [[ 0.7 % ]] .",0
7935,3303,"Finally , for ADE , our AT model << beats >> the [[ baseline F 1 ]] by [[ 0.7 % ]] .",0
7936,3303,"Finally , for [[ ADE ]] , our [[ AT model ]] beats the baseline F 1 << by >> 0.7 % .",0
7937,3303,"Finally , for [[ ADE ]] , our AT model beats the [[ baseline F 1 ]] << by >> 0.7 % .",0
7938,3303,"Finally , for [[ ADE ]] , our AT model beats the baseline F 1 << by >> [[ 0.7 % ]] .",0
7939,3303,"Finally , for ADE , our [[ AT model ]] beats the [[ baseline F 1 ]] << by >> 0.7 % .",0
7940,3303,"Finally , for ADE , our [[ AT model ]] beats the baseline F 1 << by >> [[ 0.7 % ]] .",0
7941,3303,"Finally , for ADE , our AT model beats the [[ baseline F 1 ]] << by >> [[ 0.7 % ]] .",1
7942,3640,We << observe >> that our [[ GCN model ]] Our Model ( C - GCN ) 84.8 * 76.5 * outperforms [[ all dependency - based models ]] by at least 1.6 F 1 .,0
7943,3640,We << observe >> that our [[ GCN model ]] Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by [[ at least 1.6 F 1 ]] .,0
7944,3640,We << observe >> that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms [[ all dependency - based models ]] by [[ at least 1.6 F 1 ]] .,0
7945,3640,We observe that our [[ GCN model ]] Our Model ( C - GCN ) 84.8 * 76.5 * << outperforms >> [[ all dependency - based models ]] by at least 1.6 F 1 .,1
7946,3640,We observe that our [[ GCN model ]] Our Model ( C - GCN ) 84.8 * 76.5 * << outperforms >> all dependency - based models by [[ at least 1.6 F 1 ]] .,0
7947,3640,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * << outperforms >> [[ all dependency - based models ]] by [[ at least 1.6 F 1 ]] .,0
7948,3640,We observe that our [[ GCN model ]] Our Model ( C - GCN ) 84.8 * 76.5 * outperforms [[ all dependency - based models ]] << by >> at least 1.6 F 1 .,0
7949,3640,We observe that our [[ GCN model ]] Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models << by >> [[ at least 1.6 F 1 ]] .,0
7950,3640,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms [[ all dependency - based models ]] << by >> [[ at least 1.6 F 1 ]] .,1
7951,183,"<< Each >> [[ MoE layer ]] contained up to [[ 2048 experts each ]] with about two million parameters , adding a total of about 8 billion parameters to the models .",0
7952,183,"<< Each >> [[ MoE layer ]] contained up to 2048 experts each with [[ about two million parameters ]] , adding a total of about 8 billion parameters to the models .",0
7953,183,"<< Each >> MoE layer contained up to [[ 2048 experts each ]] with [[ about two million parameters ]] , adding a total of about 8 billion parameters to the models .",0
7954,183,"Each [[ MoE layer ]] << contained up to >> [[ 2048 experts each ]] with about two million parameters , adding a total of about 8 billion parameters to the models .",1
7955,183,"Each [[ MoE layer ]] << contained up to >> 2048 experts each with [[ about two million parameters ]] , adding a total of about 8 billion parameters to the models .",0
7956,183,"Each MoE layer << contained up to >> [[ 2048 experts each ]] with [[ about two million parameters ]] , adding a total of about 8 billion parameters to the models .",0
7957,183,"Each [[ MoE layer ]] contained up to [[ 2048 experts each ]] << with >> about two million parameters , adding a total of about 8 billion parameters to the models .",0
7958,183,"Each [[ MoE layer ]] contained up to 2048 experts each << with >> [[ about two million parameters ]] , adding a total of about 8 billion parameters to the models .",0
7959,183,"Each MoE layer contained up to [[ 2048 experts each ]] << with >> [[ about two million parameters ]] , adding a total of about 8 billion parameters to the models .",1
7960,6008,"In [[ Rewrite ]] , the [[ summary ]] is << generated >> according to the hidden states of both the sentence and template .",1
7961,6008,"In [[ Rewrite ]] , the summary is << generated >> according to the [[ hidden states ]] of both the sentence and template .",0
7962,6008,"In [[ Rewrite ]] , the summary is << generated >> according to the hidden states of both the [[ sentence and template ]] .",0
7963,6008,"In Rewrite , the [[ summary ]] is << generated >> according to the [[ hidden states ]] of both the sentence and template .",0
7964,6008,"In Rewrite , the [[ summary ]] is << generated >> according to the hidden states of both the [[ sentence and template ]] .",0
7965,6008,"In Rewrite , the summary is << generated >> according to the [[ hidden states ]] of both the [[ sentence and template ]] .",0
7966,6008,"In [[ Rewrite ]] , the [[ summary ]] is generated << according to >> the hidden states of both the sentence and template .",0
7967,6008,"In [[ Rewrite ]] , the summary is generated << according to >> the [[ hidden states ]] of both the sentence and template .",0
7968,6008,"In [[ Rewrite ]] , the summary is generated << according to >> the hidden states of both the [[ sentence and template ]] .",0
7969,6008,"In Rewrite , the [[ summary ]] is generated << according to >> the [[ hidden states ]] of both the sentence and template .",1
7970,6008,"In Rewrite , the [[ summary ]] is generated << according to >> the hidden states of both the [[ sentence and template ]] .",0
7971,6008,"In Rewrite , the summary is generated << according to >> the [[ hidden states ]] of both the [[ sentence and template ]] .",0
7972,6008,"In [[ Rewrite ]] , the [[ summary ]] is generated according to the hidden states << of both >> the sentence and template .",0
7973,6008,"In [[ Rewrite ]] , the summary is generated according to the [[ hidden states ]] << of both >> the sentence and template .",0
7974,6008,"In [[ Rewrite ]] , the summary is generated according to the hidden states << of both >> the [[ sentence and template ]] .",0
7975,6008,"In Rewrite , the [[ summary ]] is generated according to the [[ hidden states ]] << of both >> the sentence and template .",0
7976,6008,"In Rewrite , the [[ summary ]] is generated according to the hidden states << of both >> the [[ sentence and template ]] .",0
7977,6008,"In Rewrite , the summary is generated according to the [[ hidden states ]] << of both >> the [[ sentence and template ]] .",1
7978,609,"The results of ( 8 - 9 ) << demonstrate >> that the [[ dense connection ]] using [[ concatenation operation ]] over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .",0
7979,609,"The results of ( 8 - 9 ) << demonstrate >> that the [[ dense connection ]] using concatenation operation over [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn textual semantics .",0
7980,609,"The results of ( 8 - 9 ) << demonstrate >> that the [[ dense connection ]] using concatenation operation over deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
7981,609,"The results of ( 8 - 9 ) << demonstrate >> that the [[ dense connection ]] using concatenation operation over deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
7982,609,"The results of ( 8 - 9 ) << demonstrate >> that the [[ dense connection ]] using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
7983,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using [[ concatenation operation ]] over [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn textual semantics .",0
7984,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using [[ concatenation operation ]] over deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
7985,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using [[ concatenation operation ]] over deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
7986,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using [[ concatenation operation ]] over deeper layers , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
7987,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using concatenation operation over [[ deeper layers ]] , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
7988,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using concatenation operation over [[ deeper layers ]] , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
7989,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using concatenation operation over [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
7990,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using concatenation operation over deeper layers , has [[ more powerful capability ]] retaining [[ collective knowledge ]] to learn textual semantics .",0
7991,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using concatenation operation over deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn [[ textual semantics ]] .",0
7992,609,"The results of ( 8 - 9 ) << demonstrate >> that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn [[ textual semantics ]] .",0
7993,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] << using >> [[ concatenation operation ]] over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .",1
7994,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] << using >> concatenation operation over [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn textual semantics .",0
7995,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] << using >> concatenation operation over deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
7996,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] << using >> concatenation operation over deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
7997,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] << using >> concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
7998,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> [[ concatenation operation ]] over [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn textual semantics .",0
7999,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> [[ concatenation operation ]] over deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
8000,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> [[ concatenation operation ]] over deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
8001,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> [[ concatenation operation ]] over deeper layers , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
8002,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> concatenation operation over [[ deeper layers ]] , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
8003,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> concatenation operation over [[ deeper layers ]] , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
8004,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> concatenation operation over [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
8005,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> concatenation operation over deeper layers , has [[ more powerful capability ]] retaining [[ collective knowledge ]] to learn textual semantics .",0
8006,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> concatenation operation over deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn [[ textual semantics ]] .",0
8007,609,"The results of ( 8 - 9 ) demonstrate that the dense connection << using >> concatenation operation over deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn [[ textual semantics ]] .",0
8008,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using [[ concatenation operation ]] << over >> deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .",0
8009,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation << over >> [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn textual semantics .",0
8010,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation << over >> deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
8011,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation << over >> deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
8012,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation << over >> deeper layers , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
8013,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] << over >> [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn textual semantics .",1
8014,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] << over >> deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
8015,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] << over >> deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
8016,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] << over >> deeper layers , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
8017,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation << over >> [[ deeper layers ]] , has [[ more powerful capability ]] retaining collective knowledge to learn textual semantics .",0
8018,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation << over >> [[ deeper layers ]] , has more powerful capability retaining [[ collective knowledge ]] to learn textual semantics .",0
8019,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation << over >> [[ deeper layers ]] , has more powerful capability retaining collective knowledge to learn [[ textual semantics ]] .",0
8020,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation << over >> deeper layers , has [[ more powerful capability ]] retaining [[ collective knowledge ]] to learn textual semantics .",0
8021,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation << over >> deeper layers , has [[ more powerful capability ]] retaining collective knowledge to learn [[ textual semantics ]] .",0
8022,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation << over >> deeper layers , has more powerful capability retaining [[ collective knowledge ]] to learn [[ textual semantics ]] .",0
8023,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using [[ concatenation operation ]] over deeper layers , has more powerful capability << retaining >> collective knowledge to learn textual semantics .",0
8024,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over [[ deeper layers ]] , has more powerful capability << retaining >> collective knowledge to learn textual semantics .",0
8025,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over deeper layers , has [[ more powerful capability ]] << retaining >> collective knowledge to learn textual semantics .",0
8026,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over deeper layers , has more powerful capability << retaining >> [[ collective knowledge ]] to learn textual semantics .",0
8027,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over deeper layers , has more powerful capability << retaining >> collective knowledge to learn [[ textual semantics ]] .",0
8028,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over [[ deeper layers ]] , has more powerful capability << retaining >> collective knowledge to learn textual semantics .",0
8029,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over deeper layers , has [[ more powerful capability ]] << retaining >> collective knowledge to learn textual semantics .",0
8030,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over deeper layers , has more powerful capability << retaining >> [[ collective knowledge ]] to learn textual semantics .",0
8031,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over deeper layers , has more powerful capability << retaining >> collective knowledge to learn [[ textual semantics ]] .",0
8032,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over [[ deeper layers ]] , has [[ more powerful capability ]] << retaining >> collective knowledge to learn textual semantics .",0
8033,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over [[ deeper layers ]] , has more powerful capability << retaining >> [[ collective knowledge ]] to learn textual semantics .",0
8034,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over [[ deeper layers ]] , has more powerful capability << retaining >> collective knowledge to learn [[ textual semantics ]] .",0
8035,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has [[ more powerful capability ]] << retaining >> [[ collective knowledge ]] to learn textual semantics .",1
8036,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has [[ more powerful capability ]] << retaining >> collective knowledge to learn [[ textual semantics ]] .",0
8037,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability << retaining >> [[ collective knowledge ]] to learn [[ textual semantics ]] .",0
8038,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using [[ concatenation operation ]] over deeper layers , has more powerful capability retaining collective knowledge << to learn >> textual semantics .",0
8039,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over [[ deeper layers ]] , has more powerful capability retaining collective knowledge << to learn >> textual semantics .",0
8040,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over deeper layers , has [[ more powerful capability ]] retaining collective knowledge << to learn >> textual semantics .",0
8041,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over deeper layers , has more powerful capability retaining [[ collective knowledge ]] << to learn >> textual semantics .",0
8042,609,"The results of ( 8 - 9 ) demonstrate that the [[ dense connection ]] using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge << to learn >> [[ textual semantics ]] .",0
8043,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over [[ deeper layers ]] , has more powerful capability retaining collective knowledge << to learn >> textual semantics .",0
8044,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over deeper layers , has [[ more powerful capability ]] retaining collective knowledge << to learn >> textual semantics .",0
8045,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over deeper layers , has more powerful capability retaining [[ collective knowledge ]] << to learn >> textual semantics .",0
8046,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using [[ concatenation operation ]] over deeper layers , has more powerful capability retaining collective knowledge << to learn >> [[ textual semantics ]] .",0
8047,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over [[ deeper layers ]] , has [[ more powerful capability ]] retaining collective knowledge << to learn >> textual semantics .",0
8048,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over [[ deeper layers ]] , has more powerful capability retaining [[ collective knowledge ]] << to learn >> textual semantics .",0
8049,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over [[ deeper layers ]] , has more powerful capability retaining collective knowledge << to learn >> [[ textual semantics ]] .",0
8050,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has [[ more powerful capability ]] retaining [[ collective knowledge ]] << to learn >> textual semantics .",0
8051,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has [[ more powerful capability ]] retaining collective knowledge << to learn >> [[ textual semantics ]] .",0
8052,609,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining [[ collective knowledge ]] << to learn >> [[ textual semantics ]] .",1
8053,2674,The [[ over all best system ]] << is >> the [[ multi-task bi - LSTM FREQBIN ]] ( it uses w + c and POLYGLOT initialization for w ) .,1
8054,2033,"Finally , a [[ decision ]] is << made through >> a [[ multi - layer perceptron ( MLP ) ]] based on the aggregated information .",1
8055,2033,"Finally , a [[ decision ]] is << made through >> a multi - layer perceptron ( MLP ) based on the [[ aggregated information ]] .",0
8056,2033,"Finally , a decision is << made through >> a [[ multi - layer perceptron ( MLP ) ]] based on the [[ aggregated information ]] .",0
8057,2033,"Finally , a [[ decision ]] is made through a [[ multi - layer perceptron ( MLP ) ]] << based on >> the aggregated information .",0
8058,2033,"Finally , a [[ decision ]] is made through a multi - layer perceptron ( MLP ) << based on >> the [[ aggregated information ]] .",0
8059,2033,"Finally , a decision is made through a [[ multi - layer perceptron ( MLP ) ]] << based on >> the [[ aggregated information ]] .",1
8060,1968,[[ Sequence lengths ]] are << padded to >> [[ batch - wise maximum ]] .,1
8061,2224,"<< With >> [[ standard training ]] , our [[ deep fusion ]] yields the state - of - the - art performance in this task .",0
8062,2224,"<< With >> [[ standard training ]] , our deep fusion yields the [[ state - of - the - art performance ]] in this task .",0
8063,2224,"<< With >> standard training , our [[ deep fusion ]] yields the [[ state - of - the - art performance ]] in this task .",0
8064,2224,"With [[ standard training ]] , our [[ deep fusion ]] << yields >> the state - of - the - art performance in this task .",0
8065,2224,"With [[ standard training ]] , our deep fusion << yields >> the [[ state - of - the - art performance ]] in this task .",0
8066,2224,"With standard training , our [[ deep fusion ]] << yields >> the [[ state - of - the - art performance ]] in this task .",1
8067,327,"<< For >> [[ NER ]] , [[ S - LSTM ]] gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8068,327,"<< For >> [[ NER ]] , S - LSTM gives an F1 - score of [[ 91.57 % ]] on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8069,327,"<< For >> [[ NER ]] , S - LSTM gives an F1 - score of 91.57 % on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8070,327,"<< For >> NER , [[ S - LSTM ]] gives an F1 - score of [[ 91.57 % ]] on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8071,327,"<< For >> NER , [[ S - LSTM ]] gives an F1 - score of 91.57 % on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8072,327,"<< For >> NER , S - LSTM gives an F1 - score of [[ 91.57 % ]] on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8073,327,"For [[ NER ]] , [[ S - LSTM ]] << gives >> an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8074,327,"For [[ NER ]] , S - LSTM << gives >> an F1 - score of [[ 91.57 % ]] on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8075,327,"For [[ NER ]] , S - LSTM << gives >> an F1 - score of 91.57 % on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8076,327,"For NER , [[ S - LSTM ]] << gives >> an F1 - score of [[ 91.57 % ]] on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8077,327,"For NER , [[ S - LSTM ]] << gives >> an F1 - score of 91.57 % on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8078,327,"For NER , S - LSTM << gives >> an F1 - score of [[ 91.57 % ]] on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8079,327,"For [[ NER ]] , [[ S - LSTM ]] gives an << F1 - score >> of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8080,327,"For [[ NER ]] , S - LSTM gives an << F1 - score >> of [[ 91.57 % ]] on the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8081,327,"For [[ NER ]] , S - LSTM gives an << F1 - score >> of 91.57 % on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8082,327,"For NER , [[ S - LSTM ]] gives an << F1 - score >> of [[ 91.57 % ]] on the CoNLL test set , which is significantly better compared with BiLSTMs .",1
8083,327,"For NER , [[ S - LSTM ]] gives an << F1 - score >> of 91.57 % on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8084,327,"For NER , S - LSTM gives an << F1 - score >> of [[ 91.57 % ]] on the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8085,327,"For [[ NER ]] , [[ S - LSTM ]] gives an F1 - score of 91.57 % << on >> the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8086,327,"For [[ NER ]] , S - LSTM gives an F1 - score of [[ 91.57 % ]] << on >> the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8087,327,"For [[ NER ]] , S - LSTM gives an F1 - score of 91.57 % << on >> the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",1
8088,327,"For NER , [[ S - LSTM ]] gives an F1 - score of [[ 91.57 % ]] << on >> the CoNLL test set , which is significantly better compared with BiLSTMs .",0
8089,327,"For NER , [[ S - LSTM ]] gives an F1 - score of 91.57 % << on >> the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8090,327,"For NER , S - LSTM gives an F1 - score of [[ 91.57 % ]] << on >> the [[ CoNLL test set ]] , which is significantly better compared with BiLSTMs .",0
8091,3270,The << performance of >> the [[ RE task ]] [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,0
8092,3270,The << performance of >> the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and only use the LSTM hidden states as inputs for the RE task .,0
8093,3270,The << performance of >> the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and [[ only use the LSTM hidden states ]] as inputs for the RE task .,0
8094,3270,The << performance of >> the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as [[ inputs ]] for the RE task .,0
8095,3270,The << performance of >> the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and only use the LSTM hidden states as inputs for the RE task .,0
8096,3270,The << performance of >> the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and [[ only use the LSTM hidden states ]] as inputs for the RE task .,0
8097,3270,The << performance of >> the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as [[ inputs ]] for the RE task .,0
8098,3270,The << performance of >> the RE task decreases ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and [[ only use the LSTM hidden states ]] as inputs for the RE task .,0
8099,3270,The << performance of >> the RE task decreases ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and only use the LSTM hidden states as [[ inputs ]] for the RE task .,0
8100,3270,The << performance of >> the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and [[ only use the LSTM hidden states ]] as [[ inputs ]] for the RE task .,0
8101,3270,The performance of the [[ RE task ]] [[ decreases ]] ( ? 1 % in terms of F 1 score ) << when >> we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,0
8102,3270,The performance of the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) << when >> we [[ remove the label embeddings layer ]] and only use the LSTM hidden states as inputs for the RE task .,0
8103,3270,The performance of the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) << when >> we remove the label embeddings layer and [[ only use the LSTM hidden states ]] as inputs for the RE task .,0
8104,3270,The performance of the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) << when >> we remove the label embeddings layer and only use the LSTM hidden states as [[ inputs ]] for the RE task .,0
8105,3270,The performance of the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) << when >> we [[ remove the label embeddings layer ]] and only use the LSTM hidden states as inputs for the RE task .,1
8106,3270,The performance of the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) << when >> we remove the label embeddings layer and [[ only use the LSTM hidden states ]] as inputs for the RE task .,1
8107,3270,The performance of the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) << when >> we remove the label embeddings layer and only use the LSTM hidden states as [[ inputs ]] for the RE task .,0
8108,3270,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) << when >> we [[ remove the label embeddings layer ]] and [[ only use the LSTM hidden states ]] as inputs for the RE task .,0
8109,3270,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) << when >> we [[ remove the label embeddings layer ]] and only use the LSTM hidden states as [[ inputs ]] for the RE task .,0
8110,3270,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) << when >> we remove the label embeddings layer and [[ only use the LSTM hidden states ]] as [[ inputs ]] for the RE task .,0
8111,3270,The performance of the [[ RE task ]] [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states << as >> inputs for the RE task .,0
8112,3270,The performance of the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and only use the LSTM hidden states << as >> inputs for the RE task .,0
8113,3270,The performance of the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and [[ only use the LSTM hidden states ]] << as >> inputs for the RE task .,0
8114,3270,The performance of the [[ RE task ]] decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states << as >> [[ inputs ]] for the RE task .,0
8115,3270,The performance of the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and only use the LSTM hidden states << as >> inputs for the RE task .,0
8116,3270,The performance of the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and [[ only use the LSTM hidden states ]] << as >> inputs for the RE task .,0
8117,3270,The performance of the RE task [[ decreases ]] ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states << as >> [[ inputs ]] for the RE task .,0
8118,3270,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and [[ only use the LSTM hidden states ]] << as >> inputs for the RE task .,0
8119,3270,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we [[ remove the label embeddings layer ]] and only use the LSTM hidden states << as >> [[ inputs ]] for the RE task .,0
8120,3270,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and [[ only use the LSTM hidden states ]] << as >> [[ inputs ]] for the RE task .,1
8121,1874,All [[ experiments ]] are << conducted in >> [[ Python ]] with Tensorflow and run on a Nvidia GTX 1080 Ti .,1
8122,1874,All [[ experiments ]] are << conducted in >> Python with [[ Tensorflow ]] and run on a Nvidia GTX 1080 Ti .,0
8123,1874,All [[ experiments ]] are << conducted in >> Python with Tensorflow and run on a [[ Nvidia GTX 1080 Ti ]] .,0
8124,1874,All experiments are << conducted in >> [[ Python ]] with [[ Tensorflow ]] and run on a Nvidia GTX 1080 Ti .,0
8125,1874,All experiments are << conducted in >> [[ Python ]] with Tensorflow and run on a [[ Nvidia GTX 1080 Ti ]] .,0
8126,1874,All experiments are << conducted in >> Python with [[ Tensorflow ]] and run on a [[ Nvidia GTX 1080 Ti ]] .,0
8127,1874,All [[ experiments ]] are conducted in [[ Python ]] << with >> Tensorflow and run on a Nvidia GTX 1080 Ti .,0
8128,1874,All [[ experiments ]] are conducted in Python << with >> [[ Tensorflow ]] and run on a Nvidia GTX 1080 Ti .,0
8129,1874,All [[ experiments ]] are conducted in Python << with >> Tensorflow and run on a [[ Nvidia GTX 1080 Ti ]] .,0
8130,1874,All experiments are conducted in [[ Python ]] << with >> [[ Tensorflow ]] and run on a Nvidia GTX 1080 Ti .,1
8131,1874,All experiments are conducted in [[ Python ]] << with >> Tensorflow and run on a [[ Nvidia GTX 1080 Ti ]] .,0
8132,1874,All experiments are conducted in Python << with >> [[ Tensorflow ]] and run on a [[ Nvidia GTX 1080 Ti ]] .,0
8133,1874,All [[ experiments ]] are conducted in [[ Python ]] with Tensorflow and << run on >> a Nvidia GTX 1080 Ti .,0
8134,1874,All [[ experiments ]] are conducted in Python with [[ Tensorflow ]] and << run on >> a Nvidia GTX 1080 Ti .,0
8135,1874,All [[ experiments ]] are conducted in Python with Tensorflow and << run on >> a [[ Nvidia GTX 1080 Ti ]] .,1
8136,1874,All experiments are conducted in [[ Python ]] with [[ Tensorflow ]] and << run on >> a Nvidia GTX 1080 Ti .,0
8137,1874,All experiments are conducted in [[ Python ]] with Tensorflow and << run on >> a [[ Nvidia GTX 1080 Ti ]] .,0
8138,1874,All experiments are conducted in Python with [[ Tensorflow ]] and << run on >> a [[ Nvidia GTX 1080 Ti ]] .,0
8139,5984,"For [[ CNN ]] , we << set >> [[ h = 3 , 4 , 5 ]] with 400 , 300 , 300 feature maps , respectively .",1
8140,5984,"For [[ CNN ]] , we << set >> h = 3 , 4 , 5 with [[ 400 , 300 , 300 feature maps ]] , respectively .",0
8141,5984,"For CNN , we << set >> [[ h = 3 , 4 , 5 ]] with [[ 400 , 300 , 300 feature maps ]] , respectively .",0
8142,5984,"For [[ CNN ]] , we set [[ h = 3 , 4 , 5 ]] << with >> 400 , 300 , 300 feature maps , respectively .",0
8143,5984,"For [[ CNN ]] , we set h = 3 , 4 , 5 << with >> [[ 400 , 300 , 300 feature maps ]] , respectively .",0
8144,5984,"For CNN , we set [[ h = 3 , 4 , 5 ]] << with >> [[ 400 , 300 , 300 feature maps ]] , respectively .",1
8145,5986,We << use >> [[ dropout ]] on [[ all non-linear connections ]] with a dropout rate of 0.5 .,0
8146,5986,We << use >> [[ dropout ]] on all non-linear connections with a [[ dropout rate of 0.5 ]] .,0
8147,5986,We << use >> dropout on [[ all non-linear connections ]] with a [[ dropout rate of 0.5 ]] .,0
8148,5986,We use [[ dropout ]] << on >> [[ all non-linear connections ]] with a dropout rate of 0.5 .,1
8149,5986,We use [[ dropout ]] << on >> all non-linear connections with a [[ dropout rate of 0.5 ]] .,0
8150,5986,We use dropout << on >> [[ all non-linear connections ]] with a [[ dropout rate of 0.5 ]] .,0
8151,5986,We use [[ dropout ]] on [[ all non-linear connections ]] << with >> a dropout rate of 0.5 .,0
8152,5986,We use [[ dropout ]] on all non-linear connections << with >> a [[ dropout rate of 0.5 ]] .,0
8153,5986,We use dropout on [[ all non-linear connections ]] << with >> a [[ dropout rate of 0.5 ]] .,1
8154,1756,The result << shows >> that [[ POS feature ( 1 ) and question - word feature ( 3 ) ]] are the [[ two most important features ]] .,0
8155,1756,The result shows that [[ POS feature ( 1 ) and question - word feature ( 3 ) ]] << are >> the [[ two most important features ]] .,1
8156,2178,"The [[ batch size ]] is << selected in >> [[ { 16 , 20 , 32 } ]] .",1
8157,4961,[[ TD- LSTM ]] : It << uses >> a [[ forward LSTM and a backward LSTM ]] to abstract the information before and after the target .,1
8158,4961,[[ TD- LSTM ]] : It << uses >> a forward LSTM and a backward LSTM to abstract the [[ information ]] before and after the target .,0
8159,4961,[[ TD- LSTM ]] : It << uses >> a forward LSTM and a backward LSTM to abstract the information before and after the [[ target ]] .,0
8160,4961,TD- LSTM : It << uses >> a [[ forward LSTM and a backward LSTM ]] to abstract the [[ information ]] before and after the target .,0
8161,4961,TD- LSTM : It << uses >> a [[ forward LSTM and a backward LSTM ]] to abstract the information before and after the [[ target ]] .,0
8162,4961,TD- LSTM : It << uses >> a forward LSTM and a backward LSTM to abstract the [[ information ]] before and after the [[ target ]] .,0
8163,4961,[[ TD- LSTM ]] : It uses a [[ forward LSTM and a backward LSTM ]] << to abstract >> the information before and after the target .,0
8164,4961,[[ TD- LSTM ]] : It uses a forward LSTM and a backward LSTM << to abstract >> the [[ information ]] before and after the target .,0
8165,4961,[[ TD- LSTM ]] : It uses a forward LSTM and a backward LSTM << to abstract >> the information before and after the [[ target ]] .,0
8166,4961,TD- LSTM : It uses a [[ forward LSTM and a backward LSTM ]] << to abstract >> the [[ information ]] before and after the target .,1
8167,4961,TD- LSTM : It uses a [[ forward LSTM and a backward LSTM ]] << to abstract >> the information before and after the [[ target ]] .,0
8168,4961,TD- LSTM : It uses a forward LSTM and a backward LSTM << to abstract >> the [[ information ]] before and after the [[ target ]] .,0
8169,4961,[[ TD- LSTM ]] : It uses a [[ forward LSTM and a backward LSTM ]] to abstract the information << before and after >> the target .,0
8170,4961,[[ TD- LSTM ]] : It uses a forward LSTM and a backward LSTM to abstract the [[ information ]] << before and after >> the target .,0
8171,4961,[[ TD- LSTM ]] : It uses a forward LSTM and a backward LSTM to abstract the information << before and after >> the [[ target ]] .,0
8172,4961,TD- LSTM : It uses a [[ forward LSTM and a backward LSTM ]] to abstract the [[ information ]] << before and after >> the target .,0
8173,4961,TD- LSTM : It uses a [[ forward LSTM and a backward LSTM ]] to abstract the information << before and after >> the [[ target ]] .,0
8174,4961,TD- LSTM : It uses a forward LSTM and a backward LSTM to abstract the [[ information ]] << before and after >> the [[ target ]] .,1
8175,6071,The basic idea << of >> our method is to jointly estimate the [[ upper-bound frequency ]] of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,1
8176,6071,The basic idea << of >> our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,0
8177,6071,The basic idea << of >> our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8178,6071,The basic idea << of >> our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8179,6071,The basic idea << of >> our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8180,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,0
8181,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8182,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8183,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8184,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8185,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8186,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8187,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8188,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8189,6071,The basic idea << of >> our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in [[ each decoding step ]] .,0
8190,6071,The basic idea of our method is to << jointly estimate >> the [[ upper-bound frequency ]] of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,0
8191,6071,The basic idea of our method is to << jointly estimate >> the [[ upper-bound frequency ]] of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,0
8192,6071,The basic idea of our method is to << jointly estimate >> the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8193,6071,The basic idea of our method is to << jointly estimate >> the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8194,6071,The basic idea of our method is to << jointly estimate >> the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8195,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of [[ each target vocabulary ]] that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,0
8196,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8197,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8198,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8199,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8200,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8201,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8202,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8203,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8204,6071,The basic idea of our method is to << jointly estimate >> the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] in [[ each decoding step ]] .,0
8205,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of [[ each target vocabulary ]] << that can occur in >> a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,0
8206,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary << that can occur in >> a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,0
8207,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary << that can occur in >> a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8208,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary << that can occur in >> a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8209,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary << that can occur in >> a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8210,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] << that can occur in >> a [[ summary ]] during the encoding process and exploit the estimation to control the output words in each decoding step .,1
8211,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] << that can occur in >> a summary during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8212,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] << that can occur in >> a summary during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8213,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] << that can occur in >> a summary during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8214,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary << that can occur in >> a [[ summary ]] during the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8215,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary << that can occur in >> a [[ summary ]] during the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8216,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary << that can occur in >> a [[ summary ]] during the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8217,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary << that can occur in >> a summary during the [[ encoding process ]] and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8218,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary << that can occur in >> a summary during the [[ encoding process ]] and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8219,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary << that can occur in >> a summary during the encoding process and exploit the estimation to control the [[ output words ]] in [[ each decoding step ]] .,0
8220,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of [[ each target vocabulary ]] that can occur in a summary << during >> the encoding process and exploit the estimation to control the output words in each decoding step .,0
8221,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a [[ summary ]] << during >> the encoding process and exploit the estimation to control the output words in each decoding step .,0
8222,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary << during >> the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8223,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary << during >> the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8224,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary << during >> the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8225,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a [[ summary ]] << during >> the encoding process and exploit the estimation to control the output words in each decoding step .,0
8226,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary << during >> the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,0
8227,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary << during >> the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8228,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary << during >> the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8229,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] << during >> the [[ encoding process ]] and exploit the estimation to control the output words in each decoding step .,1
8230,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] << during >> the encoding process and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8231,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] << during >> the encoding process and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8232,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary << during >> the [[ encoding process ]] and exploit the estimation to control the [[ output words ]] in each decoding step .,0
8233,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary << during >> the [[ encoding process ]] and exploit the estimation to control the output words in [[ each decoding step ]] .,0
8234,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary << during >> the encoding process and exploit the estimation to control the [[ output words ]] in [[ each decoding step ]] .,0
8235,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to << control >> the output words in each decoding step .,0
8236,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to << control >> the output words in each decoding step .,0
8237,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to << control >> the output words in each decoding step .,0
8238,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to << control >> the [[ output words ]] in each decoding step .,1
8239,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to << control >> the output words in [[ each decoding step ]] .,0
8240,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a [[ summary ]] during the encoding process and exploit the estimation to << control >> the output words in each decoding step .,0
8241,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the [[ encoding process ]] and exploit the estimation to << control >> the output words in each decoding step .,0
8242,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to << control >> the [[ output words ]] in each decoding step .,0
8243,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to << control >> the output words in [[ each decoding step ]] .,0
8244,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the [[ encoding process ]] and exploit the estimation to << control >> the output words in each decoding step .,0
8245,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to << control >> the [[ output words ]] in each decoding step .,0
8246,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to << control >> the output words in [[ each decoding step ]] .,0
8247,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to << control >> the [[ output words ]] in each decoding step .,0
8248,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to << control >> the output words in [[ each decoding step ]] .,0
8249,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to << control >> the [[ output words ]] in [[ each decoding step ]] .,0
8250,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words << in >> each decoding step .,0
8251,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words << in >> each decoding step .,0
8252,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words << in >> each decoding step .,0
8253,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] << in >> each decoding step .,0
8254,6071,The basic idea of our method is to jointly estimate the [[ upper-bound frequency ]] of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words << in >> [[ each decoding step ]] .,0
8255,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words << in >> each decoding step .,0
8256,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words << in >> each decoding step .,0
8257,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] << in >> each decoding step .,0
8258,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of [[ each target vocabulary ]] that can occur in a summary during the encoding process and exploit the estimation to control the output words << in >> [[ each decoding step ]] .,0
8259,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the [[ encoding process ]] and exploit the estimation to control the output words << in >> each decoding step .,0
8260,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the [[ output words ]] << in >> each decoding step .,0
8261,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a [[ summary ]] during the encoding process and exploit the estimation to control the output words << in >> [[ each decoding step ]] .,0
8262,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the [[ output words ]] << in >> each decoding step .,0
8263,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the [[ encoding process ]] and exploit the estimation to control the output words << in >> [[ each decoding step ]] .,0
8264,6071,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the [[ output words ]] << in >> [[ each decoding step ]] .,1
8265,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is << applied to >> [[ convert ]] the input sentence and each candidate template into hidden states .",1
8266,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is << applied to >> convert the [[ input sentence ]] and each candidate template into hidden states .",0
8267,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is << applied to >> convert the input sentence and [[ each candidate template ]] into hidden states .",0
8268,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is << applied to >> convert the input sentence and each candidate template into [[ hidden states ]] .",0
8269,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is << applied to >> [[ convert ]] the [[ input sentence ]] and each candidate template into hidden states .",0
8270,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is << applied to >> [[ convert ]] the input sentence and [[ each candidate template ]] into hidden states .",0
8271,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is << applied to >> [[ convert ]] the input sentence and each candidate template into [[ hidden states ]] .",0
8272,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is << applied to >> convert the [[ input sentence ]] and [[ each candidate template ]] into hidden states .",0
8273,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is << applied to >> convert the [[ input sentence ]] and each candidate template into [[ hidden states ]] .",0
8274,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is << applied to >> convert the input sentence and [[ each candidate template ]] into [[ hidden states ]] .",0
8275,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is applied to [[ convert ]] the input sentence and each candidate template << into >> hidden states .",0
8276,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is applied to convert the [[ input sentence ]] and each candidate template << into >> hidden states .",0
8277,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is applied to convert the input sentence and [[ each candidate template ]] << into >> hidden states .",0
8278,6005,"Specifically , a [[ Recurrent Neural Network ( RNN ) encoder ]] is applied to convert the input sentence and each candidate template << into >> [[ hidden states ]] .",0
8279,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to [[ convert ]] the [[ input sentence ]] and each candidate template << into >> hidden states .",0
8280,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to [[ convert ]] the input sentence and [[ each candidate template ]] << into >> hidden states .",0
8281,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to [[ convert ]] the input sentence and each candidate template << into >> [[ hidden states ]] .",1
8282,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the [[ input sentence ]] and [[ each candidate template ]] << into >> hidden states .",0
8283,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the [[ input sentence ]] and each candidate template << into >> [[ hidden states ]] .",0
8284,6005,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the input sentence and [[ each candidate template ]] << into >> [[ hidden states ]] .",0
8285,489,We can << see >> a [[ NMM ]] trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models .,0
8286,489,We can << see >> a [[ NMM ]] trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including both methods using feature engineering and deep learning models .,0
8287,489,We can << see >> a [[ NMM ]] trained with TRAIN - ALL set beats all the previous state - of - the art systems including [[ both methods ]] using feature engineering and deep learning models .,0
8288,489,We can << see >> a [[ NMM ]] trained with TRAIN - ALL set beats all the previous state - of - the art systems including both methods using [[ feature engineering and deep learning models ]] .,0
8289,489,We can << see >> a NMM trained with [[ TRAIN - ALL set ]] beats [[ all the previous state - of - the art systems ]] including both methods using feature engineering and deep learning models .,0
8290,489,We can << see >> a NMM trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including [[ both methods ]] using feature engineering and deep learning models .,0
8291,489,We can << see >> a NMM trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including both methods using [[ feature engineering and deep learning models ]] .,0
8292,489,We can << see >> a NMM trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including [[ both methods ]] using feature engineering and deep learning models .,0
8293,489,We can << see >> a NMM trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including both methods using [[ feature engineering and deep learning models ]] .,0
8294,489,We can << see >> a NMM trained with TRAIN - ALL set beats all the previous state - of - the art systems including [[ both methods ]] using [[ feature engineering and deep learning models ]] .,0
8295,489,We can see a [[ NMM ]] << trained with >> [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models .,1
8296,489,We can see a [[ NMM ]] << trained with >> TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including both methods using feature engineering and deep learning models .,0
8297,489,We can see a [[ NMM ]] << trained with >> TRAIN - ALL set beats all the previous state - of - the art systems including [[ both methods ]] using feature engineering and deep learning models .,0
8298,489,We can see a [[ NMM ]] << trained with >> TRAIN - ALL set beats all the previous state - of - the art systems including both methods using [[ feature engineering and deep learning models ]] .,0
8299,489,We can see a NMM << trained with >> [[ TRAIN - ALL set ]] beats [[ all the previous state - of - the art systems ]] including both methods using feature engineering and deep learning models .,0
8300,489,We can see a NMM << trained with >> [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including [[ both methods ]] using feature engineering and deep learning models .,0
8301,489,We can see a NMM << trained with >> [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including both methods using [[ feature engineering and deep learning models ]] .,0
8302,489,We can see a NMM << trained with >> TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including [[ both methods ]] using feature engineering and deep learning models .,0
8303,489,We can see a NMM << trained with >> TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including both methods using [[ feature engineering and deep learning models ]] .,0
8304,489,We can see a NMM << trained with >> TRAIN - ALL set beats all the previous state - of - the art systems including [[ both methods ]] using [[ feature engineering and deep learning models ]] .,0
8305,489,We can see a [[ NMM ]] trained with [[ TRAIN - ALL set ]] << beats >> all the previous state - of - the art systems including both methods using feature engineering and deep learning models .,0
8306,489,We can see a [[ NMM ]] trained with TRAIN - ALL set << beats >> [[ all the previous state - of - the art systems ]] including both methods using feature engineering and deep learning models .,1
8307,489,We can see a [[ NMM ]] trained with TRAIN - ALL set << beats >> all the previous state - of - the art systems including [[ both methods ]] using feature engineering and deep learning models .,0
8308,489,We can see a [[ NMM ]] trained with TRAIN - ALL set << beats >> all the previous state - of - the art systems including both methods using [[ feature engineering and deep learning models ]] .,0
8309,489,We can see a NMM trained with [[ TRAIN - ALL set ]] << beats >> [[ all the previous state - of - the art systems ]] including both methods using feature engineering and deep learning models .,0
8310,489,We can see a NMM trained with [[ TRAIN - ALL set ]] << beats >> all the previous state - of - the art systems including [[ both methods ]] using feature engineering and deep learning models .,0
8311,489,We can see a NMM trained with [[ TRAIN - ALL set ]] << beats >> all the previous state - of - the art systems including both methods using [[ feature engineering and deep learning models ]] .,0
8312,489,We can see a NMM trained with TRAIN - ALL set << beats >> [[ all the previous state - of - the art systems ]] including [[ both methods ]] using feature engineering and deep learning models .,0
8313,489,We can see a NMM trained with TRAIN - ALL set << beats >> [[ all the previous state - of - the art systems ]] including both methods using [[ feature engineering and deep learning models ]] .,0
8314,489,We can see a NMM trained with TRAIN - ALL set << beats >> all the previous state - of - the art systems including [[ both methods ]] using [[ feature engineering and deep learning models ]] .,0
8315,489,We can see a [[ NMM ]] trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems << including >> both methods using feature engineering and deep learning models .,0
8316,489,We can see a [[ NMM ]] trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] << including >> both methods using feature engineering and deep learning models .,0
8317,489,We can see a [[ NMM ]] trained with TRAIN - ALL set beats all the previous state - of - the art systems << including >> [[ both methods ]] using feature engineering and deep learning models .,0
8318,489,We can see a [[ NMM ]] trained with TRAIN - ALL set beats all the previous state - of - the art systems << including >> both methods using [[ feature engineering and deep learning models ]] .,0
8319,489,We can see a NMM trained with [[ TRAIN - ALL set ]] beats [[ all the previous state - of - the art systems ]] << including >> both methods using feature engineering and deep learning models .,0
8320,489,We can see a NMM trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems << including >> [[ both methods ]] using feature engineering and deep learning models .,0
8321,489,We can see a NMM trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems << including >> both methods using [[ feature engineering and deep learning models ]] .,0
8322,489,We can see a NMM trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] << including >> [[ both methods ]] using feature engineering and deep learning models .,1
8323,489,We can see a NMM trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] << including >> both methods using [[ feature engineering and deep learning models ]] .,0
8324,489,We can see a NMM trained with TRAIN - ALL set beats all the previous state - of - the art systems << including >> [[ both methods ]] using [[ feature engineering and deep learning models ]] .,0
8325,489,We can see a [[ NMM ]] trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including both methods << using >> feature engineering and deep learning models .,0
8326,489,We can see a [[ NMM ]] trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including both methods << using >> feature engineering and deep learning models .,0
8327,489,We can see a [[ NMM ]] trained with TRAIN - ALL set beats all the previous state - of - the art systems including [[ both methods ]] << using >> feature engineering and deep learning models .,0
8328,489,We can see a [[ NMM ]] trained with TRAIN - ALL set beats all the previous state - of - the art systems including both methods << using >> [[ feature engineering and deep learning models ]] .,0
8329,489,We can see a NMM trained with [[ TRAIN - ALL set ]] beats [[ all the previous state - of - the art systems ]] including both methods << using >> feature engineering and deep learning models .,0
8330,489,We can see a NMM trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including [[ both methods ]] << using >> feature engineering and deep learning models .,0
8331,489,We can see a NMM trained with [[ TRAIN - ALL set ]] beats all the previous state - of - the art systems including both methods << using >> [[ feature engineering and deep learning models ]] .,0
8332,489,We can see a NMM trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including [[ both methods ]] << using >> feature engineering and deep learning models .,0
8333,489,We can see a NMM trained with TRAIN - ALL set beats [[ all the previous state - of - the art systems ]] including both methods << using >> [[ feature engineering and deep learning models ]] .,0
8334,489,We can see a NMM trained with TRAIN - ALL set beats all the previous state - of - the art systems including [[ both methods ]] << using >> [[ feature engineering and deep learning models ]] .,1
8335,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we << could see >> that the [[ performance ]] was [[ rather higher ]] because of the regularization effect .",0
8336,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we << could see >> that the [[ performance ]] was rather higher because of the [[ regularization effect ]] .",0
8337,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we << could see >> that the performance was [[ rather higher ]] because of the [[ regularization effect ]] .",0
8338,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the [[ performance ]] << was >> [[ rather higher ]] because of the regularization effect .",1
8339,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the [[ performance ]] << was >> rather higher because of the [[ regularization effect ]] .",0
8340,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the performance << was >> [[ rather higher ]] because of the [[ regularization effect ]] .",0
8341,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the [[ performance ]] was [[ rather higher ]] << because of >> the regularization effect .",0
8342,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the [[ performance ]] was rather higher << because of >> the [[ regularization effect ]] .",1
8343,606,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the performance was [[ rather higher ]] << because of >> the [[ regularization effect ]] .",0
8344,3212,"These enhancements << alleviate >> the problem of [[ low - performance entity detection in early stages of training ]] , as well as allow [[ entity information to further help downstream relation classification ]] .",0
8345,3212,"These enhancements alleviate the problem of [[ low - performance entity detection in early stages of training ]] , as well as << allow >> [[ entity information to further help downstream relation classification ]] .",0
8346,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a [[ good sampling ratio ]] between the [[ original and augmented data ]] during training can further boost the model performance .",0
8347,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a [[ good sampling ratio ]] between the original and augmented data during [[ training ]] can further boost the model performance .",0
8348,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a [[ good sampling ratio ]] between the original and augmented data during training can further [[ boost ]] the model performance .",0
8349,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a [[ good sampling ratio ]] between the original and augmented data during training can further boost the [[ model performance ]] .",0
8350,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a good sampling ratio between the [[ original and augmented data ]] during [[ training ]] can further boost the model performance .",0
8351,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a good sampling ratio between the [[ original and augmented data ]] during training can further [[ boost ]] the model performance .",0
8352,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a good sampling ratio between the [[ original and augmented data ]] during training can further boost the [[ model performance ]] .",0
8353,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a good sampling ratio between the original and augmented data during [[ training ]] can further [[ boost ]] the model performance .",0
8354,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a good sampling ratio between the original and augmented data during [[ training ]] can further boost the [[ model performance ]] .",0
8355,889,"Although injecting more data beyond 3 does not benefit the model , we << observe that >> a good sampling ratio between the original and augmented data during training can further [[ boost ]] the [[ model performance ]] .",0
8356,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] << between >> the [[ original and augmented data ]] during training can further boost the model performance .",1
8357,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] << between >> the original and augmented data during [[ training ]] can further boost the model performance .",0
8358,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] << between >> the original and augmented data during training can further [[ boost ]] the model performance .",0
8359,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] << between >> the original and augmented data during training can further boost the [[ model performance ]] .",0
8360,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio << between >> the [[ original and augmented data ]] during [[ training ]] can further boost the model performance .",0
8361,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio << between >> the [[ original and augmented data ]] during training can further [[ boost ]] the model performance .",0
8362,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio << between >> the [[ original and augmented data ]] during training can further boost the [[ model performance ]] .",0
8363,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio << between >> the original and augmented data during [[ training ]] can further [[ boost ]] the model performance .",0
8364,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio << between >> the original and augmented data during [[ training ]] can further boost the [[ model performance ]] .",0
8365,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio << between >> the original and augmented data during training can further [[ boost ]] the [[ model performance ]] .",0
8366,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the [[ original and augmented data ]] << during >> training can further boost the model performance .",0
8367,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the original and augmented data << during >> [[ training ]] can further boost the model performance .",0
8368,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the original and augmented data << during >> training can further [[ boost ]] the model performance .",0
8369,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the original and augmented data << during >> training can further boost the [[ model performance ]] .",0
8370,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the [[ original and augmented data ]] << during >> [[ training ]] can further boost the model performance .",1
8371,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the [[ original and augmented data ]] << during >> training can further [[ boost ]] the model performance .",0
8372,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the [[ original and augmented data ]] << during >> training can further boost the [[ model performance ]] .",0
8373,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data << during >> [[ training ]] can further [[ boost ]] the model performance .",0
8374,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data << during >> [[ training ]] can further boost the [[ model performance ]] .",0
8375,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data << during >> training can further [[ boost ]] the [[ model performance ]] .",0
8376,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the [[ original and augmented data ]] during training << can further >> boost the model performance .",0
8377,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the original and augmented data during [[ training ]] << can further >> boost the model performance .",0
8378,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the original and augmented data during training << can further >> [[ boost ]] the model performance .",0
8379,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a [[ good sampling ratio ]] between the original and augmented data during training << can further >> boost the [[ model performance ]] .",0
8380,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the [[ original and augmented data ]] during [[ training ]] << can further >> boost the model performance .",0
8381,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the [[ original and augmented data ]] during training << can further >> [[ boost ]] the model performance .",1
8382,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the [[ original and augmented data ]] during training << can further >> boost the [[ model performance ]] .",0
8383,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data during [[ training ]] << can further >> [[ boost ]] the model performance .",0
8384,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data during [[ training ]] << can further >> boost the [[ model performance ]] .",0
8385,889,"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data during training << can further >> [[ boost ]] the [[ model performance ]] .",0
8386,6062,"The << maximum length of >> [[ documents and summaries ]] is [[ 120 and 25 ]] respectively , and the batch size is also 256 .",0
8387,6062,"The maximum length of [[ documents and summaries ]] << is >> [[ 120 and 25 ]] respectively , and the batch size is also 256 .",1
8388,269,"Like typical CNN layers , dilated convolutions << operate on >> a [[ sliding window of context ]] over the [[ sequence ]] , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",0
8389,269,"Like typical CNN layers , dilated convolutions operate on a [[ sliding window of context ]] << over >> the [[ sequence ]] , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",1
8390,3824,"[[ Dimensions ]] << of >> [[ hidden vectors and word embeddings ]] were selected from { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .",1
8391,3824,"[[ Dimensions ]] << of >> hidden vectors and word embeddings were selected from [[ { 250 , 300 } and { 150 , 200 , 250 , 300 } ]] , respectively .",0
8392,3824,"Dimensions << of >> [[ hidden vectors and word embeddings ]] were selected from [[ { 250 , 300 } and { 150 , 200 , 250 , 300 } ]] , respectively .",0
8393,3824,"[[ Dimensions ]] of [[ hidden vectors and word embeddings ]] were << selected from >> { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .",0
8394,3824,"[[ Dimensions ]] of hidden vectors and word embeddings were << selected from >> [[ { 250 , 300 } and { 150 , 200 , 250 , 300 } ]] , respectively .",0
8395,3824,"Dimensions of [[ hidden vectors and word embeddings ]] were << selected from >> [[ { 250 , 300 } and { 150 , 200 , 250 , 300 } ]] , respectively .",1
8396,3885,[[ Our ensemble ( PoE ) ]] has [[ an absolute improvement ]] << of >> 2.1 F1 on both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,0
8397,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement << of >> [[ 2.1 F1 ]] on both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,0
8398,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement << of >> 2.1 F1 on both [[ CoNLL 2005 ]] and CoNLL 2012 over the previous state of the art .,0
8399,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement << of >> 2.1 F1 on both CoNLL 2005 and [[ CoNLL 2012 ]] over the previous state of the art .,0
8400,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement << of >> 2.1 F1 on both CoNLL 2005 and CoNLL 2012 over the [[ previous state of the art ]] .,0
8401,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] << of >> [[ 2.1 F1 ]] on both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,0
8402,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] << of >> 2.1 F1 on both [[ CoNLL 2005 ]] and CoNLL 2012 over the previous state of the art .,0
8403,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] << of >> 2.1 F1 on both CoNLL 2005 and [[ CoNLL 2012 ]] over the previous state of the art .,0
8404,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] << of >> 2.1 F1 on both CoNLL 2005 and CoNLL 2012 over the [[ previous state of the art ]] .,0
8405,3885,Our ensemble ( PoE ) has an absolute improvement << of >> [[ 2.1 F1 ]] on both [[ CoNLL 2005 ]] and CoNLL 2012 over the previous state of the art .,0
8406,3885,Our ensemble ( PoE ) has an absolute improvement << of >> [[ 2.1 F1 ]] on both CoNLL 2005 and [[ CoNLL 2012 ]] over the previous state of the art .,0
8407,3885,Our ensemble ( PoE ) has an absolute improvement << of >> [[ 2.1 F1 ]] on both CoNLL 2005 and CoNLL 2012 over the [[ previous state of the art ]] .,0
8408,3885,Our ensemble ( PoE ) has an absolute improvement << of >> 2.1 F1 on both [[ CoNLL 2005 ]] and [[ CoNLL 2012 ]] over the previous state of the art .,0
8409,3885,Our ensemble ( PoE ) has an absolute improvement << of >> 2.1 F1 on both [[ CoNLL 2005 ]] and CoNLL 2012 over the [[ previous state of the art ]] .,0
8410,3885,Our ensemble ( PoE ) has an absolute improvement << of >> 2.1 F1 on both CoNLL 2005 and [[ CoNLL 2012 ]] over the [[ previous state of the art ]] .,0
8411,3885,[[ Our ensemble ( PoE ) ]] has [[ an absolute improvement ]] of 2.1 F1 << on >> both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,0
8412,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of [[ 2.1 F1 ]] << on >> both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,0
8413,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of 2.1 F1 << on >> both [[ CoNLL 2005 ]] and CoNLL 2012 over the previous state of the art .,0
8414,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of 2.1 F1 << on >> both CoNLL 2005 and [[ CoNLL 2012 ]] over the previous state of the art .,0
8415,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of 2.1 F1 << on >> both CoNLL 2005 and CoNLL 2012 over the [[ previous state of the art ]] .,0
8416,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of [[ 2.1 F1 ]] << on >> both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,0
8417,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of 2.1 F1 << on >> both [[ CoNLL 2005 ]] and CoNLL 2012 over the previous state of the art .,0
8418,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of 2.1 F1 << on >> both CoNLL 2005 and [[ CoNLL 2012 ]] over the previous state of the art .,0
8419,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of 2.1 F1 << on >> both CoNLL 2005 and CoNLL 2012 over the [[ previous state of the art ]] .,0
8420,3885,Our ensemble ( PoE ) has an absolute improvement of [[ 2.1 F1 ]] << on >> both [[ CoNLL 2005 ]] and CoNLL 2012 over the previous state of the art .,0
8421,3885,Our ensemble ( PoE ) has an absolute improvement of [[ 2.1 F1 ]] << on >> both CoNLL 2005 and [[ CoNLL 2012 ]] over the previous state of the art .,0
8422,3885,Our ensemble ( PoE ) has an absolute improvement of [[ 2.1 F1 ]] << on >> both CoNLL 2005 and CoNLL 2012 over the [[ previous state of the art ]] .,0
8423,3885,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 << on >> both [[ CoNLL 2005 ]] and [[ CoNLL 2012 ]] over the previous state of the art .,0
8424,3885,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 << on >> both [[ CoNLL 2005 ]] and CoNLL 2012 over the [[ previous state of the art ]] .,0
8425,3885,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 << on >> both CoNLL 2005 and [[ CoNLL 2012 ]] over the [[ previous state of the art ]] .,0
8426,3885,[[ Our ensemble ( PoE ) ]] has [[ an absolute improvement ]] of 2.1 F1 on both CoNLL 2005 and CoNLL 2012 << over >> the previous state of the art .,0
8427,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of [[ 2.1 F1 ]] on both CoNLL 2005 and CoNLL 2012 << over >> the previous state of the art .,0
8428,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of 2.1 F1 on both [[ CoNLL 2005 ]] and CoNLL 2012 << over >> the previous state of the art .,0
8429,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of 2.1 F1 on both CoNLL 2005 and [[ CoNLL 2012 ]] << over >> the previous state of the art .,0
8430,3885,[[ Our ensemble ( PoE ) ]] has an absolute improvement of 2.1 F1 on both CoNLL 2005 and CoNLL 2012 << over >> the [[ previous state of the art ]] .,0
8431,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of [[ 2.1 F1 ]] on both CoNLL 2005 and CoNLL 2012 << over >> the previous state of the art .,0
8432,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of 2.1 F1 on both [[ CoNLL 2005 ]] and CoNLL 2012 << over >> the previous state of the art .,0
8433,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of 2.1 F1 on both CoNLL 2005 and [[ CoNLL 2012 ]] << over >> the previous state of the art .,0
8434,3885,Our ensemble ( PoE ) has [[ an absolute improvement ]] of 2.1 F1 on both CoNLL 2005 and CoNLL 2012 << over >> the [[ previous state of the art ]] .,1
8435,3885,Our ensemble ( PoE ) has an absolute improvement of [[ 2.1 F1 ]] on both [[ CoNLL 2005 ]] and CoNLL 2012 << over >> the previous state of the art .,0
8436,3885,Our ensemble ( PoE ) has an absolute improvement of [[ 2.1 F1 ]] on both CoNLL 2005 and [[ CoNLL 2012 ]] << over >> the previous state of the art .,0
8437,3885,Our ensemble ( PoE ) has an absolute improvement of [[ 2.1 F1 ]] on both CoNLL 2005 and CoNLL 2012 << over >> the [[ previous state of the art ]] .,0
8438,3885,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 on both [[ CoNLL 2005 ]] and [[ CoNLL 2012 ]] << over >> the previous state of the art .,0
8439,3885,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 on both [[ CoNLL 2005 ]] and CoNLL 2012 << over >> the [[ previous state of the art ]] .,0
8440,3885,Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 on both CoNLL 2005 and [[ CoNLL 2012 ]] << over >> the [[ previous state of the art ]] .,0
8441,1799,[[ Answer candidates ]] are << limited to >> [[ spans ]] with at most 30 words .,1
8442,1799,[[ Answer candidates ]] are << limited to >> spans with [[ at most 30 words ]] .,0
8443,1799,Answer candidates are << limited to >> [[ spans ]] with [[ at most 30 words ]] .,0
8444,1799,[[ Answer candidates ]] are limited to [[ spans ]] << with >> at most 30 words .,0
8445,1799,[[ Answer candidates ]] are limited to spans << with >> [[ at most 30 words ]] .,0
8446,1799,Answer candidates are limited to [[ spans ]] << with >> [[ at most 30 words ]] .,1
8447,4967,[[ Rec - NN ]] is << better than >> [[ TD - LSTM ]] but not as good as our method .,1
8448,4967,[[ Rec - NN ]] is << better than >> TD - LSTM but not as good as [[ our method ]] .,0
8449,4967,Rec - NN is << better than >> [[ TD - LSTM ]] but not as good as [[ our method ]] .,0
8450,4967,[[ Rec - NN ]] is better than [[ TD - LSTM ]] but << not as good as >> our method .,0
8451,4967,[[ Rec - NN ]] is better than TD - LSTM but << not as good as >> [[ our method ]] .,1
8452,4967,Rec - NN is better than [[ TD - LSTM ]] but << not as good as >> [[ our method ]] .,0
8453,1572,"[[ Iterative Attention Reader , EpiReader and GA Reader ]] << are >> the [[ three multi-turn reasoning models ]] with xed reasoning steps .",1
8454,1572,"[[ Iterative Attention Reader , EpiReader and GA Reader ]] << are >> the three multi-turn reasoning models with [[ xed reasoning steps ]] .",0
8455,1572,"Iterative Attention Reader , EpiReader and GA Reader << are >> the [[ three multi-turn reasoning models ]] with [[ xed reasoning steps ]] .",0
8456,1572,"[[ Iterative Attention Reader , EpiReader and GA Reader ]] are the [[ three multi-turn reasoning models ]] << with >> xed reasoning steps .",0
8457,1572,"[[ Iterative Attention Reader , EpiReader and GA Reader ]] are the three multi-turn reasoning models << with >> [[ xed reasoning steps ]] .",0
8458,1572,"Iterative Attention Reader , EpiReader and GA Reader are the [[ three multi-turn reasoning models ]] << with >> [[ xed reasoning steps ]] .",1
8459,1067,The results << show >> that [[ our model ]] achieves a [[ 1.5 % gain ]] in the joint F1 - score with the entity graph built from a better entity recognizer .,0
8460,1067,The results << show >> that [[ our model ]] achieves a 1.5 % gain in the [[ joint F1 - score ]] with the entity graph built from a better entity recognizer .,0
8461,1067,The results << show >> that [[ our model ]] achieves a 1.5 % gain in the joint F1 - score with the [[ entity graph ]] built from a better entity recognizer .,0
8462,1067,The results << show >> that [[ our model ]] achieves a 1.5 % gain in the joint F1 - score with the entity graph built from a [[ better entity recognizer ]] .,0
8463,1067,The results << show >> that our model achieves a [[ 1.5 % gain ]] in the [[ joint F1 - score ]] with the entity graph built from a better entity recognizer .,0
8464,1067,The results << show >> that our model achieves a [[ 1.5 % gain ]] in the joint F1 - score with the [[ entity graph ]] built from a better entity recognizer .,0
8465,1067,The results << show >> that our model achieves a [[ 1.5 % gain ]] in the joint F1 - score with the entity graph built from a [[ better entity recognizer ]] .,0
8466,1067,The results << show >> that our model achieves a 1.5 % gain in the [[ joint F1 - score ]] with the [[ entity graph ]] built from a better entity recognizer .,0
8467,1067,The results << show >> that our model achieves a 1.5 % gain in the [[ joint F1 - score ]] with the entity graph built from a [[ better entity recognizer ]] .,0
8468,1067,The results << show >> that our model achieves a 1.5 % gain in the joint F1 - score with the [[ entity graph ]] built from a [[ better entity recognizer ]] .,0
8469,1067,The results show that [[ our model ]] << achieves >> a [[ 1.5 % gain ]] in the joint F1 - score with the entity graph built from a better entity recognizer .,1
8470,1067,The results show that [[ our model ]] << achieves >> a 1.5 % gain in the [[ joint F1 - score ]] with the entity graph built from a better entity recognizer .,0
8471,1067,The results show that [[ our model ]] << achieves >> a 1.5 % gain in the joint F1 - score with the [[ entity graph ]] built from a better entity recognizer .,0
8472,1067,The results show that [[ our model ]] << achieves >> a 1.5 % gain in the joint F1 - score with the entity graph built from a [[ better entity recognizer ]] .,0
8473,1067,The results show that our model << achieves >> a [[ 1.5 % gain ]] in the [[ joint F1 - score ]] with the entity graph built from a better entity recognizer .,0
8474,1067,The results show that our model << achieves >> a [[ 1.5 % gain ]] in the joint F1 - score with the [[ entity graph ]] built from a better entity recognizer .,0
8475,1067,The results show that our model << achieves >> a [[ 1.5 % gain ]] in the joint F1 - score with the entity graph built from a [[ better entity recognizer ]] .,0
8476,1067,The results show that our model << achieves >> a 1.5 % gain in the [[ joint F1 - score ]] with the [[ entity graph ]] built from a better entity recognizer .,0
8477,1067,The results show that our model << achieves >> a 1.5 % gain in the [[ joint F1 - score ]] with the entity graph built from a [[ better entity recognizer ]] .,0
8478,1067,The results show that our model << achieves >> a 1.5 % gain in the joint F1 - score with the [[ entity graph ]] built from a [[ better entity recognizer ]] .,0
8479,1067,The results show that [[ our model ]] achieves a [[ 1.5 % gain ]] << in >> the joint F1 - score with the entity graph built from a better entity recognizer .,0
8480,1067,The results show that [[ our model ]] achieves a 1.5 % gain << in >> the [[ joint F1 - score ]] with the entity graph built from a better entity recognizer .,0
8481,1067,The results show that [[ our model ]] achieves a 1.5 % gain << in >> the joint F1 - score with the [[ entity graph ]] built from a better entity recognizer .,0
8482,1067,The results show that [[ our model ]] achieves a 1.5 % gain << in >> the joint F1 - score with the entity graph built from a [[ better entity recognizer ]] .,0
8483,1067,The results show that our model achieves a [[ 1.5 % gain ]] << in >> the [[ joint F1 - score ]] with the entity graph built from a better entity recognizer .,1
8484,1067,The results show that our model achieves a [[ 1.5 % gain ]] << in >> the joint F1 - score with the [[ entity graph ]] built from a better entity recognizer .,0
8485,1067,The results show that our model achieves a [[ 1.5 % gain ]] << in >> the joint F1 - score with the entity graph built from a [[ better entity recognizer ]] .,0
8486,1067,The results show that our model achieves a 1.5 % gain << in >> the [[ joint F1 - score ]] with the [[ entity graph ]] built from a better entity recognizer .,0
8487,1067,The results show that our model achieves a 1.5 % gain << in >> the [[ joint F1 - score ]] with the entity graph built from a [[ better entity recognizer ]] .,0
8488,1067,The results show that our model achieves a 1.5 % gain << in >> the joint F1 - score with the [[ entity graph ]] built from a [[ better entity recognizer ]] .,0
8489,1067,The results show that [[ our model ]] achieves a [[ 1.5 % gain ]] in the joint F1 - score << with >> the entity graph built from a better entity recognizer .,0
8490,1067,The results show that [[ our model ]] achieves a 1.5 % gain in the [[ joint F1 - score ]] << with >> the entity graph built from a better entity recognizer .,0
8491,1067,The results show that [[ our model ]] achieves a 1.5 % gain in the joint F1 - score << with >> the [[ entity graph ]] built from a better entity recognizer .,0
8492,1067,The results show that [[ our model ]] achieves a 1.5 % gain in the joint F1 - score << with >> the entity graph built from a [[ better entity recognizer ]] .,0
8493,1067,The results show that our model achieves a [[ 1.5 % gain ]] in the [[ joint F1 - score ]] << with >> the entity graph built from a better entity recognizer .,0
8494,1067,The results show that our model achieves a [[ 1.5 % gain ]] in the joint F1 - score << with >> the [[ entity graph ]] built from a better entity recognizer .,1
8495,1067,The results show that our model achieves a [[ 1.5 % gain ]] in the joint F1 - score << with >> the entity graph built from a [[ better entity recognizer ]] .,0
8496,1067,The results show that our model achieves a 1.5 % gain in the [[ joint F1 - score ]] << with >> the [[ entity graph ]] built from a better entity recognizer .,0
8497,1067,The results show that our model achieves a 1.5 % gain in the [[ joint F1 - score ]] << with >> the entity graph built from a [[ better entity recognizer ]] .,0
8498,1067,The results show that our model achieves a 1.5 % gain in the joint F1 - score << with >> the [[ entity graph ]] built from a [[ better entity recognizer ]] .,0
8499,1067,The results show that [[ our model ]] achieves a [[ 1.5 % gain ]] in the joint F1 - score with the entity graph << built from >> a better entity recognizer .,0
8500,1067,The results show that [[ our model ]] achieves a 1.5 % gain in the [[ joint F1 - score ]] with the entity graph << built from >> a better entity recognizer .,0
8501,1067,The results show that [[ our model ]] achieves a 1.5 % gain in the joint F1 - score with the [[ entity graph ]] << built from >> a better entity recognizer .,0
8502,1067,The results show that [[ our model ]] achieves a 1.5 % gain in the joint F1 - score with the entity graph << built from >> a [[ better entity recognizer ]] .,0
8503,1067,The results show that our model achieves a [[ 1.5 % gain ]] in the [[ joint F1 - score ]] with the entity graph << built from >> a better entity recognizer .,0
8504,1067,The results show that our model achieves a [[ 1.5 % gain ]] in the joint F1 - score with the [[ entity graph ]] << built from >> a better entity recognizer .,0
8505,1067,The results show that our model achieves a [[ 1.5 % gain ]] in the joint F1 - score with the entity graph << built from >> a [[ better entity recognizer ]] .,0
8506,1067,The results show that our model achieves a 1.5 % gain in the [[ joint F1 - score ]] with the [[ entity graph ]] << built from >> a better entity recognizer .,0
8507,1067,The results show that our model achieves a 1.5 % gain in the [[ joint F1 - score ]] with the entity graph << built from >> a [[ better entity recognizer ]] .,0
8508,1067,The results show that our model achieves a 1.5 % gain in the joint F1 - score with the [[ entity graph ]] << built from >> a [[ better entity recognizer ]] .,1
8509,5195,"Besides , we can << see that >> the [[ compared method BiDANN ]] , which also considers the [[ bi-hemispheric asymmetry ]] , achieves a comparable performance .",0
8510,5195,"Besides , we can << see that >> the [[ compared method BiDANN ]] , which also considers the bi-hemispheric asymmetry , achieves a [[ comparable performance ]] .",0
8511,5195,"Besides , we can << see that >> the compared method BiDANN , which also considers the [[ bi-hemispheric asymmetry ]] , achieves a [[ comparable performance ]] .",0
8512,5195,"Besides , we can see that the [[ compared method BiDANN ]] , which also << considers >> the [[ bi-hemispheric asymmetry ]] , achieves a comparable performance .",1
8513,5195,"Besides , we can see that the [[ compared method BiDANN ]] , which also << considers >> the bi-hemispheric asymmetry , achieves a [[ comparable performance ]] .",0
8514,5195,"Besides , we can see that the compared method BiDANN , which also << considers >> the [[ bi-hemispheric asymmetry ]] , achieves a [[ comparable performance ]] .",0
8515,5195,"Besides , we can see that the [[ compared method BiDANN ]] , which also considers the [[ bi-hemispheric asymmetry ]] , << achieves >> a comparable performance .",0
8516,5195,"Besides , we can see that the [[ compared method BiDANN ]] , which also considers the bi-hemispheric asymmetry , << achieves >> a [[ comparable performance ]] .",1
8517,5195,"Besides , we can see that the compared method BiDANN , which also considers the [[ bi-hemispheric asymmetry ]] , << achieves >> a [[ comparable performance ]] .",0
8518,2760,Notice also that the [[ difference ]] << between >> [[ BERT and BiLSTM ]] is much bigger with this test set ( + 3.9 % compared to + 1.1 % ) .,1
8519,2760,Notice also that the [[ difference ]] << between >> BERT and BiLSTM is [[ much bigger ]] with this test set ( + 3.9 % compared to + 1.1 % ) .,0
8520,2760,Notice also that the difference << between >> [[ BERT and BiLSTM ]] is [[ much bigger ]] with this test set ( + 3.9 % compared to + 1.1 % ) .,0
8521,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8522,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8523,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8524,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8525,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8526,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8527,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8528,715,"In this work , we << propose >> an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8529,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8530,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8531,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8532,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8533,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8534,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8535,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8536,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8537,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8538,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8539,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8540,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8541,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8542,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8543,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8544,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8545,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8546,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8547,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8548,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8549,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8550,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8551,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the [[ important question words ]] to help extract the answer .",0
8552,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help [[ extract ]] the answer .",0
8553,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the [[ answer ]] .",0
8554,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help [[ extract ]] the answer .",0
8555,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the [[ answer ]] .",0
8556,715,"In this work , we << propose >> an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the [[ answer ]] .",0
8557,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",1
8558,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8559,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8560,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8561,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8562,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8563,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8564,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8565,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> [[ document - based question answering ]] ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8566,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> [[ document - based question answering ]] ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8567,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8568,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8569,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8570,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8571,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8572,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( [[ AMANDA ]] ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8573,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8574,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8575,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8576,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8577,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8578,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8579,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8580,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8581,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8582,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8583,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8584,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8585,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8586,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8587,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the [[ important question words ]] to help extract the answer .",0
8588,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help [[ extract ]] the answer .",0
8589,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the [[ answer ]] .",0
8590,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help [[ extract ]] the answer .",0
8591,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the [[ answer ]] .",0
8592,715,"In this work , we propose an end - to - end question - focused multi-factor attention network << for >> document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the [[ answer ]] .",0
8593,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for [[ document - based question answering ]] ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8594,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( [[ AMANDA ]] ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8595,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which << learns to >> [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",1
8596,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which << learns to >> aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8597,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8598,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8599,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8600,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8601,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( [[ AMANDA ]] ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8602,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which << learns to >> [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8603,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which << learns to >> aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8604,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which << learns to >> aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8605,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8606,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8607,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8608,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which << learns to >> [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8609,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which << learns to >> aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8610,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which << learns to >> aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8611,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8612,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8613,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8614,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> [[ aggregate ]] [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the answer .",0
8615,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> [[ aggregate ]] evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8616,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> [[ aggregate ]] evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8617,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8618,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8619,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate [[ evidence ]] distributed across [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8620,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate [[ evidence ]] distributed across multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8621,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8622,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8623,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across [[ multiple sentences ]] and identifies the [[ important question words ]] to help extract the answer .",0
8624,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help [[ extract ]] the answer .",0
8625,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words to help extract the [[ answer ]] .",0
8626,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help [[ extract ]] the answer .",0
8627,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] to help extract the [[ answer ]] .",0
8628,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which << learns to >> aggregate evidence distributed across multiple sentences and identifies the important question words to help [[ extract ]] the [[ answer ]] .",0
8629,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8630,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8631,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8632,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8633,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8634,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8635,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8636,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8637,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( [[ AMANDA ]] ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8638,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to [[ aggregate ]] evidence << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8639,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate [[ evidence ]] << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8640,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence << distributed across >> [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8641,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8642,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8643,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8644,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to [[ aggregate ]] evidence << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8645,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate [[ evidence ]] << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8646,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence << distributed across >> [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8647,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8648,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8649,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8650,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] [[ evidence ]] << distributed across >> multiple sentences and identifies the important question words to help extract the answer .",0
8651,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence << distributed across >> [[ multiple sentences ]] and identifies the important question words to help extract the answer .",0
8652,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence << distributed across >> multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8653,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence << distributed across >> multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8654,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence << distributed across >> multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8655,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] << distributed across >> [[ multiple sentences ]] and identifies the important question words to help extract the answer .",1
8656,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] << distributed across >> multiple sentences and identifies the [[ important question words ]] to help extract the answer .",0
8657,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] << distributed across >> multiple sentences and identifies the important question words to help [[ extract ]] the answer .",0
8658,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] << distributed across >> multiple sentences and identifies the important question words to help extract the [[ answer ]] .",0
8659,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> [[ multiple sentences ]] and identifies the [[ important question words ]] to help extract the answer .",0
8660,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> [[ multiple sentences ]] and identifies the important question words to help [[ extract ]] the answer .",0
8661,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> [[ multiple sentences ]] and identifies the important question words to help extract the [[ answer ]] .",0
8662,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the [[ important question words ]] to help [[ extract ]] the answer .",0
8663,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the [[ important question words ]] to help extract the [[ answer ]] .",0
8664,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence << distributed across >> multiple sentences and identifies the important question words to help [[ extract ]] the [[ answer ]] .",0
8665,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8666,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8667,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8668,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8669,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and << identifies >> the important question words to help extract the answer .",0
8670,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the [[ important question words ]] to help extract the answer .",1
8671,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help [[ extract ]] the answer .",0
8672,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help extract the [[ answer ]] .",0
8673,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8674,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8675,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8676,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and << identifies >> the important question words to help extract the answer .",0
8677,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the [[ important question words ]] to help extract the answer .",0
8678,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help [[ extract ]] the answer .",0
8679,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help extract the [[ answer ]] .",0
8680,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8681,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8682,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and << identifies >> the important question words to help extract the answer .",0
8683,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the [[ important question words ]] to help extract the answer .",0
8684,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help [[ extract ]] the answer .",0
8685,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help extract the [[ answer ]] .",0
8686,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] [[ evidence ]] distributed across multiple sentences and << identifies >> the important question words to help extract the answer .",0
8687,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across [[ multiple sentences ]] and << identifies >> the important question words to help extract the answer .",0
8688,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and << identifies >> the [[ important question words ]] to help extract the answer .",0
8689,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and << identifies >> the important question words to help [[ extract ]] the answer .",0
8690,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and << identifies >> the important question words to help extract the [[ answer ]] .",0
8691,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across [[ multiple sentences ]] and << identifies >> the important question words to help extract the answer .",0
8692,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and << identifies >> the [[ important question words ]] to help extract the answer .",0
8693,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and << identifies >> the important question words to help [[ extract ]] the answer .",0
8694,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and << identifies >> the important question words to help extract the [[ answer ]] .",0
8695,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and << identifies >> the [[ important question words ]] to help extract the answer .",0
8696,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and << identifies >> the important question words to help [[ extract ]] the answer .",0
8697,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and << identifies >> the important question words to help extract the [[ answer ]] .",0
8698,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the [[ important question words ]] to help [[ extract ]] the answer .",0
8699,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the [[ important question words ]] to help extract the [[ answer ]] .",0
8700,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and << identifies >> the important question words to help [[ extract ]] the [[ answer ]] .",0
8701,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8702,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8703,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8704,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8705,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words << to help >> extract the answer .",0
8706,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] << to help >> extract the answer .",0
8707,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> [[ extract ]] the answer .",0
8708,715,"In this work , we propose an [[ end - to - end question - focused multi-factor attention network ]] for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> extract the [[ answer ]] .",0
8709,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8710,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8711,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8712,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words << to help >> extract the answer .",0
8713,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] << to help >> extract the answer .",0
8714,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> [[ extract ]] the answer .",0
8715,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for [[ document - based question answering ]] ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> extract the [[ answer ]] .",0
8716,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8717,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8718,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words << to help >> extract the answer .",0
8719,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] << to help >> extract the answer .",0
8720,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> [[ extract ]] the answer .",0
8721,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( [[ AMANDA ]] ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> extract the [[ answer ]] .",0
8722,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] [[ evidence ]] distributed across multiple sentences and identifies the important question words << to help >> extract the answer .",0
8723,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across [[ multiple sentences ]] and identifies the important question words << to help >> extract the answer .",0
8724,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the [[ important question words ]] << to help >> extract the answer .",0
8725,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words << to help >> [[ extract ]] the answer .",0
8726,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to [[ aggregate ]] evidence distributed across multiple sentences and identifies the important question words << to help >> extract the [[ answer ]] .",0
8727,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across [[ multiple sentences ]] and identifies the important question words << to help >> extract the answer .",0
8728,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the [[ important question words ]] << to help >> extract the answer .",0
8729,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words << to help >> [[ extract ]] the answer .",0
8730,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate [[ evidence ]] distributed across multiple sentences and identifies the important question words << to help >> extract the [[ answer ]] .",0
8731,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the [[ important question words ]] << to help >> extract the answer .",0
8732,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words << to help >> [[ extract ]] the answer .",0
8733,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across [[ multiple sentences ]] and identifies the important question words << to help >> extract the [[ answer ]] .",0
8734,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] << to help >> [[ extract ]] the answer .",1
8735,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the [[ important question words ]] << to help >> extract the [[ answer ]] .",0
8736,715,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words << to help >> [[ extract ]] the [[ answer ]] .",0
8737,5634,"To solve the problem that the gradient can not pass back to the [[ generative model ]] when the output is discrete , we << regard >> the generative model as a [[ stochastic parametrized policy ]] .",0
8738,5634,"To solve the problem that the gradient can not pass back to the [[ generative model ]] when the output is discrete , we regard the generative model << as a >> [[ stochastic parametrized policy ]] .",1
8739,6082,All [[ inference parameters ]] are << tuned on >> a [[ 200 example subset ]] of the validation set .,1
8740,4660,"Moreover , to << exploit >> [[ commonsense knowledge ]] , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word in the utterances by referring to related knowledge entities .",0
8741,4660,"Moreover , to << exploit >> [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] in the utterances by referring to related knowledge entities .",0
8742,4660,"Moreover , to << exploit >> [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the understanding of each word in the [[ utterances ]] by referring to related knowledge entities .",0
8743,4660,"Moreover , to << exploit >> [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the understanding of each word in the utterances by referring to [[ related knowledge entities ]] .",0
8744,4660,"Moreover , to << exploit >> commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the [[ understanding of each word ]] in the utterances by referring to related knowledge entities .",0
8745,4660,"Moreover , to << exploit >> commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word in the [[ utterances ]] by referring to related knowledge entities .",0
8746,4660,"Moreover , to << exploit >> commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word in the utterances by referring to [[ related knowledge entities ]] .",0
8747,4660,"Moreover , to << exploit >> commonsense knowledge , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] in the [[ utterances ]] by referring to related knowledge entities .",0
8748,4660,"Moreover , to << exploit >> commonsense knowledge , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] in the utterances by referring to [[ related knowledge entities ]] .",0
8749,4660,"Moreover , to << exploit >> commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word in the [[ utterances ]] by referring to [[ related knowledge entities ]] .",0
8750,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we << leverage >> [[ external knowledge bases ]] to facilitate the understanding of each word in the utterances by referring to related knowledge entities .",1
8751,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we << leverage >> external knowledge bases to facilitate the [[ understanding of each word ]] in the utterances by referring to related knowledge entities .",0
8752,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we << leverage >> external knowledge bases to facilitate the understanding of each word in the [[ utterances ]] by referring to related knowledge entities .",0
8753,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we << leverage >> external knowledge bases to facilitate the understanding of each word in the utterances by referring to [[ related knowledge entities ]] .",0
8754,4660,"Moreover , to exploit commonsense knowledge , we << leverage >> [[ external knowledge bases ]] to facilitate the [[ understanding of each word ]] in the utterances by referring to related knowledge entities .",0
8755,4660,"Moreover , to exploit commonsense knowledge , we << leverage >> [[ external knowledge bases ]] to facilitate the understanding of each word in the [[ utterances ]] by referring to related knowledge entities .",0
8756,4660,"Moreover , to exploit commonsense knowledge , we << leverage >> [[ external knowledge bases ]] to facilitate the understanding of each word in the utterances by referring to [[ related knowledge entities ]] .",0
8757,4660,"Moreover , to exploit commonsense knowledge , we << leverage >> external knowledge bases to facilitate the [[ understanding of each word ]] in the [[ utterances ]] by referring to related knowledge entities .",0
8758,4660,"Moreover , to exploit commonsense knowledge , we << leverage >> external knowledge bases to facilitate the [[ understanding of each word ]] in the utterances by referring to [[ related knowledge entities ]] .",0
8759,4660,"Moreover , to exploit commonsense knowledge , we << leverage >> external knowledge bases to facilitate the understanding of each word in the [[ utterances ]] by referring to [[ related knowledge entities ]] .",0
8760,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage [[ external knowledge bases ]] << to facilitate >> the understanding of each word in the utterances by referring to related knowledge entities .",0
8761,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases << to facilitate >> the [[ understanding of each word ]] in the utterances by referring to related knowledge entities .",0
8762,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases << to facilitate >> the understanding of each word in the [[ utterances ]] by referring to related knowledge entities .",0
8763,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases << to facilitate >> the understanding of each word in the utterances by referring to [[ related knowledge entities ]] .",0
8764,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] << to facilitate >> the [[ understanding of each word ]] in the utterances by referring to related knowledge entities .",1
8765,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] << to facilitate >> the understanding of each word in the [[ utterances ]] by referring to related knowledge entities .",0
8766,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] << to facilitate >> the understanding of each word in the utterances by referring to [[ related knowledge entities ]] .",0
8767,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases << to facilitate >> the [[ understanding of each word ]] in the [[ utterances ]] by referring to related knowledge entities .",0
8768,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases << to facilitate >> the [[ understanding of each word ]] in the utterances by referring to [[ related knowledge entities ]] .",0
8769,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases << to facilitate >> the understanding of each word in the [[ utterances ]] by referring to [[ related knowledge entities ]] .",0
8770,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word << in >> the utterances by referring to related knowledge entities .",0
8771,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] << in >> the utterances by referring to related knowledge entities .",0
8772,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the understanding of each word << in >> the [[ utterances ]] by referring to related knowledge entities .",0
8773,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the understanding of each word << in >> the utterances by referring to [[ related knowledge entities ]] .",0
8774,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the [[ understanding of each word ]] << in >> the utterances by referring to related knowledge entities .",0
8775,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word << in >> the [[ utterances ]] by referring to related knowledge entities .",0
8776,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word << in >> the utterances by referring to [[ related knowledge entities ]] .",0
8777,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] << in >> the [[ utterances ]] by referring to related knowledge entities .",1
8778,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] << in >> the utterances by referring to [[ related knowledge entities ]] .",0
8779,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word << in >> the [[ utterances ]] by referring to [[ related knowledge entities ]] .",0
8780,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word in the utterances << by referring to >> related knowledge entities .",0
8781,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] in the utterances << by referring to >> related knowledge entities .",0
8782,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the understanding of each word in the [[ utterances ]] << by referring to >> related knowledge entities .",0
8783,4660,"Moreover , to exploit [[ commonsense knowledge ]] , we leverage external knowledge bases to facilitate the understanding of each word in the utterances << by referring to >> [[ related knowledge entities ]] .",0
8784,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the [[ understanding of each word ]] in the utterances << by referring to >> related knowledge entities .",0
8785,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word in the [[ utterances ]] << by referring to >> related knowledge entities .",0
8786,4660,"Moreover , to exploit commonsense knowledge , we leverage [[ external knowledge bases ]] to facilitate the understanding of each word in the utterances << by referring to >> [[ related knowledge entities ]] .",0
8787,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] in the [[ utterances ]] << by referring to >> related knowledge entities .",0
8788,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the [[ understanding of each word ]] in the utterances << by referring to >> [[ related knowledge entities ]] .",1
8789,4660,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word in the [[ utterances ]] << by referring to >> [[ related knowledge entities ]] .",0
8790,1798,[[ Hidden layers ]] << in >> the [[ feed forward neural networks ]] use rectified linear units .,1
8791,1798,[[ Hidden layers ]] << in >> the feed forward neural networks use [[ rectified linear units ]] .,0
8792,1798,Hidden layers << in >> the [[ feed forward neural networks ]] use [[ rectified linear units ]] .,0
8793,1798,[[ Hidden layers ]] in the [[ feed forward neural networks ]] << use >> rectified linear units .,0
8794,1798,[[ Hidden layers ]] in the feed forward neural networks << use >> [[ rectified linear units ]] .,1
8795,1798,Hidden layers in the [[ feed forward neural networks ]] << use >> [[ rectified linear units ]] .,0
8796,461,"Similar to 100D experiments , we << initialize >> the [[ word embedding matrix ]] with [[ GloVe 300D pretrained vectors ]] 4 , however we do not update the word representations during training .",0
8797,461,"Similar to 100D experiments , we initialize the [[ word embedding matrix ]] << with >> [[ GloVe 300D pretrained vectors ]] 4 , however we do not update the word representations during training .",1
8798,2944,We use [[ ADAMAX ]] << with >> the [[ coefficients ? 1 = 0.9 and ? 2 = 0.999 ]] to optimize the model .,1
8799,2944,We use [[ ADAMAX ]] << with >> the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the [[ model ]] .,0
8800,2944,We use ADAMAX << with >> the [[ coefficients ? 1 = 0.9 and ? 2 = 0.999 ]] to optimize the [[ model ]] .,0
8801,2944,We use [[ ADAMAX ]] with the [[ coefficients ? 1 = 0.9 and ? 2 = 0.999 ]] << to optimize >> the model .,0
8802,2944,We use [[ ADAMAX ]] with the coefficients ? 1 = 0.9 and ? 2 = 0.999 << to optimize >> the [[ model ]] .,0
8803,2944,We use ADAMAX with the [[ coefficients ? 1 = 0.9 and ? 2 = 0.999 ]] << to optimize >> the [[ model ]] .,1
8804,1869,"We then << develop >> a model , "" [[ reinforced self - attention ( ReSA ) ]] "" , which naturally combines the [[ RSS ]] with a soft self - attention .",0
8805,1869,"We then << develop >> a model , "" [[ reinforced self - attention ( ReSA ) ]] "" , which naturally combines the RSS with a [[ soft self - attention ]] .",0
8806,1869,"We then << develop >> a model , "" reinforced self - attention ( ReSA ) "" , which naturally combines the [[ RSS ]] with a [[ soft self - attention ]] .",0
8807,1869,"We then develop a model , "" [[ reinforced self - attention ( ReSA ) ]] "" , which naturally << combines >> the [[ RSS ]] with a soft self - attention .",1
8808,1869,"We then develop a model , "" [[ reinforced self - attention ( ReSA ) ]] "" , which naturally << combines >> the RSS with a [[ soft self - attention ]] .",0
8809,1869,"We then develop a model , "" reinforced self - attention ( ReSA ) "" , which naturally << combines >> the [[ RSS ]] with a [[ soft self - attention ]] .",0
8810,1869,"We then develop a model , "" [[ reinforced self - attention ( ReSA ) ]] "" , which naturally combines the [[ RSS ]] << with >> a soft self - attention .",0
8811,1869,"We then develop a model , "" [[ reinforced self - attention ( ReSA ) ]] "" , which naturally combines the RSS << with >> a [[ soft self - attention ]] .",0
8812,1869,"We then develop a model , "" reinforced self - attention ( ReSA ) "" , which naturally combines the [[ RSS ]] << with >> a [[ soft self - attention ]] .",1
8813,5489,"It << consists of >> [[ four layers ]] : [[ ngram convolutional layer ]] , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .",0
8814,5489,"It << consists of >> [[ four layers ]] : ngram convolutional layer , [[ primary capsule layer ]] , convolutional capsule layer , and fully connected capsule layer .",0
8815,5489,"It << consists of >> [[ four layers ]] : ngram convolutional layer , primary capsule layer , [[ convolutional capsule layer ]] , and fully connected capsule layer .",0
8816,5489,"It << consists of >> [[ four layers ]] : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and [[ fully connected capsule layer ]] .",0
8817,5489,"It << consists of >> four layers : [[ ngram convolutional layer ]] , [[ primary capsule layer ]] , convolutional capsule layer , and fully connected capsule layer .",0
8818,5489,"It << consists of >> four layers : [[ ngram convolutional layer ]] , primary capsule layer , [[ convolutional capsule layer ]] , and fully connected capsule layer .",0
8819,5489,"It << consists of >> four layers : [[ ngram convolutional layer ]] , primary capsule layer , convolutional capsule layer , and [[ fully connected capsule layer ]] .",0
8820,5489,"It << consists of >> four layers : ngram convolutional layer , [[ primary capsule layer ]] , [[ convolutional capsule layer ]] , and fully connected capsule layer .",0
8821,5489,"It << consists of >> four layers : ngram convolutional layer , [[ primary capsule layer ]] , convolutional capsule layer , and [[ fully connected capsule layer ]] .",0
8822,5489,"It << consists of >> four layers : ngram convolutional layer , primary capsule layer , [[ convolutional capsule layer ]] , and [[ fully connected capsule layer ]] .",0
8823,4791,"First , << to reduce >> the [[ task discrepancy ]] between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8824,4791,"First , << to reduce >> the [[ task discrepancy ]] between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8825,4791,"First , << to reduce >> the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8826,4791,"First , << to reduce >> the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8827,4791,"First , << to reduce >> the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8828,4791,"First , << to reduce >> the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8829,4791,"First , << to reduce >> the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8830,4791,"First , << to reduce >> the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8831,4791,"First , << to reduce >> the task discrepancy between [[ domains ]] , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8832,4791,"First , << to reduce >> the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8833,4791,"First , << to reduce >> the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8834,4791,"First , << to reduce >> the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8835,4791,"First , << to reduce >> the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8836,4791,"First , << to reduce >> the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8837,4791,"First , << to reduce >> the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8838,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8839,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8840,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8841,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8842,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8843,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8844,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8845,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8846,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8847,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8848,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8849,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8850,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8851,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8852,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8853,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8854,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8855,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8856,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8857,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8858,4791,"First , << to reduce >> the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8859,4791,"First , to reduce the [[ task discrepancy ]] << between >> [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",1
8860,4791,"First , to reduce the [[ task discrepancy ]] << between >> domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8861,4791,"First , to reduce the [[ task discrepancy ]] << between >> domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8862,4791,"First , to reduce the [[ task discrepancy ]] << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8863,4791,"First , to reduce the [[ task discrepancy ]] << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8864,4791,"First , to reduce the [[ task discrepancy ]] << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8865,4791,"First , to reduce the [[ task discrepancy ]] << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8866,4791,"First , to reduce the [[ task discrepancy ]] << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8867,4791,"First , to reduce the task discrepancy << between >> [[ domains ]] , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8868,4791,"First , to reduce the task discrepancy << between >> [[ domains ]] , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8869,4791,"First , to reduce the task discrepancy << between >> [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8870,4791,"First , to reduce the task discrepancy << between >> [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8871,4791,"First , to reduce the task discrepancy << between >> [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8872,4791,"First , to reduce the task discrepancy << between >> [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8873,4791,"First , to reduce the task discrepancy << between >> [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8874,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the [[ two tasks ]] at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8875,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8876,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8877,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8878,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8879,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8880,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8881,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8882,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8883,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8884,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8885,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8886,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8887,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8888,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8889,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8890,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8891,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8892,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8893,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8894,4791,"First , to reduce the task discrepancy << between >> domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8895,4791,"First , to reduce the [[ task discrepancy ]] between [[ domains ]] , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8896,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , << modeling >> the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",1
8897,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , << modeling >> the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8898,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8899,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8900,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8901,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8902,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8903,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , << modeling >> the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8904,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , << modeling >> the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8905,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8906,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8907,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8908,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8909,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8910,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the [[ two tasks ]] at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8911,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the [[ two tasks ]] at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8912,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8913,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8914,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8915,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8916,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8917,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8918,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8919,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8920,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8921,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8922,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8923,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8924,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8925,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8926,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8927,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8928,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8929,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8930,4791,"First , to reduce the task discrepancy between domains , i.e. , << modeling >> the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8931,4791,"First , to reduce the [[ task discrepancy ]] between [[ domains ]] , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8932,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the [[ two tasks ]] << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8933,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks << at >> the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8934,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8935,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8936,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8937,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8938,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8939,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the [[ two tasks ]] << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8940,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks << at >> the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8941,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8942,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8943,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8944,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8945,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8946,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] << at >> the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",1
8947,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] << at >> the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8948,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8949,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8950,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8951,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8952,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8953,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8954,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8955,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8956,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8957,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8958,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8959,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8960,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8961,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8962,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8963,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8964,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8965,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8966,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks << at >> the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8967,4791,"First , to reduce the [[ task discrepancy ]] between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8968,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8969,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8970,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",1
8971,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8972,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8973,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8974,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8975,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8976,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8977,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8978,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8979,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8980,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8981,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8982,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the [[ same fine - grained level ]] , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8983,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8984,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8985,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8986,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8987,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8988,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8989,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8990,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8991,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8992,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8993,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8994,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8995,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8996,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
8997,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8998,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
8999,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9000,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9001,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9002,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we << propose >> a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9003,4791,"First , to reduce the [[ task discrepancy ]] between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9004,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9005,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9006,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9007,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9008,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9009,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9010,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9011,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9012,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9013,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9014,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9015,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9016,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9017,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9018,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9019,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9020,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9021,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9022,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9023,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9024,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9025,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9026,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9027,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9028,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9029,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the [[ source task ]] automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",1
9030,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the source task automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9031,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the source task automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9032,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] << to help >> the source task automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9033,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the [[ source task ]] automatically capture the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9034,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the [[ source task ]] automatically capture the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9035,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the [[ source task ]] automatically capture the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9036,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the [[ corresponding aspect term ]] in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9037,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the [[ corresponding aspect term ]] in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9038,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module << to help >> the source task automatically capture the corresponding aspect term in the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9039,4791,"First , to reduce the [[ task discrepancy ]] between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9040,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9041,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9042,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9043,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9044,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9045,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9046,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9047,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9048,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9049,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9050,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9051,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9052,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9053,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9054,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9055,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9056,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9057,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9058,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9059,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9060,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9061,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9062,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9063,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9064,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9065,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] << automatically capture >> the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9066,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task << automatically capture >> the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9067,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task << automatically capture >> the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9068,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task << automatically capture >> the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9069,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] << automatically capture >> the [[ corresponding aspect term ]] in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",1
9070,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] << automatically capture >> the corresponding aspect term in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9071,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] << automatically capture >> the corresponding aspect term in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9072,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the [[ corresponding aspect term ]] in the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9073,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the [[ corresponding aspect term ]] in the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9074,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task << automatically capture >> the corresponding aspect term in the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9075,4791,"First , to reduce the [[ task discrepancy ]] between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9076,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9077,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9078,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9079,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9080,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9081,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9082,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9083,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9084,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9085,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9086,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9087,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9088,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9089,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9090,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9091,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9092,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9093,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9094,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9095,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9096,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9097,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9098,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9099,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9100,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9101,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] automatically capture the corresponding aspect term << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9102,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the [[ corresponding aspect term ]] << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9103,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term << in >> the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9104,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term << in >> the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9105,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the [[ corresponding aspect term ]] << in >> the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9106,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term << in >> the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9107,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term << in >> the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9108,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] << in >> the [[ context ]] towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",1
9109,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] << in >> the context towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9110,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term << in >> the [[ context ]] towards the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9111,4791,"First , to reduce the [[ task discrepancy ]] between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9112,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9113,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9114,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9115,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9116,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9117,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9118,4791,"First , to reduce the [[ task discrepancy ]] between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9119,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9120,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9121,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9122,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9123,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9124,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9125,4791,"First , to reduce the task discrepancy between [[ domains ]] , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9126,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9127,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9128,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9129,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9130,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9131,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the [[ two tasks ]] at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9132,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9133,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9134,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9135,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9136,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the [[ same fine - grained level ]] , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9137,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the [[ source task ]] automatically capture the corresponding aspect term in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9138,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the [[ corresponding aspect term ]] in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9139,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the [[ context ]] << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9140,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a [[ novel Coarse2 Fine ( C2F ) attention module ]] to help the source task automatically capture the corresponding aspect term in the context << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9141,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the [[ corresponding aspect term ]] in the context << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9142,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the [[ context ]] << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9143,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the [[ source task ]] automatically capture the corresponding aspect term in the context << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9144,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the [[ context ]] << towards >> the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",0
9145,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the [[ corresponding aspect term ]] in the context << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",1
9146,4791,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the [[ context ]] << towards >> the [[ given aspect category ]] ( e.g. , "" salmon "" to the "" food "" ) .",0
9147,5271,"From , we can see that [[ our simpler and lighter ML models ]] << either outperform or are comparable to >> the [[ much heavier current state - of - the art ]] on this dataset .",1
9148,5848,[[ SEASS ]] << is a >> [[ seq2seq model ]] with a selective gate mechanism .,1
9149,5848,[[ SEASS ]] << is a >> seq2seq model with a [[ selective gate mechanism ]] .,0
9150,5848,SEASS << is a >> [[ seq2seq model ]] with a [[ selective gate mechanism ]] .,0
9151,5848,[[ SEASS ]] is a [[ seq2seq model ]] << with >> a selective gate mechanism .,0
9152,5848,[[ SEASS ]] is a seq2seq model << with >> a [[ selective gate mechanism ]] .,0
9153,5848,SEASS is a [[ seq2seq model ]] << with >> a [[ selective gate mechanism ]] .,1
9154,5024,The [[ dimension ]] << of >> [[ word embedding d v ]] and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.,0
9155,5024,The [[ dimension ]] << of >> word embedding d v and [[ hidden stated ]] are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.,0
9156,5024,The dimension << of >> [[ word embedding d v ]] and [[ hidden stated ]] are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.,0
9157,2799,"[[ SENMLP ]] : We << take >> the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",1
9158,2799,"[[ SENMLP ]] : We << take >> the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
9159,2799,"[[ SENMLP ]] : We << take >> the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9160,2799,"[[ SENMLP ]] : We << take >> the whole sentence as input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9161,2799,"[[ SENMLP ]] : We << take >> the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9162,2799,"SENMLP : We << take >> the [[ whole sentence ]] as [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
9163,2799,"SENMLP : We << take >> the [[ whole sentence ]] as input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9164,2799,"SENMLP : We << take >> the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9165,2799,"SENMLP : We << take >> the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9166,2799,"SENMLP : We << take >> the whole sentence as [[ input ]] ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9167,2799,"SENMLP : We << take >> the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9168,2799,"SENMLP : We << take >> the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9169,2799,"SENMLP : We << take >> the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9170,2799,"SENMLP : We << take >> the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9171,2799,"SENMLP : We << take >> the whole sentence as input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the [[ score of coherence ]] .",0
9172,2799,"[[ SENMLP ]] : We take the [[ whole sentence ]] << as >> input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
9173,2799,"[[ SENMLP ]] : We take the whole sentence << as >> [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
9174,2799,"[[ SENMLP ]] : We take the whole sentence << as >> input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9175,2799,"[[ SENMLP ]] : We take the whole sentence << as >> input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9176,2799,"[[ SENMLP ]] : We take the whole sentence << as >> input ( with word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9177,2799,"SENMLP : We take the [[ whole sentence ]] << as >> [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",1
9178,2799,"SENMLP : We take the [[ whole sentence ]] << as >> input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9179,2799,"SENMLP : We take the [[ whole sentence ]] << as >> input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9180,2799,"SENMLP : We take the [[ whole sentence ]] << as >> input ( with word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9181,2799,"SENMLP : We take the whole sentence << as >> [[ input ]] ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9182,2799,"SENMLP : We take the whole sentence << as >> [[ input ]] ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9183,2799,"SENMLP : We take the whole sentence << as >> [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9184,2799,"SENMLP : We take the whole sentence << as >> input ( with [[ word embedding aligned sequentially ]] ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9185,2799,"SENMLP : We take the whole sentence << as >> input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9186,2799,"SENMLP : We take the whole sentence << as >> input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the [[ score of coherence ]] .",0
9187,2799,"[[ SENMLP ]] : We take the [[ whole sentence ]] as input ( << with >> word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
9188,2799,"[[ SENMLP ]] : We take the whole sentence as [[ input ]] ( << with >> word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
9189,2799,"[[ SENMLP ]] : We take the whole sentence as input ( << with >> [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9190,2799,"[[ SENMLP ]] : We take the whole sentence as input ( << with >> word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9191,2799,"[[ SENMLP ]] : We take the whole sentence as input ( << with >> word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9192,2799,"SENMLP : We take the [[ whole sentence ]] as [[ input ]] ( << with >> word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
9193,2799,"SENMLP : We take the [[ whole sentence ]] as input ( << with >> [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",0
9194,2799,"SENMLP : We take the [[ whole sentence ]] as input ( << with >> word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9195,2799,"SENMLP : We take the [[ whole sentence ]] as input ( << with >> word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9196,2799,"SENMLP : We take the whole sentence as [[ input ]] ( << with >> [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the score of coherence .",1
9197,2799,"SENMLP : We take the whole sentence as [[ input ]] ( << with >> word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9198,2799,"SENMLP : We take the whole sentence as [[ input ]] ( << with >> word embedding aligned sequentially ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9199,2799,"SENMLP : We take the whole sentence as input ( << with >> [[ word embedding aligned sequentially ]] ) , and use an [[ MLP ]] to obtain the score of coherence .",0
9200,2799,"SENMLP : We take the whole sentence as input ( << with >> [[ word embedding aligned sequentially ]] ) , and use an MLP to obtain the [[ score of coherence ]] .",0
9201,2799,"SENMLP : We take the whole sentence as input ( << with >> word embedding aligned sequentially ) , and use an [[ MLP ]] to obtain the [[ score of coherence ]] .",0
9202,2799,"[[ SENMLP ]] : We take the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and << use >> an MLP to obtain the score of coherence .",0
9203,2799,"[[ SENMLP ]] : We take the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and << use >> an MLP to obtain the score of coherence .",0
9204,2799,"[[ SENMLP ]] : We take the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and << use >> an MLP to obtain the score of coherence .",0
9205,2799,"[[ SENMLP ]] : We take the whole sentence as input ( with word embedding aligned sequentially ) , and << use >> an [[ MLP ]] to obtain the score of coherence .",1
9206,2799,"[[ SENMLP ]] : We take the whole sentence as input ( with word embedding aligned sequentially ) , and << use >> an MLP to obtain the [[ score of coherence ]] .",0
9207,2799,"SENMLP : We take the [[ whole sentence ]] as [[ input ]] ( with word embedding aligned sequentially ) , and << use >> an MLP to obtain the score of coherence .",0
9208,2799,"SENMLP : We take the [[ whole sentence ]] as input ( with [[ word embedding aligned sequentially ]] ) , and << use >> an MLP to obtain the score of coherence .",0
9209,2799,"SENMLP : We take the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and << use >> an [[ MLP ]] to obtain the score of coherence .",0
9210,2799,"SENMLP : We take the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and << use >> an MLP to obtain the [[ score of coherence ]] .",0
9211,2799,"SENMLP : We take the whole sentence as [[ input ]] ( with [[ word embedding aligned sequentially ]] ) , and << use >> an MLP to obtain the score of coherence .",0
9212,2799,"SENMLP : We take the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and << use >> an [[ MLP ]] to obtain the score of coherence .",0
9213,2799,"SENMLP : We take the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and << use >> an MLP to obtain the [[ score of coherence ]] .",0
9214,2799,"SENMLP : We take the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and << use >> an [[ MLP ]] to obtain the score of coherence .",0
9215,2799,"SENMLP : We take the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and << use >> an MLP to obtain the [[ score of coherence ]] .",0
9216,2799,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and << use >> an [[ MLP ]] to obtain the [[ score of coherence ]] .",0
9217,2799,"[[ SENMLP ]] : We take the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and use an MLP << to obtain >> the score of coherence .",0
9218,2799,"[[ SENMLP ]] : We take the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP << to obtain >> the score of coherence .",0
9219,2799,"[[ SENMLP ]] : We take the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP << to obtain >> the score of coherence .",0
9220,2799,"[[ SENMLP ]] : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] << to obtain >> the score of coherence .",0
9221,2799,"[[ SENMLP ]] : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP << to obtain >> the [[ score of coherence ]] .",0
9222,2799,"SENMLP : We take the [[ whole sentence ]] as [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP << to obtain >> the score of coherence .",0
9223,2799,"SENMLP : We take the [[ whole sentence ]] as input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP << to obtain >> the score of coherence .",0
9224,2799,"SENMLP : We take the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] << to obtain >> the score of coherence .",0
9225,2799,"SENMLP : We take the [[ whole sentence ]] as input ( with word embedding aligned sequentially ) , and use an MLP << to obtain >> the [[ score of coherence ]] .",0
9226,2799,"SENMLP : We take the whole sentence as [[ input ]] ( with [[ word embedding aligned sequentially ]] ) , and use an MLP << to obtain >> the score of coherence .",0
9227,2799,"SENMLP : We take the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and use an [[ MLP ]] << to obtain >> the score of coherence .",0
9228,2799,"SENMLP : We take the whole sentence as [[ input ]] ( with word embedding aligned sequentially ) , and use an MLP << to obtain >> the [[ score of coherence ]] .",0
9229,2799,"SENMLP : We take the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and use an [[ MLP ]] << to obtain >> the score of coherence .",0
9230,2799,"SENMLP : We take the whole sentence as input ( with [[ word embedding aligned sequentially ]] ) , and use an MLP << to obtain >> the [[ score of coherence ]] .",0
9231,2799,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an [[ MLP ]] << to obtain >> the [[ score of coherence ]] .",1
9232,3850,We << use >> these [[ representations ]] to predict [[ SRL graphs ]] directly over text spans .,0
9233,3850,We << use >> these [[ representations ]] to predict SRL graphs directly over [[ text spans ]] .,0
9234,3850,We << use >> these representations to predict [[ SRL graphs ]] directly over [[ text spans ]] .,0
9235,3850,We use these [[ representations ]] << to predict >> [[ SRL graphs ]] directly over text spans .,1
9236,3850,We use these [[ representations ]] << to predict >> SRL graphs directly over [[ text spans ]] .,0
9237,3850,We use these representations << to predict >> [[ SRL graphs ]] directly over [[ text spans ]] .,0
9238,3850,We use these [[ representations ]] to predict [[ SRL graphs ]] << directly over >> text spans .,0
9239,3850,We use these [[ representations ]] to predict SRL graphs << directly over >> [[ text spans ]] .,0
9240,3850,We use these representations to predict [[ SRL graphs ]] << directly over >> [[ text spans ]] .,1
9241,1312,"<< Of these >> , [[ 98 % ]] of [[ cases ]] emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .",0
9242,1312,"<< Of these >> , [[ 98 % ]] of cases emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from all five annotators .",0
9243,1312,"<< Of these >> , [[ 98 % ]] of cases emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9244,1312,"<< Of these >> , [[ 98 % ]] of cases emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9245,1312,"<< Of these >> , [[ 98 % ]] of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9246,1312,"<< Of these >> , 98 % of [[ cases ]] emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from all five annotators .",0
9247,1312,"<< Of these >> , 98 % of [[ cases ]] emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9248,1312,"<< Of these >> , 98 % of [[ cases ]] emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9249,1312,"<< Of these >> , 98 % of [[ cases ]] emerge with a threeannotator consensus , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9250,1312,"<< Of these >> , 98 % of cases emerge with a [[ threeannotator consensus ]] , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9251,1312,"<< Of these >> , 98 % of cases emerge with a [[ threeannotator consensus ]] , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9252,1312,"<< Of these >> , 98 % of cases emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9253,1312,"<< Of these >> , 98 % of cases emerge with a threeannotator consensus , and [[ 58 % ]] see a [[ unanimous consensus ]] from all five annotators .",0
9254,1312,"<< Of these >> , 98 % of cases emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from [[ all five annotators ]] .",0
9255,1312,"<< Of these >> , 98 % of cases emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from [[ all five annotators ]] .",0
9256,1312,"Of these , [[ 98 % ]] << of >> [[ cases ]] emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .",1
9257,1312,"Of these , [[ 98 % ]] << of >> cases emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from all five annotators .",0
9258,1312,"Of these , [[ 98 % ]] << of >> cases emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9259,1312,"Of these , [[ 98 % ]] << of >> cases emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9260,1312,"Of these , [[ 98 % ]] << of >> cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9261,1312,"Of these , 98 % << of >> [[ cases ]] emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from all five annotators .",0
9262,1312,"Of these , 98 % << of >> [[ cases ]] emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9263,1312,"Of these , 98 % << of >> [[ cases ]] emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9264,1312,"Of these , 98 % << of >> [[ cases ]] emerge with a threeannotator consensus , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9265,1312,"Of these , 98 % << of >> cases emerge with a [[ threeannotator consensus ]] , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9266,1312,"Of these , 98 % << of >> cases emerge with a [[ threeannotator consensus ]] , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9267,1312,"Of these , 98 % << of >> cases emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9268,1312,"Of these , 98 % << of >> cases emerge with a threeannotator consensus , and [[ 58 % ]] see a [[ unanimous consensus ]] from all five annotators .",0
9269,1312,"Of these , 98 % << of >> cases emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from [[ all five annotators ]] .",0
9270,1312,"Of these , 98 % << of >> cases emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from [[ all five annotators ]] .",0
9271,1312,"Of these , [[ 98 % ]] of [[ cases ]] << emerge with >> a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .",0
9272,1312,"Of these , [[ 98 % ]] of cases << emerge with >> a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from all five annotators .",0
9273,1312,"Of these , [[ 98 % ]] of cases << emerge with >> a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9274,1312,"Of these , [[ 98 % ]] of cases << emerge with >> a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9275,1312,"Of these , [[ 98 % ]] of cases << emerge with >> a threeannotator consensus , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9276,1312,"Of these , 98 % of [[ cases ]] << emerge with >> a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from all five annotators .",1
9277,1312,"Of these , 98 % of [[ cases ]] << emerge with >> a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9278,1312,"Of these , 98 % of [[ cases ]] << emerge with >> a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9279,1312,"Of these , 98 % of [[ cases ]] << emerge with >> a threeannotator consensus , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9280,1312,"Of these , 98 % of cases << emerge with >> a [[ threeannotator consensus ]] , and [[ 58 % ]] see a unanimous consensus from all five annotators .",0
9281,1312,"Of these , 98 % of cases << emerge with >> a [[ threeannotator consensus ]] , and 58 % see a [[ unanimous consensus ]] from all five annotators .",0
9282,1312,"Of these , 98 % of cases << emerge with >> a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus from [[ all five annotators ]] .",0
9283,1312,"Of these , 98 % of cases << emerge with >> a threeannotator consensus , and [[ 58 % ]] see a [[ unanimous consensus ]] from all five annotators .",0
9284,1312,"Of these , 98 % of cases << emerge with >> a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus from [[ all five annotators ]] .",0
9285,1312,"Of these , 98 % of cases << emerge with >> a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] from [[ all five annotators ]] .",0
9286,1312,"Of these , [[ 98 % ]] of [[ cases ]] emerge with a threeannotator consensus , and 58 % << see >> a unanimous consensus from all five annotators .",0
9287,1312,"Of these , [[ 98 % ]] of cases emerge with a [[ threeannotator consensus ]] , and 58 % << see >> a unanimous consensus from all five annotators .",0
9288,1312,"Of these , [[ 98 % ]] of cases emerge with a threeannotator consensus , and [[ 58 % ]] << see >> a unanimous consensus from all five annotators .",0
9289,1312,"Of these , [[ 98 % ]] of cases emerge with a threeannotator consensus , and 58 % << see >> a [[ unanimous consensus ]] from all five annotators .",0
9290,1312,"Of these , [[ 98 % ]] of cases emerge with a threeannotator consensus , and 58 % << see >> a unanimous consensus from [[ all five annotators ]] .",0
9291,1312,"Of these , 98 % of [[ cases ]] emerge with a [[ threeannotator consensus ]] , and 58 % << see >> a unanimous consensus from all five annotators .",0
9292,1312,"Of these , 98 % of [[ cases ]] emerge with a threeannotator consensus , and [[ 58 % ]] << see >> a unanimous consensus from all five annotators .",0
9293,1312,"Of these , 98 % of [[ cases ]] emerge with a threeannotator consensus , and 58 % << see >> a [[ unanimous consensus ]] from all five annotators .",0
9294,1312,"Of these , 98 % of [[ cases ]] emerge with a threeannotator consensus , and 58 % << see >> a unanimous consensus from [[ all five annotators ]] .",0
9295,1312,"Of these , 98 % of cases emerge with a [[ threeannotator consensus ]] , and [[ 58 % ]] << see >> a unanimous consensus from all five annotators .",0
9296,1312,"Of these , 98 % of cases emerge with a [[ threeannotator consensus ]] , and 58 % << see >> a [[ unanimous consensus ]] from all five annotators .",0
9297,1312,"Of these , 98 % of cases emerge with a [[ threeannotator consensus ]] , and 58 % << see >> a unanimous consensus from [[ all five annotators ]] .",0
9298,1312,"Of these , 98 % of cases emerge with a threeannotator consensus , and [[ 58 % ]] << see >> a [[ unanimous consensus ]] from all five annotators .",1
9299,1312,"Of these , 98 % of cases emerge with a threeannotator consensus , and [[ 58 % ]] << see >> a unanimous consensus from [[ all five annotators ]] .",0
9300,1312,"Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % << see >> a [[ unanimous consensus ]] from [[ all five annotators ]] .",0
9301,1312,"Of these , [[ 98 % ]] of [[ cases ]] emerge with a threeannotator consensus , and 58 % see a unanimous consensus << from >> all five annotators .",0
9302,1312,"Of these , [[ 98 % ]] of cases emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus << from >> all five annotators .",0
9303,1312,"Of these , [[ 98 % ]] of cases emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus << from >> all five annotators .",0
9304,1312,"Of these , [[ 98 % ]] of cases emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] << from >> all five annotators .",0
9305,1312,"Of these , [[ 98 % ]] of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus << from >> [[ all five annotators ]] .",0
9306,1312,"Of these , 98 % of [[ cases ]] emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus << from >> all five annotators .",0
9307,1312,"Of these , 98 % of [[ cases ]] emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus << from >> all five annotators .",0
9308,1312,"Of these , 98 % of [[ cases ]] emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] << from >> all five annotators .",0
9309,1312,"Of these , 98 % of [[ cases ]] emerge with a threeannotator consensus , and 58 % see a unanimous consensus << from >> [[ all five annotators ]] .",0
9310,1312,"Of these , 98 % of cases emerge with a [[ threeannotator consensus ]] , and [[ 58 % ]] see a unanimous consensus << from >> all five annotators .",0
9311,1312,"Of these , 98 % of cases emerge with a [[ threeannotator consensus ]] , and 58 % see a [[ unanimous consensus ]] << from >> all five annotators .",0
9312,1312,"Of these , 98 % of cases emerge with a [[ threeannotator consensus ]] , and 58 % see a unanimous consensus << from >> [[ all five annotators ]] .",0
9313,1312,"Of these , 98 % of cases emerge with a threeannotator consensus , and [[ 58 % ]] see a [[ unanimous consensus ]] << from >> all five annotators .",0
9314,1312,"Of these , 98 % of cases emerge with a threeannotator consensus , and [[ 58 % ]] see a unanimous consensus << from >> [[ all five annotators ]] .",0
9315,1312,"Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % see a [[ unanimous consensus ]] << from >> [[ all five annotators ]] .",1
9316,648,"To address the first problem , we << present >> a [[ reattention mechanism ]] that [[ temporally memorizes ]] past attentions and uses them to refine current attentions in a multi-round alignment architecture .",0
9317,648,"To address the first problem , we << present >> a [[ reattention mechanism ]] that temporally memorizes [[ past attentions ]] and uses them to refine current attentions in a multi-round alignment architecture .",0
9318,648,"To address the first problem , we << present >> a [[ reattention mechanism ]] that temporally memorizes past attentions and uses them to refine [[ current attentions ]] in a multi-round alignment architecture .",0
9319,648,"To address the first problem , we << present >> a [[ reattention mechanism ]] that temporally memorizes past attentions and uses them to refine current attentions in a [[ multi-round alignment architecture ]] .",0
9320,648,"To address the first problem , we << present >> a reattention mechanism that [[ temporally memorizes ]] [[ past attentions ]] and uses them to refine current attentions in a multi-round alignment architecture .",0
9321,648,"To address the first problem , we << present >> a reattention mechanism that [[ temporally memorizes ]] past attentions and uses them to refine [[ current attentions ]] in a multi-round alignment architecture .",0
9322,648,"To address the first problem , we << present >> a reattention mechanism that [[ temporally memorizes ]] past attentions and uses them to refine current attentions in a [[ multi-round alignment architecture ]] .",0
9323,648,"To address the first problem , we << present >> a reattention mechanism that temporally memorizes [[ past attentions ]] and uses them to refine [[ current attentions ]] in a multi-round alignment architecture .",0
9324,648,"To address the first problem , we << present >> a reattention mechanism that temporally memorizes [[ past attentions ]] and uses them to refine current attentions in a [[ multi-round alignment architecture ]] .",0
9325,648,"To address the first problem , we << present >> a reattention mechanism that temporally memorizes past attentions and uses them to refine [[ current attentions ]] in a [[ multi-round alignment architecture ]] .",0
9326,648,"To address the first problem , we present a [[ reattention mechanism ]] << that >> [[ temporally memorizes ]] past attentions and uses them to refine current attentions in a multi-round alignment architecture .",1
9327,648,"To address the first problem , we present a [[ reattention mechanism ]] << that >> temporally memorizes [[ past attentions ]] and uses them to refine current attentions in a multi-round alignment architecture .",0
9328,648,"To address the first problem , we present a [[ reattention mechanism ]] << that >> temporally memorizes past attentions and uses them to refine [[ current attentions ]] in a multi-round alignment architecture .",0
9329,648,"To address the first problem , we present a [[ reattention mechanism ]] << that >> temporally memorizes past attentions and uses them to refine current attentions in a [[ multi-round alignment architecture ]] .",0
9330,648,"To address the first problem , we present a reattention mechanism << that >> [[ temporally memorizes ]] [[ past attentions ]] and uses them to refine current attentions in a multi-round alignment architecture .",0
9331,648,"To address the first problem , we present a reattention mechanism << that >> [[ temporally memorizes ]] past attentions and uses them to refine [[ current attentions ]] in a multi-round alignment architecture .",0
9332,648,"To address the first problem , we present a reattention mechanism << that >> [[ temporally memorizes ]] past attentions and uses them to refine current attentions in a [[ multi-round alignment architecture ]] .",0
9333,648,"To address the first problem , we present a reattention mechanism << that >> temporally memorizes [[ past attentions ]] and uses them to refine [[ current attentions ]] in a multi-round alignment architecture .",0
9334,648,"To address the first problem , we present a reattention mechanism << that >> temporally memorizes [[ past attentions ]] and uses them to refine current attentions in a [[ multi-round alignment architecture ]] .",0
9335,648,"To address the first problem , we present a reattention mechanism << that >> temporally memorizes past attentions and uses them to refine [[ current attentions ]] in a [[ multi-round alignment architecture ]] .",0
9336,648,"To address the first problem , we present a [[ reattention mechanism ]] that [[ temporally memorizes ]] past attentions and uses them << to refine >> current attentions in a multi-round alignment architecture .",0
9337,648,"To address the first problem , we present a [[ reattention mechanism ]] that temporally memorizes [[ past attentions ]] and uses them << to refine >> current attentions in a multi-round alignment architecture .",0
9338,648,"To address the first problem , we present a [[ reattention mechanism ]] that temporally memorizes past attentions and uses them << to refine >> [[ current attentions ]] in a multi-round alignment architecture .",0
9339,648,"To address the first problem , we present a [[ reattention mechanism ]] that temporally memorizes past attentions and uses them << to refine >> current attentions in a [[ multi-round alignment architecture ]] .",0
9340,648,"To address the first problem , we present a reattention mechanism that [[ temporally memorizes ]] [[ past attentions ]] and uses them << to refine >> current attentions in a multi-round alignment architecture .",0
9341,648,"To address the first problem , we present a reattention mechanism that [[ temporally memorizes ]] past attentions and uses them << to refine >> [[ current attentions ]] in a multi-round alignment architecture .",0
9342,648,"To address the first problem , we present a reattention mechanism that [[ temporally memorizes ]] past attentions and uses them << to refine >> current attentions in a [[ multi-round alignment architecture ]] .",0
9343,648,"To address the first problem , we present a reattention mechanism that temporally memorizes [[ past attentions ]] and uses them << to refine >> [[ current attentions ]] in a multi-round alignment architecture .",1
9344,648,"To address the first problem , we present a reattention mechanism that temporally memorizes [[ past attentions ]] and uses them << to refine >> current attentions in a [[ multi-round alignment architecture ]] .",0
9345,648,"To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them << to refine >> [[ current attentions ]] in a [[ multi-round alignment architecture ]] .",0
9346,648,"To address the first problem , we present a [[ reattention mechanism ]] that [[ temporally memorizes ]] past attentions and uses them to refine current attentions << in >> a multi-round alignment architecture .",0
9347,648,"To address the first problem , we present a [[ reattention mechanism ]] that temporally memorizes [[ past attentions ]] and uses them to refine current attentions << in >> a multi-round alignment architecture .",0
9348,648,"To address the first problem , we present a [[ reattention mechanism ]] that temporally memorizes past attentions and uses them to refine [[ current attentions ]] << in >> a multi-round alignment architecture .",0
9349,648,"To address the first problem , we present a [[ reattention mechanism ]] that temporally memorizes past attentions and uses them to refine current attentions << in >> a [[ multi-round alignment architecture ]] .",0
9350,648,"To address the first problem , we present a reattention mechanism that [[ temporally memorizes ]] [[ past attentions ]] and uses them to refine current attentions << in >> a multi-round alignment architecture .",0
9351,648,"To address the first problem , we present a reattention mechanism that [[ temporally memorizes ]] past attentions and uses them to refine [[ current attentions ]] << in >> a multi-round alignment architecture .",0
9352,648,"To address the first problem , we present a reattention mechanism that [[ temporally memorizes ]] past attentions and uses them to refine current attentions << in >> a [[ multi-round alignment architecture ]] .",0
9353,648,"To address the first problem , we present a reattention mechanism that temporally memorizes [[ past attentions ]] and uses them to refine [[ current attentions ]] << in >> a multi-round alignment architecture .",0
9354,648,"To address the first problem , we present a reattention mechanism that temporally memorizes [[ past attentions ]] and uses them to refine current attentions << in >> a [[ multi-round alignment architecture ]] .",0
9355,648,"To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them to refine [[ current attentions ]] << in >> a [[ multi-round alignment architecture ]] .",1
9356,3238,"We compare our method with several classical triplet extraction methods , which can be << divided into >> the following categories : the [[ pipelined methods ]] , the [[ jointly extracting methods ]] and the end - to - end methods based our tagging scheme .",0
9357,3238,"We compare our method with several classical triplet extraction methods , which can be << divided into >> the following categories : the [[ pipelined methods ]] , the jointly extracting methods and the [[ end - to - end methods ]] based our tagging scheme .",0
9358,3238,"We compare our method with several classical triplet extraction methods , which can be << divided into >> the following categories : the pipelined methods , the [[ jointly extracting methods ]] and the [[ end - to - end methods ]] based our tagging scheme .",0
9359,4409,"Based on the analysis above , in this paper , we << propose >> a [[ hierarchical attention based positionaware network ( HAPN ) ]] for [[ aspect - level sentiment classification ]] .",0
9360,4409,"Based on the analysis above , in this paper , we propose a [[ hierarchical attention based positionaware network ( HAPN ) ]] << for >> [[ aspect - level sentiment classification ]] .",1
9361,155,"We << choose >> [[ two recent works ]] as [[ our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model ]] , which achieves state - of - the - art performance .",0
9362,155,"We choose [[ two recent works ]] << as >> [[ our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model ]] , which achieves state - of - the - art performance .",1
9363,3428,"In this paper , we << propose >> a [[ novel RNN - based model ]] for the [[ joint extraction of entity mentions and relations ]] .",0
9364,3428,"In this paper , we propose a [[ novel RNN - based model ]] << for >> the [[ joint extraction of entity mentions and relations ]] .",1
9365,3453,We conduct experiments on two SRL tasks : and the [[ predicate indicator embedding size ]] << is >> [[ 10 ]] .,1
9366,1303,"The [[ AS Reader ]] , which was the better - performing model on the summaries task , [[ underperforms ]] the simple no -context Seq2Seq baseline ( shown in ) << in terms of >> MRR .",0
9367,1303,"The [[ AS Reader ]] , which was the better - performing model on the summaries task , underperforms the [[ simple no -context Seq2Seq baseline ]] ( shown in ) << in terms of >> MRR .",0
9368,1303,"The [[ AS Reader ]] , which was the better - performing model on the summaries task , underperforms the simple no -context Seq2Seq baseline ( shown in ) << in terms of >> [[ MRR ]] .",0
9369,1303,"The AS Reader , which was the better - performing model on the summaries task , [[ underperforms ]] the [[ simple no -context Seq2Seq baseline ]] ( shown in ) << in terms of >> MRR .",0
9370,1303,"The AS Reader , which was the better - performing model on the summaries task , [[ underperforms ]] the simple no -context Seq2Seq baseline ( shown in ) << in terms of >> [[ MRR ]] .",1
9371,1303,"The AS Reader , which was the better - performing model on the summaries task , underperforms the [[ simple no -context Seq2Seq baseline ]] ( shown in ) << in terms of >> [[ MRR ]] .",0
9372,1150,"In contrast , we << focus on >> [[ capturing ]] [[ fine - grained word - level information ]] directly .",0
9373,3047,"Finally , we << train >> [[ FVTA without photos ]] to see the [[ contribution ]] of visual information .",0
9374,3047,"Finally , we << train >> [[ FVTA without photos ]] to see the contribution of [[ visual information ]] .",0
9375,3047,"Finally , we << train >> FVTA without photos to see the [[ contribution ]] of [[ visual information ]] .",0
9376,3047,"Finally , we train [[ FVTA without photos ]] << to see >> the [[ contribution ]] of visual information .",1
9377,3047,"Finally , we train [[ FVTA without photos ]] << to see >> the contribution of [[ visual information ]] .",0
9378,3047,"Finally , we train FVTA without photos << to see >> the [[ contribution ]] of [[ visual information ]] .",0
9379,3047,"Finally , we train [[ FVTA without photos ]] to see the [[ contribution ]] << of >> visual information .",0
9380,3047,"Finally , we train [[ FVTA without photos ]] to see the contribution << of >> [[ visual information ]] .",0
9381,3047,"Finally , we train FVTA without photos to see the [[ contribution ]] << of >> [[ visual information ]] .",1
9382,5897,We << note >> that the [[ entropy of C2F ]] is [[ very low ]] ( before taking the argmax at test time ) .,0
9383,5897,We note that the [[ entropy of C2F ]] << is >> [[ very low ]] ( before taking the argmax at test time ) .,1
9384,1046,"In this paper , we << propose >> [[ Dynamically Fused Graph Network ( DFGN ) ]] , a [[ novel method ]] to address the aforementioned concerns for multi-hop text - based QA .",0
9385,5591,"We << use >> the [[ Adam optimizer ]] with [[ ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 ]] and follow the same learning rate schedule in .",0
9386,5591,"We use the [[ Adam optimizer ]] << with >> [[ ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 ]] and follow the same learning rate schedule in .",1
9387,3439,We << regularize >> [[ our network ]] using [[ dropout ]] with the drop - out rate tuned using development set .,0
9388,3439,We << regularize >> [[ our network ]] using dropout with the [[ drop - out rate ]] tuned using development set .,0
9389,3439,We << regularize >> [[ our network ]] using dropout with the drop - out rate tuned using [[ development set ]] .,0
9390,3439,We << regularize >> our network using [[ dropout ]] with the [[ drop - out rate ]] tuned using development set .,0
9391,3439,We << regularize >> our network using [[ dropout ]] with the drop - out rate tuned using [[ development set ]] .,0
9392,3439,We << regularize >> our network using dropout with the [[ drop - out rate ]] tuned using [[ development set ]] .,0
9393,3439,We regularize [[ our network ]] << using >> [[ dropout ]] with the drop - out rate tuned using development set .,1
9394,3439,We regularize [[ our network ]] << using >> dropout with the [[ drop - out rate ]] tuned using development set .,0
9395,3439,We regularize [[ our network ]] << using >> dropout with the drop - out rate tuned using [[ development set ]] .,0
9396,3439,We regularize our network << using >> [[ dropout ]] with the [[ drop - out rate ]] tuned using development set .,0
9397,3439,We regularize our network << using >> [[ dropout ]] with the drop - out rate tuned using [[ development set ]] .,0
9398,3439,We regularize our network << using >> dropout with the [[ drop - out rate ]] tuned using [[ development set ]] .,0
9399,3439,We regularize [[ our network ]] using [[ dropout ]] << with >> the drop - out rate tuned using development set .,0
9400,3439,We regularize [[ our network ]] using dropout << with >> the [[ drop - out rate ]] tuned using development set .,0
9401,3439,We regularize [[ our network ]] using dropout << with >> the drop - out rate tuned using [[ development set ]] .,0
9402,3439,We regularize our network using [[ dropout ]] << with >> the [[ drop - out rate ]] tuned using development set .,1
9403,3439,We regularize our network using [[ dropout ]] << with >> the drop - out rate tuned using [[ development set ]] .,0
9404,3439,We regularize our network using dropout << with >> the [[ drop - out rate ]] tuned using [[ development set ]] .,0
9405,3439,We regularize [[ our network ]] using [[ dropout ]] with the drop - out rate << tuned using >> development set .,0
9406,3439,We regularize [[ our network ]] using dropout with the [[ drop - out rate ]] << tuned using >> development set .,0
9407,3439,We regularize [[ our network ]] using dropout with the drop - out rate << tuned using >> [[ development set ]] .,0
9408,3439,We regularize our network using [[ dropout ]] with the [[ drop - out rate ]] << tuned using >> development set .,0
9409,3439,We regularize our network using [[ dropout ]] with the drop - out rate << tuned using >> [[ development set ]] .,0
9410,3439,We regularize our network using dropout with the [[ drop - out rate ]] << tuned using >> [[ development set ]] .,1
9411,4121,"[[ Local interchannel relations ]] << connect >> [[ nearby groups of neurons ]] and may reveal anatomical connectivity at macroscale , .",1
9412,4121,"[[ Local interchannel relations ]] << connect >> nearby groups of neurons and may reveal [[ anatomical connectivity ]] at macroscale , .",0
9413,4121,"[[ Local interchannel relations ]] << connect >> nearby groups of neurons and may reveal anatomical connectivity at [[ macroscale ]] , .",0
9414,4121,"Local interchannel relations << connect >> [[ nearby groups of neurons ]] and may reveal [[ anatomical connectivity ]] at macroscale , .",0
9415,4121,"Local interchannel relations << connect >> [[ nearby groups of neurons ]] and may reveal anatomical connectivity at [[ macroscale ]] , .",0
9416,4121,"Local interchannel relations << connect >> nearby groups of neurons and may reveal [[ anatomical connectivity ]] at [[ macroscale ]] , .",0
9417,4121,"[[ Local interchannel relations ]] connect [[ nearby groups of neurons ]] and may << reveal >> anatomical connectivity at macroscale , .",0
9418,4121,"[[ Local interchannel relations ]] connect nearby groups of neurons and may << reveal >> [[ anatomical connectivity ]] at macroscale , .",1
9419,4121,"[[ Local interchannel relations ]] connect nearby groups of neurons and may << reveal >> anatomical connectivity at [[ macroscale ]] , .",0
9420,4121,"Local interchannel relations connect [[ nearby groups of neurons ]] and may << reveal >> [[ anatomical connectivity ]] at macroscale , .",0
9421,4121,"Local interchannel relations connect [[ nearby groups of neurons ]] and may << reveal >> anatomical connectivity at [[ macroscale ]] , .",0
9422,4121,"Local interchannel relations connect nearby groups of neurons and may << reveal >> [[ anatomical connectivity ]] at [[ macroscale ]] , .",0
9423,4121,"[[ Local interchannel relations ]] connect [[ nearby groups of neurons ]] and may reveal anatomical connectivity << at >> macroscale , .",0
9424,4121,"[[ Local interchannel relations ]] connect nearby groups of neurons and may reveal [[ anatomical connectivity ]] << at >> macroscale , .",0
9425,4121,"[[ Local interchannel relations ]] connect nearby groups of neurons and may reveal anatomical connectivity << at >> [[ macroscale ]] , .",0
9426,4121,"Local interchannel relations connect [[ nearby groups of neurons ]] and may reveal [[ anatomical connectivity ]] << at >> macroscale , .",0
9427,4121,"Local interchannel relations connect [[ nearby groups of neurons ]] and may reveal anatomical connectivity << at >> [[ macroscale ]] , .",0
9428,4121,"Local interchannel relations connect nearby groups of neurons and may reveal [[ anatomical connectivity ]] << at >> [[ macroscale ]] , .",1
9429,3136,"In this paper , we << propose >> a [[ method ]] to improve [[ open - domain QA ]] by explicitly aggregating evidence from across multiple passages .",0
9430,3136,"In this paper , we << propose >> a [[ method ]] to improve open - domain QA by explicitly aggregating [[ evidence ]] from across multiple passages .",0
9431,3136,"In this paper , we << propose >> a [[ method ]] to improve open - domain QA by explicitly aggregating evidence from across [[ multiple passages ]] .",0
9432,3136,"In this paper , we << propose >> a method to improve [[ open - domain QA ]] by explicitly aggregating [[ evidence ]] from across multiple passages .",0
9433,3136,"In this paper , we << propose >> a method to improve [[ open - domain QA ]] by explicitly aggregating evidence from across [[ multiple passages ]] .",0
9434,3136,"In this paper , we << propose >> a method to improve open - domain QA by explicitly aggregating [[ evidence ]] from across [[ multiple passages ]] .",0
9435,3136,"In this paper , we propose a [[ method ]] << to improve >> [[ open - domain QA ]] by explicitly aggregating evidence from across multiple passages .",1
9436,3136,"In this paper , we propose a [[ method ]] << to improve >> open - domain QA by explicitly aggregating [[ evidence ]] from across multiple passages .",0
9437,3136,"In this paper , we propose a [[ method ]] << to improve >> open - domain QA by explicitly aggregating evidence from across [[ multiple passages ]] .",0
9438,3136,"In this paper , we propose a method << to improve >> [[ open - domain QA ]] by explicitly aggregating [[ evidence ]] from across multiple passages .",0
9439,3136,"In this paper , we propose a method << to improve >> [[ open - domain QA ]] by explicitly aggregating evidence from across [[ multiple passages ]] .",0
9440,3136,"In this paper , we propose a method << to improve >> open - domain QA by explicitly aggregating [[ evidence ]] from across [[ multiple passages ]] .",0
9441,3136,"In this paper , we propose a [[ method ]] to improve [[ open - domain QA ]] << by explicitly aggregating >> evidence from across multiple passages .",0
9442,3136,"In this paper , we propose a [[ method ]] to improve open - domain QA << by explicitly aggregating >> [[ evidence ]] from across multiple passages .",1
9443,3136,"In this paper , we propose a [[ method ]] to improve open - domain QA << by explicitly aggregating >> evidence from across [[ multiple passages ]] .",0
9444,3136,"In this paper , we propose a method to improve [[ open - domain QA ]] << by explicitly aggregating >> [[ evidence ]] from across multiple passages .",0
9445,3136,"In this paper , we propose a method to improve [[ open - domain QA ]] << by explicitly aggregating >> evidence from across [[ multiple passages ]] .",0
9446,3136,"In this paper , we propose a method to improve open - domain QA << by explicitly aggregating >> [[ evidence ]] from across [[ multiple passages ]] .",0
9447,3136,"In this paper , we propose a [[ method ]] to improve [[ open - domain QA ]] by explicitly aggregating evidence << from across >> multiple passages .",0
9448,3136,"In this paper , we propose a [[ method ]] to improve open - domain QA by explicitly aggregating [[ evidence ]] << from across >> multiple passages .",0
9449,3136,"In this paper , we propose a [[ method ]] to improve open - domain QA by explicitly aggregating evidence << from across >> [[ multiple passages ]] .",0
9450,3136,"In this paper , we propose a method to improve [[ open - domain QA ]] by explicitly aggregating [[ evidence ]] << from across >> multiple passages .",0
9451,3136,"In this paper , we propose a method to improve [[ open - domain QA ]] by explicitly aggregating evidence << from across >> [[ multiple passages ]] .",0
9452,3136,"In this paper , we propose a method to improve open - domain QA by explicitly aggregating [[ evidence ]] << from across >> [[ multiple passages ]] .",1
9453,5417,"The [[ minibatch size ]] << is set as >> [[ 128 ]] , and a dropout rate of 0.2 is utilized on the embedding layer .",1
9454,5417,"The [[ minibatch size ]] << is set as >> 128 , and a [[ dropout rate ]] of 0.2 is utilized on the embedding layer .",0
9455,5417,"The [[ minibatch size ]] << is set as >> 128 , and a dropout rate of [[ 0.2 ]] is utilized on the embedding layer .",0
9456,5417,"The [[ minibatch size ]] << is set as >> 128 , and a dropout rate of 0.2 is utilized on the [[ embedding layer ]] .",0
9457,5417,"The minibatch size << is set as >> [[ 128 ]] , and a [[ dropout rate ]] of 0.2 is utilized on the embedding layer .",0
9458,5417,"The minibatch size << is set as >> [[ 128 ]] , and a dropout rate of [[ 0.2 ]] is utilized on the embedding layer .",0
9459,5417,"The minibatch size << is set as >> [[ 128 ]] , and a dropout rate of 0.2 is utilized on the [[ embedding layer ]] .",0
9460,5417,"The minibatch size << is set as >> 128 , and a [[ dropout rate ]] of [[ 0.2 ]] is utilized on the embedding layer .",0
9461,5417,"The minibatch size << is set as >> 128 , and a [[ dropout rate ]] of 0.2 is utilized on the [[ embedding layer ]] .",0
9462,5417,"The minibatch size << is set as >> 128 , and a dropout rate of [[ 0.2 ]] is utilized on the [[ embedding layer ]] .",0
9463,5417,"The [[ minibatch size ]] is set as [[ 128 ]] , and a dropout rate << of >> 0.2 is utilized on the embedding layer .",0
9464,5417,"The [[ minibatch size ]] is set as 128 , and a [[ dropout rate ]] << of >> 0.2 is utilized on the embedding layer .",0
9465,5417,"The [[ minibatch size ]] is set as 128 , and a dropout rate << of >> [[ 0.2 ]] is utilized on the embedding layer .",0
9466,5417,"The [[ minibatch size ]] is set as 128 , and a dropout rate << of >> 0.2 is utilized on the [[ embedding layer ]] .",0
9467,5417,"The minibatch size is set as [[ 128 ]] , and a [[ dropout rate ]] << of >> 0.2 is utilized on the embedding layer .",0
9468,5417,"The minibatch size is set as [[ 128 ]] , and a dropout rate << of >> [[ 0.2 ]] is utilized on the embedding layer .",0
9469,5417,"The minibatch size is set as [[ 128 ]] , and a dropout rate << of >> 0.2 is utilized on the [[ embedding layer ]] .",0
9470,5417,"The minibatch size is set as 128 , and a [[ dropout rate ]] << of >> [[ 0.2 ]] is utilized on the embedding layer .",1
9471,5417,"The minibatch size is set as 128 , and a [[ dropout rate ]] << of >> 0.2 is utilized on the [[ embedding layer ]] .",0
9472,5417,"The minibatch size is set as 128 , and a dropout rate << of >> [[ 0.2 ]] is utilized on the [[ embedding layer ]] .",0
9473,5417,"The [[ minibatch size ]] is set as [[ 128 ]] , and a dropout rate of 0.2 << is utilized on >> the embedding layer .",0
9474,5417,"The [[ minibatch size ]] is set as 128 , and a [[ dropout rate ]] of 0.2 << is utilized on >> the embedding layer .",0
9475,5417,"The [[ minibatch size ]] is set as 128 , and a dropout rate of [[ 0.2 ]] << is utilized on >> the embedding layer .",0
9476,5417,"The [[ minibatch size ]] is set as 128 , and a dropout rate of 0.2 << is utilized on >> the [[ embedding layer ]] .",0
9477,5417,"The minibatch size is set as [[ 128 ]] , and a [[ dropout rate ]] of 0.2 << is utilized on >> the embedding layer .",0
9478,5417,"The minibatch size is set as [[ 128 ]] , and a dropout rate of [[ 0.2 ]] << is utilized on >> the embedding layer .",0
9479,5417,"The minibatch size is set as [[ 128 ]] , and a dropout rate of 0.2 << is utilized on >> the [[ embedding layer ]] .",0
9480,5417,"The minibatch size is set as 128 , and a [[ dropout rate ]] of [[ 0.2 ]] << is utilized on >> the embedding layer .",0
9481,5417,"The minibatch size is set as 128 , and a [[ dropout rate ]] of 0.2 << is utilized on >> the [[ embedding layer ]] .",0
9482,5417,"The minibatch size is set as 128 , and a dropout rate of [[ 0.2 ]] << is utilized on >> the [[ embedding layer ]] .",1
9483,4979,"<< In >> [[ our experiments ]] , [[ all word vectors ]] are initialized by Glove 1 .",0
9484,4979,"<< In >> [[ our experiments ]] , all word vectors are [[ initialized ]] by Glove 1 .",0
9485,4979,"<< In >> [[ our experiments ]] , all word vectors are initialized by [[ Glove ]] 1 .",0
9486,4979,"<< In >> our experiments , [[ all word vectors ]] are [[ initialized ]] by Glove 1 .",0
9487,4979,"<< In >> our experiments , [[ all word vectors ]] are initialized by [[ Glove ]] 1 .",0
9488,4979,"<< In >> our experiments , all word vectors are [[ initialized ]] by [[ Glove ]] 1 .",0
9489,4979,"In [[ our experiments ]] , [[ all word vectors ]] << are >> initialized by Glove 1 .",0
9490,4979,"In [[ our experiments ]] , all word vectors << are >> [[ initialized ]] by Glove 1 .",0
9491,4979,"In [[ our experiments ]] , all word vectors << are >> initialized by [[ Glove ]] 1 .",0
9492,4979,"In our experiments , [[ all word vectors ]] << are >> [[ initialized ]] by Glove 1 .",1
9493,4979,"In our experiments , [[ all word vectors ]] << are >> initialized by [[ Glove ]] 1 .",0
9494,4979,"In our experiments , all word vectors << are >> [[ initialized ]] by [[ Glove ]] 1 .",0
9495,4979,"In [[ our experiments ]] , [[ all word vectors ]] are initialized << by >> Glove 1 .",0
9496,4979,"In [[ our experiments ]] , all word vectors are [[ initialized ]] << by >> Glove 1 .",0
9497,4979,"In [[ our experiments ]] , all word vectors are initialized << by >> [[ Glove ]] 1 .",0
9498,4979,"In our experiments , [[ all word vectors ]] are [[ initialized ]] << by >> Glove 1 .",0
9499,4979,"In our experiments , [[ all word vectors ]] are initialized << by >> [[ Glove ]] 1 .",0
9500,4979,"In our experiments , all word vectors are [[ initialized ]] << by >> [[ Glove ]] 1 .",1
9501,3098,"On this dataset , the [[ key competitors ]] << are >> [[ BiDAF ]] , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .",1
9502,3098,"On this dataset , the [[ key competitors ]] << are >> BiDAF , [[ Match - LSTM ]] , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .",1
9503,3098,"On this dataset , the [[ key competitors ]] << are >> BiDAF , Match - LSTM , [[ FastQA / Fast QA - Ext ]] , R2-BiLSTM , AMANDA .",1
9504,3098,"On this dataset , the [[ key competitors ]] << are >> BiDAF , Match - LSTM , FastQA / Fast QA - Ext , [[ R2-BiLSTM ]] , AMANDA .",1
9505,3098,"On this dataset , the [[ key competitors ]] << are >> BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , [[ AMANDA ]] .",1
9506,3098,"On this dataset , the key competitors << are >> [[ BiDAF ]] , [[ Match - LSTM ]] , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .",0
9507,3098,"On this dataset , the key competitors << are >> [[ BiDAF ]] , Match - LSTM , [[ FastQA / Fast QA - Ext ]] , R2-BiLSTM , AMANDA .",0
9508,3098,"On this dataset , the key competitors << are >> [[ BiDAF ]] , Match - LSTM , FastQA / Fast QA - Ext , [[ R2-BiLSTM ]] , AMANDA .",0
9509,3098,"On this dataset , the key competitors << are >> [[ BiDAF ]] , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , [[ AMANDA ]] .",0
9510,3098,"On this dataset , the key competitors << are >> BiDAF , [[ Match - LSTM ]] , [[ FastQA / Fast QA - Ext ]] , R2-BiLSTM , AMANDA .",0
9511,3098,"On this dataset , the key competitors << are >> BiDAF , [[ Match - LSTM ]] , FastQA / Fast QA - Ext , [[ R2-BiLSTM ]] , AMANDA .",0
9512,3098,"On this dataset , the key competitors << are >> BiDAF , [[ Match - LSTM ]] , FastQA / Fast QA - Ext , R2-BiLSTM , [[ AMANDA ]] .",0
9513,3098,"On this dataset , the key competitors << are >> BiDAF , Match - LSTM , [[ FastQA / Fast QA - Ext ]] , [[ R2-BiLSTM ]] , AMANDA .",0
9514,3098,"On this dataset , the key competitors << are >> BiDAF , Match - LSTM , [[ FastQA / Fast QA - Ext ]] , R2-BiLSTM , [[ AMANDA ]] .",0
9515,3098,"On this dataset , the key competitors << are >> BiDAF , Match - LSTM , FastQA / Fast QA - Ext , [[ R2-BiLSTM ]] , [[ AMANDA ]] .",0
9516,497,"The [[ initial reading module and the task module ]] << are learnt >> [[ jointly , end - to - end ]] .",1
9517,4611,[[ AEN - GloVe w/ o PCT ]] << ablates >> [[ PCT module ]] .,1
9518,6094,"Depending on the current state of the RNN , the [[ encoder ]] << computes scores over >> the [[ words ]] in the input sentence .",1
9519,6094,"Depending on the current state of the RNN , the [[ encoder ]] << computes scores over >> the words in the [[ input sentence ]] .",0
9520,6094,"Depending on the current state of the RNN , the encoder << computes scores over >> the [[ words ]] in the [[ input sentence ]] .",0
9521,6094,"Depending on the current state of the RNN , the [[ encoder ]] computes scores over the [[ words ]] << in >> the input sentence .",0
9522,6094,"Depending on the current state of the RNN , the [[ encoder ]] computes scores over the words << in >> the [[ input sentence ]] .",0
9523,6094,"Depending on the current state of the RNN , the encoder computes scores over the [[ words ]] << in >> the [[ input sentence ]] .",1
9524,5507,We use [[ 3 iteration of routing ]] << for >> [[ all datasets ]] since it optimizes the loss faster and converges to a lower loss at the end .,1
9525,1095,Results in << show >> that [[ SRL embedding ]] can [[ boost ]] the ESIM + ELMo model by + 0.7 % improvement .,0
9526,1095,Results in << show >> that [[ SRL embedding ]] can boost the [[ ESIM + ELMo model ]] by + 0.7 % improvement .,0
9527,1095,Results in << show >> that [[ SRL embedding ]] can boost the ESIM + ELMo model by [[ + 0.7 % improvement ]] .,0
9528,1095,Results in << show >> that SRL embedding can [[ boost ]] the [[ ESIM + ELMo model ]] by + 0.7 % improvement .,0
9529,1095,Results in << show >> that SRL embedding can [[ boost ]] the ESIM + ELMo model by [[ + 0.7 % improvement ]] .,0
9530,1095,Results in << show >> that SRL embedding can boost the [[ ESIM + ELMo model ]] by [[ + 0.7 % improvement ]] .,0
9531,1095,Results in show that [[ SRL embedding ]] can [[ boost ]] the ESIM + ELMo model << by >> + 0.7 % improvement .,0
9532,1095,Results in show that [[ SRL embedding ]] can boost the [[ ESIM + ELMo model ]] << by >> + 0.7 % improvement .,0
9533,1095,Results in show that [[ SRL embedding ]] can boost the ESIM + ELMo model << by >> [[ + 0.7 % improvement ]] .,0
9534,1095,Results in show that SRL embedding can [[ boost ]] the [[ ESIM + ELMo model ]] << by >> + 0.7 % improvement .,0
9535,1095,Results in show that SRL embedding can [[ boost ]] the ESIM + ELMo model << by >> [[ + 0.7 % improvement ]] .,0
9536,1095,Results in show that SRL embedding can boost the [[ ESIM + ELMo model ]] << by >> [[ + 0.7 % improvement ]] .,1
9537,4729,"[[ Our two models ]] << achieve >> the [[ best performance ]] when compared to these baselines as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .",1
9538,4729,"[[ Our two models ]] << achieve >> the best performance when compared to these [[ baselines ]] as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .",0
9539,4729,"Our two models << achieve >> the [[ best performance ]] when compared to these [[ baselines ]] as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .",0
9540,4729,"[[ Our two models ]] achieve the [[ best performance ]] when << compared to >> these baselines as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .",0
9541,4729,"[[ Our two models ]] achieve the best performance when << compared to >> these [[ baselines ]] as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .",0
9542,4729,"Our two models achieve the [[ best performance ]] when << compared to >> these [[ baselines ]] as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .",1
9543,431,"Next , we << observe >> a [[ substantial drop ]] when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9544,431,"Next , we << observe >> a [[ substantial drop ]] when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9545,431,"Next , we << observe >> a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9546,431,"Next , we << observe >> a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9547,431,"Next , we << observe >> a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9548,431,"Next , we << observe >> a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9549,431,"Next , we << observe >> a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9550,431,"Next , we << observe >> a substantial drop when removing [[ tokenspecific attentions ]] over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9551,431,"Next , we << observe >> a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9552,431,"Next , we << observe >> a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9553,431,"Next , we << observe >> a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9554,431,"Next , we << observe >> a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9555,431,"Next , we << observe >> a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9556,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9557,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9558,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9559,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9560,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9561,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9562,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9563,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9564,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9565,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9566,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9567,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9568,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that [[ token ]] rather than the over all query representation .",0
9569,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the [[ over all query representation ]] .",0
9570,431,"Next , we << observe >> a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the [[ over all query representation ]] .",0
9571,431,"Next , we observe a [[ substantial drop ]] << when removing >> [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",1
9572,431,"Next , we observe a [[ substantial drop ]] << when removing >> tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9573,431,"Next , we observe a [[ substantial drop ]] << when removing >> tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9574,431,"Next , we observe a [[ substantial drop ]] << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9575,431,"Next , we observe a [[ substantial drop ]] << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9576,431,"Next , we observe a [[ substantial drop ]] << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9577,431,"Next , we observe a [[ substantial drop ]] << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9578,431,"Next , we observe a substantial drop << when removing >> [[ tokenspecific attentions ]] over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9579,431,"Next , we observe a substantial drop << when removing >> [[ tokenspecific attentions ]] over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9580,431,"Next , we observe a substantial drop << when removing >> [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9581,431,"Next , we observe a substantial drop << when removing >> [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9582,431,"Next , we observe a substantial drop << when removing >> [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9583,431,"Next , we observe a substantial drop << when removing >> [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9584,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the [[ query in the GA module ]] , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9585,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9586,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9587,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9588,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9589,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9590,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9591,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9592,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9593,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9594,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9595,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9596,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that [[ token ]] rather than the over all query representation .",0
9597,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the [[ over all query representation ]] .",0
9598,431,"Next , we observe a substantial drop << when removing >> tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the [[ over all query representation ]] .",0
9599,431,"Next , we observe a [[ substantial drop ]] when removing [[ tokenspecific attentions ]] << over >> the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9600,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions << over >> the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9601,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions << over >> the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9602,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9603,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9604,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9605,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9606,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] << over >> the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",1
9607,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] << over >> the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9608,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] << over >> the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9609,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] << over >> the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9610,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] << over >> the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9611,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] << over >> the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9612,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the [[ query in the GA module ]] , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9613,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the [[ query in the GA module ]] , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9614,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the [[ query in the GA module ]] , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9615,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9616,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9617,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating [[ individual tokens ]] in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9618,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating [[ individual tokens ]] in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9619,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9620,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9621,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the [[ document ]] only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9622,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9623,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9624,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that [[ token ]] rather than the over all query representation .",0
9625,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the [[ over all query representation ]] .",0
9626,431,"Next , we observe a substantial drop when removing tokenspecific attentions << over >> the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the [[ over all query representation ]] .",0
9627,431,"Next , we observe a [[ substantial drop ]] when removing [[ tokenspecific attentions ]] over the query in the GA module , which << allow gating >> individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9628,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the [[ query in the GA module ]] , which << allow gating >> individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9629,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which << allow gating >> [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9630,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9631,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9632,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9633,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9634,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the [[ query in the GA module ]] , which << allow gating >> individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9635,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which << allow gating >> [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",1
9636,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which << allow gating >> individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9637,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which << allow gating >> individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9638,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which << allow gating >> individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9639,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which << allow gating >> individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9640,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which << allow gating >> [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the over all query representation .",0
9641,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which << allow gating >> individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9642,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which << allow gating >> individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9643,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which << allow gating >> individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9644,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which << allow gating >> individual tokens in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9645,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> [[ individual tokens ]] in the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9646,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> [[ individual tokens ]] in the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9647,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> [[ individual tokens ]] in the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9648,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> [[ individual tokens ]] in the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9649,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the [[ document ]] only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9650,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the [[ document ]] only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9651,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the [[ document ]] only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9652,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the document only by [[ parts of the query ]] relevant to that [[ token ]] rather than the over all query representation .",0
9653,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the document only by [[ parts of the query ]] relevant to that token rather than the [[ over all query representation ]] .",0
9654,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which << allow gating >> individual tokens in the document only by parts of the query relevant to that [[ token ]] rather than the [[ over all query representation ]] .",0
9655,431,"Next , we observe a [[ substantial drop ]] when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens << in >> the document only by parts of the query relevant to that token rather than the over all query representation .",0
9656,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens << in >> the document only by parts of the query relevant to that token rather than the over all query representation .",0
9657,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] << in >> the document only by parts of the query relevant to that token rather than the over all query representation .",0
9658,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9659,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9660,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9661,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9662,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the [[ query in the GA module ]] , which allow gating individual tokens << in >> the document only by parts of the query relevant to that token rather than the over all query representation .",0
9663,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating [[ individual tokens ]] << in >> the document only by parts of the query relevant to that token rather than the over all query representation .",0
9664,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens << in >> the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9665,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens << in >> the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9666,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens << in >> the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9667,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens << in >> the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9668,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating [[ individual tokens ]] << in >> the document only by parts of the query relevant to that token rather than the over all query representation .",0
9669,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens << in >> the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",0
9670,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens << in >> the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9671,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens << in >> the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9672,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens << in >> the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9673,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] << in >> the [[ document ]] only by parts of the query relevant to that token rather than the over all query representation .",1
9674,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] << in >> the document only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9675,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] << in >> the document only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9676,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] << in >> the document only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9677,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the [[ document ]] only by [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9678,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the [[ document ]] only by parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9679,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the [[ document ]] only by parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9680,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the document only by [[ parts of the query ]] relevant to that [[ token ]] rather than the over all query representation .",0
9681,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the document only by [[ parts of the query ]] relevant to that token rather than the [[ over all query representation ]] .",0
9682,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens << in >> the document only by parts of the query relevant to that [[ token ]] rather than the [[ over all query representation ]] .",0
9683,431,"Next , we observe a [[ substantial drop ]] when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9684,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9685,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9686,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9687,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only << by >> [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9688,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only << by >> parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9689,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only << by >> parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9690,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the [[ query in the GA module ]] , which allow gating individual tokens in the document only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9691,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating [[ individual tokens ]] in the document only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9692,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the [[ document ]] only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9693,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only << by >> [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9694,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only << by >> parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9695,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only << by >> parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9696,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating [[ individual tokens ]] in the document only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9697,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the [[ document ]] only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9698,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only << by >> [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9699,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only << by >> parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9700,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only << by >> parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9701,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the [[ document ]] only << by >> parts of the query relevant to that token rather than the over all query representation .",0
9702,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only << by >> [[ parts of the query ]] relevant to that token rather than the over all query representation .",1
9703,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only << by >> parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9704,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only << by >> parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9705,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only << by >> [[ parts of the query ]] relevant to that token rather than the over all query representation .",0
9706,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only << by >> parts of the query relevant to that [[ token ]] rather than the over all query representation .",0
9707,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only << by >> parts of the query relevant to that token rather than the [[ over all query representation ]] .",0
9708,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only << by >> [[ parts of the query ]] relevant to that [[ token ]] rather than the over all query representation .",0
9709,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only << by >> [[ parts of the query ]] relevant to that token rather than the [[ over all query representation ]] .",0
9710,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only << by >> parts of the query relevant to that [[ token ]] rather than the [[ over all query representation ]] .",0
9711,431,"Next , we observe a [[ substantial drop ]] when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9712,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9713,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9714,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9715,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] << relevant to >> that token rather than the over all query representation .",0
9716,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query << relevant to >> that [[ token ]] rather than the over all query representation .",0
9717,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query << relevant to >> that token rather than the [[ over all query representation ]] .",0
9718,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9719,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9720,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9721,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] << relevant to >> that token rather than the over all query representation .",0
9722,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query << relevant to >> that [[ token ]] rather than the over all query representation .",0
9723,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query << relevant to >> that token rather than the [[ over all query representation ]] .",0
9724,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating [[ individual tokens ]] in the document only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9725,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the [[ document ]] only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9726,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by [[ parts of the query ]] << relevant to >> that token rather than the over all query representation .",0
9727,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query << relevant to >> that [[ token ]] rather than the over all query representation .",0
9728,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query << relevant to >> that token rather than the [[ over all query representation ]] .",0
9729,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the [[ document ]] only by parts of the query << relevant to >> that token rather than the over all query representation .",0
9730,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by [[ parts of the query ]] << relevant to >> that token rather than the over all query representation .",0
9731,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query << relevant to >> that [[ token ]] rather than the over all query representation .",0
9732,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query << relevant to >> that token rather than the [[ over all query representation ]] .",0
9733,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by [[ parts of the query ]] << relevant to >> that token rather than the over all query representation .",0
9734,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query << relevant to >> that [[ token ]] rather than the over all query representation .",0
9735,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query << relevant to >> that token rather than the [[ over all query representation ]] .",0
9736,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] << relevant to >> that [[ token ]] rather than the over all query representation .",1
9737,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] << relevant to >> that token rather than the [[ over all query representation ]] .",0
9738,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query << relevant to >> that [[ token ]] rather than the [[ over all query representation ]] .",0
9739,431,"Next , we observe a [[ substantial drop ]] when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9740,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9741,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9742,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9743,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token << rather than >> the over all query representation .",0
9744,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] << rather than >> the over all query representation .",0
9745,431,"Next , we observe a [[ substantial drop ]] when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token << rather than >> the [[ over all query representation ]] .",0
9746,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9747,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9748,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9749,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token << rather than >> the over all query representation .",0
9750,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] << rather than >> the over all query representation .",0
9751,431,"Next , we observe a substantial drop when removing [[ tokenspecific attentions ]] over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token << rather than >> the [[ over all query representation ]] .",0
9752,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9753,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9754,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token << rather than >> the over all query representation .",0
9755,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] << rather than >> the over all query representation .",0
9756,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the [[ query in the GA module ]] , which allow gating individual tokens in the document only by parts of the query relevant to that token << rather than >> the [[ over all query representation ]] .",0
9757,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the [[ document ]] only by parts of the query relevant to that token << rather than >> the over all query representation .",0
9758,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by [[ parts of the query ]] relevant to that token << rather than >> the over all query representation .",0
9759,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that [[ token ]] << rather than >> the over all query representation .",0
9760,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating [[ individual tokens ]] in the document only by parts of the query relevant to that token << rather than >> the [[ over all query representation ]] .",0
9761,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by [[ parts of the query ]] relevant to that token << rather than >> the over all query representation .",0
9762,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that [[ token ]] << rather than >> the over all query representation .",0
9763,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the [[ document ]] only by parts of the query relevant to that token << rather than >> the [[ over all query representation ]] .",0
9764,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that [[ token ]] << rather than >> the over all query representation .",0
9765,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by [[ parts of the query ]] relevant to that token << rather than >> the [[ over all query representation ]] .",0
9766,431,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that [[ token ]] << rather than >> the [[ over all query representation ]] .",1
9767,5384,"We << maintain >> a [[ fast and memory efficient mapping ]] of the [[ n-grams ]] by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9768,5384,"We << maintain >> a [[ fast and memory efficient mapping ]] of the n-grams by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9769,5384,"We << maintain >> a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9770,5384,"We << maintain >> a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9771,5384,"We << maintain >> a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9772,5384,"We << maintain >> a fast and memory efficient mapping of the [[ n-grams ]] by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9773,5384,"We << maintain >> a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9774,5384,"We << maintain >> a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9775,5384,"We << maintain >> a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9776,5384,"We << maintain >> a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9777,5384,"We << maintain >> a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9778,5384,"We << maintain >> a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9779,5384,"We << maintain >> a fast and memory efficient mapping of the n-grams by using the hashing trick with the [[ same hashing function as in ]] and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9780,5384,"We << maintain >> a fast and memory efficient mapping of the n-grams by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9781,5384,"We << maintain >> a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used [[ bigrams ]] , and 100M otherwise .",0
9782,5384,"We maintain a [[ fast and memory efficient mapping ]] << of >> the [[ n-grams ]] by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",1
9783,5384,"We maintain a [[ fast and memory efficient mapping ]] << of >> the n-grams by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9784,5384,"We maintain a [[ fast and memory efficient mapping ]] << of >> the n-grams by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9785,5384,"We maintain a [[ fast and memory efficient mapping ]] << of >> the n-grams by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9786,5384,"We maintain a [[ fast and memory efficient mapping ]] << of >> the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9787,5384,"We maintain a fast and memory efficient mapping << of >> the [[ n-grams ]] by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9788,5384,"We maintain a fast and memory efficient mapping << of >> the [[ n-grams ]] by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9789,5384,"We maintain a fast and memory efficient mapping << of >> the [[ n-grams ]] by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9790,5384,"We maintain a fast and memory efficient mapping << of >> the [[ n-grams ]] by using the hashing trick with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9791,5384,"We maintain a fast and memory efficient mapping << of >> the n-grams by using the [[ hashing trick ]] with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9792,5384,"We maintain a fast and memory efficient mapping << of >> the n-grams by using the [[ hashing trick ]] with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9793,5384,"We maintain a fast and memory efficient mapping << of >> the n-grams by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9794,5384,"We maintain a fast and memory efficient mapping << of >> the n-grams by using the hashing trick with the [[ same hashing function as in ]] and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9795,5384,"We maintain a fast and memory efficient mapping << of >> the n-grams by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9796,5384,"We maintain a fast and memory efficient mapping << of >> the n-grams by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used [[ bigrams ]] , and 100M otherwise .",0
9797,5384,"We maintain a [[ fast and memory efficient mapping ]] of the [[ n-grams ]] << by using >> the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9798,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams << by using >> the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9799,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams << by using >> the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9800,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams << by using >> the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9801,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams << by using >> the hashing trick with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9802,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] << by using >> the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",1
9803,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] << by using >> the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9804,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] << by using >> the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9805,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] << by using >> the hashing trick with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9806,5384,"We maintain a fast and memory efficient mapping of the n-grams << by using >> the [[ hashing trick ]] with the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9807,5384,"We maintain a fast and memory efficient mapping of the n-grams << by using >> the [[ hashing trick ]] with the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9808,5384,"We maintain a fast and memory efficient mapping of the n-grams << by using >> the [[ hashing trick ]] with the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9809,5384,"We maintain a fast and memory efficient mapping of the n-grams << by using >> the hashing trick with the [[ same hashing function as in ]] and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9810,5384,"We maintain a fast and memory efficient mapping of the n-grams << by using >> the hashing trick with the [[ same hashing function as in ]] and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9811,5384,"We maintain a fast and memory efficient mapping of the n-grams << by using >> the hashing trick with the same hashing function as in and [[ 10M bins ]] if we only used [[ bigrams ]] , and 100M otherwise .",0
9812,5384,"We maintain a [[ fast and memory efficient mapping ]] of the [[ n-grams ]] by using the hashing trick << with >> the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9813,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the [[ hashing trick ]] << with >> the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9814,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick << with >> the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9815,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick << with >> the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9816,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick << with >> the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9817,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the [[ hashing trick ]] << with >> the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
9818,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick << with >> the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",0
9819,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick << with >> the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9820,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick << with >> the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9821,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] << with >> the [[ same hashing function as in ]] and 10M bins if we only used bigrams , and 100M otherwise .",1
9822,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] << with >> the same hashing function as in and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",1
9823,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] << with >> the same hashing function as in and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9824,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick << with >> the [[ same hashing function as in ]] and [[ 10M bins ]] if we only used bigrams , and 100M otherwise .",0
9825,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick << with >> the [[ same hashing function as in ]] and 10M bins if we only used [[ bigrams ]] , and 100M otherwise .",0
9826,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick << with >> the same hashing function as in and [[ 10M bins ]] if we only used [[ bigrams ]] , and 100M otherwise .",0
9827,5384,"We maintain a [[ fast and memory efficient mapping ]] of the [[ n-grams ]] by using the hashing trick with the same hashing function as in and 10M bins if we << only used >> bigrams , and 100M otherwise .",0
9828,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we << only used >> bigrams , and 100M otherwise .",0
9829,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we << only used >> bigrams , and 100M otherwise .",0
9830,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we << only used >> bigrams , and 100M otherwise .",0
9831,5384,"We maintain a [[ fast and memory efficient mapping ]] of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we << only used >> [[ bigrams ]] , and 100M otherwise .",0
9832,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we << only used >> bigrams , and 100M otherwise .",0
9833,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we << only used >> bigrams , and 100M otherwise .",0
9834,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we << only used >> bigrams , and 100M otherwise .",0
9835,5384,"We maintain a fast and memory efficient mapping of the [[ n-grams ]] by using the hashing trick with the same hashing function as in and 10M bins if we << only used >> [[ bigrams ]] , and 100M otherwise .",0
9836,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] with the [[ same hashing function as in ]] and 10M bins if we << only used >> bigrams , and 100M otherwise .",0
9837,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] with the same hashing function as in and [[ 10M bins ]] if we << only used >> bigrams , and 100M otherwise .",0
9838,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the [[ hashing trick ]] with the same hashing function as in and 10M bins if we << only used >> [[ bigrams ]] , and 100M otherwise .",0
9839,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the [[ same hashing function as in ]] and [[ 10M bins ]] if we << only used >> bigrams , and 100M otherwise .",0
9840,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the [[ same hashing function as in ]] and 10M bins if we << only used >> [[ bigrams ]] , and 100M otherwise .",0
9841,5384,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and [[ 10M bins ]] if we << only used >> [[ bigrams ]] , and 100M otherwise .",1
9842,4412,"Finally , we << feed >> the [[ achieved sentence representation ]] into a [[ softmax layer ]] to predict the sentiment polarity .",0
9843,4412,"Finally , we << feed >> the [[ achieved sentence representation ]] into a softmax layer to predict the [[ sentiment polarity ]] .",0
9844,4412,"Finally , we << feed >> the achieved sentence representation into a [[ softmax layer ]] to predict the [[ sentiment polarity ]] .",0
9845,4412,"Finally , we feed the [[ achieved sentence representation ]] << into >> a [[ softmax layer ]] to predict the sentiment polarity .",1
9846,4412,"Finally , we feed the [[ achieved sentence representation ]] << into >> a softmax layer to predict the [[ sentiment polarity ]] .",0
9847,4412,"Finally , we feed the achieved sentence representation << into >> a [[ softmax layer ]] to predict the [[ sentiment polarity ]] .",0
9848,4412,"Finally , we feed the [[ achieved sentence representation ]] into a [[ softmax layer ]] << to predict >> the sentiment polarity .",0
9849,4412,"Finally , we feed the [[ achieved sentence representation ]] into a softmax layer << to predict >> the [[ sentiment polarity ]] .",0
9850,4412,"Finally , we feed the achieved sentence representation into a [[ softmax layer ]] << to predict >> the [[ sentiment polarity ]] .",1
9851,6016,"Since the generated summaries are often shorter than the actual ones , we << introduce >> an [[ additional length penalty argument ]] "" [[ alpha 1 ]] "" to encourage longer generation , like .",0
9852,6016,"Since the generated summaries are often shorter than the actual ones , we << introduce >> an [[ additional length penalty argument ]] "" alpha 1 "" to encourage [[ longer generation ]] , like .",0
9853,6016,"Since the generated summaries are often shorter than the actual ones , we << introduce >> an additional length penalty argument "" [[ alpha 1 ]] "" to encourage [[ longer generation ]] , like .",0
9854,6016,"Since the generated summaries are often shorter than the actual ones , we introduce an [[ additional length penalty argument ]] "" [[ alpha 1 ]] "" << to encourage >> longer generation , like .",0
9855,6016,"Since the generated summaries are often shorter than the actual ones , we introduce an [[ additional length penalty argument ]] "" alpha 1 "" << to encourage >> [[ longer generation ]] , like .",0
9856,6016,"Since the generated summaries are often shorter than the actual ones , we introduce an additional length penalty argument "" [[ alpha 1 ]] "" << to encourage >> [[ longer generation ]] , like .",1
9857,2561,"We << adopt >> [[ two state - of - the - art methods ]] in [[ sequence labeling ]] , denoted as char - LSTM and char - CNN .",0
9858,2561,"We << adopt >> [[ two state - of - the - art methods ]] in sequence labeling , denoted as [[ char - LSTM ]] and char - CNN .",0
9859,2561,"We << adopt >> [[ two state - of - the - art methods ]] in sequence labeling , denoted as char - LSTM and [[ char - CNN ]] .",0
9860,2561,"We << adopt >> two state - of - the - art methods in [[ sequence labeling ]] , denoted as [[ char - LSTM ]] and char - CNN .",0
9861,2561,"We << adopt >> two state - of - the - art methods in [[ sequence labeling ]] , denoted as char - LSTM and [[ char - CNN ]] .",0
9862,2561,"We << adopt >> two state - of - the - art methods in sequence labeling , denoted as [[ char - LSTM ]] and [[ char - CNN ]] .",0
9863,2561,"We adopt [[ two state - of - the - art methods ]] << in >> [[ sequence labeling ]] , denoted as char - LSTM and char - CNN .",1
9864,2561,"We adopt [[ two state - of - the - art methods ]] << in >> sequence labeling , denoted as [[ char - LSTM ]] and char - CNN .",0
9865,2561,"We adopt [[ two state - of - the - art methods ]] << in >> sequence labeling , denoted as char - LSTM and [[ char - CNN ]] .",0
9866,2561,"We adopt two state - of - the - art methods << in >> [[ sequence labeling ]] , denoted as [[ char - LSTM ]] and char - CNN .",0
9867,2561,"We adopt two state - of - the - art methods << in >> [[ sequence labeling ]] , denoted as char - LSTM and [[ char - CNN ]] .",0
9868,2561,"We adopt two state - of - the - art methods << in >> sequence labeling , denoted as [[ char - LSTM ]] and [[ char - CNN ]] .",0
9869,2561,"We adopt [[ two state - of - the - art methods ]] in [[ sequence labeling ]] , << denoted as >> char - LSTM and char - CNN .",0
9870,2561,"We adopt [[ two state - of - the - art methods ]] in sequence labeling , << denoted as >> [[ char - LSTM ]] and char - CNN .",1
9871,2561,"We adopt [[ two state - of - the - art methods ]] in sequence labeling , << denoted as >> char - LSTM and [[ char - CNN ]] .",1
9872,2561,"We adopt two state - of - the - art methods in [[ sequence labeling ]] , << denoted as >> [[ char - LSTM ]] and char - CNN .",0
9873,2561,"We adopt two state - of - the - art methods in [[ sequence labeling ]] , << denoted as >> char - LSTM and [[ char - CNN ]] .",0
9874,2561,"We adopt two state - of - the - art methods in sequence labeling , << denoted as >> [[ char - LSTM ]] and [[ char - CNN ]] .",0
9875,2877,"Similarly to the single model case , our [[ ensembles ]] << achieve >> [[ state - of - the - art test performance ]] of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .",0
9876,2877,"Similarly to the single model case , our [[ ensembles ]] << achieve >> state - of - the - art test performance of [[ 75.2 and 76.1 on validation and test respectively ]] , outperforming previously published results .",0
9877,2877,"Similarly to the single model case , our [[ ensembles ]] << achieve >> state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming [[ previously published results ]] .",0
9878,2877,"Similarly to the single model case , our ensembles << achieve >> [[ state - of - the - art test performance ]] of [[ 75.2 and 76.1 on validation and test respectively ]] , outperforming previously published results .",0
9879,2877,"Similarly to the single model case , our ensembles << achieve >> [[ state - of - the - art test performance ]] of 75.2 and 76.1 on validation and test respectively , outperforming [[ previously published results ]] .",0
9880,2877,"Similarly to the single model case , our ensembles << achieve >> state - of - the - art test performance of [[ 75.2 and 76.1 on validation and test respectively ]] , outperforming [[ previously published results ]] .",0
9881,2877,"Similarly to the single model case , our [[ ensembles ]] achieve [[ state - of - the - art test performance ]] << of >> 75.2 and 76.1 on validation and test respectively , outperforming previously published results .",0
9882,2877,"Similarly to the single model case , our [[ ensembles ]] achieve state - of - the - art test performance << of >> [[ 75.2 and 76.1 on validation and test respectively ]] , outperforming previously published results .",1
9883,2877,"Similarly to the single model case , our [[ ensembles ]] achieve state - of - the - art test performance << of >> 75.2 and 76.1 on validation and test respectively , outperforming [[ previously published results ]] .",0
9884,2877,"Similarly to the single model case , our ensembles achieve [[ state - of - the - art test performance ]] << of >> [[ 75.2 and 76.1 on validation and test respectively ]] , outperforming previously published results .",0
9885,2877,"Similarly to the single model case , our ensembles achieve [[ state - of - the - art test performance ]] << of >> 75.2 and 76.1 on validation and test respectively , outperforming [[ previously published results ]] .",0
9886,2877,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance << of >> [[ 75.2 and 76.1 on validation and test respectively ]] , outperforming [[ previously published results ]] .",0
9887,2877,"Similarly to the single model case , our [[ ensembles ]] achieve [[ state - of - the - art test performance ]] of 75.2 and 76.1 on validation and test respectively , << outperforming >> previously published results .",0
9888,2877,"Similarly to the single model case , our [[ ensembles ]] achieve state - of - the - art test performance of [[ 75.2 and 76.1 on validation and test respectively ]] , << outperforming >> previously published results .",0
9889,2877,"Similarly to the single model case , our [[ ensembles ]] achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , << outperforming >> [[ previously published results ]] .",1
9890,2877,"Similarly to the single model case , our ensembles achieve [[ state - of - the - art test performance ]] of [[ 75.2 and 76.1 on validation and test respectively ]] , << outperforming >> previously published results .",0
9891,2877,"Similarly to the single model case , our ensembles achieve [[ state - of - the - art test performance ]] of 75.2 and 76.1 on validation and test respectively , << outperforming >> [[ previously published results ]] .",0
9892,2877,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of [[ 75.2 and 76.1 on validation and test respectively ]] , << outperforming >> [[ previously published results ]] .",0
9893,4415,"[[ All out - of - vocabulary words ]] are << initialized as >> [[ zero vectors ]] , and all biases are set to zero .",1
9894,4415,"[[ All out - of - vocabulary words ]] are << initialized as >> zero vectors , and [[ all biases ]] are set to zero .",0
9895,4415,"[[ All out - of - vocabulary words ]] are << initialized as >> zero vectors , and all biases are set to [[ zero ]] .",0
9896,4415,"All out - of - vocabulary words are << initialized as >> [[ zero vectors ]] , and [[ all biases ]] are set to zero .",0
9897,4415,"All out - of - vocabulary words are << initialized as >> [[ zero vectors ]] , and all biases are set to [[ zero ]] .",0
9898,4415,"All out - of - vocabulary words are << initialized as >> zero vectors , and [[ all biases ]] are set to [[ zero ]] .",0
9899,4415,"[[ All out - of - vocabulary words ]] are initialized as [[ zero vectors ]] , and all biases are << set to >> zero .",0
9900,4415,"[[ All out - of - vocabulary words ]] are initialized as zero vectors , and [[ all biases ]] are << set to >> zero .",0
9901,4415,"[[ All out - of - vocabulary words ]] are initialized as zero vectors , and all biases are << set to >> [[ zero ]] .",0
9902,4415,"All out - of - vocabulary words are initialized as [[ zero vectors ]] , and [[ all biases ]] are << set to >> zero .",0
9903,4415,"All out - of - vocabulary words are initialized as [[ zero vectors ]] , and all biases are << set to >> [[ zero ]] .",0
9904,4415,"All out - of - vocabulary words are initialized as zero vectors , and [[ all biases ]] are << set to >> [[ zero ]] .",1
9905,1031,"We << observe >> [[ gains ]] of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .",0
9906,1031,"We << observe >> [[ gains ]] of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",0
9907,1031,"We << observe >> [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9908,1031,"We << observe >> [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9909,1031,"We << observe >> [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9910,1031,"We << observe >> [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9911,1031,"We << observe >> [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9912,1031,"We << observe >> gains of [[ 1.1 - 2.0 % ]] on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",0
9913,1031,"We << observe >> gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9914,1031,"We << observe >> gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9915,1031,"We << observe >> gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9916,1031,"We << observe >> gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9917,1031,"We << observe >> gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9918,1031,"We << observe >> gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9919,1031,"We << observe >> gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9920,1031,"We << observe >> gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9921,1031,"We << observe >> gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9922,1031,"We << observe >> gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9923,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9924,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9925,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9926,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9927,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , [[ SUBJ ]] & MPQA ) over Infersent .",0
9928,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & [[ MPQA ]] ) over Infersent .",0
9929,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over [[ Infersent ]] .",0
9930,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & [[ MPQA ]] ) over Infersent .",0
9931,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over [[ Infersent ]] .",0
9932,1031,"We << observe >> gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over [[ Infersent ]] .",0
9933,1031,"We observe [[ gains ]] << of >> [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .",1
9934,1031,"We observe [[ gains ]] << of >> 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",0
9935,1031,"We observe [[ gains ]] << of >> 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9936,1031,"We observe [[ gains ]] << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9937,1031,"We observe [[ gains ]] << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9938,1031,"We observe [[ gains ]] << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9939,1031,"We observe [[ gains ]] << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9940,1031,"We observe gains << of >> [[ 1.1 - 2.0 % ]] on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",0
9941,1031,"We observe gains << of >> [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9942,1031,"We observe gains << of >> [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9943,1031,"We observe gains << of >> [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9944,1031,"We observe gains << of >> [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9945,1031,"We observe gains << of >> [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9946,1031,"We observe gains << of >> 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9947,1031,"We observe gains << of >> 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9948,1031,"We observe gains << of >> 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9949,1031,"We observe gains << of >> 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9950,1031,"We observe gains << of >> 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9951,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9952,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9953,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9954,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9955,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , [[ SUBJ ]] & MPQA ) over Infersent .",0
9956,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & [[ MPQA ]] ) over Infersent .",0
9957,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over [[ Infersent ]] .",0
9958,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & [[ MPQA ]] ) over Infersent .",0
9959,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over [[ Infersent ]] .",0
9960,1031,"We observe gains << of >> 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over [[ Infersent ]] .",0
9961,1031,"We observe [[ gains ]] of [[ 1.1 - 2.0 % ]] << on >> the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .",0
9962,1031,"We observe [[ gains ]] of 1.1 - 2.0 % << on >> the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",0
9963,1031,"We observe [[ gains ]] of 1.1 - 2.0 % << on >> the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9964,1031,"We observe [[ gains ]] of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9965,1031,"We observe [[ gains ]] of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9966,1031,"We observe [[ gains ]] of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9967,1031,"We observe [[ gains ]] of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9968,1031,"We observe gains of [[ 1.1 - 2.0 % ]] << on >> the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over Infersent .",1
9969,1031,"We observe gains of [[ 1.1 - 2.0 % ]] << on >> the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9970,1031,"We observe gains of [[ 1.1 - 2.0 % ]] << on >> the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9971,1031,"We observe gains of [[ 1.1 - 2.0 % ]] << on >> the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9972,1031,"We observe gains of [[ 1.1 - 2.0 % ]] << on >> the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9973,1031,"We observe gains of [[ 1.1 - 2.0 % ]] << on >> the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9974,1031,"We observe gains of 1.1 - 2.0 % << on >> the [[ sentiment classification tasks ]] ( [[ MR ]] , CR , SUBJ & MPQA ) over Infersent .",0
9975,1031,"We observe gains of 1.1 - 2.0 % << on >> the [[ sentiment classification tasks ]] ( MR , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9976,1031,"We observe gains of 1.1 - 2.0 % << on >> the [[ sentiment classification tasks ]] ( MR , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9977,1031,"We observe gains of 1.1 - 2.0 % << on >> the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9978,1031,"We observe gains of 1.1 - 2.0 % << on >> the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9979,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( [[ MR ]] , [[ CR ]] , SUBJ & MPQA ) over Infersent .",0
9980,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( [[ MR ]] , CR , [[ SUBJ ]] & MPQA ) over Infersent .",0
9981,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & [[ MPQA ]] ) over Infersent .",0
9982,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) over [[ Infersent ]] .",0
9983,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , [[ CR ]] , [[ SUBJ ]] & MPQA ) over Infersent .",0
9984,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & [[ MPQA ]] ) over Infersent .",0
9985,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) over [[ Infersent ]] .",0
9986,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & [[ MPQA ]] ) over Infersent .",0
9987,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) over [[ Infersent ]] .",0
9988,1031,"We observe gains of 1.1 - 2.0 % << on >> the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) over [[ Infersent ]] .",0
9989,1031,"We observe [[ gains ]] of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) << over >> Infersent .",0
9990,1031,"We observe [[ gains ]] of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) << over >> Infersent .",0
9991,1031,"We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) << over >> Infersent .",0
9992,1031,"We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) << over >> Infersent .",0
9993,1031,"We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) << over >> Infersent .",0
9994,1031,"We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) << over >> Infersent .",0
9995,1031,"We observe [[ gains ]] of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) << over >> [[ Infersent ]] .",0
9996,1031,"We observe gains of [[ 1.1 - 2.0 % ]] on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) << over >> Infersent .",0
9997,1031,"We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) << over >> Infersent .",0
9998,1031,"We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) << over >> Infersent .",0
9999,1031,"We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) << over >> Infersent .",0
10000,1031,"We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) << over >> Infersent .",0
10001,1031,"We observe gains of [[ 1.1 - 2.0 % ]] on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) << over >> [[ Infersent ]] .",1
10002,1031,"We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( [[ MR ]] , CR , SUBJ & MPQA ) << over >> Infersent .",0
10003,1031,"We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , [[ CR ]] , SUBJ & MPQA ) << over >> Infersent .",0
10004,1031,"We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , [[ SUBJ ]] & MPQA ) << over >> Infersent .",0
10005,1031,"We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & [[ MPQA ]] ) << over >> Infersent .",0
10006,1031,"We observe gains of 1.1 - 2.0 % on the [[ sentiment classification tasks ]] ( MR , CR , SUBJ & MPQA ) << over >> [[ Infersent ]] .",0
10007,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , [[ CR ]] , SUBJ & MPQA ) << over >> Infersent .",0
10008,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , [[ SUBJ ]] & MPQA ) << over >> Infersent .",0
10009,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & [[ MPQA ]] ) << over >> Infersent .",0
10010,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( [[ MR ]] , CR , SUBJ & MPQA ) << over >> [[ Infersent ]] .",0
10011,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , [[ SUBJ ]] & MPQA ) << over >> Infersent .",0
10012,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & [[ MPQA ]] ) << over >> Infersent .",0
10013,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , [[ CR ]] , SUBJ & MPQA ) << over >> [[ Infersent ]] .",0
10014,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & [[ MPQA ]] ) << over >> Infersent .",0
10015,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , [[ SUBJ ]] & MPQA ) << over >> [[ Infersent ]] .",0
10016,1031,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & [[ MPQA ]] ) << over >> [[ Infersent ]] .",0
10017,1110,The [[ Glo Ve 300 dimensional word vectors ]] released by are << used for >> [[ word embeddings ]] .,1
10018,1888,"In this paper , we << propose >> a [[ Discourse Marker Augmented Network ]] for [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10019,1888,"In this paper , we << propose >> a [[ Discourse Marker Augmented Network ]] for natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10020,1888,"In this paper , we << propose >> a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10021,1888,"In this paper , we << propose >> a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10022,1888,"In this paper , we << propose >> a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10023,1888,"In this paper , we << propose >> a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10024,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for [[ natural language inference ]] , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10025,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10026,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10027,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10028,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10029,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10030,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10031,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10032,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10033,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10034,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10035,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10036,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10037,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10038,1888,"In this paper , we << propose >> a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an [[ integrated NLI model ]] .",0
10039,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] << for >> [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",1
10040,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] << for >> natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10041,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] << for >> natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10042,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] << for >> natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10043,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] << for >> natural language inference , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10044,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] << for >> natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10045,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> [[ natural language inference ]] , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10046,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> [[ natural language inference ]] , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10047,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> [[ natural language inference ]] , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10048,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10049,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10050,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we [[ transfer ]] the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10051,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we [[ transfer ]] the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10052,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10053,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10054,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we transfer the [[ knowledge ]] from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10055,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10056,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10057,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10058,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10059,1888,"In this paper , we propose a Discourse Marker Augmented Network << for >> natural language inference , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an [[ integrated NLI model ]] .",0
10060,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for [[ natural language inference ]] , << where >> we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10061,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , << where >> we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",1
10062,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , << where >> we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10063,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , << where >> we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10064,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , << where >> we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10065,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , << where >> we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10066,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , << where >> we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10067,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , << where >> we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10068,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , << where >> we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10069,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , << where >> we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10070,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , << where >> we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10071,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we [[ transfer ]] the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10072,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we [[ transfer ]] the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10073,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we [[ transfer ]] the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10074,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10075,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we transfer the [[ knowledge ]] from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10076,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we transfer the [[ knowledge ]] from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10077,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10078,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we transfer the knowledge from the [[ existing supervised task ]] : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10079,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10080,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , << where >> we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an [[ integrated NLI model ]] .",0
10081,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for [[ natural language inference ]] , where we transfer the knowledge << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10082,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we [[ transfer ]] the knowledge << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10083,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the [[ knowledge ]] << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10084,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge << from >> the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10085,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge << from >> the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10086,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10087,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we [[ transfer ]] the knowledge << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10088,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the [[ knowledge ]] << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10089,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge << from >> the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10090,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge << from >> the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10091,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10092,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the [[ knowledge ]] << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10093,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge << from >> the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",0
10094,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge << from >> the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10095,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10096,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] << from >> the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",1
10097,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] << from >> the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10098,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] << from >> the existing supervised task : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10099,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge << from >> the [[ existing supervised task ]] : [[ Discourse Marker Prediction ( DMP ) ]] , to an integrated NLI model .",0
10100,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge << from >> the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , to an [[ integrated NLI model ]] .",0
10101,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge << from >> the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , to an [[ integrated NLI model ]] .",0
10102,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10103,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10104,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10105,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10106,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , << to >> an integrated NLI model .",0
10107,1888,"In this paper , we propose a [[ Discourse Marker Augmented Network ]] for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an [[ integrated NLI model ]] .",0
10108,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10109,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10110,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10111,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , << to >> an integrated NLI model .",0
10112,1888,"In this paper , we propose a Discourse Marker Augmented Network for [[ natural language inference ]] , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an [[ integrated NLI model ]] .",0
10113,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10114,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10115,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , << to >> an integrated NLI model .",0
10116,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we [[ transfer ]] the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an [[ integrated NLI model ]] .",0
10117,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , << to >> an integrated NLI model .",0
10118,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , << to >> an integrated NLI model .",0
10119,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the [[ knowledge ]] from the existing supervised task : Discourse Marker Prediction ( DMP ) , << to >> an [[ integrated NLI model ]] .",0
10120,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : [[ Discourse Marker Prediction ( DMP ) ]] , << to >> an integrated NLI model .",0
10121,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the [[ existing supervised task ]] : Discourse Marker Prediction ( DMP ) , << to >> an [[ integrated NLI model ]] .",1
10122,1888,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : [[ Discourse Marker Prediction ( DMP ) ]] , << to >> an [[ integrated NLI model ]] .",0
10123,4764,"<< As in >> [[ conventional CNNs ]] , the computation of the [[ convolution operation ]] with RNFs can be easily parallelized .",0
10124,4764,"<< As in >> [[ conventional CNNs ]] , the computation of the convolution operation with [[ RNFs ]] can be easily parallelized .",0
10125,4764,"<< As in >> [[ conventional CNNs ]] , the computation of the convolution operation with RNFs can be easily [[ parallelized ]] .",0
10126,4764,"<< As in >> conventional CNNs , the computation of the [[ convolution operation ]] with [[ RNFs ]] can be easily parallelized .",0
10127,4764,"<< As in >> conventional CNNs , the computation of the [[ convolution operation ]] with RNFs can be easily [[ parallelized ]] .",0
10128,4764,"<< As in >> conventional CNNs , the computation of the convolution operation with [[ RNFs ]] can be easily [[ parallelized ]] .",0
10129,4764,"As in [[ conventional CNNs ]] , the << computation of >> the [[ convolution operation ]] with RNFs can be easily parallelized .",0
10130,4764,"As in [[ conventional CNNs ]] , the << computation of >> the convolution operation with [[ RNFs ]] can be easily parallelized .",0
10131,4764,"As in [[ conventional CNNs ]] , the << computation of >> the convolution operation with RNFs can be easily [[ parallelized ]] .",0
10132,4764,"As in conventional CNNs , the << computation of >> the [[ convolution operation ]] with [[ RNFs ]] can be easily parallelized .",0
10133,4764,"As in conventional CNNs , the << computation of >> the [[ convolution operation ]] with RNFs can be easily [[ parallelized ]] .",0
10134,4764,"As in conventional CNNs , the << computation of >> the convolution operation with [[ RNFs ]] can be easily [[ parallelized ]] .",0
10135,4764,"As in [[ conventional CNNs ]] , the computation of the [[ convolution operation ]] << with >> RNFs can be easily parallelized .",0
10136,4764,"As in [[ conventional CNNs ]] , the computation of the convolution operation << with >> [[ RNFs ]] can be easily parallelized .",0
10137,4764,"As in [[ conventional CNNs ]] , the computation of the convolution operation << with >> RNFs can be easily [[ parallelized ]] .",0
10138,4764,"As in conventional CNNs , the computation of the [[ convolution operation ]] << with >> [[ RNFs ]] can be easily parallelized .",1
10139,4764,"As in conventional CNNs , the computation of the [[ convolution operation ]] << with >> RNFs can be easily [[ parallelized ]] .",0
10140,4764,"As in conventional CNNs , the computation of the convolution operation << with >> [[ RNFs ]] can be easily [[ parallelized ]] .",0
10141,4764,"As in [[ conventional CNNs ]] , the computation of the [[ convolution operation ]] with RNFs << can be >> easily parallelized .",0
10142,4764,"As in [[ conventional CNNs ]] , the computation of the convolution operation with [[ RNFs ]] << can be >> easily parallelized .",0
10143,4764,"As in [[ conventional CNNs ]] , the computation of the convolution operation with RNFs << can be >> easily [[ parallelized ]] .",0
10144,4764,"As in conventional CNNs , the computation of the [[ convolution operation ]] with [[ RNFs ]] << can be >> easily parallelized .",0
10145,4764,"As in conventional CNNs , the computation of the [[ convolution operation ]] with RNFs << can be >> easily [[ parallelized ]] .",0
10146,4764,"As in conventional CNNs , the computation of the convolution operation with [[ RNFs ]] << can be >> easily [[ parallelized ]] .",1
10147,3562,This ablation study << demonstrates >> that both the [[ special separate tokens and the hidden entity vectors ]] make [[ important contributions ]] to our approach .,0
10148,3562,This ablation study demonstrates that both the [[ special separate tokens and the hidden entity vectors ]] << make >> [[ important contributions ]] to our approach .,1
10149,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a [[ sequential conditional random layer ]] << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",1
10150,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer << above it >> ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10151,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10152,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10153,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10154,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10155,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] << above it >> ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10156,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10157,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10158,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10159,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10160,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10161,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10162,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10163,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10164,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10165,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10166,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10167,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10168,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10169,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer << above it >> ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( [[ S - LSTM ]] ; 3 ) .",0
10170,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10171,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10172,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10173,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10174,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10175,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10176,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10177,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10178,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10179,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10180,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10181,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that << constructs and labels >> [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10182,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10183,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10184,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10185,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> [[ chunks of input sentences ]] using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10186,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10187,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10188,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10189,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10190,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that << constructs and labels >> chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( [[ S - LSTM ]] ; 3 ) .",0
10191,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10192,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10193,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10194,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10195,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10196,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10197,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10198,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10199,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10200,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10201,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10202,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10203,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",0
10204,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10205,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10206,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] << using >> an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( S - LSTM ; 3 ) .",1
10207,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] << using >> an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10208,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] << using >> an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10209,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an [[ algorithm inspired by transition - based parsing with states ]] represented by [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10210,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an [[ algorithm inspired by transition - based parsing with states ]] represented by stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10211,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences << using >> an algorithm inspired by transition - based parsing with states represented by [[ stack LSTMs ]] ( [[ S - LSTM ]] ; 3 ) .",0
10212,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10213,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10214,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10215,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10216,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10217,252,"We compare two models here , ( i ) a [[ bidirectional LSTM ]] with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10218,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10219,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10220,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10221,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10222,252,"We compare two models here , ( i ) a bidirectional LSTM with a [[ sequential conditional random layer ]] above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10223,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10224,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10225,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10226,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( [[ LSTM - CRF ]] ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10227,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an [[ algorithm inspired by transition - based parsing with states ]] << represented by >> stack LSTMs ( S - LSTM ; 3 ) .",0
10228,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states << represented by >> [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",0
10229,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels [[ chunks of input sentences ]] using an algorithm inspired by transition - based parsing with states << represented by >> stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10230,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] << represented by >> [[ stack LSTMs ]] ( S - LSTM ; 3 ) .",1
10231,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an [[ algorithm inspired by transition - based parsing with states ]] << represented by >> stack LSTMs ( [[ S - LSTM ]] ; 3 ) .",0
10232,252,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states << represented by >> [[ stack LSTMs ]] ( [[ S - LSTM ]] ; 3 ) .",0
10233,2516,[[ Residual LSTM ]] << is >> also the [[ current state - of - the - art ]] on the MSCOCO dataset .,1
10234,2516,[[ Residual LSTM ]] << is >> also the current state - of - the - art on the [[ MSCOCO dataset ]] .,0
10235,2516,Residual LSTM << is >> also the [[ current state - of - the - art ]] on the [[ MSCOCO dataset ]] .,0
10236,2516,[[ Residual LSTM ]] is also the [[ current state - of - the - art ]] << on >> the MSCOCO dataset .,0
10237,2516,[[ Residual LSTM ]] is also the current state - of - the - art << on >> the [[ MSCOCO dataset ]] .,0
10238,2516,Residual LSTM is also the [[ current state - of - the - art ]] << on >> the [[ MSCOCO dataset ]] .,1
10239,2962,"The [[ passage - aligned question representation ]] << is >> [[ crucial ]] , since lexically similar regions of the passage provide strong signal for relevant answer spans .",1
10240,4453,We also << observe >> [[ d- TBCNN ]] achieves [[ higher performance ]] than c - TBCNN .,0
10241,4453,We also << observe >> [[ d- TBCNN ]] achieves higher performance than [[ c - TBCNN ]] .,0
10242,4453,We also << observe >> d- TBCNN achieves [[ higher performance ]] than [[ c - TBCNN ]] .,0
10243,4453,We also observe [[ d- TBCNN ]] << achieves >> [[ higher performance ]] than c - TBCNN .,1
10244,4453,We also observe [[ d- TBCNN ]] << achieves >> higher performance than [[ c - TBCNN ]] .,0
10245,4453,We also observe d- TBCNN << achieves >> [[ higher performance ]] than [[ c - TBCNN ]] .,0
10246,4453,We also observe [[ d- TBCNN ]] achieves [[ higher performance ]] << than >> c - TBCNN .,0
10247,4453,We also observe [[ d- TBCNN ]] achieves higher performance << than >> [[ c - TBCNN ]] .,0
10248,4453,We also observe d- TBCNN achieves [[ higher performance ]] << than >> [[ c - TBCNN ]] .,1
10249,1202,[[ BART ]] << is >> a [[ denoising autoencoder ]] built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .,1
10250,1202,[[ BART ]] << is >> a denoising autoencoder built with a [[ sequence - to - sequence model ]] that is applicable to a very wide range of end tasks .,0
10251,1202,[[ BART ]] << is >> a denoising autoencoder built with a sequence - to - sequence model that is applicable to a [[ very wide range of end tasks ]] .,0
10252,1202,BART << is >> a [[ denoising autoencoder ]] built with a [[ sequence - to - sequence model ]] that is applicable to a very wide range of end tasks .,0
10253,1202,BART << is >> a [[ denoising autoencoder ]] built with a sequence - to - sequence model that is applicable to a [[ very wide range of end tasks ]] .,0
10254,1202,BART << is >> a denoising autoencoder built with a [[ sequence - to - sequence model ]] that is applicable to a [[ very wide range of end tasks ]] .,0
10255,1202,[[ BART ]] is a [[ denoising autoencoder ]] << built with >> a sequence - to - sequence model that is applicable to a very wide range of end tasks .,0
10256,1202,[[ BART ]] is a denoising autoencoder << built with >> a [[ sequence - to - sequence model ]] that is applicable to a very wide range of end tasks .,0
10257,1202,[[ BART ]] is a denoising autoencoder << built with >> a sequence - to - sequence model that is applicable to a [[ very wide range of end tasks ]] .,0
10258,1202,BART is a [[ denoising autoencoder ]] << built with >> a [[ sequence - to - sequence model ]] that is applicable to a very wide range of end tasks .,1
10259,1202,BART is a [[ denoising autoencoder ]] << built with >> a sequence - to - sequence model that is applicable to a [[ very wide range of end tasks ]] .,0
10260,1202,BART is a denoising autoencoder << built with >> a [[ sequence - to - sequence model ]] that is applicable to a [[ very wide range of end tasks ]] .,0
10261,1202,[[ BART ]] is a [[ denoising autoencoder ]] built with a sequence - to - sequence model that is << applicable to >> a very wide range of end tasks .,0
10262,1202,[[ BART ]] is a denoising autoencoder built with a [[ sequence - to - sequence model ]] that is << applicable to >> a very wide range of end tasks .,0
10263,1202,[[ BART ]] is a denoising autoencoder built with a sequence - to - sequence model that is << applicable to >> a [[ very wide range of end tasks ]] .,0
10264,1202,BART is a [[ denoising autoencoder ]] built with a [[ sequence - to - sequence model ]] that is << applicable to >> a very wide range of end tasks .,0
10265,1202,BART is a [[ denoising autoencoder ]] built with a sequence - to - sequence model that is << applicable to >> a [[ very wide range of end tasks ]] .,1
10266,1202,BART is a denoising autoencoder built with a [[ sequence - to - sequence model ]] that is << applicable to >> a [[ very wide range of end tasks ]] .,0
10267,3462,The [[ span representations ]] are used << to perform >> [[ entity mention detection ]] on all spans in parallel .,1
10268,3462,The [[ span representations ]] are used << to perform >> entity mention detection on [[ all spans ]] in parallel .,0
10269,3462,The [[ span representations ]] are used << to perform >> entity mention detection on all spans in [[ parallel ]] .,0
10270,3462,The span representations are used << to perform >> [[ entity mention detection ]] on [[ all spans ]] in parallel .,0
10271,3462,The span representations are used << to perform >> [[ entity mention detection ]] on all spans in [[ parallel ]] .,0
10272,3462,The span representations are used << to perform >> entity mention detection on [[ all spans ]] in [[ parallel ]] .,0
10273,3462,The [[ span representations ]] are used to perform [[ entity mention detection ]] << on >> all spans in parallel .,0
10274,3462,The [[ span representations ]] are used to perform entity mention detection << on >> [[ all spans ]] in parallel .,0
10275,3462,The [[ span representations ]] are used to perform entity mention detection << on >> all spans in [[ parallel ]] .,0
10276,3462,The span representations are used to perform [[ entity mention detection ]] << on >> [[ all spans ]] in parallel .,1
10277,3462,The span representations are used to perform [[ entity mention detection ]] << on >> all spans in [[ parallel ]] .,0
10278,3462,The span representations are used to perform entity mention detection << on >> [[ all spans ]] in [[ parallel ]] .,0
10279,3462,The [[ span representations ]] are used to perform [[ entity mention detection ]] on all spans << in >> parallel .,0
10280,3462,The [[ span representations ]] are used to perform entity mention detection on [[ all spans ]] << in >> parallel .,0
10281,3462,The [[ span representations ]] are used to perform entity mention detection on all spans << in >> [[ parallel ]] .,0
10282,3462,The span representations are used to perform [[ entity mention detection ]] on [[ all spans ]] << in >> parallel .,0
10283,3462,The span representations are used to perform [[ entity mention detection ]] on all spans << in >> [[ parallel ]] .,0
10284,3462,The span representations are used to perform entity mention detection on [[ all spans ]] << in >> [[ parallel ]] .,1
10285,2797,"We << use >> the [[ Unfolding Recursive Autoencoder ]] to get a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10286,2797,"We << use >> the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10287,2797,"We << use >> the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10288,2797,"We << use >> the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10289,2797,"We << use >> the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10290,2797,"We << use >> the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10291,2797,"We << use >> the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10292,2797,"We << use >> the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10293,2797,"We << use >> the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10294,2797,"We << use >> the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10295,2797,"We << use >> the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10296,2797,"We << use >> the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10297,2797,"We << use >> the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10298,2797,"We << use >> the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10299,2797,"We << use >> the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; [[ SENNA + MLP / SIM ]] :",0
10300,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to << get >> a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",1
10301,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to << get >> a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10302,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to << get >> a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10303,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to << get >> a 100 dimensional vector representation of each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10304,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to << get >> a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10305,2797,"We use the Unfolding Recursive Autoencoder to << get >> a [[ 100 dimensional vector representation ]] of [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10306,2797,"We use the Unfolding Recursive Autoencoder to << get >> a [[ 100 dimensional vector representation ]] of each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10307,2797,"We use the Unfolding Recursive Autoencoder to << get >> a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10308,2797,"We use the Unfolding Recursive Autoencoder to << get >> a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10309,2797,"We use the Unfolding Recursive Autoencoder to << get >> a 100 dimensional vector representation of [[ each sentence ]] , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10310,2797,"We use the Unfolding Recursive Autoencoder to << get >> a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10311,2797,"We use the Unfolding Recursive Autoencoder to << get >> a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10312,2797,"We use the Unfolding Recursive Autoencoder to << get >> a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10313,2797,"We use the Unfolding Recursive Autoencoder to << get >> a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10314,2797,"We use the Unfolding Recursive Autoencoder to << get >> a 100 dimensional vector representation of each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; [[ SENNA + MLP / SIM ]] :",0
10315,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a [[ 100 dimensional vector representation ]] << of >> each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10316,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation << of >> [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10317,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation << of >> each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10318,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation << of >> each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10319,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation << of >> each sentence , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10320,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] << of >> [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",1
10321,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] << of >> each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10322,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] << of >> each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10323,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] << of >> each sentence , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10324,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation << of >> [[ each sentence ]] , and put [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10325,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation << of >> [[ each sentence ]] , and put an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10326,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation << of >> [[ each sentence ]] , and put an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10327,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation << of >> each sentence , and put [[ an MLP ]] on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10328,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation << of >> each sentence , and put [[ an MLP ]] on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10329,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation << of >> each sentence , and put an MLP on the top as in [[ WORDEMBED ]] ; [[ SENNA + MLP / SIM ]] :",0
10330,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a [[ 100 dimensional vector representation ]] of each sentence , and << put >> an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10331,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of [[ each sentence ]] , and << put >> an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10332,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and << put >> [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",1
10333,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and << put >> an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10334,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and << put >> an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10335,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of [[ each sentence ]] , and << put >> an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10336,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and << put >> [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10337,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and << put >> an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10338,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and << put >> an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10339,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and << put >> [[ an MLP ]] on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
10340,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and << put >> an MLP on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10341,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and << put >> an MLP on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10342,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and << put >> [[ an MLP ]] on the top as in [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10343,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and << put >> [[ an MLP ]] on the top as in WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10344,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and << put >> an MLP on the top as in [[ WORDEMBED ]] ; [[ SENNA + MLP / SIM ]] :",0
10345,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP << on the top as in >> WORDEMBED ; SENNA + MLP / SIM :",0
10346,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP << on the top as in >> WORDEMBED ; SENNA + MLP / SIM :",0
10347,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] << on the top as in >> WORDEMBED ; SENNA + MLP / SIM :",0
10348,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and put an MLP << on the top as in >> [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10349,2797,"We use the [[ Unfolding Recursive Autoencoder ]] to get a 100 dimensional vector representation of each sentence , and put an MLP << on the top as in >> WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10350,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of [[ each sentence ]] , and put an MLP << on the top as in >> WORDEMBED ; SENNA + MLP / SIM :",0
10351,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and put [[ an MLP ]] << on the top as in >> WORDEMBED ; SENNA + MLP / SIM :",0
10352,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP << on the top as in >> [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10353,2797,"We use the Unfolding Recursive Autoencoder to get a [[ 100 dimensional vector representation ]] of each sentence , and put an MLP << on the top as in >> WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10354,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and put [[ an MLP ]] << on the top as in >> WORDEMBED ; SENNA + MLP / SIM :",0
10355,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP << on the top as in >> [[ WORDEMBED ]] ; SENNA + MLP / SIM :",0
10356,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of [[ each sentence ]] , and put an MLP << on the top as in >> WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10357,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] << on the top as in >> [[ WORDEMBED ]] ; SENNA + MLP / SIM :",1
10358,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put [[ an MLP ]] << on the top as in >> WORDEMBED ; [[ SENNA + MLP / SIM ]] :",0
10359,2797,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP << on the top as in >> [[ WORDEMBED ]] ; [[ SENNA + MLP / SIM ]] :",0
10360,5056,"From the results , we << see >> [[ some improvement ]] on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .",0
10361,5056,"From the results , we << see >> [[ some improvement ]] on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is statistically significant only for the Nuclearity and Relation predictions .",0
10362,5056,"From the results , we << see >> [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10363,5056,"From the results , we << see >> [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10364,5056,"From the results , we << see >> [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10365,5056,"From the results , we << see >> some improvement on [[ Discourse Structure prediction ]] when we are using a [[ joint model ]] but the improvement is statistically significant only for the Nuclearity and Relation predictions .",0
10366,5056,"From the results , we << see >> some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10367,5056,"From the results , we << see >> some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10368,5056,"From the results , we << see >> some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10369,5056,"From the results , we << see >> some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10370,5056,"From the results , we << see >> some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10371,5056,"From the results , we << see >> some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10372,5056,"From the results , we << see >> some improvement on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10373,5056,"From the results , we << see >> some improvement on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10374,5056,"From the results , we << see >> some improvement on Discourse Structure prediction when we are using a joint model but the improvement is [[ statistically significant ]] only for the [[ Nuclearity and Relation predictions ]] .",0
10375,5056,"From the results , we see [[ some improvement ]] << on >> [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .",1
10376,5056,"From the results , we see [[ some improvement ]] << on >> Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is statistically significant only for the Nuclearity and Relation predictions .",0
10377,5056,"From the results , we see [[ some improvement ]] << on >> Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10378,5056,"From the results , we see [[ some improvement ]] << on >> Discourse Structure prediction when we are using a joint model but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10379,5056,"From the results , we see [[ some improvement ]] << on >> Discourse Structure prediction when we are using a joint model but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10380,5056,"From the results , we see some improvement << on >> [[ Discourse Structure prediction ]] when we are using a [[ joint model ]] but the improvement is statistically significant only for the Nuclearity and Relation predictions .",0
10381,5056,"From the results , we see some improvement << on >> [[ Discourse Structure prediction ]] when we are using a joint model but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10382,5056,"From the results , we see some improvement << on >> [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10383,5056,"From the results , we see some improvement << on >> [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10384,5056,"From the results , we see some improvement << on >> Discourse Structure prediction when we are using a [[ joint model ]] but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10385,5056,"From the results , we see some improvement << on >> Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10386,5056,"From the results , we see some improvement << on >> Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10387,5056,"From the results , we see some improvement << on >> Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10388,5056,"From the results , we see some improvement << on >> Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10389,5056,"From the results , we see some improvement << on >> Discourse Structure prediction when we are using a joint model but the improvement is [[ statistically significant ]] only for the [[ Nuclearity and Relation predictions ]] .",0
10390,5056,"From the results , we see [[ some improvement ]] on [[ Discourse Structure prediction ]] << when we are using >> a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .",0
10391,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction << when we are using >> a [[ joint model ]] but the improvement is statistically significant only for the Nuclearity and Relation predictions .",0
10392,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction << when we are using >> a joint model but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10393,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction << when we are using >> a joint model but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10394,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction << when we are using >> a joint model but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10395,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] << when we are using >> a [[ joint model ]] but the improvement is statistically significant only for the Nuclearity and Relation predictions .",1
10396,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] << when we are using >> a joint model but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10397,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] << when we are using >> a joint model but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10398,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] << when we are using >> a joint model but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10399,5056,"From the results , we see some improvement on Discourse Structure prediction << when we are using >> a [[ joint model ]] but the [[ improvement ]] is statistically significant only for the Nuclearity and Relation predictions .",0
10400,5056,"From the results , we see some improvement on Discourse Structure prediction << when we are using >> a [[ joint model ]] but the improvement is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10401,5056,"From the results , we see some improvement on Discourse Structure prediction << when we are using >> a [[ joint model ]] but the improvement is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10402,5056,"From the results , we see some improvement on Discourse Structure prediction << when we are using >> a joint model but the [[ improvement ]] is [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10403,5056,"From the results , we see some improvement on Discourse Structure prediction << when we are using >> a joint model but the [[ improvement ]] is statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10404,5056,"From the results , we see some improvement on Discourse Structure prediction << when we are using >> a joint model but the improvement is [[ statistically significant ]] only for the [[ Nuclearity and Relation predictions ]] .",0
10405,5056,"From the results , we see [[ some improvement ]] on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement << is >> statistically significant only for the Nuclearity and Relation predictions .",0
10406,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement << is >> statistically significant only for the Nuclearity and Relation predictions .",0
10407,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] << is >> statistically significant only for the Nuclearity and Relation predictions .",0
10408,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the improvement << is >> [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10409,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the improvement << is >> statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10410,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a [[ joint model ]] but the improvement << is >> statistically significant only for the Nuclearity and Relation predictions .",0
10411,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the [[ improvement ]] << is >> statistically significant only for the Nuclearity and Relation predictions .",0
10412,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement << is >> [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10413,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement << is >> statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10414,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the [[ improvement ]] << is >> statistically significant only for the Nuclearity and Relation predictions .",0
10415,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement << is >> [[ statistically significant ]] only for the Nuclearity and Relation predictions .",0
10416,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement << is >> statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10417,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] << is >> [[ statistically significant ]] only for the Nuclearity and Relation predictions .",1
10418,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] << is >> statistically significant only for the [[ Nuclearity and Relation predictions ]] .",0
10419,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the improvement << is >> [[ statistically significant ]] only for the [[ Nuclearity and Relation predictions ]] .",0
10420,5056,"From the results , we see [[ some improvement ]] on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is statistically significant << only for >> the Nuclearity and Relation predictions .",0
10421,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is statistically significant << only for >> the Nuclearity and Relation predictions .",0
10422,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is statistically significant << only for >> the Nuclearity and Relation predictions .",0
10423,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the improvement is [[ statistically significant ]] << only for >> the Nuclearity and Relation predictions .",0
10424,5056,"From the results , we see [[ some improvement ]] on Discourse Structure prediction when we are using a joint model but the improvement is statistically significant << only for >> the [[ Nuclearity and Relation predictions ]] .",0
10425,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a [[ joint model ]] but the improvement is statistically significant << only for >> the Nuclearity and Relation predictions .",0
10426,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the [[ improvement ]] is statistically significant << only for >> the Nuclearity and Relation predictions .",0
10427,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is [[ statistically significant ]] << only for >> the Nuclearity and Relation predictions .",0
10428,5056,"From the results , we see some improvement on [[ Discourse Structure prediction ]] when we are using a joint model but the improvement is statistically significant << only for >> the [[ Nuclearity and Relation predictions ]] .",0
10429,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the [[ improvement ]] is statistically significant << only for >> the Nuclearity and Relation predictions .",0
10430,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is [[ statistically significant ]] << only for >> the Nuclearity and Relation predictions .",0
10431,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a [[ joint model ]] but the improvement is statistically significant << only for >> the [[ Nuclearity and Relation predictions ]] .",0
10432,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is [[ statistically significant ]] << only for >> the Nuclearity and Relation predictions .",0
10433,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the [[ improvement ]] is statistically significant << only for >> the [[ Nuclearity and Relation predictions ]] .",0
10434,5056,"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the improvement is [[ statistically significant ]] << only for >> the [[ Nuclearity and Relation predictions ]] .",1
10435,2763,"We << construct and publicly release >> a [[ dataset of 25,100 queries ]] annotated with the [[ probability ]] of being a well - formed natural language question ( 2.1 ) .",0
10436,2763,"We << construct and publicly release >> a [[ dataset of 25,100 queries ]] annotated with the probability of being a [[ well - formed natural language question ]] ( 2.1 ) .",0
10437,2763,"We << construct and publicly release >> a dataset of 25,100 queries annotated with the [[ probability ]] of being a [[ well - formed natural language question ]] ( 2.1 ) .",0
10438,2763,"We construct and publicly release a [[ dataset of 25,100 queries ]] << annotated with >> the [[ probability ]] of being a well - formed natural language question ( 2.1 ) .",1
10439,2763,"We construct and publicly release a [[ dataset of 25,100 queries ]] << annotated with >> the probability of being a [[ well - formed natural language question ]] ( 2.1 ) .",0
10440,2763,"We construct and publicly release a dataset of 25,100 queries << annotated with >> the [[ probability ]] of being a [[ well - formed natural language question ]] ( 2.1 ) .",0
10441,2763,"We construct and publicly release a [[ dataset of 25,100 queries ]] annotated with the [[ probability ]] << of being >> a well - formed natural language question ( 2.1 ) .",0
10442,2763,"We construct and publicly release a [[ dataset of 25,100 queries ]] annotated with the probability << of being >> a [[ well - formed natural language question ]] ( 2.1 ) .",0
10443,2763,"We construct and publicly release a dataset of 25,100 queries annotated with the [[ probability ]] << of being >> a [[ well - formed natural language question ]] ( 2.1 ) .",1
10444,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10445,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10446,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10447,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10448,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10449,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10450,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10451,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10452,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10453,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10454,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10455,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> [[ twelve methods ]] , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10456,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10457,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10458,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10459,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10460,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10461,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10462,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10463,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10464,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10465,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10466,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10467,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10468,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10469,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10470,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10471,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10472,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10473,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10474,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10475,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10476,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10477,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10478,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10479,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10480,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10481,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10482,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10483,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10484,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10485,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10486,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10487,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10488,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10489,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10490,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10491,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10492,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10493,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10494,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10495,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10496,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10497,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10498,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10499,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10500,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10501,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10502,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10503,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10504,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10505,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10506,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10507,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10508,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10509,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10510,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10511,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10512,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10513,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10514,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10515,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10516,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10517,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10518,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10519,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10520,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10521,5192,"To validate the superiority of BiHDM , we also conduct the same experiments << using >> twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10522,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10523,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10524,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10525,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10526,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10527,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10528,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10529,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10530,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10531,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",1
10532,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",1
10533,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using [[ twelve methods ]] , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",1
10534,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10535,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10536,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10537,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10538,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10539,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10540,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10541,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10542,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10543,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10544,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> [[ linear support vector machine ( SVM ) ]] , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10545,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10546,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10547,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10548,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10549,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10550,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10551,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10552,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10553,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10554,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , [[ random forest ( RF ) ]] , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10555,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10556,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10557,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10558,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10559,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10560,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10561,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10562,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10563,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , [[ canonical correlation analysis ( CCA ) ]] , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10564,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10565,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10566,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10567,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10568,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10569,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10570,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10571,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , [[ group sparse canonical correlation analysis ( GSCCA ) ]] , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10572,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10573,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10574,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10575,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10576,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10577,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10578,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , [[ deep believe network ( DBN ) ]] , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10579,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10580,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10581,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10582,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10583,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10584,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , [[ graph regularization sparse linear regression ( GRSLR ) ]] , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10585,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10586,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10587,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10588,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10589,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , [[ graph convolutional neural network ( GCNN ) ]] , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10590,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10591,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10592,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10593,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , [[ dynamical graph convolutional neural network ( DGCNN ) ]] , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10594,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .",0
10595,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10596,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , [[ domain adversarial neural networks ( DANN ) ]] , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10597,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , [[ EmotionMeter ]] , and attention - long short - term memory ( A - LSTM ) .",0
10598,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , [[ bi-hemisphere domain adversarial neural network ( BiDANN ) ]] , EmotionMeter , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10599,5192,"To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , << including >> linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , [[ EmotionMeter ]] , and [[ attention - long short - term memory ( A - LSTM ) ]] .",0
10600,3980,"In this work , we << present >> a [[ hierarchical neural network model ]] for the [[ sequential sentence classification task ]] , which we call a hierarchical sequential labeling network ( HSLN ) .",0
10601,3980,"In this work , we << present >> a [[ hierarchical neural network model ]] for the sequential sentence classification task , which we call a [[ hierarchical sequential labeling network ( HSLN ) ]] .",0
10602,3980,"In this work , we << present >> a hierarchical neural network model for the [[ sequential sentence classification task ]] , which we call a [[ hierarchical sequential labeling network ( HSLN ) ]] .",0
10603,3980,"In this work , we present a [[ hierarchical neural network model ]] << for >> the [[ sequential sentence classification task ]] , which we call a hierarchical sequential labeling network ( HSLN ) .",1
10604,3980,"In this work , we present a [[ hierarchical neural network model ]] << for >> the sequential sentence classification task , which we call a [[ hierarchical sequential labeling network ( HSLN ) ]] .",0
10605,3980,"In this work , we present a hierarchical neural network model << for >> the [[ sequential sentence classification task ]] , which we call a [[ hierarchical sequential labeling network ( HSLN ) ]] .",0
10606,3980,"In this work , we present a [[ hierarchical neural network model ]] for the [[ sequential sentence classification task ]] , which we << call >> a hierarchical sequential labeling network ( HSLN ) .",0
10607,3980,"In this work , we present a [[ hierarchical neural network model ]] for the sequential sentence classification task , which we << call >> a [[ hierarchical sequential labeling network ( HSLN ) ]] .",1
10608,3980,"In this work , we present a hierarchical neural network model for the [[ sequential sentence classification task ]] , which we << call >> a [[ hierarchical sequential labeling network ( HSLN ) ]] .",0
10609,235,"To this end , we << use >> the [[ p ( 0 < p ? 1 ) - norm based robust measure ]] for the [[ representation loss ]] , preventing large errors from dominating our objective .",0
10610,235,"To this end , we use the [[ p ( 0 < p ? 1 ) - norm based robust measure ]] << for >> the [[ representation loss ]] , preventing large errors from dominating our objective .",1
