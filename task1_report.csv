,paper,idx,text,main_heading,heading,topic,paper_idx,offset,mask,b_labels,m_labels,b_score,b_pred,m_pred,ablation-analysis,approach,baselines,code,dataset,experimental-setup,experiments,hyperparameters,model,negative,research-problem,results,tasks
0,text_generation4,1,title,,,text_generation,4,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
1,text_generation4,2,Generating Text through Adversarial Training using Skip - Thought Vectors,title,title,text_generation,4,1,1,1,research-problem,0.998599886,1,research-problem,4.62E-08,4.09E-05,1.45E-07,4.68E-08,6.59E-08,1.25E-07,7.85E-07,3.36E-06,5.29E-06,0.003670837,0.996278007,3.71E-07,4.87E-08
2,text_generation4,3,abstract,,,text_generation,4,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
3,text_generation4,4,"In the past few years , various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks ( GANs ) .",abstract,abstract,text_generation,4,1,1,0,,0.180824986,0,research-problem,7.70E-08,3.05E-05,2.85E-08,2.11E-06,6.16E-07,1.70E-06,6.42E-07,1.08E-05,3.19E-06,0.073372273,0.926577979,4.16E-08,7.65E-08
4,text_generation4,5,GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer .,abstract,abstract,text_generation,4,2,1,0,,0.117384612,0,research-problem,1.26E-07,2.37E-05,6.58E-08,1.12E-06,6.55E-07,8.55E-07,9.41E-07,5.43E-06,2.06E-06,0.04464221,0.955322622,1.45E-07,9.67E-08
5,text_generation4,6,"In the field of Natural Language Processing , word embeddings such as word2vec and GLoVe are state - of - the - art methods for applying neural network models on textual data .",abstract,abstract,text_generation,4,3,1,0,,0.074352419,0,research-problem,3.32E-08,1.01E-05,1.83E-08,1.53E-06,3.04E-07,1.11E-06,5.88E-07,5.05E-06,6.21E-07,0.028627824,0.971352778,2.50E-08,6.42E-08
6,text_generation4,7,Attempts have been made for utilizing GANs with word embeddings for text generation .,abstract,abstract,text_generation,4,4,1,1,research-problem,0.600754633,1,research-problem,6.75E-08,1.93E-05,2.07E-08,7.88E-07,3.40E-07,1.14E-06,5.66E-07,9.30E-06,1.36E-06,0.059505844,0.940461207,4.38E-08,5.60E-08
7,text_generation4,8,This work presents an approach to text generation using Skip - Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures .,abstract,abstract,text_generation,4,5,1,0,,0.84669458,1,research-problem,2.88E-06,0.004957041,9.91E-06,9.11E-06,3.85E-05,3.15E-06,1.04E-05,2.84E-05,7.42E-05,0.025457749,0.969404484,2.75E-06,1.33E-06
8,text_generation4,9,The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used .,abstract,abstract,text_generation,4,6,1,0,,0.135198961,0,research-problem,2.61E-05,0.001676547,2.83E-06,2.58E-06,1.49E-05,1.16E-05,4.63E-05,0.000269545,4.74E-05,0.414710654,0.583008111,0.000181866,1.67E-06
9,text_generation4,10,Introduction,,,text_generation,4,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
10,text_generation4,11,Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .,Introduction,Introduction,text_generation,4,1,1,1,research-problem,0.743679126,1,research-problem,6.44E-07,0.000112348,1.24E-07,3.28E-06,7.39E-06,4.50E-06,7.88E-06,6.57E-06,2.88E-05,0.057587619,0.942239075,7.67E-07,9.71E-07
11,text_generation4,12,"Early techniques for generating text conditioned on some input information were template or rule - based engines , or probabilistic models such as n-gram .",Introduction,Introduction,text_generation,4,2,1,0,,0.03506782,0,research-problem,4.74E-06,0.000480197,1.49E-06,2.17E-05,9.75E-05,5.19E-05,3.31E-05,3.43E-05,0.000236572,0.293785375,0.70524596,3.34E-06,3.81E-06
12,text_generation4,13,"In recent times , state - of - the - art results on these tasks have been achieved by recurrent and convolutional neural network models trained for likelihood maximization .",Introduction,Introduction,text_generation,4,3,1,0,,0.239828365,0,research-problem,1.78E-06,0.000512186,9.87E-07,1.10E-05,1.64E-05,3.21E-05,2.01E-05,3.24E-05,0.000466064,0.178510949,0.820391478,1.99E-06,2.53E-06
13,text_generation4,14,This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .,Introduction,Introduction,text_generation,4,4,1,1,code,0.965144647,1,code,0.000102249,0.012745502,0.000110872,0.629140668,0.074537326,0.004235515,0.000562914,0.000306254,0.003106045,0.204894986,0.069948467,1.86E-05,0.00029064
14,text_generation4,15,GANs are a class of neural networks that explicitly train a generator to produce high - quality samples by pitting against an adversarial discriminative model .,Introduction,Introduction,text_generation,4,5,1,0,,0.501475493,1,research-problem,5.26E-06,0.014751116,2.35E-05,2.65E-05,7.93E-05,0.000141343,8.64E-05,0.000324826,0.026252349,0.176240526,0.782047889,5.98E-06,1.51E-05
15,text_generation4,16,GANs output differentiable values and hence the task of discrete text generation has to use vectors as differentiable inputs .,Introduction,Introduction,text_generation,4,6,1,0,,0.648391911,1,research-problem,9.39E-07,0.001224832,7.34E-07,4.53E-06,7.07E-06,1.79E-05,1.09E-05,5.59E-05,0.001397098,0.103971724,0.893305173,1.41E-06,1.80E-06
16,text_generation4,17,"This is achieved by training the GAN with sentence embedding vectors produced by Skip - Thought , a neural network model for learning fixed length representations of sentences .",Introduction,Introduction,text_generation,4,7,1,0,,0.939530033,1,model,3.61E-05,0.393896305,8.08E-05,3.38E-06,0.000705302,4.49E-05,1.72E-05,0.000143811,0.586490573,0.018220103,0.00035609,3.61E-06,1.71E-06
17,text_generation4,18,Related Works,,,text_generation,4,0,1,0,,0.000144394,0,negative,8.00E-06,1.03E-05,1.47E-06,1.57E-07,1.75E-07,1.81E-05,1.36E-05,0.000169567,3.75E-06,0.999096229,0.000638872,3.92E-05,4.90E-07
18,text_generation4,43,Skip - Thought Architecture,,,text_generation,4,0,1,0,,0.059006326,0,negative,0.000360724,0.002596505,0.009777054,3.35E-05,1.16E-05,0.001056331,0.00184148,0.006042046,0.013926632,0.65175032,0.309800227,0.002489238,0.000314323
19,text_generation4,44,"For a given sentence tuple ( s i?1 , s i , s i + 1 ) , let wt i denote the t- th word for sentence s i , and let x ti denote it s word embedding .",Skip - Thought Architecture,Skip - Thought Architecture,text_generation,4,1,1,0,,8.33E-07,0,negative,5.97E-07,8.38E-06,2.31E-06,5.21E-09,1.47E-08,9.70E-07,2.12E-07,2.10E-05,0.000148349,0.999813509,2.76E-06,1.84E-06,1.20E-08
20,text_generation4,45,The model has three components : Encoder .,Skip - Thought Architecture,Skip - Thought Architecture,text_generation,4,2,1,0,,0.000165751,0,negative,0.000225944,0.000521186,0.001886767,8.51E-06,5.28E-06,5.78E-05,2.75E-05,0.000309058,0.057424329,0.939206093,0.000234287,7.57E-05,1.76E-05
21,text_generation4,46,"Encoded vectors for a sentence s i with N words w i , w i + 1 ,... , w n are computed by iterating over the following sequence of equations :",Skip - Thought Architecture,Skip - Thought Architecture,text_generation,4,3,1,0,,8.08E-06,0,negative,3.32E-06,2.76E-05,3.37E-05,2.90E-08,9.00E-08,1.71E-06,5.67E-07,2.42E-05,0.000761146,0.999131928,8.85E-06,6.86E-06,7.77E-08
22,text_generation4,47,"where ht i is a hidden state at each time step and interpreted as a sequence of words w 1 i , ... , w n i , t is the proposed state update at time t , z t is the update gate and rt is the reset gate .",Skip - Thought Architecture,Skip - Thought Architecture,text_generation,4,4,1,0,,1.63E-06,0,negative,3.39E-06,1.33E-05,5.96E-06,6.41E-08,8.23E-08,1.83E-06,5.61E-07,3.31E-05,0.000382448,0.999548826,5.65E-06,4.70E-06,7.82E-08
23,text_generation4,48,Both update gates take values between zero and one .,Skip - Thought Architecture,Skip - Thought Architecture,text_generation,4,5,1,0,,0.002569372,0,negative,9.88E-06,0.000139516,1.45E-05,2.44E-07,2.23E-07,9.63E-05,9.40E-06,0.004483503,0.003213969,0.992011372,1.13E-05,9.09E-06,6.70E-07
24,text_generation4,49,Decoder .,Skip - Thought Architecture,,text_generation,4,6,1,0,,0.003480659,0,negative,2.03E-05,9.11E-05,5.66E-05,5.29E-07,5.80E-07,1.76E-05,6.40E-06,0.000266464,0.007429804,0.992021762,5.18E-05,3.48E-05,2.28E-06
25,text_generation4,50,A neural language model conditioned on the encoder output hi serves as the decoder .,Skip - Thought Architecture,Decoder .,text_generation,4,7,1,0,,7.76E-06,0,negative,1.96E-05,0.001259856,0.00068208,2.33E-07,4.10E-06,1.74E-05,3.70E-06,0.000113117,0.084383232,0.913442016,7.02E-05,3.50E-06,1.02E-06
26,text_generation4,51,"Bias matrices C z , Cr , C are introduced for the update gate , reset gate and hidden state computation by the encoder .",Skip - Thought Architecture,Decoder .,text_generation,4,8,1,0,,0.000276014,0,negative,8.57E-05,0.003197272,3.40E-05,4.39E-06,1.45E-05,0.000556041,3.12E-05,0.010062613,0.050952378,0.935008699,4.23E-05,7.05E-06,3.92E-06
27,text_generation4,52,"Two decoders are used in parallel , one each for sentences s i + 1 and s i ?",Skip - Thought Architecture,Decoder .,text_generation,4,9,1,0,,1.23E-05,0,negative,0.00011056,0.003598175,0.0021692,8.21E-07,1.47E-05,7.27E-05,2.51E-05,0.000251722,0.272458099,0.721151809,0.000131047,1.21E-05,3.93E-06
28,text_generation4,53,1 . The following equations are iterated over for decoding :,Skip - Thought Architecture,Decoder .,text_generation,4,10,1,0,,1.79E-06,0,negative,9.68E-06,7.59E-05,1.66E-05,4.35E-08,4.76E-07,1.25E-05,1.16E-06,5.98E-05,0.005233188,0.994581511,7.42E-06,1.65E-06,1.06E-07
29,text_generation4,54,Objective .,Skip - Thought Architecture,,text_generation,4,11,1,0,,2.11E-06,0,negative,1.02E-05,5.22E-05,6.96E-06,2.56E-06,8.06E-07,1.56E-05,3.57E-06,0.000265916,0.000560025,0.998961041,0.000106963,1.21E-05,2.00E-06
30,text_generation4,55,"For the same tuple of sentences , objective function is the sum of log-probabilities for the forward and backward sentences conditioned on the encoder representation :",Skip - Thought Architecture,Objective .,text_generation,4,12,1,0,,1.61E-05,0,negative,5.99E-06,1.01E-05,9.95E-06,2.03E-08,5.47E-08,1.21E-06,4.79E-07,2.18E-05,0.000167722,0.999736865,6.26E-06,3.94E-05,1.61E-07
31,text_generation4,56,Generative Adversarial,Skip - Thought Architecture,,text_generation,4,13,1,0,,0.010537749,0,negative,0.000118006,0.000624889,0.00513932,3.28E-06,4.13E-06,0.000101237,0.000262211,0.001182432,0.008669723,0.981169958,0.000436431,0.002229588,5.88E-05
32,text_generation4,57,Networks,Skip - Thought Architecture,,text_generation,4,14,1,0,,0.000214995,0,negative,0.000116943,8.68E-05,0.001125213,1.08E-06,4.12E-06,2.30E-05,0.000123791,0.000113048,0.001433997,0.991779037,0.000104663,0.005072194,1.61E-05
33,text_generation4,58,Generative Adversarial,Skip - Thought Architecture,,text_generation,4,15,1,0,,0.009599018,0,negative,0.000115449,0.000589049,0.004695313,3.07E-06,4.32E-06,9.30E-05,0.0002556,0.00110851,0.008533811,0.981995631,0.000330202,0.00221601,6.01E-05
34,text_generation4,59,"Networks are deep neural net architectures comprised of two networks , contesting with each other in a zero - sum game framework .",Skip - Thought Architecture,Generative Adversarial,text_generation,4,16,1,0,,0.002030812,0,negative,3.48E-05,1.89E-05,0.009324893,3.51E-06,5.41E-07,6.38E-05,5.19E-05,0.000514279,0.001532526,0.988098557,6.47E-05,0.000175599,0.000115949
35,text_generation4,60,"For a given data , GANs can mimic learning the underlying distribution and generate artificial data samples similar to those from the real distribution .",Skip - Thought Architecture,Generative Adversarial,text_generation,4,17,1,0,,7.86E-05,0,negative,9.56E-06,1.18E-06,3.61E-05,2.23E-08,2.11E-08,7.13E-07,8.21E-07,1.71E-05,1.22E-05,0.999693851,1.74E-06,0.000226199,5.38E-07
36,text_generation4,61,Generative Adversarial Networks consists of two players -a Generator and a Discriminator .,Skip - Thought Architecture,Generative Adversarial,text_generation,4,18,1,0,,0.000372655,0,negative,1.87E-05,1.42E-05,0.003706662,1.68E-06,2.23E-07,1.54E-05,2.39E-05,0.000205387,0.000378097,0.994937719,0.000239944,0.000373054,8.50E-05
37,text_generation4,62,The generator G tries to produce data close to the real distribution P ( x ) from some stochastic distribution P ( z ) termed as noise .,Skip - Thought Architecture,Generative Adversarial,text_generation,4,19,1,0,,2.08E-05,0,negative,8.59E-06,1.28E-06,2.85E-05,3.71E-08,2.86E-08,1.33E-06,8.48E-07,7.67E-05,3.25E-05,0.999827788,1.64E-07,2.19E-05,4.04E-07
38,text_generation4,63,The discriminator D 's objective is to differentiate between real and generated data G ( z ) .,Skip - Thought Architecture,Generative Adversarial,text_generation,4,20,1,0,,7.28E-06,0,negative,2.37E-06,1.37E-06,2.06E-05,1.29E-08,9.48E-09,4.75E-07,2.54E-07,2.48E-05,7.34E-05,0.999866258,3.32E-07,9.89E-06,2.06E-07
39,text_generation4,64,The two networks - generator and discriminator compete against each other in a zero - sum game .,Skip - Thought Architecture,Generative Adversarial,text_generation,4,21,1,0,,8.40E-05,0,negative,8.12E-06,1.89E-06,0.0001594,1.33E-08,2.18E-08,5.93E-07,8.44E-07,1.63E-05,0.000180641,0.999587661,3.46E-07,4.36E-05,5.31E-07
40,text_generation4,65,The minimax strategy dictates that each network plays optimally with the assumption that the other network is optimal .,Skip - Thought Architecture,Generative Adversarial,text_generation,4,22,1,0,,4.85E-05,0,negative,1.39E-05,1.16E-05,0.000360083,1.89E-08,4.94E-08,9.63E-07,1.53E-06,3.62E-05,0.000389322,0.999101328,9.33E-07,8.34E-05,6.01E-07
41,text_generation4,66,This leads to Nash equilibrium which is the point of convergence for GAN model .,Skip - Thought Architecture,Generative Adversarial,text_generation,4,23,1,0,,0.000150873,0,negative,2.24E-05,1.16E-06,8.74E-05,2.29E-08,3.00E-08,8.82E-07,2.08E-06,2.28E-05,6.40E-05,0.999473619,4.06E-07,0.000324204,1.06E-06
42,text_generation4,67,"Objective . have formulated the minimax game for a generator G , discriminator D adversarial network with value function V ( G , D ) as :",Skip - Thought Architecture,Generative Adversarial,text_generation,4,24,1,0,,1.81E-05,0,negative,1.37E-05,3.18E-06,0.000199658,6.58E-08,4.98E-08,1.23E-06,1.84E-06,2.46E-05,5.13E-05,0.999417378,5.18E-06,0.000279604,2.21E-06
43,text_generation4,68,Model Architecture,,,text_generation,4,0,1,0,,0.012027743,0,negative,0.002077781,0.004304933,0.001063385,0.001680559,0.000148392,0.002327543,0.001259945,0.007127733,0.020357931,0.924862365,0.03336821,0.000759331,0.000661892
44,text_generation4,69,"The STGAN model uses a deep convolutional generative adversarial network , similar to the one used in .",Model Architecture,Model Architecture,text_generation,4,1,1,0,,0.383831865,0,model,0.000104973,0.028137463,0.009295971,3.95E-06,0.000111848,0.000270607,0.000124389,0.000507867,0.828831619,0.132118626,0.000468139,1.57E-05,8.89E-06
45,text_generation4,70,The generator network is updated twice for each discriminator network update to prevent fast convergence of the discriminator network .,Model Architecture,Model Architecture,text_generation,4,2,1,0,,0.554760214,1,model,7.01E-05,0.035983165,0.000110397,9.58E-07,2.23E-05,0.000141762,1.75E-05,0.001564301,0.815850592,0.146051586,0.00018258,3.20E-06,1.50E-06
46,text_generation4,71,The Skip - Thought encoder for the model encodes sentences with length less than 30 words using 2400 GRU units with word vector dimensionality of 620 to produce 4800 - dimensional combineskip vectors . .,Model Architecture,Model Architecture,text_generation,4,3,1,1,hyperparameters,0.667233703,1,negative,8.41E-05,0.047978423,0.000213971,3.56E-05,0.000278898,0.008392623,0.000630389,0.032144661,0.386637971,0.522967388,0.000568556,2.02E-05,4.72E-05
47,text_generation4,72,"The combine - skip vectors , with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model , are used as they have been found to be the best performing in the experiments",Model Architecture,Model Architecture,text_generation,4,4,1,1,hyperparameters,0.857902298,1,negative,0.000128651,0.023867321,0.000137416,1.66E-05,0.000209222,0.009602781,0.000736607,0.026784256,0.075558219,0.862627774,0.000251369,6.26E-05,1.73E-05
48,text_generation4,73,1 .,Model Architecture,Model Architecture,text_generation,4,5,1,0,,0.001583763,0,negative,1.09E-05,0.000279381,1.56E-06,9.56E-07,5.53E-06,2.85E-05,1.37E-06,7.98E-05,0.005867478,0.993624497,9.78E-05,2.06E-06,2.05E-07
49,text_generation4,74,The decoder uses greedy decoding tak -,Model Architecture,Model Architecture,text_generation,4,6,1,0,,0.497550004,0,model,0.000244872,0.014597607,0.001398703,5.00E-07,3.23E-05,2.99E-05,1.42E-05,8.48E-05,0.535713778,0.447551193,0.000304889,2.63E-05,9.38E-07
50,text_generation4,75,Improving Training and Loss,,,text_generation,4,0,1,0,,0.40213053,0,negative,0.003614494,0.000185947,0.000104267,4.56E-06,6.08E-06,5.92E-05,0.000456183,0.000563844,3.17E-05,0.960521611,0.006840535,0.027591053,2.05E-05
51,text_generation4,76,"The training process of a GAN is notably difficult and several improvement techniques such as batch normalization , feature matching , historical averaging and unrolling GAN have been suggested for making the training more stable .",Improving Training and Loss,Improving Training and Loss,text_generation,4,1,1,0,,0.010948008,0,negative,0.001070526,2.97E-06,4.55E-05,2.77E-05,2.23E-06,7.28E-05,4.18E-05,5.95E-05,5.61E-06,0.989181657,0.008247019,0.000169526,0.001073045
52,text_generation4,77,Training the Skip - Thought GAN often results in mode dropping with a parameter setting where it outputs a very narrow distribution of points .,Improving Training and Loss,Improving Training and Loss,text_generation,4,2,1,0,,0.016925579,0,negative,0.002027885,4.18E-06,0.000101942,8.29E-06,1.37E-06,8.20E-05,4.90E-05,0.000116161,6.63E-06,0.99555955,0.001102023,0.000278539,0.00066252
53,text_generation4,78,"To overcome this , it uses minibatch discrimination by looking at an entire batch of samples and modeling the distance between a given sample and all the other samples present in that batch .",Improving Training and Loss,Improving Training and Loss,text_generation,4,3,1,0,,0.0032062,0,negative,0.01862913,0.000224022,0.379752817,5.80E-06,1.28E-05,0.000168453,0.000241018,0.00018302,0.00071644,0.59881502,0.000202699,0.000817621,0.000231127
54,text_generation4,79,The minimax formulation for an optimal discriminator in a vanilla GAN is Jensen - Shannon Distance between the generated distribution and the real distribution .,Improving Training and Loss,Improving Training and Loss,text_generation,4,4,1,0,,0.000153938,0,negative,0.000327047,2.44E-05,0.000410285,1.76E-06,5.49E-07,3.35E-05,2.10E-05,0.000176298,7.47E-05,0.996860734,0.00175865,0.000152875,0.000158159
55,text_generation4,80,GANs can be conditioned on data attributes to generate samples .,Improving Training and Loss,Improving Training and Loss,text_generation,4,5,1,0,,0.002761183,0,negative,0.001274057,1.34E-05,0.003438537,9.29E-07,8.05E-07,2.95E-05,1.07E-05,8.22E-05,0.000282571,0.994598647,7.01E-05,0.000130454,6.81E-05
56,text_generation4,81,"In this experiment , both the generator and discriminator are conditioned on Skip - Thought encoded vectors .",Improving Training and Loss,Improving Training and Loss,text_generation,4,6,1,0,,0.000970397,0,negative,0.001434367,3.61E-05,0.00187053,4.76E-07,1.90E-06,9.19E-05,3.77E-05,0.000310367,0.000121332,0.995956282,3.54E-06,0.000120928,1.45E-05
57,text_generation4,82,"The dataset comprises of 70,000 sentences chosen from the BookCorpus dataset , which belong to one series of fantasy novels of a particular author of English language .",Improving Training and Loss,Improving Training and Loss,text_generation,4,7,1,0,,0.001062188,0,negative,0.001530487,6.95E-06,0.000470458,8.38E-05,0.001308894,0.0001504,4.83E-05,2.21E-05,5.25E-06,0.995994858,2.95E-06,0.000136173,0.000239303
58,text_generation4,83,"This selection implies that the author 's word choice , sentence structure , figurative language , and sentence arrangement are consistent and well - represented across the dataset .",Improving Training and Loss,Improving Training and Loss,text_generation,4,8,1,0,,5.01E-05,0,negative,0.000581891,1.68E-06,6.25E-05,7.54E-07,5.50E-06,1.53E-05,3.68E-06,2.06E-05,3.86E-06,0.999243226,4.34E-07,5.61E-05,4.47E-06
59,text_generation4,84,Conditioning on this high - level outline gives more robustness to the model in terms of generated samples .,Improving Training and Loss,Improving Training and Loss,text_generation,4,9,1,0,,0.001950313,0,negative,0.298274981,5.00E-06,0.000244582,6.73E-07,2.04E-06,1.48E-05,4.82E-05,2.99E-05,1.98E-05,0.695598919,1.85E-06,0.005740352,1.88E-05
60,text_generation4,85,The encoder converts the dataset with a training / test / validation split of 5/1/1 into vectors to be used as real samples for discriminator .,Improving Training and Loss,Improving Training and Loss,text_generation,4,10,1,0,,0.084435305,0,negative,0.002866434,0.000176128,0.004877732,1.11E-05,7.33E-06,0.001668502,0.000358222,0.011486558,0.002786474,0.974814068,1.60E-05,4.78E-05,0.000883613
61,text_generation4,86,"The decoded sentences are used to evaluate model performance under corpus level BLEU - 2 , BLEU - 3 and BLEU - 4 metrics , once using only test set as reference and then entire corpus as reference .",Improving Training and Loss,Improving Training and Loss,text_generation,4,11,1,0,,0.001239636,0,negative,0.001038102,8.12E-06,0.001038024,1.26E-07,5.39E-06,1.28E-05,3.85E-05,2.87E-05,1.28E-05,0.997485007,5.36E-07,0.00032177,1.01E-05
62,text_generation4,87,Table 1 compares these results for different architectures that have been experimented within this paper against baselines of using character level RNNs and LSTMs 2 .,Improving Training and Loss,Improving Training and Loss,text_generation,4,12,1,0,,0.000174589,0,negative,0.000195468,1.53E-07,1.58E-05,2.44E-08,1.36E-07,5.91E-06,1.35E-05,1.00E-05,3.49E-07,0.999397185,7.39E-07,0.000356815,3.84E-06
63,text_generation4,88,The comparison is also made to RNNs and LSTMs fed with Skip Thought embeddings .,Improving Training and Loss,Improving Training and Loss,text_generation,4,13,1,0,,0.001064707,0,negative,0.001174144,6.97E-06,0.000661048,1.56E-07,6.96E-07,1.42E-05,2.34E-05,3.43E-05,1.63E-05,0.997762164,2.95E-06,0.000294665,8.90E-06
64,text_generation4,89,The models generate sentence vectors which are again decoded to produce sentences in english for computing the BLEU metrics .,Improving Training and Loss,Improving Training and Loss,text_generation,4,14,1,0,,0.004037091,0,negative,0.001083982,5.87E-05,0.013254402,5.22E-07,1.58E-06,7.98E-05,6.00E-05,0.000333029,0.003314848,0.981610831,7.78E-06,6.52E-05,0.000129342
65,text_generation4,90,Language Generation .,Improving Training and Loss,,text_generation,4,15,1,0,,0.001293615,0,negative,0.005858484,4.74E-06,0.000910605,6.08E-06,5.97E-06,0.000107667,0.001027157,8.07E-05,8.12E-06,0.967720083,0.000768496,0.011931379,0.011570484
66,text_generation4,91,Language generation is done on a dataset comprising simple English sentences referred to as CMU - SE 3 in .,Improving Training and Loss,Language Generation .,text_generation,4,16,1,0,,0.057785597,0,negative,0.003951413,1.78E-05,0.002290504,8.48E-06,5.16E-05,6.47E-05,0.001108903,3.64E-05,1.29E-05,0.966700713,2.76E-05,0.022416167,0.00331279
67,text_generation4,92,"The CMU - SE dataset consists of 44,016 sentences with a vocabulary of 3,122 words .",Improving Training and Loss,Language Generation .,text_generation,4,17,1,0,,0.043454017,0,negative,0.001601311,1.38E-06,0.000341279,3.19E-05,0.000201708,7.14E-05,0.000224573,1.02E-05,1.89E-06,0.991180294,2.08E-06,0.003212663,0.003119315
68,text_generation4,93,"For encoding , the vectors are extracted in batches of sentences having the same length .",Improving Training and Loss,Language Generation .,text_generation,4,18,1,0,,0.098382277,0,negative,0.000877826,2.01E-05,0.000759406,5.48E-07,1.62E-06,3.22E-05,5.50E-05,0.000147985,0.000286075,0.997583732,8.01E-07,0.00013039,0.000104328
69,text_generation4,94,The samples represent how mode collapse is manifested when using least - squares distance f- measure without minibatch discrimination .,Improving Training and Loss,Language Generation .,text_generation,4,19,1,0,,0.000783467,0,negative,0.000213998,1.18E-07,2.54E-05,4.24E-08,1.29E-07,3.99E-06,5.88E-06,6.94E-06,2.16E-06,0.999648834,3.12E-08,8.68E-05,5.62E-06
70,text_generation4,95,"Table 2 ( a ) contains sentences generated from STGAN using least - squares distance in which there was no mode collapse observed , while 2 ( b ) contains examples wherein it is observed .",Improving Training and Loss,Language Generation .,text_generation,4,20,1,0,,0.000241078,0,negative,0.000203493,3.72E-08,1.41E-05,7.94E-08,1.29E-06,1.45E-06,4.05E-06,8.30E-07,1.15E-07,0.999345443,2.09E-08,0.000423002,6.08E-06
71,text_generation4,96,Table 2 ( c ) shows generated sentences using gradient penalty regularizer ( GAN - GP ) .,Improving Training and Loss,Language Generation .,text_generation,4,21,1,0,,0.310792294,0,negative,0.002782424,1.04E-07,5.79E-05,2.03E-08,1.11E-07,1.65E-06,0.000147366,2.36E-06,2.26E-07,0.961284675,1.36E-07,0.035695727,2.73E-05
72,text_generation4,97,using Wasserstein distance f - measure as WGAN ) and 2 ( e ) contains samples when using a gradient penalty regularizer term as WGAN - GP .,Improving Training and Loss,Language Generation .,text_generation,4,22,1,0,,0.000610641,0,negative,0.00230691,2.35E-07,0.000186696,3.42E-08,2.65E-07,3.10E-06,3.47E-05,4.35E-06,1.27E-06,0.995339817,1.07E-07,0.002110169,1.23E-05
73,text_generation4,98,Proposed Oracle Experiment .,,,text_generation,4,0,1,0,,0.043828708,0,negative,2.16E-05,0.000205256,2.55E-05,1.73E-07,2.57E-07,4.04E-05,2.42E-05,0.001206843,0.000158099,0.990041509,0.00788976,0.000384789,1.66E-06
74,text_generation4,99,The performance of this approach in generating sentences with the writing style of one certain author can be measured through an experiment with the help of a set of people .,Proposed Oracle Experiment .,Proposed Oracle Experiment .,text_generation,4,1,1,0,,6.55E-05,0,negative,0.00010414,5.17E-06,5.14E-05,2.53E-07,2.84E-07,7.63E-06,1.20E-05,0.000141778,1.45E-06,0.99511274,5.49E-05,0.004501641,6.66E-06
75,text_generation4,100,This set is familiar with the writing style but has n't read all the works of the author .,Proposed Oracle Experiment .,Proposed Oracle Experiment .,text_generation,4,2,1,0,,1.04E-05,0,negative,0.000108296,3.72E-07,9.84E-06,1.13E-05,2.12E-06,5.43E-05,6.77E-06,9.69E-05,1.04E-06,0.999600749,6.88E-07,0.000102275,5.37E-06
76,text_generation4,101,This prevents them from being certain whether a sentence in question has or has not occurred in any work of the said author .,Proposed Oracle Experiment .,Proposed Oracle Experiment .,text_generation,4,3,1,0,,1.01E-05,0,negative,1.17E-05,3.79E-07,2.00E-06,1.25E-07,4.70E-08,5.43E-06,6.18E-07,7.48E-05,1.29E-06,0.999855998,1.60E-06,4.55E-05,5.53E-07
77,text_generation4,102,The generated samples mixed with real sentences are chosen from a random pool and form the evaluation set given to such an audience who marks if the sentences are fake or not .,Proposed Oracle Experiment .,Proposed Oracle Experiment .,text_generation,4,4,1,0,,0.000203606,0,negative,1.58E-05,5.47E-06,4.86E-06,1.68E-07,3.27E-07,9.05E-05,6.77E-06,0.004380151,2.70E-06,0.99541963,2.36E-07,7.21E-05,1.26E-06
78,text_generation4,103,This data run through a scoring metric would determine a model 's performance .,Proposed Oracle Experiment .,Proposed Oracle Experiment .,text_generation,4,5,1,0,,1.20E-05,0,negative,2.02E-05,7.68E-07,1.19E-05,1.22E-07,1.23E-07,6.00E-06,1.24E-06,0.000102714,2.86E-06,0.999734535,6.92E-07,0.000117363,1.48E-06
79,text_generation4,104,An experiment along similar lines is currently being carried out and will be included in the future revision .,Proposed Oracle Experiment .,Proposed Oracle Experiment .,text_generation,4,6,1,0,,1.96E-05,0,negative,0.000126467,2.11E-06,2.56E-05,7.35E-07,1.81E-07,7.80E-05,1.57E-05,0.001022196,7.27E-06,0.997867043,2.84E-06,0.000839691,1.21E-05
80,text_generation4,105,Further Work,Proposed Oracle Experiment .,,text_generation,4,7,1,0,,5.58E-05,0,negative,7.84E-05,2.10E-06,7.60E-06,3.27E-07,1.04E-07,8.26E-05,1.29E-05,0.001353942,5.31E-06,0.997656135,4.96E-06,0.000787074,8.53E-06
81,text_generation4,106,Another performance metric that can be computed for this setup has been described in which is a parallel work to this .,Proposed Oracle Experiment .,Further Work,text_generation,4,8,1,0,,2.59E-06,0,negative,1.06E-05,1.44E-06,1.28E-05,3.31E-08,6.30E-08,1.43E-05,2.72E-06,3.68E-05,1.63E-06,0.999867415,1.43E-05,3.77E-05,3.03E-07
82,text_generation4,107,Simple CFG 4 and more complex ones like Penn Treebank CFG generate samples which are used as input to GAN and the model is evaluated by computing the diversity and accuracy of generated samples conforming to the given CFG .,Proposed Oracle Experiment .,Further Work,text_generation,4,9,1,0,,8.61E-05,0,negative,5.74E-06,2.38E-05,0.000285662,2.06E-07,4.25E-07,7.68E-05,1.48E-05,0.000332004,1.52E-05,0.999068717,0.00014991,2.32E-05,3.55E-06
83,text_generation4,108,Skip - Thought sentence embeddings can be used to generate images with GANs conditioned on text vectors for text - to - image conversion tasks like those achieved in .,Proposed Oracle Experiment .,Further Work,text_generation,4,10,1,0,,0.004781247,0,negative,0.000133421,6.12E-05,0.001530242,4.97E-07,7.77E-07,8.73E-05,5.79E-05,0.000225003,6.96E-05,0.996548835,0.00036826,0.000906105,1.09E-05
84,text_generation4,109,These embeddings have also been used to Models like neuralstoryteller 5 which use these sentence embeddings can be experimented with generative adversarial networks to generate unique samples .,Proposed Oracle Experiment .,Further Work,text_generation,4,11,1,0,,3.98E-05,0,negative,2.77E-05,1.02E-05,8.98E-05,1.01E-06,6.98E-07,0.000215635,1.78E-05,0.000335921,1.83E-05,0.999203941,4.25E-05,3.03E-05,6.09E-06
85,sentiment_analysis16,1,title,,,sentiment_analysis,16,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
86,sentiment_analysis16,2,Target - Sensitive Memory Networks for Aspect Sentiment Classification,title,,sentiment_analysis,16,1,1,1,research-problem,0.99871928,1,research-problem,3.11E-08,6.32E-06,7.48E-08,6.81E-08,4.56E-08,6.74E-08,1.03E-06,9.90E-07,6.09E-07,0.00144237,0.998548187,1.53E-07,5.27E-08
87,sentiment_analysis16,3,abstract,,,sentiment_analysis,16,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
88,sentiment_analysis16,4,Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .,abstract,abstract,sentiment_analysis,16,1,1,1,research-problem,0.958831449,1,research-problem,5.06E-08,6.83E-06,4.62E-08,1.47E-06,4.24E-07,1.98E-07,9.57E-07,7.99E-07,2.36E-07,0.003808424,0.996180423,4.71E-08,1.01E-07
89,sentiment_analysis16,5,"Given an aspect / target and a sentence , the task classifies the sentiment polarity expressed on the target in the sentence .",abstract,abstract,sentiment_analysis,16,2,1,0,,0.234614225,0,research-problem,7.71E-07,0.005020876,2.15E-06,3.94E-06,3.69E-05,2.72E-06,1.45E-06,3.81E-05,0.000259144,0.247476961,0.747156088,5.53E-07,2.84E-07
90,sentiment_analysis16,6,Memory networks ( MNs ) have been used for this task recently and have achieved state - of - the - art results .,abstract,abstract,sentiment_analysis,16,3,1,0,,0.704707789,1,research-problem,5.18E-08,3.58E-05,1.19E-07,3.78E-07,2.58E-07,9.91E-07,5.13E-07,8.31E-06,1.72E-05,0.039377765,0.960558454,3.28E-08,6.11E-08
91,sentiment_analysis16,7,"In MNs , attention mechanism plays a crucial role in detecting the sentiment context for the given target .",abstract,abstract,sentiment_analysis,16,4,1,0,,0.698067331,1,research-problem,2.87E-07,0.000193552,2.34E-07,1.18E-06,1.45E-06,8.57E-07,6.39E-07,1.06E-05,1.29E-05,0.062333671,0.937444392,1.49E-07,1.03E-07
92,sentiment_analysis16,8,"However , we found an important problem with the current MNs in performing the ASC task .",abstract,abstract,sentiment_analysis,16,5,1,1,research-problem,0.12183605,0,research-problem,1.81E-06,0.000166455,3.05E-07,2.06E-06,6.44E-06,2.34E-06,1.80E-06,1.49E-05,7.98E-06,0.411222898,0.58857174,1.13E-06,1.31E-07
93,sentiment_analysis16,9,Simply improving the attention mechanism will not solve it .,abstract,abstract,sentiment_analysis,16,6,1,0,,0.000237777,0,negative,2.18E-06,7.25E-05,6.52E-08,1.80E-05,1.98E-05,1.42E-05,9.54E-07,5.10E-05,1.42E-05,0.98030983,0.019496903,2.71E-07,1.00E-07
94,sentiment_analysis16,10,"The problem is referred to as target - sensitive sentiment , which means that the sentiment polarity of the ( detected ) context is dependent on the given target and it can not be inferred from the context alone .",abstract,abstract,sentiment_analysis,16,7,1,0,,0.012262777,0,research-problem,4.30E-07,0.000626895,2.51E-07,2.47E-05,2.16E-05,6.51E-06,1.32E-06,3.61E-05,4.94E-05,0.380859722,0.618372652,1.68E-07,2.98E-07
95,sentiment_analysis16,11,"To tackle this problem , we propose the targetsensitive memory networks ( TMNs ) .",abstract,abstract,sentiment_analysis,16,8,1,0,,0.605414302,1,research-problem,1.75E-05,0.022187424,0.000255536,8.95E-06,8.22E-05,2.40E-05,1.61E-05,0.000157839,0.010071669,0.218924703,0.748246678,5.10E-06,2.37E-06
96,sentiment_analysis16,12,Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated .,abstract,abstract,sentiment_analysis,16,9,1,0,,0.012969703,0,negative,6.04E-06,0.011344701,6.32E-06,4.67E-06,6.30E-05,1.08E-05,5.23E-06,0.000134931,0.000361668,0.698613736,0.28944489,3.57E-06,4.49E-07
97,sentiment_analysis16,13,Introduction,,,sentiment_analysis,16,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
98,sentiment_analysis16,14,Aspect sentiment classification ( ASC ) is a core problem of sentiment analysis .,Introduction,Introduction,sentiment_analysis,16,1,1,0,,0.939322668,1,research-problem,2.32E-06,0.000307885,1.42E-06,1.43E-05,3.27E-05,5.39E-06,2.94E-05,5.92E-06,5.79E-05,0.019720687,0.979814835,2.94E-06,4.29E-06
99,sentiment_analysis16,15,"Given an aspect and a sentence containing the aspect , ASC classifies the sentiment polarity expressed in the sentence about the aspect , namely , positive , neutral , or negative .",Introduction,Introduction,sentiment_analysis,16,2,1,0,,0.903514912,1,model,3.42E-05,0.250562934,0.000203443,1.27E-06,0.000384504,2.48E-05,1.96E-05,7.39E-05,0.683746535,0.054025778,0.010912321,8.98E-06,1.75E-06
100,sentiment_analysis16,16,"Aspects are also called opinion targets ( or simply targets ) , which are usually product / service features in customer reviews .",Introduction,Introduction,sentiment_analysis,16,3,1,0,,0.464713314,0,negative,1.32E-05,0.020602319,2.04E-05,2.19E-05,0.000844809,0.000140469,7.17E-05,0.000178524,0.019442018,0.699251508,0.259390744,1.47E-05,7.72E-06
101,sentiment_analysis16,17,"In this paper , we use aspect and target interchangeably .",Introduction,Introduction,sentiment_analysis,16,4,1,0,,0.075733454,0,negative,8.03E-06,0.137397712,1.33E-05,1.59E-05,0.000443546,0.000579977,6.15E-05,0.001563375,0.319843048,0.498259067,0.041802682,5.90E-06,5.92E-06
102,sentiment_analysis16,18,"In practice , aspects can be specified by the user or extracted automatically using an aspect extraction technique .",Introduction,Introduction,sentiment_analysis,16,5,1,0,,0.502472204,1,negative,1.04E-05,0.004065272,2.15E-06,8.08E-05,0.000399663,0.000164737,2.81E-05,0.000160024,0.002250804,0.781148494,0.21168025,4.74E-06,4.66E-06
103,sentiment_analysis16,19,"In this work , we assume the aspect terms are given and only focus on the classification task .",Introduction,Introduction,sentiment_analysis,16,6,1,0,,0.105687401,0,approach,2.72E-05,0.557509818,3.13E-05,6.19E-06,0.000494703,6.34E-05,2.49E-05,0.000338808,0.318881819,0.106867739,0.01573984,1.13E-05,2.94E-06
104,sentiment_analysis16,20,"Due to their impressive results in many NLP tasks , neural networks have been applied to ASC ( see the survey ) .",Introduction,Introduction,sentiment_analysis,16,7,1,0,,0.392295426,0,research-problem,1.73E-06,0.000661775,8.10E-07,2.20E-06,9.17E-06,1.79E-05,2.25E-05,3.22E-05,0.0004972,0.172867685,0.825882652,2.54E-06,1.66E-06
105,sentiment_analysis16,21,"Memory networks ( MNs ) , a type of neural networks which were first proposed for question answering , have achieved the state - of - the - art results in ASC .",Introduction,Introduction,sentiment_analysis,16,8,1,0,,0.900929112,1,research-problem,2.61E-06,0.002074746,7.11E-06,1.96E-06,1.08E-05,1.29E-05,2.79E-05,2.84E-05,0.00539149,0.079074407,0.913361554,3.28E-06,2.95E-06
106,sentiment_analysis16,22,A key factor for their success is the attention mechanism .,Introduction,Introduction,sentiment_analysis,16,9,1,0,,0.00325852,0,negative,1.33E-05,0.001394095,2.05E-06,1.75E-05,0.000201082,6.16E-05,2.03E-05,4.97E-05,0.001260339,0.937548371,0.059424579,5.49E-06,1.56E-06
107,sentiment_analysis16,23,"However , we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling can not solve it .",Introduction,Introduction,sentiment_analysis,16,10,1,0,,0.237171069,0,negative,0.000110081,0.005540237,4.50E-06,2.65E-05,0.000531752,3.84E-05,7.60E-05,4.68E-05,0.000554937,0.795332346,0.197683237,5.10E-05,4.24E-06
108,sentiment_analysis16,24,"That is , their performance degrades when the sentiment of a context word is sensitive to the given target .",Introduction,Introduction,sentiment_analysis,16,11,1,0,,0.008499368,0,negative,5.05E-05,0.003363376,6.23E-06,6.96E-05,0.00072583,7.35E-05,3.35E-05,5.66E-05,0.000839005,0.932006525,0.062756499,1.53E-05,3.55E-06
109,sentiment_analysis16,25,Let us consider the following sentences :,Introduction,Introduction,sentiment_analysis,16,12,1,0,,0.00089934,0,negative,2.33E-06,0.000780118,7.35E-07,1.42E-05,0.000207718,6.92E-05,1.43E-05,4.87E-05,0.001106954,0.987070489,0.010682254,2.03E-06,1.00E-06
110,sentiment_analysis16,26,( 1 ) The screen resolution is excellent but the price is ridiculous .,Introduction,Introduction,sentiment_analysis,16,13,1,0,,0.111415862,0,negative,0.001500297,0.018213,0.000172444,0.000490694,0.018397887,0.000400067,0.000247636,0.000100058,0.003159819,0.952850054,0.00430072,0.000153789,1.35E-05
111,sentiment_analysis16,27,( 2 ) The screen resolution is excellent but the price is high .,Introduction,Introduction,sentiment_analysis,16,14,1,0,,0.042145496,0,negative,0.001125128,0.014489723,0.000158906,0.000195099,0.009567036,0.000356165,0.000211056,0.000108879,0.003749911,0.965954567,0.003928636,0.0001446,1.03E-05
112,sentiment_analysis16,28,( 3 ) The price is high .,Introduction,Introduction,sentiment_analysis,16,15,1,0,,0.004561295,0,negative,4.59E-05,0.003529447,6.56E-06,8.84E-06,0.000384956,0.000105016,4.14E-05,0.000113725,0.003702579,0.987572532,0.00446827,1.89E-05,1.90E-06
113,sentiment_analysis16,29,( 4 ) The screen resolution is high .,Introduction,Introduction,sentiment_analysis,16,16,1,0,,0.157263528,0,negative,0.000897189,0.038807298,0.000189129,0.000137736,0.006858157,0.000616965,0.000345143,0.000391875,0.014814911,0.929505767,0.007227896,0.000185001,2.29E-05
114,sentiment_analysis16,30,"In sentence ( 1 ) , the sentiment expressed on aspect screen resolution ( or resolution for short ) is positive , whereas the sentiment on aspect price is negative .",Introduction,Introduction,sentiment_analysis,16,17,1,0,,0.024131256,0,negative,9.81E-06,0.006455304,5.89E-06,4.69E-05,0.006830059,9.60E-05,5.92E-05,6.43E-05,0.001560919,0.97549968,0.0093546,1.33E-05,4.02E-06
115,sentiment_analysis16,31,"For the sake of predicting correct sentiment , a crucial step is to first detect the sentiment context about the given aspect / target .",Introduction,Introduction,sentiment_analysis,16,18,1,0,,0.497356313,0,negative,0.000198267,0.088450707,3.25E-05,0.000251817,0.007288788,0.000226173,0.000107236,0.000304967,0.042697151,0.829930155,0.030438819,5.41E-05,1.93E-05
116,sentiment_analysis16,32,We call this step targeted - context detection .,Introduction,Introduction,sentiment_analysis,16,19,1,0,,0.897459495,1,model,0.000160323,0.28609235,0.000402576,2.40E-05,0.006056754,0.000102281,9.85E-05,8.86E-05,0.488005415,0.214544507,0.00437045,4.46E-05,9.59E-06
117,sentiment_analysis16,33,Memory networks ( MNs ) can deal with this step quite well because the sentiment context of a given aspect can be captured by the internal attention mechanism in MNs .,Introduction,Introduction,sentiment_analysis,16,20,1,0,,0.680391309,1,research-problem,7.87E-06,0.006926471,9.80E-06,8.87E-06,5.30E-05,6.99E-05,8.28E-05,0.000121256,0.013835762,0.395479418,0.583385305,1.07E-05,8.72E-06
118,sentiment_analysis16,34,"Concretely , in sentence ( 1 ) the word "" excellent "" can be identified as the sentiment context when resolution is specified .",Introduction,Introduction,sentiment_analysis,16,21,1,0,,0.011581969,0,negative,7.32E-06,0.004408003,3.48E-06,7.90E-05,0.004359387,0.00021475,3.79E-05,0.000142444,0.002779569,0.986716474,0.001242457,5.46E-06,3.68E-06
119,sentiment_analysis16,35,"Likewise , the context word "" ridiculous "" will be placed with a high attention when price is the target .",Introduction,Introduction,sentiment_analysis,16,22,1,0,,0.002631802,0,negative,6.78E-05,0.046103781,4.00E-05,5.91E-06,0.001079807,0.000177248,6.75E-05,0.000320168,0.167037041,0.78443615,0.000638573,2.33E-05,2.77E-06
120,sentiment_analysis16,36,"With the correct targeted - context detected , a trained MN , which recognizes "" excellent "" as positive sentiment and "" ridiculous "" as negative sentiment , will infer correct sentiment polarity for the given target .",Introduction,Introduction,sentiment_analysis,16,23,1,0,,0.171885131,0,model,9.68E-05,0.229069922,8.55E-05,1.96E-06,0.001142032,3.63E-05,7.18E-05,8.72E-05,0.564100594,0.203915556,0.001343789,4.61E-05,2.51E-06
121,sentiment_analysis16,37,"This is relatively easy as "" excellent "" and "" ridiculous "" are both target - independent sentiment words , i.e. , the words themselves already indicate clear sentiments .",Introduction,Introduction,sentiment_analysis,16,24,1,0,,0.00751003,0,negative,2.12E-05,0.003665606,3.42E-06,4.95E-05,0.001694596,0.000178159,3.14E-05,0.00010785,0.002180265,0.991236647,0.000821693,7.42E-06,2.22E-06
122,sentiment_analysis16,38,"As illustrated above , the attention mechanism addressing the targeted - context detection problem is very useful for ASC , and it helps classify many sentences like sentence ( 1 ) accurately .",Introduction,Introduction,sentiment_analysis,16,25,1,0,,0.804948866,1,negative,0.012053276,0.15370956,0.000216421,4.58E-05,0.003304677,0.000348774,0.005465143,0.00076341,0.056892151,0.75483295,0.007774803,0.004511566,8.15E-05
123,sentiment_analysis16,39,This also led to existing and potential research in improving attention modeling ( discussed in Section 5 ) .,Introduction,Introduction,sentiment_analysis,16,26,1,0,,0.023212837,0,negative,1.27E-05,0.002509841,4.46E-06,1.59E-05,0.000289074,8.69E-05,5.45E-05,6.78E-05,0.001599639,0.983481301,0.01186389,1.15E-05,2.55E-06
124,sentiment_analysis16,40,"However , we observed that simply focusing on tackling the target - context detection problem and learning better attention are not sufficient to solve the problem found in sentences ( 2 ) , ( 3 ) and ( 4 ) .",Introduction,Introduction,sentiment_analysis,16,27,1,0,,0.036325127,0,negative,7.37E-05,0.00469924,3.20E-06,8.45E-06,0.000842084,6.63E-05,0.00011527,7.14E-05,0.0011613,0.989799516,0.003089245,6.78E-05,2.43E-06
125,sentiment_analysis16,41,"Sentence ( 2 ) is similar to sentence ( 1 ) except that the ( sentiment ) context modifying aspect / target price is "" high "" .",Introduction,Introduction,sentiment_analysis,16,28,1,0,,0.015703602,0,negative,1.30E-05,0.012911535,1.67E-05,3.62E-05,0.012914845,0.000322215,0.000184359,0.000212917,0.004130309,0.968502826,0.000726556,2.05E-05,7.99E-06
126,sentiment_analysis16,42,"In this case , when "" high "" is assigned the correct attention for the aspect price , the model also needs to capture the sentiment interaction between "" high "" and price in order to identify the correct sentiment polarity .",Introduction,Introduction,sentiment_analysis,16,29,1,0,,0.026486263,0,model,4.22E-05,0.075771643,3.53E-05,1.08E-06,0.00023,3.06E-05,2.20E-05,7.28E-05,0.676395933,0.246743789,0.000640461,1.30E-05,1.32E-06
127,sentiment_analysis16,43,"This is not as easy as sentence ( 1 ) because "" high "" itself indicates no clear sentiment .",Introduction,Introduction,sentiment_analysis,16,30,1,0,,0.001440859,0,negative,5.18E-06,0.000498162,7.56E-07,2.70E-05,0.000384604,0.000107327,3.22E-05,6.20E-05,0.000276948,0.997380785,0.001219352,3.86E-06,1.71E-06
128,sentiment_analysis16,44,"Instead , its sentiment polarity is dependent on the given target .",Introduction,Introduction,sentiment_analysis,16,31,1,0,,0.006689129,0,negative,2.93E-05,0.032273067,2.41E-05,7.67E-06,0.001133768,5.66E-05,2.48E-05,8.84E-05,0.061196972,0.904170259,0.000981438,1.21E-05,1.64E-06
129,sentiment_analysis16,45,"Looking at sentences ( 3 ) and ( 4 ) , we further see the importance of this problem and also why relying on attention mechanism alone is insufficient .",Introduction,Introduction,sentiment_analysis,16,32,1,0,,0.007417185,0,negative,0.000121279,0.003854917,5.25E-06,8.57E-06,0.001350432,6.46E-05,8.85E-05,4.73E-05,0.001202062,0.992876296,0.000331196,4.81E-05,1.49E-06
130,sentiment_analysis16,46,"In these two sentences , sentiment contexts are both "" high "" ( i.e. , same attention ) , but sentence ( 3 ) is negative and sentence ( 4 ) is positive simply because their target aspects are different .",Introduction,Introduction,sentiment_analysis,16,33,1,0,,0.007857537,0,negative,5.88E-05,0.022527832,3.07E-05,1.61E-05,0.024465825,0.000189763,0.000305904,0.000132322,0.004061407,0.947975094,0.000155588,7.59E-05,4.73E-06
131,sentiment_analysis16,47,"Therefore , focusing on improving attention will not help in these cases .",Introduction,Introduction,sentiment_analysis,16,34,1,0,,0.002218041,0,negative,7.97E-05,0.001373812,2.68E-06,2.92E-05,0.001022381,0.000117292,6.46E-05,9.80E-05,0.000633693,0.996389972,0.00016472,2.21E-05,1.86E-06
132,sentiment_analysis16,48,We will give a theoretical insight about this problem with MNs in Section 3 .,Introduction,Introduction,sentiment_analysis,16,35,1,0,,0.006671166,0,negative,1.14E-05,0.010478445,6.23E-06,1.22E-05,0.000800305,0.00017572,5.17E-05,0.000213667,0.012111787,0.975784927,0.00034204,8.51E-06,2.99E-06
133,sentiment_analysis16,49,"In this work , we aim to solve this problem .",Introduction,Introduction,sentiment_analysis,16,36,1,0,,0.027219053,0,negative,2.26E-05,0.137682216,2.92E-05,0.000102424,0.00218366,0.000494258,0.000190322,0.001014756,0.07036678,0.773837925,0.014028642,2.42E-05,2.31E-05
134,sentiment_analysis16,50,"To distinguish it from the aforementioned targetedcontext detection problem as shown by sentence ( 1 ) , we refer to the problem in ( 2 ) , ( 3 ) and ( 4 ) as the target - sensitive sentiment ( or target - dependent sentiment ) problem , which means that the sentiment polarity of a detected / attended context word is conditioned on the target and can not be directly inferred from the context word alone , unlike "" excellent "" and "" ridiculous "" .",Introduction,Introduction,sentiment_analysis,16,37,1,0,,0.059655368,0,negative,9.98E-06,0.09924774,2.40E-05,1.48E-05,0.001641173,0.000133489,0.000114181,0.000270231,0.040851104,0.848645429,0.009013957,2.57E-05,8.22E-06
135,sentiment_analysis16,51,"To address this problem , we propose target - sensitive memory networks ( TMNs ) , which can capture the sentiment interaction between targets and contexts .",Introduction,Introduction,sentiment_analysis,16,38,1,1,model,0.952852263,1,model,5.62E-05,0.236287569,0.000254957,1.82E-06,0.000505631,4.64E-05,0.000182843,6.07E-05,0.725205584,0.036491752,0.000873965,2.54E-05,7.17E-06
136,sentiment_analysis16,52,We present several approaches to implementing TMNs and experimentally evaluate their effectiveness .,Introduction,Introduction,sentiment_analysis,16,39,1,0,,0.116476284,0,negative,8.04E-05,0.279690115,0.000146965,2.15E-05,0.005473878,0.000251203,0.000690114,0.000319842,0.038327541,0.669791745,0.005055057,0.000129055,2.26E-05
137,sentiment_analysis16,53,Memory Network for ASC,Introduction,,sentiment_analysis,16,40,1,0,,0.398248848,0,model,1.92E-05,0.034347357,0.000236115,3.62E-06,0.000120118,0.000358999,0.001533388,0.000503546,0.611723978,0.322112497,0.028878832,9.16E-05,7.08E-05
138,sentiment_analysis16,54,"This section describes our basic memory network for ASC , also as a background knowledge .",Introduction,Memory Network for ASC,sentiment_analysis,16,41,1,0,,3.25E-05,0,negative,2.06E-05,2.30E-05,1.60E-05,2.59E-07,2.48E-06,2.28E-06,2.13E-06,1.04E-05,0.000632254,0.999218913,1.28E-06,6.82E-05,2.18E-06
139,sentiment_analysis16,55,"It does not include the proposed target - sensitive sentiment solutions , which are introduced in Section 4 .",Introduction,Memory Network for ASC,sentiment_analysis,16,42,1,0,,1.12E-05,0,negative,3.17E-05,4.99E-06,3.34E-05,8.70E-09,3.03E-07,2.77E-07,9.38E-07,1.24E-06,3.67E-05,0.999779532,1.79E-07,0.00011053,1.77E-07
140,sentiment_analysis16,56,The model design follows previous studies except that a different attention alignment function is used ( shown in Eq. 1 ) .,Introduction,Memory Network for ASC,sentiment_analysis,16,43,1,0,,2.61E-05,0,negative,9.61E-06,2.92E-05,8.33E-06,1.22E-07,8.01E-07,3.82E-06,2.99E-06,3.65E-05,0.000771183,0.999121935,3.23E-07,1.40E-05,1.16E-06
141,sentiment_analysis16,57,Their original models will be compared in our experiments as well .,Introduction,Memory Network for ASC,sentiment_analysis,16,44,1,0,,7.11E-06,0,negative,4.69E-07,9.25E-08,4.04E-07,7.48E-10,1.56E-08,1.14E-07,1.93E-07,7.87E-07,1.29E-06,0.999988666,4.18E-09,7.95E-06,1.19E-08
142,sentiment_analysis16,58,"The definitions of related notations are given in . Input Representation : Given a target aspect t , an embedding matrix A is used to convert t into a vector representation , v t ( v t = At ) .",Introduction,Memory Network for ASC,sentiment_analysis,16,45,1,0,,2.57E-05,0,negative,1.15E-06,3.31E-06,1.81E-06,3.04E-08,1.04E-07,9.92E-07,5.99E-07,1.11E-05,0.000161921,0.999813359,1.96E-07,4.95E-06,4.43E-07
143,sentiment_analysis16,59,"Similarly , each context word ( non - aspect word in a sentence ) x i ? {x 1 , x 2 , ...x n } is also projected to the continuous space stored in memory , denoted by mi ( m i = Ax i ) ? {m 1 , m 2 , ...m n }.",Introduction,Memory Network for ASC,sentiment_analysis,16,46,1,0,,3.11E-06,0,negative,1.48E-06,1.19E-06,1.84E-06,2.75E-09,4.24E-08,1.19E-07,1.34E-07,1.18E-06,7.09E-05,0.999918803,2.77E-08,4.26E-06,4.78E-08
144,sentiment_analysis16,60,Here n is the number of words in a sentence and i is the word position / index .,Introduction,Memory Network for ASC,sentiment_analysis,16,47,1,0,,2.47E-06,0,negative,8.72E-07,5.38E-07,2.62E-07,9.51E-09,4.19E-08,5.02E-07,3.02E-07,6.56E-06,1.81E-05,0.999969222,3.35E-08,3.41E-06,1.01E-07
145,sentiment_analysis16,61,Both t and xi are one - hot vectors .,Introduction,Memory Network for ASC,sentiment_analysis,16,48,1,0,,5.17E-05,0,negative,2.12E-06,2.08E-06,1.03E-06,5.79E-09,5.85E-08,1.15E-06,8.60E-07,2.14E-05,9.10E-05,0.999875778,1.50E-08,4.40E-06,1.44E-07
146,sentiment_analysis16,62,"For an aspect expression with multiple words , its aspect representation v t is the averaged vector of those words .",Introduction,Memory Network for ASC,sentiment_analysis,16,49,1,0,,6.39E-06,0,negative,9.99E-07,7.52E-07,1.23E-06,1.84E-09,2.97E-08,1.42E-07,1.61E-07,1.41E-06,2.82E-05,0.999960041,3.54E-08,6.92E-06,5.92E-08
147,sentiment_analysis16,63,Attention : Attention can be obtained based on the above input representation .,Introduction,Memory Network for ASC,sentiment_analysis,16,50,1,0,,0.001299067,0,negative,1.05E-05,4.35E-06,6.14E-05,2.24E-08,1.81E-07,9.31E-07,2.63E-06,5.35E-06,0.000447817,0.999416097,4.00E-07,4.85E-05,1.81E-06
148,sentiment_analysis16,64,"Specifically , an attention weight ?",Introduction,Memory Network for ASC,sentiment_analysis,16,51,1,0,,0.002140688,0,negative,1.74E-06,2.26E-07,5.81E-07,2.85E-09,2.04E-08,1.58E-07,2.58E-07,1.37E-06,1.10E-05,0.999974639,2.08E-08,9.90E-06,6.98E-08
149,sentiment_analysis16,65,i for the context word xi is computed based on the alignment function :,Introduction,Memory Network for ASC,sentiment_analysis,16,52,1,0,,3.26E-06,0,negative,1.10E-05,8.08E-07,5.43E-06,4.54E-09,1.10E-07,1.68E-07,2.96E-07,8.44E-07,1.66E-05,0.999922276,1.38E-08,4.24E-05,7.17E-08
150,sentiment_analysis16,66,where M ?,Introduction,Memory Network for ASC,sentiment_analysis,16,53,1,0,,1.42E-06,0,negative,8.63E-07,6.45E-08,1.34E-07,1.33E-09,9.05E-09,1.40E-07,1.89E-07,9.73E-07,1.79E-06,0.99998882,7.74E-09,6.98E-06,2.80E-08
151,sentiment_analysis16,67,R dd is the general learning matrix suggested by .,Introduction,Memory Network for ASC,sentiment_analysis,16,54,1,0,,1.30E-05,0,negative,4.47E-06,1.37E-06,7.22E-07,3.08E-08,1.68E-07,1.29E-06,1.02E-06,2.43E-05,1.85E-05,0.999938462,1.31E-08,9.26E-06,3.36E-07
152,sentiment_analysis16,68,"In this manner , attention ? = {? 1 , ? 2 , ..? n } is represented as a vector of probabilities , indicating the weight / importance of context words towards a given target .",Introduction,Memory Network for ASC,sentiment_analysis,16,55,1,0,,7.99E-06,0,negative,1.22E-06,2.29E-06,1.26E-06,4.89E-09,6.09E-08,2.84E-07,2.82E-07,4.43E-06,0.000224984,0.999761876,1.89E-08,3.13E-06,1.64E-07
153,sentiment_analysis16,69,Note that ? i ?,Introduction,Memory Network for ASC,sentiment_analysis,16,56,1,0,,1.08E-06,0,negative,1.37E-06,5.29E-08,1.53E-07,1.21E-09,1.07E-08,9.48E-08,1.32E-07,6.40E-07,1.31E-06,0.999987401,2.40E-09,8.81E-06,1.84E-08
154,sentiment_analysis16,70,"( 0 , 1 ) and",Introduction,Memory Network for ASC,sentiment_analysis,16,57,1,0,,2.59E-05,0,negative,1.53E-06,6.38E-07,1.18E-06,4.92E-09,5.56E-08,1.07E-06,1.57E-06,1.34E-05,1.49E-05,0.999951867,8.15E-09,1.34E-05,2.86E-07
155,sentiment_analysis16,71,Output Representation :,Introduction,Memory Network for ASC,sentiment_analysis,16,58,1,0,,0.000158378,0,negative,1.06E-06,1.03E-06,1.92E-05,8.04E-10,2.20E-08,1.83E-07,7.29E-07,1.24E-06,0.000172712,0.999784868,4.62E-08,1.86E-05,2.59E-07
156,sentiment_analysis16,72,Another embedding matrix C is used for generating the individual ( output ) continuous vector c i ( c i = Cx i ) for each context word x i .,Introduction,Memory Network for ASC,sentiment_analysis,16,59,1,0,,8.65E-06,0,negative,3.67E-06,2.30E-06,1.56E-06,1.05E-08,1.13E-07,5.34E-07,5.17E-07,1.11E-05,0.000131454,0.999845191,7.33E-09,3.28E-06,2.50E-07
157,sentiment_analysis16,73,"A final response / output vector o is produced by summing over these vectors weighted with the attention ? , i.e. , o = i ?",Introduction,Memory Network for ASC,sentiment_analysis,16,60,1,0,,1.02E-05,0,negative,2.46E-06,1.06E-06,3.51E-06,1.49E-09,5.25E-08,1.18E-07,2.72E-07,1.13E-06,7.62E-05,0.999908795,5.21E-09,6.30E-06,8.26E-08
158,sentiment_analysis16,74,i c i . Sentiment Score ( or Logit ) :,Introduction,Memory Network for ASC,sentiment_analysis,16,61,1,0,,1.26E-05,0,negative,1.03E-06,6.39E-07,4.01E-06,3.06E-09,3.72E-08,3.16E-07,7.75E-07,3.15E-06,2.26E-05,0.999953126,2.14E-08,1.40E-05,2.72E-07
159,sentiment_analysis16,75,"The aspect sentiment scores ( also called logits ) for positive , neutral , and negative classes are then calculated , where a sentiment - specific weight matrix W ? R Kd is used .",Introduction,Memory Network for ASC,sentiment_analysis,16,62,1,0,,1.53E-05,0,negative,3.88E-06,1.46E-06,3.37E-06,4.94E-10,3.94E-08,7.94E-08,2.83E-07,8.97E-07,5.95E-05,0.999920519,2.03E-09,9.92E-06,3.06E-08
160,sentiment_analysis16,76,The sentiment scores are represented in a vector s ?,Introduction,Memory Network for ASC,sentiment_analysis,16,63,1,0,,6.11E-06,0,negative,5.46E-07,2.74E-07,3.07E-07,1.03E-09,1.67E-08,1.91E-07,2.55E-07,2.89E-06,1.58E-05,0.99997635,1.95E-09,3.32E-06,6.04E-08
161,sentiment_analysis16,77,"R K1 , where K is the number of ( sentiment ) classes , which is 3 in ASC .",Introduction,Memory Network for ASC,sentiment_analysis,16,64,1,0,,2.02E-05,0,negative,8.57E-05,9.21E-07,5.64E-06,8.16E-09,4.29E-07,6.12E-07,4.79E-06,2.99E-06,3.83E-06,0.999558397,5.88E-09,0.000336118,5.48E-07
162,sentiment_analysis16,78,"The final sentiment probability y is produced with a sof tmax operation , i.e. , y = sof tmax ( s ) .",Introduction,Memory Network for ASC,sentiment_analysis,16,65,1,0,,4.18E-05,0,negative,3.26E-06,1.10E-06,3.31E-06,4.68E-10,3.51E-08,6.42E-08,2.81E-07,8.90E-07,5.68E-05,0.999926159,1.22E-09,8.10E-06,4.33E-08
163,sentiment_analysis16,79,Problem of the above Model for Target - Sensitive Sentiment,Introduction,Memory Network for ASC,sentiment_analysis,16,66,1,0,,0.00043704,0,negative,7.36E-06,6.60E-06,5.58E-06,5.70E-08,1.79E-07,1.56E-06,2.27E-05,1.29E-05,5.41E-05,0.999220611,7.62E-06,0.000643306,1.75E-05
164,sentiment_analysis16,80,This section analyzes the problem of targetsensitive sentiment in the above model .,Introduction,Memory Network for ASC,sentiment_analysis,16,67,1,0,,0.000253245,0,negative,3.27E-05,9.29E-06,1.03E-05,1.21E-08,5.36E-07,2.41E-07,2.29E-06,1.44E-06,3.97E-05,0.999646375,8.49E-08,0.000256252,8.14E-07
165,sentiment_analysis16,81,The analysis can be generalized to many existing MNs as long as their improvements are on attention ?,Introduction,Memory Network for ASC,sentiment_analysis,16,68,1,0,,7.53E-06,0,negative,1.44E-06,3.17E-07,2.05E-07,8.75E-09,3.00E-08,3.83E-07,4.54E-07,3.42E-06,7.40E-06,0.999977491,8.66E-09,8.60E-06,2.45E-07
166,sentiment_analysis16,82,only .,Introduction,Memory Network for ASC,sentiment_analysis,16,69,1,0,,1.90E-05,0,negative,5.21E-06,6.81E-08,1.42E-06,9.29E-09,1.58E-07,3.33E-07,6.20E-07,1.17E-06,1.68E-06,0.999966865,1.23E-09,2.22E-05,2.92E-07
167,sentiment_analysis16,83,We first expand the sentiment score calculation from Eq. 2 to its individual terms :,Introduction,Memory Network for ASC,sentiment_analysis,16,70,1,0,,1.49E-05,0,negative,1.59E-05,1.42E-06,6.84E-06,1.96E-09,8.64E-08,1.11E-07,4.67E-07,9.72E-07,6.32E-05,0.999893451,1.80E-09,1.74E-05,1.41E-07
168,sentiment_analysis16,84,"where "" + "" denotes element - wise summation .",Introduction,Memory Network for ASC,sentiment_analysis,16,71,1,0,,2.69E-06,0,negative,3.81E-07,1.76E-07,3.14E-07,3.68E-09,2.17E-08,3.21E-07,3.00E-07,3.32E-06,8.70E-06,0.999984174,1.67E-09,2.14E-06,1.45E-07
169,sentiment_analysis16,85,"In Eq. 3 , ? i W c i can be viewed as the individual sentiment logit for a context word and W v t is the sentiment logit of an aspect .",Introduction,Memory Network for ASC,sentiment_analysis,16,72,1,0,,2.82E-06,0,negative,8.28E-07,2.50E-07,3.47E-07,2.03E-09,2.52E-08,1.66E-07,1.80E-07,2.94E-06,1.07E-05,0.999981144,9.08E-10,3.29E-06,8.70E-08
170,sentiment_analysis16,86,They are linearly combined to determine the final sentiment score s .,Introduction,Memory Network for ASC,sentiment_analysis,16,73,1,0,,9.09E-06,0,negative,5.98E-06,1.67E-06,2.36E-06,2.07E-09,8.05E-08,9.67E-08,3.25E-07,1.42E-06,0.000105792,0.999873899,1.06E-09,8.23E-06,1.39E-07
171,sentiment_analysis16,87,This can be problematic in ASC .,Introduction,Memory Network for ASC,sentiment_analysis,16,74,1,0,,1.75E-06,0,negative,2.87E-06,3.62E-08,1.92E-07,2.45E-09,1.67E-08,9.89E-08,1.44E-07,4.23E-07,6.68E-07,0.99998248,1.69E-09,1.30E-05,6.19E-08
172,sentiment_analysis16,88,"First , an aspect word often expresses no sentiment , for example , "" screen "" .",Introduction,Memory Network for ASC,sentiment_analysis,16,75,1,0,,3.44E-06,0,negative,3.16E-06,6.25E-08,2.81E-07,5.49E-08,2.91E-07,6.13E-07,4.83E-07,1.22E-06,1.82E-07,0.999975953,1.62E-09,1.74E-05,3.27E-07
173,sentiment_analysis16,89,"However , if the aspect term v t is simply removed from Eq. 3 , it also causes the problem that the model can not handle target - dependent sentiment .",Introduction,Memory Network for ASC,sentiment_analysis,16,76,1,0,,3.41E-06,0,negative,2.91E-05,1.39E-07,4.51E-07,1.09E-09,2.70E-08,5.12E-08,2.08E-07,3.18E-07,1.94E-06,0.999932705,7.08E-10,3.50E-05,3.90E-08
174,sentiment_analysis16,90,"For instance , the sentences ( 3 ) and ( 4 ) in Section 1 will then be treated as identical if their aspect words are not considered .",Introduction,Memory Network for ASC,sentiment_analysis,16,77,1,0,,7.58E-07,0,negative,2.79E-07,3.04E-08,1.26E-07,2.21E-10,9.89E-09,2.48E-08,3.54E-08,2.26E-07,1.10E-06,0.999996442,1.07E-10,1.72E-06,9.74E-09
175,sentiment_analysis16,91,"Second , if an aspect word is considered and it directly bears some positive or negative sentiment , then when an aspect word occurs with different context words for expressing opposite sentiments , a contradiction can be resulted from them , especially in the case that the context word is a target - sensitive sentiment word .",Introduction,Memory Network for ASC,sentiment_analysis,16,78,1,0,,3.16E-06,0,negative,2.08E-06,5.33E-08,1.51E-07,1.53E-09,2.15E-08,7.65E-08,1.50E-07,5.91E-07,8.21E-07,0.999985992,7.83E-10,1.00E-05,5.44E-08
176,sentiment_analysis16,92,We explain it as follows .,Introduction,Memory Network for ASC,sentiment_analysis,16,79,1,0,,1.77E-07,0,negative,6.43E-07,3.88E-08,8.78E-08,8.04E-09,6.15E-08,1.92E-07,7.69E-08,8.95E-07,6.50E-07,0.999995587,1.51E-10,1.70E-06,5.59E-08
177,sentiment_analysis16,93,Let us say we have two target words price and resolution ( denoted asp and r) .,Introduction,Memory Network for ASC,sentiment_analysis,16,80,1,0,,1.92E-06,0,negative,1.78E-07,4.76E-08,2.19E-07,1.56E-09,5.01E-08,9.66E-08,1.19E-07,7.97E-07,1.27E-06,0.999994804,2.80E-10,2.31E-06,1.04E-07
178,sentiment_analysis16,94,"We also have two possible context words "" high "" and "" low "" ( denoted ash and l ) .",Introduction,Memory Network for ASC,sentiment_analysis,16,81,1,0,,4.18E-06,0,negative,8.78E-07,3.99E-07,1.01E-06,1.29E-08,2.46E-07,9.18E-07,7.68E-07,1.09E-05,1.09E-05,0.999970228,3.05E-10,3.13E-06,6.56E-07
179,sentiment_analysis16,95,"As these two sentiment words can modify both aspects , we can construct four snippets "" high price "" , "" low price "" , "" high resolution "" and "" low resolution "" .",Introduction,Memory Network for ASC,sentiment_analysis,16,82,1,0,,2.61E-05,0,negative,5.21E-06,1.51E-07,1.05E-06,1.26E-08,1.45E-06,3.76E-07,6.82E-07,1.44E-06,9.59E-07,0.999971947,3.74E-11,1.65E-05,2.29E-07
180,sentiment_analysis16,96,"Their sentiments are negative , positive , positive , and negative respectively .",Introduction,Memory Network for ASC,sentiment_analysis,16,83,1,0,,8.20E-06,0,negative,1.09E-07,2.68E-08,1.26E-07,3.25E-10,3.03E-08,8.46E-08,1.42E-07,1.07E-06,4.75E-07,0.999995976,2.93E-11,1.92E-06,4.45E-08
181,sentiment_analysis16,97,Let us set W to R 1 d so that s becomes a 1 - dimensional sentiment score indicator .,Introduction,Memory Network for ASC,sentiment_analysis,16,84,1,0,,5.17E-06,0,negative,5.00E-06,5.57E-07,2.85E-07,4.24E-09,8.42E-08,7.60E-07,2.04E-06,2.55E-05,5.94E-06,0.999945442,2.06E-10,1.39E-05,4.85E-07
182,sentiment_analysis16,98,s >,Introduction,Memory Network for ASC,sentiment_analysis,16,85,1,0,,1.06E-06,0,negative,1.44E-06,2.77E-08,1.74E-07,1.89E-09,1.72E-08,1.18E-07,2.69E-07,7.89E-07,6.23E-07,0.999988254,1.95E-10,8.16E-06,1.27E-07
183,sentiment_analysis16,99,0 indicates a positive sentiment and s < 0 indicates a negative sentiment .,Introduction,Memory Network for ASC,sentiment_analysis,16,86,1,0,,8.45E-06,0,negative,1.18E-06,4.36E-07,6.18E-07,5.79E-10,2.61E-08,2.22E-07,1.19E-06,3.77E-06,8.61E-06,0.999967429,5.04E-10,1.62E-05,2.88E-07
184,sentiment_analysis16,100,Based on the above example snippets or phrases we have four corresponding inequalities :,Introduction,Memory Network for ASC,sentiment_analysis,16,87,1,0,,1.06E-06,0,negative,6.14E-07,5.65E-08,5.52E-07,6.34E-10,1.56E-08,6.12E-08,1.89E-07,4.71E-07,2.35E-06,0.999988325,3.15E-10,7.26E-06,9.91E-08
185,sentiment_analysis16,101,We can drop all ?,Introduction,Memory Network for ASC,sentiment_analysis,16,88,1,0,,8.15E-07,0,negative,7.27E-07,5.96E-08,1.38E-07,1.95E-08,3.62E-08,9.15E-07,7.12E-07,5.85E-06,1.03E-06,0.999987087,3.88E-10,2.86E-06,5.61E-07
186,sentiment_analysis16,102,"terms here as they all equal to 1 , i.e. , they are the only context word in the snippets to attend to ( the target words are not contexts ) .",Introduction,Memory Network for ASC,sentiment_analysis,16,89,1,0,,1.90E-06,0,negative,2.12E-07,5.58E-08,1.97E-07,2.07E-09,3.19E-08,4.87E-07,6.53E-07,4.86E-06,9.12E-07,0.999988972,6.56E-11,3.36E-06,2.56E-07
187,sentiment_analysis16,103,From ( a ) and ( b ) we can infer,Introduction,Memory Network for ASC,sentiment_analysis,16,90,1,0,,2.05E-06,0,negative,1.67E-06,4.60E-08,2.83E-07,4.97E-10,1.34E-08,4.12E-08,2.15E-07,3.59E-07,1.59E-06,0.999985175,1.37E-10,1.05E-05,8.90E-08
188,sentiment_analysis16,104,This contradiction means that MNs can not learn a set of parameters W and C to correctly classify the above four snippets / sentences at the same time .,Introduction,Memory Network for ASC,sentiment_analysis,16,91,1,0,,1.99E-06,0,negative,8.83E-07,2.81E-08,8.67E-08,3.07E-10,1.05E-08,3.02E-08,1.21E-07,2.48E-07,4.97E-07,0.9999896,9.83E-11,8.45E-06,4.17E-08
189,sentiment_analysis16,105,This contradiction also generalizes to realworld sentences .,Introduction,Memory Network for ASC,sentiment_analysis,16,92,1,0,,1.72E-06,0,negative,4.28E-07,2.31E-08,1.23E-07,6.00E-10,1.33E-08,6.66E-08,1.64E-07,4.09E-07,4.33E-07,0.999994652,1.37E-10,3.61E-06,8.04E-08
190,sentiment_analysis16,106,"That is , although real - world review sentences are usually longer and contain more words , since the attention mechanism makes MNs focus on the most important sentiment context ( the context with high ?",Introduction,Memory Network for ASC,sentiment_analysis,16,93,1,0,,1.53E-05,0,negative,2.54E-06,8.90E-08,3.67E-07,7.23E-10,2.99E-08,4.16E-08,2.41E-07,4.13E-07,5.94E-07,0.999975968,1.65E-10,1.96E-05,9.79E-08
191,sentiment_analysis16,107,"i scores ) , the problem is essentially the same .",Introduction,Memory Network for ASC,sentiment_analysis,16,94,1,0,,6.79E-07,0,negative,8.30E-07,1.09E-08,1.66E-07,5.21E-11,5.83E-09,9.56E-09,8.06E-08,5.63E-08,1.42E-07,0.999987416,1.50E-11,1.13E-05,1.34E-08
192,sentiment_analysis16,108,"For example , in sentences and in Section 1 , when price is targeted , the main attention will be placed on "" high "" .",Introduction,Memory Network for ASC,sentiment_analysis,16,95,1,0,,5.13E-07,0,negative,3.02E-07,1.11E-08,1.59E-07,1.04E-09,2.54E-08,1.03E-07,9.83E-08,3.91E-07,2.99E-07,0.999996891,1.92E-11,1.65E-06,7.29E-08
193,sentiment_analysis16,109,"For MNs , these situations are nearly the same as that for classifying the snippet "" high price "" .",Introduction,Memory Network for ASC,sentiment_analysis,16,96,1,0,,2.17E-06,0,negative,6.20E-07,3.92E-08,2.57E-07,2.43E-09,6.25E-08,1.36E-07,5.66E-07,6.54E-07,1.48E-07,0.999971455,4.13E-10,2.57E-05,3.93E-07
194,sentiment_analysis16,110,We will also show real examples in the experiment section .,Introduction,Memory Network for ASC,sentiment_analysis,16,97,1,0,,8.29E-07,0,negative,4.17E-07,7.90E-09,3.70E-08,1.07E-09,4.50E-08,9.29E-08,2.35E-07,6.03E-07,4.88E-08,0.999993191,3.76E-12,5.26E-06,6.44E-08
195,sentiment_analysis16,111,"One may then ask whether improving attention can help address the problem , as ?",Introduction,Memory Network for ASC,sentiment_analysis,16,98,1,0,,3.59E-07,0,negative,2.30E-07,8.31E-09,6.26E-08,5.03E-10,6.14E-09,5.40E-08,7.04E-08,3.25E-07,2.51E-07,0.999997599,5.00E-11,1.34E-06,4.74E-08
196,sentiment_analysis16,112,i can affect the final results by adjusting the sentiment effect of the context word via ?,Introduction,Memory Network for ASC,sentiment_analysis,16,99,1,0,,1.63E-06,0,negative,6.36E-07,1.90E-08,1.20E-07,3.27E-09,1.67E-08,2.08E-07,2.27E-07,1.27E-06,3.29E-07,0.999993902,4.48E-11,3.01E-06,2.61E-07
197,sentiment_analysis16,113,i W c i .,Introduction,Memory Network for ASC,sentiment_analysis,16,100,1,0,,8.09E-07,0,negative,5.42E-07,1.44E-08,1.76E-07,2.58E-09,1.87E-08,1.59E-07,1.83E-07,8.90E-07,3.13E-07,0.999994854,1.51E-11,2.66E-06,1.86E-07
198,sentiment_analysis16,114,"This is unlikely , if not impossible .",Introduction,Memory Network for ASC,sentiment_analysis,16,101,1,0,,7.01E-07,0,negative,4.97E-07,1.16E-08,7.35E-08,3.12E-09,1.85E-08,2.67E-07,2.58E-07,1.44E-06,2.16E-07,0.999994263,4.42E-11,2.73E-06,2.20E-07
199,sentiment_analysis16,115,"First , notice that ?",Introduction,Memory Network for ASC,sentiment_analysis,16,102,1,0,,4.57E-07,0,negative,6.07E-07,1.08E-08,1.17E-07,1.42E-10,4.60E-09,2.69E-08,1.06E-07,1.75E-07,2.00E-07,0.999995138,1.31E-11,3.59E-06,2.96E-08
200,sentiment_analysis16,116,"i is a scalar ranging in ( 0 , 1 ) , which means it essentially assigns higher or lower weight to increase or decrease the sentiment effect of a context word .",Introduction,Memory Network for ASC,sentiment_analysis,16,103,1,0,,3.75E-06,0,negative,7.14E-07,4.99E-07,4.18E-07,2.01E-09,5.16E-08,5.34E-07,1.10E-06,2.30E-05,1.16E-05,0.999959378,3.29E-11,2.05E-06,6.78E-07
201,sentiment_analysis16,117,"It can not change the intrinsic sentiment orientation / polarity of the context , which is determined by W c i .",Introduction,Memory Network for ASC,sentiment_analysis,16,104,1,0,,2.12E-06,0,negative,1.53E-05,6.29E-08,8.23E-07,2.92E-10,2.46E-08,2.94E-08,4.44E-07,2.03E-07,5.31E-07,0.999925586,2.22E-11,5.69E-05,8.29E-08
202,sentiment_analysis16,118,"For example , if W c i assigns the context word "" high "" a positive sentiment ( W c i > 0 ) , ?",Introduction,Memory Network for ASC,sentiment_analysis,16,105,1,0,,3.46E-07,0,negative,6.31E-07,1.38E-08,3.90E-07,1.62E-11,4.67E-09,7.56E-09,8.98E-08,4.18E-08,2.02E-07,0.999992095,3.88E-12,6.51E-06,1.32E-08
203,sentiment_analysis16,119,"i will not make it negative ( i.e. , ?",Introduction,Memory Network for ASC,sentiment_analysis,16,106,1,0,,2.23E-07,0,negative,1.27E-07,3.80E-09,3.43E-08,2.63E-10,6.66E-09,4.96E-08,6.93E-08,3.24E-07,5.90E-08,0.999998335,3.08E-12,9.53E-07,3.75E-08
204,sentiment_analysis16,120,i W c i < 0 can not be achieved by chang - ing ? i ) .,Introduction,Memory Network for ASC,sentiment_analysis,16,107,1,0,,3.12E-06,0,negative,2.16E-06,2.23E-08,1.99E-07,1.77E-09,2.48E-08,1.25E-07,5.08E-07,8.80E-07,2.59E-07,0.999975925,3.23E-11,1.95E-05,3.81E-07
205,sentiment_analysis16,121,"Second , other irrelevant / unimportant context words often carry no or little sentiment information , so increasing or decreasing their weights does not help .",Introduction,Memory Network for ASC,sentiment_analysis,16,108,1,0,,0.00022007,0,negative,0.00029797,6.31E-08,9.03E-07,8.00E-09,1.94E-07,5.37E-07,1.41E-05,1.88E-06,9.66E-08,0.998829068,2.24E-11,0.000853597,1.60E-06
206,sentiment_analysis16,122,"For example , in the sentence "" the price is high "" , adjusting the weights of context words "" the "" and "" is "" will neither help solve the problem nor be intuitive to do so .",Introduction,Memory Network for ASC,sentiment_analysis,16,109,1,0,,3.66E-07,0,negative,2.52E-07,3.91E-09,5.60E-08,9.15E-10,1.61E-08,1.30E-07,2.05E-07,4.67E-07,4.93E-08,0.999996109,7.31E-12,2.54E-06,1.69E-07
207,sentiment_analysis16,123,The Proposed Approaches,,,sentiment_analysis,16,0,1,0,,0.01895288,0,negative,3.02E-05,0.000374688,1.39E-05,1.68E-06,9.18E-07,0.000130409,4.65E-05,0.002150479,0.000266492,0.977850051,0.01900108,0.000126204,7.43E-06
208,sentiment_analysis16,124,"This section introduces six ( 6 ) alternative targetsensitive memory networks ( TMNs ) , which all can deal with the target - sensitive sentiment problem .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,1,1,0,,0.020130079,0,negative,0.000626933,0.083777582,0.015393646,7.35E-06,0.000184418,0.000137816,9.24E-05,0.001258017,0.008412814,0.881804206,0.006542783,0.001747863,1.42E-05
209,sentiment_analysis16,125,Each of them has its characteristics .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,2,1,0,,4.46E-07,0,negative,5.95E-06,6.65E-05,2.40E-06,1.60E-07,1.02E-06,4.81E-06,2.89E-07,6.34E-05,4.81E-05,0.999793302,8.62E-06,5.35E-06,4.33E-08
210,sentiment_analysis16,126,Non- linear Projection ( NP ) :,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,3,1,0,,0.116043043,0,negative,4.14E-05,0.0108512,0.044722995,3.24E-07,2.53E-06,8.45E-05,3.99E-05,0.001058785,0.003806611,0.921521632,0.017490761,0.000375252,4.10E-06
211,sentiment_analysis16,127,This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,4,1,0,,0.011682228,0,negative,0.000737484,0.043509435,0.00226735,1.32E-05,0.00016229,9.94E-05,3.77E-05,0.000822209,0.002874745,0.94448049,0.004021582,0.000963488,1.07E-05
212,sentiment_analysis16,128,"Instead of directly following the common linear combination as shown in Eq. 3 , we use a non-linear projection ( tanh ) as the replacement to calculate the aspect - specific sentiment score .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,5,1,0,,0.00048654,0,negative,0.00023129,0.027580021,0.002323741,8.59E-07,1.81E-05,5.78E-05,7.48E-06,0.001414924,0.016268669,0.951927371,9.24E-05,7.59E-05,1.45E-06
213,sentiment_analysis16,129,"As shown in Eq. 4 , by applying a non-linear projection over attention - weighted c i and v t , the context and aspect information are coupled in a way that the final sentiment score can not be obtained by simply summing their individual contributions ( compared with Eq. 3 ) .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,6,1,0,,2.32E-05,0,negative,0.000784051,0.00330038,0.000166682,4.52E-07,7.91E-06,1.48E-05,2.30E-06,0.000284007,0.002697838,0.992426375,3.18E-05,0.000282874,5.09E-07
214,sentiment_analysis16,130,This technique is also intuitive in neural networks .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,7,1,0,,1.03E-05,0,negative,6.53E-06,0.000211196,9.83E-06,1.25E-06,1.36E-06,7.13E-05,3.18E-06,0.000621405,0.000150062,0.9985635,0.000350031,9.68E-06,7.02E-07
215,sentiment_analysis16,131,"However , notice that by using the non-linear projection ( or adding more sophisticated hidden layers ) over them in this way , we sacrifice some interpretability .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,8,1,0,,2.38E-06,0,negative,0.0026593,0.000197026,1.23E-05,7.03E-07,4.48E-06,1.32E-05,1.90E-06,0.000118441,3.96E-05,0.996502667,7.43E-06,0.000442727,2.28E-07
216,sentiment_analysis16,132,"For example , we may have difficulty in tracking how each individual context word ( c i ) affects the final sentiment score s , as all context and target representations are coupled .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,9,1,0,,2.43E-06,0,negative,4.64E-06,0.000204092,1.61E-06,2.90E-07,5.14E-07,1.06E-05,9.44E-07,0.000224738,9.30E-05,0.998912848,0.000530684,1.57E-05,2.73E-07
217,sentiment_analysis16,133,"To avoid this , we can use the following five alternative techniques .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,10,1,0,,1.59E-06,0,negative,4.42E-05,0.000142129,5.33E-05,1.86E-07,2.48E-06,1.02E-05,1.01E-06,6.43E-05,5.08E-05,0.999575412,3.80E-06,5.20E-05,9.15E-08
218,sentiment_analysis16,134,Contextual Non-linear Projection ( CNP ) :,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,11,1,0,,0.060585442,0,negative,9.35E-05,0.011284082,0.017802381,9.06E-07,5.97E-06,5.03E-05,6.69E-05,0.000623342,0.003209307,0.931057231,0.033803015,0.00199263,1.04E-05
219,sentiment_analysis16,135,"Despite the fact that it also uses the non-linear projection , this approach incorporates the interplay between a context word and the given target into its ( output ) context representation .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,12,1,0,,3.81E-05,0,negative,7.29E-05,0.004367502,0.000181491,1.78E-06,1.14E-05,3.96E-05,4.42E-06,0.000599581,0.000983243,0.993206535,0.000460474,6.92E-05,1.90E-06
220,sentiment_analysis16,136,We thus name it Contextual Non-linear Projection ( CNP ) .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,13,1,0,,0.00082291,0,negative,3.94E-05,0.002699131,0.002532697,2.92E-07,5.21E-06,1.69E-05,5.34E-06,0.000170151,0.001373823,0.992808626,0.000261536,8.59E-05,1.03E-06
221,sentiment_analysis16,137,"From Eq. 5 , we can see that this approach can keep the linearity of attention - weighted context aggregation while taking into account the aspect information with non-linear projection , which works in a different way compared to NP .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,14,1,0,,0.000514789,0,negative,0.000827289,0.001396517,6.70E-05,2.61E-07,3.74E-06,1.08E-05,5.39E-06,0.000266913,0.000258974,0.994635675,8.98E-05,0.00243692,6.20E-07
222,sentiment_analysis16,138,If we defin ?,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,15,1,0,,3.96E-07,0,negative,2.18E-06,2.37E-05,4.86E-07,1.62E-07,4.08E-07,6.25E-06,2.44E-07,9.33E-05,2.04E-05,0.999844422,5.64E-06,2.75E-06,5.32E-08
223,sentiment_analysis16,139,"c i = tanh ( c i + v t ) , c i can be viewed as the target - aware context representation of context xi and the final sentiment score is calculated based on the aggregation of suchc i .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,16,1,0,,6.59E-06,0,negative,3.04E-05,0.000677421,6.50E-05,1.41E-07,2.56E-06,1.17E-05,1.01E-06,0.000240742,0.000664634,0.998264296,6.61E-06,3.51E-05,2.49E-07
224,sentiment_analysis16,140,This could be a more reasonable way to carry the aspect information rather than simply summing the aspect representation ( Eq. 3 ) .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,17,1,0,,1.90E-06,0,negative,7.16E-06,6.17E-05,3.36E-06,1.44E-07,5.99E-07,2.16E-05,7.47E-07,0.000243886,0.00016699,0.999481891,3.99E-06,7.76E-06,1.56E-07
225,sentiment_analysis16,141,"However , one potential dis advantage is that this setting uses the same set of vector representations ( learned by embeddings C ) for multiple purposes , i.e. , to learn output ( context ) representations and to capture the interplay between contexts and aspects .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,18,1,0,,1.03E-05,0,negative,7.61E-05,0.000613723,5.49E-05,1.69E-07,4.00E-06,4.86E-06,1.60E-06,6.69E-05,0.000110141,0.998840149,2.78E-05,0.000199379,2.15E-07
226,sentiment_analysis16,142,"This may degenerate its model performance when the computational layers in memory networks ( called "" hops "" ) are deep , because too much information is required to be encoded in such cases and a sole set of vectors may fail to capture all of it .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,19,1,0,,2.93E-06,0,negative,4.71E-05,5.71E-05,3.31E-06,5.25E-07,3.19E-06,7.52E-06,6.13E-07,6.59E-05,2.54E-05,0.999754051,4.85E-06,3.03E-05,1.61E-07
227,sentiment_analysis16,143,"To overcome this , we suggest the involvement of an additional new set of embeddings / vectors , which is exclusively designed for modeling the sentiment interaction between an aspect and its context .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,20,1,0,,0.002726093,0,negative,0.001754296,0.010780889,0.000102639,1.34E-05,0.000120452,9.28E-05,9.63E-06,0.001843795,0.003438324,0.981524112,9.63E-06,0.000305489,4.59E-06
228,sentiment_analysis16,144,"The key idea is to decouple different functioning components with different representations , but still make them work jointly .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,21,1,0,,0.000923955,0,negative,0.00024376,0.008870769,0.000210358,1.18E-06,3.31E-05,1.46E-05,2.13E-06,0.000270868,0.004829831,0.985381919,1.51E-05,0.000125366,1.11E-06
229,sentiment_analysis16,145,The following four techniques are based on this idea .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,22,1,0,,2.34E-06,0,negative,1.61E-05,0.000226491,3.21E-05,2.41E-06,1.72E-05,4.52E-05,4.22E-06,0.000243225,0.000138465,0.999231069,1.73E-05,2.50E-05,1.25E-06
230,sentiment_analysis16,146,Interaction Term ( IT ) :,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,23,1,0,,8.97E-05,0,negative,0.000726913,0.002825991,0.009762047,8.84E-07,2.28E-05,4.36E-05,2.03E-05,0.000334368,0.004638704,0.980733039,2.98E-05,0.000856406,5.17E-06
231,sentiment_analysis16,147,The third approach is to formulate explicit target - context sentiment interaction terms .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,24,1,0,,0.000236165,0,negative,6.92E-05,0.001776522,0.000108514,4.45E-06,2.81E-05,4.53E-05,8.47E-06,0.00041832,0.000227982,0.996962005,0.000253606,9.41E-05,3.42E-06
232,sentiment_analysis16,148,"Different from the targeted - context detection problem which is captured by attention ( discussed in Section 1 ) , here the targetcontext sentiment ( TCS ) interaction measures the sentiment - oriented interaction effect between targets and contexts , which we refer to as TCS interaction ( or sentiment interaction ) for short in the rest of this paper .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,25,1,0,,2.87E-05,0,negative,4.36E-05,0.008081605,0.000250267,1.94E-06,3.81E-05,3.63E-05,9.76E-06,0.000599051,0.00150874,0.989074609,0.00021765,0.000134607,3.84E-06
233,sentiment_analysis16,149,"Such sentiment interaction is captured by a new set of vectors , and we thus also call such vectors TCS vectors .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,26,1,0,,7.19E-06,0,negative,1.24E-05,0.000970505,0.000130512,4.79E-08,4.68E-06,3.20E-06,5.90E-07,5.85E-05,0.001463023,0.997331508,1.91E-06,2.29E-05,1.69E-07
234,sentiment_analysis16,150,"In Eq. 6 , W s ?",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,27,1,0,,1.43E-06,0,negative,2.48E-05,0.000105665,1.01E-05,6.06E-08,1.66E-06,5.80E-06,9.66E-07,0.00010959,3.54E-05,0.999613294,2.12E-06,9.05E-05,1.00E-07
235,sentiment_analysis16,151,R Kd and w I ?,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,28,1,0,,3.33E-06,0,negative,3.41E-05,0.000103929,3.78E-06,1.42E-06,8.37E-06,7.37E-05,6.75E-06,0.001243954,3.85E-05,0.998385901,3.52E-06,9.46E-05,1.43E-06
236,sentiment_analysis16,152,R K1 are used instead of W in Eq .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,29,1,0,,1.03E-06,0,negative,9.08E-05,0.000187442,3.48E-05,1.05E-07,3.76E-06,1.26E-05,1.84E-06,0.000191754,0.000101652,0.999273046,5.62E-07,0.000101472,1.81E-07
237,sentiment_analysis16,153,3 . W s models the direct sentiment effect from c i while w I works with d i and d t together for learning the TCS interaction .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,30,1,0,,0.000330786,0,negative,0.000111687,0.001167808,0.001210353,2.03E-07,8.30E-06,1.48E-05,3.33E-06,0.000206419,0.001772069,0.995387223,2.98E-06,0.000113705,1.06E-06
238,sentiment_analysis16,154,"d i and d tare TCS vector representations of context xi and aspect t , produced from a new embedding matrix D , i.e. , d i = Dx i , d t = Dt ( D ? R d V and d i , d t ?",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,31,1,0,,1.10E-06,0,negative,2.68E-05,0.000291163,3.29E-05,1.59E-07,7.50E-06,9.62E-06,1.75E-06,0.000139452,9.77E-05,0.999314616,1.12E-06,7.70E-05,2.51E-07
239,sentiment_analysis16,155,R d1 ) .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,32,1,0,,7.03E-06,0,negative,6.57E-05,7.17E-05,5.98E-05,2.40E-08,1.48E-06,2.40E-06,1.17E-06,3.48E-05,5.26E-05,0.999508809,8.28E-07,0.000200479,1.16E-07
240,sentiment_analysis16,156,"Unlike input and output embeddings A and C , Dis designed to capture the sentiment interac- tion .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,33,1,0,,3.92E-06,0,negative,0.000112898,0.001604588,0.000161013,3.50E-07,2.56E-05,1.34E-05,3.22E-06,0.000276298,0.000460102,0.997218469,1.15E-06,0.000122363,5.60E-07
241,sentiment_analysis16,157,"The vectors from D affect the final sentiment score through w I d i , d t , where w I is a sentimentspecific vector and d i , d t ?",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,34,1,0,,1.03E-06,0,negative,1.31E-05,0.000126842,1.04E-05,3.79E-08,1.87E-06,3.65E-06,5.15E-07,6.83E-05,7.80E-05,0.999660976,4.16E-07,3.57E-05,9.00E-08
242,sentiment_analysis16,158,R denotes the dot product of the two TCS vectors d i and d t .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,35,1,0,,4.61E-07,0,negative,2.13E-06,5.82E-05,1.99E-06,1.55E-07,1.78E-06,1.15E-05,5.93E-07,0.000201243,6.39E-05,0.999653083,3.90E-07,4.83E-06,1.52E-07
243,sentiment_analysis16,159,"Compared to the basic MNs , this model can better capture target - sensitive sentiment because the interactions between a context word hand different aspect words ( say , p and r) can be different , i.e. , d h , d p = d h , d r .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,36,1,0,,6.26E-05,0,negative,0.000454969,0.006192935,0.000555956,3.87E-07,2.63E-05,1.45E-05,1.93E-05,0.000380387,0.001764796,0.986746713,1.24E-05,0.003828158,3.23E-06
244,sentiment_analysis16,160,The key advantage is that now the sentiment effect is explicitly dependent on its target and context .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,37,1,0,,4.09E-05,0,negative,0.00036512,0.002988974,0.000102655,2.00E-06,5.83E-05,1.96E-05,7.16E-06,0.000268623,0.001589565,0.994116687,4.57E-06,0.000474217,2.59E-06
245,sentiment_analysis16,161,"For example , d h , d p can help shift the final sentiment to negative and d h , d r can help shift it to positive .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,38,1,0,,1.26E-06,0,negative,4.92E-05,3.24E-05,6.92E-06,1.11E-07,2.94E-06,1.29E-05,1.19E-06,0.000148907,3.18E-05,0.999652142,1.10E-07,6.12E-05,1.93E-07
246,sentiment_analysis16,162,Note that ?,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,39,1,0,,3.87E-07,0,negative,3.79E-06,7.77E-06,6.80E-07,3.34E-08,5.52E-07,3.06E-06,2.90E-07,4.43E-05,6.77E-06,0.999920943,1.36E-07,1.16E-05,4.50E-08
247,sentiment_analysis16,163,is still needed to control the importance of different contexts .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,40,1,0,,2.93E-06,0,negative,1.54E-05,6.09E-05,1.95E-06,1.51E-06,5.19E-06,3.68E-05,3.07E-06,0.000550588,5.26E-05,0.999222902,3.98E-06,4.35E-05,1.56E-06
248,sentiment_analysis16,164,"In this manner , targeted - context detection ( attention ) and TCS interaction are jointly modeled and work together for sentiment inference .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,41,1,0,,0.00048959,0,negative,0.000217173,0.034328732,0.000745559,2.11E-06,0.000120236,2.63E-05,1.19E-05,0.000588453,0.039916092,0.923839168,8.65E-06,0.000187403,8.28E-06
249,sentiment_analysis16,165,The proposed techniques introduced below also follow this core idea but with different implementations or properties .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,42,1,0,,5.90E-07,0,negative,3.98E-06,0.000183825,3.27E-06,5.83E-07,4.49E-06,3.03E-05,1.81E-06,0.000486595,0.000164833,0.999100156,1.99E-06,1.73E-05,8.64E-07
250,sentiment_analysis16,166,We thus will not repeat similar discussions .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,43,1,0,,3.45E-07,0,negative,1.17E-06,9.73E-06,5.11E-07,1.94E-07,2.19E-06,4.95E-06,2.35E-07,5.50E-05,7.85E-06,0.999914974,1.09E-07,3.01E-06,6.30E-08
251,sentiment_analysis16,167,Coupled Interaction ( CI ) :,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,44,1,0,,0.000830206,0,negative,0.000282342,0.004400006,0.026200246,7.68E-07,3.57E-05,2.97E-05,4.70E-05,0.000209908,0.007793323,0.959978126,2.92E-05,0.000978982,1.47E-05
252,sentiment_analysis16,168,This proposed technique associates the TCS interaction with an additional set of context representation .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,45,1,0,,4.05E-05,0,negative,0.000283402,0.018582253,0.001909698,1.87E-06,0.000127815,2.87E-05,1.41E-05,0.000369241,0.013182294,0.965207775,1.45E-05,0.000269002,9.37E-06
253,sentiment_analysis16,169,This representation is for capturing the global correlation between context and different sentiment classes .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,46,1,0,,6.06E-05,0,negative,2.07E-05,0.002252305,0.000595494,1.65E-07,1.31E-05,8.62E-06,2.49E-06,0.000170284,0.006025858,0.990868205,1.51E-06,3.97E-05,1.46E-06
254,sentiment_analysis16,170,"Specifically , e i is another output representation for x i , which is coupled with the sentiment interaction factor d i , d t .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,47,1,0,,2.76E-07,0,negative,4.89E-06,0.000100201,1.37E-05,1.92E-08,1.42E-06,1.94E-06,2.22E-07,6.61E-05,0.000398769,0.999407877,3.57E-08,4.72E-06,7.93E-08
255,sentiment_analysis16,171,"For each context word x i , e i is generated as e i = Ex i where E ?",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,48,1,0,,4.17E-07,0,negative,2.16E-06,5.81E-05,5.63E-06,1.08E-08,1.19E-06,3.43E-06,5.34E-07,0.000107722,4.22E-05,0.999768421,4.88E-08,1.05E-05,7.62E-08
256,sentiment_analysis16,172,R dV is an embedding matrix .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,49,1,0,,3.13E-06,0,negative,6.44E-06,0.000143563,5.39E-06,2.26E-07,4.89E-06,1.58E-05,1.61E-06,0.00062371,0.000134687,0.999048901,1.92E-07,1.39E-05,6.90E-07
257,sentiment_analysis16,173,"d i , d t and e i function together as a target - sensitive context vector and are used to produce sentiment scores with WI ( W I ? R Kd ) . Joint Coupled Interaction ( JCI ) :",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,50,1,0,,1.22E-05,0,negative,6.64E-05,0.002474339,0.005009939,2.36E-07,2.15E-05,1.21E-05,8.28E-06,0.000136861,0.004536734,0.987610225,1.67E-06,0.000118282,3.47E-06
258,sentiment_analysis16,174,"A natural variant of the above model is to replace e i with c i , which means to learn a joint output representation .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,51,1,0,,4.25E-07,0,negative,4.41E-06,0.000153005,1.91E-05,2.25E-07,3.35E-06,1.18E-05,1.39E-06,0.000215084,0.000345575,0.999234548,1.01E-06,9.61E-06,8.93E-07
259,sentiment_analysis16,175,This can also reduce the number of learning parameters and simplify the CI model .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,52,1,0,,4.18E-06,0,negative,6.27E-05,8.09E-05,5.89E-06,1.25E-07,4.72E-06,4.38E-06,7.23E-07,8.15E-05,6.15E-05,0.999653158,4.15E-08,4.42E-05,2.26E-07
260,sentiment_analysis16,176,Joint Projected Interaction ( JPI ) :,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,53,1,0,,0.002019438,0,negative,0.000751841,0.007688486,0.023377228,2.53E-06,0.00013142,4.67E-05,0.000297442,0.000290772,0.003265423,0.952358289,8.97E-05,0.011623206,7.69E-05
261,sentiment_analysis16,177,"This model also employs a unified output representation like JCI , but a context output vector c i will be projected to two different continuous spaces before sentiment score calculation .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,54,1,0,,4.17E-06,0,negative,3.23E-05,0.001082059,0.001562868,2.10E-07,1.29E-05,1.15E-05,5.83E-06,0.000158343,0.002020837,0.995053082,1.09E-06,5.59E-05,3.10E-06
262,sentiment_analysis16,178,"To achieve the goal , two projection matrices W 1 , W 2 and the non-linear projection function tanh are used .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,55,1,0,,1.00E-05,0,negative,4.72E-05,0.001288464,1.71E-05,6.59E-07,2.59E-05,8.72E-05,8.47E-06,0.005998994,0.000658053,0.991824841,8.78E-08,4.05E-05,2.44E-06
263,sentiment_analysis16,179,"The intuition is that , when we want to reduce the ( embedding ) parameters and still learn a joint representation , two different sentiment effects need to be separated in different vector spaces .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,56,1,0,,2.66E-06,0,negative,2.26E-05,0.000411673,1.35E-05,4.65E-08,5.62E-06,2.34E-06,5.48E-07,7.55E-05,0.000283338,0.999160218,6.35E-08,2.43E-05,1.86E-07
264,sentiment_analysis16,180,The two sentiment effects are modeled as two terms :,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,57,1,0,,7.07E-06,0,negative,1.59E-05,0.000256521,3.86E-05,7.80E-08,3.82E-06,8.02E-06,1.65E-06,0.000306979,0.001240712,0.998112336,6.28E-08,1.46E-05,7.48E-07
265,sentiment_analysis16,181,where the first term can be viewed as learning target - independent sentiment effect while the second term captures the TCS interaction .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,58,1,0,,3.85E-07,0,negative,5.50E-05,0.000131966,3.94E-05,6.37E-08,4.88E-06,2.96E-06,9.98E-07,7.87E-05,0.000181646,0.999458334,3.34E-08,4.58E-05,2.75E-07
266,sentiment_analysis16,182,A joint sentiment - specific weight matrix W J ( W J ? R Kd ) is used to control / balance the interplay between these two effects .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,59,1,0,,2.12E-05,0,negative,2.50E-05,0.003083373,4.33E-05,4.09E-07,1.89E-05,3.32E-05,5.92E-06,0.002997958,0.006313296,0.987454771,1.81E-07,2.04E-05,3.26E-06
267,sentiment_analysis16,183,"Discussions : ( a ) In IT , CI , JCI , and JPI , their first - order terms are still needed , because not in all cases sentiment inference needs TCS interaction .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,60,1,0,,0.000174929,0,negative,0.000386605,0.00021375,4.05E-05,1.37E-07,1.17E-05,7.42E-06,2.70E-05,7.95E-05,2.44E-05,0.996252588,1.32E-06,0.002953138,2.03E-06
268,sentiment_analysis16,184,"For some simple examples like "" the battery is good "" , the context word "" good "" simply indicates clear sentiment , which can be captured by their first - order term .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,61,1,0,,5.34E-07,0,negative,8.01E-07,5.76E-06,6.83E-07,1.03E-07,5.69E-06,1.41E-05,9.70E-07,0.00015919,2.64E-06,0.999797168,1.56E-08,1.26E-05,2.92E-07
269,sentiment_analysis16,185,"However , notice that the modeling of second - order terms offers additional help in both general and target - sensitive scenarios .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,62,1,0,,0.005838333,0,negative,0.006100986,0.000213861,3.43E-05,1.04E-06,4.14E-05,2.46E-05,4.06E-05,0.000254895,3.16E-05,0.98512953,2.10E-07,0.008122731,4.26E-06
270,sentiment_analysis16,186,( b ) TCS interaction can be calculated by other modeling functions .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,63,1,0,,6.63E-05,0,negative,3.31E-05,7.90E-05,4.42E-05,3.11E-08,4.01E-06,1.83E-06,1.37E-06,3.52E-05,6.67E-05,0.999606549,5.80E-08,0.000127651,2.65E-07
271,sentiment_analysis16,187,"We have tried several methods and found that using the dot product d i , d tor d Ti W d t ( with a projection matrix W ) generally produces good results .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,64,1,0,,4.06E-05,0,negative,1.05E-05,2.06E-05,2.06E-06,5.84E-07,6.10E-06,8.24E-05,2.68E-05,0.00122975,3.29E-06,0.998229917,4.03E-07,0.000383766,3.81E-06
272,sentiment_analysis16,188,"( c ) One may ask whether we can use fewer embeddings or just use one universal embedding to replace A , C and D ( the definition of D can be found in the introduction of IT ) .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,65,1,0,,7.39E-07,0,negative,1.11E-06,7.99E-06,9.18E-07,4.83E-08,5.55E-07,5.97E-06,5.48E-07,0.000107403,5.83E-06,0.999862232,3.65E-08,7.19E-06,1.73E-07
273,sentiment_analysis16,189,We have investigated them as well .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,66,1,0,,1.69E-07,0,negative,1.43E-06,6.14E-06,1.48E-06,8.85E-08,3.25E-06,1.07E-05,1.08E-06,0.000101253,3.96E-06,0.999863259,2.04E-08,7.07E-06,2.64E-07
274,sentiment_analysis16,190,We found that merging A and C is basically workable .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,67,1,0,,0.001017731,0,negative,0.000138624,4.86E-05,3.11E-06,7.82E-07,1.35E-05,0.000101311,7.59E-05,0.002309974,9.80E-06,0.994796567,2.69E-07,0.002491827,9.78E-06
275,sentiment_analysis16,191,But merging D and A/C produces poor results because they essentially function with different purposes .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,68,1,0,,0.003420496,0,negative,0.000813519,1.97E-05,1.41E-05,2.55E-07,1.66E-05,1.33E-05,0.000184402,0.00014098,1.18E-06,0.938474848,1.97E-07,0.060314573,6.29E-06
276,sentiment_analysis16,192,"While A and C handle targeted - context detection ( attention ) , D captures the TCS interaction .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,69,1,0,,7.46E-07,0,negative,4.71E-05,0.00023468,0.000189934,4.84E-08,1.14E-05,3.89E-06,2.35E-06,5.18E-05,0.000504062,0.998907944,1.58E-08,4.61E-05,6.67E-07
277,sentiment_analysis16,193,"( d ) Except NP , we do not apply non-linear projection to the sentiment score layer .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,70,1,0,,5.35E-05,0,negative,0.000202471,0.000561214,0.000223453,4.77E-07,7.20E-05,1.23E-05,1.83E-05,0.000164888,2.54E-05,0.997551147,1.37E-07,0.001164664,3.62E-06
278,sentiment_analysis16,194,"Although adding non-linear transformation to it may further improve model performance , the individual sentiment effect from each context will become untraceable , i.e. , losing some interpretability .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,71,1,0,,2.20E-05,0,negative,0.001461809,5.30E-05,9.95E-06,3.84E-07,2.20E-05,6.24E-06,4.87E-06,6.63E-05,1.24E-05,0.997528942,1.47E-08,0.000833295,8.29E-07
279,sentiment_analysis16,195,"In order to show the effectiveness of learning TCS interaction and for analysis purpose , we do not use it in this work .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,72,1,0,,3.71E-07,0,negative,1.50E-06,1.44E-05,2.38E-06,3.26E-07,3.31E-05,9.67E-06,1.42E-06,9.29E-05,1.83E-06,0.999823098,1.62E-08,1.90E-05,4.14E-07
280,sentiment_analysis16,196,But it can be flexibly added for specific tasks / analyses that do not require strong interpretability .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,73,1,0,,4.86E-06,0,negative,8.82E-06,1.79E-05,3.25E-06,1.98E-08,1.81E-06,1.72E-06,5.38E-07,3.72E-05,1.26E-05,0.999886669,4.36E-09,2.93E-05,1.14E-07
281,sentiment_analysis16,197,Loss function :,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,74,1,0,,0.000100545,0,negative,1.85E-05,0.000209807,0.000596264,1.15E-08,3.08E-06,4.88E-06,5.73E-06,0.000109228,0.000380682,0.998589953,2.07E-08,8.08E-05,1.01E-06
282,sentiment_analysis16,198,The proposed models are all trained in an end - to - end manner by minimizing the cross entropy loss .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,75,1,0,,0.000101222,0,negative,6.15E-05,0.013089612,0.000110189,1.19E-06,0.000120189,3.30E-05,1.36E-05,0.002660112,0.00523633,0.978620853,6.23E-08,4.62E-05,7.10E-06
283,sentiment_analysis16,199,Let us denote a sentence and a target aspect as x and t respectively .,The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,76,1,0,,3.65E-07,0,negative,1.48E-07,1.47E-05,4.39E-07,1.76E-08,9.78E-07,4.75E-06,5.85E-07,0.000239747,1.36E-05,0.999722043,5.13E-09,2.74E-06,2.30E-07
284,sentiment_analysis16,200,"They appear together in a pair format ( x , t) as input and all such pairs construct the dataset H. g ( x , t ) is a one - hot vector and g k ( x , t ) ?",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,77,1,0,,1.75E-07,0,negative,8.22E-07,1.50E-05,2.06E-06,6.84E-09,1.57E-06,1.15E-06,2.31E-07,3.54E-05,1.07E-05,0.999928931,7.61E-10,4.09E-06,5.51E-08
285,sentiment_analysis16,201,"{ 0 , 1 } denotes a gold sentiment label , i.e. , whether ( x , t) shows sentiment k.",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,78,1,0,,5.72E-07,0,negative,3.50E-07,3.06E-05,1.22E-06,1.77E-08,1.59E-06,5.63E-06,7.03E-07,0.000374146,1.90E-05,0.999562949,2.95E-09,3.56E-06,2.70E-07
286,sentiment_analysis16,202,"y x ,t is the model - predicted sentiment distribution for ( x , t ) .",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,79,1,0,,1.41E-07,0,negative,7.29E-07,1.15E-05,1.71E-06,3.77E-09,6.77E-07,9.93E-07,2.70E-07,3.89E-05,1.24E-05,0.99992872,8.52E-10,4.07E-06,6.86E-08
287,sentiment_analysis16,203,"y k x ,t denotes it s probability in class k.",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,80,1,0,,1.30E-07,0,negative,9.44E-07,6.56E-06,1.59E-06,2.07E-08,1.61E-06,1.95E-06,3.61E-07,5.88E-05,6.38E-06,0.999916347,1.25E-09,5.32E-06,1.41E-07
288,sentiment_analysis16,204,"Based on them , the training loss is constructed as :",The Proposed Approaches,The Proposed Approaches,sentiment_analysis,16,81,1,0,,4.62E-07,0,negative,3.07E-06,4.07E-05,2.98E-06,1.68E-08,2.11E-06,3.00E-06,7.43E-07,0.000252827,3.42E-05,0.999652311,1.24E-09,7.87E-06,2.49E-07
289,sentiment_analysis16,205,Related Work,,,sentiment_analysis,16,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
290,sentiment_analysis16,224,Experiments,,,sentiment_analysis,16,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
291,sentiment_analysis16,225,"We perform experiments on the datasets of Se -m Eval Task 2014 , which contain online reviews from domain Laptop and Restaurant .",Experiments,Experiments,sentiment_analysis,16,1,1,0,,0.044816213,0,negative,0.001878962,0.005562806,0.00292053,0.000242936,0.00136271,0.00561883,0.018787408,0.040812683,0.000143744,0.909110962,0.000591976,0.011891416,0.001075036
292,sentiment_analysis16,226,"In these datasets , aspect sentiment polarities are labeled .",Experiments,Experiments,sentiment_analysis,16,2,1,0,,0.00698216,0,negative,0.000123463,5.85E-05,0.001019772,9.59E-06,2.98E-05,0.00063474,0.001239709,0.003061703,6.54E-06,0.992164171,0.000442876,0.001133062,7.61E-05
293,sentiment_analysis16,227,The training and test sets have also been provided .,Experiments,Experiments,sentiment_analysis,16,3,1,0,,0.000651923,0,negative,0.000100204,1.25E-05,2.41E-05,3.74E-06,5.86E-06,0.001660378,0.00018006,0.012291627,9.36E-06,0.9854234,2.93E-06,0.000273825,1.21E-05
294,sentiment_analysis16,228,Full statistics of the datasets are given in .,Experiments,Experiments,sentiment_analysis,16,4,1,0,,0.003846171,0,negative,0.000171617,1.29E-05,5.34E-05,5.67E-05,3.46E-05,0.004094205,0.000913819,0.00865245,9.74E-06,0.985300441,1.67E-05,0.000622685,6.08E-05
295,sentiment_analysis16,229,AMN : A state - of - the - art memory network used for ASC .,Experiments,Experiments,sentiment_analysis,16,5,1,1,baselines,0.790061431,1,baselines,0.000254126,8.44E-05,0.928140969,1.00E-05,5.25E-06,0.00132525,0.013192442,0.003439513,0.000125504,0.045970255,0.002040633,0.00372561,0.001686031
296,sentiment_analysis16,230,"The main difference from MN is in its attention alignment function , which concatenates the distributed representations of the context and aspect , and uses an additional weight matrix for attention calculation , following the method introduced in .",Experiments,Experiments,sentiment_analysis,16,6,1,0,,0.014719147,0,negative,0.009870458,0.005202786,0.302796223,8.42E-05,0.000176045,0.003779626,0.003728964,0.018091162,0.003320553,0.644269518,0.000811698,0.006597426,0.001271335
297,sentiment_analysis16,231,"BL - MN : Our basic memory network presented in Section 2 , which does not use the proposed techniques for capturing target - sensitive sentiments .",Experiments,Experiments,sentiment_analysis,16,7,1,1,baselines,0.871195408,1,baselines,0.000236162,0.000116665,0.925952199,3.74E-06,4.94E-06,0.001270775,0.009976566,0.003569005,6.51E-05,0.054658474,0.000174845,0.003420601,0.000550898
298,sentiment_analysis16,232,AE - LSTM : RNN / LSTM is another popular attention based neural model .,Experiments,Experiments,sentiment_analysis,16,8,1,1,baselines,0.65669415,1,baselines,0.000522885,0.000239704,0.565261577,8.13E-05,2.51E-05,0.02541605,0.096869569,0.073884008,0.000536991,0.217527912,0.00240791,0.005594069,0.011632979
299,sentiment_analysis16,233,"Here we compare with a state - of - the - art attention - based LSTM for ASC , AE - LSTM .",Experiments,Experiments,sentiment_analysis,16,9,1,1,baselines,0.65210619,1,negative,0.002497058,0.006977806,0.283615329,5.01E-05,0.000170198,0.004146482,0.024519583,0.025919908,0.00065849,0.624134518,0.001244256,0.024451984,0.001614337
300,sentiment_analysis16,234,ATAE - LSTM :,Experiments,Experiments,sentiment_analysis,16,10,1,1,baselines,0.919135416,1,baselines,0.001147133,5.65E-05,0.930134339,1.15E-05,1.17E-05,0.001726375,0.019867048,0.002339492,4.44E-05,0.032554694,6.64E-05,0.010760341,0.001280109
301,sentiment_analysis16,235,Another attention - based LSTM for ASC reported in .,Experiments,Experiments,sentiment_analysis,16,11,1,1,baselines,0.150919877,0,negative,0.000771754,0.000232252,0.092430264,1.26E-05,1.29E-05,0.00357388,0.040538295,0.017730269,6.84E-05,0.797095305,0.014916312,0.030978262,0.001639532
302,sentiment_analysis16,236,Target - sensitive Memory Networks ( TMNs ) :,Experiments,Experiments,sentiment_analysis,16,12,1,1,baselines,0.766572872,1,baselines,0.001080759,0.000577662,0.599858796,1.50E-05,1.52E-05,0.001359256,0.022313865,0.005398529,0.000511901,0.315223207,0.016307081,0.034830398,0.00250838
303,sentiment_analysis16,237,"The six proposed techniques , NP , CNP , IT , CI , JCI , and JPI give six target - sensitive memory networks .",Experiments,Experiments,sentiment_analysis,16,13,1,1,baselines,0.060256493,0,negative,0.002619548,0.00183973,0.093425884,5.72E-05,0.000170729,0.006026816,0.005625565,0.039200756,0.002025076,0.841637859,0.00027675,0.005285533,0.001808575
304,sentiment_analysis16,238,"Note that other non-neural network based models like SVM and neural models without attention mechanism like traditional LSTMs have been compared and reported with inferior performance in the ASC task , so they are excluded from comparisons here .",Experiments,Experiments,sentiment_analysis,16,14,1,0,,0.008109608,0,negative,0.000145057,1.19E-05,0.000135422,2.46E-06,7.87E-06,0.000627572,0.000404274,0.002798368,4.02E-06,0.994383902,9.02E-06,0.001445976,2.41E-05
305,sentiment_analysis16,239,"Also , note that non-neural models like SVMs require feature engineering to manually encode aspect information , while this work aims to improve the aspect representation learning based approaches .",Experiments,Experiments,sentiment_analysis,16,15,1,0,,0.005342568,0,negative,0.001156737,0.000582436,0.001010593,0.000401141,0.00018357,0.003311506,0.001149493,0.018194896,0.000203446,0.970808291,0.000619606,0.001593294,0.000784991
306,sentiment_analysis16,240,Evaluation Measure,,,sentiment_analysis,16,0,1,0,,0.001629158,0,negative,1.34E-05,2.54E-05,3.17E-06,2.38E-07,3.70E-07,3.86E-05,3.86E-05,0.00062628,8.68E-06,0.99773056,0.00132231,0.000191331,1.10E-06
307,sentiment_analysis16,241,"Since we have a three - class classification task ( positive , negative and neutral ) and the classes are imbalanced as shown in , we use F1 - score as our evaluation measure .",Evaluation Measure,Evaluation Measure,sentiment_analysis,16,1,1,0,,0.000240323,0,negative,1.27E-05,3.56E-06,0.000111172,1.98E-08,7.20E-08,4.69E-05,1.29E-05,0.000521154,8.73E-07,0.999180425,3.81E-06,0.000106268,2.48E-07
308,sentiment_analysis16,242,We report both F1 - Macro over all classes and all individual classbased F1 scores .,Evaluation Measure,Evaluation Measure,sentiment_analysis,16,2,1,0,,0.000193025,0,negative,5.83E-05,3.63E-06,7.75E-05,6.16E-08,2.64E-07,3.28E-05,2.67E-05,0.000247529,8.52E-07,0.998825928,1.93E-06,0.000724171,4.32E-07
309,sentiment_analysis16,243,"As our problem requires finegrained sentiment interaction , the class - based F1 provides more indicative information .",Evaluation Measure,Evaluation Measure,sentiment_analysis,16,3,1,0,,0.001103178,0,negative,0.005210824,1.51E-05,0.000654557,2.25E-07,1.54E-06,1.94E-05,2.88E-05,0.000138358,3.96E-06,0.983768806,4.93E-06,0.010151862,1.61E-06
310,sentiment_analysis16,244,"In addition , we report the accuracy ( same as F1-Micro ) , as it is used in previous studies .",Evaluation Measure,Evaluation Measure,sentiment_analysis,16,4,1,0,,6.62E-05,0,negative,6.70E-05,5.12E-06,0.000140903,4.59E-07,1.85E-06,4.06E-05,2.64E-05,0.000178694,1.59E-06,0.99929637,2.13E-06,0.000238063,7.98E-07
311,sentiment_analysis16,245,"However , we suggest using F1 - score because accuracy biases towards the majority class .",Evaluation Measure,Evaluation Measure,sentiment_analysis,16,5,1,0,,4.04E-05,0,negative,0.000303284,3.20E-06,0.00015182,1.42E-07,2.89E-07,4.30E-05,9.00E-06,0.000240585,2.14E-06,0.999000453,4.19E-06,0.000241186,6.98E-07
312,sentiment_analysis16,246,Training Details,,,sentiment_analysis,16,0,1,0,,0.002340091,0,negative,6.05E-05,0.001338931,1.43E-05,0.000123468,5.30E-05,0.00076714,0.000215992,0.00645336,0.000307534,0.988848599,0.00167076,0.000130888,1.56E-05
313,sentiment_analysis16,247,We use the open - domain word embeddings 1 for the initialization of word vectors .,Training Details,Training Details,sentiment_analysis,16,1,1,1,hyperparameters,0.991039277,1,experimental-setup,2.17E-06,6.73E-06,1.65E-06,5.11E-07,1.85E-07,0.621721514,0.000316109,0.374961959,4.98E-06,0.002979152,1.05E-06,1.35E-06,2.64E-06
314,sentiment_analysis16,248,"We initialize other model parameters from a uniform distribution U ( - 0.05 , 0.05 ) .",Training Details,Training Details,sentiment_analysis,16,2,1,1,hyperparameters,0.984872226,1,experimental-setup,4.02E-06,6.81E-06,2.34E-06,5.13E-07,2.62E-07,0.675402225,0.00034668,0.317216633,5.74E-06,0.007008678,1.08E-06,2.35E-06,2.68E-06
315,sentiment_analysis16,249,The dimension of the word embedding and the size of the hidden layers are 300 .,Training Details,Training Details,sentiment_analysis,16,3,1,1,hyperparameters,0.994810377,1,experimental-setup,2.28E-06,5.38E-06,1.49E-06,7.31E-07,2.12E-07,0.61114981,0.000452658,0.38589166,5.21E-06,0.00248196,1.69E-06,1.89E-06,5.03E-06
316,sentiment_analysis16,250,The learning rate is set to 0.01 and the dropout rate is set to 0.1 .,Training Details,Training Details,sentiment_analysis,16,4,1,1,hyperparameters,0.991953653,1,experimental-setup,3.63E-06,7.08E-06,1.23E-06,1.86E-06,4.38E-07,0.587513518,0.000439831,0.409797871,4.75E-06,0.002219553,1.22E-06,2.23E-06,6.81E-06
317,sentiment_analysis16,251,Stochastic gradient descent is used as our optimizer .,Training Details,Training Details,sentiment_analysis,16,5,1,1,hyperparameters,0.94025889,1,experimental-setup,3.11E-06,1.18E-05,6.76E-06,1.03E-06,2.78E-07,0.679775791,0.000480164,0.316370903,1.04E-05,0.003331714,1.07E-06,1.43E-06,5.50E-06
318,sentiment_analysis16,252,The position encoding is also used .,Training Details,Training Details,sentiment_analysis,16,6,1,0,,0.630171938,1,experimental-setup,0.002307294,0.001461297,0.047813279,2.77E-05,4.37E-05,0.448912535,0.004517908,0.184063291,0.011101175,0.298720907,0.000222209,0.000577567,0.000231156
319,sentiment_analysis16,253,"We also compare the memory networks in their multiple computational layers version ( i.e. , multiple hops ) and the number of hops is set to 3 as used in the mentioned previous studies .",Training Details,Training Details,sentiment_analysis,16,7,1,1,hyperparameters,0.561724794,1,experimental-setup,0.000203893,0.000273008,0.000624676,3.02E-06,6.97E-06,0.578007391,0.004227022,0.307253016,0.000144208,0.108919067,1.82E-05,0.000287708,3.19E-05
320,sentiment_analysis16,254,"We implemented all models in the TensorFlow environment using same input , embedding size , dropout rate , optimizer , etc.",Training Details,Training Details,sentiment_analysis,16,8,1,1,hyperparameters,0.983319749,1,experimental-setup,2.09E-06,3.00E-06,2.80E-06,0.000113445,4.57E-06,0.975301059,0.000898993,0.020680495,1.62E-06,0.002978642,5.82E-07,1.63E-06,1.11E-05
321,sentiment_analysis16,255,"so as to test our hypotheses , i.e. , to make sure the achieved improvements do not come from elsewhere .",Training Details,Training Details,sentiment_analysis,16,9,1,0,,0.001512214,0,negative,4.06E-05,7.25E-06,8.73E-06,3.49E-06,8.73E-06,0.08279538,0.000270988,0.014422804,9.45E-06,0.902369525,8.05E-06,5.16E-05,3.43E-06
322,sentiment_analysis16,256,"Meanwhile , we can also report all evaluation measures discussed above 2 . 10 % of the training data is used as the development set .",Training Details,Training Details,sentiment_analysis,16,10,1,0,,0.004781273,0,negative,4.74E-05,1.76E-05,1.81E-05,1.55E-06,6.34E-06,0.232108244,0.000800957,0.094207137,1.19E-05,0.672626455,5.27E-06,0.000143714,5.26E-06
323,sentiment_analysis16,257,We report the best results for all models based on their F - 1 Macro scores .,Training Details,Training Details,sentiment_analysis,16,11,1,0,,0.014752502,0,negative,0.000133172,2.59E-05,3.57E-05,7.83E-06,1.91E-05,0.350101209,0.00607709,0.094652151,8.95E-06,0.546117583,2.07E-05,0.002767934,3.28E-05
324,sentiment_analysis16,258,Result Analysis,,,sentiment_analysis,16,0,1,0,,0.000601765,0,negative,1.37E-05,3.72E-05,1.33E-06,2.64E-07,1.89E-07,6.01E-05,7.71E-05,0.001320129,1.84E-05,0.993161332,0.00486798,0.000440059,2.18E-06
325,sentiment_analysis16,259,The classification results are shown in .,Result Analysis,Result Analysis,sentiment_analysis,16,1,1,0,,0.758561233,1,negative,0.000568898,7.12E-07,6.63E-05,2.70E-08,4.28E-08,2.17E-05,0.000216946,0.000122098,1.28E-07,0.615086887,2.93E-05,0.383885817,1.23E-06
326,sentiment_analysis16,260,"Note that the candidate models are all based on classic / standard attention mechanism , i.e. , without sophisticated or multiple attentions involved .",Result Analysis,Result Analysis,sentiment_analysis,16,2,1,0,,0.295043936,0,negative,0.000488459,3.61E-06,3.28E-05,1.06E-06,6.28E-07,0.000121937,2.56E-05,0.00044102,2.34E-06,0.997946308,1.09E-06,0.000934659,4.96E-07
327,sentiment_analysis16,261,We compare the 1 - hop and 3 - hop memory networks as two different settings .,Result Analysis,Result Analysis,sentiment_analysis,16,3,1,0,,0.156820503,0,negative,0.001445753,0.000129321,0.0047526,1.02E-06,1.45E-06,0.000310112,0.000426854,0.001959314,2.63E-05,0.961494585,6.09E-05,0.029383565,8.21E-06
328,sentiment_analysis16,262,The top three F1 - Macro scores are marked in bold .,Result Analysis,Result Analysis,sentiment_analysis,16,4,1,0,,0.005467086,0,negative,0.000146717,2.11E-06,2.93E-05,7.13E-07,3.61E-07,0.000293613,0.000148757,0.001062205,1.97E-06,0.991148566,1.28E-05,0.007149757,3.15E-06
329,sentiment_analysis16,263,"Based on them , we have the following observations :",Result Analysis,Result Analysis,sentiment_analysis,16,5,1,0,,0.000287279,0,negative,0.000371204,2.38E-06,2.08E-05,1.88E-07,1.47E-07,2.10E-05,1.28E-05,7.76E-05,1.85E-06,0.996619417,4.03E-06,0.002868203,4.50E-07
330,sentiment_analysis16,264,1 .,Result Analysis,Result Analysis,sentiment_analysis,16,6,1,0,,0.000396758,0,negative,0.000104517,1.68E-06,5.84E-06,3.22E-07,1.11E-07,6.96E-05,6.82E-06,0.000317508,3.96E-06,0.999187286,4.02E-06,0.000297879,5.05E-07
331,sentiment_analysis16,265,"Comparing the 1 - hop memory networks ( first nine rows ) , we see significant performance gains achieved by CNP , CI , JCI , and JPI on both datasets , where each of them has p < 0.01 over the strongest baseline ( BL - MN ) from paired t- test using F1 - Macro .",Result Analysis,Result Analysis,sentiment_analysis,16,7,1,1,results,0.95177582,1,results,0.004745597,6.84E-07,2.90E-05,1.53E-07,1.81E-07,6.37E-06,0.000537336,4.54E-05,8.14E-08,0.027894344,4.21E-06,0.966731134,5.58E-06
332,sentiment_analysis16,266,IT also outperforms the other baselines while NP has similar performance to BL - MN .,Result Analysis,Result Analysis,sentiment_analysis,16,8,1,0,,0.82890127,1,results,0.001560783,5.68E-07,4.35E-05,3.10E-07,2.22E-07,1.43E-05,0.001057918,7.20E-05,9.62E-08,0.031141527,8.60E-06,0.966087703,1.25E-05
333,sentiment_analysis16,267,"This indicates that TCS interaction is very useful , as BL - MN and NP do not model it .",Result Analysis,Result Analysis,sentiment_analysis,16,9,1,0,,0.00882447,0,negative,0.046731257,4.63E-06,0.000245929,6.89E-07,1.29E-06,3.16E-05,0.000130558,0.000106086,2.50E-06,0.812394216,5.81E-06,0.140342157,3.31E-06
334,sentiment_analysis16,268,"2 . In the 3 - hop setting , TMNs achieve much better results on Restaurant .",Result Analysis,Result Analysis,sentiment_analysis,16,10,1,1,results,0.863737883,1,results,0.000523646,1.85E-07,1.33E-05,6.73E-08,5.30E-08,8.62E-06,0.000529183,5.78E-05,2.59E-08,0.028238383,5.58E-06,0.970617682,5.42E-06
335,sentiment_analysis16,269,"JCI , IT , and CI achieve the best scores , outperforming the strongest baseline AMN by 2.38 % , 2.18 % , and 2.03 % .",Result Analysis,Result Analysis,sentiment_analysis,16,11,1,1,results,0.962820518,1,results,0.001861877,2.80E-07,2.94E-05,2.37E-07,2.09E-07,4.77E-06,0.000944116,2.42E-05,4.20E-08,0.012924523,2.06E-06,0.984196725,1.16E-05
336,sentiment_analysis16,270,"On Laptop , BL - MN and most TMNs ( except CNP and JPI ) perform similarly .",Result Analysis,Result Analysis,sentiment_analysis,16,12,1,1,results,0.864321289,1,results,0.00129873,7.47E-07,2.12E-05,4.57E-07,2.43E-07,5.45E-05,0.001586603,0.000310742,1.29E-07,0.06751758,1.02E-05,0.929180813,1.80E-05
337,sentiment_analysis16,271,"However , BL - MN performs poorly on Restaurant ( only better than two models ) while TMNs show more stable performance .",Result Analysis,Result Analysis,sentiment_analysis,16,13,1,0,,0.761914051,1,results,0.003110914,1.43E-07,1.37E-05,1.71E-07,1.05E-07,5.24E-06,0.00046836,2.56E-05,2.57E-08,0.016073868,1.84E-06,0.980293554,6.48E-06
338,sentiment_analysis16,272,"3 . Comparing all TMNs , we see that JCI works the best as it always obtains the top - three scores on two datasets and in two settings .",Result Analysis,Result Analysis,sentiment_analysis,16,14,1,1,results,0.974402718,1,results,0.001061072,1.13E-06,1.96E-05,7.96E-07,2.92E-07,8.33E-05,0.002299919,0.000509312,1.90E-07,0.054215772,1.78E-05,0.94176868,2.21E-05
339,sentiment_analysis16,273,CI and JPI also perform well in most cases .,Result Analysis,Result Analysis,sentiment_analysis,16,15,1,1,results,0.882774415,1,results,0.006744977,2.03E-06,4.58E-05,1.12E-05,1.42E-06,0.000178569,0.003727162,0.000734077,9.84E-07,0.09699468,3.82E-05,0.891431151,8.97E-05
340,sentiment_analysis16,274,"IT , NP , and CNP can achieve very good scores in some cases but are less stable .",Result Analysis,Result Analysis,sentiment_analysis,16,16,1,1,results,0.701837486,1,results,0.002150736,1.44E-07,3.48E-05,1.09E-07,1.25E-07,5.49E-06,0.000439559,1.96E-05,4.15E-08,0.048482391,2.64E-06,0.948859038,5.35E-06
341,sentiment_analysis16,275,We also analyzed their potential issues in Section 4 .,Result Analysis,Result Analysis,sentiment_analysis,16,17,1,0,,0.000594168,0,negative,0.000261047,1.28E-06,8.34E-06,4.09E-07,6.36E-07,2.29E-05,6.45E-06,6.66E-05,1.57E-06,0.998923123,3.43E-07,0.000706943,3.48E-07
342,sentiment_analysis16,276,4 . It is important to note that these improvements are quite large because in many cases sentiment interactions may not be necessary ( like sentence ( 1 ) in Section 1 ) .,Result Analysis,Result Analysis,sentiment_analysis,16,18,1,0,,0.609170955,1,results,0.011364743,3.70E-06,3.65E-05,8.64E-06,2.83E-06,0.00018105,0.002504672,0.000981431,2.17E-06,0.395569969,2.15E-05,0.589243535,7.92E-05
343,sentiment_analysis16,277,"The over all good results obtained by TMNs demonstrate their capability of handling both general and target - sensitive sentiments , i.e. , the proposed Integration with Improved Attention :",Result Analysis,Result Analysis,sentiment_analysis,16,19,1,0,,0.828681734,1,results,0.007889053,5.74E-07,3.44E-05,1.27E-07,2.18E-07,3.97E-06,0.000309646,1.80E-05,1.03E-07,0.050974625,2.37E-06,0.940761681,5.23E-06
344,sentiment_analysis16,278,"As discussed , the goal of this work is not for learning better attention but addressing the targetsensitive sentiment .",Result Analysis,Result Analysis,sentiment_analysis,16,20,1,0,,0.08747006,0,negative,0.002327653,0.00021712,0.000493026,7.72E-06,1.02E-05,0.000143977,0.000189255,0.001006561,4.21E-05,0.973487581,0.000172728,0.021868467,3.36E-05
345,sentiment_analysis16,279,"In fact , solely improving attention does not solve our problem ( see Sections 1 and 3 ) .",Result Analysis,Result Analysis,sentiment_analysis,16,21,1,0,,0.009727902,0,negative,0.025233559,2.97E-06,4.01E-05,1.17E-06,2.19E-06,3.80E-05,0.000295141,0.00016221,1.01E-06,0.670080293,3.94E-06,0.304129867,9.48E-06
346,sentiment_analysis16,280,"However , better attention can certainly help achieve an over all better performance for the ASC task , as it makes the targeted - context detection more accurate .",Result Analysis,Result Analysis,sentiment_analysis,16,22,1,0,,0.401423658,0,results,0.018283597,1.28E-06,3.22E-05,4.23E-07,5.70E-07,1.76E-05,0.000298201,9.03E-05,4.13E-07,0.234814626,3.01E-06,0.746449666,8.10E-06
347,sentiment_analysis16,281,"Here we integrate our pro-posed technique JCI with a state - of - the - art sophisticated attention mechanism , namely , the recurrent attention framework , which involves multiple attentions learned iteratively .",Result Analysis,Result Analysis,sentiment_analysis,16,23,1,0,,0.689693422,1,negative,0.019762792,0.003994181,0.03178601,2.52E-05,8.58E-05,0.000299843,0.001425992,0.00156072,0.00055193,0.754527791,0.000257913,0.185493572,0.000228264
348,sentiment_analysis16,282,We name our model with this integration as,Result Analysis,Result Analysis,sentiment_analysis,16,24,1,0,,0.004374428,0,negative,0.002527971,0.000157068,0.009392663,1.28E-05,1.04E-05,0.000202184,0.000163742,0.00085793,0.000984395,0.980448484,3.62E-05,0.005130432,7.58E-05
349,sentiment_analysis16,283,", each of which results in a 3 - dimensional vector .",Result Analysis,Result Analysis,sentiment_analysis,16,25,1,0,,0.02138159,0,negative,0.00080743,9.51E-06,0.000191652,1.21E-07,5.64E-07,0.000147627,4.03E-05,0.001030235,1.70E-05,0.995503244,2.22E-07,0.002250488,1.64E-06
350,sentiment_analysis16,284,Illustrative Examples : shows two records in Laptop .,Result Analysis,Result Analysis,sentiment_analysis,16,26,1,0,,0.003327069,0,negative,0.000240614,2.69E-06,4.18E-05,5.97E-07,1.71E-06,5.91E-05,0.000171635,0.00028796,1.64E-06,0.979782439,2.53E-06,0.019399784,7.48E-06
351,sentiment_analysis16,285,"In record 1 , to identify the sentiment of target price in the presented sentence , the sentiment interaction between the context word "" higher "" and the target word price is the key .",Result Analysis,Result Analysis,sentiment_analysis,16,27,1,0,,0.016284867,0,negative,0.001340744,4.17E-06,6.27E-05,4.24E-06,1.62E-05,6.03E-05,8.85E-05,0.000162195,2.43E-06,0.985979698,8.69E-07,0.012270235,7.65E-06
352,sentiment_analysis16,286,"The specific sentiment scores of the word "" higher "" towards negative , neutral and positive classes in both models are reported .",Result Analysis,Result Analysis,sentiment_analysis,16,28,1,0,,0.023069173,0,negative,0.000377303,2.75E-06,5.64E-06,1.34E-07,6.01E-07,1.84E-05,2.23E-05,0.000238677,9.62E-07,0.987679488,3.00E-07,0.01165242,1.02E-06
353,sentiment_analysis16,287,We can see both models accurately assign the highest sentiment scores to the negative class .,Result Analysis,Result Analysis,sentiment_analysis,16,29,1,0,,0.682671791,1,results,0.00506922,1.09E-06,1.24E-05,1.55E-07,3.53E-07,1.32E-05,0.000325114,0.00013613,2.85E-07,0.25823732,9.36E-07,0.736197444,6.36E-06
354,sentiment_analysis16,288,"We also observe that in MN the negative score ( 0.3641 ) in the 3 - dimension vector { 0.3641 , ? 0.3275 , ?0.0750 } calculated by ? i W c i is greater than neutral ( ? 0.3275 ) and positive ( ? 0.0750 ) scores .",Result Analysis,Result Analysis,sentiment_analysis,16,30,1,0,,0.019001881,0,results,0.039287129,2.29E-06,4.25E-05,4.97E-07,1.34E-06,3.45E-05,0.000351353,0.000291773,8.31E-07,0.423965788,9.02E-07,0.536013001,8.05E-06
355,sentiment_analysis16,289,Notice that ?,Result Analysis,Result Analysis,sentiment_analysis,16,31,1,0,,0.000348666,0,negative,0.000117598,6.52E-07,5.56E-06,8.14E-08,1.30E-07,9.27E-06,5.22E-06,5.28E-05,9.16E-07,0.998661818,2.43E-07,0.001145311,3.64E-07
356,sentiment_analysis16,290,"i is always positive ( ranging in ( 0 , 1 ) ) , so it can be inferred that the first value in vector W c i is greater than the other two values .",Result Analysis,Result Analysis,sentiment_analysis,16,32,1,0,,0.003034352,0,negative,0.000861993,7.86E-06,4.59E-05,1.31E-07,5.64E-07,1.24E-05,1.33E-05,0.000167216,4.64E-06,0.98941153,3.33E-07,0.009473117,1.06E-06
357,sentiment_analysis16,291,"Here c i denotes the vector representation of "" higher "" so we use c higher to highlight it and we have { W c higher } N egative > { W c higher } N eutral / P ositive as an inference .",Result Analysis,Result Analysis,sentiment_analysis,16,33,1,0,,0.000957738,0,negative,0.000111248,2.25E-06,2.67E-05,6.55E-08,2.70E-07,1.30E-05,6.44E-06,0.000104705,2.86E-06,0.998493576,1.33E-07,0.001238181,5.84E-07
358,sentiment_analysis16,292,"In record 2 , the target is resolution and its sentiment is positive in the presented sentence .",Result Analysis,Result Analysis,sentiment_analysis,16,34,1,0,,0.008405224,0,negative,0.000722477,1.77E-06,7.76E-05,3.69E-07,6.99E-06,1.82E-05,0.000110359,7.18E-05,4.75E-07,0.963859461,1.67E-07,0.035126546,3.78E-06
359,sentiment_analysis16,293,"Although we have the same context word "" higher "" , different from record 1 , it requires a positive sentiment interaction with the current target .",Result Analysis,Result Analysis,sentiment_analysis,16,35,1,0,,0.000303256,0,negative,0.002397477,1.30E-06,5.24E-05,4.39E-07,4.89E-06,1.36E-05,4.39E-05,4.38E-05,5.77E-07,0.979275746,9.24E-08,0.018163934,1.86E-06
360,sentiment_analysis16,294,"Looking at the results , we see TMN assigns the highest sentiment score of word "" higher "" to positive class correctly , whereas MN assigns it to negative class .",Result Analysis,Result Analysis,sentiment_analysis,16,36,1,0,,0.835582907,1,results,0.006509575,3.91E-07,8.52E-06,1.06E-07,3.31E-07,6.78E-06,0.000440241,6.28E-05,7.43E-08,0.091088262,2.18E-07,0.901875344,7.40E-06
361,sentiment_analysis16,295,This error is expected if we consider the above inference { W c higher } N egative > { W c higher } N eutral / P ositive in MN .,Result Analysis,Result Analysis,sentiment_analysis,16,37,1,0,,0.000796775,0,negative,0.000793551,1.31E-06,3.61E-05,2.86E-08,2.42E-07,4.41E-06,8.98E-06,3.02E-05,1.09E-06,0.989739778,1.28E-07,0.009383683,5.50E-07
362,sentiment_analysis16,296,The cause of this unavoidable error is that W c i is not conditioned on the target .,Result Analysis,Result Analysis,sentiment_analysis,16,38,1,0,,0.000358864,0,negative,0.000801368,4.72E-06,4.73E-05,1.95E-07,9.00E-07,9.21E-06,1.25E-05,7.37E-05,4.25E-06,0.99462478,4.27E-07,0.004418712,1.87E-06
363,sentiment_analysis16,297,"In contrast , W J d i , d t tanh ( W 2 c i ) can change the sentiment polarity with the aspect vector d t encoded .",Result Analysis,Result Analysis,sentiment_analysis,16,39,1,0,,0.005205356,0,negative,0.005588983,7.56E-06,0.000242013,1.78E-07,9.45E-07,1.24E-05,2.65E-05,0.000107586,5.67E-06,0.968851054,2.20E-07,0.025154696,2.14E-06
364,sentiment_analysis16,298,"Other TMNs also achieve it ( like WI d i , d t c i in JCI ) .",Result Analysis,Result Analysis,sentiment_analysis,16,40,1,0,,0.001304176,0,negative,0.000397957,1.68E-06,0.000163296,9.64E-08,4.04E-07,8.41E-06,4.20E-05,3.45E-05,1.25E-06,0.979316664,1.48E-06,0.020029508,2.68E-06
365,sentiment_analysis16,299,One may notice that the aspect information ( v t ) is actually also considered in the form of ?,Result Analysis,Result Analysis,sentiment_analysis,16,41,1,0,,0.000322537,0,negative,0.000248149,1.55E-06,9.05E-06,2.77E-07,4.38E-07,1.77E-05,9.91E-06,0.000129365,2.05E-06,0.997797792,1.92E-07,0.001782247,1.24E-06
366,sentiment_analysis16,300,i W c i + W v tin MNs and wonder whether W v t may help address the problem given different v t .,Result Analysis,Result Analysis,sentiment_analysis,16,42,1,0,,0.000479343,0,negative,0.000222032,1.37E-06,3.61E-05,4.22E-08,2.82E-07,3.78E-06,7.30E-06,2.18E-05,9.58E-07,0.993533356,2.98E-07,0.006172115,6.02E-07
367,sentiment_analysis16,301,"Let us assume it helps , which means in the above example an MN makes W v resolution favor the positive class and W v price favor the negative class .",Result Analysis,Result Analysis,sentiment_analysis,16,43,1,0,,0.000274151,0,negative,0.00127695,4.09E-06,6.12E-05,1.23E-07,9.04E-07,1.03E-05,1.73E-05,9.30E-05,4.09E-06,0.990224756,9.07E-08,0.008305518,1.58E-06
368,sentiment_analysis16,302,"But then we will have trouble when the context word is "" lower "" , where it requires W v resolution to favor the negative class and W v price to favor the positive class .",Result Analysis,Result Analysis,sentiment_analysis,16,44,1,0,,0.000354796,0,negative,0.000388089,5.70E-07,1.22E-05,8.50E-08,3.23E-07,6.67E-06,1.13E-05,2.99E-05,6.71E-07,0.994660555,1.41E-07,0.004888613,8.88E-07
369,sentiment_analysis16,303,This contradiction reflects the theoretical problem discussed in Section 3 .,Result Analysis,Result Analysis,sentiment_analysis,16,45,1,0,,0.000207576,0,negative,7.70E-05,1.27E-06,3.31E-06,2.39E-07,3.98E-07,1.56E-05,1.06E-05,0.000133205,2.36E-06,0.998440706,3.16E-07,0.001313064,1.89E-06
370,sentiment_analysis16,304,Other Examples :,Result Analysis,Result Analysis,sentiment_analysis,16,46,1,0,,0.000576414,0,negative,2.09E-05,1.67E-07,1.01E-05,1.36E-08,1.31E-07,2.06E-06,9.49E-06,1.11E-05,1.24E-07,0.997542435,4.87E-08,0.002403137,3.62E-07
371,sentiment_analysis16,305,"We also found other interesting target - sensitive sentiment expressions like "" large bill "" and "" large portion "" , "" small tip "" and "" small portion "" from Restaurant .",Result Analysis,Result Analysis,sentiment_analysis,16,47,1,0,,0.035241814,0,negative,0.000469332,3.09E-06,2.45E-05,2.78E-06,2.60E-05,6.63E-05,0.000206147,0.000351922,7.87E-07,0.976857801,1.96E-07,0.021969414,2.17E-05
372,sentiment_analysis16,306,Notice that TMNs can also improve the neutral sentiment ( see ) .,Result Analysis,Result Analysis,sentiment_analysis,16,48,1,0,,0.81087291,1,results,0.011523622,3.20E-07,2.82E-05,8.43E-08,3.10E-07,3.56E-06,0.000296988,2.52E-05,1.04E-07,0.134199363,1.03E-07,0.853915962,6.25E-06
373,sentiment_analysis16,307,"For instance , TMN generates a sentiment score vector of the context "" over "" for target aspect price : { 0.1373 , 0.0066 , - 0.1433 } ( negative ) and for target aspect dinner : { 0.0496 , 0.0591 , - 0.1128 } ( neutral ) accurately .",Result Analysis,Result Analysis,sentiment_analysis,16,49,1,0,,0.007794508,0,negative,0.000247938,3.83E-06,0.000314066,5.39E-08,1.61E-06,1.06E-05,4.19E-05,9.30E-05,2.33E-06,0.987293135,5.70E-08,0.011987703,3.76E-06
374,sentiment_analysis16,308,"But MN produces both negative scores { 0.0069 , 0.0025 , - 0.0090 } ( negative ) and { 0.0078 , 0.0028 , - 0.0102 } ( negative ) for the two different targets .",Result Analysis,Result Analysis,sentiment_analysis,16,50,1,0,,0.346714571,0,negative,0.014830821,1.46E-06,4.13E-05,8.68E-08,1.22E-06,5.24E-06,0.000177475,5.98E-05,3.96E-07,0.525745888,6.70E-08,0.459131488,4.76E-06
375,sentiment_analysis16,309,The latter one in MN is incorrect .,Result Analysis,Result Analysis,sentiment_analysis,16,51,1,0,,0.000248969,0,negative,3.86E-05,4.04E-07,3.03E-06,9.80E-08,2.76E-07,9.04E-06,8.25E-06,7.09E-05,4.38E-07,0.998742266,1.41E-07,0.001125429,1.21E-06
376,sentiment_analysis16,310,Conclusion and Future Work,,,sentiment_analysis,16,0,1,0,,0.000993755,0,negative,6.55E-05,5.57E-05,6.74E-06,8.33E-07,5.66E-07,8.35E-05,7.13E-05,0.000591015,4.69E-05,0.993684537,0.004983168,0.000405376,4.84E-06
377,natural_language_inference4,1,title,,,natural_language_inference,4,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
378,natural_language_inference4,2,DCN + : MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING,title,title,natural_language_inference,4,1,1,1,research-problem,0.999225813,1,research-problem,3.66E-08,8.92E-06,1.71E-07,1.04E-07,6.84E-08,1.45E-07,2.02E-06,1.62E-06,8.77E-07,0.001635336,0.998350375,2.32E-07,9.23E-08
379,natural_language_inference4,3,abstract,,,natural_language_inference,4,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
380,natural_language_inference4,4,"Traditional models for question answering optimize using cross entropy loss , which encourages exact answers at the cost of penalizing nearby or overlapping answers thatare sometimes equally accurate .",abstract,abstract,natural_language_inference,4,1,1,0,,0.418549399,0,research-problem,4.58E-08,2.12E-05,3.14E-08,1.12E-06,4.77E-07,4.37E-07,5.88E-07,3.08E-06,5.77E-07,0.015839392,0.98413296,3.68E-08,5.99E-08
381,natural_language_inference4,5,We propose a mixed objective that combines cross entropy loss with self - critical policy learning .,abstract,abstract,natural_language_inference,4,2,1,0,,0.129013135,0,approach,6.14E-05,0.415060117,0.000174833,4.27E-05,0.000437077,6.16E-05,2.15E-05,0.000984869,0.040962603,0.270810359,0.271363804,1.43E-05,4.90E-06
382,natural_language_inference4,6,The objective uses rewards derived from word overlap to solve the mis alignment between evaluation metric and optimization objective .,abstract,abstract,natural_language_inference,4,3,1,0,,0.08686461,0,negative,1.86E-05,0.244770493,0.000221294,5.11E-06,0.000112811,5.70E-05,8.08E-06,0.001150972,0.164189249,0.501462425,0.087999301,3.40E-06,1.27E-06
383,natural_language_inference4,7,"In addition to the mixed objective , we improve dynamic coattention networks ( DCN ) with a deep residual coattention encoder that is inspired by recent work in deep self - attention and residual networks .",abstract,abstract,natural_language_inference,4,4,1,0,,0.433337372,0,research-problem,0.000102758,0.368352827,0.001092318,5.69E-05,0.000741197,9.13E-05,9.08E-05,0.000933068,0.032331525,0.156510422,0.4396316,5.16E-05,1.37E-05
384,natural_language_inference4,8,"Our proposals improve model performance across question types and input lengths , especially for long questions that requires the ability to capture long - term dependencies .",abstract,abstract,natural_language_inference,4,5,1,0,,0.031945177,0,negative,0.000186939,0.007368185,8.67E-06,5.30E-05,0.000140166,4.64E-05,7.70E-05,0.000617362,0.000311997,0.587344166,0.403548326,0.000288813,9.02E-06
385,natural_language_inference4,9,"On the Stanford Question Answering Dataset , our model achieves state - of - the - art results with 75.1 % exact match accuracy and 83.1 % F1 , while the ensemble obtains 78.9 % exact match accuracy and 86.0 % F1 .",abstract,abstract,natural_language_inference,4,6,1,0,,0.020135577,0,negative,8.11E-05,0.002943408,1.26E-05,2.73E-05,0.000185021,4.04E-05,0.000306657,0.000560384,3.13E-05,0.583173867,0.411351353,0.001274584,1.21E-05
386,natural_language_inference4,10,INTRODUCTION,,,natural_language_inference,4,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
387,natural_language_inference4,11,Existing state - of - the - art question answering models are trained to produce exact answer spans for a question and a document .,INTRODUCTION,INTRODUCTION,natural_language_inference,4,1,1,0,,0.858717962,1,research-problem,5.72E-07,0.000154182,2.23E-07,5.76E-06,1.20E-05,4.80E-06,7.71E-06,6.10E-06,3.01E-05,0.040558304,0.959218395,6.16E-07,1.26E-06
388,natural_language_inference4,12,"In this setting , aground truth answer used to supervise the model is defined as a start and an end position within the document .",INTRODUCTION,INTRODUCTION,natural_language_inference,4,2,1,0,,0.27351643,0,model,7.23E-06,0.106786372,3.10E-05,3.30E-06,0.000279776,5.48E-05,1.17E-05,0.000143962,0.637007057,0.22644063,0.029228146,4.29E-06,1.66E-06
389,natural_language_inference4,13,Existing training approaches optimize using cross entropy loss over the two positions .,INTRODUCTION,INTRODUCTION,natural_language_inference,4,3,1,0,,0.087310176,0,research-problem,1.14E-05,0.015923083,7.72E-06,9.55E-05,0.000228849,0.000301472,6.58E-05,0.00076713,0.007737802,0.477316355,0.497521875,7.86E-06,1.51E-05
390,natural_language_inference4,14,"However , this suffers from a fundamental disconnect between the optimization , which is tied to the position of a particular ground truth answer span , and the evaluation , which is based on the textual content of the answer .",INTRODUCTION,INTRODUCTION,natural_language_inference,4,4,1,0,,0.145889776,0,research-problem,4.83E-06,0.001219661,1.33E-06,2.16E-05,0.000122628,2.13E-05,1.36E-05,2.10E-05,0.000300627,0.435629132,0.562639285,3.11E-06,1.87E-06
391,natural_language_inference4,15,"This disconnect is especially harmful in cases where answers thatare textually similar to , but distinct in positions from , the ground truth are penalized in the same fashion as answers thatare textually dissimilar .",INTRODUCTION,INTRODUCTION,natural_language_inference,4,5,1,0,,0.104596904,0,negative,0.000308825,0.023048383,1.57E-05,1.37E-05,0.000752013,6.32E-05,3.24E-05,9.64E-05,0.017186155,0.939298797,0.019138022,4.45E-05,1.87E-06
392,natural_language_inference4,16,"For example , suppose we are given the sentence "" Some believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history "" , the question "" which team is considered to be one of the greatest teams in NBA history "" , and aground truth answer of "" the Golden State Warriors team of 2017 "" .",INTRODUCTION,INTRODUCTION,natural_language_inference,4,6,1,0,,0.008916728,0,negative,2.57E-06,0.00207606,1.20E-06,1.08E-05,0.000254399,8.61E-05,1.60E-05,7.83E-05,0.003899662,0.943836003,0.049735095,2.20E-06,1.68E-06
393,natural_language_inference4,17,"The span "" Warriors "" is also a correct answer , but from the perspective of traditional cross entropy based training it is no better than the span "" history "" .",INTRODUCTION,INTRODUCTION,natural_language_inference,4,7,1,0,,0.002432142,0,negative,9.00E-06,0.000392621,1.40E-06,6.09E-05,0.00111519,0.000190811,4.63E-05,6.91E-05,0.000202004,0.984924408,0.012977843,6.85E-06,3.46E-06
394,natural_language_inference4,18,"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning .",INTRODUCTION,INTRODUCTION,natural_language_inference,4,8,1,1,model,0.949018401,1,model,3.04E-05,0.349294137,7.30E-05,3.08E-06,0.000400005,2.48E-05,1.76E-05,6.88E-05,0.637812468,0.010363073,0.001903057,7.34E-06,2.16E-06
395,natural_language_inference4,19,We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,INTRODUCTION,INTRODUCTION,natural_language_inference,4,9,1,1,model,0.893752547,1,model,1.87E-05,0.334518158,3.37E-05,1.49E-06,0.000188942,1.64E-05,1.29E-05,8.33E-05,0.638138391,0.023218102,0.003760703,7.94E-06,1.35E-06
396,natural_language_inference4,20,Our mixed objective brings two benefits : ( i ) the reinforcement learning objective encourages answers thatare textually similar to the ground truth answer and discourages those thatare not ; ( ii ) the cross entropy objective significantly facilitates policy learning by encouraging trajectories thatare known to be correct .,INTRODUCTION,INTRODUCTION,natural_language_inference,4,10,1,0,,0.726133134,1,approach,0.002696624,0.538444749,0.000262234,3.87E-05,0.003089079,0.000166261,0.000581689,0.000511409,0.297465638,0.151397338,0.004739286,0.000582875,2.41E-05
397,natural_language_inference4,21,The resulting objective is one that is both faithful to the evaluation metric and converges quickly in practice .,INTRODUCTION,INTRODUCTION,natural_language_inference,4,11,1,0,,0.519762526,1,model,3.53E-05,0.344514928,2.23E-05,3.71E-06,0.000318388,4.81E-05,4.11E-05,0.000354719,0.572442475,0.075508631,0.006686245,2.06E-05,3.57E-06
398,natural_language_inference4,22,"In addition to our mixed training objective , we extend the Dynamic Coattention Network ( DCN ) by with a deep residual coattention encoder .",INTRODUCTION,INTRODUCTION,natural_language_inference,4,12,1,1,model,0.961307365,1,model,3.57E-05,0.185231249,0.000345039,2.84E-06,0.000458473,3.16E-05,4.83E-05,5.01E-05,0.80546006,0.007313965,0.001007292,1.10E-05,4.43E-06
399,natural_language_inference4,23,This allows the network to build richer representations of the input by enabling each input sequence to attend to previous attention contexts .,INTRODUCTION,INTRODUCTION,natural_language_inference,4,13,1,0,,0.829155887,1,model,1.13E-05,0.04431618,3.49E-05,3.16E-07,6.30E-05,8.71E-06,4.54E-06,1.85E-05,0.930170915,0.024996943,0.000372128,2.16E-06,4.23E-07
400,natural_language_inference4,24,show that the stacking of attention layers helps model long - range BiLSTM1 BiLSTM1,INTRODUCTION,INTRODUCTION,natural_language_inference,4,14,1,0,,0.067973424,0,negative,0.000141819,0.010008623,9.40E-05,3.03E-05,0.000346612,0.000265267,0.000669893,0.000224326,0.045045002,0.83484051,0.108100439,0.000203468,2.98E-05
401,natural_language_inference4,25,Coattention1 BiLSTM2,INTRODUCTION,,natural_language_inference,4,15,1,0,,0.082851521,0,negative,0.000109513,0.022124382,0.000103455,0.001557664,0.007888566,0.002434937,0.000625345,0.001140971,0.056527325,0.880541272,0.026805959,5.16E-05,8.90E-05
402,natural_language_inference4,26,BiLSTM2,INTRODUCTION,,natural_language_inference,4,16,1,0,,0.082724168,0,negative,7.35E-05,0.021470859,0.000506506,9.02E-05,0.001117408,0.001795266,0.001943488,0.001119976,0.225722253,0.679351142,0.066520557,0.000156522,0.000132268
403,natural_language_inference4,27,Output BiLSTM dependencies .,INTRODUCTION,,natural_language_inference,4,17,1,0,,0.011117483,0,negative,3.48E-05,0.013330238,6.23E-05,1.22E-06,6.53E-05,8.06E-05,5.34E-05,9.30E-05,0.480477972,0.49965592,0.006125176,1.80E-05,2.11E-06
404,natural_language_inference4,28,We merge coattention outputs from each layer by means of residual connections to reduce the length of signal paths .,INTRODUCTION,Output BiLSTM dependencies .,natural_language_inference,4,18,1,0,,0.543507826,1,negative,0.001209309,0.002688422,0.001328435,2.02E-06,3.89E-05,4.17E-05,3.04E-05,9.44E-05,0.066992763,0.927254383,3.45E-05,0.000279973,4.71E-06
405,natural_language_inference4,29,show that skip layer connections facilitate signal propagation and alleviate gradient degradation .,INTRODUCTION,Output BiLSTM dependencies .,natural_language_inference,4,19,1,0,,0.017545868,0,negative,0.000506743,4.49E-05,0.00015645,3.89E-07,3.45E-06,7.39E-06,1.37E-05,1.62E-05,0.000507149,0.997627966,3.00E-05,0.001084453,1.17E-06
406,natural_language_inference4,30,"The combination of the deep residual coattention encoder and the mixed objective leads to higher performance across question types , question lengths , and answer lengths on the Stanford Question Answering Dataset ( SQuAD ) compared to our DCN baseline .",INTRODUCTION,Output BiLSTM dependencies .,natural_language_inference,4,20,1,0,,0.193883081,0,negative,0.004519456,0.000513377,0.000146824,2.22E-06,1.02E-05,5.76E-05,0.000843891,0.000315699,0.000527291,0.832631124,0.000264872,0.160138682,2.87E-05
407,natural_language_inference4,31,"The improvement is especially apparent on long questions , which require the model to capture long - range dependencies between the document and the question .",INTRODUCTION,Output BiLSTM dependencies .,natural_language_inference,4,21,1,0,,0.154605716,0,negative,0.018983953,6.16E-05,2.50E-05,3.43E-06,1.38E-05,4.55E-05,0.00046001,0.000194402,7.56E-05,0.874492162,2.29E-05,0.10560855,1.31E-05
408,natural_language_inference4,32,"Our model , which we call DCN + , achieves state - of - the - art results on SQuAD , with 75.1 % exact match accuracy and 83.1 % F1 .",INTRODUCTION,Output BiLSTM dependencies .,natural_language_inference,4,22,1,0,,0.060488879,0,negative,0.000880862,0.000450612,0.000453126,6.14E-06,5.08E-05,9.58E-05,0.002235542,0.000271953,0.000245714,0.795811219,0.00025457,0.199104936,0.000138711
409,natural_language_inference4,33,"When ensembled , the DCN + obtains 78.9 % exact match accuracy and 86.0 % F1 .",INTRODUCTION,Output BiLSTM dependencies .,natural_language_inference,4,23,1,0,,0.046785538,0,negative,0.011381079,5.84E-05,0.000103782,1.98E-06,1.96E-05,2.79E-05,0.0015639,8.46E-05,2.66E-05,0.558469338,2.82E-05,0.428195744,3.89E-05
410,natural_language_inference4,34,DCN +,INTRODUCTION,,natural_language_inference,4,24,1,0,,0.003917666,0,negative,3.81E-05,0.021075931,8.34E-05,4.73E-06,0.000255134,0.000254549,0.000204446,0.000329796,0.160158639,0.812161734,0.005385067,3.99E-05,8.56E-06
411,natural_language_inference4,35,"We consider the question answering task in which we are given a document and a question , and are asked to find the answer in the document .",INTRODUCTION,DCN +,natural_language_inference,4,25,1,0,,0.009567238,0,negative,0.000173989,3.55E-05,0.000104247,0.000152859,1.99E-05,8.71E-05,6.17E-05,6.23E-05,0.000247685,0.983010362,0.003518464,0.001241543,0.011284452
412,natural_language_inference4,36,"Our model is based on the DCN by , which consists of a coattention encoder and a dynamic decoder .",INTRODUCTION,DCN +,natural_language_inference,4,26,1,0,,0.020279798,0,negative,0.000402925,0.000500624,0.004284437,1.56E-06,8.97E-06,1.72E-05,1.70E-05,5.16E-05,0.078685918,0.915636687,6.30E-05,0.000148342,0.000181717
413,natural_language_inference4,37,"The encoder first encodes the question and the document separately , then builds a codependent representation through coattention .",INTRODUCTION,DCN +,natural_language_inference,4,27,1,0,,0.02117842,0,negative,0.000545958,0.000125906,0.00127937,1.96E-07,2.13E-06,2.37E-06,3.67E-06,9.17E-06,0.038644599,0.959254888,6.31E-06,0.000107403,1.80E-05
414,natural_language_inference4,38,The decoder then produces a start and endpoint estimate given the coattention .,INTRODUCTION,DCN +,natural_language_inference,4,28,1,0,,0.000560827,0,negative,0.000165178,2.75E-05,0.000225267,1.24E-07,9.41E-07,1.60E-06,1.65E-06,5.44E-06,0.006417514,0.99304261,4.08E-06,9.68E-05,1.13E-05
415,natural_language_inference4,39,"The DCN decoder is dynamic in the sense that it iteratively estimates the start and end positions , stopping when estimates between iterations converge to the same positions or when a predefined maximum number of iterations is reached .",INTRODUCTION,DCN +,natural_language_inference,4,29,1,0,,0.034253279,0,negative,0.000672678,0.00048236,0.003047996,1.29E-06,8.28E-06,1.26E-05,1.29E-05,5.06E-05,0.077778081,0.917703741,2.10E-05,0.000119671,8.88E-05
416,natural_language_inference4,40,We make two significant changes to the DCN by introducing a deep residual coattention encoder and a mixed training objective that combines cross entropy loss from maximum likelihood estimation and reinforcement learning rewards from self - critical policy learning .,INTRODUCTION,DCN +,natural_language_inference,4,30,1,0,,0.024556489,0,negative,0.022628961,0.001775915,0.005145335,1.95E-05,8.30E-05,5.41E-05,9.35E-05,0.000118436,0.025881761,0.940053179,3.38E-05,0.00353935,0.000573153
417,natural_language_inference4,41,DEEP RESIDUAL COATTENTION ENCODER,INTRODUCTION,DCN +,natural_language_inference,4,31,1,0,,0.001462428,0,negative,0.001712428,4.09E-05,0.001924107,2.43E-05,9.55E-06,6.31E-05,5.07E-05,4.96E-05,0.008627133,0.983148513,0.000117182,0.001622285,0.002610279
418,natural_language_inference4,42,"Because it only has a single - layer coattention encoder , the DCN is limited in its ability to compose complex input representations .",INTRODUCTION,DCN +,natural_language_inference,4,32,1,0,,0.000509591,0,negative,0.000497425,1.38E-05,0.000185334,1.11E-06,2.23E-06,5.50E-06,5.38E-06,7.90E-06,0.000238289,0.998088135,2.12E-05,0.000868184,6.55E-05
419,natural_language_inference4,43,proposed stacked self - attention modules to facilitate signal traversal .,INTRODUCTION,DCN +,natural_language_inference,4,33,1,0,,8.89E-06,0,negative,0.000185382,4.83E-06,0.000121869,1.57E-06,1.20E-06,6.75E-06,4.10E-06,1.29E-05,0.000770768,0.99866846,7.04E-06,0.000111187,0.000103909
420,natural_language_inference4,44,They also showed that the network 's ability to model long - range dependencies can be improved by reducing the length of signal paths .,INTRODUCTION,DCN +,natural_language_inference,4,34,1,0,,2.86E-05,0,negative,0.000252719,1.58E-06,2.13E-05,1.84E-07,3.83E-07,2.37E-06,2.81E-06,5.03E-06,6.20E-05,0.999283881,2.01E-06,0.00035334,1.23E-05
421,natural_language_inference4,45,We propose two modifications to the coattention encoder to leverage these findings .,INTRODUCTION,DCN +,natural_language_inference,4,35,1,0,,0.007513679,0,negative,0.004608911,0.00035018,0.001886288,3.49E-06,2.18E-05,1.30E-05,1.64E-05,3.04E-05,0.028604062,0.963669371,7.37E-06,0.000646639,0.000142163
422,natural_language_inference4,46,"First , we extend the coattention encoder with self - attention by stacking coattention layers .",INTRODUCTION,DCN +,natural_language_inference,4,36,1,0,,0.14006503,0,negative,0.009819742,0.00076541,0.003077597,5.99E-06,3.36E-05,2.22E-05,6.73E-05,6.04E-05,0.009658507,0.97199579,1.36E-05,0.004161165,0.000318721
423,natural_language_inference4,47,This allows the network to build richer representations over the input .,INTRODUCTION,DCN +,natural_language_inference,4,37,1,0,,0.000110461,0,negative,5.38E-05,3.19E-06,2.44E-05,2.67E-08,1.70E-07,3.70E-07,3.37E-07,1.87E-06,0.000530389,0.999350248,2.35E-07,3.30E-05,1.93E-06
424,natural_language_inference4,48,"Second , we merge coattention outputs from each layer with residual connections .",INTRODUCTION,DCN +,natural_language_inference,4,38,1,0,,0.051240581,0,negative,0.003813623,0.000186181,0.002200313,7.61E-07,1.00E-05,6.78E-06,1.33E-05,1.90E-05,0.012345822,0.980896255,1.34E-06,0.000449478,5.71E-05
425,natural_language_inference4,49,This reduces the length of signal paths .,INTRODUCTION,,natural_language_inference,4,39,1,0,,0.140492075,0,negative,0.000787724,0.030927569,0.000188861,2.29E-05,0.004055512,0.000233303,0.000194482,0.000137333,0.050181911,0.913054707,0.000135011,7.44E-05,6.22E-06
426,natural_language_inference4,50,Our encoder is shown in .,INTRODUCTION,,natural_language_inference,4,40,1,0,,0.186920433,0,model,2.07E-05,0.037467037,9.55E-05,4.95E-05,0.001334976,0.000689826,0.000233179,0.000657609,0.511626455,0.446837911,0.000935098,1.04E-05,4.19E-05
427,natural_language_inference4,51,Suppose we are given a document of m words and a question of n words .,INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,41,1,0,,1.45E-05,0,negative,3.26E-06,0.000671025,4.09E-06,7.68E-07,0.000726032,0.000169478,1.30E-05,1.66E-05,0.003327892,0.995038068,2.73E-05,6.33E-07,1.83E-06
428,natural_language_inference4,52,Let L D ?,INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,42,1,0,,1.17E-05,0,negative,1.76E-05,0.000193404,2.36E-06,8.30E-07,0.000287245,0.00027057,2.16E-05,1.50E-05,0.000625176,0.998560389,4.00E-06,1.11E-06,6.66E-07
429,natural_language_inference4,53,R em and L Q ?,INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,43,1,0,,5.69E-05,0,negative,9.01E-05,0.000376347,8.59E-06,1.11E-06,0.000731951,0.000218327,3.76E-05,1.23E-05,0.000647038,0.997866614,5.47E-06,3.61E-06,9.15E-07
430,natural_language_inference4,54,"R en respectively denote the word embeddings for the document and the question , where e is the dimension of the word embeddings .",INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,44,1,0,,3.38E-05,0,negative,4.87E-06,0.001487131,2.75E-06,2.88E-07,0.000162737,9.73E-05,8.75E-06,2.24E-05,0.009918519,0.988287406,6.89E-06,4.26E-07,4.94E-07
431,natural_language_inference4,55,"We obtain document encodings E D 1 and question encodings E Q 1 through a bidirectional Long Short - Term Memory Network ( LSTM ) , where we use integer subscripts to denote the coattention layer number .",INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,45,1,0,,8.20E-05,0,model,2.07E-05,0.014879876,0.000135204,1.68E-07,0.000287289,4.53E-05,1.73E-05,7.53E-06,0.868904092,0.115688206,1.17E-05,9.36E-07,1.60E-06
432,natural_language_inference4,56,"Here , h denotes the hidden state size and the + 1 indicates the presence of an additional sentinel word which allows the coattention to not focus on any part of the input .",INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,46,1,0,,1.49E-05,0,negative,1.77E-05,0.001609519,6.92E-06,8.05E-07,0.000256848,0.000215101,1.42E-05,3.98E-05,0.027671747,0.97016204,3.97E-06,4.34E-07,9.90E-07
433,natural_language_inference4,57,"Like the original DCN , we add a non-linear transform to the question encoding .",INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,47,1,0,,0.00503102,0,model,0.000133045,0.043805057,0.000297354,5.42E-07,0.000816877,8.66E-05,3.90E-05,1.34E-05,0.898671438,0.056127943,4.39E-06,1.68E-06,2.67E-06
434,natural_language_inference4,58,We compute the affinity matrix between the document and the question as,INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,48,1,0,,5.24E-05,0,negative,2.50E-05,0.002349917,1.37E-05,4.06E-07,0.000369388,0.000159693,1.49E-05,3.03E-05,0.04808763,0.948943564,3.49E-06,8.62E-07,1.09E-06
435,natural_language_inference4,59,.,INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,49,1,0,,3.93E-06,0,negative,1.54E-06,4.00E-05,3.81E-07,5.86E-08,1.75E-05,3.05E-05,1.92E-06,3.07E-06,0.000932013,0.998972047,6.95E-07,9.20E-08,1.07E-07
436,natural_language_inference4,60,Let softmax ( X ) denote the softmax operation over the matrix X that normalizes X colum n - wise .,INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,50,1,0,,5.58E-05,0,negative,2.64E-06,0.000683161,4.73E-06,5.45E-08,5.84E-05,6.07E-05,6.15E-06,8.93E-06,0.010779504,0.988393423,1.75E-06,2.40E-07,2.77E-07
437,natural_language_inference4,61,The document summary vectors and question summary vectors are computed as,INTRODUCTION,Our encoder is shown in .,natural_language_inference,4,51,1,0,,7.53E-05,0,negative,1.51E-05,0.002304514,7.18E-06,1.55E-07,0.000131136,0.000168845,2.33E-05,4.70E-05,0.051562074,0.945734431,4.13E-06,8.80E-07,1.19E-06
438,natural_language_inference4,62,We define the document coattention context as follows .,INTRODUCTION,,natural_language_inference,4,52,1,0,,0.014967038,0,negative,8.26E-06,0.05423315,2.79E-05,5.43E-06,0.001011167,0.000243879,0.000126776,0.000532926,0.173928651,0.769760778,0.000104044,8.33E-06,8.74E-06
439,natural_language_inference4,63,Note that we drop the dimension corresponding to the sentinel vector - it has already been used during the summary computation and is not a potential position candidate for the decoder .,INTRODUCTION,We define the document coattention context as follows .,natural_language_inference,4,53,1,0,,0.000814083,0,negative,0.000409694,7.02E-05,2.31E-05,2.01E-06,0.000179762,4.46E-05,1.38E-05,1.24E-05,0.001060057,0.998161211,1.77E-07,7.11E-06,1.60E-05
440,natural_language_inference4,64,We further encode the summaries using another bidirectional LSTM .,INTRODUCTION,We define the document coattention context as follows .,natural_language_inference,4,54,1,0,,0.227740365,0,model,0.000534639,0.00237863,0.00243372,1.05E-06,0.000279979,7.27E-05,0.000100796,2.61E-05,0.552499356,0.441335019,3.18E-06,1.12E-05,0.000323642
441,natural_language_inference4,65,Equation to equation 5 describe a single coattention layer .,INTRODUCTION,We define the document coattention context as follows .,natural_language_inference,4,55,1,0,,0.002457644,0,negative,0.000119789,4.47E-05,5.40E-05,2.88E-06,0.000110053,8.39E-05,3.02E-05,2.27E-05,0.005745617,0.993605614,1.41E-06,5.32E-06,0.000173787
442,natural_language_inference4,66,We compute the second coattention layer in a similar fashion .,INTRODUCTION,We define the document coattention context as follows .,natural_language_inference,4,56,1,0,,0.00146258,0,negative,0.000217432,0.000306856,0.000160976,6.34E-07,8.69E-05,4.82E-05,3.26E-05,1.87E-05,0.02426257,0.974790848,4.48E-07,6.11E-06,6.76E-05
443,natural_language_inference4,67,"Namely , let coattn denote a multi-valued mapping whose inputs are the two input sequences E D 1 and E Q 1 .",INTRODUCTION,We define the document coattention context as follows .,natural_language_inference,4,57,1,0,,0.000529621,0,negative,1.26E-05,5.58E-05,1.52E-05,2.40E-07,3.48E-05,1.80E-05,6.71E-06,9.85E-06,0.002948466,0.99687371,4.41E-07,1.46E-06,2.27E-05
444,natural_language_inference4,68,We have,INTRODUCTION,,natural_language_inference,4,58,1,0,,0.000629676,0,negative,1.29E-05,0.000898624,2.68E-06,1.32E-05,0.000590027,0.000278929,0.000156649,0.000178068,0.001144364,0.996701746,1.01E-05,8.28E-06,4.40E-06
445,natural_language_inference4,69,The output of our encoder is then obtained as,INTRODUCTION,We have,natural_language_inference,4,59,1,0,,0.000300045,0,negative,9.69E-06,0.001702632,1.28E-05,7.23E-07,0.000395137,7.17E-05,1.77E-05,2.64E-05,0.024904793,0.972849383,5.10E-06,9.42E-07,2.95E-06
446,natural_language_inference4,70,"where concat ( A , B ) denotes the concatenation between the matrices A and B along the first dimension .",INTRODUCTION,We have,natural_language_inference,4,60,1,0,,0.000135321,0,negative,2.24E-06,0.000279097,6.18E-06,2.74E-07,0.000262657,5.24E-05,8.78E-06,8.46E-06,0.001529705,0.997848027,8.38E-07,2.49E-07,1.09E-06
447,natural_language_inference4,71,This encoder is different than the original DCN in its depth and it s use of residual connections .,INTRODUCTION,We have,natural_language_inference,4,61,1,0,,0.006726521,0,model,9.13E-05,0.022344085,0.003798111,1.97E-06,0.00348973,0.000146475,0.000118805,1.96E-05,0.66065285,0.309300583,1.05E-05,4.39E-06,2.16E-05
448,natural_language_inference4,72,"We use not only the output of the deep coattention network CD 2 as input to the final bidirectional LSTM , but add skip connections to initial encodings E D 1 , E D 2 , summary vectors SD 1 , SD 2 , and coattention context CD 1 .",INTRODUCTION,We have,natural_language_inference,4,62,1,0,,0.002057629,0,negative,0.000117219,0.027802786,0.000270715,2.02E-06,0.003884397,0.000382598,0.000203393,0.000185263,0.286610804,0.680522127,1.08E-06,2.37E-06,1.52E-05
449,natural_language_inference4,73,"This is akin to transformer networks , which achieved stateof - the - art results on machine translation using deep self - attention layers to help model long - range dependencies , and residual networks , which achieved state - of - the - art results in image classification through the addition of skip layer connections to facilitate signal propagation and alleviate gradient degradation .",INTRODUCTION,We have,natural_language_inference,4,63,1,0,,0.043819177,0,negative,7.40E-05,0.018471881,0.001268982,1.86E-06,0.004052307,0.000131992,0.000100389,1.40E-05,0.086358085,0.889497719,1.10E-05,6.09E-06,1.16E-05
450,natural_language_inference4,74,MIXED OBJECTIVE USING SELF - CRITICAL POLICY LEARNING,INTRODUCTION,We have,natural_language_inference,4,64,1,0,,0.171605012,0,negative,5.30E-05,0.018888582,0.00015471,6.32E-06,0.000484849,0.000689945,0.00396606,0.000302873,0.067193559,0.905596896,0.002232917,8.66E-05,0.000343699
451,natural_language_inference4,75,The DCN produces a distribution over the start position of the answer and a distribution over the end position of the answer .,INTRODUCTION,We have,natural_language_inference,4,65,1,0,,0.045223652,0,model,5.06E-06,0.009985345,0.000116085,6.87E-08,0.000185628,3.04E-05,2.39E-05,1.55E-05,0.714877038,0.27475464,2.77E-06,4.13E-07,3.12E-06
452,natural_language_inference4,76,Let sand e denote the respective start and end points of the ground truth answer .,INTRODUCTION,We have,natural_language_inference,4,66,1,0,,0.000128986,0,negative,1.75E-06,0.000376808,2.15E-06,5.02E-08,0.000159559,1.23E-05,3.79E-06,5.35E-06,0.002035977,0.997401598,1.52E-07,2.75E-07,2.53E-07
453,natural_language_inference4,77,"Because the decoder of the DCN is dynamic , we denote the start and end distributions produced at the tth decoding step by p start t ?",INTRODUCTION,We have,natural_language_inference,4,67,1,0,,0.000417568,0,negative,1.29E-05,0.004579463,1.80E-05,7.89E-08,0.000454584,3.51E-05,4.03E-05,1.36E-05,0.010560504,0.984282045,4.89E-07,1.98E-06,9.22E-07
454,natural_language_inference4,78,R m and pend t ?,INTRODUCTION,We have,natural_language_inference,4,68,1,0,,0.001300203,0,negative,4.59E-05,0.000347365,1.07E-05,3.81E-06,0.00268193,0.000482912,0.000221627,5.00E-05,0.000305506,0.995832171,9.14E-07,2.86E-06,1.44E-05
455,natural_language_inference4,79,R m .,INTRODUCTION,,natural_language_inference,4,69,1,0,,0.001522776,0,negative,1.08E-05,0.000465788,3.83E-06,4.78E-06,0.000414308,0.000202881,0.000239997,0.000100193,0.000281457,0.998262736,2.29E-06,7.38E-06,3.55E-06
456,natural_language_inference4,80,"For convenience , we denote the greedy estimate of the start and end positions by the model at the tth decoding step by st and e t .",INTRODUCTION,R m .,natural_language_inference,4,70,1,0,,0.000609533,0,negative,1.49E-06,0.000675912,1.77E-06,2.49E-07,0.000120965,5.70E-05,7.32E-06,7.25E-05,0.003452636,0.995608617,3.70E-07,5.42E-07,6.44E-07
457,natural_language_inference4,81,"Moreover , let ?",INTRODUCTION,R m .,natural_language_inference,4,71,1,0,,4.98E-05,0,negative,3.70E-06,4.91E-05,9.72E-07,1.72E-07,8.84E-05,2.74E-05,3.33E-06,1.09E-05,0.000157973,0.999656979,9.12E-08,7.95E-07,1.76E-07
458,natural_language_inference4,82,denote the parameters of the model .,INTRODUCTION,R m .,natural_language_inference,4,72,1,0,,0.001346521,0,negative,1.88E-06,0.000302571,8.11E-07,2.45E-07,6.87E-05,9.98E-05,1.21E-05,0.000230149,0.001425835,0.997855776,2.95E-07,7.57E-07,1.11E-06
459,natural_language_inference4,83,"Similar to other question answering models , the DCN is supervised using the cross entropy loss on the start position distribution and the end position distribution :",INTRODUCTION,R m .,natural_language_inference,4,73,1,0,,0.082974345,0,negative,0.000123501,0.084106948,0.002313463,1.32E-06,0.002673219,0.000224217,0.000185249,0.000143099,0.281464249,0.628731449,2.63E-06,1.40E-05,1.66E-05
460,natural_language_inference4,84,Equation 11 states that the model accumulates across entropy loss over each position during each decoding step given previous estimates of the start and end positions .,INTRODUCTION,R m .,natural_language_inference,4,74,1,0,,0.002851186,0,negative,2.19E-05,0.001815858,2.62E-05,2.70E-07,0.000209736,7.58E-05,1.83E-05,7.28E-05,0.07124234,0.926512763,2.79E-07,1.54E-06,2.18E-06
461,natural_language_inference4,85,The question answering task consists of two evaluation metrics .,INTRODUCTION,R m .,natural_language_inference,4,75,1,0,,0.006839752,0,negative,2.12E-05,0.007885413,0.000191269,3.21E-06,0.007022354,0.000169615,0.000243676,4.49E-05,0.001339398,0.982992467,2.14E-05,4.46E-05,2.05E-05
462,natural_language_inference4,86,"The first , exact match , is a binary score that denotes whether the answer span produced by the model has exact string match with the ground truth answer span .",INTRODUCTION,R m .,natural_language_inference,4,76,1,0,,0.029928892,0,negative,3.87E-05,0.011114794,0.000584077,4.62E-07,0.002845693,0.000218205,9.91E-05,7.77E-05,0.039832003,0.945177019,2.28E-07,6.04E-06,5.96E-06
463,natural_language_inference4,87,"The second , F1 , computes the degree of word overlap between the answer span produced by the model and the ground truth answer span .",INTRODUCTION,R m .,natural_language_inference,4,77,1,0,,0.012286129,0,negative,4.46E-05,0.005305924,0.000257387,1.61E-07,0.000747381,6.19E-05,3.56E-05,2.55E-05,0.031973084,0.96154157,1.54E-07,4.60E-06,2.13E-06
464,natural_language_inference4,88,We note that there is a disconnect between the cross entropy optimization objective and the evaluation metrics .,INTRODUCTION,R m .,natural_language_inference,4,78,1,0,,0.000220073,0,negative,4.76E-06,0.00012293,1.47E-06,8.67E-07,0.000114171,5.80E-05,4.33E-06,2.18E-05,0.000432829,0.999237426,2.07E-07,5.68E-07,6.05E-07
465,natural_language_inference4,89,"For example , suppose we are given the answer estimates A and B , neither of which match the ground truth positions .",INTRODUCTION,R m .,natural_language_inference,4,79,1,0,,6.21E-05,0,negative,5.90E-07,2.60E-05,1.27E-06,4.51E-08,7.55E-05,1.28E-05,2.09E-06,3.50E-06,8.12E-05,0.999796393,5.55E-08,3.31E-07,2.01E-07
466,natural_language_inference4,90,"However , A has an exact string match with the ground truth answer whereas B does not .",INTRODUCTION,R m .,natural_language_inference,4,80,1,0,,4.51E-05,0,negative,2.41E-06,3.30E-05,1.37E-06,4.85E-08,9.99E-05,8.87E-06,2.19E-06,2.79E-06,9.29E-05,0.999755682,3.97E-08,6.84E-07,1.36E-07
467,natural_language_inference4,91,"The cross entropy objective penalizes A and B equally , despite the former being correct under both evaluation metrics .",INTRODUCTION,R m .,natural_language_inference,4,81,1,0,,0.287873854,0,negative,0.002525805,0.020103247,0.000208837,2.49E-06,0.003363428,0.000450499,0.000727365,0.000465604,0.025350274,0.946682219,1.82E-07,9.98E-05,2.03E-05
468,natural_language_inference4,92,"In the less extreme case where A does not have exact match but has some degree of word overlap with the ground truth , the F1 metric still prefers A over B despite its wrongly predicted positions .",INTRODUCTION,R m .,natural_language_inference,4,82,1,0,,0.002334473,0,negative,0.000296437,0.000691994,2.05E-05,5.95E-07,0.000661598,0.000123634,0.00059599,6.25E-05,0.000215133,0.997141817,3.15E-07,0.000183286,6.27E-06
469,natural_language_inference4,93,"We encode this preference using reinforcement learning , using the F 1 score as the reward function .",INTRODUCTION,R m .,natural_language_inference,4,83,1,0,,0.013040962,0,model,1.79E-05,0.016973201,0.000108693,1.65E-07,0.000344751,6.24E-05,3.92E-05,8.58E-05,0.49932468,0.483037106,2.15E-07,1.61E-06,4.18E-06
470,natural_language_inference4,94,Let ? t ?,INTRODUCTION,R m .,natural_language_inference,4,84,1,0,,2.63E-05,0,negative,2.08E-06,1.60E-05,4.89E-07,9.75E-08,8.54E-05,4.37E-05,6.24E-06,1.63E-05,3.07E-05,0.999798157,1.19E-08,4.60E-07,2.98E-07
471,natural_language_inference4,95,p start t and t ?,INTRODUCTION,R m .,natural_language_inference,4,85,1,0,,0.000254701,0,negative,5.20E-06,4.38E-05,6.89E-06,4.05E-08,5.90E-05,2.89E-05,1.41E-05,7.34E-06,0.000293908,0.999538773,2.37E-08,1.47E-06,5.82E-07
472,natural_language_inference4,96,p start t denote the sampled start and end positions from the estimated distributions at decoding step t.,INTRODUCTION,R m .,natural_language_inference,4,86,1,0,,0.000323444,0,negative,1.15E-06,0.00016524,1.47E-06,5.46E-08,8.73E-05,2.98E-05,5.96E-06,3.02E-05,0.00055168,0.999126399,1.41E-08,3.47E-07,4.04E-07
473,natural_language_inference4,97,We define a trajectory ?,INTRODUCTION,R m .,natural_language_inference,4,87,1,0,,0.000111694,0,negative,6.06E-07,1.57E-05,4.87E-07,4.70E-07,6.69E-05,6.34E-05,4.02E-06,1.53E-05,4.14E-05,0.999790922,3.37E-08,1.98E-07,6.09E-07
474,natural_language_inference4,98,as a sequence of sampled start and end points ?,INTRODUCTION,R m .,natural_language_inference,4,88,1,0,,0.001429456,0,negative,4.78E-07,3.20E-05,8.59E-07,3.34E-08,1.42E-05,1.95E-05,4.43E-06,1.05E-05,0.000468675,0.999448585,4.96E-08,2.50E-07,3.60E-07
475,natural_language_inference4,99,t and t through all T decoder time steps .,INTRODUCTION,R m .,natural_language_inference,4,89,1,0,,0.000473049,0,negative,1.82E-05,0.000130969,1.60E-05,3.59E-08,0.000161744,1.31E-05,1.33E-05,4.43E-06,0.000465596,0.999173497,8.68E-09,2.75E-06,3.22E-07
476,natural_language_inference4,100,The reinforcement learning objective is then the negative expected rewards,INTRODUCTION,R m .,natural_language_inference,4,90,1,0,,0.000505429,0,negative,1.37E-06,0.000251628,2.18E-06,4.57E-08,2.42E-05,3.19E-05,1.17E-05,5.27E-05,0.003045542,0.996576649,1.14E-07,6.94E-07,1.28E-06
477,natural_language_inference4,101,Rover trajectories .,INTRODUCTION,,natural_language_inference,4,91,1,0,,0.019990357,0,negative,2.37E-05,0.002023683,4.76E-05,3.07E-06,0.000861226,0.000749898,0.021863793,0.000482743,0.001074974,0.972329371,5.29E-06,0.000337189,0.00019742
478,natural_language_inference4,102,"We use F 1 to denote the F1 scoring function and ans ( s , e ) to denote the answer span retrieved using the start point sand endpoint e .",INTRODUCTION,Rover trajectories .,natural_language_inference,4,92,1,0,,7.25E-05,0,negative,2.74E-07,1.64E-06,2.28E-07,4.05E-09,3.50E-07,4.49E-06,3.64E-06,1.16E-05,1.68E-05,0.999960024,1.06E-09,6.47E-07,3.05E-07
479,natural_language_inference4,103,"In equation 13 , instead of using only the F1 word overlap as the reward , we subtract from it a baseline .",INTRODUCTION,Rover trajectories .,natural_language_inference,4,93,1,0,,6.83E-05,0,negative,2.03E-05,7.34E-06,5.33E-06,5.28E-09,1.86E-06,1.58E-06,3.37E-06,2.62E-06,0.000146751,0.999806883,4.52E-10,3.74E-06,2.41E-07
480,natural_language_inference4,104,show that a good baseline reduces the variance of gradient estimates and facilitates convergence .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,94,1,0,,0.007089776,0,negative,0.000120889,1.17E-06,2.72E-06,7.81E-09,1.46E-06,2.02E-06,3.12E-05,1.51E-06,3.79E-06,0.999733718,7.34E-10,0.0001007,8.35E-07
481,natural_language_inference4,105,"In our case , we employ a self - critic ( Konda & Tsitsiklis , 1999 ) that uses the F1 score produced by the current model during greedy inference without teacher forcing .",INTRODUCTION,Rover trajectories .,natural_language_inference,4,95,1,0,,0.000954573,0,negative,5.65E-05,0.000384904,0.000141278,9.54E-08,2.25E-05,2.25E-05,5.36E-05,3.75E-05,0.007106692,0.9921548,1.17E-08,9.19E-06,1.05E-05
482,natural_language_inference4,106,"For ease of notation , we abbreviate R ( s , e , ? T , T ; ? ) as R . As per and , the expected gradient of a non-differentiable reward function can be computed as In equation 16 , we approximate the expected gradient using a single Monte - Carlo sample ?",INTRODUCTION,Rover trajectories .,natural_language_inference,4,96,1,0,,7.92E-05,0,negative,1.45E-06,7.20E-07,5.19E-07,1.19E-09,3.66E-07,7.29E-07,1.39E-06,9.73E-07,5.10E-06,0.999987286,2.35E-10,1.39E-06,7.58E-08
483,natural_language_inference4,107,drawn from p ? .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,97,1,0,,0.000107938,0,negative,1.30E-06,6.42E-07,4.29E-07,3.20E-09,4.42E-07,4.69E-06,5.65E-06,1.01E-05,1.14E-05,0.999963402,2.17E-10,1.48E-06,4.21E-07
484,natural_language_inference4,108,This sample trajectory ?,INTRODUCTION,Rover trajectories .,natural_language_inference,4,98,1,0,,0.000118543,0,negative,4.98E-07,1.01E-07,1.02E-07,2.94E-09,1.96E-07,1.64E-06,1.17E-06,1.58E-06,1.62E-06,0.999992221,1.52E-10,7.49E-07,1.32E-07
485,natural_language_inference4,109,contains the start and end positions ?,INTRODUCTION,Rover trajectories .,natural_language_inference,4,99,1,0,,0.000339255,0,negative,4.08E-07,2.57E-07,3.50E-07,3.78E-09,4.11E-07,1.62E-06,1.30E-06,1.56E-06,3.52E-06,0.999989744,2.37E-10,6.49E-07,1.89E-07
486,natural_language_inference4,110,t and t sampled during all decoding steps .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,100,1,0,,0.000132554,0,negative,1.38E-06,3.70E-07,2.69E-07,4.37E-10,1.78E-07,3.82E-07,9.22E-07,6.40E-07,4.28E-06,0.99999029,7.12E-11,1.25E-06,3.47E-08
487,natural_language_inference4,111,One of the key problems in applying RL to natural language processing is the discontinuous and discrete space the agent must explore in order to find a good policy .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,101,1,0,,0.001504222,0,negative,5.69E-06,4.20E-06,3.81E-06,7.75E-07,3.37E-06,5.09E-05,0.000104867,1.91E-05,6.24E-06,0.999737194,5.58E-07,2.95E-05,3.38E-05
488,natural_language_inference4,112,"For problems with large exploration space , RL approaches tend to be applied as fine - tuning steps after a maximum likelihood model has already been trained .",INTRODUCTION,Rover trajectories .,natural_language_inference,4,102,1,0,,0.002053448,0,negative,4.86E-06,1.74E-06,9.09E-07,1.38E-07,1.43E-06,2.36E-05,1.87E-05,2.17E-05,3.09E-06,0.999912211,1.39E-08,6.17E-06,5.39E-06
489,natural_language_inference4,113,The resulting model is constrained in its exploration during fine - tuning because it is biased by heavy pretraining .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,103,1,0,,0.004383302,0,negative,0.000160895,3.43E-05,5.98E-06,1.75E-08,5.91E-06,3.20E-06,1.21E-05,6.74E-06,0.000156673,0.999593619,6.04E-10,1.95E-05,1.02E-06
490,natural_language_inference4,114,We instead treat the optimization problem as a multi-task learning problem .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,104,1,0,,0.004337286,0,negative,4.85E-06,2.04E-05,2.34E-06,9.84E-09,1.24E-06,3.18E-06,6.54E-06,8.18E-06,0.000114037,0.999833386,3.18E-09,4.34E-06,1.50E-06
491,natural_language_inference4,115,The first task is to optimize for positional match with the ground truth answer using the the cross entropy objective .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,105,1,0,,0.004262055,0,negative,3.56E-05,4.52E-05,2.22E-05,1.63E-08,1.09E-05,2.61E-06,1.08E-05,2.94E-06,0.000150477,0.999702386,1.03E-09,1.53E-05,1.61E-06
492,natural_language_inference4,116,The second task is to optimize for word overlap with the ground truth answer with the self - critical reinforcement learning objective .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,106,1,0,,0.005228767,0,negative,2.10E-05,7.06E-05,3.01E-05,2.23E-08,1.12E-05,2.98E-06,1.80E-05,3.65E-06,0.000170612,0.999645009,3.08E-09,2.37E-05,3.08E-06
493,natural_language_inference4,117,"In a similar fashion to , we combine the two losses using homoscedastic uncertainty as task - dependent weightings .",INTRODUCTION,Rover trajectories .,natural_language_inference,4,107,1,0,,0.000717748,0,negative,2.42E-05,6.04E-05,2.92E-05,9.59E-09,4.29E-06,2.57E-06,1.00E-05,4.36E-06,0.000842919,0.99901423,4.99E-10,6.29E-06,1.52E-06
494,natural_language_inference4,118,"Here , ? ce and ?",INTRODUCTION,Rover trajectories .,natural_language_inference,4,108,1,0,,7.20E-05,0,negative,3.43E-06,4.76E-07,6.87E-07,1.19E-08,2.11E-06,3.54E-06,3.80E-06,2.61E-06,9.57E-07,0.999979321,4.47E-11,2.79E-06,2.73E-07
495,natural_language_inference4,119,rl are learned parameters .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,109,1,0,,0.000221951,0,negative,3.11E-06,4.37E-06,1.51E-06,3.03E-08,2.09E-06,2.16E-05,1.85E-05,9.96E-05,6.78E-05,0.999776695,2.38E-10,1.09E-06,3.64E-06
496,natural_language_inference4,120,The gradient of the cross entropy objective can be derived using straight - forward backpropagation .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,110,1,0,,0.000151259,0,negative,3.37E-06,4.37E-06,1.33E-06,8.69E-09,1.24E-06,4.33E-06,5.95E-06,1.35E-05,4.96E-05,0.999913731,2.05E-10,1.39E-06,1.15E-06
497,natural_language_inference4,121,The gradient of the self - critical reinforcement learning objective is shown in equation 16 . illustrates how the mixed objective is computed .,INTRODUCTION,Rover trajectories .,natural_language_inference,4,111,1,0,,0.0001962,0,negative,1.29E-06,5.56E-07,4.62E-07,3.86E-09,4.08E-07,1.81E-06,2.49E-06,3.90E-06,1.05E-05,0.9999773,8.18E-11,9.09E-07,3.84E-07
498,natural_language_inference4,122,"In practice , we find that adding the cross entropy task significantly facilitates policy learning by pruning the space of candidate trajectories - without the former , it is very difficult for policy learning to converge due to the large space of potential answers , documents , and questions .",INTRODUCTION,Rover trajectories .,natural_language_inference,4,112,1,0,,0.539587641,1,negative,0.006213845,3.92E-05,8.42E-06,1.40E-06,3.80E-05,0.000303322,0.012386872,0.00035801,2.14E-05,0.976645122,1.85E-09,0.003441526,0.000542969
499,natural_language_inference4,123,EXPERIMENTS,,,natural_language_inference,4,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
500,natural_language_inference4,124,We train and evaluate our model on the Stanford Question Answering Dataset ( SQuAD ) .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,1,1,0,,0.307150892,0,negative,0.001464006,0.003433007,0.008773555,0.000222271,0.001140035,0.005913752,0.063280031,0.030265551,0.000117622,0.833276482,0.00278752,0.046228545,0.003097623
501,natural_language_inference4,125,"We show our test performance of our model against other published models , and demonstrate the importance of our proposals via ablation studies on the development set .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,2,1,0,,0.016002861,0,negative,0.00877176,0.000101551,0.000611164,2.30E-05,7.60E-05,0.000520383,0.00358522,0.002559588,1.15E-05,0.843003702,5.94E-05,0.140523897,0.000152867
502,natural_language_inference4,126,"To preprocess the corpus , we use the reversible tokenizer from Stanford CoreNLP .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,3,1,1,experimental-setup,0.930687959,1,hyperparameters,0.000235143,0.000362237,0.000745704,0.00041077,9.04E-05,0.251338229,0.023069489,0.654526992,0.000170794,0.067377471,4.03E-05,0.000196983,0.001435501
503,natural_language_inference4,127,"For word embeddings , we use GloVe embeddings pretrained on the 840B Common Crawl corpus as well as character ngram embeddings by .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,4,1,1,experimental-setup,0.897125235,1,hyperparameters,3.18E-05,9.89E-05,3.31E-05,1.18E-05,2.90E-06,0.079817609,0.001800805,0.901886989,3.94E-05,0.016115406,6.96E-06,2.94E-05,0.000124969
504,natural_language_inference4,128,"In addition , we concatenate these embeddings with context vectors ( CoVe ) trained on .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,5,1,1,experimental-setup,0.182396554,0,negative,0.00386593,0.010077943,0.043326174,9.92E-05,0.000110317,0.026133595,0.003364298,0.319891074,0.017481016,0.572888057,0.000192012,0.001115558,0.001454844
505,natural_language_inference4,129,"For out of vocabulary words , we set the embeddings and context vectors to zero .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,6,1,1,experimental-setup,0.763194171,1,hyperparameters,0.000182735,0.00025699,0.000183938,3.90E-06,2.93E-06,0.044166949,0.001432659,0.849441106,0.000141129,0.10396764,9.98E-06,0.000117882,9.22E-05
506,natural_language_inference4,130,We perform word dropout on the document which zeros a word embedding with probability 0.075 .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,7,1,1,experimental-setup,0.752000835,1,hyperparameters,8.77E-05,0.000136766,8.71E-05,1.07E-05,2.98E-06,0.05705878,0.001621695,0.925980364,8.54E-05,0.014694562,5.87E-06,3.13E-05,0.000196786
507,natural_language_inference4,131,"In addition , we swap the first maxout layer of the highway maxout network in the DCN decoder with a sparse mixture of experts layer .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,8,1,1,experimental-setup,0.723471469,1,negative,0.10199105,0.017731065,0.126406135,0.000596239,0.000698667,0.034995763,0.021726888,0.333303679,0.011313408,0.33349966,0.00014978,0.010943803,0.006643863
508,natural_language_inference4,132,"This layer is similar to the maxout layer , except instead of taking the top scoring expert , we take the top k = 2 expert. , Document Reader , FastQA , DCN .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,9,1,0,,0.01237076,0,negative,0.003142421,0.001791505,0.104253707,0.000364486,0.000115859,0.063351478,0.017471018,0.315089001,0.011068904,0.468015417,0.000475953,0.001891442,0.012968811
509,natural_language_inference4,133,"The CoVe authors did not submit their model , which we use as our baseline , for SQuAD test evaluation .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,10,1,0,,0.001102737,0,negative,0.000133265,1.27E-05,0.000310919,1.18E-05,2.98E-05,0.000650436,0.001176599,0.00171914,3.56E-06,0.993523775,3.78E-05,0.002316476,7.38E-05
510,natural_language_inference4,134,"The performance of our model is shown in , except that it augments the word representations with context vectors trained on WMT16 .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,11,1,0,,0.488083709,0,negative,0.006515721,7.00E-05,0.002622501,1.52E-05,2.92E-05,0.003540336,0.031110376,0.019541345,1.49E-05,0.608920173,8.10E-05,0.326646792,0.000892643
511,natural_language_inference4,135,Comparison to baseline DCN with CoVe. DCN + outperforms the baseline by 3.2 % exact match accuracy and 3.2 % F1 on the SQuAD development set .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,12,1,1,results,0.883409706,1,results,0.00140449,3.51E-05,0.001531236,1.65E-06,3.72E-06,0.000490743,0.034523608,0.005455073,4.17E-06,0.169779455,0.000177753,0.785955117,0.000637902
512,natural_language_inference4,136,"shows the consistent performance gain of DCN + over the baseline across question types , question lengths , and answer lengths .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,13,1,1,results,0.259689315,0,results,0.002218337,1.11E-05,0.000527431,3.41E-06,7.72E-06,0.000245784,0.018227496,0.002051397,1.51E-06,0.146136165,8.10E-05,0.829999688,0.000488967
513,natural_language_inference4,137,"In particular , DCN + provides a significant advantage for long questions .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,4,14,1,1,results,0.800459485,1,results,0.012005481,1.01E-05,0.000117303,6.91E-06,9.30E-06,0.000201239,0.015996656,0.00228573,1.72E-06,0.018234136,1.68E-05,0.950193298,0.000921305
514,natural_language_inference4,138,Ablation study .,,,natural_language_inference,4,0,1,0,,0.021143969,0,negative,0.01075599,0.000215234,0.001479921,5.74E-05,3.77E-05,0.000197097,0.00036376,0.000395721,0.000126582,0.982033111,0.001329446,0.002984505,2.35E-05
515,natural_language_inference4,139,The contributions of each part of our model are shown in .,Ablation study .,Ablation study .,natural_language_inference,4,1,1,0,,0.001473477,0,negative,0.078223425,6.18E-06,0.000473814,5.14E-06,1.70E-05,8.83E-05,5.62E-06,7.38E-06,0.00020507,0.92087002,1.85E-05,2.31E-06,7.72E-05
516,natural_language_inference4,140,"We note that the deep residual coattention yielded the highest contribution to model performance , followed by the mixed objective .",Ablation study .,Ablation study .,natural_language_inference,4,2,1,1,ablation-analysis,0.958766523,1,ablation-analysis,0.999310707,2.08E-07,6.32E-06,1.40E-07,5.71E-07,9.60E-07,9.10E-06,1.29E-07,3.51E-07,0.000650183,2.51E-07,1.31E-05,8.00E-06
517,natural_language_inference4,141,The sparse mixture of experts layer in the decoder added minor improvements to the model performance . :,Ablation study .,Ablation study .,natural_language_inference,4,3,1,1,ablation-analysis,0.959842035,1,ablation-analysis,0.998931854,1.41E-07,2.05E-05,1.25E-07,3.41E-07,1.23E-06,2.89E-06,1.07E-07,7.35E-07,0.001031447,1.57E-07,3.93E-06,6.52E-06
518,natural_language_inference4,142,Training curve of DCN + with and without reinforcement learning .,Ablation study .,Ablation study .,natural_language_inference,4,4,1,0,,0.002955461,0,negative,0.493095326,2.54E-05,0.001588057,3.99E-06,9.73E-06,0.000363167,0.000238327,9.94E-05,0.000380749,0.500855433,0.000686385,4.66E-05,0.002607413
519,natural_language_inference4,143,"In the latter case , only the cross entropy objective is used .",Ablation study .,Ablation study .,natural_language_inference,4,5,1,0,,0.011550561,0,negative,0.403826994,2.59E-05,0.003449658,8.69E-07,4.38E-06,3.33E-05,1.67E-05,1.22E-05,0.000228367,0.59231434,3.86E-05,1.81E-05,3.06E-05
520,natural_language_inference4,144,"The mixed objective initially performs worse as it begins policy learning from scratch , but quickly outperforms the cross entropy model .",Ablation study .,Ablation study .,natural_language_inference,4,6,1,0,,0.847978723,1,ablation-analysis,0.994416956,7.26E-07,1.28E-05,2.01E-06,2.71E-06,1.88E-05,7.60E-05,3.07E-06,8.99E-07,0.005156316,2.28E-06,6.15E-05,0.000245929
521,natural_language_inference4,145,Mixed objective convergence .,Ablation study .,,natural_language_inference,4,7,1,0,,0.059688352,0,negative,0.072414224,4.27E-05,0.000513299,1.16E-06,5.37E-06,0.000118535,3.36E-05,8.57E-05,0.000572339,0.925914744,0.000124169,1.00E-05,0.000164207
522,natural_language_inference4,146,The training curves for DCN + with reinforcement learning and DCN + without reinforcement learning are shown in to illustrate the effectiveness of our proposed mixed objective .,Ablation study .,Mixed objective convergence .,natural_language_inference,4,8,1,0,,0.001183625,0,negative,0.001435718,5.01E-06,5.62E-06,6.90E-08,9.14E-07,4.42E-05,1.87E-05,3.19E-05,1.93E-05,0.998410835,6.62E-06,2.06E-05,4.88E-07
523,natural_language_inference4,147,"In particular , we note that without mixing in the cross entropy loss , it is extremely difficult to learn the policy .",Ablation study .,Mixed objective convergence .,natural_language_inference,4,9,1,0,,0.000867625,0,negative,0.074864437,1.85E-05,1.37E-05,1.07E-06,7.78E-06,3.51E-05,2.72E-05,1.32E-05,2.88E-05,0.924797651,3.93E-05,0.000151535,1.89E-06
524,natural_language_inference4,148,"When we combine the cross entropy loss with the reinforcement learning objective , we find that the model initially performs worse early on as it begins policy learning from scratch ( shown in ) .",Ablation study .,Mixed objective convergence .,natural_language_inference,4,10,1,0,,0.58429549,1,ablation-analysis,0.761768774,1.43E-05,1.85E-05,2.59E-06,1.04E-05,0.000157868,0.000970826,7.27E-05,1.31E-05,0.234334704,2.15E-05,0.00258368,3.10E-05
525,natural_language_inference4,149,"However , with the addition of cross entropy loss , the model quickly learns a reasonable policy and subsequently outperforms the purely cross entropy model ( shown in ) .",Ablation study .,Mixed objective convergence .,natural_language_inference,4,11,1,0,,0.658785388,1,ablation-analysis,0.918999611,3.00E-06,1.60E-05,7.79E-07,5.15E-06,2.36E-05,0.000318042,8.00E-06,3.32E-06,0.078975253,5.77E-06,0.001630704,1.09E-05
526,natural_language_inference4,150,Sample predictions .,Ablation study .,,natural_language_inference,4,12,1,0,,0.001488168,0,negative,0.032103018,3.45E-06,0.000130911,5.18E-08,1.63E-06,2.71E-05,2.59E-05,8.25E-06,2.59E-05,0.967643391,3.19E-06,6.38E-06,2.09E-05
527,natural_language_inference4,151,compares predictions by DCN + and by the baseline on the development set .,Ablation study .,Sample predictions .,natural_language_inference,4,13,1,0,,0.00018036,0,negative,0.003712343,7.38E-06,0.000288475,6.88E-08,3.02E-06,0.000105231,9.67E-05,1.44E-05,1.95E-05,0.995712354,1.65E-05,2.07E-05,3.33E-06
528,natural_language_inference4,152,Both models retrieve answers that have sensible entity types .,Ablation study .,Sample predictions .,natural_language_inference,4,14,1,0,,0.004403972,0,negative,0.009473019,7.33E-05,0.003585903,1.13E-07,1.02E-05,3.22E-05,2.46E-05,3.96E-06,0.000425396,0.986183673,0.00014851,3.56E-05,3.51E-06
529,natural_language_inference4,153,"For example , the second example asks for "" what game "" and both models retrieve an American football game ; the third example asks for "" type of Turing machine "" and both models retrieve a type of turing machine .",Ablation study .,Sample predictions .,natural_language_inference,4,15,1,0,,2.08E-05,0,negative,0.00014888,1.77E-06,7.69E-06,6.59E-07,5.55E-05,5.45E-05,4.95E-06,3.70E-06,4.11E-06,0.999712683,3.68E-06,7.02E-07,1.13E-06
530,natural_language_inference4,154,"We find , however , that DCN + consistently make less mistakes on finding the correct entity .",Ablation study .,Sample predictions .,natural_language_inference,4,16,1,0,,0.307095096,0,ablation-analysis,0.586472472,9.91E-06,3.58E-05,3.05E-06,0.000211058,7.73E-05,0.000297954,7.87E-06,2.40E-06,0.41257865,7.15E-06,0.000278997,1.73E-05
531,natural_language_inference4,155,"This is especially apparent in the examples we show , which contain several entities or candidate answers of the correct type .",Ablation study .,Sample predictions .,natural_language_inference,4,17,1,0,,7.13E-05,0,negative,0.001809589,1.16E-06,3.98E-06,5.70E-07,3.87E-05,2.98E-05,4.01E-06,1.69E-06,1.28E-06,0.998105185,1.54E-06,1.90E-06,5.87E-07
532,natural_language_inference4,156,"In the first example , Gasquet wrote about the plague and called it "" Great Pestilence "" .",Ablation study .,Sample predictions .,natural_language_inference,4,18,1,0,,1.74E-05,0,negative,0.000399982,6.35E-07,8.03E-06,1.28E-06,5.67E-05,5.56E-05,4.12E-06,2.29E-06,1.21E-06,0.999466111,2.07E-06,5.80E-07,1.39E-06
533,natural_language_inference4,157,"While he likely did think of the plague as a "" great pestilence "" , the phrase "" suggested that it would appear to be some form of ordinary Eastern or bubonic plague "" provides evidence for the correct answer - "" some form of ordinary Eastern or bubonic plague "" .",Ablation study .,Sample predictions .,natural_language_inference,4,19,1,0,,9.79E-06,0,negative,0.000183414,1.06E-06,2.77E-06,5.57E-07,1.17E-05,5.16E-05,2.63E-06,5.01E-06,5.21E-06,0.999732354,2.37E-06,3.53E-07,9.69E-07
534,natural_language_inference4,158,"Similarly , the second example states that Thomas Davis was injured in the "" NFC Championship Game "" , but the game he insisted on playing in is the "" Super Bowl "" .",Ablation study .,Sample predictions .,natural_language_inference,4,20,1,0,,3.32E-05,0,negative,0.000387621,1.05E-06,9.25E-06,3.09E-06,0.000123973,9.70E-05,5.91E-06,4.17E-06,2.12E-06,0.999359559,2.52E-06,7.42E-07,3.00E-06
535,natural_language_inference4,159,"Finally , "" multi - tape "" and "" single - tape "" both appear in the sentence that provides provenance for the answer to the question .",Ablation study .,Sample predictions .,natural_language_inference,4,21,1,0,,4.76E-05,0,negative,0.00048368,2.44E-06,1.13E-05,3.89E-05,0.000462108,0.00040549,6.99E-06,8.81E-06,4.91E-06,0.998568433,9.72E-07,5.62E-07,5.41E-06
536,natural_language_inference4,160,"However , it is the "" single - tape "" Turing machine that implies quadratic time .",Ablation study .,Sample predictions .,natural_language_inference,4,22,1,0,,6.55E-05,0,negative,0.000358851,9.30E-06,4.99E-05,6.57E-07,3.61E-06,0.000149319,4.37E-05,2.23E-05,6.13E-05,0.998774955,0.000502899,2.09E-06,2.12E-05
537,natural_language_inference4,161,"In these examples , DCN + finds the correct entity out of ones that have the right type whereas the baseline does not .",Ablation study .,Sample predictions .,natural_language_inference,4,23,1,0,,0.000173143,0,negative,0.005509671,1.93E-06,3.40E-05,1.40E-07,5.97E-05,2.81E-05,3.86E-05,1.75E-06,7.22E-07,0.994287834,8.69E-07,3.54E-05,1.36E-06
538,natural_language_inference4,162,RELATED WORK,Ablation study .,Sample predictions .,natural_language_inference,4,24,1,0,,6.26E-06,0,negative,0.000441768,3.37E-06,2.32E-05,3.52E-07,8.23E-06,9.01E-05,2.23E-05,8.49E-06,7.31E-06,0.999372787,1.42E-05,1.64E-06,6.26E-06
539,natural_language_inference4,163,Neural models for question answering .,Ablation study .,,natural_language_inference,4,25,1,0,,0.022661328,0,negative,0.299835709,8.82E-05,0.004121065,7.77E-06,2.64E-05,0.000317249,0.002864474,4.37E-05,0.000401091,0.617988628,0.007389265,0.000290205,0.066626168
540,natural_language_inference4,164,Current state - of - the - art approaches for question answering over unstructured text tend to be neural approaches .,Ablation study .,Neural models for question answering .,natural_language_inference,4,26,1,0,,0.658207321,1,negative,0.00593532,3.48E-05,7.49E-05,7.76E-06,5.77E-05,0.00018722,0.001415507,7.19E-06,1.98E-05,0.98323,0.008131092,2.95E-05,0.000869236
541,natural_language_inference4,165,proposed one of the first conditional attention mechanisms in the Match - LSTM encoder .,Ablation study .,Neural models for question answering .,natural_language_inference,4,27,1,0,,0.003071083,0,negative,0.002562135,2.13E-06,0.000133256,2.43E-06,5.17E-05,0.000141902,7.17E-05,1.60E-06,1.58E-05,0.99697695,6.25E-06,1.67E-06,3.25E-05
542,natural_language_inference4,166,"Coattention , bidirectional attention flow , and self - matching attention ( Microsoft Asia Natural Language Computing Group , 2017 ) all build codependent representations of the question and the document .",Ablation study .,Neural models for question answering .,natural_language_inference,4,28,1,0,,0.06991186,0,negative,0.011786405,1.58E-05,0.00085119,3.88E-06,6.05E-05,0.000138907,8.99E-05,3.11E-06,0.000263493,0.986728612,6.87E-06,4.43E-06,4.69E-05
543,natural_language_inference4,167,These approaches of conditionally encoding two sequences are widely used in,Ablation study .,Neural models for question answering .,natural_language_inference,4,29,1,0,,0.001322968,0,negative,0.000556781,7.74E-06,2.13E-05,8.37E-07,4.23E-06,0.000112447,0.000175991,5.76E-06,4.68E-05,0.998760218,0.000244327,2.96E-06,6.06E-05
544,natural_language_inference4,168,"The historian Francis Aidan Gasquet wrote about the ' Great Pestilence ' in 1893 and suggested that "" it would appear to be some form of the ordinary Eastern or bubonic plague "" .",Ablation study .,Neural models for question answering .,natural_language_inference,4,30,1,0,,0.000133201,0,negative,0.000411779,1.33E-06,5.98E-06,2.81E-06,5.91E-05,9.13E-05,1.82E-05,1.88E-06,6.29E-06,0.999384287,2.12E-06,5.08E-07,1.43E-05
545,natural_language_inference4,169,"He was able to adopt the epidemiology of the bubonic plague for the Black Death for the second edition in 1908 , implicating rats and fleas in the process , and his interpretation was widely accepted for other ancient and medieval epidemics , such as the Justinian plague that was prevalent in the Eastern Roman Empire from 541 to 700 CE .",Ablation study .,Neural models for question answering .,natural_language_inference,4,31,1,0,,0.000179157,0,negative,0.00049373,5.33E-06,1.14E-05,2.13E-06,3.22E-05,0.000118226,2.84E-05,4.33E-06,5.21E-05,0.999233676,3.40E-06,5.44E-07,1.46E-05
546,natural_language_inference4,170,What did Gasquet think the plague was ?,Ablation study .,Neural models for question answering .,natural_language_inference,4,32,1,0,,8.83E-05,0,negative,0.000155275,2.23E-06,3.61E-06,1.98E-07,2.65E-06,5.05E-05,2.41E-05,3.88E-06,3.49E-05,0.999714422,2.07E-06,4.01E-07,5.80E-06
547,natural_language_inference4,171,"Carolina suffered a major setback when Thomas Davis , an 11 - year veteran who had already overcome three ACL tears in his career , went down with a broken arm in the NFC Championship Game .",Ablation study .,Neural models for question answering .,natural_language_inference,4,33,1,0,,0.000679183,0,negative,0.001248945,1.85E-06,2.33E-05,1.44E-06,0.000106301,5.05E-05,3.03E-05,6.64E-07,6.85E-06,0.998508491,1.28E-06,1.56E-06,1.85E-05
548,natural_language_inference4,172,"Despite this , he insisted he would still find away to play in the Super Bowl .",Ablation study .,Neural models for question answering .,natural_language_inference,4,34,1,0,,8.31E-05,0,negative,0.000210807,3.26E-07,2.17E-06,7.84E-08,5.45E-06,1.32E-05,6.95E-06,4.01E-07,3.03E-06,0.999755891,1.79E-07,2.93E-07,1.26E-06
549,natural_language_inference4,173,His prediction turned out to be accurate .,Ablation study .,,natural_language_inference,4,35,1,0,,0.000208479,0,negative,0.014974702,3.49E-07,1.89E-05,4.00E-07,1.02E-05,2.77E-05,8.11E-06,2.66E-06,4.07E-06,0.984860403,3.45E-07,1.30E-06,9.08E-05
550,natural_language_inference4,174,"What game did Thomas Davis say he would play in , despite breaking a bone earlier on ?",Ablation study .,His prediction turned out to be accurate .,natural_language_inference,4,36,1,0,,6.54E-05,0,negative,0.000433768,4.23E-06,2.07E-05,5.92E-07,1.43E-05,9.52E-05,2.03E-05,4.97E-06,3.98E-05,0.999336989,4.65E-06,3.05E-07,2.42E-05
551,natural_language_inference4,175,But bounding the computation time above by some concrete function f ( n ) often yields complexity classes that depend on the chosen machine model .,Ablation study .,His prediction turned out to be accurate .,natural_language_inference,4,37,1,0,,0.001970525,0,negative,0.001500634,1.31E-05,2.40E-05,1.23E-06,1.83E-05,0.000138499,7.85E-05,8.14E-06,4.11E-05,0.9980108,7.81E-05,1.82E-06,8.57E-05
552,natural_language_inference4,176,"For instance , the language { xx | x is any binary string } can be solved in linear time on a multi - tape Turing machine , but necessarily requires quadratic time in the model of single - tape Turing machines .",Ablation study .,His prediction turned out to be accurate .,natural_language_inference,4,38,1,0,,0.001738805,0,negative,0.000911815,2.85E-05,0.000103558,1.91E-06,4.93E-05,0.000464676,0.000253555,2.05E-05,0.000223497,0.997470148,9.85E-05,2.76E-06,0.000371267
553,natural_language_inference4,177,"If we allow polynomial variations in running time , Cobham - Edmonds thesis states that "" the time complexities in any two reasonable and general models of computation are polynomially related "" ( Goldreich 2008 , Chapter 1.2 ) .",Ablation study .,His prediction turned out to be accurate .,natural_language_inference,4,39,1,0,,0.002493952,0,negative,0.001217431,4.44E-05,0.000211591,2.17E-06,4.34E-05,0.000317782,0.000104592,1.58E-05,0.000492878,0.997317328,3.67E-05,1.09E-06,0.000194791
554,natural_language_inference4,178,"This forms the basis for the complexity class P , which is the set of decision problems solvable by a deterministic Turing machine within polynomial time .",Ablation study .,His prediction turned out to be accurate .,natural_language_inference,4,40,1,0,,0.00535761,0,negative,0.000376426,7.07E-05,0.000282815,3.11E-07,3.60E-05,8.81E-05,3.65E-05,6.36E-06,0.000673514,0.998389955,6.28E-06,6.30E-07,3.24E-05
555,natural_language_inference4,179,The corresponding set of function problems is FP .,Ablation study .,,natural_language_inference,4,41,1,0,,0.000294324,0,negative,0.004085827,6.42E-06,6.26E-05,2.22E-07,7.90E-06,4.21E-05,1.87E-05,1.82E-05,4.88E-05,0.995508393,5.33E-07,1.63E-06,0.000198725
556,natural_language_inference4,180,A language solved in quadratic time implies the use of what type of Turing machine ?,Ablation study .,The corresponding set of function problems is FP .,natural_language_inference,4,42,1,0,,1.93E-05,0,negative,0.000663587,2.03E-06,1.93E-05,3.58E-07,6.12E-06,5.92E-05,4.29E-05,4.34E-06,1.89E-05,0.998962229,6.54E-06,1.62E-06,0.000212812
557,natural_language_inference4,181,question answering .,Ablation study .,The corresponding set of function problems is FP .,natural_language_inference,4,43,1,0,,0.001495944,0,negative,0.034084595,1.06E-05,0.000303696,6.60E-05,0.005195458,0.000739278,0.00444937,9.45E-06,3.88E-06,0.91160463,2.42E-05,0.000224303,0.043284525
558,natural_language_inference4,182,"After building codependent encodings , most models predict the answer by generating the start position and the end position corresponding to the estimated answer span .",Ablation study .,The corresponding set of function problems is FP .,natural_language_inference,4,44,1,0,,0.002463818,0,negative,0.005775466,2.05E-05,0.000478196,4.63E-07,1.90E-05,5.40E-05,4.04E-05,5.36E-06,0.000216634,0.993265265,2.38E-06,3.05E-06,0.000119234
559,natural_language_inference4,183,The generation process utilizes a pointer network over the positions in the document .,Ablation study .,The corresponding set of function problems is FP .,natural_language_inference,4,45,1,0,,0.004386792,0,negative,0.006727125,0.000130093,0.004766036,3.17E-07,3.42E-05,5.95E-05,4.39E-05,7.47E-06,0.013864605,0.974244287,8.34E-07,2.54E-06,0.000119028
560,natural_language_inference4,184,"also introduced the dynamic decoder , which iteratively proposes answers by alternating between start position and end position estimates , and in some cases is able to recover from initially erroneous predictions .",Ablation study .,The corresponding set of function problems is FP .,natural_language_inference,4,46,1,0,,0.000328936,0,negative,0.003461127,3.18E-06,0.000644327,1.29E-06,2.94E-05,9.69E-05,3.10E-05,3.96E-06,8.20E-05,0.995497202,5.70E-07,1.05E-06,0.000148005
561,natural_language_inference4,185,Neural attention models .,,,natural_language_inference,4,0,1,0,,0.027551158,0,negative,4.45E-05,0.000268286,0.000544063,1.04E-06,5.69E-07,0.000124304,0.000642992,0.001379162,0.000241244,0.528282947,0.466393059,0.002048741,2.91E-05
562,natural_language_inference4,186,Neural attention models saw early adoption in machine translation and has since become to de-facto architecture for neural machine translation models .,Neural attention models .,Neural attention models .,natural_language_inference,4,1,1,0,,0.010772485,0,negative,3.77E-05,1.64E-06,0.000318146,9.54E-07,3.85E-08,3.21E-05,0.000261381,0.000108718,6.71E-06,0.985706118,0.011996196,0.001480678,4.95E-05
563,natural_language_inference4,187,"Self-attention , or intra-attention , has been applied to language modeling , sentiment analysis , natural language inference , and abstractive text summarization .",Neural attention models .,Neural attention models .,natural_language_inference,4,2,1,0,,0.030095145,0,negative,4.19E-05,3.78E-06,0.000221467,1.22E-06,5.52E-08,2.24E-05,0.000169489,0.000132818,6.90E-06,0.98754816,0.010422947,0.001397135,3.18E-05
564,natural_language_inference4,188,extended this idea to a deep self - attentional network which obtained state - of - the - art results in machine translation .,Neural attention models .,Neural attention models .,natural_language_inference,4,3,1,0,,2.73E-05,0,negative,3.93E-05,6.05E-07,0.000665938,1.06E-07,3.43E-08,6.42E-06,1.50E-05,2.09E-05,7.21E-06,0.998903102,2.28E-05,0.000316456,2.06E-06
565,natural_language_inference4,189,"Coattention , which builds codependent representations of multiple inputs , has been applied to visual question answering .",Neural attention models .,Neural attention models .,natural_language_inference,4,4,1,0,,0.046913973,0,negative,0.000250792,1.32E-05,0.004383376,4.00E-06,3.79E-07,4.40E-05,0.001609403,0.000137647,1.01E-05,0.926514349,0.044230072,0.022516837,0.000285838
566,natural_language_inference4,190,introduced coattention for question answering .,Neural attention models .,Neural attention models .,natural_language_inference,4,5,1,0,,2.06E-05,0,negative,0.000149552,3.07E-07,0.000202093,1.35E-06,7.56E-08,9.86E-06,3.52E-05,1.30E-05,3.01E-06,0.998727187,0.000180864,0.000667544,9.94E-06
567,natural_language_inference4,191,"Bidirectional attention flow and self - matching attention ( Microsoft Asia Natural Language Computing Group , 2017 ) also build codependent representations between the question and the document .",Neural attention models .,Neural attention models .,natural_language_inference,4,6,1,0,,0.000685187,0,negative,2.36E-05,2.76E-07,0.000250271,5.25E-08,8.02E-09,5.52E-06,1.16E-05,2.07E-05,6.71E-06,0.999461797,1.50E-05,0.000202958,1.42E-06
568,natural_language_inference4,192,Reinforcement learning in NLP .,Neural attention models .,,natural_language_inference,4,7,1,0,,0.000543467,0,negative,3.36E-05,2.02E-06,0.000280924,2.43E-07,2.20E-08,1.37E-05,0.000314895,9.37E-05,5.68E-06,0.989387927,0.003100644,0.006735654,3.10E-05
569,natural_language_inference4,193,Many tasks in natural language processing have evaluation metrics thatare not differentiable .,Neural attention models .,Reinforcement learning in NLP .,natural_language_inference,4,8,1,0,,0.00055348,0,negative,1.11E-05,1.27E-06,3.44E-06,5.46E-07,5.49E-08,1.46E-05,5.66E-05,3.19E-05,1.77E-06,0.994760586,0.004900753,0.000210567,6.75E-06
570,natural_language_inference4,194,proposed a hierarchical reinforcement learning technique for generating text in a simulated way - finding domain .,Neural attention models .,Reinforcement learning in NLP .,natural_language_inference,4,9,1,0,,0.000273028,0,negative,9.51E-05,9.15E-07,0.000205,5.83E-07,3.06E-07,2.67E-05,6.13E-05,2.14E-05,7.12E-06,0.999312583,4.66E-05,0.00021771,4.70E-06
571,natural_language_inference4,195,applied deep Q-networks to learn policies for text - based games using game rewards as feedback .,Neural attention models .,Reinforcement learning in NLP .,natural_language_inference,4,10,1,0,,0.000364413,0,negative,3.89E-05,5.50E-07,7.52E-05,4.26E-07,1.39E-07,2.07E-05,7.36E-05,1.51E-05,2.97E-06,0.999317841,0.0001701,0.000278425,6.13E-06
572,natural_language_inference4,196,"introduced a neural conversational model trained using policy gradient methods , whose reward function consisted of heuristics for ease of answering , information flow , and semantic coherence .",Neural attention models .,Reinforcement learning in NLP .,natural_language_inference,4,11,1,0,,0.000178098,0,negative,0.000362881,6.88E-07,0.000310421,1.93E-06,9.57E-07,2.65E-05,8.03E-05,8.76E-06,6.87E-06,0.998705516,4.42E-05,0.000443339,7.70E-06
573,natural_language_inference4,197,"proposed a general actor-critic temporal - difference method for sequence prediction , performing metric optimization on language modeling and machine translation .",Neural attention models .,Reinforcement learning in NLP .,natural_language_inference,4,12,1,0,,0.000824694,0,negative,0.000152695,2.14E-06,0.00046736,1.86E-06,7.98E-07,5.12E-05,0.000200375,3.47E-05,9.65E-06,0.998231495,0.000226052,0.000599208,2.25E-05
574,natural_language_inference4,198,"Direct word overlap metric optimization has also been applied to summarization , and machine translation .",Neural attention models .,Reinforcement learning in NLP .,natural_language_inference,4,13,1,0,,0.006969829,0,negative,1.88E-05,2.50E-06,1.66E-05,6.99E-07,8.32E-08,3.20E-05,0.000100089,5.81E-05,6.64E-06,0.997343205,0.002160452,0.00024835,1.25E-05
575,relation_extraction2,1,title,,,relation_extraction,2,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
576,relation_extraction2,2,Enriching Pre-trained Language Model with Entity Information for Relation Classification,title,title,relation_extraction,2,1,1,1,research-problem,0.99882637,1,research-problem,4.93E-08,1.44E-05,1.01E-07,4.33E-08,4.32E-08,8.18E-08,1.00E-06,1.81E-06,1.48E-06,0.0018441,0.998136463,3.59E-07,5.45E-08
577,relation_extraction2,3,abstract,,,relation_extraction,2,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
578,relation_extraction2,4,Relation classification is an important NLP task to extract relations between entities .,abstract,abstract,relation_extraction,2,1,1,0,,0.889703176,1,research-problem,1.79E-08,2.78E-06,1.31E-08,8.02E-07,2.20E-07,1.36E-07,3.72E-07,5.24E-07,1.51E-07,0.005284118,0.994710802,1.65E-08,4.40E-08
579,relation_extraction2,5,The state - of - the - art methods for relation classification are primarily based on Convolutional or Recurrent Neural Networks .,abstract,abstract,relation_extraction,2,2,1,0,,0.243893224,0,research-problem,3.10E-08,1.04E-05,2.76E-08,6.34E-07,3.25E-07,4.73E-07,3.96E-07,2.49E-06,6.57E-07,0.022294829,0.977689647,2.25E-08,4.03E-08
580,relation_extraction2,6,"Recently , the pre-trained BERT model achieves very successful results in many NLP classification / sequence labeling tasks .",abstract,abstract,relation_extraction,2,3,1,0,,0.149550641,0,research-problem,6.84E-08,2.82E-05,3.67E-08,1.51E-06,4.58E-07,1.49E-06,9.06E-07,1.17E-05,2.43E-06,0.034594972,0.965358005,7.58E-08,1.07E-07
581,relation_extraction2,7,Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities .,abstract,abstract,relation_extraction,2,4,1,0,,0.239968382,0,research-problem,3.39E-08,1.52E-05,3.42E-08,3.31E-06,1.87E-06,5.47E-07,6.76E-07,1.94E-06,5.83E-07,0.021573976,0.978401767,2.91E-08,8.26E-08
582,relation_extraction2,8,"In this paper , we propose a model that both leverages the pretrained BERT language model and incorporates information from the target entities to tackle the relation classification task .",abstract,abstract,relation_extraction,2,5,1,0,,0.519450801,1,research-problem,2.52E-05,0.25996064,0.000291211,1.92E-05,0.000225889,5.02E-05,2.99E-05,0.000623792,0.052039035,0.141508491,0.545207569,1.23E-05,6.44E-06
583,relation_extraction2,9,We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities .,abstract,abstract,relation_extraction,2,6,1,0,,0.056108988,0,negative,2.74E-05,0.197920271,0.000161894,9.67E-06,0.000283471,6.04E-05,7.75E-06,0.000899971,0.152479957,0.628331178,0.019812469,4.16E-06,1.40E-06
584,relation_extraction2,10,We achieve significant improvement over the state - of - the - art method on the SemEval - 2010 task 8 relational dataset .,abstract,abstract,relation_extraction,2,7,1,0,,0.02437563,0,research-problem,7.30E-05,0.004341143,6.76E-06,2.43E-05,8.20E-05,4.04E-05,0.000240955,0.000919835,0.000114828,0.426683172,0.566432751,0.001027248,1.36E-05
585,relation_extraction2,11,Introduction,,,relation_extraction,2,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
586,relation_extraction2,12,The task of relation classification is to predict semantic relations between pairs of nominals .,Introduction,Introduction,relation_extraction,2,1,1,0,,0.880526337,1,research-problem,4.63E-07,9.38E-05,2.59E-07,1.25E-05,1.52E-05,4.53E-06,9.06E-06,3.77E-06,2.73E-05,0.024707474,0.97512312,5.54E-07,1.90E-06
587,relation_extraction2,13,"Given a sequence of text ( usually a sentence ) sand a pair of nominals e 1 and e 2 , the objective is to identify the relation between e 1 and e 2 .",Introduction,Introduction,relation_extraction,2,2,1,0,,0.055526529,0,research-problem,8.97E-07,0.001628288,5.99E-07,9.26E-06,3.42E-05,1.47E-05,8.39E-06,3.26E-05,0.001051617,0.115757293,0.881458925,1.15E-06,2.09E-06
588,relation_extraction2,14,It is an important NLP task which is normally used as an intermediate step in variety of NLP applications .,Introduction,Introduction,relation_extraction,2,3,1,0,,0.202978936,0,research-problem,8.79E-07,0.000119934,2.88E-07,4.31E-05,5.58E-05,1.35E-05,1.58E-05,7.52E-06,2.63E-05,0.081727494,0.917985659,9.12E-07,2.82E-06
589,relation_extraction2,15,"The following example shows the Component - Whole relation between the nominals "" kitchen "" and "" house "" :",Introduction,Introduction,relation_extraction,2,4,1,0,,0.006327288,0,negative,3.78E-06,0.003884034,3.86E-06,5.80E-06,0.000109991,0.000100178,3.44E-05,0.000104282,0.015161938,0.822691177,0.157893214,4.61E-06,2.74E-06
590,relation_extraction2,16,""" The [ kitchen ] e 1 is the last renovated part of the [ house ] e 1 . """,Introduction,Introduction,relation_extraction,2,5,1,0,,0.002167307,0,negative,7.34E-06,0.000692732,1.83E-06,0.000108163,0.002247608,0.000275554,3.56E-05,8.70E-05,0.000380513,0.991315604,0.004841782,3.39E-06,2.87E-06
591,relation_extraction2,17,"Recently , deep neural networks have applied to relation classification .",Introduction,Introduction,relation_extraction,2,6,1,0,,0.044070474,0,research-problem,1.06E-06,0.000382835,4.01E-07,2.03E-06,4.74E-06,1.08E-05,1.23E-05,2.13E-05,0.000399829,0.097024256,0.902137509,1.60E-06,1.37E-06
592,relation_extraction2,18,These methods usually use some features derived from lexical resources such as Word - Net or NLP tools such as dependency parsers and named entity recognizers ( NER ) .,Introduction,Introduction,relation_extraction,2,7,1,0,,0.026884845,0,negative,8.62E-06,0.001150968,2.15E-06,4.40E-05,0.000208872,0.000100618,5.33E-05,8.05E-05,0.000280819,0.528826768,0.469231111,6.06E-06,6.15E-06
593,relation_extraction2,19,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,Introduction,Introduction,relation_extraction,2,8,1,0,,0.762686686,1,research-problem,6.84E-06,0.000896922,1.54E-06,1.78E-05,5.26E-05,1.95E-05,6.79E-05,3.93E-05,0.000144304,0.106781444,0.891952905,1.10E-05,7.88E-06
594,relation_extraction2,20,The pretrained model BERT proposed by has especially significant impact .,Introduction,Introduction,relation_extraction,2,9,1,0,,0.205748265,0,negative,0.211243059,0.017448451,0.000185041,0.002365651,0.018113055,0.001180253,0.008662098,0.001168374,0.002026137,0.724857525,0.005762546,0.006766592,0.000221218
595,relation_extraction2,21,It has been applied to multiple NLP tasks and obtains new state - of - theart results on eleven tasks .,Introduction,Introduction,relation_extraction,2,10,1,0,,0.078743527,0,approach,0.000217176,0.457002669,0.000221384,1.43E-05,0.003382555,0.000263208,0.001528163,0.000700523,0.060171262,0.421620717,0.054073752,0.000763442,4.08E-05
596,relation_extraction2,22,The tasks that BERT has been applied to are typically modeled as classification problems and sequence labeling problems .,Introduction,Introduction,relation_extraction,2,11,1,0,,0.044242896,0,negative,1.70E-06,0.002020317,1.23E-06,2.64E-05,0.000114064,6.77E-05,2.70E-05,8.62E-05,0.000828496,0.523685675,0.473135171,2.60E-06,3.43E-06
597,relation_extraction2,23,"It has also been applied to the SQuAD question answering problem , in which the objective is to find the starting point and ending point of an answer span .",Introduction,Introduction,relation_extraction,2,12,1,0,,0.308340929,0,research-problem,1.91E-06,0.001917615,2.00E-06,9.70E-06,2.74E-05,5.89E-05,4.02E-05,9.15E-05,0.001300367,0.260952521,0.735590069,3.81E-06,3.94E-06
598,relation_extraction2,24,"As far as we know , the pretrained BERT model has not been applied to relation classification , which relies not only on the information of the whole sentence but also on the information of the specific target entities .",Introduction,Introduction,relation_extraction,2,13,1,0,,0.350630018,0,research-problem,2.93E-06,0.003365213,2.13E-06,1.56E-05,7.11E-05,7.12E-05,5.68E-05,0.000137779,0.001156654,0.356942448,0.638166794,5.99E-06,5.39E-06
599,relation_extraction2,25,"In this paper , we apply the pretrained BERT model for relation classification .",Introduction,Introduction,relation_extraction,2,14,1,1,model,0.949933178,1,approach,4.02E-05,0.722479708,0.000157301,8.81E-06,0.000840192,6.13E-05,0.000152467,0.000240556,0.227154386,0.027001937,0.02181255,3.68E-05,1.38E-05
600,relation_extraction2,26,"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",Introduction,Introduction,relation_extraction,2,15,1,1,model,0.873557696,1,model,9.23E-05,0.299249494,0.000157873,7.94E-06,0.002332062,0.000151206,8.44E-05,0.000301284,0.670746839,0.026678929,0.000181719,1.08E-05,5.20E-06
601,relation_extraction2,27,We then locate the positions of the two target entities in the output embedding from BERT model .,Introduction,Introduction,relation_extraction,2,16,1,1,model,0.775726282,1,model,5.16E-06,0.058531006,2.36E-05,2.62E-07,7.32E-05,9.25E-06,5.05E-06,2.31E-05,0.911131005,0.029678128,0.000517955,1.89E-06,4.28E-07
602,relation_extraction2,28,We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .,Introduction,Introduction,relation_extraction,2,17,1,1,model,0.569060579,1,model,6.44E-06,0.180085254,2.63E-05,5.33E-06,0.000584889,0.000412122,6.51E-05,0.0017195,0.734483679,0.082176664,0.000427405,2.47E-06,4.85E-06
603,relation_extraction2,29,"By this way , it captures both the semantics of the sentence and the two target entities to better fit the relation classification task .",Introduction,Introduction,relation_extraction,2,18,1,0,,0.902008515,1,model,9.19E-06,0.062562938,3.38E-05,3.17E-07,8.38E-05,8.61E-06,7.77E-06,1.69E-05,0.924481007,0.01245359,0.000337934,3.53E-06,6.83E-07
604,relation_extraction2,30,Our contributions are as follows :,Introduction,Introduction,relation_extraction,2,19,1,0,,0.00226048,0,negative,2.17E-05,0.009805355,1.23E-05,0.000403262,0.003333686,0.000506595,7.97E-05,0.000240908,0.005530885,0.968956477,0.011089513,7.67E-06,1.20E-05
605,relation_extraction2,31,( 1 ) We put forward an innovative approach to incorporate entity - level information into the pretrained language model for relation classification .,Introduction,Introduction,relation_extraction,2,20,1,0,,0.447875436,0,approach,0.001069386,0.579695273,0.001112689,0.000287239,0.020247613,0.0003941,0.000572183,0.000290311,0.130842755,0.24492902,0.020268847,0.000234445,5.61E-05
606,relation_extraction2,32,( 2 ) We achieve the new state - of - the - art for the relation classification task .,Introduction,Introduction,relation_extraction,2,21,1,0,,0.125693904,0,negative,0.00043885,0.2816239,0.000550643,2.11E-05,0.001213095,0.000134207,0.001071022,0.000243705,0.122867599,0.472840832,0.118270479,0.000684716,3.99E-05
607,relation_extraction2,33,Related Work,,,relation_extraction,2,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
608,relation_extraction2,47,Methodology,,,relation_extraction,2,0,1,0,,0.043255215,0,negative,1.91E-05,0.000159627,1.45E-05,1.37E-06,4.99E-07,0.000139902,0.000211372,0.00286638,0.000149264,0.913404109,0.082542361,0.000474296,1.72E-05
609,relation_extraction2,48,Pre-trained Model BERT,Methodology,Methodology,relation_extraction,2,1,1,0,,0.000307396,0,negative,5.30E-06,4.13E-05,2.08E-05,1.76E-07,8.82E-08,5.71E-05,7.58E-06,0.00242512,0.000155374,0.997223024,3.54E-05,2.81E-05,5.63E-07
610,relation_extraction2,49,The pre-trained BERT model is a multi - layer bidirectional Transformer encoder .,Methodology,Methodology,relation_extraction,2,2,1,0,,0.00469165,0,negative,0.000116039,0.001701229,0.001607445,6.59E-06,3.96E-06,0.000488268,9.98E-05,0.014014965,0.008058097,0.973678957,0.000169317,4.37E-05,1.16E-05
611,relation_extraction2,50,The design of input representation of BERT is to be able to represent both a single text sentence and a pair of text sentences in one token sequence .,Methodology,Methodology,relation_extraction,2,3,1,0,,0.000213414,0,negative,5.10E-06,5.56E-05,1.13E-05,8.00E-07,2.15E-07,1.21E-05,1.71E-05,0.000267221,2.38E-05,0.954246447,0.045268447,8.92E-05,2.72E-06
612,relation_extraction2,51,"The input representation of each token is constructed by the summation of the corresponding token , segment and position embeddings .",Methodology,Methodology,relation_extraction,2,4,1,0,,0.00039695,0,negative,1.11E-05,0.000218729,0.000125213,1.01E-07,2.10E-07,4.25E-05,4.24E-06,0.001182508,0.001771844,0.996625186,1.14E-05,6.71E-06,2.67E-07
613,relation_extraction2,52,'' is appended to the beginning of each sequence as the first token of the sequence .,Methodology,Methodology,relation_extraction,2,5,1,0,,6.69E-06,0,negative,1.32E-05,1.09E-05,1.11E-05,1.85E-07,2.57E-07,2.44E-05,3.62E-06,0.000277239,3.90E-05,0.999604162,1.72E-06,1.41E-05,1.23E-07
614,relation_extraction2,53,The final hidden state from the Transformer output corresponding to the first token is used as the sentence representation for classification tasks .,Methodology,Methodology,relation_extraction,2,6,1,0,,5.02E-05,0,negative,8.39E-06,5.53E-05,0.000177953,1.36E-08,8.56E-08,6.23E-06,1.91E-06,0.000169504,0.000186479,0.999372927,3.65E-06,1.75E-05,6.33E-08
615,relation_extraction2,54,"In case there are two sentences in a task , ' [ SEP ] ' is used to separate the two sentences .",Methodology,Methodology,relation_extraction,2,7,1,0,,1.35E-06,0,negative,4.47E-06,7.75E-06,6.25E-06,2.01E-07,4.09E-07,1.71E-05,2.02E-06,0.000138475,1.35E-05,0.999803476,1.27E-06,5.05E-06,7.57E-08
616,relation_extraction2,55,"BERT pre-trains the model parameters by using a pre-training objective : the masked language model ( MLM ) , which randomly masks some of the tokens from the input , and set the optimization objective to predict the original vocabulary id of the masked word according to its context .",Methodology,Methodology,relation_extraction,2,8,1,0,,0.003452605,0,negative,1.23E-05,0.000394663,0.000587618,7.89E-08,4.30E-07,1.24E-05,4.30E-06,0.00042833,0.000599705,0.997933916,7.89E-06,1.81E-05,2.29E-07
617,relation_extraction2,56,"Unlike left - to - right language model pre-training , the MLM objective can help a state output to utilize both the left and the right context , which allows a pre-training system to apply a deep bidirectional Transformer .",Methodology,Methodology,relation_extraction,2,9,1,0,,0.000279965,0,negative,8.92E-05,0.000164678,0.000123648,6.61E-08,5.14E-07,1.72E-06,2.90E-06,4.55E-05,4.02E-05,0.998576033,5.14E-05,0.000903929,1.91E-07
618,relation_extraction2,57,"Besides the masked language model , BERT also trains a "" next sentence prediction "" task that jointly pre-trains text - pair representations .",Methodology,Methodology,relation_extraction,2,10,1,0,,0.00834566,0,negative,0.000286489,0.00055032,0.017729666,7.31E-07,8.43E-06,2.24E-05,0.000143683,0.000162064,0.0001529,0.977120588,0.000159527,0.003657939,5.27E-06
619,relation_extraction2,58,shows the architecture of our approach .,Methodology,Methodology,relation_extraction,2,11,1,0,,1.36E-06,0,negative,2.53E-06,1.63E-05,9.58E-06,1.38E-07,1.64E-07,5.66E-06,1.57E-06,8.29E-05,0.000120464,0.999737788,1.47E-05,7.84E-06,2.84E-07
620,relation_extraction2,59,Model Architecture,,,relation_extraction,2,0,1,0,,0.012027743,0,negative,0.002077781,0.004304933,0.001063385,0.001680559,0.000148392,0.002327543,0.001259945,0.007127733,0.020357931,0.924862365,0.03336821,0.000759331,0.000661892
621,relation_extraction2,60,"For a sentence s with two target entities e 1 and e 2 , to make the BERT module capture the location information of the two entities , at both the beginning and end of the first entity , we insert a special token ' $ ' , and at both the beginning and end of the second entity , we insert a special token '# ' .",Model Architecture,Model Architecture,relation_extraction,2,1,1,0,,0.017663227,0,negative,3.49E-05,0.008905133,4.09E-05,2.08E-06,5.80E-05,5.51E-05,8.22E-06,0.000318338,0.126346525,0.863432093,0.000786624,1.07E-05,1.36E-06
622,relation_extraction2,61,We also add '' to the beginning of each sentence .,Model Architecture,Model Architecture,relation_extraction,2,2,1,0,,0.030447192,0,negative,0.000107134,0.00942844,9.90E-05,2.18E-05,0.00026437,0.000449787,3.70E-05,0.001110718,0.271487124,0.7168207,0.000158117,1.03E-05,5.46E-06
623,relation_extraction2,62,"For example , after insertion of the special separate tokens , for a sentence with target entities "" kitchen "" and "" house "" will become to :",Model Architecture,Model Architecture,relation_extraction,2,3,1,0,,0.001929511,0,negative,1.37E-05,0.000292954,1.08E-05,3.61E-07,2.29E-05,1.40E-05,3.02E-06,2.25E-05,0.0041917,0.995234836,0.000184264,8.77E-06,1.79E-07
624,relation_extraction2,63,""" [ CLS ]",Model Architecture,Model Architecture,relation_extraction,2,4,1,0,,0.001132338,0,negative,9.24E-06,9.57E-05,1.97E-06,5.64E-07,1.15E-05,1.19E-05,1.87E-06,1.91E-05,0.000748388,0.998886001,0.000209338,4.36E-06,1.38E-07
625,relation_extraction2,64,"The $ kitchen $ is the last renovated part of the # house # . "" :",Model Architecture,Model Architecture,relation_extraction,2,5,1,0,,0.002333617,0,negative,3.00E-05,0.000161297,1.01E-05,0.0003067,0.001634232,0.00021972,1.33E-05,4.40E-05,0.000836824,0.996638687,9.69E-05,5.59E-06,2.67E-06
626,relation_extraction2,65,The model architecture .,,,relation_extraction,2,0,1,0,,0.006422906,0,negative,8.03E-05,0.000774277,7.11E-05,1.45E-05,4.94E-06,0.000286214,6.65E-05,0.003255158,0.0040375,0.986771622,0.004528071,8.41E-05,2.57E-05
627,relation_extraction2,66,"Given a sentence s with entity e 1 and e 2 , suppose its final hidden state output from BERT module is H . Suppose vectors H i to H j are the final hidden state vectors from BERT for entity e 1 , and H k to H mare the final hidden state vectors from BERT for entity e 2 .",The model architecture .,The model architecture .,relation_extraction,2,1,1,0,,0.000142159,0,negative,2.77E-06,0.000429983,4.86E-06,1.15E-07,1.41E-06,1.62E-05,1.11E-06,0.000517254,0.001161972,0.997744501,0.00011219,7.42E-06,1.89E-07
628,relation_extraction2,67,We apply the average operation to get a vector representation for each of the two target entities .,The model architecture .,The model architecture .,relation_extraction,2,2,1,0,,0.005706914,0,negative,5.90E-05,0.006067752,0.000544552,4.57E-07,5.18E-06,0.000109459,5.42E-06,0.001894882,0.077160189,0.914056025,8.51E-05,1.09E-05,1.17E-06
629,relation_extraction2,68,"Then after an activation operation ( i.e. tanh ) , we add a fully connected layer to each of the two vectors , and the output fore 1 and e 2 are H 1 and H 2 respectively .",The model architecture .,The model architecture .,relation_extraction,2,3,1,0,,0.004097848,0,negative,0.000302894,0.003540708,0.000250745,2.15E-06,1.80E-05,0.000172161,1.04E-05,0.00283038,0.02259131,0.970176995,5.60E-05,4.59E-05,2.42E-06
630,relation_extraction2,69,This process can be mathematically formalized as Equation .,The model architecture .,,relation_extraction,2,4,1,0,,9.08E-05,0,negative,2.56E-06,0.00029105,6.29E-06,1.27E-07,5.13E-07,8.81E-06,7.70E-07,0.000220668,0.008333119,0.99064067,0.000492438,2.70E-06,2.83E-07
631,relation_extraction2,70,"We make W 1 and W 2 , b 1 and b 2 share the same parameters .",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,5,1,0,,1.73E-05,0,negative,6.10E-05,0.000321621,1.80E-05,4.60E-07,1.64E-06,5.24E-05,3.92E-06,0.002326117,0.000904425,0.996273145,5.58E-06,3.11E-05,5.77E-07
632,relation_extraction2,71,"In other words , we set W 1 = W 2 , b 1 = b 2 .",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,6,1,0,,9.41E-06,0,negative,2.81E-05,0.000115017,4.78E-06,1.90E-07,6.98E-07,5.40E-05,2.71E-06,0.002288906,0.000279818,0.997194462,4.33E-06,2.67E-05,3.15E-07
633,relation_extraction2,72,"For the final hidden state vector of the first token ( i.e. ' [ CLS ] ' ) , we also add an activation operation and a fully connected layer , which is formally expressed as :",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,7,1,0,,0.001634587,0,negative,0.000150951,0.000222636,0.000470537,1.51E-07,1.10E-06,9.78E-06,1.46E-06,0.000199119,0.0024032,0.99648118,9.00E-06,5.03E-05,5.66E-07
634,relation_extraction2,73,"Matrices W 0 , W 1 , W 2 have the same dimensions , i.e. W 0 ?",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,8,1,0,,0.000115335,0,negative,1.91E-05,0.000242186,1.16E-05,4.33E-07,1.64E-06,7.05E-05,4.99E-06,0.002016653,0.00053802,0.997023253,2.55E-05,4.54E-05,8.03E-07
635,relation_extraction2,74,"R dd , W 1 ?",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,9,1,0,,6.33E-05,0,negative,6.24E-05,5.69E-05,7.45E-06,8.54E-07,1.27E-06,2.51E-05,4.45E-06,0.000553031,0.000106627,0.999030782,3.27E-05,0.000117347,1.08E-06
636,relation_extraction2,75,"R dd , W 2 ?",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,10,1,0,,0.0001295,0,negative,9.10E-05,6.99E-05,8.09E-06,1.64E-06,2.19E-06,4.87E-05,8.18E-06,0.001102542,0.00011857,0.998344735,3.72E-05,0.000165076,2.16E-06
637,relation_extraction2,76,"R dd , where d is the hidden state size from BERT .",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,11,1,0,,5.56E-05,0,negative,0.000321804,0.000102511,2.16E-05,8.82E-07,4.55E-06,1.72E-05,5.36E-06,0.000516708,0.000116597,0.998568085,1.38E-05,0.000309733,1.18E-06
638,relation_extraction2,77,"We concatenate H 0 , H 1 , H 2 and then add a fully connected layer and a softmax layer , which can be expressed as following :",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,12,1,0,,0.000206322,0,negative,4.49E-05,0.000245772,3.40E-05,4.31E-07,1.25E-06,2.35E-05,1.59E-06,0.00075078,0.003008954,0.995868286,6.64E-06,1.29E-05,1.01E-06
639,relation_extraction2,78,"where W 3 ? R L3d ( L is the number of relation types ) , and p is the probability output .",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,13,1,0,,1.82E-05,0,negative,1.30E-05,5.97E-05,3.18E-06,2.98E-07,7.72E-07,1.82E-05,1.64E-06,0.000697503,0.000195608,0.998976887,1.10E-05,2.17E-05,4.24E-07
640,relation_extraction2,79,"In Equations ( 1 ) , ( 2 ) , ( 3 ) , b 0 , b 1 , b 2 , b 3 are bias vectors .",The model architecture .,This process can be mathematically formalized as Equation .,relation_extraction,2,14,1,0,,2.54E-05,0,negative,1.33E-05,0.000220679,2.66E-06,2.66E-07,8.07E-07,2.78E-05,1.58E-06,0.002234705,0.000764821,0.99671459,5.79E-06,1.26E-05,4.52E-07
641,relation_extraction2,80,We use cross entropy as the loss function .,The model architecture .,,relation_extraction,2,15,1,0,,0.354670135,0,hyperparameters,2.05E-05,0.014312902,3.14E-05,7.28E-06,1.55E-05,0.006769931,0.000140281,0.554658256,0.023383369,0.40057644,4.56E-05,1.58E-05,2.27E-05
642,relation_extraction2,81,We apply dropout before each fully connected layer during training .,The model architecture .,We use cross entropy as the loss function .,relation_extraction,2,16,1,0,,0.845878535,1,hyperparameters,0.000371112,0.005897352,0.000318147,0.000187187,0.000165688,0.083822221,0.00077261,0.74329154,0.014202018,0.150604628,2.02E-05,1.99E-05,0.000327389
643,relation_extraction2,82,We call our approach as R - BERT .,The model architecture .,,relation_extraction,2,17,1,0,,0.02024467,0,negative,8.59E-05,0.027678735,0.006374221,2.14E-06,0.000101898,4.56E-05,2.06E-05,0.000500042,0.059944892,0.904617283,0.000403452,0.000218088,7.09E-06
644,relation_extraction2,83,Experiments,,,relation_extraction,2,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
645,relation_extraction2,84,Dataset and Evaluation Metric,,,relation_extraction,2,0,1,0,,0.001325931,0,negative,1.14E-05,3.75E-05,1.01E-05,1.02E-06,1.41E-06,6.03E-05,0.000188694,0.000755473,1.10E-05,0.987349864,0.010908869,0.000655833,8.56E-06
646,relation_extraction2,85,We use the SemEval - 2010 Task 8 dataset in our experiments .,Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,1,1,0,,0.029114598,0,negative,3.69E-05,3.48E-05,0.002207294,9.32E-07,1.30E-05,0.000131082,0.006130894,0.000452625,3.07E-06,0.987562747,8.12E-06,0.003406901,1.18E-05
647,relation_extraction2,86,"The dataset contains nine semantic relation types and one artificial relation type Other , which means that the relation does not belong to any of the nine relation types .",Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,2,1,0,,0.012027837,0,negative,2.86E-05,9.35E-06,0.000512348,2.41E-06,4.94E-05,0.00010803,0.000523496,0.000158277,2.73E-06,0.998421668,7.66E-07,0.000180158,2.76E-06
648,relation_extraction2,87,"The nine relation types are Cause - Effect , Component - Whole , Content - Container , Entity - Destination , Entity - Origin , Instrument - Agency , Member - Collection , Message - Topic and Product - Producer .",Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,3,1,0,,0.090325131,0,negative,1.11E-05,8.41E-06,0.000513141,1.76E-06,5.27E-06,0.000266279,0.00061151,0.000637053,1.41E-05,0.997839457,1.81E-06,8.57E-05,4.42E-06
649,relation_extraction2,88,"The dataset contains 10,717 sentences , with each containing two nominals e 1 and e 2 , and the corresponding relation type in the sentence .",Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,4,1,0,,0.02605625,0,negative,9.51E-05,1.44E-05,0.001119711,2.07E-05,0.000489593,0.000192033,0.002530943,0.000104953,2.79E-06,0.994635747,3.49E-06,0.000773799,1.67E-05
650,relation_extraction2,89,"The relation is directional , which means that Component - Whole ( e1 , e 2 ) is different from Component - Whole ( e 2 , e 1 ) .",Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,5,1,0,,0.00072651,0,negative,2.03E-05,3.69E-06,0.000688649,6.41E-08,1.84E-07,1.93E-05,9.04E-05,8.60E-05,1.49E-05,0.998911358,3.02E-06,0.000161262,9.52E-07
651,relation_extraction2,90,"The dataset has already been partitioned into 8,000 training instances and 2,717 test instances .",Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,6,1,0,,0.026406054,0,negative,3.85E-05,6.97E-06,0.000157546,1.50E-05,8.95E-05,0.000466907,0.001596535,0.000417969,3.16E-06,0.996919611,1.30E-06,0.000272916,1.41E-05
652,relation_extraction2,91,We evaluate our solution by using the SemEval - 2010 Task 8 official scorer script .,Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,7,1,0,,0.01666275,0,negative,1.73E-05,5.81E-06,0.000124991,2.48E-07,2.16E-06,5.62E-05,0.001381906,0.000232688,8.84E-07,0.995985249,1.78E-06,0.002187779,3.00E-06
653,relation_extraction2,92,It computes the macro-averaged F1 - scores for the nine actual relations ( excluding Other ) and considers directionality .,Dataset and Evaluation Metric,Dataset and Evaluation Metric,relation_extraction,2,8,1,0,,0.03110117,0,negative,0.000264596,0.000101874,0.286495948,3.75E-07,4.03E-06,7.31E-05,0.002348963,0.000219359,0.000109538,0.708274951,1.27E-05,0.002079572,1.50E-05
654,relation_extraction2,93,Parameter Settings,Dataset and Evaluation Metric,,relation_extraction,2,9,1,0,,0.001317226,0,negative,1.32E-05,3.08E-05,0.000203218,1.20E-06,8.19E-07,0.000804239,0.002237726,0.005999359,5.11E-05,0.990276378,1.12E-05,0.000359588,1.11E-05
655,relation_extraction2,94,Table shows the major parameters used in our experiments .,Dataset and Evaluation Metric,Parameter Settings,relation_extraction,2,10,1,0,,5.34E-05,0,negative,1.94E-05,8.18E-07,6.03E-05,9.27E-07,1.42E-06,0.001238326,0.000221528,0.001041988,1.37E-06,0.997310164,5.00E-07,9.66E-05,6.66E-06
656,relation_extraction2,95,We add dropout before each add - on layer .,Dataset and Evaluation Metric,Parameter Settings,relation_extraction,2,11,1,1,hyperparameters,0.455743515,0,negative,0.008959747,0.000251779,0.028677079,0.000170734,6.68E-05,0.156362237,0.04466104,0.243336328,0.000423172,0.513165734,5.42E-06,0.000953366,0.002966585
657,relation_extraction2,96,"For the pre-trained BERT model , we use the uncased basic model .",Dataset and Evaluation Metric,Parameter Settings,relation_extraction,2,12,1,1,hyperparameters,0.214562123,0,negative,0.000319413,5.35E-05,0.126953614,1.59E-06,2.92E-06,0.03815392,0.017311346,0.074908617,0.000210789,0.741482918,1.88E-06,0.000278735,0.000320736
658,relation_extraction2,97,"For the parameters of the pre-trained BERT model , please refer to for details .",Dataset and Evaluation Metric,Parameter Settings,relation_extraction,2,13,1,0,,4.36E-05,0,negative,1.19E-05,3.85E-07,3.18E-05,2.62E-07,3.44E-07,0.000486899,5.37E-05,0.000608072,8.56E-07,0.998783659,3.76E-08,2.08E-05,1.24E-06
659,relation_extraction2,98,Comparison with other,Dataset and Evaluation Metric,,relation_extraction,2,14,1,0,,0.044223564,0,negative,1.08E-05,1.41E-06,0.000685743,2.24E-08,7.25E-08,4.32E-05,0.002323624,0.000200071,2.46E-06,0.992476021,3.66E-06,0.00424896,3.93E-06
660,relation_extraction2,99,Methods,,,relation_extraction,2,0,1,0,,2.98E-05,0,negative,4.58E-06,0.000323553,9.58E-06,8.17E-07,1.29E-06,6.39E-05,1.47E-05,0.001183839,0.000161795,0.996781926,0.001437094,1.57E-05,1.25E-06
661,relation_extraction2,100,"We compare our method , R - BERT , against results by multiple methods recently published for the SemEval - 2010 Task 8 dataset , including SVM , RNN , MVRNN , CNN + Softmax , FCM , CR - CNN , Attention - CNN , Entity Attention Bi-LSTM .",Methods,Methods,relation_extraction,2,1,1,1,baselines,0.183024143,0,negative,0.001117046,0.026260172,0.185922208,4.06E-05,0.00038441,0.001452468,0.00582861,0.017913329,0.001960736,0.74198189,0.002293211,0.014687004,0.000158273
662,relation_extraction2,101,"The SVM method by uses a rich feature set in a traditional way , which was the best result during the SemEval - 2010 task 8 competition .",Methods,Methods,relation_extraction,2,2,1,0,,0.196457804,0,negative,0.000703082,0.001959952,0.095743615,7.74E-05,5.66E-05,0.003674596,0.006930639,0.031727234,0.000868147,0.813242614,0.04061783,0.003973544,0.000424761
663,relation_extraction2,102,Details of all other methods are briefly reviewed in Section 2 . reports the results .,Methods,Methods,relation_extraction,2,3,1,0,,0.036087123,0,negative,0.000130935,0.000412066,0.000414107,6.81E-05,6.30E-05,0.000667034,0.000164443,0.005852697,0.000425842,0.991540054,0.000108175,0.000139823,1.37E-05
664,relation_extraction2,103,We can see that R - BERT significantly beats all the baseline methods .,Methods,Methods,relation_extraction,2,4,1,1,results,0.943220488,1,results,0.014575638,0.000334258,0.004329239,4.19E-05,7.49E-05,0.000355028,0.016618373,0.005009317,4.29E-05,0.068310422,0.000629956,0.888969613,0.000708478
665,relation_extraction2,104,"The MACRO F1 value of R - BERT is 89. 25 , which is much better than the previous best solution on this dataset .",Methods,Methods,relation_extraction,2,5,1,1,results,0.876802641,1,results,0.00839677,0.001173022,0.005194384,5.44E-05,0.000152231,0.001512646,0.02219367,0.029115078,0.000137514,0.205877761,0.000756671,0.72451144,0.000924401
666,relation_extraction2,105,Ablation Studies,,,relation_extraction,2,0,1,0,,0.021159808,0,negative,0.034012097,0.00021365,0.002345379,9.76E-05,6.44E-05,0.000255535,0.001237971,0.00044869,0.000112365,0.947355862,0.002829733,0.010971711,5.50E-05
667,relation_extraction2,106,Effect of Model Components,,,relation_extraction,2,0,1,0,,0.679572328,1,negative,0.341522581,0.000116849,0.00028623,0.000112914,5.58E-05,0.000193028,0.001200534,0.000405279,6.77E-05,0.609558659,0.000231484,0.046209042,3.99E-05
668,relation_extraction2,107,We have demonstrated the strong empirical results based on the proposed approach .,Effect of Model Components,Effect of Model Components,relation_extraction,2,1,1,0,,0.018400271,0,negative,0.050335402,2.98E-06,3.52E-05,2.61E-06,1.67E-05,0.000102016,0.00013596,9.58E-06,5.17E-06,0.947816236,4.96E-05,0.001215648,0.000272901
669,relation_extraction2,108,We further want to understand the specific contributions by the components besides the pre-trained BERT component .,Effect of Model Components,Effect of Model Components,relation_extraction,2,2,1,0,,0.005204303,0,negative,0.047090317,3.79E-05,0.000163485,9.70E-06,1.51E-05,0.000447766,6.83E-05,6.29E-05,0.000407598,0.950714143,0.000198733,0.000237595,0.000546465
670,relation_extraction2,109,"For this purpose , we create three more configurations .",Effect of Model Components,Effect of Model Components,relation_extraction,2,3,1,0,,0.101306196,0,negative,0.273192895,0.000193175,0.005168311,6.41E-06,0.000120048,0.000728817,0.000845851,0.000117256,0.000637047,0.717646235,1.78E-05,0.0009174,0.000408735
671,relation_extraction2,110,The first configuration is to discard the special separate tokens ( i.e. ' $ ' and ' # ') around the two SVM 82.2 RNN 77.6 MVRNN 82.4,Effect of Model Components,Effect of Model Components,relation_extraction,2,4,1,0,,0.029223714,0,ablation-analysis,0.651235656,0.000541064,0.11197296,1.33E-05,0.000139795,0.001266343,0.001643092,0.000104667,0.002372274,0.227714946,8.86E-05,0.001337544,0.00156981
672,relation_extraction2,111,CNN+ Softmax 82.7 FCM 83.0 84.1,Effect of Model Components,Effect of Model Components,relation_extraction,2,5,1,0,,0.021767175,0,negative,0.397353491,3.68E-05,0.008375203,0.000519124,0.000822054,0.007362998,0.003332396,0.000159456,0.000173455,0.47486796,0.000226099,0.002843297,0.10392766
673,relation_extraction2,112,CR,Effect of Model Components,,relation_extraction,2,6,1,0,,0.026354753,0,negative,0.016085295,4.20E-05,0.000140686,1.15E-05,1.52E-05,0.000592627,0.000133992,9.23E-05,0.000742533,0.979280599,0.000742879,0.000146961,0.001973497
674,relation_extraction2,113,Attention CNN 85.9,Effect of Model Components,,relation_extraction,2,7,1,0,,0.025604229,0,negative,0.246088867,3.32E-05,0.001539306,0.001767198,0.001653333,0.010257861,0.003314147,0.000236008,0.000113002,0.568487289,0.000153604,0.002301547,0.164054666
675,relation_extraction2,114,Att-Pooling-CNN 88.0,Effect of Model Components,,relation_extraction,2,8,1,0,,0.100972054,0,negative,0.338084953,3.96E-05,0.003406746,0.000414274,0.00075954,0.009845706,0.00799495,0.000376365,0.000115873,0.445971458,0.000117017,0.005025491,0.187848036
676,relation_extraction2,115,Entity Attention Bi-LSTM 85.2 R- BERT 89.25,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,9,1,0,,0.003206026,0,negative,0.005538988,9.47E-07,0.001152085,0.000175035,1.32E-05,0.000379203,0.00083828,0.000155587,4.51E-06,0.802949796,1.87E-05,0.176084069,0.012689567
677,relation_extraction2,116,entities in the sentence and discard the hidden vector output of the two entities from concatenating with the hidden vector output of the sentence .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,10,1,0,,0.003992692,0,negative,0.002645938,1.98E-06,0.008021546,8.31E-07,8.08E-07,1.85E-05,2.03E-05,3.25E-05,1.65E-05,0.9737839,3.05E-06,0.015390129,6.41E-05
678,relation_extraction2,117,"In other words , we add ' [ CLS ] ' at the beginning of the sentence and feed the sentence with the two entities into the BERT module , and use the first output vector for classification .",Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,11,1,0,,0.025394276,0,negative,0.001244983,1.67E-05,0.032073865,7.18E-07,8.07E-07,3.20E-05,1.58E-05,0.000183886,0.0003597,0.964362906,3.95E-06,0.001631842,7.28E-05
679,relation_extraction2,118,We label this method as BERT - NO - SEP - NO - ENT .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,12,1,0,,0.000950423,0,negative,0.001009634,8.69E-06,0.072526065,1.17E-06,1.43E-06,2.43E-05,3.24E-05,4.85E-05,4.55E-05,0.91986668,7.65E-06,0.006296405,0.000131625
680,relation_extraction2,119,"The second configuration is to discard the special separate tokens ( i.e. ' $ ' and ' # ' ) around the two entities in the sentence , but keep the hidden vector output of the two entities in concatenation for classification .",Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,13,1,0,,0.067332405,0,negative,0.005699476,0.000112553,0.077949141,5.38E-06,6.48E-06,6.11E-05,7.98E-05,0.000212834,0.000475635,0.903250671,2.60E-05,0.011864234,0.000256808
681,relation_extraction2,120,We label this method as BERT - NO - SEP .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,14,1,0,,0.014724009,0,negative,0.001232576,1.70E-05,0.17237299,9.59E-07,1.47E-06,2.36E-05,6.21E-05,5.57E-05,5.22E-05,0.813944732,1.34E-05,0.012082684,0.000140609
682,relation_extraction2,121,"The third configuration is to discard the hidden vector output of the two entities from concatenation for classification , but keep the special separate tokens .",Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,15,1,0,,0.092264856,0,negative,0.007136066,6.47E-05,0.104382151,3.97E-06,4.52E-06,5.31E-05,7.52E-05,0.000163344,0.000468106,0.875800787,1.49E-05,0.011596933,0.000236267
683,relation_extraction2,122,We label this method as BERT - NO - ENT .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,16,1,0,,0.001476311,0,negative,0.00079695,8.17E-06,0.054592336,1.25E-06,1.31E-06,2.42E-05,2.99E-05,5.02E-05,4.75E-05,0.939420625,7.24E-06,0.004861008,0.000159253
684,relation_extraction2,123,reports the results of the ablation study with the above three configurations .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,17,1,0,,0.002342493,0,negative,0.001400827,1.20E-06,0.001328505,4.47E-06,2.62E-06,1.48E-05,2.18E-05,2.25E-05,7.00E-06,0.981838824,3.02E-06,0.015086926,0.000267564
685,relation_extraction2,124,We observe that the three methods all perform worse than R - BERT .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,18,1,1,ablation-analysis,0.72339573,1,results,0.006799008,6.52E-07,3.83E-05,3.52E-06,6.69E-07,2.72E-05,0.000810617,8.31E-05,4.04E-07,0.140042363,3.91E-06,0.851239078,0.000951222
686,relation_extraction2,125,"Of the methods , BERT - NO - SEP - NO - ENT performs worst , with its F1 8.16 absolute points worse than R - BERT .",Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,19,1,1,ablation-analysis,0.844903729,1,results,0.003964267,1.64E-07,7.34E-05,8.64E-07,2.37E-07,5.43E-06,0.000387605,1.03E-05,8.74E-08,0.034107938,9.35E-07,0.961075112,0.000373634
687,relation_extraction2,126,This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,20,1,1,ablation-analysis,0.759558333,1,negative,0.054307098,3.72E-06,0.000382047,4.40E-06,2.78E-06,1.63E-05,0.00014477,3.90E-05,6.68E-06,0.594094566,2.75E-06,0.350632486,0.000363335
688,relation_extraction2,127,"In relation classification , the relation label is de-pendent on both the semantics of the sentence and the two target entities .",Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,21,1,0,,0.003978978,0,negative,0.000673743,3.14E-05,0.004228382,1.38E-05,3.60E-06,8.08E-05,7.65E-05,0.000348029,9.21E-05,0.974710435,0.000267487,0.016075766,0.003397973
689,relation_extraction2,128,BERT without special separate tokens can not locate the target entities and lose this key information .,Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,22,1,1,ablation-analysis,0.003290386,0,negative,0.005561656,1.63E-06,0.002808261,2.64E-06,1.87E-06,1.42E-05,0.000111585,1.61E-05,3.24E-06,0.681642435,1.84E-05,0.308541379,0.001276519
690,relation_extraction2,129,"The reason why the special separate tokens help to improve the accuracy is that they identify the locations of the two target entities and transfer the information into the BERT model , which make the BERT output contain the location information of the two entities .",Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,23,1,0,,0.006855923,0,negative,0.071690475,8.35E-07,0.000275383,9.08E-07,7.86E-07,5.73E-06,7.10E-05,1.40E-05,1.41E-06,0.517051105,1.02E-06,0.410786849,0.000100573
691,relation_extraction2,130,"On the other hand , incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction .",Effect of Model Components,Att-Pooling-CNN 88.0,relation_extraction,2,24,1,1,ablation-analysis,0.747252168,1,results,0.069258792,2.71E-06,0.000309742,2.12E-06,1.43E-06,1.08E-05,0.000177597,4.04E-05,4.67E-06,0.3657561,1.51E-06,0.564120123,0.000314052
692,relation_extraction2,131,Conclusions,,,relation_extraction,2,0,1,0,,0.000388901,0,negative,8.30E-05,6.93E-05,4.02E-06,8.07E-07,8.64E-07,3.36E-05,1.76E-05,0.00031599,6.31E-05,0.998796926,0.000458235,0.000155019,1.50E-06
693,sentiment_analysis1,1,title,,,sentiment_analysis,1,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
694,sentiment_analysis1,2,EEG - Based Emotion Recognition Using Regularized Graph Neural Networks,title,title,sentiment_analysis,1,1,1,1,research-problem,0.998742976,1,research-problem,3.34E-08,7.46E-06,7.03E-08,6.53E-08,4.25E-08,9.50E-08,1.12E-06,1.70E-06,8.90E-07,0.002110658,0.997877552,2.51E-07,5.59E-08
695,sentiment_analysis1,3,abstract,,,sentiment_analysis,1,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
696,sentiment_analysis1,4,EEG signals measure the neuronal activities on different brain regions via electrodes .,abstract,abstract,sentiment_analysis,1,1,1,0,,0.020937622,0,research-problem,1.91E-07,0.000234248,2.30E-07,4.12E-06,5.23E-06,7.99E-06,1.66E-06,5.64E-05,6.27E-05,0.291368866,0.708258009,1.33E-07,2.57E-07
697,sentiment_analysis1,5,Many existing studies on EEG - based emotion recognition do not exploit the topological structure of EEG signals .,abstract,abstract,sentiment_analysis,1,2,1,0,,0.610733248,1,research-problem,2.82E-08,1.18E-05,1.05E-08,2.60E-07,1.66E-07,1.99E-07,2.16E-07,1.88E-06,4.51E-07,0.024401656,0.975583272,2.08E-08,2.07E-08
698,sentiment_analysis1,6,"In this paper , we propose a regularized graph neural network ( RGNN ) for EEG - based emotion recognition , which is biologically supported and captures both local and global inter-channel relations .",abstract,abstract,sentiment_analysis,1,3,1,0,,0.881730222,1,research-problem,3.33E-06,0.009974039,3.02E-05,3.36E-06,1.80E-05,4.01E-06,9.29E-06,5.57E-05,0.00063421,0.020686532,0.968576826,3.25E-06,1.29E-06
699,sentiment_analysis1,7,"Specifically , we model the inter-channel relations in EEG signals via an adjacency matrix in our graph neural network where the connection and sparseness of the adjacency matrix are supported by the neurosicience theories of human brain organization .",abstract,abstract,sentiment_analysis,1,4,1,0,,0.261597807,0,approach,3.89E-05,0.392139918,0.000148451,1.91E-05,0.000285442,8.73E-05,1.52E-05,0.001255457,0.176855554,0.33890607,0.090237948,7.31E-06,3.43E-06
700,sentiment_analysis1,8,"In addition , we propose two regularizers , namely node - wise domain adversarial training ( NodeDAT ) and emotion - aware distribution learning ( EmotionDL ) , to improve the robustness of our model against cross - subject EEG variations and noisy labels , respectively .",abstract,abstract,sentiment_analysis,1,5,1,0,,0.416842223,0,approach,0.000223724,0.797778153,0.000185642,0.000319203,0.0043737,0.000145723,4.10E-05,0.001707417,0.025799735,0.155906426,0.013490822,1.89E-05,9.50E-06
701,sentiment_analysis1,9,"To thoroughly evaluate our model , we conduct extensive experiments in both subject - dependent and subject - independent classification settings on two public datasets : SEED and SEED - IV .",abstract,abstract,sentiment_analysis,1,6,1,0,,0.03577958,0,negative,2.26E-05,0.048945404,9.28E-06,0.000179641,0.007495412,0.000107715,2.46E-05,0.000743188,0.000211331,0.936084216,0.006167679,7.65E-06,1.22E-06
702,sentiment_analysis1,10,"Our model obtains better performance than competitive baselines such as SVM , DBN , DGCNN , BiDANN , and the state - of - the - art BiHDM in most experimental settings .",abstract,abstract,sentiment_analysis,1,7,1,0,,0.018117679,0,negative,5.93E-05,0.002843789,5.06E-06,9.14E-05,0.000158141,0.000125118,0.000294162,0.001709725,4.70E-05,0.59254789,0.401516893,0.00058566,1.59E-05
703,sentiment_analysis1,11,Our model analysis demonstrates that the proposed biologically supported adjacency matrix and two regularizers contribute consistent and significant gain to the performance .,abstract,abstract,sentiment_analysis,1,8,1,0,,0.017765916,0,negative,0.004706673,0.01040313,1.88E-05,0.00025988,0.000995737,8.55E-05,8.05E-05,0.000723694,0.000624631,0.952924853,0.028780386,0.000387574,8.63E-06
704,sentiment_analysis1,12,"Investigations on the neuronal activities reveal that pre-frontal , parietal and occipital regions maybe the most informative regions for emotion recognition , which is consistent with relevant prior studies .",abstract,abstract,sentiment_analysis,1,9,1,0,,0.007533685,0,negative,1.41E-05,0.001112268,1.80E-06,3.69E-06,4.49E-05,5.07E-06,4.34E-06,5.28E-05,9.31E-05,0.792229398,0.206432063,6.24E-06,3.21E-07
705,sentiment_analysis1,13,"In addition , experimental results suggest that global inter-channel relations between the left and right hemispheres are important for emotion recognition and local inter-channel relations between ( FP1 , AF3 ) , ( F6 , F8 ) and ( FP2 , AF4 ) may also provide useful information .",abstract,abstract,sentiment_analysis,1,10,1,0,,0.002929375,0,research-problem,9.10E-06,0.00194907,1.48E-06,1.90E-06,1.51E-05,2.83E-06,3.20E-06,5.46E-05,0.000278997,0.449901049,0.547776827,5.50E-06,3.46E-07
706,sentiment_analysis1,14,INTRODUCTION,,,sentiment_analysis,1,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
707,sentiment_analysis1,15,"E MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,1,1,1,research-problem,0.404243101,0,research-problem,6.63E-07,0.000117625,3.86E-07,2.26E-06,6.19E-06,2.12E-06,7.85E-06,2.95E-06,3.81E-05,0.033846917,0.965972878,1.12E-06,9.02E-07
708,sentiment_analysis1,16,"Compared to other modalities , physiological signals , such as electroencephalogram ( EEG ) , electrocardiogram ( ECG ) , electromyogram ( EMG ) , galvanic skin response ( GSR ) , etc. , have the advantage of being difficult to hide or disguise .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,2,1,0,,0.014301846,0,negative,5.79E-06,0.001262641,1.76E-06,4.67E-05,0.000282756,4.63E-05,1.88E-05,3.35E-05,0.000587615,0.551954135,0.445753067,3.56E-06,3.35E-06
709,sentiment_analysis1,17,"In recent years , due to the rapid development of noninvasive , easy - to - use and inexpensive EEG recording devices , EEG - based emotion recognition has received an increasing amount of attention in both research and applications .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,3,1,0,,0.818446915,1,research-problem,8.39E-07,0.000216061,2.15E-07,5.91E-06,1.09E-05,5.13E-06,8.78E-06,9.21E-06,6.95E-05,0.050941743,0.948729342,9.43E-07,1.43E-06
710,sentiment_analysis1,18,Emotion models can be broadly categorized into discrete models and dimensional models .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,4,1,0,,0.112603542,0,research-problem,1.71E-06,0.001397783,2.04E-06,2.96E-06,1.20E-05,2.82E-05,2.71E-05,6.47E-05,0.003962574,0.1741438,0.820351031,3.17E-06,2.84E-06
711,sentiment_analysis1,19,"The former categorizes emotions into discrete entities , e.g. , anger , disgust , fear , happiness , sadness , and surprise in Ekman 's theory .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,5,1,0,,0.134440779,0,negative,2.60E-05,0.011851684,3.79E-05,4.55E-05,0.000976687,0.000238778,7.13E-05,0.00016189,0.012685947,0.837090315,0.136797992,8.73E-06,7.14E-06
712,sentiment_analysis1,20,"The latter describes emotions using their underlying dimensions , e.g. , valence , arousal and dominance , which measures emotions from unpleasant to pleasant , passive to active , and submissive to dominant , respectively .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,6,1,0,,0.100596216,0,negative,5.36E-05,0.075772231,0.000178219,8.80E-05,0.004381356,0.000428018,0.000149749,0.000304088,0.070873678,0.771312691,0.07641964,2.37E-05,1.50E-05
713,sentiment_analysis1,21,EEG signals measure voltage fluctuations from the cortex in the brain and have been shown to reveal important information about human emotional states .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,7,1,0,,0.041639247,0,research-problem,2.86E-06,0.001048311,2.03E-06,4.42E-05,0.000101906,7.74E-05,3.47E-05,6.96E-05,0.000754484,0.290325907,0.707528479,2.90E-06,7.21E-06
714,sentiment_analysis1,22,"For example , greater relative left frontal EEG activity has been observed P. Zhong , D. when experiencing positive emotions .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,8,1,0,,0.007888827,0,negative,8.60E-06,0.002120115,2.76E-06,9.44E-06,7.33E-05,8.67E-05,3.75E-05,0.000126428,0.004237477,0.80800577,0.185281048,7.64E-06,3.26E-06
715,sentiment_analysis1,23,The voltage fluctuations on different brain regions are measured by electrodes attached to the scalp .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,9,1,0,,0.635248351,1,model,3.76E-05,0.204631923,7.12E-05,1.26E-05,0.001130882,0.000113615,5.66E-05,0.000247746,0.5202824,0.243182324,0.030210748,1.65E-05,5.88E-06
716,sentiment_analysis1,24,Each electrode collects EEG signals in one channel .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,10,1,0,,0.801150738,1,model,0.000131738,0.268109444,0.000286944,8.83E-05,0.011484423,0.000475951,0.000176413,0.00045324,0.49621945,0.216863435,0.005658048,3.19E-05,2.07E-05
717,sentiment_analysis1,25,"The collected EEG signals are often analyzed in specific frequency bands for each channel , namely delta ( 1 - 4 Hz ) , thet a ( 4 -7 Hz ) , alpha , beta , and gamma ( > 30 Hz ) .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,11,1,0,,0.036822207,0,negative,4.82E-06,0.0048452,2.33E-06,1.53E-05,0.000177218,0.000114414,3.82E-05,0.00017683,0.004131084,0.809885874,0.180599616,5.00E-06,4.07E-06
718,sentiment_analysis1,26,Many existing EEG - based emotion recognition methods are primarily based on the supervised machine learning approach wherein features are extracted from preprocessed EEG signals in each channel over a time window and then a classifier is trained on the extracted features to recognize emotions .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,12,1,0,,0.206755758,0,research-problem,3.36E-06,0.001081458,1.80E-06,5.10E-05,9.63E-05,6.60E-05,4.53E-05,6.63E-05,0.000273224,0.224246133,0.774057277,3.43E-06,8.27E-06
719,sentiment_analysis1,27,Wang et al.,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,13,1,0,,0.020358684,0,negative,0.000180396,0.011917611,5.61E-05,0.00015018,0.004473833,0.000553549,0.000170734,0.000306009,0.017750865,0.955175324,0.009206901,4.26E-05,1.59E-05
720,sentiment_analysis1,28,"compared power spectral density features ( PSD ) , wavelet features and nonlinear dynamical features with a Support Vector Machine ( SVM ) classifier .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,14,1,0,,0.011733,0,negative,0.000332027,0.11702186,0.000778022,0.000166099,0.033747889,0.000567878,0.000401631,0.000219338,0.028114676,0.807604184,0.010897608,0.000125896,2.29E-05
721,sentiment_analysis1,29,"Zheng and Lu investigated critical frequency bands and channels using PSD , differential entropy ( DE ) and PSD asymmetry features , and obtained robust accuracy using deep belief networks ( DBN ) .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,15,1,0,,0.016624321,0,negative,6.75E-05,0.015248807,4.25E-05,6.27E-05,0.001385391,0.000402826,0.000182025,0.000242361,0.010005448,0.913301995,0.059014196,2.96E-05,1.48E-05
722,sentiment_analysis1,30,"However , most existing EEGbased emotion recognition approaches do not address the following three challenges :",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,16,1,0,,0.023776592,0,research-problem,8.56E-06,0.003024494,3.43E-06,1.50E-05,8.85E-05,2.60E-05,4.19E-05,3.53E-05,0.00085912,0.420658224,0.575226653,9.01E-06,3.75E-06
723,sentiment_analysis1,31,"1 ) the topological structure of EEG signals are not effectively exploited to learn more discriminative EEG representations ; 2 ) EEG signals vary significantly across different subjects , which hinders the generalizability of the trained classifiers ; and 3 ) participants may not always generate the intended emotions when watching emotion - eliciting stimuli .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,17,1,0,,0.05520824,0,negative,4.59E-05,0.020296263,6.63E-06,3.15E-05,0.000908611,8.54E-05,6.40E-05,0.000123278,0.004392988,0.910008176,0.064003962,2.86E-05,4.73E-06
724,sentiment_analysis1,32,"Consequently , the emotion labels in the collected EEG data are noisy and may not be consistent with the actual elicited emotions .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,18,1,0,,0.020258556,0,negative,5.62E-06,0.005419379,1.48E-06,5.08E-05,0.000589557,0.00013099,3.24E-05,0.000206677,0.002409972,0.960991406,0.030152329,5.55E-06,3.83E-06
725,sentiment_analysis1,33,There have been several attempts to address the first challenge .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,19,1,0,,0.005982486,0,negative,8.42E-06,0.002212456,3.97E-06,5.71E-05,0.000185068,0.000177646,6.90E-05,0.000145681,0.002074183,0.870414157,0.12463607,8.41E-06,7.81E-06
726,sentiment_analysis1,34,Zhang et al. and Zhang et al .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,20,1,0,,0.011606362,0,negative,0.000106936,0.003732215,3.61E-05,4.30E-05,0.002247567,0.000330366,0.000194447,0.00013296,0.004781213,0.986390924,0.001950924,4.51E-05,8.14E-06
727,sentiment_analysis1,35,"incorporated spatial relations in EEG signals using convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) , respectively .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,21,1,0,,0.032489193,0,negative,0.000223284,0.053055192,0.000320283,9.30E-05,0.002011247,0.000224533,0.00054987,0.000187878,0.037506765,0.713049484,0.192589698,0.000147452,4.14E-05
728,sentiment_analysis1,36,"However , their approaches require a 2D representation of EEG channels on the scalp , which may cause information loss during flattening because channels are actually arranged in the 3D space .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,22,1,0,,0.017325422,0,negative,1.80E-05,0.013827772,1.19E-05,0.000118776,0.000619672,0.000440133,0.00013717,0.000467753,0.007163658,0.897526426,0.079635218,1.64E-05,1.72E-05
729,sentiment_analysis1,37,"In addition , their approach of using CNNs and RNNs to capture inter-channel relations has difficulty in learning long - range dependencies .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,23,1,0,,0.014270929,0,negative,1.95E-05,0.005817332,1.05E-05,4.37E-05,0.00053764,0.000223985,0.000122035,0.000165692,0.001877834,0.937156809,0.053994361,2.00E-05,1.06E-05
730,sentiment_analysis1,38,Graph neural networks ( GNN ) has been applied in to capture inter-channel relations using an adjacency matrix .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,24,1,0,,0.174200723,0,negative,1.27E-05,0.01180887,4.30E-05,1.60E-05,9.05E-05,0.000211526,0.000245445,0.000315737,0.046622594,0.543756563,0.396835036,2.11E-05,2.10E-05
731,sentiment_analysis1,39,"However , similar to CNNs and RNNs , their approach only considers relations between the nearest channels , which thus may lose valuable information between distant channels , such as PSD asymmetry between channels on the left and right hemispheres in the frontal region , which has been shown as informative in valence prediction .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,25,1,0,,0.110750734,0,negative,5.23E-05,0.0572376,5.23E-05,1.29E-05,0.000611769,0.000166831,0.00011968,0.000241227,0.051927079,0.858694034,0.030840976,3.58E-05,7.54E-06
732,sentiment_analysis1,40,A recent work applies RNNs to learn EEG representations in the two hemispheres separately and then adopts the asymmetric differences between them to recognize emotions .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,26,1,0,,0.016654084,0,negative,2.01E-05,0.011189492,3.50E-05,5.14E-05,0.000441396,0.00050073,0.000343828,0.000514996,0.011940191,0.868777086,0.106132612,2.08E-05,3.24E-05
733,sentiment_analysis1,41,"However , their approach is limited to using only the bi-hemispherical discrepancies and ignores other useful features such as neuronal activities recorded from each channel .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,27,1,0,,0.011397766,0,negative,2.04E-05,0.019064523,1.85E-05,6.39E-05,0.000822947,0.000530966,0.000162063,0.000564705,0.014863481,0.946993874,0.016861958,1.84E-05,1.43E-05
734,sentiment_analysis1,42,"In recent years , several studies , investigated the transferability of EEG - based emotion recognition models across subjects .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,28,1,0,,0.118981875,0,negative,7.08E-06,0.004761457,2.46E-06,8.54E-06,8.11E-05,6.31E-05,0.000184492,0.000161532,0.001650761,0.691094286,0.301949848,2.58E-05,9.52E-06
735,sentiment_analysis1,43,Lan et al.,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,29,1,0,,0.00828049,0,negative,0.000260799,0.008309327,0.00013142,0.000104967,0.008292027,0.000660086,0.000525687,0.000260922,0.007530359,0.972994694,0.000802122,0.000104872,2.27E-05
736,sentiment_analysis1,44,"compared several domain adaptation techniques such as maximum independence domain adaptation ( MIDA ) , transfer component analysis ( TCA ) , subspace alignment ( SA ) , etc .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,30,1,0,,0.073375737,0,negative,2.56E-05,0.017188892,2.62E-05,9.83E-06,0.000333995,8.88E-05,0.000332381,0.000154654,0.003362023,0.876493451,0.101886376,8.64E-05,1.14E-05
737,sentiment_analysis1,45,They found that the subject - independent classification accuracy can be improved by around 10 % .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,31,1,0,,0.019933686,0,negative,0.000486376,0.030910334,6.72E-05,8.56E-05,0.002276136,0.000623603,0.001425071,0.000902332,0.010959733,0.946603521,0.005150831,0.000438438,7.08E-05
738,sentiment_analysis1,46,Li et al. applied domain adversarial learning to lower the influence of individual subject on EEG data and obtained improved performance as well .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,32,1,0,,0.005046319,0,negative,6.79E-05,0.032319501,9.94E-05,0.000338301,0.004921606,0.001844577,0.001003222,0.001463851,0.010635809,0.937743819,0.009405601,5.75E-05,9.90E-05
739,sentiment_analysis1,47,"However , their approaches do not exploit any graph structure and only leads to small performance improvement ( see Section 7.1 ) .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,33,1,0,,0.002356102,0,negative,3.37E-05,0.004624549,9.02E-06,0.000112074,0.000572503,0.000554594,0.000357524,0.000556002,0.001326893,0.982534657,0.009254664,3.86E-05,2.52E-05
740,sentiment_analysis1,48,"To the best of our knowledge , no attempt has been made to address the problem of noisy labels in EEG - based emotion recognition .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,34,1,0,,0.057602584,0,negative,5.49E-06,0.009274443,5.84E-06,6.89E-05,0.000417462,0.000382031,0.000332311,0.00065862,0.001705316,0.889265646,0.097836201,1.94E-05,2.84E-05
741,sentiment_analysis1,49,"In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,35,1,1,model,0.967123284,1,model,3.13E-05,0.335070392,0.000273003,2.65E-06,0.000554652,4.63E-05,0.000182519,7.53E-05,0.648561763,0.014244964,0.000927546,1.98E-05,9.77E-06
742,sentiment_analysis1,50,"Graph analysis for human brain has been studied extensively in the neuroscience literature , .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,36,1,0,,0.180003689,0,negative,1.01E-05,0.004695573,9.98E-06,6.22E-05,0.000329558,0.000233024,0.000460533,0.000190073,0.001158725,0.801192129,0.191605785,2.76E-05,2.47E-05
743,sentiment_analysis1,51,"However , making an accurate connectome is still an open question and subject to different scales .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,37,1,0,,0.011507156,0,negative,5.34E-06,0.002351211,3.36E-06,7.42E-05,0.000477791,0.000251596,0.000120134,0.000222178,0.000952125,0.982237467,0.01328637,8.71E-06,9.55E-06
744,sentiment_analysis1,52,"Inspired by , , we consider each channel in EEG signals as a node in our graph .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,38,1,1,model,0.588139485,1,model,6.22E-06,0.070403883,2.10E-05,3.07E-06,0.000416715,0.000103174,2.96E-05,0.0002833,0.827326794,0.101332269,6.96E-05,1.70E-06,2.64E-06
745,sentiment_analysis1,53,"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,39,1,1,model,0.931115173,1,model,1.64E-05,0.101252431,0.000134441,9.18E-07,0.00025243,2.24E-05,4.68E-05,2.87E-05,0.890177183,0.007983609,7.68E-05,4.68E-06,3.20E-06
746,sentiment_analysis1,54,"Local interchannel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale , .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,40,1,1,model,0.041698227,0,negative,4.74E-05,0.021239319,4.68E-05,2.15E-05,0.001422456,0.000207825,0.000209317,0.000179363,0.057057108,0.918084799,0.001435403,3.75E-05,1.13E-05
747,sentiment_analysis1,55,"Global inter-channel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotion - related functional connectivity , .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,41,1,1,model,0.069467015,0,negative,4.48E-05,0.029355541,5.61E-05,1.33E-05,0.00074955,0.00015661,0.000317691,0.000197851,0.112378118,0.851901462,0.004761934,5.12E-05,1.58E-05
748,sentiment_analysis1,56,"In addition , we propose a node - wise domain adversarial training ( NodeDAT ) to regularize our graph model for better generalization in subject - independent classification scenarios .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,42,1,0,,0.929130455,1,approach,0.000102851,0.625523024,0.000206522,1.83E-05,0.005014832,0.000147729,0.000209881,0.000299048,0.352727719,0.015705666,1.90E-05,1.16E-05,1.38E-05
749,sentiment_analysis1,57,"Different from the domain adversarial training adopted by , , our Node DAT gives a finer - grained regularization by minimizing the domain discrepancies be-tween features in the source and target domains for each channel / node .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,43,1,0,,0.913269424,1,model,9.48E-05,0.441233239,0.000137274,3.54E-06,0.00160278,5.42E-05,9.71E-05,0.000116962,0.529245132,0.027380626,1.80E-05,1.20E-05,4.31E-06
750,sentiment_analysis1,58,"Moreover , we propose an emotion - aware distribution learning ( Emotion DL ) method to address the problem of noisy labels in the datasets .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,44,1,0,,0.936534411,1,approach,6.75E-05,0.719828702,0.00026877,7.07E-06,0.002172189,8.02E-05,0.000337595,0.000154137,0.25239764,0.024339538,0.000297973,3.34E-05,1.53E-05
751,sentiment_analysis1,59,Prior studies have shown that noisy labels can adversely impact classification accuracy .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,45,1,0,,0.066908664,0,negative,7.63E-06,0.001505233,2.70E-06,7.01E-05,0.000322392,0.000285011,0.000337631,0.000280954,0.000215834,0.986032752,0.01089871,1.96E-05,2.15E-05
752,sentiment_analysis1,60,"Instead of learning single - label classification , our Emotion DL learns a distribution of labels of the training data and thus acts as a regularizer to improve the robustness of our model against noisy labels .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,46,1,0,,0.865787258,1,model,3.71E-05,0.404975652,0.000138303,1.58E-06,0.001550873,3.53E-05,8.78E-05,6.52E-05,0.561410191,0.031657678,2.60E-05,1.06E-05,3.59E-06
753,sentiment_analysis1,61,"Finally , we conduct extensive experiments to validate the effectiveness of our proposed model and investigate emotion - related informative neuronal activities .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,47,1,0,,0.008940494,0,negative,0.000540124,0.087014557,3.73E-05,0.000170649,0.069989878,0.000834472,0.001631956,0.000860185,0.003741516,0.834963018,1.57E-05,0.000178978,2.16E-05
754,sentiment_analysis1,62,"In summary , the main contributions of this paper are as follows :",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,48,1,0,,0.001120938,0,negative,2.33E-05,0.00520933,1.32E-05,0.000449033,0.002261823,0.001434376,0.00031471,0.000722315,0.002717639,0.986617827,0.000198831,1.10E-05,2.66E-05
755,sentiment_analysis1,63,1 ) We propose a regularized graph neural network ( RGNN ) model to recognize emotions based on EEG signals .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,49,1,0,,0.941913966,1,approach,0.000325697,0.626571992,0.001354328,0.000182091,0.0099338,0.001174977,0.0037416,0.001107754,0.1516382,0.200364945,0.003154898,0.00020982,0.000239898
756,sentiment_analysis1,64,Our model is biologically supported and captures both local and global inter-channel relations .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,50,1,0,,0.626955989,1,model,2.71E-05,0.079353778,0.000111667,4.89E-06,0.00072848,0.00013609,0.000153736,0.000196526,0.837224047,0.08197917,6.31E-05,1.06E-05,1.08E-05
757,sentiment_analysis1,65,"2 ) We propose two regularizers : a node - wise domain adversarial training ( NodeDAT ) and an emotionaware distribution learning ( EmotionDL ) , which aim to improve the robustness of our model against cross - subject variations and noisy labels , respectively .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,51,1,0,,0.925777255,1,approach,0.000143768,0.644484437,0.000393871,1.29E-05,0.005494934,0.000185781,0.000432523,0.000267728,0.328632819,0.019906317,8.73E-06,1.83E-05,1.79E-05
758,sentiment_analysis1,66,"3 ) We conduct extensive experiment in both subjectdependent and subject - independent classification settings on two public EEG datasets , namely SEED and SEED - IV .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,52,1,0,,0.200825094,0,approach,0.000519275,0.546847877,0.000395934,0.000204727,0.167070102,0.00151309,0.006343569,0.001235346,0.009673182,0.26572904,1.68E-05,0.000376788,7.42E-05
759,sentiment_analysis1,67,Experimental results demonstrate the effectiveness of our proposed model and regularizers .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,53,1,0,,0.00471758,0,negative,0.000393752,0.014435376,1.16E-05,4.05E-06,0.002066066,0.000139508,0.00318267,0.00025788,0.001337337,0.977168605,2.14E-05,0.000966859,1.49E-05
760,sentiment_analysis1,68,"In addition , our RGNN achieves superior performance over the state - of - the - art baselines in most experimental settings .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,54,1,0,,0.076392635,0,negative,0.001069226,0.027830213,6.50E-05,0.00010375,0.004493494,0.002072638,0.178946897,0.004845469,0.001358741,0.760027726,0.000247114,0.01762328,0.001316453
761,sentiment_analysis1,69,"4 ) We investigate the neuronal activities and the results reveal that pre-frontal , parietal and occipital regions maybe the most informative regions for emotion recognition .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,55,1,0,,0.127203338,0,negative,0.003816819,0.205594921,0.000441979,0.000231897,0.043609754,0.001632037,0.007678746,0.001439526,0.052449906,0.682303079,4.57E-05,0.000586078,0.000169575
762,sentiment_analysis1,70,"In addition , global inter-channel relations between the left and right hemispheres are important and local inter-channel relations between ( FP1 , AF3 ) , ( F6 , F8 ) and ( FP2 , AF4 ) may also provide useful information .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,56,1,0,,0.034081749,0,negative,0.00300584,0.039092117,9.78E-05,2.79E-05,0.005323528,0.000399427,0.001975293,0.000427715,0.024981877,0.924281921,4.13E-05,0.000317902,2.73E-05
763,sentiment_analysis1,71,RELATED WORK,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,57,1,0,,0.000850769,0,negative,3.27E-06,0.001268993,4.44E-06,4.44E-06,0.00028566,0.000286099,0.000323975,0.000207967,0.000595572,0.996920829,8.14E-05,9.88E-06,7.43E-06
764,sentiment_analysis1,72,"In this section , we review related work in the fields of EEG - based emotion recognition , graph neural networks , unsupervised domain adaptation and learning with noisy labels .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,58,1,0,,0.01705903,0,negative,5.71E-06,0.005857465,1.07E-05,7.94E-05,0.001659682,0.000714258,0.000656375,0.000550343,0.000726627,0.988874388,0.000812553,1.65E-05,3.60E-05
765,sentiment_analysis1,73,EEG - Based Emotion Recognition,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,59,1,0,,0.581904059,1,negative,0.000141577,0.024890752,0.000176942,3.76E-05,0.001120809,0.001248397,0.181137168,0.001894367,0.007365892,0.76357695,0.011182828,0.004968966,0.002257753
766,sentiment_analysis1,74,EEG feature extractors and classifiers are the two fundamental components in the machine learning approach of EEGbased emotion recognition .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,60,1,0,,0.077182865,0,negative,1.72E-05,0.005437944,4.08E-05,0.000279659,0.001335162,0.001756912,0.003432715,0.000995738,0.001408839,0.979557992,0.005412969,7.04E-05,0.000253667
767,sentiment_analysis1,75,EEG features can be broadly divided into single - channel features and multi-channel ones .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,61,1,0,,0.015692898,0,negative,5.67E-06,0.002847289,1.52E-05,1.01E-05,0.000593903,0.000447255,0.0005386,0.000374908,0.001738602,0.993157062,0.00023061,1.80E-05,2.27E-05
768,sentiment_analysis1,76,"The majority of existing features are single - channel features such as statistical features , , fractal dimension ( FD ) , PSD , differential entropy ( DE ) , and wavelet features .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,62,1,0,,0.017816142,0,negative,9.67E-06,0.004573841,2.04E-05,2.43E-05,0.000681542,0.001165746,0.001036647,0.000988032,0.001494271,0.989685495,0.000259983,1.80E-05,4.21E-05
769,sentiment_analysis1,77,"A few features are computed on multiple channels to capture the inter-channel relations , e.g. , the asymmetry features of PSD and functional connectivity , , where common indices such as correlation , coherence and phase synchronization were used estimate brain functional connectivity between channels .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,63,1,0,,0.00617664,0,negative,1.60E-05,0.004133716,1.95E-05,1.24E-05,0.000825639,0.000400174,0.000283553,0.000261697,0.002420036,0.991566688,4.01E-05,1.12E-05,9.24E-06
770,sentiment_analysis1,78,"However , leveraging functional connectivity require labor - intensive manual connectivity analysis for each subject and may not be ideal for real - time applications .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,64,1,0,,0.003041263,0,negative,1.53E-05,0.004112548,8.94E-06,5.48E-05,0.001009773,0.000442776,0.000672576,0.000414386,0.000541915,0.99241755,0.000251726,3.09E-05,2.67E-05
771,sentiment_analysis1,79,EEG classifiers can be broadly divided into topologyinvariant classifiers and topology - aware ones .,INTRODUCTION,INTRODUCTION,sentiment_analysis,1,65,1,0,,0.113964893,0,negative,9.09E-06,0.00783619,4.90E-05,6.91E-06,0.000485068,0.000487973,0.00104945,0.000472596,0.004901527,0.98426321,0.000377081,2.68E-05,3.51E-05
772,sentiment_analysis1,80,"The majority of existing classifiers are topology - invariant classifiers such as SVM , k - Nearest Neighbors ( KNN ) , DBNs and RNNs , which do not take the topological structure of EEG features into account when learning the EEG representations .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,66,1,0,,0.075949857,0,negative,9.91E-06,0.005675023,3.38E-05,3.45E-05,0.000745205,0.00146082,0.001593368,0.001168336,0.001444135,0.987451821,0.000292397,2.13E-05,6.93E-05
773,sentiment_analysis1,81,"In contrast , topology - aware classifiers such as CNNs , , , and GNNs consider the inter-channel topological relations and learn EEG representations for each channel by aggregating features from nearby channels using convolutional operations either in the Euclidean space or in the non-Euclidean space .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,67,1,0,,0.058620774,0,negative,2.32E-05,0.035365595,0.000188281,1.38E-05,0.001077645,0.000690969,0.000977398,0.000660047,0.040861497,0.91990591,0.000168873,2.40E-05,4.27E-05
774,sentiment_analysis1,82,"However , as discussed in Section 1 , existing CNNs and GNNs have difficulty in learning the dependencies between distant channels , which may reveal important emotion - related information .",INTRODUCTION,INTRODUCTION,sentiment_analysis,1,68,1,0,,0.022982581,0,negative,6.95E-06,0.001816415,5.29E-06,4.34E-05,0.00052234,0.000453709,0.00078008,0.000375014,0.00020828,0.995386276,0.00035386,1.98E-05,2.85E-05
775,sentiment_analysis1,83,"Recently ,",INTRODUCTION,,sentiment_analysis,1,69,1,0,,0.001054856,0,negative,8.58E-06,0.001631222,5.41E-06,8.43E-06,0.000835777,0.000258716,0.000339254,0.000290762,0.001151531,0.99544304,8.09E-06,8.08E-06,1.11E-05
776,sentiment_analysis1,84,Zhang et al. and Li et al.,INTRODUCTION,"Recently ,",sentiment_analysis,1,70,1,0,,0.003485529,0,negative,0.000158768,0.000212928,4.72E-05,9.80E-06,0.001429041,0.000129522,5.27E-05,5.30E-05,0.000447545,0.997415637,7.08E-07,1.33E-05,3.00E-05
777,sentiment_analysis1,85,proposed to use RNNs to learn spatial topological relations between channels by scanning electrodes in both vertical and horizontal directions .,INTRODUCTION,"Recently ,",sentiment_analysis,1,71,1,0,,0.007422862,0,negative,3.80E-05,0.00017447,9.45E-05,9.03E-06,0.000671015,0.000201386,5.94E-05,5.52E-05,0.000383669,0.998252609,4.07E-06,4.94E-06,5.17E-05
778,sentiment_analysis1,86,"However , their approaches do not fully exploit the topological structure of EEG channels .",INTRODUCTION,"Recently ,",sentiment_analysis,1,72,1,0,,0.001231266,0,negative,1.89E-05,0.000433829,1.69E-05,3.27E-05,0.00040044,0.000394674,7.23E-05,0.000275921,0.000454102,0.997774265,1.54E-05,7.15E-06,0.000103429
779,sentiment_analysis1,87,"For example , two topologically close channels maybe faraway from each other in the scanning sequence .",INTRODUCTION,"Recently ,",sentiment_analysis,1,73,1,0,,0.000249441,0,negative,8.19E-06,0.000126343,2.80E-06,3.53E-07,7.40E-05,1.77E-05,4.35E-06,2.64E-05,0.000579367,0.999157187,1.22E-07,1.15E-06,2.09E-06
780,sentiment_analysis1,88,Graph Neural Networks,INTRODUCTION,,sentiment_analysis,1,74,1,0,,0.024958953,0,negative,1.30E-05,0.008714574,0.000116928,2.61E-06,0.000146798,0.00079325,0.011776286,0.000741299,0.047537494,0.929282639,0.000511597,0.000152682,0.000210851
781,sentiment_analysis1,89,"Graph neural networks ( GNN ) is a class of neural networks dealing with data in the graph domains , e.g. , molecular structures , social networks and knowledge graphs .",INTRODUCTION,Graph Neural Networks,sentiment_analysis,1,75,1,0,,0.000397581,0,negative,2.43E-06,8.22E-06,4.68E-05,1.10E-07,4.95E-07,6.27E-06,1.90E-05,2.32E-05,0.0001099,0.999746248,2.26E-06,2.61E-05,9.01E-06
782,sentiment_analysis1,90,One early work on GNNs aimed to learn a converged static state embedding for each node in the graph using a transition function applied to its neighborhood .,INTRODUCTION,Graph Neural Networks,sentiment_analysis,1,76,1,0,,1.22E-05,0,negative,3.34E-06,3.91E-06,5.87E-06,6.38E-07,8.40E-07,1.65E-05,9.73E-06,4.88E-05,1.89E-05,0.999871063,5.95E-07,1.45E-05,5.26E-06
783,sentiment_analysis1,91,"Later , inspired by the convolutional operation of CNN in Euclidean domains , Bruna et al. combined spectral graph theory with neural networks and defined convolutional operations in graph domains using the spectral filters computed from the normalized graph Laplacian .",INTRODUCTION,Graph Neural Networks,sentiment_analysis,1,77,1,0,,4.93E-06,0,negative,1.77E-06,1.30E-06,3.96E-06,3.48E-08,3.18E-07,2.74E-06,2.53E-06,9.36E-06,1.77E-05,0.999954175,2.11E-08,5.27E-06,8.70E-07
784,sentiment_analysis1,92,"Following this line of research , Defferrard et al. proposed fast localized convolutions by using a recursive formulation of the K-order Chebyshev polynomials to approximate the filters .",INTRODUCTION,Graph Neural Networks,sentiment_analysis,1,78,1,0,,8.77E-06,0,negative,2.46E-06,2.06E-06,5.28E-06,6.06E-08,4.94E-07,4.45E-06,4.73E-06,1.20E-05,1.50E-05,0.999942473,6.35E-08,9.35E-06,1.57E-06
785,sentiment_analysis1,93,The resulting representation for each node is an aggregation of its K thorder neighborhood .,INTRODUCTION,Graph Neural Networks,sentiment_analysis,1,79,1,0,,8.78E-06,0,negative,1.66E-06,2.62E-06,5.67E-06,1.05E-09,1.32E-07,2.28E-07,5.15E-07,1.45E-06,0.000264438,0.999721139,1.45E-09,2.04E-06,9.56E-08
786,sentiment_analysis1,94,Kipf and Welling further limited K = 1 and proposed the standard graph convolutional network ( GCN ) with a faster localized graph convolutional operation .,INTRODUCTION,Graph Neural Networks,sentiment_analysis,1,80,1,0,,9.12E-06,0,negative,3.62E-06,6.35E-07,6.60E-06,3.32E-08,4.17E-07,2.89E-06,3.58E-06,6.06E-06,5.98E-06,0.999958217,1.04E-08,1.08E-05,1.15E-06
787,sentiment_analysis1,95,The convolutional layers in GCN can be stacked K times to effectively convolve the K th -order neighborhood of anode .,INTRODUCTION,Graph Neural Networks,sentiment_analysis,1,81,1,0,,4.30E-05,0,negative,8.09E-06,1.32E-05,8.71E-06,2.47E-08,9.84E-07,3.16E-06,3.91E-06,3.03E-05,0.000171639,0.999755247,3.15E-09,3.61E-06,1.12E-06
788,sentiment_analysis1,96,"Recently ,",INTRODUCTION,,sentiment_analysis,1,82,1,0,,0.000884898,0,negative,6.68E-06,0.000977535,5.11E-06,4.80E-06,0.0007882,0.000264649,0.000465766,0.000269854,0.000603942,0.996593835,1.26E-06,6.25E-06,1.21E-05
789,sentiment_analysis1,97,"Wu et al. simplified GCN by removing the nonlinearities between convolutional layers in GCN and proposed the simple graph convolution network ( SGC ) , which effectively behaves like a linear feature transformation followed by a logistic regression .",INTRODUCTION,"Recently ,",sentiment_analysis,1,83,1,0,,0.004817692,0,negative,5.30E-05,0.00115429,0.000300564,3.94E-05,0.001471969,0.00091056,0.000300102,0.000627893,0.002195177,0.992152765,2.39E-06,8.16E-06,0.000783806
790,sentiment_analysis1,98,SGC performs orders of magnitude faster than GCNs with comparable classification accuracy .,INTRODUCTION,"Recently ,",sentiment_analysis,1,84,1,0,,0.15771362,0,negative,0.003462435,0.007074903,0.000794952,9.20E-05,0.009160118,0.002564296,0.06443787,0.002074738,0.001841109,0.883953973,2.98E-06,0.005219821,0.019320822
791,sentiment_analysis1,99,"In this paper , we extend SGC to model EEG signals and propose a biologically supported adjacency matrix and two regularizers for robust EEG - based emotion recognition .",INTRODUCTION,"Recently ,",sentiment_analysis,1,85,1,0,,0.749217986,1,negative,0.001023303,0.278824263,0.003792474,1.41E-05,0.006867008,0.000441715,0.00219762,0.000627996,0.203960043,0.501292315,1.83E-05,0.000172667,0.000768234
792,sentiment_analysis1,100,"Apart from the convolution operation used in GCNs , there are other types of operations in GNNs , such as attention or RNN .",INTRODUCTION,"Recently ,",sentiment_analysis,1,86,1,0,,0.002541069,0,negative,1.00E-05,0.000243847,6.38E-05,9.53E-07,0.000143988,8.93E-05,4.74E-05,7.38E-05,0.000998387,0.998288486,6.39E-07,2.74E-06,3.68E-05
793,sentiment_analysis1,101,"However , they are often trained significantly slower than SGC .",INTRODUCTION,"Recently ,",sentiment_analysis,1,87,1,0,,0.000532551,0,negative,1.52E-05,7.87E-05,1.50E-05,6.96E-06,0.000237511,0.000124257,5.74E-05,6.56E-05,5.58E-05,0.999283319,7.13E-07,4.88E-06,5.47E-05
794,sentiment_analysis1,102,Unsupervised Domain Adaptation,INTRODUCTION,,sentiment_analysis,1,88,1,0,,0.295322234,0,negative,0.00012015,0.061448458,0.000428351,6.42E-06,0.000586974,0.001133677,0.064105189,0.001487556,0.060370551,0.808688183,0.000182392,0.000657008,0.000785091
795,sentiment_analysis1,103,Unsupervised domain adaptation aims to mitigate the domain shift in knowledge transfer from a supervised source domain to an unsupervised target domain .,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,89,1,0,,0.000503773,0,negative,2.13E-05,4.83E-06,4.09E-05,2.16E-06,2.46E-06,1.86E-05,6.87E-05,3.47E-05,1.21E-05,0.999286844,8.24E-07,0.000231246,0.000275483
796,sentiment_analysis1,104,"The most common approaches are instance re-weighting , domaininvariant feature learning , domain mapping and normalization statistics .",INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,90,1,0,,0.000203627,0,negative,6.57E-06,4.02E-07,4.75E-06,6.25E-08,3.20E-07,2.56E-06,4.86E-06,6.04E-06,1.28E-06,0.999946094,6.15E-09,2.05E-05,6.56E-06
797,sentiment_analysis1,105,Instance re-weighting methods aim to infer the resampling weight directly by feature distribution matching across source and target domains in a non-parametric manner .,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,91,1,0,,7.08E-05,0,negative,9.81E-06,4.17E-06,1.50E-05,7.49E-07,1.03E-06,1.21E-05,1.93E-05,5.40E-05,1.05E-05,0.999775726,9.01E-08,3.57E-05,6.17E-05
798,sentiment_analysis1,106,Domain - invariant feature leaning methods align features from both source and target domains to a common feature space .,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,92,1,0,,0.000261397,0,negative,2.34E-05,5.89E-06,6.75E-05,1.19E-06,2.19E-06,1.56E-05,3.70E-05,3.96E-05,2.58E-05,0.999553421,9.40E-08,7.32E-05,0.000155095
799,sentiment_analysis1,107,"The alignment can be achieved by minimizing divergence , maximizing reconstruction or adversarial training .",INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,93,1,0,,1.26E-05,0,negative,2.36E-06,7.11E-07,2.00E-06,1.30E-08,1.76E-07,4.66E-07,7.10E-07,3.51E-06,8.39E-06,0.999975249,5.32E-10,5.33E-06,1.08E-06
800,sentiment_analysis1,108,The domain mapping technique is typically applied in the computer vision field where pixel - level image - to - image translation from one domain to another domain improves domain adaptation performance .,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,94,1,0,,7.73E-05,0,negative,5.18E-06,1.46E-06,1.31E-05,2.72E-07,6.25E-07,5.92E-06,1.22E-05,1.61E-05,7.40E-06,0.999866513,4.22E-08,3.07E-05,4.05E-05
801,sentiment_analysis1,109,Normalization statistics are based on the assumption that the batch norm statistics learn domain knowledge .,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,95,1,0,,3.93E-06,0,negative,4.62E-06,9.51E-07,2.09E-05,1.14E-08,4.44E-07,8.17E-07,3.22E-06,2.78E-06,4.84E-06,0.999945586,1.00E-09,1.38E-05,1.96E-06
802,sentiment_analysis1,110,Cariucci et al.,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,96,1,0,,1.65E-05,0,negative,1.79E-05,3.94E-08,3.32E-06,4.72E-09,1.86E-07,3.83E-07,1.08E-06,4.99E-07,5.79E-07,0.999957055,4.51E-11,1.85E-05,4.46E-07
803,sentiment_analysis1,111,performed domain adaptation by modulating the batch norm layers ' statistics from source to target domain .,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,97,1,0,,5.75E-06,0,negative,6.79E-06,3.60E-07,1.44E-05,1.32E-07,1.31E-06,1.80E-06,1.05E-06,2.51E-06,6.92E-06,0.999958162,1.54E-10,2.85E-06,3.67E-06
804,sentiment_analysis1,112,Our proposed NodeDAT regularizer extends the domain adversarial training to graph neural networks and achieves finer - grained regularization by minimizing the discrepancies between features in source and target domains for each channel / node individually .,INTRODUCTION,Unsupervised Domain Adaptation,sentiment_analysis,1,98,1,0,,0.001652937,0,negative,0.000316475,0.000242839,0.000266966,4.52E-07,1.67E-05,7.80E-06,5.60E-05,5.16E-05,0.000871613,0.997955833,4.03E-09,0.000149525,6.43E-05
805,sentiment_analysis1,113,Learning with Noisy Labels,INTRODUCTION,,sentiment_analysis,1,99,1,0,,0.097096885,0,negative,7.63E-06,0.007890859,2.46E-05,2.22E-06,0.000211014,0.000670635,0.01943847,0.001273377,0.003610071,0.966450286,3.28E-05,0.000116701,0.000271384
806,sentiment_analysis1,114,Commonly adopted approaches to learning with noisy labels are based on the noise transition matrix and robust loss functions .,INTRODUCTION,Learning with Noisy Labels,sentiment_analysis,1,100,1,0,,7.95E-05,0,negative,1.44E-06,6.21E-06,3.71E-06,7.70E-07,8.25E-06,0.000115547,0.000245449,0.000161932,3.30E-06,0.999394302,7.28E-08,3.81E-05,2.09E-05
807,sentiment_analysis1,115,The noise transition matrix specifies the probabilities of transition from each ground true label to each noisy label and is often applied to modify the crossentropy loss .,INTRODUCTION,Learning with Noisy Labels,sentiment_analysis,1,101,1,0,,1.91E-05,0,negative,1.79E-06,1.63E-05,3.21E-06,1.62E-08,3.25E-06,8.42E-06,1.46E-05,4.44E-05,0.000102535,0.999801234,5.95E-10,3.35E-06,7.84E-07
808,sentiment_analysis1,116,The matrix can be pre-computed as a prior or estimated from noisy data .,INTRODUCTION,Learning with Noisy Labels,sentiment_analysis,1,102,1,0,,1.23E-05,0,negative,6.25E-07,2.56E-06,6.03E-07,1.60E-08,1.51E-06,1.02E-05,1.02E-05,4.58E-05,1.55E-05,0.99991024,2.33E-10,2.31E-06,5.37E-07
809,sentiment_analysis1,117,"A few studies tackle noisy labels by using noise - tolerant robust loss functions , such as unhinged loss and ramp loss .",INTRODUCTION,Learning with Noisy Labels,sentiment_analysis,1,103,1,0,,0.000258137,0,negative,1.27E-06,4.68E-06,2.66E-06,1.99E-07,6.11E-06,4.13E-05,0.000156094,8.09E-05,2.03E-06,0.999660907,2.15E-08,3.36E-05,1.03E-05
810,sentiment_analysis1,118,Several other approaches include bootstrap that leverages predicted labels to generate training targets and alternatively updating network parameters and labels during training .,INTRODUCTION,Learning with Noisy Labels,sentiment_analysis,1,104,1,0,,2.50E-05,0,negative,1.37E-06,2.83E-06,9.40E-06,1.23E-07,6.37E-06,6.45E-05,0.000135167,5.94E-05,3.31E-06,0.999699078,2.56E-09,1.03E-05,8.19E-06
811,sentiment_analysis1,119,"Our proposed Emotion DL regularizer is inspired by , which applies distribution learning to learn labels with ambiguity in the computer vision domain .",INTRODUCTION,Learning with Noisy Labels,sentiment_analysis,1,105,1,0,,0.000908843,0,negative,8.45E-05,0.002429107,0.001713123,7.05E-07,0.000408469,0.000155014,0.001835236,0.000281962,0.002397972,0.990336495,1.73E-08,0.000224816,0.000132572
812,sentiment_analysis1,120,PRELIMINARIES,INTRODUCTION,,sentiment_analysis,1,106,1,0,,0.00039251,0,negative,2.03E-06,0.00066709,8.71E-06,1.49E-05,0.001217695,0.001787377,0.001151131,0.000815104,0.000391574,0.993912083,7.06E-08,2.13E-06,3.01E-05
813,sentiment_analysis1,121,"In this section , we introduce the preliminaries of the simple graph convolution network ( SGC ) and its spectral analysis , which is the basis of our RGNN model .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,107,1,0,,0.000188062,0,negative,4.89E-06,0.05891699,0.000161727,1.18E-05,0.022301731,0.00059134,0.000415824,0.001745164,0.008211008,0.907614905,1.39E-07,7.02E-06,1.75E-05
814,sentiment_analysis1,122,Simple Graph Convolution Network ( SGC ),INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,108,1,0,,0.082091247,0,negative,7.95E-06,0.024030454,0.004795066,6.84E-07,0.000637565,0.000379366,0.004068232,0.000664818,0.198108853,0.767130913,7.49E-06,4.69E-05,0.000121689
815,sentiment_analysis1,123,"Given a graph G = ( V , E ) , where V denotes a set of nodes and E denotes a set of edges between nodes in V .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,109,1,0,,3.37E-05,0,negative,3.47E-07,0.005001522,1.62E-05,4.83E-07,0.000752866,9.31E-05,4.54E-05,0.000574052,0.002704712,0.99080716,1.23E-07,9.83E-07,3.07E-06
816,sentiment_analysis1,124,Data on V can be represented by a feature matrix X ?,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,110,1,0,,6.13E-05,0,negative,4.26E-07,0.001212495,1.30E-05,1.70E-07,0.000575881,5.86E-05,2.80E-05,0.000155632,0.001182559,0.996771274,1.62E-08,9.30E-07,9.73E-07
817,sentiment_analysis1,125,"R nd , where n denotes the number of nodes and d denotes the input feature dimension .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,111,1,0,,7.20E-05,0,negative,4.62E-06,0.002847787,2.33E-05,1.18E-06,0.002132583,0.000283514,0.000254217,0.001048875,0.000599334,0.992795149,2.75E-08,4.79E-06,4.59E-06
818,sentiment_analysis1,126,The edge set E can be represented by a weighted adjacency matrix A ?,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,112,1,0,,5.43E-05,0,negative,5.37E-07,0.002101157,1.99E-05,1.66E-07,0.000455535,9.25E-05,5.53E-05,0.000401022,0.003717583,0.993152963,2.47E-08,9.98E-07,2.36E-06
819,sentiment_analysis1,127,"R nn with self - loops , i.e. , A ii = 1 , i = 1 , 2 , ... , n.",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,113,1,0,,2.36E-05,0,negative,9.03E-07,0.001144809,4.51E-05,1.19E-07,0.000555001,6.14E-05,7.57E-05,0.00017127,0.000998544,0.996943331,1.90E-08,2.22E-06,1.55E-06
820,sentiment_analysis1,128,"In general , GNNs learn a feature transformation function for X and produces output Z ?",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,114,1,0,,8.09E-05,0,negative,4.10E-07,0.002156852,1.58E-05,9.81E-08,0.000295773,5.63E-05,3.88E-05,0.000290469,0.003005712,0.994137777,1.55E-08,7.08E-07,1.32E-06
821,sentiment_analysis1,129,"R nd , where d denotes the output feature dimension .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,115,1,0,,8.65E-05,0,negative,8.86E-06,0.001990833,3.59E-05,1.42E-06,0.002968601,0.000236363,0.000251278,0.000567699,0.000390504,0.993537957,1.09E-08,6.54E-06,4.04E-06
822,sentiment_analysis1,130,"Between adjacent layers in GNNs , the feature transformation can be written as",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,116,1,0,,5.59E-05,0,negative,1.98E-06,0.004434859,0.00012712,2.68E-07,0.001016449,8.39E-05,9.32E-05,0.000312241,0.010527334,0.983397325,1.74E-08,1.80E-06,3.47E-06
823,sentiment_analysis1,131,"where l = 0 , 1 , ... , L ?",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,117,1,0,,1.17E-05,0,negative,7.84E-07,0.000656745,6.94E-06,4.55E-07,0.000908802,0.000190238,7.77E-05,0.000430995,0.000248669,0.99747614,2.90E-09,1.07E-06,1.49E-06
824,sentiment_analysis1,132,"1 , L denotes the number of layers , H 0 = X , H L = Z , and f denotes the function we want to learn .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,118,1,0,,1.32E-05,0,negative,7.03E-07,0.001579403,8.45E-06,3.13E-07,0.000860351,0.000174946,7.59E-05,0.000865109,0.000674429,0.995757982,2.47E-09,8.41E-07,1.59E-06
825,sentiment_analysis1,133,A simple definition off would be,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,119,1,0,,1.31E-06,0,negative,4.61E-07,0.000154474,3.31E-06,1.16E-07,0.000285942,5.26E-05,2.12E-05,0.000114801,0.000115902,0.99925012,1.34E-09,5.50E-07,4.83E-07
826,sentiment_analysis1,134,where ?,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,120,1,0,,1.68E-06,0,negative,1.31E-07,0.000125464,2.18E-06,3.99E-07,0.000167539,0.000146058,3.00E-05,0.000285377,0.000120577,0.999121048,3.16E-09,1.98E-07,9.93E-07
827,sentiment_analysis1,135,denotes a non-linear function and W l denotes a weight matrix at layer l .,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,121,1,0,,5.44E-05,0,negative,1.47E-06,0.003299314,1.40E-05,2.14E-06,0.001814072,0.000556676,0.000168627,0.004309897,0.002033169,0.987791087,4.73E-09,8.82E-07,8.63E-06
828,sentiment_analysis1,136,"For each node x , function f simply sums up all node features in its neighborhood including x itself , followed by a non-linear transformation .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,122,1,0,,0.000508191,0,negative,3.54E-06,0.015854228,0.000145362,2.52E-07,0.001320748,0.00017059,0.000210837,0.000919526,0.038226413,0.943139084,5.39E-09,1.09E-06,8.33E-06
829,sentiment_analysis1,137,"However , one major limitation off in is that repeatedly applying f along multiple layers may lead to H l with overly large values due to summation .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,123,1,0,,4.76E-05,0,negative,3.36E-06,0.000479355,1.69E-05,7.56E-07,0.000749245,0.000118394,0.000168305,0.000160454,5.67E-05,0.998237873,1.66E-08,6.03E-06,2.59E-06
830,sentiment_analysis1,138,Kipf and Welling alleviated this limitation by proposing the graph convolution network ( GCN ) as follows :,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,124,1,0,,0.00119985,0,negative,1.64E-05,0.01057218,0.011189047,1.26E-06,0.004495583,0.00026251,0.000812211,0.00020942,0.023532517,0.948862242,3.01E-08,8.32E-06,3.83E-05
831,sentiment_analysis1,139,"where D denotes the diagonal degree matrix of A , i.e. ,",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,125,1,0,,5.96E-06,0,negative,1.04E-06,0.000469776,1.02E-05,2.72E-07,0.000790331,8.25E-05,5.30E-05,0.000189098,0.00017902,0.998222479,1.88E-09,1.04E-06,1.24E-06
832,sentiment_analysis1,140,prevents,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,126,1,0,,3.40E-05,0,negative,1.73E-06,9.81E-05,1.24E-05,9.93E-07,0.001337144,0.000425661,0.000249467,0.000348808,9.63E-05,0.997421582,1.52E-09,1.82E-06,5.99E-06
833,sentiment_analysis1,141,H from growing overly large .,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,127,1,0,,2.36E-05,0,negative,1.88E-05,0.000341606,3.60E-05,2.50E-07,0.001213254,0.000105219,0.000281283,0.00014204,0.000173292,0.997678634,1.03E-09,6.58E-06,3.02E-06
834,sentiment_analysis1,142,If we ignore ?,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,128,1,0,,2.44E-06,0,negative,3.03E-07,0.000160358,2.95E-06,1.14E-06,0.000251164,0.00053338,0.000121344,0.001169428,0.000108893,0.997645838,2.17E-09,4.05E-07,4.79E-06
835,sentiment_analysis1,143,"and W l temporarily and expand , the hidden state H l + 1 i for node x i , i = 1 , 2 , ... , n , can be computed via",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,129,1,0,,1.91E-05,0,negative,4.73E-06,0.001633599,5.04E-05,2.96E-07,0.002137906,9.79E-05,0.000171102,0.000205798,0.000833759,0.994858327,1.09E-09,3.82E-06,2.40E-06
836,sentiment_analysis1,144,Note that each neighboring H l j is now normalized by both the degrees of x i and x j .,INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,130,1,0,,8.31E-06,0,negative,1.47E-06,0.001660753,2.48E-05,6.36E-08,0.000535304,5.45E-05,4.58E-05,0.000193127,0.003490373,0.993992184,3.16E-10,6.17E-07,9.94E-07
837,sentiment_analysis1,145,"Therefore , essentially , for each node , the feature transformation function fin GCN is a nonlinear transformation of the weighted sum of node features of itself and its neighborhood .",INTRODUCTION,PRELIMINARIES,sentiment_analysis,1,131,1,0,,0.000147892,0,negative,2.72E-06,0.007601559,0.000192342,1.41E-07,0.001322684,5.87E-05,8.23E-05,0.000245439,0.028029046,0.962460596,1.41E-09,1.02E-06,3.50E-06
838,sentiment_analysis1,146,Successively applying,INTRODUCTION,,sentiment_analysis,1,132,1,0,,0.00124141,0,negative,6.14E-05,0.000872852,0.000119091,2.38E-07,0.000847689,0.000155696,0.003486079,4.74E-05,0.000919717,0.993459385,8.03E-10,1.45E-05,1.60E-05
839,sentiment_analysis1,147,L graph convolutional layers aggregates node features within a neighborhood of size L.,INTRODUCTION,Successively applying,sentiment_analysis,1,133,1,0,,7.73E-05,0,negative,7.80E-06,4.44E-06,2.46E-05,5.32E-08,8.83E-06,9.45E-06,2.69E-05,3.25E-05,7.62E-05,0.999567771,1.37E-11,6.23E-07,0.000240817
840,sentiment_analysis1,148,"To further accelerate training while keeping comparable performance , Wu et al.",INTRODUCTION,Successively applying,sentiment_analysis,1,134,1,0,,2.82E-06,0,negative,0.000112718,3.90E-06,0.000121424,2.34E-08,1.84E-05,2.54E-06,3.07E-05,2.76E-06,1.26E-05,0.999634044,1.61E-11,5.52E-06,5.54E-05
841,sentiment_analysis1,149,proposed SGC by removing the non-linear function ? in ( 3 ) and reparameterizing all linear transformations,INTRODUCTION,Successively applying,sentiment_analysis,1,135,1,0,,4.35E-05,0,negative,6.82E-06,9.35E-06,8.48E-05,2.85E-09,1.52E-06,1.11E-06,5.30E-05,3.17E-06,0.000143409,0.999607929,3.93E-10,6.37E-06,8.25E-05
842,sentiment_analysis1,150,W l across all layers into one linear transformation W as follows :,INTRODUCTION,Successively applying,sentiment_analysis,1,136,1,0,,1.73E-06,0,negative,1.43E-06,3.51E-07,2.23E-06,9.75E-10,5.13E-07,4.07E-07,1.58E-06,1.43E-06,9.37E-06,0.999978532,1.76E-12,2.02E-07,3.96E-06
843,sentiment_analysis1,151,"where S = D ? 1 2 AD 1 2 , and W = W L?1 W L?2 ... W 0 .",INTRODUCTION,Successively applying,sentiment_analysis,1,137,1,0,,9.53E-07,0,negative,7.47E-07,2.26E-07,5.65E-07,4.56E-09,9.46E-07,2.67E-06,6.78E-06,9.10E-06,1.55E-06,0.999963869,2.47E-12,4.95E-07,1.30E-05
844,sentiment_analysis1,152,"Essentially , SGC computes a topology - aware linear trans - formation X = S L X , followed by one final linear transformation Z = XW .",INTRODUCTION,Successively applying,sentiment_analysis,1,138,1,0,,8.71E-06,0,negative,5.63E-06,7.78E-06,7.13E-05,7.63E-10,2.56E-06,2.84E-07,4.37E-06,8.42E-07,0.000309984,0.999586462,1.03E-11,6.99E-07,1.01E-05
845,sentiment_analysis1,153,Spectral Graph Convolution,INTRODUCTION,,sentiment_analysis,1,139,1,0,,0.033910969,0,negative,1.97E-05,0.001007292,0.000296981,3.05E-07,0.000213217,0.000753677,0.140263809,0.00034653,0.00160003,0.854541015,2.09E-08,0.000144968,0.000812423
846,sentiment_analysis1,154,We analyze GCN from the perspective of spectral graph theory .,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,140,1,0,,8.47E-05,0,negative,6.73E-06,2.18E-06,2.43E-05,1.62E-09,4.07E-07,5.59E-07,2.90E-05,2.63E-06,4.25E-06,0.999880336,2.44E-11,4.20E-05,7.65E-06
847,sentiment_analysis1,155,Graph Fourier analysis relies on the graph Laplacian L = D ?,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,141,1,0,,2.98E-05,0,negative,2.57E-06,7.39E-08,1.82E-05,1.07E-10,7.35E-08,1.47E-07,3.80E-06,2.08E-07,4.63E-07,0.999966401,1.79E-12,7.19E-06,8.52E-07
848,sentiment_analysis1,156,A or the normalized graph Lapla -,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,142,1,0,,4.23E-06,0,negative,5.02E-07,2.13E-08,2.87E-06,5.42E-11,1.56E-08,5.25E-08,9.74E-07,3.50E-07,5.32E-07,0.99999215,5.34E-13,1.99E-06,5.47E-07
849,sentiment_analysis1,157,"SinceL is asymmetric positive semidefinite matrix , it can be decomposed as L = U?U T , where U is the orthonormal eigenvector matrix ofL and ? = diag ( ?",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,143,1,0,,2.11E-06,0,negative,7.82E-07,3.56E-08,5.62E-07,5.89E-11,2.08E-08,1.02E-07,1.58E-06,7.71E-07,1.52E-07,0.999992447,3.20E-13,3.26E-06,2.78E-07
850,sentiment_analysis1,158,"1 , ... , ? N ) is the diagonal matrix of corresponding eigenvalues .",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,144,1,0,,7.05E-07,0,negative,1.14E-06,1.74E-08,5.08E-07,3.60E-11,2.70E-08,4.67E-08,4.06E-07,2.40E-07,7.99E-08,0.999996258,4.27E-14,1.23E-06,5.01E-08
851,sentiment_analysis1,159,"Given graph data X , the graph Fourier transform of X is X = UT X , and the inverse Fourier transform of X is X = UX .",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,145,1,0,,4.74E-07,0,negative,3.13E-08,8.26E-09,2.40E-07,6.38E-11,9.89E-09,7.49E-08,2.55E-07,4.16E-07,8.87E-08,0.999998435,2.21E-13,2.72E-07,1.69E-07
852,sentiment_analysis1,160,"Hence , the graph convolution between X and a filter G is computed as follows :",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,146,1,0,,1.70E-07,0,negative,9.67E-08,3.52E-09,2.43E-07,5.90E-12,3.00E-09,8.04E-09,5.54E-08,3.61E-08,6.40E-08,0.999999223,2.95E-14,2.48E-07,1.93E-08
853,sentiment_analysis1,161,"where denotes element - wise multiplication , and ? = diag ( ?",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,147,1,0,,1.14E-06,0,negative,4.20E-07,1.15E-08,2.91E-07,1.63E-11,8.19E-09,3.98E-08,5.39E-07,2.75E-07,5.30E-08,0.999997413,3.83E-14,8.85E-07,6.43E-08
854,sentiment_analysis1,162,"1 , ..., ?",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,148,1,0,,3.60E-07,0,negative,4.15E-08,2.08E-09,3.40E-08,9.28E-11,6.17E-09,9.64E-08,1.98E-07,6.58E-07,1.75E-08,0.99999864,3.12E-14,2.17E-07,8.88E-08
855,sentiment_analysis1,163,N ) denotes a diagonal matrix with n spectral filter coefficients .,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,149,1,0,,2.31E-06,0,negative,3.58E-07,1.47E-08,5.83E-07,2.01E-11,1.24E-08,2.41E-08,2.52E-07,1.92E-07,1.37E-07,0.99999783,2.81E-14,5.34E-07,6.14E-08
856,sentiment_analysis1,164,"To reduce the current learning complexity of O ( n ) to that of conventional CNN , i.e. , O ( K ) , ( 6 ) can be approximated using the Kth order polynomials as follows :",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,150,1,0,,2.05E-06,0,negative,1.01E-06,4.33E-08,9.22E-07,6.04E-11,2.42E-08,8.33E-08,1.39E-06,6.37E-07,3.43E-07,0.999993231,1.55E-13,1.77E-06,5.33E-07
857,sentiment_analysis1,165,where ?,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,151,1,0,,1.25E-07,0,negative,2.03E-08,1.17E-09,3.28E-08,4.96E-11,2.23E-09,6.29E-08,1.19E-07,3.37E-07,2.12E-08,0.999999252,3.34E-14,8.35E-08,6.77E-08
858,sentiment_analysis1,166,i denotes coefficients .,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,152,1,0,,1.11E-06,0,negative,4.65E-08,2.49E-09,4.95E-08,1.15E-10,6.04E-09,8.85E-08,2.23E-07,9.79E-07,3.74E-08,0.999998227,2.41E-14,1.68E-07,1.72E-07
859,sentiment_analysis1,167,"To further reduce computational cost , Defferrard et al. proposed to use Chebyshev polynomials to approximate the filtering operation as follows :",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,153,1,0,,1.20E-06,0,negative,7.75E-07,4.11E-08,4.73E-06,2.25E-10,3.21E-08,1.69E-07,1.29E-06,5.94E-07,5.51E-07,0.99998936,5.06E-13,9.42E-07,1.51E-06
860,sentiment_analysis1,168,where ?,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,154,1,0,,1.21E-07,0,negative,1.82E-08,1.01E-09,3.11E-08,4.23E-11,2.05E-09,5.96E-08,1.19E-07,3.23E-07,1.81E-08,0.999999283,2.20E-14,7.61E-08,6.95E-08
861,sentiment_analysis1,169,"i denotes learnable parameters , L denotes the scaled normalized Laplacian L = 2 ? maxL ?",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,155,1,0,,1.00E-06,0,negative,2.72E-07,2.59E-08,1.86E-07,2.40E-10,4.20E-08,6.20E-07,3.65E-06,8.63E-06,8.12E-08,0.999984565,2.74E-14,8.86E-07,1.04E-06
862,sentiment_analysis1,170,"I with its eigenvalues lying within [ ? 1 , 1 ] , and Ti ( x ) denotes the Chebyshev polynomials recursively defined as",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,156,1,0,,4.71E-07,0,negative,2.23E-07,4.70E-09,2.84E-07,1.06E-11,8.68E-09,2.31E-08,2.82E-07,1.32E-07,2.68E-08,0.999998222,8.55E-15,7.51E-07,4.27E-08
863,sentiment_analysis1,171,The GCN proposed in made a few approximations to simplify the filtering operation in ( 8 ) :,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,157,1,0,,5.61E-05,0,negative,4.80E-06,3.08E-08,5.04E-06,9.30E-11,6.22E-08,8.25E-08,1.17E-06,2.38E-07,2.56E-07,0.999986132,3.44E-14,1.74E-06,4.44E-07
864,sentiment_analysis1,172,1 ) use K = 1 ; 2 ) set ? max = 2 ; and 3 ) set ?,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,158,1,0,,4.14E-05,0,negative,3.56E-06,9.64E-08,5.97E-07,4.96E-10,7.75E-08,2.54E-06,7.83E-05,5.02E-05,1.64E-07,0.99984416,9.39E-14,7.98E-06,1.23E-05
865,sentiment_analysis1,173,0 = ??,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,159,1,0,,1.50E-07,0,negative,3.69E-08,9.96E-10,3.37E-08,2.77E-11,4.10E-09,1.07E-07,3.99E-07,5.67E-07,9.66E-09,0.999998468,1.06E-14,2.29E-07,1.45E-07
866,sentiment_analysis1,174,1 . The resulted GCN arrives at .,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,160,1,0,,7.31E-06,0,negative,1.03E-06,7.16E-09,8.67E-07,6.74E-12,5.05E-09,2.01E-08,1.10E-06,9.69E-08,1.11E-07,0.999993862,2.76E-14,2.64E-06,2.58E-07
867,sentiment_analysis1,175,"Essentially , the graph convolutional operations defined in and behave like a low - pass filter by smoothing the features of each node on the graph using node features in its neighborhood .",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,161,1,0,,2.09E-06,0,negative,2.90E-07,4.23E-08,1.74E-06,3.12E-10,4.10E-08,1.94E-07,5.24E-07,9.94E-07,1.22E-06,0.999994039,1.91E-14,1.37E-07,7.76E-07
868,sentiment_analysis1,176,REGULARIZED GRAPH NEURAL NETWORK,INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,162,1,0,,9.04E-05,0,negative,8.94E-07,5.23E-08,1.68E-05,2.17E-09,5.75E-08,1.85E-06,8.33E-05,5.98E-06,1.41E-06,0.999697563,8.18E-12,2.12E-05,0.000170856
869,sentiment_analysis1,177,"In this section we present our regularized graph neural network ( RGNN ) , specifically , the biologically supported adjacency matrix , and RGNN with two regularizers , i.e. , node - wise domain adversarial training ( NodeDAT ) and emotion - aware distribution learning ( EmotionDL ) .",INTRODUCTION,Spectral Graph Convolution,sentiment_analysis,1,163,1,0,,0.000931773,0,negative,7.90E-06,3.28E-06,0.000158916,2.01E-09,1.19E-06,1.49E-06,0.000110507,5.64E-06,5.07E-06,0.999637529,8.83E-13,2.73E-05,4.11E-05
870,sentiment_analysis1,178,Adjacency Matrix in RGNN,INTRODUCTION,,sentiment_analysis,1,164,1,0,,0.014664824,0,negative,5.14E-07,0.000148752,2.06E-05,8.53E-09,1.81E-05,0.000182688,0.013620469,0.000299982,0.000851983,0.984686929,4.34E-10,3.65E-06,0.000166251
871,sentiment_analysis1,179,The adjacency matrix A ?,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,165,1,0,,4.97E-06,0,negative,2.22E-08,2.83E-09,2.25E-07,3.69E-12,2.75E-09,1.79E-08,1.88E-07,4.59E-07,4.78E-08,0.999998311,8.84E-15,6.52E-07,7.28E-08
872,sentiment_analysis1,180,"R nn in RGNN represents the topological structure of EEG channels , where n denotes the number of channels in EEG signals or nodes on the graph .",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,166,1,0,,5.08E-06,0,negative,4.05E-08,2.07E-08,6.60E-07,4.75E-11,1.74E-08,8.04E-08,3.06E-07,3.32E-06,4.86E-07,0.999994355,1.58E-14,3.04E-07,4.10E-07
873,sentiment_analysis1,181,Each entry A ij in the adjacency matrix indicates the weight of connection between channels i and j.,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,167,1,0,,3.78E-06,0,negative,3.83E-08,1.04E-08,9.59E-08,7.78E-12,4.89E-09,2.04E-08,8.32E-08,1.16E-06,2.16E-07,0.999998138,3.50E-15,1.91E-07,4.69E-08
874,sentiment_analysis1,182,Note that A contains self - loops .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,168,1,0,,1.44E-06,0,negative,3.64E-08,9.82E-10,5.46E-08,3.51E-12,1.78E-09,7.56E-09,3.86E-08,1.44E-07,1.42E-08,0.999999381,1.27E-15,3.02E-07,1.94E-08
875,sentiment_analysis1,183,"To reduce overfitting , we model A as asymmetric matrix by using only n ( n + 1 ) 2 number of parameters instead of n 2 .",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,169,1,0,,4.11E-05,0,negative,1.22E-06,6.26E-07,2.68E-06,2.79E-10,2.35E-07,7.53E-07,7.64E-06,4.23E-05,1.69E-06,0.999936652,1.73E-14,2.66E-06,3.53E-06
876,sentiment_analysis1,184,Salvador et al.,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,170,1,0,,7.14E-05,0,negative,3.16E-07,1.33E-09,6.32E-07,3.04E-11,1.37E-08,6.10E-08,6.02E-07,4.96E-07,1.36E-08,0.999993865,1.52E-15,3.74E-06,2.55E-07
877,sentiment_analysis1,185,observed that the strength of connection between brain regions decays as an inverse square or gravity - law function of physical distance . :,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,171,1,0,,4.70E-07,0,negative,1.70E-07,1.67E-09,1.37E-06,4.81E-10,4.96E-08,1.72E-07,5.46E-07,5.56E-07,1.44E-08,0.999994135,1.33E-14,1.96E-06,1.02E-06
878,sentiment_analysis1,186,The 62 - channel EEG placement used to collect data in SEED and SEED - IV .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,172,1,0,,7.08E-06,0,negative,6.92E-08,2.61E-08,7.54E-07,5.34E-10,1.37E-07,2.57E-06,8.67E-06,4.96E-05,1.05E-07,0.999932287,2.93E-15,1.05E-06,4.73E-06
879,sentiment_analysis1,187,Gray symmetric channels are connected globally via red dashed lines .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,173,1,0,,2.60E-06,0,negative,7.50E-08,1.47E-08,8.35E-07,1.55E-10,1.91E-08,3.30E-07,1.33E-06,9.56E-06,4.51E-07,0.999985222,5.23E-15,6.03E-07,1.56E-06
880,sentiment_analysis1,188,"Hence , we initialize the local inter-channel relations in our adjacency matrix as follows :",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,174,1,0,,2.51E-05,0,negative,1.21E-07,5.75E-08,3.37E-07,2.24E-11,1.93E-08,1.72E-07,1.32E-06,1.53E-05,4.71E-07,0.999981358,2.27E-15,4.71E-07,3.45E-07
881,sentiment_analysis1,189,"where d ij , i , j = 1 , 2 , ... , n , denotes the physical distance between channels i and j , computed from the data sheet of the recording device , and ?",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,175,1,0,,3.97E-07,0,negative,2.51E-08,2.09E-09,5.78E-08,1.85E-11,5.32E-09,2.91E-08,1.20E-07,8.02E-07,8.55E-09,0.99999851,1.24E-15,3.82E-07,5.90E-08
882,sentiment_analysis1,190,denotes a sparsity hyperparameter controlling the decay rate of the connection between channels .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,176,1,0,,7.46E-05,0,negative,2.57E-07,1.22E-07,3.29E-07,2.61E-09,1.04E-07,3.18E-06,8.06E-06,0.000539483,5.57E-07,0.999436889,8.97E-15,1.06E-06,9.96E-06
883,sentiment_analysis1,191,Bullmore and Sporns proposed that the brain organization is shaped by an economic trade - off between minimizing wiring costs and network running costs .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,177,1,0,,7.16E-07,0,negative,8.26E-08,6.56E-09,5.58E-07,1.36E-09,3.27E-08,8.89E-07,1.28E-06,1.05E-05,6.96E-08,0.999982487,1.65E-14,6.93E-07,3.41E-06
884,sentiment_analysis1,192,Minimizing wiring costs encourages local inter-channel connections as modelled in .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,178,1,0,,0.000123336,0,negative,4.12E-07,2.08E-08,2.07E-06,2.61E-11,9.52E-09,2.43E-08,8.45E-07,5.99E-07,2.22E-07,0.999987218,2.12E-14,7.85E-06,7.24E-07
885,sentiment_analysis1,193,"However , minimizing network running costs encourages certain global inter-channel connections for high efficiency of information transfer across the network as a whole .",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,179,1,0,,8.40E-06,0,negative,1.63E-07,3.67E-09,1.37E-07,8.76E-11,6.02E-09,4.07E-08,2.50E-07,1.00E-06,1.45E-08,0.999995807,7.34E-15,2.19E-06,3.84E-07
886,sentiment_analysis1,194,"To this end , we add several global connections to our adjacency matrix .",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,180,1,0,,0.000660326,0,negative,7.49E-06,6.38E-07,1.78E-05,2.55E-10,2.88E-07,4.52E-07,8.85E-06,1.23E-05,8.52E-06,0.99993363,4.23E-15,3.82E-06,6.14E-06
887,sentiment_analysis1,195,The global connections are subject to the specific EEG channel placement adopted in experiments .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,181,1,0,,9.05E-06,0,negative,2.82E-07,1.25E-07,1.51E-06,3.00E-11,3.67E-08,9.92E-08,1.20E-06,6.30E-06,1.40E-06,0.999988007,1.33E-15,6.29E-07,4.05E-07
888,sentiment_analysis1,196,depicts the global connections in both SEED and SEED - IV .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,182,1,0,,1.39E-05,0,negative,5.83E-08,4.29E-09,7.52E-07,1.17E-11,4.69E-09,5.36E-08,5.04E-07,1.71E-06,1.88E-07,0.999995695,8.69E-16,3.86E-07,6.41E-07
889,sentiment_analysis1,197,"The selection of global channels is supported by prior studies showing that the asymmetry in neuronal activities between the left and right hemispheres is informative in valence and arousal predictions , , .",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,183,1,0,,1.16E-05,0,negative,7.33E-08,3.01E-09,2.15E-07,6.66E-12,3.71E-09,2.58E-08,2.79E-07,6.20E-07,2.36E-08,0.999997623,1.45E-15,9.86E-07,1.48E-07
890,sentiment_analysis1,198,"To leverage the differential asymmetry information , we initialize the global inter-channel relations in A to [ ?1 , 0 ] as follows :",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,184,1,0,,4.28E-05,0,negative,2.53E-07,3.77E-08,3.02E-07,2.76E-11,2.45E-08,2.92E-07,3.58E-06,2.37E-05,1.34E-07,0.99996946,7.54E-16,1.34E-06,8.98E-07
891,sentiment_analysis1,199,"where ( i , j ) denotes the indices of empirically selected symmetric channel pairs that balance wiring cost and global efficiency : ( FP1 , FP2 ) , ( AF3 , AF4 ) , ( F5 , F6 ) , ( FC5 , FC6 ) , ( C5 , C6 ) , ( CP5 , CP6 ) , ( P5 , P6 ) , ( PO5 , PO6 ) , and ( O1 , O2 ) .",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,185,1,0,,1.22E-06,0,negative,6.11E-09,7.94E-10,3.75E-08,2.67E-12,2.99E-09,7.78E-09,3.59E-08,3.04E-07,5.41E-09,0.999999495,4.48E-17,8.63E-08,1.78E-08
892,sentiment_analysis1,200,Note that our adjacency matrix A obtained in ( 10 ) aims to represent the brain network which combines both local anatomical connectivity and emotion - related global functional connectivity .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,186,1,0,,8.93E-06,0,negative,7.28E-08,8.71E-09,2.20E-07,6.37E-12,6.99E-09,1.57E-08,1.05E-07,5.79E-07,1.83E-07,0.999998461,3.87E-16,2.56E-07,9.16E-08
893,sentiment_analysis1,201,The last step in constructing the adjacency matrix is finding an optimal value of ? to regularize the weights of connections between local channels .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,187,1,0,,1.94E-05,0,negative,1.39E-07,1.08E-08,1.43E-07,9.88E-11,2.07E-08,1.52E-07,5.27E-07,5.99E-06,6.97E-08,0.999991638,9.84E-16,6.71E-07,6.36E-07
894,sentiment_analysis1,202,"Achard and Bullmore observed that sparse f MRI networks , comprising around 20 % of all possible connections , typically maximize the efficiency of the network topology .",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,188,1,0,,2.49E-06,0,negative,8.51E-08,2.50E-09,2.75E-07,2.25E-10,1.51E-08,3.60E-07,3.04E-06,5.47E-06,9.41E-09,0.99998297,6.43E-15,2.77E-06,5.00E-06
895,sentiment_analysis1,203,"Thus , we choose ?",INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,189,1,0,,6.01E-07,0,negative,4.97E-08,1.05E-09,2.44E-08,6.71E-11,9.72E-09,1.29E-07,3.21E-07,3.91E-06,4.53E-09,0.999994929,1.13E-16,4.35E-07,1.89E-07
896,sentiment_analysis1,204,such that around 20 % of entries in A are larger than 0.1 in absolute values .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,190,1,0,,2.72E-06,0,negative,1.54E-07,5.52E-09,7.63E-08,7.40E-11,4.53E-08,3.82E-07,2.51E-06,2.51E-05,8.51E-09,0.999968988,1.14E-16,1.24E-06,1.48E-06
897,sentiment_analysis1,205,We empirically pick 0.1 as the threshold of having negligible connections between channels .,INTRODUCTION,Adjacency Matrix in RGNN,sentiment_analysis,1,191,1,0,,0.000210122,0,negative,6.35E-07,2.01E-07,3.57E-07,3.72E-09,2.84E-07,2.27E-05,0.000139155,0.004575769,2.40E-07,0.995167028,1.39E-15,3.67E-06,9.00E-05
898,sentiment_analysis1,206,Dynamics of RGNN,INTRODUCTION,,sentiment_analysis,1,192,1,0,,0.014088434,0,negative,2.06E-06,0.000130032,4.80E-05,1.51E-08,5.20E-05,0.0003552,0.07705087,0.000542697,0.000334576,0.920474069,1.37E-11,5.75E-06,0.001004718
899,sentiment_analysis1,207,Our RGNN model extends the SGC model .,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,193,1,0,,0.00250682,0,negative,1.04E-06,3.91E-07,0.000150375,1.45E-10,6.73E-07,1.04E-06,3.56E-05,4.55E-06,7.34E-06,0.999770004,7.33E-15,3.98E-06,2.50E-05
900,sentiment_analysis1,208,The architecture of RGNN is illustrated in .,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,194,1,0,,7.54E-06,0,negative,3.39E-08,3.28E-08,9.38E-07,1.96E-10,6.28E-08,8.92E-07,2.97E-06,1.07E-05,1.19E-06,0.999975776,5.12E-15,2.35E-07,7.21E-06
901,sentiment_analysis1,209,Given EEG features X ?,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,195,1,0,,5.89E-08,0,negative,1.19E-09,8.22E-10,9.00E-09,5.02E-13,1.75E-09,1.14E-08,7.79E-08,3.29E-07,4.73E-09,0.999999498,8.21E-17,4.92E-08,1.74E-08
902,sentiment_analysis1,210,"RN nd and labels Y ? Z N , where N denotes the number of training samples , n denotes the number of nodes or channels , d denotes the input feature dimension , Y i ? { 0 , 1 , ... , C ?1 } denotes the label index , and C denotes the number of classes .",INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,196,1,0,,1.84E-07,0,negative,5.82E-08,8.99E-09,1.71E-07,8.85E-12,2.89E-08,7.56E-08,9.69E-07,1.03E-06,1.52E-08,0.999996774,2.10E-16,7.63E-07,1.06E-07
903,sentiment_analysis1,211,Our model aims to minimize the following cross - entropy loss :,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,197,1,0,,3.86E-06,0,negative,4.61E-08,7.16E-08,2.57E-07,1.60E-11,2.96E-08,1.72E-07,1.37E-06,7.76E-06,3.80E-07,0.999989084,5.87E-16,1.98E-07,6.24E-07
904,sentiment_analysis1,212,where ?,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,198,1,0,,5.19E-08,0,negative,1.86E-09,6.44E-10,1.01E-08,8.77E-12,2.54E-09,7.96E-08,1.19E-07,8.65E-07,6.33E-09,0.999998843,8.93E-17,2.30E-08,4.96E-08
905,sentiment_analysis1,213,"denotes the model parameters we want to optimize , and ?",INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,199,1,0,,1.34E-07,0,negative,2.56E-09,4.01E-09,1.02E-08,2.59E-11,8.32E-09,2.75E-07,6.58E-07,1.14E-05,1.34E-08,0.999987399,1.09E-16,5.63E-08,1.91E-07
906,sentiment_analysis1,214,denotes the L1 sparse regularization strength of our adjacency matrix A .,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,200,1,0,,5.25E-07,0,negative,1.84E-08,5.48E-09,2.87E-08,1.96E-11,1.87E-08,1.64E-07,5.09E-07,4.86E-06,1.48E-08,0.999994107,3.96E-17,1.05E-07,1.69E-07
907,sentiment_analysis1,215,"By passing each feature matrix X i into our RGNN , the output probability of class Y i can be computed as",INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,201,1,0,,8.83E-08,0,negative,5.06E-09,1.06E-09,1.57E-08,6.51E-12,5.95E-09,4.33E-08,1.36E-07,8.24E-07,8.86E-09,0.999998823,5.16E-17,6.74E-08,6.89E-08
908,sentiment_analysis1,216,where S ?,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,202,1,0,,3.00E-08,0,negative,1.98E-09,1.66E-10,4.97E-09,3.90E-13,7.74E-10,9.47E-09,5.93E-08,1.31E-07,1.18E-09,0.999999742,1.58E-17,3.83E-08,1.06E-08
909,sentiment_analysis1,217,"R nn , W ? R dd and L follow the definitions in , ?( x ) = max ( 0 , x ) , W O ?",INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,203,1,0,,3.25E-07,0,negative,2.37E-08,3.74E-09,1.30E-07,3.06E-12,1.06E-08,6.83E-08,1.15E-06,9.36E-07,7.50E-09,0.999997127,5.76E-17,4.10E-07,1.38E-07
910,sentiment_analysis1,218,"Rd C denotes the output weight matrix , and pool ( ) denotes the sum pooling across all nodes on the graph .",INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,204,1,0,,3.35E-07,0,negative,8.11E-09,7.10E-09,5.38E-08,2.03E-11,1.44E-08,1.90E-07,4.98E-07,5.18E-06,5.06E-08,0.999993652,5.99E-17,4.03E-08,3.01E-07
911,sentiment_analysis1,219,We choose sum pooling because it demonstrated more expressive power than mean pooling and max pooling .,INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,205,1,0,,0.000151001,0,negative,2.15E-07,8.60E-08,1.75E-06,1.29E-10,4.20E-07,5.69E-06,4.83E-05,6.41E-05,9.40E-08,0.999870883,7.18E-17,7.78E-07,7.68E-06
912,sentiment_analysis1,220,"Note that we use the absolute values of A to compute the degree matrix D because A has negative elements , e.g. , global connections .",INTRODUCTION,Dynamics of RGNN,sentiment_analysis,1,206,1,0,,2.34E-07,0,negative,1.76E-08,2.90E-09,5.33E-08,1.44E-12,7.41E-09,2.19E-08,1.16E-07,2.85E-07,1.38E-08,0.99999939,1.30E-17,6.13E-08,3.02E-08
913,sentiment_analysis1,221,Node - wise Domain Adversarial Training,INTRODUCTION,,sentiment_analysis,1,207,1,0,,0.739788156,1,negative,8.88E-06,0.001626814,0.00054727,4.30E-08,0.000650567,0.000761319,0.276919212,0.000850934,0.000471135,0.715434777,7.72E-13,1.32E-05,0.002715822
914,sentiment_analysis1,222,"EEG signals vary significantly across different subjects , which hinders the generalizability of trained classifiers .",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,208,1,0,,2.50E-06,0,negative,9.88E-09,2.14E-10,1.06E-07,1.27E-11,4.57E-09,6.45E-08,2.04E-06,2.43E-07,6.88E-10,0.999993562,1.63E-16,6.45E-07,3.32E-06
915,sentiment_analysis1,223,"To improve subject - independent classification performance , we extend the domain adversatial training by proposing a node - wise domain adversarial training ( NodeDAT ) to reduce the discrepancies between source and target domains , i.e. , training and testing sets , respectively .",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,209,1,0,,0.002459215,0,negative,1.03E-05,2.04E-06,0.000106781,1.39E-09,1.16E-06,3.31E-06,0.000406806,2.55E-05,3.51E-06,0.998818169,1.69E-15,1.40E-05,0.000608421
916,sentiment_analysis1,224,"Specifically , a domain classifier is proposed to classify each node representation into either source domain or target domain .",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,210,1,0,,5.12E-05,0,negative,9.32E-07,1.45E-07,2.55E-05,3.02E-11,7.83E-08,2.53E-07,1.66E-05,2.22E-06,3.78E-06,0.999917075,3.07E-16,7.60E-07,3.26E-05
917,sentiment_analysis1,225,"Compared to , which only regularizes the pooled representation in the last layer , our NodeDAT has finer - grained regularization because it explicitly regularizes each node representation before pooling ( see Section 7.1 ) .",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,211,1,0,,0.000420448,0,negative,1.17E-05,2.06E-07,3.28E-05,7.70E-10,5.24E-07,1.77E-06,0.00015846,7.35E-06,4.56E-07,0.999613895,1.71E-16,1.24E-05,0.000160396
918,sentiment_analysis1,226,"During optimization , our model aims to confuse the domain classifier by learning domain - invariant representations for each node .",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,212,1,0,,0.000127402,0,negative,1.37E-06,1.79E-07,5.25E-06,7.65E-11,9.94E-08,5.25E-07,2.64E-05,8.78E-06,1.04E-06,0.999923113,1.77E-16,1.00E-06,3.22E-05
919,sentiment_analysis1,227,"Specifically , given source / training data XS ?",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,213,1,0,,2.07E-08,0,negative,6.70E-09,4.06E-10,8.15E-08,9.14E-14,7.14E-10,2.32E-09,1.04E-07,2.33E-08,3.45E-09,0.999999673,2.87E-18,5.89E-08,4.60E-08
920,sentiment_analysis1,228,"RN nd ( in this subsection , we denote X by XS for better clarity ) and unlabelled target / testing data X T ?",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,214,1,0,,1.48E-07,0,negative,5.39E-09,1.69E-10,4.22E-08,2.52E-12,2.06E-09,3.82E-08,1.68E-06,3.91E-07,4.84E-10,0.999996525,1.81E-17,2.86E-07,1.03E-06
921,sentiment_analysis1,229,"RN nd , wherein practice X T can be either oversampled or donwsampled to have the same number of samples as XS , the domain classifier aims to minimize the sum of the following two binary cross - entropy losses :",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,215,1,0,,2.36E-07,0,negative,3.58E-08,1.50E-09,4.02E-07,8.23E-12,6.07E-09,5.27E-08,1.59E-06,5.19E-07,8.40E-09,0.999995111,2.36E-17,3.51E-07,1.93E-06
922,sentiment_analysis1,230,"where 0 and 1 denote source and target domains , respectively .",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,216,1,0,,8.41E-07,0,negative,3.95E-09,4.03E-10,3.17E-08,2.94E-12,1.55E-09,6.63E-08,8.95E-07,1.45E-06,4.56E-09,0.999996434,6.56E-18,4.95E-08,1.07E-06
923,sentiment_analysis1,231,Intuitively the domain classifier aims to classify source data as 0 and target data as 1 .,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,217,1,0,,4.55E-07,0,negative,5.53E-09,4.22E-10,1.83E-07,1.24E-13,6.74E-10,5.60E-09,2.24E-07,7.53E-08,2.10E-08,0.999999219,3.04E-18,3.58E-08,2.30E-07
924,sentiment_analysis1,232,The domain probabilities p D ( ) j for node j are computed as,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,218,1,0,,9.61E-08,0,negative,2.62E-08,3.43E-10,2.38E-07,3.84E-13,1.90E-09,1.11E-08,3.97E-07,6.66E-08,5.12E-09,0.99999891,2.28E-18,1.14E-07,2.30E-07
925,sentiment_analysis1,233,where Z Note that our domain classifier implements a gradient reversal layer ( GRL ) to reverse the gradients of the domain classifier during backpropagation .,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,219,1,0,,6.86E-07,0,negative,2.37E-08,4.15E-10,4.13E-07,1.53E-12,3.08E-09,2.17E-08,4.00E-07,1.14E-07,6.19E-09,0.999998527,2.66E-18,6.44E-08,4.27E-07
926,sentiment_analysis1,234,The gradients are further scaled by a GRL scaling factor ?,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,220,1,0,,1.27E-05,0,negative,1.32E-07,2.37E-09,4.78E-07,1.40E-11,1.22E-08,3.13E-07,4.88E-06,2.77E-06,2.53E-08,0.999985504,4.94E-18,1.25E-07,5.75E-06
927,sentiment_analysis1,235,which gradually increases from 0 to 1 as the training progresses .,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,221,1,0,,1.40E-06,0,negative,3.91E-07,1.98E-08,4.51E-07,1.24E-10,5.87E-08,4.75E-06,0.000227488,0.000211638,6.05E-08,0.999411177,2.74E-17,1.06E-06,0.000142906
928,sentiment_analysis1,236,The gradually increasing ?,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,222,1,0,,4.03E-07,0,negative,2.04E-08,1.33E-10,4.79E-08,2.81E-12,1.40E-09,6.62E-08,1.40E-06,5.26E-07,2.28E-09,0.999996687,4.34E-18,9.45E-08,1.16E-06
929,sentiment_analysis1,237,allows our domain classifier to be less sensitive to noisy inputs at the early stages of the training process .,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,223,1,0,,1.22E-06,0,negative,2.62E-07,9.23E-10,8.26E-07,8.11E-12,1.01E-08,5.62E-08,3.50E-06,2.79E-07,6.71E-09,0.999990368,3.89E-18,7.24E-07,3.96E-06
930,sentiment_analysis1,238,"Specifically , as suggested in , we let ? = 2 1 +e ?10 p ? 1 , where p ?",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,224,1,0,,1.72E-07,0,negative,9.62E-08,1.79E-09,1.21E-07,2.87E-11,2.15E-08,1.04E-06,2.33E-05,1.04E-05,3.95E-09,0.999952246,4.11E-18,3.76E-07,1.24E-05
931,sentiment_analysis1,239,"[ 0 , 1 ] denotes the training progress .",INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,225,1,0,,2.47E-06,0,negative,4.93E-09,1.68E-09,9.87E-08,3.63E-12,2.65E-09,2.45E-07,1.04E-05,9.38E-06,1.72E-08,0.999970185,1.24E-17,1.13E-07,9.51E-06
932,sentiment_analysis1,240,Emotion - aware Distribution,INTRODUCTION,Node - wise Domain Adversarial Training,sentiment_analysis,1,226,1,0,,8.95E-05,0,negative,3.20E-07,1.30E-08,8.33E-06,9.22E-11,2.02E-08,1.31E-06,0.001759053,7.32E-06,6.07E-08,0.996135465,2.17E-15,3.36E-05,0.00205446
933,sentiment_analysis1,241,Learning,INTRODUCTION,,sentiment_analysis,1,227,1,0,,0.004440392,0,negative,6.23E-07,1.19E-05,1.04E-05,4.20E-08,0.000261731,0.000657177,0.214186541,0.000760765,1.57E-06,0.780699186,1.81E-14,4.34E-06,0.00340575
934,sentiment_analysis1,242,Participants may not always generate the intended emotions when watching emotion - eliciting stimuli .,INTRODUCTION,Learning,sentiment_analysis,1,228,1,0,,9.98E-07,0,negative,1.70E-09,4.87E-10,1.74E-08,5.85E-12,2.20E-08,2.34E-07,6.28E-06,4.24E-07,3.26E-10,0.999992327,3.47E-17,5.50E-08,6.40E-07
935,sentiment_analysis1,243,"To address the problem of noisy emotion labels in the datasets , we propose an emotion - aware distribution learning method ( EmotionDL ) to learn a distribution of classes instead of one single class for each training sample .",INTRODUCTION,Learning,sentiment_analysis,1,229,1,0,,0.001205312,0,negative,2.99E-06,1.05E-05,0.000186311,1.77E-09,1.35E-05,3.36E-05,0.006016154,4.87E-05,2.11E-06,0.993112147,2.21E-15,5.37E-06,0.000568675
936,sentiment_analysis1,244,"Specifically , we convert each training label Y i ? { 0 , 1 , ... , C ? 1 } into a prior probability distribution of all classes ?",INTRODUCTION,Learning,sentiment_analysis,1,230,1,0,,1.01E-07,0,negative,1.38E-08,1.04E-08,1.42E-07,2.20E-12,5.00E-08,2.67E-07,4.49E-06,8.62E-07,3.09E-08,0.999993796,1.51E-18,1.08E-08,3.25E-07
937,sentiment_analysis1,245,"i ? RC , where ?",INTRODUCTION,Learning,sentiment_analysis,1,231,1,0,,1.34E-07,0,negative,5.19E-09,3.95E-10,2.54E-08,1.37E-11,3.56E-08,1.15E-06,1.19E-05,1.09E-06,4.08E-10,0.999985164,2.21E-18,4.38E-08,6.15E-07
938,sentiment_analysis1,246,ic denotes the probability of class c in ?,INTRODUCTION,Learning,sentiment_analysis,1,232,1,0,,1.66E-07,0,negative,2.12E-09,3.43E-10,2.97E-08,4.08E-13,1.27E-08,7.77E-08,1.61E-06,1.25E-07,5.62E-10,0.999998028,6.90E-19,1.71E-08,9.26E-08
939,sentiment_analysis1,247,i .,INTRODUCTION,Learning,sentiment_analysis,1,233,1,0,,1.16E-07,0,negative,5.01E-09,2.37E-10,1.97E-08,9.20E-12,1.95E-08,8.48E-07,4.09E-06,7.04E-07,6.03E-10,0.999993866,7.63E-19,1.31E-08,4.35E-07
940,sentiment_analysis1,248,The conversion is dataset - dependent .,INTRODUCTION,Learning,sentiment_analysis,1,234,1,0,,5.67E-06,0,negative,4.71E-08,5.52E-09,5.34E-07,9.49E-12,1.36E-07,2.35E-07,7.16E-06,2.33E-07,3.13E-09,0.99999084,3.92E-18,8.50E-08,7.17E-07
941,sentiment_analysis1,249,"In SEED , there are three classes : negative , neutral , and positive with corresponding class indices 0 , 1 , and 2 , respectively .",INTRODUCTION,Learning,sentiment_analysis,1,235,1,0,,4.20E-06,0,negative,5.20E-09,8.87E-09,2.03E-07,6.62E-12,1.93E-07,3.38E-06,3.71E-05,1.27E-05,1.85E-08,0.999943152,7.50E-19,1.44E-08,3.24E-06
942,sentiment_analysis1,250,We convert Y as follows :,INTRODUCTION,Learning,sentiment_analysis,1,236,1,0,,1.57E-07,0,negative,2.18E-09,4.15E-10,1.15E-07,1.14E-12,1.20E-08,1.64E-07,2.19E-06,1.40E-07,2.52E-09,0.999997071,9.47E-19,5.86E-09,2.98E-07
943,sentiment_analysis1,251,where ?,INTRODUCTION,Learning,sentiment_analysis,1,237,1,0,,6.08E-08,0,negative,5.40E-10,1.54E-10,9.48E-09,1.90E-12,4.73E-09,3.21E-07,1.46E-06,3.59E-07,5.56E-10,0.999997703,7.59E-19,2.72E-09,1.42E-07
944,sentiment_analysis1,252,"[ 0 , 1 ] denotes a hyper - parameter controlling the noise level in the training labels .",INTRODUCTION,Learning,sentiment_analysis,1,238,1,0,,2.44E-05,0,negative,1.63E-08,3.37E-08,1.26E-07,7.86E-11,2.33E-07,2.37E-05,0.000309385,0.000216882,3.61E-08,0.999422204,5.72E-18,4.65E-08,2.74E-05
945,sentiment_analysis1,253,This conversion mechanism is based on our assumption that participants are unlikely to generate opposite emotions when watching emotioneliciting stimuli .,INTRODUCTION,Learning,sentiment_analysis,1,239,1,0,,5.90E-07,0,negative,1.09E-08,4.27E-09,6.55E-07,1.70E-12,3.93E-08,2.48E-07,6.48E-06,3.88E-07,3.50E-08,0.999991032,2.09E-18,9.78E-09,1.10E-06
946,sentiment_analysis1,254,"Therefore , the converted class distribution centers on the original class and has non-zero and zero probabilities at its nearest and opposite classes , respectively .",INTRODUCTION,Learning,sentiment_analysis,1,240,1,0,,3.33E-06,0,negative,5.36E-09,4.75E-09,1.65E-07,1.37E-13,1.22E-08,6.82E-08,2.81E-06,2.59E-07,4.63E-08,0.999996505,3.61E-19,6.65E-09,1.18E-07
947,sentiment_analysis1,255,"In SEED - IV , there are four classes : neutral , sad , fear , and happy with corresponding class indices 0 , 1 , 2 , and 3 , respectively .",INTRODUCTION,Learning,sentiment_analysis,1,241,1,0,,7.79E-06,0,negative,5.44E-09,8.01E-09,3.06E-07,3.47E-12,3.43E-07,3.07E-06,0.000110288,9.75E-06,5.82E-09,0.999869954,2.74E-19,3.19E-08,6.24E-06
948,sentiment_analysis1,256,We can convert Y as follows :,INTRODUCTION,Learning,sentiment_analysis,1,242,1,0,,1.23E-07,0,negative,9.95E-10,1.53E-10,3.42E-08,3.88E-13,4.65E-09,8.71E-08,1.39E-06,8.82E-08,9.69E-10,0.999998248,4.01E-19,4.24E-09,1.43E-07
949,sentiment_analysis1,257,The intuition behind this conversion is based on the distances between the four emotions on the valence - arousal plane .,INTRODUCTION,Learning,sentiment_analysis,1,243,1,0,,1.12E-07,0,negative,2.70E-09,5.22E-09,4.68E-07,3.75E-13,1.89E-08,1.64E-07,5.19E-06,3.12E-07,4.96E-08,0.999993181,1.35E-18,5.26E-09,6.02E-07
950,sentiment_analysis1,258,"Specifically , in the self - reported ratings , neutral , sad , fear , and happy movie ratings cluster in the zero valence zero arousal , negative valence negative arousal , negative valence positive arousal , and positive valence positive arousal regions , respectively .",INTRODUCTION,Learning,sentiment_analysis,1,244,1,0,,0.000182349,0,negative,1.42E-06,4.18E-09,3.67E-07,1.17E-11,6.50E-07,9.73E-07,0.000256165,9.34E-07,1.19E-09,0.999733421,2.16E-19,1.45E-06,4.61E-06
951,sentiment_analysis1,259,"Thus , we assume that participants are likely to generate emotions that have similar ratings in either valence or arousal dimensions , e.g. , both angry and happy have high arousal , but unlikely to generate emotions thatare faraway in both dimensions , e.g. , sad and happy are different in both valence and arousal .",INTRODUCTION,Learning,sentiment_analysis,1,245,1,0,,2.64E-07,0,negative,1.02E-09,1.60E-09,3.16E-08,7.53E-13,1.66E-08,3.93E-07,5.91E-06,1.75E-06,5.06E-09,0.999991476,2.08E-19,4.56E-09,4.10E-07
952,sentiment_analysis1,260,"After obtaining the converted class distributions ? , our model can be optimized by minimizing the following Kullback - Leibler ( KL ) divergence instead of ( 11 ) :",INTRODUCTION,Learning,sentiment_analysis,1,246,1,0,,6.50E-07,0,negative,8.78E-09,2.05E-09,7.64E-08,3.89E-13,1.33E-08,8.91E-08,5.25E-06,2.32E-07,2.22E-09,0.999994106,3.06E-19,2.30E-08,1.98E-07
953,sentiment_analysis1,261,"where p ( Y| X i , ? ) denotes the output probability distribution computed via .",INTRODUCTION,Learning,sentiment_analysis,1,247,1,0,,3.98E-08,0,negative,5.87E-10,7.15E-11,1.01E-08,1.59E-13,3.28E-09,4.86E-08,6.99E-07,6.38E-08,1.79E-10,0.999999117,6.42E-20,2.73E-09,5.44E-08
954,sentiment_analysis1,262,"Note that our EmotionDL is different from label smoothing , which simply adds uniform noise to other classes .",INTRODUCTION,Learning,sentiment_analysis,1,248,1,0,,2.50E-06,0,negative,1.46E-07,3.40E-08,3.49E-06,9.77E-12,4.69E-07,1.04E-06,4.37E-05,7.97E-07,1.66E-08,0.999947463,4.41E-19,1.00E-07,2.72E-06
955,sentiment_analysis1,263,Optimization of RGNN,INTRODUCTION,,sentiment_analysis,1,249,1,0,,0.034466658,0,negative,1.43E-07,1.17E-05,2.42E-06,2.03E-09,1.64E-05,0.000643324,0.184763469,0.002328268,4.18E-06,0.810439611,1.85E-15,8.42E-07,0.001789687
956,sentiment_analysis1,264,"Combining both NodeDAT and Emotion DL , the over all loss function ?",INTRODUCTION,Optimization of RGNN,sentiment_analysis,1,250,1,0,,1.92E-06,0,negative,1.69E-08,3.33E-09,2.94E-07,1.44E-13,1.43E-08,2.01E-08,1.43E-06,1.08E-07,7.99E-09,0.999997808,6.89E-19,8.36E-08,2.12E-07
957,sentiment_analysis1,265,of RGNN is computed as follows :,INTRODUCTION,Optimization of RGNN,sentiment_analysis,1,251,1,0,,4.61E-08,0,negative,7.58E-09,2.42E-09,2.18E-07,1.10E-12,3.02E-08,1.50E-07,9.77E-07,1.12E-06,1.18E-08,0.999996954,4.07E-19,1.39E-08,5.16E-07
958,sentiment_analysis1,266,The detailed algorithm for training RGNN is presented in Algorithm,INTRODUCTION,Optimization of RGNN,sentiment_analysis,1,252,1,0,,4.94E-07,0,negative,1.97E-09,2.76E-09,6.10E-08,2.64E-12,2.99E-08,3.83E-07,1.89E-06,3.60E-06,1.42E-08,0.999992635,5.70E-19,1.32E-08,1.37E-06
959,sentiment_analysis1,267,1 . repeat Draw one batch of training samples X B and ?,INTRODUCTION,Optimization of RGNN,sentiment_analysis,1,253,1,0,,3.06E-08,0,negative,1.43E-08,1.23E-09,3.09E-08,1.67E-12,3.70E-08,3.32E-07,5.20E-06,2.59E-06,1.24E-09,0.999990873,1.68E-19,4.31E-08,8.79E-07
960,sentiment_analysis1,268,"B from X and ? , respectively ; Draw one batch of testing samples X TB from X T ; Compute degree matrix D based on ( 3 ) ;",INTRODUCTION,Optimization of RGNN,sentiment_analysis,1,254,1,0,,1.46E-07,0,negative,9.21E-09,1.52E-09,6.64E-08,7.22E-13,6.77E-08,8.31E-08,2.10E-06,5.85E-07,1.09E-09,0.999996721,8.34E-20,4.10E-08,3.21E-07
961,sentiment_analysis1,269,Algorithm,INTRODUCTION,,sentiment_analysis,1,255,1,0,,0.000554582,0,negative,2.56E-08,1.42E-06,1.59E-06,9.53E-10,1.14E-05,0.000190373,0.012489702,0.000313227,2.01E-06,0.986694579,1.59E-16,9.36E-08,0.000295585
962,sentiment_analysis1,270,1 The Training Algorithm for RGNN,INTRODUCTION,Algorithm,sentiment_analysis,1,256,1,0,,4.20E-06,0,negative,7.87E-09,1.86E-07,2.14E-06,5.60E-11,1.32E-06,9.61E-06,3.76E-05,7.81E-05,2.97E-07,0.999855294,3.01E-17,6.20E-08,1.55E-05
963,sentiment_analysis1,271,7:00,INTRODUCTION,Algorithm,sentiment_analysis,1,257,1,0,,1.04E-07,0,negative,9.88E-09,1.36E-08,2.01E-07,2.30E-10,1.14E-06,1.38E-05,7.32E-06,4.72E-05,1.33E-08,0.99992698,1.16E-18,1.53E-08,3.29E-06
964,sentiment_analysis1,272,Compute normalized adjacency matrix S based on ( 5 ) ; Compute output representation Z based on ( 12 ) ; Use X B and ?,INTRODUCTION,Algorithm,sentiment_analysis,1,258,1,0,,4.30E-07,0,negative,4.32E-08,1.10E-07,2.24E-06,5.57E-12,2.00E-06,1.43E-06,2.05E-05,5.54E-06,1.88E-08,0.999966832,1.84E-18,1.62E-07,1.15E-06
965,sentiment_analysis1,273,B to compute KL loss ? based on ( 17 ) ; Use X B and X TB to compute domain loss ?,INTRODUCTION,Algorithm,sentiment_analysis,1,259,1,0,,4.12E-07,0,negative,5.97E-09,2.14E-08,1.92E-07,8.86E-12,1.06E-06,1.55E-06,7.21E-06,1.28E-05,4.16E-09,0.999976113,7.00E-19,3.86E-08,9.90E-07
966,sentiment_analysis1,274,D based on ( 13 ) ; Compute GRL scaling factor ? ;,INTRODUCTION,Algorithm,sentiment_analysis,1,260,1,0,,5.93E-07,0,negative,2.48E-08,6.77E-08,5.52E-07,7.93E-11,3.21E-06,7.59E-06,2.01E-05,5.28E-05,1.56E-08,0.999911573,1.73E-18,6.78E-08,3.96E-06
967,sentiment_analysis1,275,Update,INTRODUCTION,,sentiment_analysis,1,261,1,0,,0.000125355,0,negative,9.42E-08,1.87E-06,3.81E-06,5.89E-10,1.98E-05,0.000158502,0.017930439,0.000233086,2.87E-06,0.981278008,1.92E-17,8.03E-08,0.00037144
968,sentiment_analysis1,276,; until all samples in X are drawn ;,INTRODUCTION,Update,sentiment_analysis,1,262,1,0,,6.72E-06,0,negative,2.19E-08,5.28E-07,4.30E-06,1.86E-11,4.91E-06,6.74E-06,0.000102777,1.10E-05,4.03E-07,0.99986473,8.75E-18,1.07E-08,4.55E-06
969,sentiment_analysis1,277,EXPERIMENTAL,,,sentiment_analysis,1,0,1,0,,0.001281501,0,negative,1.14E-05,1.42E-05,6.85E-07,2.03E-07,2.28E-07,6.11E-05,2.04E-05,0.000682205,1.85E-05,0.998905252,0.000178689,0.000106534,6.71E-07
970,sentiment_analysis1,278,SETTINGS,EXPERIMENTAL,,sentiment_analysis,1,1,1,0,,0.001615611,0,negative,2.03E-05,5.30E-05,6.50E-05,6.10E-06,2.07E-06,0.001096708,0.000179675,0.010413033,2.81E-05,0.987887109,6.93E-05,0.0001696,1.01E-05
971,sentiment_analysis1,279,"In this section , we present the datasets , classification settings and model settings in our experiments .",EXPERIMENTAL,SETTINGS,sentiment_analysis,1,2,1,0,,0.035089371,0,experimental-setup,3.59E-06,4.42E-06,3.60E-06,4.49E-06,4.15E-05,0.642477254,0.00032047,0.001490798,1.16E-06,0.355647082,4.08E-06,7.44E-07,8.44E-07
972,sentiment_analysis1,280,Datasets,EXPERIMENTAL,,sentiment_analysis,1,3,1,0,,0.001117269,0,negative,1.09E-05,6.58E-06,7.70E-05,4.80E-07,1.67E-07,0.000361316,0.00014446,0.002423729,7.63E-06,0.996581099,0.000178931,0.000201385,6.27E-06
973,sentiment_analysis1,281,We use both SEED and SEED - IV datasets in our experiments .,EXPERIMENTAL,Datasets,sentiment_analysis,1,4,1,0,,0.019662473,0,negative,5.66E-06,5.66E-05,6.85E-05,3.69E-07,3.16E-06,0.000907041,4.84E-05,0.004951337,6.30E-06,0.993898473,2.43E-06,5.13E-05,4.17E-07
974,sentiment_analysis1,282,The SEED dataset comprises EEG data of 15 subjects ( 7 males ) recorded in 62 channels using the ESI NeuroScan System 1 .,EXPERIMENTAL,Datasets,sentiment_analysis,1,5,1,0,,0.011842117,0,negative,6.03E-05,8.68E-05,0.000658612,1.56E-05,0.000327852,0.000623,8.65E-05,0.00045436,1.11E-05,0.997519043,4.16E-06,0.000150561,2.04E-06
975,sentiment_analysis1,283,The EEG data was collected when 1 .,EXPERIMENTAL,Datasets,sentiment_analysis,1,6,1,0,,3.89E-05,0,negative,4.48E-06,3.98E-06,7.54E-05,1.79E-08,1.85E-07,0.000107575,3.54E-06,0.000505692,4.23E-06,0.9992692,2.24E-06,2.34E-05,8.37E-08
976,sentiment_analysis1,284,"https://compumedicsneuroscan.com/ participants watch emotion - eliciting movies in three types of emotions , namely negative , neutral and positive .",EXPERIMENTAL,Datasets,sentiment_analysis,1,7,1,0,,0.008835429,0,negative,5.73E-05,2.72E-05,0.000104494,0.014403664,0.000513697,0.003571631,4.67E-05,0.000318369,1.65E-05,0.980888301,2.12E-05,1.65E-05,1.44E-05
977,sentiment_analysis1,285,Each movie lasts around 4 minutes .,EXPERIMENTAL,Datasets,sentiment_analysis,1,8,1,0,,0.017766318,0,negative,3.33E-05,4.33E-05,4.27E-05,1.83E-05,9.30E-05,0.001114399,5.80E-05,0.001419058,1.11E-05,0.99708145,8.61E-06,7.17E-05,5.03E-06
978,sentiment_analysis1,286,There are three sessions of data collected and each session comprises 15 trials / movies for each subject .,EXPERIMENTAL,Datasets,sentiment_analysis,1,9,1,0,,0.004355105,0,negative,1.73E-05,7.18E-05,2.79E-05,7.63E-06,7.44E-05,0.00085805,6.18E-05,0.002149798,9.23E-06,0.996651606,5.59E-06,6.22E-05,2.74E-06
979,sentiment_analysis1,287,"To make a fair comparison with existing studies , we directly use the pre-computed differential entropy ( DE ) features smoothed by linear dynamic systems ( LDS ) , in SEED .",EXPERIMENTAL,Datasets,sentiment_analysis,1,10,1,0,,0.039355711,0,negative,0.000137836,0.000605906,0.003038982,5.39E-07,1.19E-05,0.000249022,3.58E-05,0.001434758,9.41E-05,0.994070627,4.19E-06,0.000315374,9.38E-07
980,sentiment_analysis1,288,DE extends the idea of Shannon entropy and measures the complexity of a continuous random variable .,EXPERIMENTAL,Datasets,sentiment_analysis,1,11,1,0,,0.021572441,0,negative,4.30E-05,0.000329522,0.026974,4.12E-07,2.77E-06,8.23E-05,2.58E-05,0.000390649,0.000356196,0.971135593,0.00034252,0.000313349,3.87E-06
981,sentiment_analysis1,289,"For a fixed length EEG segment , DE features are computed as the logarithm energy spectrum in a certain frequency band .",EXPERIMENTAL,Datasets,sentiment_analysis,1,12,1,0,,0.019811363,0,negative,1.88E-05,0.000381156,0.000661471,2.23E-07,2.77E-06,0.000167858,5.44E-06,0.001588678,0.000353907,0.996764925,1.14E-05,4.26E-05,7.78E-07
982,sentiment_analysis1,290,"In SEED , DE features are pre-computed over five frequency bands ( delta , theta , alpha , beta and gamma ) for each second of EEG signals ( without overlapping ) in each channel .",EXPERIMENTAL,Datasets,sentiment_analysis,1,13,1,0,,0.100932839,0,negative,2.09E-05,0.000578663,0.000328076,5.05E-07,4.27E-06,0.00141994,2.46E-05,0.018458105,0.000393963,0.978739348,3.76E-06,2.63E-05,1.47E-06
983,sentiment_analysis1,291,The SEED - IV dataset comprises EEG data of 15 subjects ( 7 males ) recorded in 62 channels,EXPERIMENTAL,Datasets,sentiment_analysis,1,14,1,0,,0.024378655,0,negative,0.000183689,8.98E-05,0.002127656,6.68E-06,0.000309424,0.000478671,0.000260681,0.000426256,9.81E-06,0.993676012,4.81E-06,0.002419797,6.71E-06
984,sentiment_analysis1,292,2 .,EXPERIMENTAL,Datasets,sentiment_analysis,1,15,1,0,,8.77E-05,0,negative,3.08E-06,2.86E-06,3.51E-06,4.68E-08,1.06E-07,5.49E-05,8.52E-07,0.000280675,8.55E-06,0.999639199,8.79E-07,5.23E-06,8.32E-08
985,sentiment_analysis1,293,The recording device is the same as the one used in SEED .,EXPERIMENTAL,Datasets,sentiment_analysis,1,16,1,0,,0.000368686,0,negative,4.76E-06,4.24E-05,0.00013877,8.21E-07,2.97E-06,0.000514751,2.83E-05,0.001694172,6.94E-05,0.997458316,9.64E-06,3.38E-05,1.89E-06
986,sentiment_analysis1,294,"The EEG data were collected when participants watch emotion - eliciting movies in four types of emotions , namely , neutral , sad , fear , and happy .",EXPERIMENTAL,Datasets,sentiment_analysis,1,17,1,0,,0.008009366,0,negative,5.24E-06,0.000115669,3.88E-05,3.74E-07,1.54E-05,0.000103154,1.43E-05,0.000705067,2.36E-05,0.99890425,6.40E-06,6.72E-05,5.94E-07
987,sentiment_analysis1,295,Each movie lasts around 2 minutes .,EXPERIMENTAL,Datasets,sentiment_analysis,1,18,1,0,,0.013629881,0,negative,2.67E-05,3.68E-05,3.38E-05,1.17E-05,0.00011184,0.000736153,4.04E-05,0.001264945,9.81E-06,0.997632883,2.59E-06,8.69E-05,5.50E-06
988,sentiment_analysis1,296,There are three sessions of data collected and each session comprises 24 trials / movies for each subject .,EXPERIMENTAL,Datasets,sentiment_analysis,1,19,1,0,,0.003932731,0,negative,1.46E-05,7.42E-05,2.84E-05,7.47E-06,0.000124807,0.000646852,5.70E-05,0.00177434,8.64E-06,0.997169144,2.30E-06,8.80E-05,4.17E-06
989,sentiment_analysis1,297,"Similar to SEED , we adopt the pre-computed DE features from SEED - IV .",EXPERIMENTAL,Datasets,sentiment_analysis,1,20,1,0,,0.075685621,0,negative,0.000172024,0.000360225,0.008816959,6.13E-07,1.23E-05,0.000390864,7.12E-05,0.002344484,0.000139073,0.987222474,8.52E-07,0.000466423,2.58E-06
990,sentiment_analysis1,298,Classification Settings,EXPERIMENTAL,,sentiment_analysis,1,21,1,0,,0.012889813,0,negative,5.13E-05,7.82E-05,0.000717414,1.33E-05,1.47E-05,0.005686688,0.003937417,0.027195852,3.35E-05,0.959052715,3.77E-05,0.003007586,0.000173526
991,sentiment_analysis1,299,We conduct both subject - dependent and subjectindependent classifications on both SEED and SEED - IV to evaluate our model .,EXPERIMENTAL,Classification Settings,sentiment_analysis,1,22,1,0,,0.108190766,0,negative,5.38E-05,1.50E-05,7.42E-05,4.87E-07,4.00E-05,0.011722964,0.002515123,0.000518308,2.84E-06,0.984962437,1.06E-07,9.24E-05,2.43E-06
992,sentiment_analysis1,300,Subject - Dependent Classification,EXPERIMENTAL,,sentiment_analysis,1,23,1,0,,0.010387302,0,negative,5.91E-05,0.000118075,0.000406249,1.38E-05,7.09E-06,0.002702843,0.002019729,0.029270383,0.000182474,0.962173928,0.000111512,0.00258575,0.00034903
993,sentiment_analysis1,301,Subject - Independent Classification,EXPERIMENTAL,,sentiment_analysis,1,24,1,0,,0.019867413,0,negative,5.30E-05,8.03E-05,0.000205367,1.25E-05,7.01E-06,0.0020591,0.000839854,0.019731585,0.000149507,0.97549549,3.24E-05,0.001167885,0.000166006
994,sentiment_analysis1,302,"Similar to , presents the subject - independent classification results .",EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,25,1,0,,0.000145355,0,negative,2.62E-06,1.66E-05,6.14E-06,5.44E-08,9.33E-07,2.09E-05,2.12E-06,5.52E-05,3.22E-05,0.999845475,8.79E-06,8.91E-06,6.00E-08
995,sentiment_analysis1,303,"When using features from all frequency bands , our model performs marginally worse than BiHDM on SEED but much better than BiHDM on SEED - IV ( nearly 5 % improvement ) .",EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,26,1,0,,0.326039151,0,negative,0.036776247,0.000532165,0.000418717,0.000101685,0.000368967,0.002692225,0.00830787,0.004799971,6.20E-05,0.590569996,0.000213902,0.354850177,0.000306121
996,sentiment_analysis1,304,"In addition , our model achieves the lowest standard deviation in accuracy compared to all baselines on both datasets , demonstrating the robustness of our model .",EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,27,1,0,,0.279969077,0,negative,0.011653457,0.000407188,0.000163002,6.36E-05,0.000350368,0.000880098,0.004025271,0.001893888,2.55E-05,0.760839101,0.000117186,0.219464788,0.000116535
997,sentiment_analysis1,305,"Comparing the results shown in Tables 1 and 2 , we find that the accuracy obtained in subject - independent settings is consistently worse than the accuracy obtained in subjectdependent settings by around 5 % to 30 % for every model .",EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,28,1,0,,0.390160424,0,negative,0.00508059,0.000275146,3.17E-05,2.69E-05,0.000198871,0.001030859,0.001362822,0.002154505,2.18E-05,0.964603643,0.000113388,0.025067701,3.20E-05
998,sentiment_analysis1,306,This finding is unsurprising because the variability of EEG signals across subjects makes subject - independent classification more challenging .,EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,29,1,0,,1.16E-05,0,negative,3.21E-06,5.26E-06,8.16E-07,7.60E-08,1.42E-06,1.58E-05,1.71E-06,3.33E-05,3.58E-06,0.999912589,1.16E-05,1.06E-05,6.10E-08
999,sentiment_analysis1,307,"However , the interesting part is that the performance gap between these two settings is gradually decreasing from around 27 % on SEED and 19 % on SEED - IV using SVM to around 9 % on SEED and 6 % on SEED - IV using our model .",EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,30,1,0,,0.001916229,0,negative,0.026702664,0.000302734,8.58E-05,5.59E-06,0.000122016,0.00018671,0.00027017,0.000439565,5.25E-05,0.958927904,2.00E-05,0.012876611,7.72E-06
1000,sentiment_analysis1,308,One possible reason for the diminishing gap is that recent deep learning models in subjectindependent settings are becoming better at leveraging a larger amount of data and learning more subject - invariant EEG representations .,EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,31,1,0,,2.02E-05,0,negative,6.48E-06,5.72E-05,5.17E-06,9.21E-07,5.63E-06,7.26E-05,1.01E-05,0.000194129,5.30E-05,0.999389557,0.000180642,2.35E-05,1.04E-06
1001,sentiment_analysis1,309,This observation seems to indicate that transfer learning maybe a necessary tool for emotion recognition in cross - subject settings .,EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,32,1,0,,9.24E-05,0,negative,1.21E-05,1.13E-05,1.94E-06,5.94E-08,2.18E-06,1.46E-05,2.31E-06,4.10E-05,1.11E-05,0.999870832,6.82E-06,2.56E-05,9.04E-08
1002,sentiment_analysis1,310,"With the increasing amount of data available from different subjects and a proper transfer learning tool , it would not be surprising that subject - independent classification accuracy will surpass the subject - dependent classification accuracy in the future .",EXPERIMENTAL,Subject - Independent Classification,sentiment_analysis,1,33,1,0,,0.000417368,0,negative,4.32E-06,1.38E-05,1.19E-06,2.73E-07,1.86E-06,4.66E-05,7.35E-06,0.000150013,2.41E-05,0.999683512,4.27E-05,2.39E-05,4.47E-07
1003,sentiment_analysis1,311,Model Settings in RGNN,,,sentiment_analysis,1,0,1,0,,0.26618796,0,negative,5.63E-05,0.000535043,5.21E-05,3.79E-06,3.28E-06,0.002403571,0.000893985,0.031129665,0.000228028,0.959509309,0.004257025,0.000904011,2.39E-05
1004,sentiment_analysis1,312,"For our RGNN in all experiments , we empirically set the number of convolutional layers L = 2 , dropout rate of 0.7 at the output fully - connected layer , and batch size of 16 .",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,1,1,1,hyperparameters,0.906101255,1,hyperparameters,1.82E-05,1.62E-05,6.27E-05,7.87E-06,3.55E-07,0.148669501,0.002227511,0.809970783,1.46E-05,0.038954771,4.56E-06,2.74E-05,2.55E-05
1005,sentiment_analysis1,313,"We use Adam optimization with default values , i.e. , ? 1 = 0.9 and ? 2 = 0.999 .",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,2,1,1,hyperparameters,0.950417275,1,hyperparameters,8.10E-06,8.99E-06,4.58E-05,2.03E-06,9.21E-08,0.12896171,0.001062558,0.842426981,1.21E-05,0.027445909,2.76E-06,1.06E-05,1.24E-05
1006,sentiment_analysis1,314,"We only tune the output feature dimension d , label noise level , learning rate ? , L1 regularization factor ? , and L2 regularization for each experiment .",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,3,1,1,hyperparameters,0.362440042,0,hyperparameters,9.63E-05,6.51E-05,0.000202225,3.67E-06,8.30E-07,0.037524874,0.000853416,0.571983603,4.56E-05,0.389057704,9.24E-06,0.000138819,1.86E-05
1007,sentiment_analysis1,315,Note that we only adopt NodeDAT in subjectindependent classification experiments .,Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,4,1,0,,0.011886129,0,negative,0.000785903,3.25E-05,0.008297959,4.07E-07,4.81E-07,0.000988959,0.000157167,0.006035404,1.84E-05,0.982617401,2.74E-06,0.001059921,2.81E-06
1008,sentiment_analysis1,316,"We compare our model with several baselines , which are cited from published results , , , .",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,5,1,0,,0.00147032,0,negative,1.64E-05,3.29E-06,0.000563497,8.09E-08,1.03E-07,0.000809983,0.000144523,0.002888889,1.59E-06,0.995347882,1.71E-06,0.000221273,7.81E-07
1009,sentiment_analysis1,317,PERFORMANCE EVALUATIONS,Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,6,1,0,,0.009494401,0,negative,2.12E-05,1.25E-06,0.000126448,1.40E-07,8.43E-08,0.001080602,0.000706609,0.004047372,1.12E-06,0.992253812,7.99E-06,0.001750176,3.19E-06
1010,sentiment_analysis1,318,In this section we present model evaluation results in both subject - dependent and subject - independent classification settings on both datasets .,Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,7,1,0,,0.298659719,0,negative,0.000159063,7.82E-06,0.000438779,1.75E-07,3.70E-07,0.000254433,0.000357892,0.001421571,1.09E-06,0.987761252,3.63E-06,0.009591619,2.30E-06
1011,sentiment_analysis1,319,We also investigate critical frequency bands and confusion matrix of our model .,Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,8,1,0,,0.015322805,0,negative,0.000450227,1.70E-05,0.00096763,9.96E-07,8.17E-07,0.000764395,0.000105385,0.003408179,2.41E-05,0.993426759,3.94E-06,0.000826643,3.95E-06
1012,sentiment_analysis1,320,presents the subject - dependent classification accuracy ( mean / standard deviation ) of our RGNN model and all baselines on both SEED and SEED - IV using the precomputed DE features .,Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,9,1,0,,0.004500322,0,negative,3.54E-05,1.13E-06,0.000242944,5.79E-07,2.66E-07,0.001834467,0.000480471,0.005744112,1.30E-06,0.989766834,4.84E-06,0.001880818,6.86E-06
1013,sentiment_analysis1,321,"The performance on SEED using DE feature in the individual delta , theta , alpha , beta , and gamma bands is reported as well .",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,10,1,0,,0.076663143,0,negative,0.000803614,2.81E-06,0.003292202,1.35E-07,3.00E-07,0.000229379,0.001076982,0.000825395,8.23E-07,0.890132502,3.59E-06,0.103626913,5.35E-06
1014,sentiment_analysis1,322,It is encouraging to see that our model achieves superior performance on both datasets as compared to all baselines including the stateof - the - art BiHDM when DE features from all frequency bands are used .,Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,11,1,1,results,0.952251045,1,results,0.000968027,1.31E-06,0.000255855,1.01E-06,5.88E-07,0.000428957,0.004712787,0.001697746,3.81E-07,0.064577297,6.58E-06,0.927282702,6.68E-05
1015,sentiment_analysis1,323,It is worth noting that our model improves the accuracy of the state - of - the - art model on SEED - IV by around 5 % .,Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,12,1,1,results,0.961607404,1,results,0.00684568,2.94E-06,0.000631691,4.87E-06,3.10E-06,0.000519608,0.007307258,0.001731745,1.15E-06,0.046703458,4.74E-06,0.93595111,0.000292647
1016,sentiment_analysis1,324,"In particular , our model performs better than DGCNN , which is another GNN - based model that leverages the topological structure in EEG signals .",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,13,1,1,results,0.958400599,1,results,0.002128415,2.81E-06,0.000287937,4.25E-06,1.83E-06,0.001616441,0.008372575,0.006817021,9.95E-07,0.087888433,7.73E-06,0.892702426,0.000169132
1017,sentiment_analysis1,325,"Besides the proposed two regularizers ( see ) , the main performance improvement can be attributed to two factors :",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,14,1,0,,0.002629871,0,negative,0.010144291,1.74E-05,0.018059622,1.85E-06,2.36E-06,0.000425995,0.000542683,0.000936554,1.73E-05,0.948024953,5.48E-06,0.02180195,1.96E-05
1018,sentiment_analysis1,326,"1 ) our adjacency matrix incorporates the global inter-channel asymmetry relation between the left and right hemispheres ; and 2 ) our model has less concern of overfitting by extending SGC , which is much simpler than ChebNet used in DGCNN .",Model Settings in RGNN,Model Settings in RGNN,sentiment_analysis,1,15,1,0,,0.222578783,0,negative,0.021182383,5.38E-05,0.009970854,9.27E-06,9.39E-06,0.001442537,0.002516658,0.004169334,2.86E-05,0.837999175,1.20E-05,0.122513485,9.25E-05
1019,sentiment_analysis1,327,Performance Comparison of Frequency Bands,Model Settings in RGNN,,sentiment_analysis,1,16,1,0,,0.575219541,1,negative,0.000364404,3.62E-06,0.004916134,1.59E-07,1.77E-07,0.00068088,0.005735978,0.002655471,2.98E-06,0.75756148,4.00E-05,0.22801025,2.85E-05
1020,sentiment_analysis1,328,"We further compare the performance of our model and all baselines using features from different frequency bands , as reported in .",Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,17,1,0,,0.093854107,0,negative,8.54E-05,1.39E-07,2.52E-05,3.97E-09,2.03E-08,3.91E-06,9.11E-06,1.54E-05,1.01E-07,0.996538109,4.33E-08,0.003322489,8.94E-08
1021,sentiment_analysis1,329,"In subject - dependent experiments on SEED , STRNN achieves the highest accuracy in delta , theta and alpha bands , BiDANN performs best in beta band , and our model performs best in gamma band .",Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,18,1,1,results,0.916597301,1,results,0.002135042,9.84E-08,4.12E-05,6.34E-08,7.38E-08,4.38E-06,0.000242387,1.84E-05,3.84E-08,0.049252083,4.18E-07,0.948297356,8.46E-06
1022,sentiment_analysis1,330,"In subject - independent experiments on SEED , BiDANN - S achieves the highest accuracy in theta and alpha bands , and our model performs best in delta , beta and gamma bands .",Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,19,1,1,results,0.90352175,1,results,0.00170815,1.17E-07,4.67E-05,7.11E-08,8.63E-08,5.01E-06,0.000267851,2.13E-05,4.51E-08,0.054197519,3.99E-07,0.943742997,9.83E-06
1023,sentiment_analysis1,331,We investigate the critical frequency bands for emotion recognition .,Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,20,1,0,,0.0056572,0,negative,0.000156426,1.54E-05,0.000651602,3.10E-07,4.66E-07,2.28E-05,4.68E-05,0.00021056,1.16E-05,0.992471756,0.000105363,0.006292769,1.41E-05
1024,sentiment_analysis1,332,"For both subject - dependent and subjectindependent settings on SEED , we compare the performance of each model across different frequency bands .",Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,21,1,1,results,0.00237092,0,negative,0.000116327,1.45E-06,0.000128953,8.34E-09,6.90E-08,4.49E-06,7.75E-06,3.47E-05,9.61E-07,0.998200033,5.14E-08,0.001505028,1.68E-07
1025,sentiment_analysis1,333,"In general , most models including our model achieve better performance on beta and gamma bands than delta , theta and alpha bands , with one exception of STRNN , which performs the worst on gamma band .",Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,22,1,1,results,0.934329634,1,results,0.001454793,8.35E-08,2.94E-05,8.69E-08,7.57E-08,9.10E-06,0.000288833,3.28E-05,4.78E-08,0.096069216,4.71E-07,0.902105592,9.51E-06
1026,sentiment_analysis1,334,"This observation is consistent with the literature , .",Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,23,1,0,,2.91E-05,0,negative,1.34E-05,2.08E-08,3.47E-06,8.31E-09,1.51E-08,3.08E-06,1.51E-06,1.17E-05,1.14E-07,0.9997405,3.80E-08,0.000226008,1.26E-07
1027,sentiment_analysis1,335,"One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band , whereas other models perform comparably in both bands , indicating that gamma band maybe the most discriminative band for our model .",Model Settings in RGNN,Performance Comparison of Frequency Bands,sentiment_analysis,1,24,1,1,results,0.805014497,1,results,0.00463612,3.15E-07,3.59E-05,3.24E-07,2.66E-07,3.19E-05,0.000331193,0.000135084,2.33E-07,0.381654216,6.58E-07,0.61315952,1.43E-05
1028,sentiment_analysis1,336,Confusion Matrix,Model Settings in RGNN,,sentiment_analysis,1,25,1,0,,0.000953916,0,negative,1.12E-05,8.24E-07,0.000298506,1.19E-07,1.17E-07,0.000714058,0.000265883,0.004364711,9.65E-06,0.993895972,1.11E-06,0.000430312,7.58E-06
1029,sentiment_analysis1,337,We present the confusion matrix of our model in .,Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,26,1,0,,8.16E-05,0,negative,1.02E-05,8.11E-06,3.51E-05,5.23E-07,1.54E-06,7.25E-05,7.78E-06,0.006135407,7.47E-06,0.993614301,9.57E-07,0.000104961,1.14E-06
1030,sentiment_analysis1,338,"For both subject - dependent and subject - independent settings on SEED , our model can recognize better for positive and neutral emotions than negative emotion .",Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,27,1,0,,0.049371178,0,results,0.002308493,4.30E-05,0.000179308,5.59E-06,7.99E-06,0.000429613,0.001315323,0.062202181,6.47E-06,0.213553106,1.11E-05,0.719805176,0.000132626
1031,sentiment_analysis1,339,"By combining training data from other subjects ( see ( a ) and ( b ) ) , our model is getting much worse at detecting negative emotion , indicating that participants are likely to generate distinct EEG patterns when experiencing negative emotion .",Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,28,1,0,,0.452977452,0,results,0.006009217,4.23E-05,0.00032162,3.56E-06,1.23E-05,0.000240518,0.000870791,0.021072479,6.52E-06,0.261386816,4.99E-06,0.709950516,7.84E-05
1032,sentiment_analysis1,340,Similar phenomenon is observed in SEED - IV for sad emotion as well ( see ) .,Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,29,1,0,,7.60E-05,0,negative,1.58E-05,4.38E-06,4.37E-05,1.61E-07,5.97E-07,8.67E-05,2.94E-05,0.010016412,3.08E-06,0.988904332,2.64E-06,0.00089052,2.24E-06
1033,sentiment_analysis1,341,"For SEED - IV , our model performs significantly better on sad emotion than all other emotions in both classification settings .",Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,30,1,0,,0.803691588,1,results,0.001476838,1.72E-05,0.000171498,4.20E-06,5.51E-06,0.000274079,0.002596775,0.024676615,1.70E-06,0.041942632,4.81E-06,0.92865456,0.000173588
1034,sentiment_analysis1,342,We notice that fear is the only emotion that performs better in subject - independent classification than in subject - dependent classification .,Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,31,1,0,,0.357329423,0,results,0.003786411,2.31E-05,0.000230574,8.16E-06,1.19E-05,0.000464706,0.002089373,0.033476422,3.07E-06,0.129602001,4.61E-06,0.830165221,0.000134439
1035,sentiment_analysis1,343,This finding indicates that participants watching horror movies may generate similar EEG patterns .,Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,32,1,0,,0.000244358,0,negative,4.79E-05,1.91E-05,8.35E-05,2.95E-07,2.97E-06,7.66E-05,3.88E-05,0.010793411,9.02E-06,0.98578789,1.18E-06,0.003136012,3.27E-06
1036,sentiment_analysis1,344,MODEL ANALYSIS ON RGNN,Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,33,1,0,,0.001536228,0,negative,0.000294004,0.000197041,0.007886523,1.48E-05,1.89E-05,0.000964344,0.00264096,0.091481805,0.00019548,0.851445695,0.000417563,0.043787009,0.000655927
1037,sentiment_analysis1,345,In this section we conduct ablation study and sensitivity analysis for model .,Model Settings in RGNN,Confusion Matrix,sentiment_analysis,1,34,1,0,,0.000143762,0,negative,0.000347593,0.000204138,0.000588478,8.00E-06,2.99E-05,0.00019196,5.93E-05,0.019643029,4.85E-05,0.977081657,1.55E-06,0.001782328,1.35E-05
1038,sentiment_analysis1,346,Ablation Study,,,sentiment_analysis,1,0,1,0,,0.006818876,0,negative,0.046739945,0.000279772,0.002808551,0.000142874,8.76E-05,0.000276199,0.001210224,0.00046093,0.000147782,0.934438164,0.002965146,0.010376843,6.60E-05
1039,sentiment_analysis1,347,We conduct ablation study to investigate the contribution of each key component in our model .,Ablation Study,Ablation Study,sentiment_analysis,1,1,1,0,,0.017943563,0,ablation-analysis,0.844701186,2.91E-06,0.000387463,9.27E-07,1.40E-05,1.04E-05,6.69E-06,5.65E-07,3.13E-05,0.154822089,2.34E-06,2.76E-06,1.74E-05
1040,sentiment_analysis1,348,reports the results obtained in subject - independent setting on both datasets .,Ablation Study,Ablation Study,sentiment_analysis,1,2,1,0,,0.106644117,0,negative,0.34349966,3.95E-06,0.000897132,2.84E-06,5.07E-05,4.59E-05,9.39E-05,3.29E-06,1.14E-05,0.655025955,2.28E-05,2.74E-05,0.00031496
1041,sentiment_analysis1,349,"The two major designs in our adjacency matrix A , i.e. , global connection and symmetric adjacency matrix designs , are helpful in recognizing emotions .",Ablation Study,Ablation Study,sentiment_analysis,1,3,1,1,ablation-analysis,0.366461951,0,ablation-analysis,0.987690775,5.66E-07,4.47E-05,1.34E-07,3.35E-06,1.54E-06,6.48E-06,1.15E-07,1.95E-06,0.012232352,7.13E-07,6.07E-06,1.13E-05
1042,sentiment_analysis1,350,"The global connection models the asymmetric difference between neuronal activities in the left and right hemispheres and have been shown to reveal certain emotions , , .",Ablation Study,Ablation Study,sentiment_analysis,1,4,1,1,ablation-analysis,0.167777986,0,ablation-analysis,0.549584648,3.50E-05,0.055210685,1.97E-06,8.15E-06,8.46E-05,6.05E-05,6.41E-06,0.00374814,0.39035756,0.000273852,9.83E-06,0.00061866
1043,sentiment_analysis1,351,"The symmetric adjacency matrix design is mostly motivated to reduce the number of model parameters and prevent overfitting , especially in subject - dependent classifications where lesser training data is available .",Ablation Study,Ablation Study,sentiment_analysis,1,5,1,0,,0.181288056,0,negative,0.143334754,3.05E-05,0.000649689,1.42E-05,2.11E-05,0.00014646,3.51E-05,2.26E-05,0.000173496,0.853913225,0.000631263,4.64E-06,0.001022932
1044,sentiment_analysis1,352,"Our NodeDAT regularizer has a noticeable positive impact on the performance of our model , which demonstrates that domain adaptation is significantly helpful in crosssubject classification .",Ablation Study,Ablation Study,sentiment_analysis,1,6,1,1,ablation-analysis,0.98430671,1,ablation-analysis,0.999543277,1.62E-07,2.86E-06,5.48E-08,2.21E-07,4.33E-07,1.65E-05,7.12E-08,2.43E-07,0.000409569,1.69E-07,7.73E-06,1.87E-05
1045,sentiment_analysis1,353,"To further investigate the impact of our node - level domain classifier , we further experimented with replacing NodeDAT with a generic domain classifier ( DAT ) that operates after the pooling operation , i.e. , ( - NodeDAT + DAT ) in .",Ablation Study,Ablation Study,sentiment_analysis,1,7,1,0,,0.044784957,0,ablation-analysis,0.856750379,3.63E-05,0.004174775,3.36E-07,9.40E-06,3.62E-05,9.13E-05,2.90E-06,0.000262879,0.1385863,3.10E-06,6.32E-06,3.98E-05
1046,sentiment_analysis1,354,The clear performance gap between ( - NodeDAT + DAT ) and our RGNN model indicates that our NodeDAT can better regularize the model by learning subject - invariant representation at node level than graph level .,Ablation Study,Ablation Study,sentiment_analysis,1,8,1,0,,0.951613241,1,ablation-analysis,0.997273538,1.46E-07,9.51E-06,3.72E-08,2.54E-07,5.85E-07,1.84E-05,6.59E-08,2.34E-07,0.002659781,5.84E-07,1.83E-05,1.86E-05
1047,sentiment_analysis1,355,"In addition , if NodeDAT is removed , the performance of our model has a greater variance , demonstrating the importance of NodeDAT in improving the robustness of our model against cross - subject variations .",Ablation Study,Ablation Study,sentiment_analysis,1,9,1,1,ablation-analysis,0.973904824,1,ablation-analysis,0.998456334,1.41E-07,5.69E-06,2.70E-08,2.72E-07,2.55E-07,1.86E-06,2.50E-08,6.70E-07,0.001531405,6.81E-08,1.24E-06,2.02E-06
1048,sentiment_analysis1,356,Our Emotion,Ablation Study,,sentiment_analysis,1,10,1,0,,0.022115243,0,negative,0.18033739,3.45E-05,0.002880769,6.74E-05,0.000216209,0.000601239,0.000251625,3.03E-05,0.00095922,0.786754361,0.000414871,1.99E-05,0.02743225
1049,sentiment_analysis1,357,DL regularizer improves performance of our model by around 3 % in accuracy on both datasets .,Ablation Study,Our Emotion,sentiment_analysis,1,11,1,1,ablation-analysis,0.816750692,1,ablation-analysis,0.995976273,2.66E-06,5.36E-05,2.30E-06,5.65E-06,7.85E-06,0.000141612,1.58E-06,4.60E-06,0.003373265,1.27E-06,0.000175297,0.000254082
1050,sentiment_analysis1,358,This performance gain validates our assumption that participants are not always generating the intended emotions when watching emotion - eliciting stimuli .,Ablation Study,Our Emotion,sentiment_analysis,1,12,1,0,,0.066967297,0,ablation-analysis,0.730566615,7.16E-06,9.56E-05,4.34E-07,1.15E-05,6.64E-06,9.63E-05,1.73E-06,2.33E-05,0.2687416,5.82E-06,0.00040161,4.17E-05
1051,sentiment_analysis1,359,"In addition , our Emotion DL can be easily adopted by other deep learning models .",Ablation Study,Our Emotion,sentiment_analysis,1,13,1,0,,0.004577521,0,negative,0.034778991,0.000385958,0.001217926,2.08E-05,0.000129336,0.000109887,6.39E-05,5.97E-05,0.006960639,0.955617054,9.26E-05,5.40E-05,0.000509067
1052,sentiment_analysis1,360,Sensitivity Analysis,Ablation Study,,sentiment_analysis,1,14,1,0,,0.080125071,0,negative,0.404553834,1.69E-05,0.002555387,5.92E-06,2.96E-05,0.000264568,0.000323505,1.90E-05,0.00026667,0.590021224,7.18E-05,1.55E-05,0.001856094
1053,sentiment_analysis1,361,We analyze the performance of our model across varying L1 sparsity coefficient ?,Ablation Study,Sensitivity Analysis,sentiment_analysis,1,15,1,0,,0.000442825,0,negative,9.85E-05,6.63E-06,2.19E-05,3.24E-07,2.41E-06,1.44E-05,2.74E-06,3.61E-05,7.87E-06,0.999792715,1.05E-05,5.57E-06,3.15E-07
1054,sentiment_analysis1,362,"( see ) and noise coefficient in Emotion DL ( see and ) , as illustrated in .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,16,1,0,,0.000258221,0,negative,0.005491288,4.25E-06,0.000774154,9.61E-08,2.88E-06,3.84E-06,5.72E-06,5.56E-06,8.50E-06,0.99359303,6.07E-06,0.000104406,2.16E-07
1055,sentiment_analysis1,363,"For subject - dependent classification , increasing ?",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,17,1,0,,0.011460496,0,negative,0.001106044,2.94E-05,0.000278695,9.19E-08,1.20E-06,1.44E-05,5.09E-06,6.64E-05,8.11E-05,0.998367326,2.17E-05,2.82E-05,3.73E-07
1056,sentiment_analysis1,364,from 0 to 0.1 will generally increase the model performance .,Ablation Study,Sensitivity Analysis,sentiment_analysis,1,18,1,0,,0.003446612,0,negative,0.008912982,1.81E-05,0.000106189,1.19E-06,8.53E-06,0.0001123,3.87E-05,0.000433446,4.93E-05,0.990217138,4.19E-06,9.43E-05,3.68E-06
1057,sentiment_analysis1,365,"However , for subject - independent classification , increasing ?",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,19,1,0,,0.000361155,0,negative,0.001821474,7.50E-06,6.72E-05,1.51E-07,1.18E-06,6.36E-06,4.18E-06,2.89E-05,1.62E-05,0.997952213,4.72E-05,4.70E-05,3.75E-07
1058,sentiment_analysis1,366,"beyond a certain threshold , i.e , 0.01 in , will decrease the model performance .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,20,1,0,,0.001809843,0,negative,0.14763816,1.61E-05,0.000165674,3.83E-06,4.50E-05,4.44E-05,3.75E-05,0.00010001,2.53E-05,0.851447906,4.10E-06,0.000467062,4.93E-06
1059,sentiment_analysis1,367,"One possible explanation for the difference in model behaviors is that there is much less training data in subject - dependent classification , which requires a stronger regularization to reduce overfitting , whereas for subject - independent classification where the number of training data is less of a concern , adding stronger and regularization may introduce bias and hinder the learning efficacy .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,21,1,0,,9.67E-05,0,negative,0.000687712,1.00E-05,6.51E-05,3.72E-07,5.94E-06,1.28E-05,5.26E-06,2.86E-05,1.21E-05,0.99913583,3.84E-06,3.19E-05,5.20E-07
1060,sentiment_analysis1,368,"As illustrated in , our model behaves consistently across different experimental settings with varying noise coefficient .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,22,1,0,,0.083000563,0,negative,0.021765993,0.000106113,0.000122068,1.24E-05,3.45E-05,0.000105493,0.000169104,0.000409703,7.79E-05,0.974775535,7.71E-05,0.002320325,2.38E-05
1061,sentiment_analysis1,369,"Specifically , by increasing , the performance of our model first increases and then decreases .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,23,1,0,,0.001602824,0,negative,0.016950754,5.49E-05,9.83E-05,2.08E-06,1.98E-05,2.88E-05,2.35E-05,0.000136744,9.30E-05,0.982403009,1.04E-05,0.000175184,3.45E-06
1062,sentiment_analysis1,370,"In particular , our model usually performs best when is set to 0.2 , demonstrating the existence of label noises and the necessity of addressing them on both datasets .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,24,1,0,,0.603851131,1,ablation-analysis,0.508276982,3.43E-05,0.000105543,1.20E-05,5.76E-05,0.000102702,0.000283638,0.000175369,1.14E-05,0.486151811,1.34E-05,0.004753947,2.12E-05
1063,sentiment_analysis1,371,"Introducing excessive noise in Emotion DL causes performance drop , which is expected because excessive noise weakens the true learning signals .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,25,1,0,,0.044799993,0,negative,0.026270297,4.23E-06,0.000133183,2.61E-07,1.79E-05,3.58E-06,1.19E-05,4.01E-06,1.73E-06,0.973167008,3.98E-06,0.000381186,7.49E-07
1064,sentiment_analysis1,372,NEURONAL ACTIVITY ANALYSIS FOR EMOTION RECOGNITION,Ablation Study,Sensitivity Analysis,sentiment_analysis,1,26,1,0,,0.566183315,1,negative,0.003389363,0.000205795,0.002827855,2.68E-05,3.34E-05,0.000211129,0.004871429,0.000390832,0.000271407,0.870143695,0.113174666,0.002641482,0.001812158
1065,sentiment_analysis1,373,In this section we analyze and identify important neuronal activities for emotion recognition .,Ablation Study,Sensitivity Analysis,sentiment_analysis,1,27,1,0,,0.000764754,0,negative,0.000992691,0.000150098,0.001169858,3.89E-07,5.58E-05,7.34E-06,1.07E-05,1.28E-05,9.77E-05,0.997417879,1.95E-05,6.39E-05,1.46E-06
1066,sentiment_analysis1,374,"that there are strong activations on the prefrontal , parietal , and occipital regions , indicating that these regions maybe strongly related to the emotion processing of the brain .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,28,1,0,,0.007202645,0,negative,0.004092631,1.20E-06,0.000112731,1.42E-07,8.10E-06,5.43E-06,7.09E-06,5.76E-06,3.48E-06,0.995667923,6.90E-07,9.42E-05,6.04E-07
1067,sentiment_analysis1,375,"Our finding is consistent with existing studies , which observed that asymmetrical frontal and parietal EEG activity may reflect changes on both valence and arousal , .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,29,1,0,,0.002010235,0,negative,0.003770243,3.79E-05,0.000209282,4.37E-07,2.28E-05,1.67E-05,1.94E-05,2.99E-05,7.52E-05,0.995663341,2.28E-06,0.000150933,1.63E-06
1068,sentiment_analysis1,376,"The synchronization between frontal and occipital regions has also been reported to be related to positive and fear emotion , .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,30,1,0,,0.000478984,0,negative,9.83E-05,2.68E-06,8.77E-05,1.64E-07,1.96E-06,8.04E-06,4.75E-06,1.40E-05,2.07E-05,0.999739871,1.31E-05,7.00E-06,1.71E-06
1069,sentiment_analysis1,377,The symmetry pattern on the activation map of channels indicate again that the asymmetry in EEG activity between the left and right hemispheres is critical for emotion recognition .,Ablation Study,Sensitivity Analysis,sentiment_analysis,1,31,1,0,,0.002292125,0,negative,0.004116911,6.76E-06,0.000157723,6.58E-07,2.80E-05,1.33E-05,1.25E-05,1.38E-05,2.91E-05,0.995526422,1.36E-06,9.19E-05,1.65E-06
1070,sentiment_analysis1,378,"shows the top 10 connections between channels having the largest edge weights in our adjacency matrix A . Note that all global connections remain among the strongest connections after A is learned , demonstrating again that global inter-channel relations are essential for emotion recognition .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,32,1,0,,0.000754647,0,negative,0.000603599,4.30E-06,0.000116799,8.39E-07,1.96E-05,4.76E-05,2.50E-05,4.61E-05,1.28E-05,0.999083024,1.39E-06,3.47E-05,4.40E-06
1071,sentiment_analysis1,379,"It is obvious from that there are both similarities and differences between these two plots , indicating that our initialization strategy presented in can capture local inter-channel relations to a certain degree .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,33,1,0,,0.002747583,0,negative,0.014724821,8.63E-06,0.000183333,1.11E-07,4.60E-06,6.44E-06,2.71E-05,1.16E-05,9.06E-06,0.983948975,1.03E-06,0.00107359,6.97E-07
1072,sentiment_analysis1,380,"One notable difference between the two plots is that a few strong connections are gone in , e.g. , ( POZ , PO3 ) , ( PO6 , PO8 ) , and ( P3 , P5 ) , indicating that these connections may not be critical for emotion recognition .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,34,1,0,,0.000177442,0,negative,0.000596419,1.92E-06,8.13E-05,2.74E-08,3.41E-06,3.05E-06,3.27E-06,3.77E-06,2.68E-06,0.99925127,1.44E-07,5.26E-05,1.15E-07
1073,sentiment_analysis1,381,"In addition , it is clear from that the connection between the channel pair ( FP1 , AF3 ) is the strongest , followed by ( F6 , F8 ) , ( FP2 , AF4 ) , and ( PO8 , CB2 ) , indicating that local inter-channel relations in the frontal region maybe important for emotion recognition .",Ablation Study,Sensitivity Analysis,sentiment_analysis,1,35,1,0,,0.335296321,0,negative,0.318326932,0.000117073,0.001041433,3.11E-06,9.10E-05,4.71E-05,0.00033829,9.49E-05,0.0001572,0.674425324,5.50E-06,0.005329657,2.25E-05
1074,sentiment_analysis1,382,Activation Maps of Channels,Ablation Study,,sentiment_analysis,1,36,1,0,,0.089193411,0,negative,0.364203164,3.28E-06,0.002174566,3.65E-07,7.82E-06,2.82E-05,0.000126318,1.67E-06,0.000159958,0.632480226,3.03E-06,1.56E-05,0.000795742
1075,sentiment_analysis1,383,Inter-channel Relations,Ablation Study,,sentiment_analysis,1,37,1,0,,0.004380824,0,negative,0.131080366,5.62E-06,0.002742942,3.88E-07,7.91E-06,2.95E-05,8.73E-05,1.76E-06,0.000239566,0.865124273,3.42E-06,1.08E-05,0.000666183
1076,sentiment_analysis1,384,CONCLUSION,,,sentiment_analysis,1,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
1077,question_generation1,1,title,,,question_generation,1,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
1078,question_generation1,2,Multimodal Differential Network for Visual Question Generation,title,,question_generation,1,1,1,1,research-problem,0.997529142,1,research-problem,1.90E-08,8.44E-06,7.51E-08,3.28E-08,2.23E-08,7.90E-08,6.12E-07,1.44E-06,2.54E-06,0.001723481,0.998263098,1.25E-07,3.45E-08
1079,question_generation1,3,abstract,,,question_generation,1,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
1080,question_generation1,4,Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .,abstract,abstract,question_generation,1,1,1,1,research-problem,0.920357591,1,research-problem,2.87E-08,3.91E-06,1.31E-08,5.07E-06,9.35E-07,3.84E-07,5.09E-07,8.07E-07,2.01E-07,0.014520312,0.985467744,1.56E-08,7.05E-08
1081,question_generation1,5,"Images can have multiple visual and language contexts thatare relevant for generating questions namely places , captions , and tags .",abstract,abstract,question_generation,1,2,1,0,,0.013190141,0,research-problem,2.50E-07,0.000211701,2.39E-07,5.95E-05,9.27E-05,1.40E-05,2.70E-06,2.89E-05,1.69E-05,0.468160266,0.531412347,1.68E-07,4.62E-07
1082,question_generation1,6,"In this paper , we propose the use of exemplars for obtaining the relevant context .",abstract,abstract,question_generation,1,3,1,0,,0.430121815,0,research-problem,4.55E-05,0.205797625,7.47E-05,3.07E-05,0.000370926,3.72E-05,1.61E-05,0.000503363,0.009654575,0.363987309,0.419469347,9.69E-06,3.02E-06
1083,question_generation1,7,We obtain this by using a Multimodal Differential Network to produce natural and engaging questions .,abstract,abstract,question_generation,1,4,1,0,,0.017610458,0,negative,8.11E-06,0.018707213,8.38E-06,0.000393004,0.00306238,7.25E-05,8.94E-06,0.000187216,0.001217111,0.909241304,0.06709005,2.02E-06,1.75E-06
1084,question_generation1,8,The generated questions show a remarkable similarity to the natural questions as validated by a human study .,abstract,abstract,question_generation,1,5,1,0,,0.010550432,0,negative,4.75E-06,0.001076293,3.01E-07,5.41E-06,3.75E-05,1.86E-05,1.14E-05,0.000212153,6.46E-05,0.841679097,0.156866209,2.34E-05,4.80E-07
1085,question_generation1,9,"Further , we observe that the proposed approach substantially improves over state - of - the - art benchmarks on the quantitative metrics ( BLEU , METEOR , ROUGE , and CIDEr ) .",abstract,abstract,question_generation,1,6,1,0,,0.061268013,0,negative,0.00048115,0.004863316,9.33E-06,0.000253842,0.000342789,0.00014017,0.000497525,0.001905189,9.98E-05,0.722779977,0.267084145,0.001512617,3.02E-05
1086,question_generation1,10,Introduction,,,question_generation,1,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
1087,question_generation1,11,"To understand the progress towards multimedia vision and language understanding , a visual Turing test was proposed by that was aimed at visual question answering .",Introduction,Introduction,question_generation,1,1,1,0,,0.890144929,1,research-problem,2.69E-06,0.000884916,1.72E-06,2.60E-05,0.00029285,1.32E-05,3.21E-05,1.10E-05,0.0001141,0.094302283,0.904311746,3.07E-06,4.32E-06
1088,question_generation1,12,Visual ) is a natural extension for VQA .,Introduction,Introduction,question_generation,1,2,1,0,,0.417713075,0,research-problem,0.000106789,0.107485226,0.001472316,7.35E-06,0.000691356,8.51E-05,0.000323833,0.000102471,0.198257659,0.303615808,0.3876839,0.000149685,1.85E-05
1089,question_generation1,13,"Current dialog systems as evaluated in show that when trained between bots , AI - AI dialog systems show improvement , but that does not translate to actual improvement for Human - AI dialog .",Introduction,Introduction,question_generation,1,3,1,0,,0.033042709,0,research-problem,8.24E-06,0.000653485,1.46E-06,1.13E-05,8.06E-05,1.96E-05,7.02E-05,2.98E-05,0.000155427,0.317938856,0.681000834,2.53E-05,5.01E-06
1090,question_generation1,14,"This is because , the questions generated by bots are not natural ( human - like ) and therefore does not translate to improved human dialog .",Introduction,Introduction,question_generation,1,4,1,0,,0.030646327,0,negative,1.44E-05,0.001303938,1.58E-06,1.20E-05,0.000399728,2.23E-05,9.16E-06,1.79E-05,0.000426368,0.958306952,0.039479349,5.62E-06,6.93E-07
1091,question_generation1,15,Therefore it is imperative that improvement in the quality of questions will enable dialog agents to perform well in human interactions .,Introduction,Introduction,question_generation,1,5,1,0,,0.187686435,0,research-problem,7.06E-06,0.000940958,8.12E-07,1.07E-05,6.75E-05,1.81E-05,1.73E-05,2.56E-05,0.000464359,0.485757725,0.512680901,7.19E-06,1.74E-06
1092,question_generation1,16,"Further , show that unanswered questions can be used for improving VQA , Image captioning and Object Classification .",Introduction,Introduction,question_generation,1,6,1,0,,0.005988499,0,negative,5.17E-05,0.003113055,6.73E-06,0.000147496,0.00239194,0.000164588,6.20E-05,6.78E-05,0.002225929,0.965287782,0.026456156,1.90E-05,5.81E-06
1093,question_generation1,17,An interesting line of work in this respect is the work of .,Introduction,Introduction,question_generation,1,7,1,0,,0.003491172,0,negative,4.51E-06,0.001028873,1.77E-06,1.24E-05,4.71E-05,7.85E-05,3.92E-05,7.44E-05,0.001235351,0.655519065,0.341951245,4.92E-06,2.67E-06
1094,question_generation1,18,Here the au-thors have proposed the challenging task of generating natural questions for an image .,Introduction,Introduction,question_generation,1,8,1,1,research-problem,0.06445786,0,research-problem,2.38E-06,0.000312361,9.66E-07,0.000115424,0.000261139,4.60E-05,2.85E-05,1.76E-05,7.07E-05,0.378718642,0.620420075,1.94E-06,4.21E-06
1095,question_generation1,19,One aspect that is central to a question is the context that is relevant to generate it .,Introduction,Introduction,question_generation,1,9,1,0,,0.007764832,0,negative,1.04E-05,0.004500069,2.88E-06,3.61E-05,0.000796937,9.75E-05,9.53E-06,6.02E-05,0.007412019,0.980805016,0.006265319,2.84E-06,1.10E-06
1096,question_generation1,20,"However , this context changes for every image .",Introduction,Introduction,question_generation,1,10,1,0,,0.03879372,0,negative,5.89E-05,0.076217495,3.32E-05,4.68E-06,0.000466226,3.47E-05,3.00E-05,8.70E-05,0.11471527,0.767436857,0.040882603,3.11E-05,1.90E-06
1097,question_generation1,21,"As can be seen in , an image with a person on a skateboard would result in questions related to the event .",Introduction,Introduction,question_generation,1,11,1,0,,0.005108194,0,negative,5.33E-06,0.000399474,5.42E-07,1.98E-05,0.00052044,4.74E-05,1.15E-05,2.53E-05,0.000294935,0.989564722,0.009106723,2.87E-06,1.03E-06
1098,question_generation1,22,"Whereas for a little girl , the questions could be related to age rather than the action .",Introduction,Introduction,question_generation,1,12,1,0,,0.001412678,0,negative,7.39E-06,0.000710944,1.18E-06,1.19E-05,0.000264277,9.28E-05,1.33E-05,5.32E-05,0.001149852,0.994432362,0.003259297,2.91E-06,6.76E-07
1099,question_generation1,23,How can one have widely varying context provided for generating questions ?,Introduction,Introduction,question_generation,1,13,1,0,,0.001509526,0,negative,4.37E-06,0.003968258,1.26E-06,1.84E-05,6.79E-05,0.00016504,1.97E-05,0.000222935,0.005715395,0.944776561,0.045035342,2.68E-06,2.02E-06
1100,question_generation1,24,"To solve this problem , we use the context obtained by considering exemplars , specifically we use the difference between relevant and irrelevant exemplars .",Introduction,Introduction,question_generation,1,14,1,1,approach,0.778119495,1,model,4.42E-05,0.237089417,6.51E-05,1.51E-06,0.000451916,2.17E-05,1.25E-05,5.51E-05,0.674038179,0.086284882,0.001926031,8.41E-06,1.09E-06
1101,question_generation1,25,"We consider different contexts in the form of Location , Caption , and Part of Speech tags .",Introduction,Introduction,question_generation,1,15,1,1,approach,0.727394685,1,approach,6.75E-05,0.714714864,0.000153043,8.99E-05,0.035446039,0.000450163,0.000228982,0.000800601,0.138832287,0.108275177,0.000878901,4.41E-05,1.84E-05
1102,question_generation1,26,Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding .,Introduction,Introduction,question_generation,1,16,1,1,approach,0.922127742,1,model,2.48E-05,0.336265652,4.59E-05,3.46E-06,0.000336479,3.41E-05,1.70E-05,0.000125041,0.644548501,0.017612814,0.000978971,4.98E-06,2.30E-06
1103,question_generation1,27,This embedding is used by a question decoder to decode the appropriate question .,Introduction,Introduction,question_generation,1,17,1,1,approach,0.88496115,1,model,4.60E-06,0.038345133,1.77E-05,5.87E-07,6.56E-05,1.64E-05,6.38E-06,5.19E-05,0.939097133,0.021978186,0.000414377,1.18E-06,8.06E-07
1104,question_generation1,28,"As discussed further , we observe this implicit differential context to perform better than an explicit keyword based context .",Introduction,Introduction,question_generation,1,18,1,0,,0.162841048,0,negative,0.01546854,0.043478731,0.000106981,0.001861696,0.014581418,0.004234711,0.022648821,0.005765585,0.004474691,0.86926686,0.007147815,0.010436785,0.000527366
1105,question_generation1,29,The difference between the two approaches is illustrated in .,Introduction,Introduction,question_generation,1,19,1,0,,0.01675431,0,negative,2.37E-05,0.00486755,8.01E-06,7.38E-06,0.000217851,5.99E-05,2.10E-05,6.30E-05,0.011137632,0.978908078,0.004674772,9.63E-06,1.34E-06
1106,question_generation1,30,This also allows for better optimization as we can backpropagate through the whole network .,Introduction,Introduction,question_generation,1,20,1,0,,0.766151899,1,negative,0.000162074,0.088531016,5.38E-05,2.10E-05,0.001065787,0.000209476,5.51E-05,0.000324856,0.297470179,0.611198931,0.000878647,2.53E-05,3.80E-06
1107,question_generation1,31,We provide detailed empirical evidence to support our hypothesis .,Introduction,Introduction,question_generation,1,21,1,0,,0.00396219,0,negative,1.46E-05,0.006207512,2.94E-06,0.00011478,0.00453125,0.000288015,5.10E-05,0.00023171,0.001946077,0.98537947,0.001220905,7.36E-06,4.40E-06
1108,question_generation1,32,As seen in our method generates natural questions and improves over the state - of the - art techniques for this problem .,Introduction,Introduction,question_generation,1,22,1,0,,0.088972517,0,negative,0.000174324,0.022134973,1.59E-05,4.73E-05,0.002663278,0.000276201,0.004729504,0.000561745,0.002561142,0.926694242,0.036868569,0.003166049,0.000106693
1109,question_generation1,33,Figure,Introduction,,question_generation,1,23,1,0,,0.000474655,0,negative,8.15E-06,0.003785395,3.96E-06,8.50E-06,7.52E-05,0.000193588,6.54E-05,0.000349965,0.058076342,0.931751175,0.005671266,6.16E-06,4.88E-06
1110,question_generation1,34,2 : Here we provide intuition for using implicit embeddings instead of explicit ones .,Introduction,Figure,question_generation,1,24,1,0,,0.004017533,0,negative,1.45E-05,0.037491201,1.91E-05,7.96E-05,0.006421687,0.000162503,7.27E-06,0.000151517,0.005331468,0.94921064,0.001107675,2.03E-06,8.50E-07
1111,question_generation1,35,"As explained in section 1 , the question obtained by the implicit embeddings are natural and holistic than the explicit ones .",Introduction,Figure,question_generation,1,25,1,0,,0.071294366,0,negative,4.66E-05,0.007378837,2.13E-06,3.11E-06,0.001336827,2.87E-05,1.95E-05,7.02E-05,0.000631319,0.988681822,0.001754848,4.58E-05,3.57E-07
1112,question_generation1,36,"To summarize , we propose a multimodal differential network to solve the task of visual question generation .",Introduction,Figure,question_generation,1,26,1,1,approach,0.681293994,1,model,2.44E-05,0.2571697,0.000105089,6.71E-06,0.000897017,2.59E-05,1.46E-05,6.68E-05,0.66189905,0.070240941,0.00954098,6.25E-06,2.49E-06
1113,question_generation1,37,Our contributions are : ( 1 ) A method to incorporate exemplars to learn differential embeddings that captures the subtle differences between supporting and contrasting examples and aid in generating natural questions .,Introduction,Figure,question_generation,1,27,1,0,,0.537021351,1,approach,7.17E-05,0.555293186,0.000249681,0.001061248,0.138529234,0.000344671,0.000100117,0.000261071,0.016764096,0.27741282,0.009879912,1.81E-05,1.42E-05
1114,question_generation1,38,"( 2 ) We provide Multimodal differential embeddings , as image or text alone does not capture the whole context and we show that these embeddings outperform the ablations which incorporate cues such as only image , or tags or place information .",Introduction,Figure,question_generation,1,28,1,0,,0.504763858,1,approach,0.001394198,0.569730004,0.000221169,6.48E-05,0.025137742,0.000218262,0.000251982,0.000464449,0.027842416,0.372591919,0.001615929,0.000457278,9.84E-06
1115,question_generation1,39,( 3 ) We provide a thorough comparison of the proposed network against state - of - the - art benchmarks along with a user study and statistical significance test .,Introduction,Figure,question_generation,1,29,1,0,,0.009583251,0,negative,6.04E-05,0.232821433,2.96E-05,0.000103574,0.104043163,0.000131479,1.82E-05,0.000184696,0.006319456,0.656105357,0.000168944,1.25E-05,1.13E-06
1116,question_generation1,40,Related Work,,,question_generation,1,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
1117,question_generation1,58,Approach,,,question_generation,1,0,1,0,,6.30E-05,0,negative,0.000256653,0.000405673,1.67E-05,0.000511976,2.16E-05,0.00097816,0.00011784,0.00317846,0.000653531,0.990596627,0.003176926,4.36E-05,4.23E-05
1118,question_generation1,59,"Figure 3 : An illustrative example shows the validity of our obtained exemplars with the help of an object classification network , RESNET - 101 .",Approach,Approach,question_generation,1,1,1,0,,0.000983759,0,negative,3.51E-06,0.002035421,8.14E-07,6.97E-06,0.000155154,4.63E-05,2.76E-06,0.000110973,0.00054636,0.991292634,0.005796759,2.10E-06,2.79E-07
1119,question_generation1,60,We see that the probability scores of target and supporting exemplar image are similar .,Approach,Approach,question_generation,1,2,1,0,,0.091414267,0,negative,0.000543838,0.03247854,1.30E-05,1.66E-06,0.00011084,4.08E-05,2.60E-05,0.000696888,0.003608712,0.953564476,0.008483188,0.000431277,8.39E-07
1120,question_generation1,61,That is not the case with the contrasting exemplar .,Approach,Approach,question_generation,1,3,1,0,,0.000667062,0,negative,3.78E-06,0.000910553,3.08E-07,1.55E-06,2.23E-05,1.69E-05,7.08E-07,8.02E-05,0.00028431,0.996092734,0.00258558,1.02E-06,6.28E-08
1121,question_generation1,62,The corresponding generated questions when considering the individual images are also shown .,Approach,Approach,question_generation,1,4,1,0,,0.001632625,0,negative,9.07E-07,0.002209619,2.46E-07,2.40E-07,1.33E-05,2.06E-05,5.61E-07,0.000166783,0.000796299,0.996279942,0.000510663,8.17E-07,2.75E-08
1122,question_generation1,63,"In this section , we clarify the basis for our approach of using exemplars for question generation .",Approach,Approach,question_generation,1,5,1,0,,0.003388879,0,negative,5.06E-06,0.065666593,2.66E-06,3.00E-05,0.000813991,8.58E-05,3.06E-06,0.000397897,0.006446584,0.923309959,0.003236458,1.36E-06,6.21E-07
1123,question_generation1,64,"To use exemplars for our method , we need to ensure that our exemplars can provide context and that our method generates valid exemplars .",Approach,Approach,question_generation,1,6,1,0,,0.016942265,0,negative,6.49E-06,0.020968254,1.12E-06,4.72E-06,0.000171533,2.32E-05,1.81E-06,0.000227837,0.001094411,0.971309343,0.006186972,4.01E-06,2.74E-07
1124,question_generation1,65,We first analyze whether the exemplars are valid or not .,Approach,Approach,question_generation,1,7,1,0,,0.024230858,0,negative,1.80E-05,0.041422474,3.88E-06,1.65E-06,0.000211233,1.72E-05,1.25E-06,0.000177717,0.006733699,0.950783602,0.000624724,4.41E-06,1.25E-07
1125,question_generation1,66,We illustrate this in figure 3 .,Approach,Approach,question_generation,1,8,1,0,,0.000326091,0,negative,2.69E-06,0.003015675,9.33E-07,4.61E-06,9.52E-05,2.95E-05,8.04E-07,6.85E-05,0.001932163,0.994510689,0.000338542,6.23E-07,9.28E-08
1126,question_generation1,67,"We used a pre-trained object classification network on the target , supporting and contrasting images .",Approach,Approach,question_generation,1,9,1,0,,0.069458061,0,approach,5.14E-06,0.387911333,3.70E-05,6.53E-06,0.000466868,0.000832952,1.92E-05,0.008220841,0.240347361,0.361466758,0.000682395,1.72E-06,2.00E-06
1127,question_generation1,68,We observed that the supporting image and target image have quite similar probability scores .,Approach,Approach,question_generation,1,10,1,0,,0.006658201,0,negative,5.14E-05,0.004873835,2.31E-06,6.27E-07,0.000116182,1.70E-05,3.99E-06,0.000129205,0.000502217,0.993696923,0.000559939,4.63E-05,1.07E-07
1128,question_generation1,69,"The contrasting exemplar image , on the other hand , has completely different probability scores .",Approach,Approach,question_generation,1,11,1,0,,0.000973724,0,negative,4.21E-05,0.050528832,8.22E-05,2.69E-07,0.000215134,1.46E-05,6.44E-06,0.000121763,0.021836138,0.926020816,0.001088507,4.29E-05,1.69E-07
1129,question_generation1,70,Exemplars aim to provide appropriate context .,Approach,Approach,question_generation,1,12,1,0,,0.002590244,0,negative,3.69E-06,0.012418358,4.52E-06,1.74E-05,0.001349215,7.67E-05,2.68E-06,0.000176457,0.003977447,0.98155385,0.000417554,1.70E-06,3.89E-07
1130,question_generation1,71,"To better understand the context , we experimented by analysing the questions generated through an exemplar .",Approach,Approach,question_generation,1,13,1,0,,0.007229632,0,negative,4.62E-06,0.031909649,7.27E-06,3.59E-05,0.020061166,0.000105875,6.74E-06,0.000166007,0.001197111,0.946153442,0.00034868,2.92E-06,6.10E-07
1131,question_generation1,72,We observed that indeed a supporting exemplar could identify relevant tags ( cows in ) for generating questions .,Approach,Approach,question_generation,1,14,1,0,,0.004054997,0,negative,2.88E-05,0.00297314,1.59E-06,2.28E-05,0.001734935,3.62E-05,3.18E-06,6.54E-05,0.000226037,0.99425923,0.000641158,7.09E-06,3.82E-07
1132,question_generation1,73,We improve use of exemplars by using a triplet network .,Approach,Approach,question_generation,1,15,1,0,,0.497445913,0,approach,0.000294697,0.623880277,0.000450008,1.81E-05,0.003017627,7.23E-05,2.38E-05,0.000397213,0.197742506,0.173373843,0.00068153,4.46E-05,3.43E-06
1133,question_generation1,74,This network ensures that the joint image - caption embedding for the supporting exemplar are closer to that of the target image - caption and vice - versa .,Approach,Approach,question_generation,1,16,1,0,,0.227229307,0,model,6.66E-06,0.106117337,6.76E-05,2.33E-06,0.000144278,5.28E-05,3.86E-06,0.000502578,0.754740979,0.138196093,0.000163712,9.14E-07,8.11E-07
1134,question_generation1,75,"We empirically evaluated whether an explicit approach that uses the differential set of tags as a one - hot encoding improves the question generation , or the implicit embedding obtained based on the triplet network .",Approach,Approach,question_generation,1,17,1,0,,0.009532894,0,negative,1.80E-05,0.218238491,5.04E-06,4.07E-06,0.000634384,0.00012785,9.13E-06,0.001921289,0.012734411,0.765572819,0.000717856,1.59E-05,7.59E-07
1135,question_generation1,76,We observed that the implicit multimodal differential network empirically provided better context for generating questions .,Approach,Approach,question_generation,1,18,1,0,,0.013375688,0,negative,0.000124848,0.010006374,1.93E-06,1.15E-05,0.00070779,8.21E-05,1.60E-05,0.000523286,0.000860786,0.986766419,0.000807497,9.02E-05,1.17E-06
1136,question_generation1,77,Our understanding of this phenomenon is that both target and supporting exemplars generate similar questions whereas contrasting exemplars generate very different questions from the target question .,Approach,Approach,question_generation,1,19,1,0,,0.001325723,0,negative,1.15E-06,0.010064821,1.13E-06,7.68E-06,0.000143894,4.28E-05,2.25E-06,0.000220103,0.002576102,0.98196452,0.004974198,9.47E-07,3.92E-07
1137,question_generation1,78,The triplet network that enhances the joint embedding thus aids to improve the generation of target question .,Approach,Approach,question_generation,1,20,1,0,,0.13664493,0,negative,0.002108953,0.062481585,6.75E-05,5.09E-05,0.004728295,0.000154675,5.27E-05,0.000677965,0.018170489,0.910537072,0.000524478,0.000440556,4.83E-06
1138,question_generation1,79,These are observed to be better than the explicitly obtained context tags as can be seen in .,Approach,Approach,question_generation,1,21,1,0,,0.102632199,0,negative,0.002612655,0.030347111,2.43E-05,0.000111174,0.005610322,0.000545789,0.000545818,0.003634533,0.000864476,0.950364663,0.000681095,0.004635059,2.30E-05
1139,question_generation1,80,We now explain our method in detail .,Approach,Approach,question_generation,1,22,1,0,,0.000155099,0,negative,2.30E-06,0.033752801,2.38E-06,2.51E-05,0.000570992,0.000161824,4.14E-06,0.000970088,0.010187664,0.954142548,0.0001778,1.48E-06,9.11E-07
1140,question_generation1,81,Method,,,question_generation,1,0,1,0,,0.000211124,0,negative,4.12E-05,0.000349831,8.47E-06,1.11E-05,3.83E-06,0.0002083,4.56E-05,0.00372844,0.000354333,0.992353364,0.00284456,4.13E-05,9.63E-06
1141,question_generation1,82,"The task in visual question generation ( VQG ) is to generate a natural language questionQ , for an image I .",Method,Method,question_generation,1,1,1,0,,0.077688412,0,research-problem,4.32E-06,0.000591385,1.69E-06,2.60E-05,1.53E-05,3.59E-05,3.10E-05,0.000164943,4.31E-05,0.242154411,0.756914461,9.77E-06,7.63E-06
1142,question_generation1,83,We consider a set of pre-generated context C from image I .,Method,Method,question_generation,1,2,1,0,,0.001164516,0,negative,3.14E-05,0.027278965,3.34E-05,2.66E-06,0.00014356,0.000179506,6.79E-06,0.003861618,0.003502438,0.964746446,0.000183464,2.87E-05,1.00E-06
1143,question_generation1,84,We maximize the conditional probability of generated question given image and context as follows :,Method,Method,question_generation,1,3,1,0,,0.001265433,0,negative,1.91E-05,0.011309881,3.38E-05,7.80E-07,1.01E-05,0.000134806,2.47E-06,0.003035251,0.007664461,0.977479805,0.000299933,9.06E-06,4.70E-07
1144,question_generation1,85,where ?,Method,Method,question_generation,1,4,1,0,,2.62E-05,0,negative,2.06E-06,0.000195426,4.86E-07,9.21E-07,1.60E-06,5.08E-05,7.81E-07,0.000347039,0.000114075,0.999118018,0.000167199,1.46E-06,1.29E-07
1145,question_generation1,86,is a vector for all possible parameters of our model .,Method,Method,question_generation,1,5,1,0,,0.003660843,0,negative,1.43E-05,0.007719579,4.91E-06,2.56E-06,2.26E-05,0.000485526,5.25E-06,0.011003628,0.002858835,0.977694584,0.000176691,1.06E-05,9.82E-07
1146,question_generation1,87,Q is the ground truth question .,Method,Method,question_generation,1,6,1,0,,0.000240055,0,negative,2.95E-06,0.000341185,8.03E-07,6.32E-07,4.51E-06,2.77E-05,1.26E-06,0.00034111,8.93E-05,0.998825418,0.000361347,3.62E-06,2.06E-07
1147,question_generation1,88,"The log probability for the question is calculated by using joint probability over {q 0 , q 1 , ..... , q N } with the help of chain rule .",Method,Method,question_generation,1,7,1,0,,0.00118468,0,negative,8.92E-06,0.007437158,3.08E-05,3.40E-07,7.82E-06,0.00017507,2.99E-06,0.002675709,0.01072067,0.978789039,0.00014736,3.75E-06,3.95E-07
1148,question_generation1,89,"For a particular question , the above term is obtained as :",Method,Method,question_generation,1,8,1,0,,0.000303346,0,negative,2.08E-05,0.001190912,5.44E-06,1.24E-06,1.29E-05,5.38E-05,1.73E-06,0.000518377,0.000683458,0.997429012,7.07E-05,1.14E-05,2.56E-07
1149,question_generation1,90,"where N is length of the sequence , and qt is the t th word of the question .",Method,Method,question_generation,1,9,1,0,,5.89E-05,0,negative,4.28E-06,0.000555584,1.25E-06,5.05E-07,6.03E-06,4.45E-05,1.33E-06,0.000663375,0.000212838,0.998441197,6.48E-05,4.08E-06,1.68E-07
1150,question_generation1,91,We have removed ? for simplicity .,Method,Method,question_generation,1,10,1,0,,0.000255148,0,negative,8.26E-06,0.000481098,8.75E-07,1.74E-05,4.69E-05,0.00014113,1.59E-06,0.000480888,0.000133225,0.998656847,2.92E-05,2.21E-06,3.37E-07
1151,question_generation1,92,Our method is based on a sequence to sequence network .,Method,Method,question_generation,1,11,1,0,,0.040323724,0,negative,0.000100932,0.370073454,0.001104745,8.98E-06,0.000308513,0.000292027,2.70E-05,0.004005281,0.108209626,0.513986016,0.001803151,7.04E-05,9.91E-06
1152,question_generation1,93,The sequence to sequence network has a text sequence as input and output .,Method,Method,question_generation,1,12,1,0,,0.106290296,0,negative,6.73E-05,0.061572342,0.002389193,4.64E-06,0.000100962,0.000489797,2.95E-05,0.004834472,0.281819706,0.648009934,0.000639296,3.16E-05,1.12E-05
1153,question_generation1,94,"In our method , we take an image as input and generate a natural question as output .",Method,Method,question_generation,1,13,1,0,,0.016159361,0,negative,3.63E-05,0.140853562,0.000241841,4.93E-06,0.000462229,0.000119721,1.57E-05,0.002214046,0.018139987,0.836486911,0.001348928,7.16E-05,4.26E-06
1154,question_generation1,95,The architecture for our model is shown in .,Method,Method,question_generation,1,14,1,0,,0.019250934,0,negative,5.73E-05,0.024101276,7.58E-05,9.49E-05,0.000308551,0.000961326,4.88E-05,0.006400772,0.042439414,0.923834382,0.001583847,5.16E-05,4.20E-05
1155,question_generation1,96,"Our model contains three main modules , ( a ) Representation Module that extracts multimodal features ( b ) Mixture Module that fuses the multimodal representation and ( c ) Decoder that generates question using an LSTM - based language model .",Method,Method,question_generation,1,15,1,0,,0.249055208,0,negative,0.000213,0.211337577,0.000698866,7.37E-05,0.00087884,0.000756605,6.50E-05,0.00679544,0.333338687,0.4448487,0.000873407,6.22E-05,5.80E-05
1156,question_generation1,97,"During inference , we sample a question word q i from the softmax distribution and continue sampling until the end token or maximum length for the question is reached .",Method,Method,question_generation,1,16,1,0,,0.002674737,0,negative,2.64E-05,0.022293674,2.53E-05,2.89E-06,5.03E-05,0.000395751,8.57E-06,0.010211056,0.010507626,0.956422062,4.45E-05,9.98E-06,1.88E-06
1157,question_generation1,98,We experimented with both sampling and argmax and found out that argmax works better .,Method,Method,question_generation,1,17,1,0,,0.079080818,0,negative,4.99E-05,0.001730481,3.57E-06,1.31E-05,9.47E-05,0.00273438,5.94E-05,0.015098751,0.00011671,0.979915621,4.78E-05,0.000131079,4.47E-06
1158,question_generation1,99,This result is provided in the supplementary material .,Method,Method,question_generation,1,18,1,0,,0.00044606,0,negative,1.37E-05,0.000217912,8.13E-07,1.79E-06,2.57E-05,5.32E-05,2.83E-06,0.000385513,7.18E-05,0.999187459,9.28E-06,2.98E-05,2.47E-07
1159,question_generation1,100,Multimodal Differential,Method,,question_generation,1,19,1,0,,0.012311288,0,negative,0.000482997,0.028737497,0.002266201,2.00E-05,0.000336315,0.00043239,0.000224848,0.002279955,0.026762606,0.928540908,0.008079643,0.001789936,4.67E-05
1160,question_generation1,101,Network,Method,,question_generation,1,20,1,0,,0.000709144,0,negative,0.000190247,0.035538947,0.000408708,6.17E-05,0.000630423,0.000805123,0.000180535,0.00665117,0.054003786,0.898132497,0.002598086,0.000708108,9.07E-05
1161,question_generation1,102,The proposed Multimodal Differential Network ( MDN ) consists of a representation module and a joint mixture module .,Method,Network,question_generation,1,21,1,0,,0.002265269,0,negative,0.000139161,0.00134083,0.001869023,3.42E-07,3.57E-06,1.83E-05,6.84E-06,0.000105701,0.018212227,0.978077498,4.87E-05,0.000170284,7.48E-06
1162,question_generation1,103,Finding Exemplars,Method,,question_generation,1,22,1,0,,0.000500735,0,negative,2.52E-05,0.003496152,7.28E-06,4.03E-06,2.89E-05,0.000128849,0.000173784,0.002079729,0.000577408,0.947522734,0.045043083,0.000893985,1.88E-05
1163,question_generation1,104,We used an efficient KNN - based approach ( k -d tree ) with Euclidean metric to obtain the exemplars .,Method,Finding Exemplars,question_generation,1,23,1,0,,0.000333241,0,negative,9.96E-05,0.007936205,0.000395613,1.46E-06,0.00015084,0.000449097,0.000134204,0.00050085,0.008318877,0.981870785,1.98E-05,0.000120083,2.53E-06
1164,question_generation1,105,"This is obtained through a coarse quantization of nearest neighbors of the training examples into 50 clusters , and selecting the nearest as supporting and farthest as the contrasting exemplars .",Method,Finding Exemplars,question_generation,1,24,1,0,,0.000522093,0,negative,4.87E-05,0.001578283,8.58E-06,5.17E-06,0.000128557,0.005407774,0.000328517,0.012658856,0.000733612,0.97906023,2.25E-06,3.56E-05,3.94E-06
1165,question_generation1,106,We experimented with ITML based metric learning for image features .,Method,Finding Exemplars,question_generation,1,25,1,0,,0.000199259,0,negative,4.92E-05,0.00328984,0.000263394,2.64E-06,0.000459537,0.000322586,0.000258169,0.000234517,0.000442162,0.994300524,3.28E-05,0.000341131,3.47E-06
1166,question_generation1,107,"Surprisingly , the KNN - based approach outperforms the latter one .",Method,Finding Exemplars,question_generation,1,26,1,0,,0.120190338,0,negative,0.005612123,8.47E-05,1.24E-05,1.88E-05,0.000116856,0.001097633,0.005189391,0.001001837,1.72E-05,0.909319199,2.30E-05,0.077472468,3.45E-05
1167,question_generation1,108,We also tried random exemplars and different number of exemplars and found that k = 5 works best .,Method,Finding Exemplars,question_generation,1,27,1,0,,0.012173011,0,negative,0.000103182,0.000184599,3.18E-06,2.08E-06,2.14E-05,0.004903691,0.000672812,0.007033446,8.27E-05,0.986605232,5.69E-06,0.000378077,3.84E-06
1168,question_generation1,109,We provide these results in the supplementary material .,Method,Finding Exemplars,question_generation,1,28,1,0,,1.69E-06,0,negative,2.10E-06,2.59E-06,1.56E-07,1.24E-06,1.66E-05,2.72E-05,1.23E-06,9.88E-06,2.52E-06,0.999933657,8.13E-08,2.72E-06,2.48E-08
1169,question_generation1,110,Representation Module,Method,,question_generation,1,29,1,0,,0.002861984,0,negative,4.79E-05,0.005825413,0.000111436,6.03E-06,9.76E-05,0.000273257,6.51E-05,0.002592922,0.021554228,0.96864272,0.000513743,0.000250055,1.95E-05
1170,question_generation1,111,We use a triplet network in our representation module .,Method,Representation Module,question_generation,1,30,1,0,,0.000210147,0,negative,6.49E-05,0.000291013,0.000290958,2.92E-07,1.69E-06,1.39E-05,4.22E-06,0.000360525,0.005979581,0.992958484,1.11E-06,3.07E-05,2.66E-06
1171,question_generation1,112,"We refereed a similar kind of work done in ( Patro and Namboodiri , 2018 ) for building our triplet network .",Method,Representation Module,question_generation,1,31,1,0,,1.13E-06,0,negative,2.78E-06,6.77E-06,1.16E-06,9.98E-08,1.41E-07,3.26E-06,5.23E-07,4.88E-05,1.63E-05,0.999908755,1.85E-06,9.44E-06,1.86E-07
1172,question_generation1,113,"The triplet network consists of three sub-parts : target , supporting , and contrasting networks .",Method,Representation Module,question_generation,1,32,1,0,,0.0001179,0,negative,2.76E-05,0.000167025,0.000138021,9.61E-08,1.14E-06,3.34E-06,1.46E-06,7.14E-05,0.003733011,0.995828701,1.31E-06,2.56E-05,1.20E-06
1173,question_generation1,114,All three networks share the same parameters .,Method,Representation Module,question_generation,1,33,1,0,,6.07E-07,0,negative,7.15E-06,3.02E-05,4.53E-06,2.09E-08,2.34E-07,1.40E-06,5.28E-07,6.05E-05,0.000124013,0.999759107,1.01E-07,1.21E-05,8.18E-08
1174,question_generation1,115,"Given an image xi we obtain an embedding g i using a CNN parameterized by a function G ( x i , W c ) where W care the weights for the CNN .",Method,Representation Module,question_generation,1,34,1,0,,6.60E-07,0,negative,1.80E-06,1.57E-05,1.97E-06,1.45E-08,1.16E-07,8.08E-07,1.84E-07,2.74E-05,8.79E-05,0.999859007,2.67E-07,4.69E-06,9.19E-08
1175,question_generation1,116,"The caption Ci results in a caption embedding f i through an LSTM parameterized by a function F ( C i , W l ) where W l are the weights for the LSTM .",Method,Representation Module,question_generation,1,35,1,0,,1.65E-06,0,negative,3.37E-06,1.73E-05,6.91E-06,6.71E-09,1.20E-07,4.62E-07,1.97E-07,1.30E-05,0.000171533,0.999775848,2.28E-07,1.09E-05,9.97E-08
1176,question_generation1,117,This is shown in part 1 of,Method,Representation Module,question_generation,1,36,1,0,,1.01E-07,0,negative,4.91E-06,8.90E-07,1.00E-06,1.26E-08,1.07E-07,4.04E-07,1.02E-07,4.05E-06,5.39E-06,0.999973388,4.05E-08,9.69E-06,2.64E-08
1177,question_generation1,118,Mixture Module,Method,,question_generation,1,37,1,0,,0.135784216,0,negative,0.000213697,0.022435077,0.001108529,3.23E-05,0.00049288,0.000978809,0.000228915,0.007230929,0.117112682,0.849637514,0.00013901,0.000279407,0.000110293
1178,question_generation1,119,The Mixture module brings the image and caption embeddings to a joint feature embedding space .,Method,Mixture Module,question_generation,1,38,1,0,,0.000448058,0,negative,9.38E-05,0.000160817,0.000125405,2.78E-07,1.28E-06,5.73E-06,2.40E-06,0.000136589,0.005596678,0.993853809,3.60E-07,2.04E-05,2.52E-06
1179,question_generation1,120,The input to the module is the embeddings obtained from the representation module .,Method,Mixture Module,question_generation,1,39,1,0,,2.80E-05,0,negative,2.30E-06,1.40E-05,4.15E-06,1.42E-08,9.23E-08,1.47E-06,3.53E-07,5.94E-05,0.000423134,0.999492499,8.48E-08,2.40E-06,1.75E-07
1180,question_generation1,121,"We have evaluated four different approaches for fusion viz. , joint , element - wise addition , hadamard and attention method .",Method,Mixture Module,question_generation,1,40,1,0,,0.005348559,0,negative,0.000278441,0.000177958,0.000534877,1.52E-07,5.32E-06,5.69E-06,2.17E-05,3.72E-05,0.000121066,0.996653472,2.89E-06,0.002158343,2.84E-06
1181,question_generation1,122,"Each of these variants receives image features g i & the caption embedding f i , and outputs a fixed dimensional feature vector s i .",Method,Mixture Module,question_generation,1,41,1,0,,5.02E-06,0,negative,2.75E-05,7.78E-06,4.08E-05,1.65E-08,1.95E-07,4.13E-07,3.02E-07,5.06E-06,7.98E-05,0.999815658,3.72E-08,2.24E-05,8.17E-08
1182,question_generation1,123,The Joint method concatenates g i & f i and maps them to a fixed length feature vector s i as follows :,Method,Mixture Module,question_generation,1,42,1,0,,1.07E-06,0,negative,6.70E-05,2.17E-05,0.000275058,2.51E-08,4.64E-07,7.58E-07,7.83E-07,7.25E-06,0.000205528,0.999371673,1.02E-07,4.94E-05,2.47E-07
1183,question_generation1,124,where g i is the 4096 - dimensional convolutional feature from the FC7 layer of pretrained is to obtain context vectors that bring the supporting exemplar embeddings closer to the target embedding and vice - versa .,Method,Mixture Module,question_generation,1,43,1,0,,2.74E-07,0,negative,1.12E-05,2.42E-06,1.80E-06,1.50E-08,1.96E-07,8.81E-07,2.39E-07,1.61E-05,8.29E-06,0.99994488,9.78E-09,1.38E-05,5.58E-08
1184,question_generation1,125,This is obtained as follows :,Method,Mixture Module,question_generation,1,44,1,0,,2.89E-07,0,negative,3.16E-06,4.25E-07,1.58E-06,7.63E-10,1.18E-08,6.45E-08,3.71E-08,7.75E-07,8.79E-06,0.999978276,7.36E-09,6.87E-06,7.75E-09
1185,question_generation1,126,"where D ( t ( s i ) , t ( s j ) ) = ||t ( s i ) ? t ( s j ) || 2 2 is the euclidean distance between two embeddings t( s i ) and t(s j ) .",Method,Mixture Module,question_generation,1,45,1,0,,2.87E-07,0,negative,7.18E-07,1.43E-06,4.44E-07,4.69E-09,3.33E-08,5.91E-07,1.67E-07,1.48E-05,6.84E-06,0.999971758,3.40E-08,3.14E-06,4.98E-08
1186,question_generation1,127,M is the training dataset that contains all set of possible triplets .,Method,Mixture Module,question_generation,1,46,1,0,,4.48E-07,0,negative,1.18E-06,1.54E-06,4.86E-07,3.87E-09,8.38E-08,3.86E-07,1.36E-07,1.56E-05,5.31E-06,0.99996981,6.67E-09,5.44E-06,3.39E-08
1187,question_generation1,128,"T ( s i , s + i , s ? i ) is the triplet loss function .",Method,Mixture Module,question_generation,1,47,1,0,,4.28E-07,0,negative,1.32E-06,3.90E-06,1.15E-06,5.53E-09,4.58E-08,7.49E-07,2.12E-07,3.93E-05,4.36E-05,0.999907716,1.63E-08,1.95E-06,7.34E-08
1188,question_generation1,129,"This is decomposed into two terms , one that brings the supporting sample closer and one that pushes the contrasting sample further .",Method,Mixture Module,question_generation,1,48,1,0,,8.24E-07,0,negative,3.24E-05,1.18E-05,1.58E-05,5.44E-09,1.28E-07,2.63E-07,2.01E-07,6.05E-06,0.000486164,0.99943618,1.61E-08,1.09E-05,7.98E-08
1189,question_generation1,130,This is given by,Method,Mixture Module,question_generation,1,49,1,0,,1.02E-07,0,negative,9.47E-07,2.14E-07,2.74E-07,2.37E-09,1.76E-08,1.63E-07,4.75E-08,2.10E-06,4.18E-06,0.999989475,4.87E-09,2.56E-06,1.47E-08
1190,question_generation1,131,"Here D + , D ? represent the euclidean distance between the target and supporting sample , and target and opposing sample respectively .",Method,Mixture Module,question_generation,1,50,1,0,,1.45E-07,0,negative,8.89E-07,1.17E-06,3.64E-07,1.02E-09,1.46E-08,1.94E-07,5.33E-08,6.99E-06,1.41E-05,0.999974604,4.64E-09,1.58E-06,1.21E-08
1191,question_generation1,132,The parameter ?(=,Method,Mixture Module,question_generation,1,51,1,0,,8.67E-07,0,negative,6.83E-06,3.87E-06,5.22E-07,2.77E-08,1.43E-07,4.96E-06,1.20E-06,0.000231258,1.49E-05,0.999722459,1.93E-08,1.36E-05,2.46E-07
1192,question_generation1,133,0.2 ) controls the separation margin between these and is obtained through validation data .,Method,Mixture Module,question_generation,1,52,1,0,,5.59E-06,0,negative,5.53E-05,2.37E-05,3.84E-06,1.19E-07,6.84E-07,1.04E-05,3.73E-06,0.000729194,8.27E-05,0.999048527,2.53E-08,4.08E-05,1.11E-06
1193,question_generation1,134,Decoder : Question Generator,Method,,question_generation,1,53,1,0,,0.485813223,0,negative,7.63E-05,0.029237204,0.001624643,9.96E-06,0.000571153,0.000484759,0.000237334,0.00477533,0.093740978,0.868906679,2.48E-05,0.0001981,0.000112763
1194,question_generation1,135,"The role of decoder is to predict the probability for a question , given s i .",Method,Decoder : Question Generator,question_generation,1,54,1,0,,1.63E-06,0,negative,7.52E-06,1.64E-05,3.24E-05,2.26E-08,7.21E-07,1.69E-06,5.06E-07,1.03E-05,0.000610028,0.999316426,3.18E-08,3.04E-06,8.53E-07
1195,question_generation1,136,RNN provides a nice way to perform conditioning on previous state value using a fixed length hidden vector .,Method,Decoder : Question Generator,question_generation,1,55,1,0,,0.000426702,0,negative,3.60E-05,2.12E-05,0.000363454,1.43E-07,2.62E-06,1.70E-05,1.90E-05,3.53E-05,0.000345568,0.999040202,1.05E-06,9.86E-05,1.98E-05
1196,question_generation1,137,The conditional probability of a question token at particular time step qt is modeled using an LSTM as used in machine translation .,Method,Decoder : Question Generator,question_generation,1,56,1,0,,1.93E-06,0,negative,4.32E-06,1.78E-05,3.48E-05,1.93E-08,4.34E-07,1.14E-05,4.30E-06,0.000120042,0.001306007,0.998496298,3.39E-08,2.23E-06,2.34E-06
1197,question_generation1,138,"At time step t , the conditional probability is denoted by",Method,Decoder : Question Generator,question_generation,1,57,1,0,,1.17E-07,0,negative,1.02E-06,8.15E-07,1.29E-06,8.40E-10,4.11E-08,4.55E-07,1.66E-07,3.54E-06,2.13E-05,0.999969874,2.30E-09,1.46E-06,3.67E-08
1198,question_generation1,139,"where ht is the hidden state of the LSTM cell at time step t , which is conditioned on all the previously generated words {q 0 , q 1 , ...q N ?1 }.",Method,Decoder : Question Generator,question_generation,1,58,1,0,,1.58E-07,0,negative,1.58E-06,1.05E-06,1.92E-06,3.66E-09,1.47E-07,9.29E-07,3.54E-07,5.74E-06,1.28E-05,0.999973249,3.98E-09,2.12E-06,1.16E-07
1199,question_generation1,140,The word with maximum probability in the probability distribution of the LSTM cell at step k is fed as an input to the LSTM cell at step k + 1 as shown in part 3 of Figure,Method,Decoder : Question Generator,question_generation,1,59,1,0,,4.59E-07,0,negative,6.19E-06,1.05E-06,1.37E-05,7.46E-10,8.88E-08,3.85E-07,4.30E-07,1.56E-06,2.76E-05,0.999941289,1.24E-09,7.70E-06,6.56E-08
1200,question_generation1,141,4 .,Method,Decoder : Question Generator,question_generation,1,60,1,0,,2.89E-06,0,negative,2.32E-06,4.17E-07,4.90E-07,5.88E-09,7.74E-08,3.65E-06,7.00E-07,2.22E-05,6.12E-06,0.999961971,1.72E-09,1.84E-06,2.20E-07
1201,question_generation1,142,"At t = ? 1 , we are feeding the output of the mixture module to LSTM.Q = {q 0 , q 1 , ...q N ?1 } are the predicted question tokens for the input image I .",Method,Decoder : Question Generator,question_generation,1,61,1,0,,4.66E-07,0,negative,9.85E-07,1.20E-06,7.92E-07,1.52E-09,7.27E-08,3.02E-06,1.12E-06,3.97E-05,1.41E-05,0.99993715,1.73E-09,1.68E-06,1.59E-07
1202,question_generation1,143,"Here , we are usingq 0 andq N ? 1 as the special token START and STOP respectively .",Method,Decoder : Question Generator,question_generation,1,62,1,0,,6.27E-07,0,negative,9.99E-07,1.83E-06,9.58E-07,4.49E-09,1.19E-07,5.99E-06,9.46E-07,7.34E-05,2.55E-05,0.999889124,8.98E-10,1.00E-06,1.72E-07
1203,question_generation1,144,The softmax probability for the predicted question token at different time steps is given by the following equations where LSTM refers to the standard LSTM cell equations :,Method,Decoder : Question Generator,question_generation,1,63,1,0,,4.50E-07,0,negative,1.46E-06,1.34E-06,2.73E-06,1.63E-09,6.47E-08,1.28E-06,5.04E-07,1.17E-05,7.97E-05,0.999899675,2.46E-09,1.42E-06,1.73E-07
1204,question_generation1,145,Where ?,Method,Decoder : Question Generator,question_generation,1,64,1,0,,1.22E-07,0,negative,2.77E-07,1.77E-07,2.09E-07,4.78E-09,3.32E-08,1.73E-06,2.18E-07,9.39E-06,3.06E-06,0.999984289,1.93E-09,5.25E-07,8.89E-08
1205,question_generation1,146,t+ 1 is the probability distribution over all question tokens .,Method,Decoder : Question Generator,question_generation,1,65,1,0,,1.13E-07,0,negative,7.32E-07,5.68E-07,8.94E-07,4.43E-10,3.15E-08,5.23E-07,2.32E-07,5.79E-06,1.14E-05,0.999978902,4.19E-10,8.88E-07,4.40E-08
1206,question_generation1,147,loss is cross entropy loss .,Method,Decoder : Question Generator,question_generation,1,66,1,0,,0.000203381,0,negative,8.91E-05,3.99E-05,0.000167546,4.92E-07,1.20E-05,0.000216937,0.000222491,0.001976125,0.000117184,0.996885036,2.33E-08,0.000145632,0.000127548
1207,question_generation1,148,Cost function,Method,,question_generation,1,67,1,0,,0.00025173,0,negative,1.14E-05,0.001039013,7.95E-06,1.43E-06,4.16E-05,0.000208156,2.87E-05,0.00562919,0.001630768,0.991360671,8.40E-07,3.31E-05,7.31E-06
1208,question_generation1,149,"Our objective is to minimize the total loss , that is the sum of cross entropy loss and triplet loss over all training examples .",Method,Cost function,question_generation,1,68,1,0,,0.000124141,0,negative,7.91E-06,0.000357556,1.02E-05,6.23E-08,2.72E-06,1.35E-05,2.87E-06,0.002378752,0.000253138,0.996923655,6.22E-08,4.67E-05,2.90E-06
1209,question_generation1,150,The total loss is :,Method,Cost function,question_generation,1,69,1,0,,0.000270878,0,negative,6.24E-05,1.01E-05,1.38E-05,8.36E-08,2.13E-06,1.66E-05,4.65E-06,0.000470137,3.29E-05,0.999250028,6.72E-09,0.000133988,3.20E-06
1210,question_generation1,151,"where M is the total number of samples , ?",Method,Cost function,question_generation,1,70,1,0,,2.87E-07,0,negative,1.79E-06,2.13E-06,6.08E-07,4.47E-09,2.07E-07,2.79E-06,4.51E-07,0.000147236,2.30E-06,0.999829964,2.05E-09,1.24E-05,1.28E-07
1211,question_generation1,152,"is a constant , which controls both the loss .",Method,Cost function,question_generation,1,71,1,0,,6.15E-05,0,negative,1.77E-05,2.58E-05,4.81E-06,1.47E-07,2.28E-06,3.38E-05,5.34E-06,0.003812674,4.08E-05,0.996013969,7.62E-09,3.96E-05,3.10E-06
1212,question_generation1,153,L triplet is the triplet loss function 5 .,Method,Cost function,question_generation,1,72,1,0,,2.57E-05,0,negative,5.70E-06,2.37E-05,1.10E-05,1.35E-07,2.21E-06,4.18E-05,5.08E-06,0.004982917,4.30E-05,0.994863241,5.38E-09,1.65E-05,4.68E-06
1213,question_generation1,154,L cross is the cross entropy loss between the predicted and ground truth questions and is given by :,Method,Cost function,question_generation,1,73,1,0,,2.63E-06,0,negative,4.33E-06,4.51E-06,5.87E-06,3.26E-08,1.25E-06,1.09E-05,1.51E-06,0.000404464,4.32E-06,0.999542137,1.59E-09,2.00E-05,7.38E-07
1214,question_generation1,155,"where , N is the total number of question tokens , y t is the ground truth label .",Method,Cost function,question_generation,1,74,1,0,,5.30E-07,0,negative,7.76E-07,1.48E-06,3.83E-07,6.30E-09,1.72E-07,2.97E-06,2.99E-07,0.000247754,2.40E-06,0.999738715,8.37E-10,4.89E-06,1.57E-07
1215,question_generation1,156,The code for MDN - VQG model is provided 1 .,Method,Cost function,question_generation,1,75,1,0,,0.00017684,0,negative,2.80E-06,4.95E-06,3.44E-06,4.93E-07,5.85E-06,9.25E-05,7.62E-06,0.001590918,4.95E-06,0.998250835,4.83E-09,3.13E-05,4.28E-06
1216,question_generation1,157,Variations of Proposed Method,,,question_generation,1,0,1,0,,0.012771852,0,negative,0.000314117,0.000298925,4.16E-05,2.25E-05,6.38E-06,0.000163588,0.000128858,0.001154585,0.000358088,0.990129953,0.006818495,0.000538502,2.44E-05
1217,question_generation1,158,"While , we advocate the use of multimodal differential network for generating embeddings that can be used by the decoder for generating questions , we also evaluate several variants of this architecture .",Variations of Proposed Method,Variations of Proposed Method,question_generation,1,1,1,0,,0.009575576,0,negative,0.001367425,0.009702321,0.000550009,1.47E-06,4.20E-05,0.000203611,3.20E-05,0.000512152,0.000779792,0.986391774,0.000127768,0.000288856,8.12E-07
1218,question_generation1,159,These are as follows :,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,2,1,0,,1.35E-05,0,negative,7.11E-05,2.93E-05,6.19E-05,4.21E-08,1.51E-06,6.48E-06,6.28E-07,6.60E-06,2.20E-05,0.999781713,7.26E-06,1.14E-05,2.07E-08
1219,question_generation1,160,Tag Net :,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,3,1,0,,0.001620039,0,negative,0.000501567,0.000600235,0.000103438,1.70E-05,3.96E-05,0.00020338,1.32E-05,0.000160348,0.000630066,0.996543995,0.001146191,3.60E-05,4.92E-06
1220,question_generation1,161,"In this variant , we consider extracting the part - of - speech ( POS ) tags for the words present in the caption and obtaining a Tag embedding by considering different methods of combining the one - hot vectors .",Variations of Proposed Method,Variations of Proposed Method,question_generation,1,4,1,0,,0.004534582,0,negative,0.001508698,0.04208378,0.002693957,4.94E-06,0.000106588,0.000130757,2.32E-05,0.000362513,0.0090345,0.942656027,0.001137087,0.000254534,3.47E-06
1221,question_generation1,162,Further details and experimental results are present in the supplementary .,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,5,1,0,,4.22E-06,0,negative,1.33E-05,1.02E-05,7.84E-07,4.22E-06,1.46E-05,4.36E-05,8.34E-07,1.91E-05,4.44E-06,0.999885221,2.03E-06,1.57E-06,4.08E-08
1222,question_generation1,163,This Tag embedding is then combined with the image embedding and provided to the decoder network .,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,6,1,0,,0.001951128,0,negative,0.000636633,0.007409794,0.000986308,5.62E-07,1.80E-05,5.35E-05,5.84E-06,0.000213051,0.025803992,0.9647385,8.42E-05,4.87E-05,9.57E-07
1223,question_generation1,164,Place Net :,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,7,1,0,,0.109359449,0,negative,0.001133069,0.003860434,0.270230365,7.61E-07,1.14E-05,0.000198112,0.000112076,0.000202163,0.024464673,0.696605846,0.002593695,0.000576943,1.05E-05
1224,question_generation1,165,In this variant we explore obtaining embeddings based on the visual scene understanding .,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,8,1,0,,0.023341637,0,negative,0.001696805,0.050106143,0.005764553,9.53E-06,0.000221068,0.000219331,4.90E-05,0.00051938,0.009638629,0.930135309,0.00126925,0.00036244,8.61E-06
1225,question_generation1,166,This is obtained using a pre-trained that is trained to classify 365 different types of scene categories .,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,9,1,0,,0.000205957,0,negative,3.78E-05,0.001357951,2.48E-05,1.22E-06,0.000141567,0.000281746,9.53E-06,0.000896557,0.000269888,0.996961695,5.43E-06,1.13E-05,5.35E-07
1226,question_generation1,167,We then combine the activation map for the input image and the VGG - 19 based place embedding to obtain the joint embedding used by the decoder .,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,10,1,0,,0.001553458,0,negative,0.000249454,0.008658756,0.000729628,6.26E-07,1.48E-05,0.00013838,9.52E-06,0.000534549,0.035808175,0.953753859,7.98E-05,2.12E-05,1.27E-06
1227,question_generation1,168,"Differential Image Network : Instead of using multimodal differential network for generating embeddings , we also evaluate differential image network for the same .",Variations of Proposed Method,Variations of Proposed Method,question_generation,1,11,1,0,,0.10248309,0,negative,0.001136722,0.011713268,0.065505654,8.99E-07,4.38E-05,0.000219365,0.000179254,0.000462906,0.003805408,0.915319798,0.000551499,0.001056731,4.74E-06
1228,question_generation1,169,"In this case , the embedding does not include the caption but is based only on the image feature .",Variations of Proposed Method,Variations of Proposed Method,question_generation,1,12,1,0,,0.000112056,0,negative,0.000138575,0.000395545,2.06E-05,2.62E-07,6.00E-06,2.86E-05,1.34E-06,0.00010407,0.00024145,0.9990325,8.97E-06,2.19E-05,1.14E-07
1229,question_generation1,170,We also experimented with using multiple exemplars and random exemplars .,Variations of Proposed Method,Variations of Proposed Method,question_generation,1,13,1,0,,0.003258204,0,negative,0.000225544,0.00034791,6.94E-05,1.32E-06,9.19E-05,0.00014692,1.04E-05,0.000104556,6.01E-05,0.998877,5.40E-06,5.92E-05,3.42E-07
1230,question_generation1,171,"Further details , pseudocode and results regarding these are present in the supplementary material .",Variations of Proposed Method,Variations of Proposed Method,question_generation,1,14,1,0,,6.02E-06,0,negative,1.53E-05,3.03E-05,1.19E-06,0.000102482,0.000135887,0.000239914,1.69E-06,8.72E-05,1.10E-05,0.999371289,2.00E-06,1.49E-06,2.52E-07
1231,question_generation1,172,Dataset,Variations of Proposed Method,,question_generation,1,15,1,0,,1.07E-05,0,negative,1.59E-05,2.34E-05,2.17E-06,3.24E-06,8.93E-06,0.000150775,6.61E-06,0.000130736,2.98E-05,0.999570084,4.54E-05,1.20E-05,8.96E-07
1232,question_generation1,173,"We conduct our experiments on Visual Question Generation ( VQG ) dataset , which contains human annotated questions based on images of MS - COCO dataset .",Variations of Proposed Method,Dataset,question_generation,1,16,1,0,,0.112010099,0,negative,0.000424053,0.075594576,0.000873026,2.92E-05,0.018596475,0.00031097,0.000411455,0.000808758,0.000622805,0.899078242,0.001079755,0.002153603,1.71E-05
1233,question_generation1,174,This dataset was developed for generating natural and engaging questions based on commonsense reasoning .,Variations of Proposed Method,Dataset,question_generation,1,17,1,0,,0.002078033,0,negative,2.86E-05,0.000508763,3.81E-05,0.001919268,0.207030874,0.000484801,3.02E-05,5.11E-05,1.86E-05,0.789788099,8.48E-05,1.24E-05,4.44E-06
1234,question_generation1,175,"We use VQG - COCO dataset for our experiments which contains a total of 2500 training images , 1250 validation images , and 1250 testing images .",Variations of Proposed Method,Dataset,question_generation,1,18,1,0,,0.019847155,0,negative,5.86E-05,0.006916041,0.000151489,1.69E-05,0.011391307,0.000926437,0.000216507,0.00124774,0.00013908,0.978655973,2.84E-05,0.000246184,5.32E-06
1235,question_generation1,176,Each image in the dataset contains five natural questions and five ground truth captions .,Variations of Proposed Method,Dataset,question_generation,1,19,1,0,,0.006254022,0,negative,9.63E-06,0.00053637,1.18E-05,1.38E-05,0.013604435,0.000192353,1.18E-05,0.000116902,1.74E-05,0.985470073,3.56E-06,1.12E-05,6.31E-07
1236,question_generation1,177,"It is worth noting that the work of ( Jain et al. , 2017 ) also used the questions from VQA dataset for training purpose , whereas the work by ( Mostafazadeh et al. , 2016 ) uses only the VQG - COCO dataset .",Variations of Proposed Method,Dataset,question_generation,1,20,1,0,,0.000716254,0,negative,6.05E-06,0.000193247,7.03E-06,2.07E-06,0.000153073,3.50E-05,6.63E-06,5.62E-05,1.04E-05,0.999348443,0.000167446,1.40E-05,4.13E-07
1237,question_generation1,178,VQA - 1.0 dataset is also built on images from MS - COCO dataset .,Variations of Proposed Method,Dataset,question_generation,1,21,1,0,,0.024457192,0,negative,0.000232224,0.004336627,0.000764456,8.87E-06,0.007055236,0.00048218,0.000486696,0.000505205,0.0001151,0.982788417,7.63E-05,0.003140225,8.50E-06
1238,question_generation1,179,"It contains a total of 82783 images for training , 40504 for validation and 81434 for testing .",Variations of Proposed Method,Dataset,question_generation,1,22,1,0,,0.007722092,0,negative,5.08E-05,0.002557895,4.74E-05,0.000160891,0.069210768,0.001169794,7.71E-05,0.000594266,7.21E-05,0.926000034,9.09E-06,4.42E-05,5.71E-06
1239,question_generation1,180,Each image is associated with 3 questions .,Variations of Proposed Method,Dataset,question_generation,1,23,1,0,,0.000311033,0,negative,2.72E-05,0.00124086,3.38E-05,1.65E-06,0.001084958,0.000179154,1.33E-05,0.000344538,0.000207942,0.996835322,2.33E-06,2.84E-05,5.73E-07
1240,question_generation1,181,We used pretrained caption generation model to extract captions for VQA dataset as the human annotated captions are not therein the dataset .,Variations of Proposed Method,Dataset,question_generation,1,24,1,0,,0.007647973,0,negative,0.000848997,0.095664029,0.002340935,3.93E-05,0.024954095,0.00139683,0.000517634,0.002146097,0.001850895,0.868497305,0.000303853,0.001409168,3.08E-05
1241,question_generation1,182,We also get good results on the VQA dataset ( as shown in ) which shows that our method does n't necessitate the presence of ground truth captions .,Variations of Proposed Method,Dataset,question_generation,1,25,1,0,,0.519644036,1,negative,0.003344879,0.000519148,4.13E-05,4.95E-06,0.00018835,0.000255463,0.002391737,0.0007271,1.85E-05,0.743370303,0.000389427,0.248718288,3.06E-05
1242,question_generation1,183,We train our model separately for VQG - COCO and VQA dataset .,Variations of Proposed Method,Dataset,question_generation,1,26,1,0,,0.066171291,0,negative,0.00031285,0.019728898,0.000168205,2.50E-06,0.000671442,0.00058337,0.000160109,0.003011591,0.00077787,0.974099365,1.19E-05,0.000469074,2.82E-06
1243,question_generation1,184,Inference,Variations of Proposed Method,,question_generation,1,27,1,0,,1.91E-05,0,negative,5.62E-05,0.000394168,2.32E-05,3.67E-05,0.00044188,0.000441248,4.19E-05,0.000387252,0.000165938,0.997927055,3.97E-05,3.42E-05,1.05E-05
1244,question_generation1,185,We made use of the 1250 validation images to tune the hyperparameters and are providing the results on test set of VQG - COCO dataset .,Variations of Proposed Method,Inference,question_generation,1,28,1,0,,0.000117882,0,negative,0.000120444,0.00020046,8.05E-06,8.24E-07,0.000263332,0.000633508,7.96E-05,0.000334786,1.69E-05,0.998264813,5.85E-07,7.55E-05,1.22E-06
1245,question_generation1,186,"During inference ,",Variations of Proposed Method,Inference,question_generation,1,29,1,0,,1.63E-06,0,negative,2.56E-05,3.65E-05,2.08E-05,9.85E-08,8.66E-06,2.82E-05,2.44E-06,1.41E-05,0.000106977,0.999752218,6.41E-07,3.61E-06,1.41E-07
1246,question_generation1,187,We use the Representation module to find the embeddings for the image and ground truth caption without using the supporting and contrasting exemplars .,Variations of Proposed Method,Inference,question_generation,1,30,1,0,,5.75E-05,0,negative,7.95E-05,0.003696499,0.000333289,2.70E-07,2.23E-05,0.000141833,1.08E-05,0.000207937,0.025990576,0.969505539,4.86E-06,5.35E-06,1.22E-06
1247,question_generation1,188,The mixture module provides the joint representation of the target image and ground truth caption .,Variations of Proposed Method,Inference,question_generation,1,31,1,0,,0.000135253,0,negative,0.000190562,0.002521951,0.000877121,5.83E-07,3.08E-05,0.000243439,2.41E-05,0.000224697,0.041799068,0.954069243,4.79E-06,1.06E-05,3.02E-06
1248,question_generation1,189,"Finally , the decoder takes in the joint features and generates the question .",Variations of Proposed Method,Inference,question_generation,1,32,1,0,,1.75E-05,0,negative,8.25E-05,0.000359685,0.000240827,1.10E-07,1.05E-05,5.87E-05,7.82E-06,4.44E-05,0.006020712,0.993166984,1.06E-06,6.11E-06,6.09E-07
1249,question_generation1,190,"We also experimented with the captions generated by an Image - Captioning network ( Karpathy and Fei-Fei , 2015 ) for VQG - COCO dataset and the result for that and training details are present in the supplementary material .",Variations of Proposed Method,Inference,question_generation,1,33,1,0,,4.67E-05,0,negative,4.47E-05,0.000175625,3.04E-05,7.30E-07,0.000790482,0.000381117,9.88E-05,7.09E-05,9.84E-06,0.998308306,4.91E-07,8.77E-05,9.39E-07
1250,question_generation1,191,Experiments,,,question_generation,1,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
1251,question_generation1,192,We evaluate our proposed MDN method in the following ways :,Experiments,Experiments,question_generation,1,1,1,0,,0.000857462,0,negative,0.000159189,6.26E-05,0.000443323,9.16E-07,2.19E-06,0.001098107,0.000565807,0.011237994,9.23E-06,0.984709112,4.35E-05,0.001652588,1.54E-05
1252,question_generation1,193,"First , we evaluate it against other variants described in section 4.4 and 4.1.3 .",Experiments,Experiments,question_generation,1,2,1,0,,0.009532779,0,negative,0.002054704,0.000738636,0.001215469,1.73E-05,9.15E-05,0.002040445,0.002864967,0.01261902,4.92E-05,0.968270009,4.52E-05,0.009875673,0.000117894
1253,question_generation1,194,"Second , we further compare our network with stateof - the - art methods for VQA 1.0 and VQG - COCO dataset .",Experiments,Experiments,question_generation,1,3,1,0,,0.417842302,0,negative,0.004048857,0.001043659,0.003027969,1.76E-05,8.22E-05,0.001724568,0.009574742,0.014126621,4.12E-05,0.868794465,0.000164434,0.097067718,0.000285862
1254,question_generation1,195,We perform a user study to gauge human opinion on naturalness of the generated question and analyze the word statistics in .,Experiments,Experiments,question_generation,1,4,1,0,,0.015032433,0,negative,0.006056837,0.001321564,0.002653398,0.00212042,0.004571952,0.005835176,0.009108211,0.011290274,0.000140408,0.948747098,0.000332317,0.006141848,0.001680497
1255,question_generation1,196,This is an important test as humans are the best .,Experiments,Experiments,question_generation,1,5,1,0,,0.001846564,0,negative,0.000151663,1.28E-05,5.92E-05,9.17E-06,4.95E-06,0.00187387,0.00059419,0.010249134,1.40E-05,0.985809924,0.000158022,0.000988737,7.44E-05
1256,question_generation1,197,Although these metrics have not been shown to correlate with ' naturalness ' of the question these still provide a reasonable quantitative measure for comparison .,Experiments,Experiments,question_generation,1,6,1,0,,0.000765979,0,negative,0.000205077,2.18E-05,5.96E-05,2.48E-05,1.57E-05,0.001153297,0.000801205,0.005426192,1.07E-05,0.989321774,0.000199998,0.002681298,7.85E-05
1257,question_generation1,198,"Here we only provide the BLEU1 scores , but the remaining BLEU -n metric scores are present in the supplementary .",Experiments,Experiments,question_generation,1,7,1,0,,0.000645281,0,negative,0.000247521,2.63E-05,7.61E-05,2.71E-05,3.55E-05,0.001492789,0.000365454,0.005940735,1.38E-05,0.99094171,6.21E-06,0.000795398,3.13E-05
1258,question_generation1,199,We observe that the proposed MDN provides improved embeddings to the decoder .,Experiments,Experiments,question_generation,1,8,1,0,,0.766095264,1,results,0.043302424,0.000147884,0.000306383,2.28E-05,3.34E-05,0.001268727,0.011136464,0.018205744,2.58E-05,0.1837027,9.48E-05,0.740851609,0.000901327
1259,question_generation1,200,We believe that these embeddings capture instance specific differential information that helps in guiding the question generation .,Experiments,Experiments,question_generation,1,9,1,0,,0.054015131,0,negative,0.002400784,0.00033486,0.000373044,8.36E-05,7.78E-05,0.007427124,0.000995807,0.055734026,0.00032961,0.928985291,3.73E-05,0.002778695,0.00044205
1260,question_generation1,201,Details regarding the metrics are given in the supplementary material .,Experiments,Experiments,question_generation,1,10,1,0,,0.000576992,0,negative,6.76E-05,2.72E-05,1.45E-05,1.21E-05,9.25E-06,0.002336735,0.000220711,0.020576443,1.89E-05,0.976385338,5.80E-06,0.000297782,2.77E-05
1261,question_generation1,202,Ablation Analysis,,,question_generation,1,0,1,0,,0.00263953,0,negative,0.027886983,0.000245704,0.001914002,0.000140888,8.16E-05,0.000373127,0.001101303,0.000624503,0.000135788,0.957862925,0.001371201,0.008214408,4.76E-05
1262,question_generation1,203,We considered different variations of our method mentioned in section 4.4 and the various ways to obtain the joint multimodal embedding as described in section 4.1.3 .,Ablation Analysis,Ablation Analysis,question_generation,1,1,1,0,,0.188071023,0,negative,0.126705282,8.16E-05,0.006895613,2.29E-05,0.000146556,0.001372843,0.000381872,0.000132439,0.000230252,0.863063011,5.08E-05,8.25E-06,0.000908613
1263,question_generation1,204,The results for the VQG - COCO test set are given in table,Ablation Analysis,Ablation Analysis,question_generation,1,2,1,0,,0.50102771,1,ablation-analysis,0.834300613,2.86E-06,0.000869893,2.20E-07,2.19E-06,1.87E-05,0.00077698,3.65E-06,2.31E-06,0.16302722,6.04E-05,0.00076425,0.000170724
1264,question_generation1,205,1 .,Ablation Analysis,Ablation Analysis,question_generation,1,3,1,0,,0.00498252,0,negative,0.023074005,2.04E-06,0.000392468,1.39E-06,2.29E-06,7.89E-05,6.81E-06,1.09E-05,6.32E-05,0.976291414,1.56E-05,7.66E-07,6.03E-05
1265,question_generation1,206,"In this table , every block provides the results for one of the variations of obtaining the embeddings and different ways of combining them .",Ablation Analysis,Ablation Analysis,question_generation,1,4,1,0,,0.064504804,0,negative,0.021487333,2.49E-06,0.000520664,3.37E-06,4.81E-05,0.000110589,3.86E-05,1.05E-05,1.50E-05,0.977651704,5.91E-06,4.18E-06,0.000101557
1266,question_generation1,207,We observe that the,Ablation Analysis,Ablation Analysis,question_generation,1,5,1,0,,0.003687515,0,negative,0.120986733,5.05E-07,0.000679752,3.26E-07,1.52E-06,1.45E-05,7.50E-06,1.62E-06,1.24E-05,0.878272456,3.33E-06,3.08E-06,1.63E-05
1267,question_generation1,208,Baseline and State - of - the - Art,Ablation Analysis,Ablation Analysis,question_generation,1,6,1,0,,0.316312999,0,negative,0.02114864,1.98E-05,0.021481422,1.07E-06,5.48E-06,0.000296916,0.000349089,5.06E-05,0.000435273,0.955276452,0.000122588,7.80E-06,0.000804919
1268,question_generation1,209,The comparison of our method with various baselines and state - of - the - art methods is provided in module mentioned in section 4.1.3 and also against the state - of - the - art methods .,Ablation Analysis,Ablation Analysis,question_generation,1,7,1,0,,0.02832739,0,negative,0.008210013,2.12E-06,0.000191849,1.13E-06,1.70E-05,5.54E-05,2.10E-05,8.17E-06,5.42E-06,0.991449408,4.04E-06,2.99E-06,3.14E-05
1269,question_generation1,210,The Critical Difference ( CD ) for Nemenyi test depends upon the given ?,Ablation Analysis,Ablation Analysis,question_generation,1,8,1,0,,0.040503692,0,negative,0.019846714,1.44E-05,0.000649045,2.87E-06,5.12E-06,9.38E-05,3.47E-05,2.71E-05,0.000124322,0.977596221,0.001018707,3.50E-06,0.000583444
1270,question_generation1,211,"( confidence level , which is 0.05 in our case ) for average ranks and N ( number of tested datasets ) .",Ablation Analysis,Ablation Analysis,question_generation,1,9,1,0,,0.045413527,0,negative,0.067145065,1.11E-05,0.000686308,6.88E-06,3.21E-05,0.000726819,0.00014809,0.0001932,6.22E-05,0.930235171,1.19E-05,3.19E-06,0.000738043
1271,question_generation1,212,"If the difference in the rank of the two methods lies within CD , then they are not significantly different and vice - versa. visualizes the post - hoc analysis using the CD diagram .",Ablation Analysis,Ablation Analysis,question_generation,1,10,1,0,,0.044464033,0,negative,0.049198925,4.74E-06,0.001636913,2.70E-06,6.20E-06,0.000194407,6.88E-05,2.14E-05,0.000122095,0.94813179,2.35E-05,3.00E-06,0.000585479
1272,question_generation1,213,"From the figure , it is clear that MDN - Joint works best and is statistically significantly different from the state - of - the - art methods ..",Ablation Analysis,Ablation Analysis,question_generation,1,11,1,0,,0.829970805,1,ablation-analysis,0.991683928,4.02E-07,6.96E-05,3.65E-07,1.55E-06,6.40E-06,0.000363581,9.44E-07,3.01E-07,0.007350267,2.55E-06,0.000269868,0.000250232
1273,question_generation1,214,The colored lines between the two models represents that these models are not significantly different from each other .,Ablation Analysis,Ablation Analysis,question_generation,1,12,1,0,,0.019475319,0,negative,0.054019583,4.03E-06,0.002943339,4.69E-07,5.00E-06,7.94E-05,3.92E-05,1.07E-05,0.000154032,0.942667854,2.46E-06,2.52E-06,7.14E-05
1274,question_generation1,215,Here every question has different number of responses and hence the threshold which is the half of total responses for each question is varying .,Ablation Analysis,Ablation Analysis,question_generation,1,13,1,0,,0.026588448,0,negative,0.093969453,6.56E-06,0.004539285,9.03E-07,9.09E-06,4.57E-05,2.40E-05,7.78E-06,0.000112252,0.901141562,2.04E-05,5.65E-06,0.000117433
1275,question_generation1,216,This plot is only for 50 of the 100 questions involved in the survey .,Ablation Analysis,Ablation Analysis,question_generation,1,14,1,0,,0.028164689,0,negative,0.226436028,2.23E-06,0.000678783,2.55E-06,2.49E-05,0.000137895,0.000111871,1.31E-05,1.87E-05,0.772133552,9.35E-06,2.02E-05,0.000410799
1276,question_generation1,217,See section 5.4 for more details .,Ablation Analysis,Ablation Analysis,question_generation,1,15,1,0,,0.002376891,0,negative,0.023756252,1.62E-06,0.000270322,9.95E-06,2.20E-05,0.00010351,1.59E-05,9.25E-06,1.59E-05,0.975647393,1.57E-06,1.54E-06,0.000144873
1277,question_generation1,218,Perceptual Realism,Ablation Analysis,,question_generation,1,16,1,0,,0.701771824,1,ablation-analysis,0.635097213,0.000104879,0.036902439,8.38E-06,2.17E-05,0.000256333,0.010609585,4.23E-05,0.000566707,0.256969683,0.002971787,0.001152345,0.055296701
1278,question_generation1,219,"A human is the best judge of naturalness of any question , We evaluated our proposed MDN method using a ' Naturalness ' Turing test ( Zhang et al. , 2016 ) on 175 people .",Ablation Analysis,Perceptual Realism,question_generation,1,17,1,0,,0.00011094,0,negative,0.000489438,2.60E-06,3.35E-05,2.00E-07,1.69E-05,9.45E-06,8.14E-05,4.78E-06,1.46E-06,0.999131418,1.90E-06,0.000218525,8.52E-06
1279,question_generation1,220,People were shown an image with 2 questions just as in and were asked to rate the naturalness of both the questions on a scale of 1 to 5 where 1 means ' Least Natural ' and 5 is the ' Most Natural ' .,Ablation Analysis,Perceptual Realism,question_generation,1,18,1,0,,1.38E-05,0,negative,4.52E-05,2.60E-07,5.09E-06,9.37E-08,1.01E-06,1.49E-05,1.71E-05,5.17E-06,1.18E-06,0.999892415,3.91E-07,1.43E-05,2.87E-06
1280,question_generation1,221,We provided 175 people with 100 such images from the VQG - COCO validation dataset which has 1250 images .,Ablation Analysis,Perceptual Realism,question_generation,1,19,1,0,,0.000418153,0,negative,0.000656731,6.10E-07,3.36E-05,1.14E-06,0.000123376,1.92E-05,5.60E-05,2.34E-06,5.12E-07,0.998946014,1.67E-07,0.000148004,1.23E-05
1281,question_generation1,222,indicates the number of people who were fooled ( rated the generated question more or equal to the ground truth question ) .,Ablation Analysis,Perceptual Realism,question_generation,1,20,1,0,,1.12E-05,0,negative,8.53E-05,2.75E-07,5.22E-06,3.11E-08,1.36E-07,6.23E-06,9.19E-06,6.35E-06,3.10E-06,0.999873336,2.12E-07,9.45E-06,1.18E-06
1282,question_generation1,223,"For the 100 images , on an average 59.7 % people were fooled in this experiment and this shows that our model is able to generate natural questions .",Ablation Analysis,Perceptual Realism,question_generation,1,21,1,0,,0.009057667,0,negative,0.004963406,1.70E-07,3.97E-06,3.12E-07,1.22E-05,8.23E-06,0.000204749,2.01E-06,1.44E-07,0.992432984,4.52E-07,0.002348261,2.31E-05
1283,question_generation1,224,Conclusion,,,question_generation,1,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
1284,question_generation1,246,B.2 Training Configuration,,,question_generation,1,0,1,0,,0.030861909,0,negative,0.000996326,0.001816228,0.001208192,5.96E-07,3.18E-06,6.75E-05,5.86E-05,0.001621207,0.000835975,0.989236073,0.000819324,0.003332767,4.05E-06
1285,question_generation1,247,"We have used RMSPROP optimizer to update the model parameter and configured hyperparameter values to be as follows : learning rate = 0.0004 , batch size = 200 , ? = 0.99 , = 1e ? 8 to train the classification network .",B.2 Training Configuration,B.2 Training Configuration,question_generation,1,1,1,0,,0.068089352,0,negative,0.001225973,8.06E-05,0.000258679,8.87E-05,4.12E-06,0.028939378,0.000417815,0.447809832,0.000249055,0.514950481,2.68E-05,0.000629801,0.005318724
1286,question_generation1,248,"In order to train a triplet model , we have used RM - SPROP to optimize the triplet model model parameter and configure hyper - parameter values to be : learning rate = 0.001 , batch size = 200 , ? = 0.9 , = 1 e ?",B.2 Training Configuration,B.2 Training Configuration,question_generation,1,2,1,0,,0.022873571,0,negative,0.001409001,8.37E-05,0.00025695,3.01E-05,3.38E-06,0.011477312,0.000262672,0.226931905,0.000178311,0.756056907,2.13E-05,0.000841107,0.002447353
1287,question_generation1,249,8 . We also used learning rate decay to decrease the learning rate on every epoch by a factor given by :,B.2 Training Configuration,B.2 Training Configuration,question_generation,1,3,1,0,,0.000854094,0,negative,0.008388027,2.29E-05,0.003823185,6.09E-06,1.70E-06,0.000914057,4.52E-05,0.006615006,0.000152997,0.977912668,7.02E-06,0.001605751,0.000505396
1288,question_generation1,250,where values of a =1500 and b=1250 are set empirically .,B.2 Training Configuration,B.2 Training Configuration,question_generation,1,4,1,0,,4.49E-05,0,negative,0.001150304,2.02E-05,8.78E-05,1.09E-05,1.51E-06,0.003738088,9.25E-05,0.096534673,5.59E-05,0.896482836,6.98E-06,0.0007865,0.001032005
1289,question_generation1,251,C Ablation Analysis of Model,B.2 Training Configuration,,question_generation,1,5,1,0,,0.000279875,0,negative,0.024167359,4.16E-06,0.007117845,6.91E-06,1.46E-06,5.38E-05,2.12E-05,0.000132282,4.70E-05,0.938010445,2.15E-05,0.029664028,0.000751954
1290,question_generation1,252,"While , we advocate the use of multimodal differential network ( MDN ) for generating embeddings that can be used by the decoder for generating questions , we also evaluate several variants of this architecture namely ( a ) Differential Image Network , ( b ) Tag net and ( c ) Place net .",B.2 Training Configuration,C Ablation Analysis of Model,question_generation,1,6,1,0,,0.010095866,0,negative,0.009788054,6.94E-05,0.026157201,1.26E-06,2.07E-06,2.90E-05,2.92E-05,9.29E-05,0.000335667,0.961779332,3.12E-05,0.001528102,0.00015658
1291,question_generation1,253,These are described in detail as follows :,B.2 Training Configuration,C Ablation Analysis of Model,question_generation,1,7,1,0,,3.40E-07,0,negative,0.004286758,4.80E-07,0.000750484,1.42E-07,3.54E-07,1.84E-06,4.94E-07,2.15E-06,1.35E-05,0.994794575,2.51E-07,0.000141707,7.22E-06
1292,question_generation1,254,C.1 Differential Image Network,B.2 Training Configuration,,question_generation,1,8,1,0,,0.000272315,0,negative,0.003510332,1.21E-05,0.157667888,7.05E-07,4.18E-07,4.18E-05,3.07E-05,0.000148124,0.000209227,0.816400687,0.000157735,0.021042282,0.000777975
1293,question_generation1,255,"For obtaining the exemplar image based context embedding , we propose a triplet network consist of three network , one is target net , supporting net and opposing net .",B.2 Training Configuration,C.1 Differential Image Network,question_generation,1,9,1,0,,0.001231776,0,negative,0.009532248,0.00060063,0.204292222,2.76E-05,2.87E-05,0.00066781,0.00022215,0.001391489,0.020102486,0.753460552,0.000215802,0.004079895,0.005378394
1294,question_generation1,256,All these three networks designed with convolution neural network and shared the same parameters .,B.2 Training Configuration,C.1 Differential Image Network,question_generation,1,10,1,0,,4.55E-05,0,negative,0.000417356,8.38E-06,0.00407775,7.44E-06,3.99E-06,0.002034394,0.000157407,0.001617249,0.000144969,0.990002539,3.67E-06,0.000226134,0.001298726
1295,question_generation1,257,The weights of this network are learnt through end - to - end learning using a triplet loss .,B.2 Training Configuration,C.1 Differential Image Network,question_generation,1,11,1,0,,0.000215119,0,negative,0.000767069,0.000106207,0.004369928,3.37E-06,2.24E-06,0.00060367,3.92E-05,0.004906573,0.004514999,0.98413883,6.98E-06,9.68E-05,0.00044417
1296,question_generation1,258,The aim is to obtain latent weight vectors that bring the supporting exemplar close to the target image and enhances the difference between opposing examples .,B.2 Training Configuration,C.1 Differential Image Network,question_generation,1,12,1,0,,0.000479351,0,negative,0.001129173,0.000193294,0.003081426,2.71E-06,3.03E-06,9.95E-05,1.20E-05,0.000550485,0.002499276,0.991686669,2.56E-05,0.000509124,0.000207708
1297,question_generation1,259,"More formally , given an image xi we obtain an embedding g i using a CNN that we parameterize through a function G ( x i , W c ) where W care the weights of the CNN .",B.2 Training Configuration,C.1 Differential Image Network,question_generation,1,13,1,0,,1.76E-06,0,negative,6.38E-05,4.64E-06,0.000410323,1.28E-06,4.38E-07,7.39E-05,4.38E-06,0.00018207,0.000232113,0.998861965,2.64E-06,3.25E-05,0.000129873
1298,question_generation1,260,This is illustrated in figure 11 . : Full State - of - the - Art comparison on VQG - COCO Dataset .,B.2 Training Configuration,C.1 Differential Image Network,question_generation,1,14,1,0,,0.002294792,0,negative,0.001344003,3.14E-07,0.000275135,2.59E-07,2.44E-07,5.96E-05,0.000105836,4.47E-05,1.65E-06,0.960739715,5.13E-06,0.036935341,0.000488047
1299,question_generation1,261,"The first block consists of the state - of - the - art results , second block refers to the baselines mentioned in State - of - the - art section of main paper and the third block provides the results for the best method for different ablations of our method .",B.2 Training Configuration,C.1 Differential Image Network,question_generation,1,15,1,0,,1.71E-05,0,negative,0.000108624,2.94E-07,3.93E-05,1.23E-06,1.73E-06,6.69E-05,5.09E-06,3.55E-05,2.93E-06,0.999522154,1.42E-07,0.000158786,5.74E-05
1300,question_generation1,262,C.2,B.2 Training Configuration,,question_generation,1,16,1,0,,3.50E-06,0,negative,0.001803304,1.03E-06,0.000153633,9.11E-06,7.32E-07,8.01E-05,5.88E-06,0.000192737,2.19E-05,0.995822485,7.13E-07,0.001031817,0.000876513
1301,question_generation1,263,Tag net,B.2 Training Configuration,,question_generation,1,17,1,0,,0.000851988,0,negative,0.000775427,9.09E-06,0.000224239,2.35E-05,1.60E-06,0.0001515,1.03E-05,0.001532036,0.000320841,0.990874208,8.31E-06,0.000793088,0.005275871
1302,question_generation1,264,The tag net consists of two parts Context Extractor & Tag Embedding Net .,B.2 Training Configuration,Tag net,question_generation,1,18,1,0,,0.00217029,0,negative,0.004649181,4.09E-05,0.12063203,1.08E-05,4.91E-06,0.000180341,0.000119575,0.000621585,0.005899782,0.83344923,1.23E-05,0.002233934,0.032145409
1303,question_generation1,265,This is illustrated in .,B.2 Training Configuration,Tag net,question_generation,1,19,1,0,,9.52E-07,0,negative,0.000188347,8.06E-08,8.35E-05,1.49E-07,6.73E-08,3.11E-06,6.32E-07,6.08E-06,6.17E-06,0.999539351,4.01E-08,0.000133874,3.86E-05
1304,question_generation1,266,Extract Context :,B.2 Training Configuration,Tag net,question_generation,1,20,1,0,,0.000318048,0,negative,0.002488404,5.90E-06,0.025745816,4.45E-06,1.37E-06,6.45E-05,6.59E-05,0.00010144,0.000109797,0.930733492,2.24E-05,0.026170845,0.014485646
1305,question_generation1,267,The first step is to extract the caption of the image using Neural Talk 2 model .,B.2 Training Configuration,Tag net,question_generation,1,21,1,0,,0.002135609,0,negative,0.003796213,9.22E-06,0.010082439,4.98E-06,2.36E-06,5.32E-05,2.16E-05,0.000176895,0.000583111,0.976325665,2.93E-06,0.002238583,0.006702787
1306,question_generation1,268,We find the part - ofs peech ( POS ) tag present in the caption .,B.2 Training Configuration,Tag net,question_generation,1,22,1,0,,1.14E-05,0,negative,0.000136174,3.16E-07,0.000167541,2.14E-06,1.24E-06,3.70E-05,4.19E-06,6.05E-05,1.15E-05,0.998571065,6.20E-08,0.000182033,0.000826351
1307,question_generation1,269,"POS taggers have been developed for two well known corpuses , the Brown Corpus and the Penn Treebanks .",B.2 Training Configuration,Tag net,question_generation,1,23,1,0,,0.000242093,0,negative,0.000978619,3.23E-06,0.001930277,8.30E-06,3.71E-06,0.000144619,0.000108742,0.00014609,2.50E-05,0.919429646,3.84E-05,0.014632846,0.062550462
1308,question_generation1,270,"For our work , we are using the Brown Corpus tags .",B.2 Training Configuration,Tag net,question_generation,1,24,1,0,,2.99E-05,0,negative,0.000167307,1.30E-06,0.000832238,1.41E-06,2.37E-06,0.00012281,4.14E-05,0.000412205,1.21E-05,0.996663131,4.88E-08,0.000342477,0.001401136
1309,question_generation1,271,"The tags are clustered into three category namely Noun tag , Verb tag and Question tags ( What , Where , . . . ) .",B.2 Training Configuration,Tag net,question_generation,1,25,1,0,,0.000198041,0,negative,0.00020398,1.61E-06,0.00028985,2.98E-06,1.90E-06,9.67E-05,1.19E-05,0.000475817,5.94E-05,0.996376028,7.47E-08,0.000143904,0.002335781
1310,question_generation1,272,"Noun tag consists of all the noun & pronouns present in the caption sentence and similarly , verb tag consists of verb & adverbs present in the caption sentence .",B.2 Training Configuration,Tag net,question_generation,1,26,1,0,,2.03E-05,0,negative,5.72E-05,1.06E-06,0.000415805,1.45E-06,7.19E-07,8.20E-05,1.48E-05,0.000356725,4.66E-05,0.996417544,2.47E-07,0.000172814,0.002433074
1311,question_generation1,273,"The question tags consists of the 7 - well know question words i.e. , why , how , what , when , where , who and which .",B.2 Training Configuration,Tag net,question_generation,1,27,1,0,,8.96E-05,0,negative,7.81E-05,4.86E-07,0.000232254,3.46E-06,5.49E-06,0.000108659,2.17E-05,0.000259063,6.50E-06,0.995547637,4.17E-08,0.000179163,0.003557464
1312,question_generation1,274,Each tag token is represented as a one - hot vector of the dimension of vocabulary size .,B.2 Training Configuration,Tag net,question_generation,1,28,1,0,,7.62E-05,0,negative,0.000186098,4.57E-06,0.000810515,8.69E-07,4.41E-07,6.31E-05,1.56E-05,0.001254993,0.000505915,0.995722153,2.84E-07,0.000110959,0.001324508
1313,question_generation1,275,"For generalization , we have considered 5 tokens from each category of the Tags .",B.2 Training Configuration,Tag net,question_generation,1,29,1,0,,0.000129721,0,negative,0.000271567,1.12E-06,9.05E-05,4.05E-07,5.82E-07,0.00013885,5.14E-05,0.001765008,1.24E-05,0.996430443,3.19E-08,0.000294794,0.000942979
1314,question_generation1,276,Tag Embedding,B.2 Training Configuration,,question_generation,1,30,1,0,,0.004911373,0,negative,0.017327768,1.30E-05,0.002878748,0.000284542,1.09E-05,0.000353181,8.43E-05,0.000938567,0.000534861,0.83917205,1.30E-05,0.012154927,0.126234137
1315,question_generation1,277,Net :,B.2 Training Configuration,Tag Embedding,question_generation,1,31,1,0,,0.002890941,0,negative,0.001009993,1.66E-06,0.056280706,1.42E-07,2.18E-07,6.35E-06,1.39E-05,2.41E-05,0.000120334,0.935193721,1.29E-06,0.00656573,0.00078184
1316,question_generation1,278,The embedding network consists of word embedding followed by temporal convolutions neural network followed by maxpooling network .,B.2 Training Configuration,Tag Embedding,question_generation,1,32,1,0,,0.002254789,0,negative,0.003228109,2.45E-05,0.013969134,1.88E-05,3.67E-06,0.000226657,9.41E-05,0.003372469,0.00355879,0.928982645,1.73E-06,0.00104695,0.04547251
1317,question_generation1,279,"In the first step , sparse high dimensional one - hot vector is transformed to dense low dimension vector using word embedding .",B.2 Training Configuration,Tag Embedding,question_generation,1,33,1,0,,0.000920092,0,negative,0.002704031,2.47E-05,0.006122467,1.01E-06,1.26E-06,1.50E-05,1.27E-05,0.000324172,0.001138928,0.986864292,4.37E-07,0.001307409,0.001483558
1318,question_generation1,280,"After this , we apply temporal convolution on the word embedding vector .",B.2 Training Configuration,Tag Embedding,question_generation,1,34,1,0,,7.49E-05,0,negative,0.001549324,2.76E-06,0.003386028,7.01E-08,1.51E-07,3.78E-06,3.61E-06,5.74E-05,0.000264044,0.994154159,5.55E-08,0.0004323,0.000146344
1319,question_generation1,281,"The uni-gram , bi-gram and tri-gram feature are computed by applying convolution filter of size 1 , 2 and 3 respectability .",B.2 Training Configuration,Tag Embedding,question_generation,1,35,1,0,,0.000156954,0,negative,0.00088115,6.92E-06,0.000230821,3.78E-06,1.13E-06,0.000441456,8.10E-05,0.012429983,0.000141373,0.980699599,9.11E-08,0.000307508,0.004775179
1320,question_generation1,282,"Finally , we applied max - pooling on this to get a vector representation of the tags as shown figure 12 .",B.2 Training Configuration,Tag Embedding,question_generation,1,36,1,0,,4.58E-05,0,negative,0.000364025,4.34E-07,0.000909259,6.27E-08,1.02E-07,3.77E-06,2.30E-06,3.23E-05,4.82E-05,0.998089428,2.24E-08,0.000386401,0.000163698
1321,question_generation1,283,We concatenated all the tag words fol - lowed by fully connected layer to get feature dimension of 512 .,B.2 Training Configuration,Tag Embedding,question_generation,1,37,1,0,,0.004764268,0,negative,0.001426841,2.10E-05,0.000488564,2.89E-05,5.64E-06,0.00249199,0.000602384,0.045537144,0.000306489,0.829998905,4.95E-07,0.000845512,0.118246158
1322,question_generation1,284,"We also explored joint networks based on concatenation of all the tags , on elementwise addition and element - wise multiplication of the tag vectors .",B.2 Training Configuration,Tag Embedding,question_generation,1,38,1,0,,0.001200274,0,negative,0.000253194,1.54E-06,0.002710874,1.23E-07,1.95E-07,4.34E-06,2.81E-06,3.74E-05,0.000109603,0.996279702,1.79E-07,0.000382703,0.000217382
1323,question_generation1,285,"However , we observed that convolution over max pooling and joint concatenation gives better performance based on CIDer score .",B.2 Training Configuration,Tag Embedding,question_generation,1,39,1,0,,0.001155891,0,results,0.017516028,1.60E-07,8.19E-05,4.49E-07,1.57E-07,7.81E-06,0.000111981,3.49E-05,7.01E-07,0.353908684,1.46E-07,0.626185161,0.002151965
1324,question_generation1,286,"Where , T CNN is Temporally Convolution Neural Network applied on word embedding vector with kernel size three .",B.2 Training Configuration,Tag Embedding,question_generation,1,40,1,0,,7.49E-06,0,negative,0.000126856,7.34E-07,0.000221069,7.45E-07,2.55E-07,4.92E-05,1.12E-05,0.001149271,3.21E-05,0.996395493,4.42E-08,0.000124603,0.001888441
1325,question_generation1,287,C.3,B.2 Training Configuration,,question_generation,1,41,1,0,,2.43E-06,0,negative,0.001336277,3.48E-07,0.000136309,2.65E-06,6.66E-07,2.77E-05,4.66E-06,8.90E-05,7.56E-06,0.996004879,5.20E-08,0.001322322,0.00106764
1326,question_generation1,288,Place net,B.2 Training Configuration,,question_generation,1,42,1,0,,0.000102223,0,negative,0.000905329,5.31E-06,0.002270584,4.01E-06,2.03E-06,5.43E-05,2.13E-05,0.000553107,0.000530519,0.975314274,1.99E-06,0.003246927,0.017090302
1327,question_generation1,289,Visual object and scene recognition plays a crucial role in the image .,B.2 Training Configuration,Place net,question_generation,1,43,1,0,,0.000809173,0,negative,0.000695575,3.00E-07,0.000670366,2.60E-06,2.92E-07,1.18E-05,3.60E-05,3.19E-05,6.75E-06,0.867527445,3.05E-06,0.040174531,0.090839348
1328,question_generation1,290,"Here , places in the image are labeled with scene semantic categories , comprise of large and diverse type of environment in the world , such as ( amusement park , tower , swimming pool , shoe shop , cafeteria , rain - forest , conference center , fishpond , etc . ) .",B.2 Training Configuration,Place net,question_generation,1,44,1,0,,1.08E-05,0,negative,1.47E-05,3.30E-08,9.69E-05,1.97E-08,4.16E-08,5.80E-07,4.02E-07,6.33E-06,1.72E-06,0.999659042,1.01E-09,0.000147269,7.30E-05
1329,question_generation1,291,So we have used different type of scene semantic categories present in the image as a place based context to generate natural question .,B.2 Training Configuration,Place net,question_generation,1,45,1,0,,7.11E-05,0,negative,0.000182652,7.03E-08,0.000266179,6.68E-08,9.89E-08,1.16E-06,1.66E-06,5.14E-06,1.83E-06,0.996255003,9.63E-09,0.002917329,0.000368797
1330,question_generation1,292,"A place365 is a convolution neural network is modeled to classify 365 types of scene categories , which is trained on the place2 dataset consist of 1.8 million of scene images .",B.2 Training Configuration,Place net,question_generation,1,46,1,0,,5.15E-05,0,negative,0.00067312,1.81E-06,0.111815614,9.54E-07,8.32E-07,1.36E-05,4.45E-05,6.11E-05,0.000111122,0.859095501,3.18E-07,0.005545507,0.022635953
1331,question_generation1,293,We have used a pre-trained VGG16 - places 365 network to obtain place based context embedding feature for various type scene categories present in the image .,B.2 Training Configuration,Place net,question_generation,1,47,1,0,,0.002659239,0,negative,0.000544938,5.59E-06,0.002952781,4.97E-06,1.75E-06,0.000134251,0.000127483,0.002961565,0.00015429,0.908558255,8.28E-08,0.00170109,0.082852955
1332,question_generation1,294,The context feature F C is obtained by :,B.2 Training Configuration,Place net,question_generation,1,48,1,0,,1.19E-06,0,negative,8.82E-05,3.25E-08,7.45E-05,3.20E-08,1.57E-08,6.95E-07,5.82E-07,9.76E-06,1.88E-06,0.999411236,2.19E-09,0.000287853,0.000125141
1333,question_generation1,295,"Where , p conv is Place 365 CNN .",B.2 Training Configuration,Place net,question_generation,1,49,1,0,,1.09E-06,0,negative,2.59E-05,7.40E-08,0.000259188,5.56E-08,2.19E-08,2.39E-06,1.51E-06,6.01E-05,1.03E-05,0.998898651,5.95E-09,9.78E-05,0.000643974
1334,question_generation1,296,"We have extracted conv5 features of dimension 14x14x512 for attention model and FC8 features of dimension 365 for joint , addition and hadamard model of places 365 .",B.2 Training Configuration,Place net,question_generation,1,50,1,0,,0.010397497,0,negative,0.00064056,3.22E-06,0.000341664,2.09E-05,2.00E-06,0.001084665,0.000675439,0.019742726,3.46E-05,0.65950477,4.94E-08,0.001647701,0.316301697
1335,question_generation1,297,"Finally , we use a linear transformation to obtain a 512 dimensional vector .",B.2 Training Configuration,Place net,question_generation,1,51,1,0,,0.000909761,0,negative,0.000574126,1.25E-06,0.001053117,5.09E-07,4.22E-07,8.48E-05,0.000111599,0.00166503,4.46E-05,0.978074682,1.15E-08,0.000730734,0.017659123
1336,question_generation1,298,"We explored using the CONV5 having feature dimension 14x14 512 , FC7 having 4096 and FC8 having feature dimension of 365 of places 365 .",B.2 Training Configuration,Place net,question_generation,1,52,1,0,,0.015444956,0,tasks,0.000670492,2.45E-06,0.00061321,5.11E-05,1.92E-06,0.001832938,0.001009001,0.015561711,2.83E-05,0.392117154,5.58E-08,0.001919934,0.586191727
1337,question_generation1,299,D Ablation Analysis,,,question_generation,1,0,1,0,,0.003187531,0,negative,0.022558404,0.00017168,0.000993088,4.80E-05,3.88E-05,0.000142158,0.00047843,0.000338933,8.93E-05,0.968287345,0.00072261,0.006111615,1.96E-05
1338,question_generation1,300,D.1 Sampling Exemplar : KNN vs ITML,D Ablation Analysis,D Ablation Analysis,question_generation,1,1,1,0,,0.495207319,0,ablation-analysis,0.679969244,4.07E-05,0.103354202,6.73E-07,1.91E-06,4.35E-05,0.00027656,6.46E-06,0.000182031,0.213263017,0.001980582,0.000262431,0.000618686
1339,question_generation1,301,Our method is aimed at using efficient exemplarbased retrieval techniques .,D Ablation Analysis,D Ablation Analysis,question_generation,1,2,1,0,,0.033819265,0,negative,0.282832499,0.001268616,0.051713328,4.92E-05,0.000170466,0.000421179,0.00014896,9.60E-05,0.005611557,0.644015757,0.008235973,6.72E-05,0.00536925
1340,question_generation1,302,"We have experimented with various exemplar methods , such as ITML based metric learning for image features and KNN based approaches .",D Ablation Analysis,D Ablation Analysis,question_generation,1,3,1,0,,0.040609063,0,negative,0.062587418,4.22E-05,0.008959794,8.98E-06,8.80E-05,0.000358313,0.000248537,3.29E-05,5.92E-05,0.925240602,0.000404595,2.94E-05,0.001939991
1341,question_generation1,303,We observed KNN based approach ( K - D tree ) with Euclidean metric is a efficient method for finding exemplars .,D Ablation Analysis,D Ablation Analysis,question_generation,1,4,1,0,,0.108778875,0,ablation-analysis,0.6384707,7.26E-06,0.00110349,3.45E-06,1.35E-05,0.000105481,0.000116978,1.08E-05,2.34E-05,0.357810597,0.000410937,0.000103218,0.001820134
1342,question_generation1,304,Also we observed that ITML is computationally expensive and also depends on the training procedure .,D Ablation Analysis,D Ablation Analysis,question_generation,1,5,1,0,,0.004540217,0,negative,0.317476544,3.45E-06,0.00031669,3.17E-05,3.91E-05,0.0001455,4.74E-05,8.93E-06,1.15E-05,0.679496849,0.000291307,2.69E-05,0.002104186
1343,question_generation1,305,"The We see that both give more or less similar results but since ITML is computationally expensive and the dataset size is also small , it is not that efficient for our use .",D Ablation Analysis,D Ablation Analysis,question_generation,1,6,1,0,,0.029189735,0,ablation-analysis,0.793096876,3.50E-06,0.000205351,1.10E-05,2.00E-05,0.000146785,0.000375899,2.35E-05,6.49E-06,0.201710761,7.25E-05,0.000363387,0.003963889
1344,question_generation1,306,All these experiment are for the differential image network for K = 2 only .,D Ablation Analysis,D Ablation Analysis,question_generation,1,7,1,0,,0.009775375,0,negative,0.039306149,8.67E-06,0.000991268,1.13E-06,1.48E-05,0.000101992,6.09E-05,2.48E-05,2.97E-05,0.959301968,7.13E-06,8.75E-06,0.000142806
1345,question_generation1,307,D.2 Question Generation approaches :,D Ablation Analysis,D Ablation Analysis,question_generation,1,8,1,0,,0.162545033,0,negative,0.441158872,0.000107885,0.082534242,1.65E-06,6.91E-06,0.000120876,0.00031467,1.54E-05,0.000465541,0.470541957,0.002833795,0.000154786,0.001743372
1346,question_generation1,308,Sampling vs Argmax,D Ablation Analysis,,question_generation,1,9,1,0,,0.016058407,0,negative,0.06410078,2.26E-05,0.008802444,3.21E-06,1.22E-05,0.000136654,4.37E-05,3.45E-05,0.00073875,0.925605195,3.38E-05,5.04E-06,0.000461221
1347,question_generation1,309,We obtained the decoding using standard practice followed in the literature .,D Ablation Analysis,Sampling vs Argmax,question_generation,1,10,1,0,,3.74E-05,0,negative,0.000197835,2.61E-06,5.17E-05,6.60E-07,8.63E-07,7.25E-05,5.34E-06,9.38E-05,4.11E-05,0.999516064,1.43E-06,6.86E-06,9.22E-06
1348,question_generation1,310,This method selects the argmax sentence .,D Ablation Analysis,Sampling vs Argmax,question_generation,1,11,1,0,,0.000398039,0,negative,0.00048755,5.65E-06,0.003474981,1.97E-07,4.74E-07,1.11E-05,1.77E-06,2.14E-05,0.000149846,0.995815975,7.94E-06,1.37E-05,9.35E-06
1349,question_generation1,311,"Also , we evaluated our method by sampling from the probability distributions and provide the results for our proposed MDN - Joint method for VQG dataset as follows :",D Ablation Analysis,Sampling vs Argmax,question_generation,1,12,1,0,,0.000742745,0,negative,0.002000922,4.05E-07,2.98E-05,1.78E-08,1.52E-07,2.69E-06,6.24E-06,4.45E-06,5.69E-07,0.996814252,1.40E-06,0.001138082,1.02E-06
1350,question_generation1,312,D.3,D Ablation Analysis,,question_generation,1,13,1,0,,0.000220569,0,negative,0.130078969,3.06E-06,0.000700816,9.25E-05,5.87E-05,0.000333534,2.64E-05,1.51E-05,4.78E-05,0.865993961,1.17E-05,6.67E-06,0.002630779
1351,question_generation1,313,How are exemplars improving Embedding,D Ablation Analysis,D.3,question_generation,1,14,1,0,,0.000315316,0,negative,0.042289416,6.02E-05,0.000972531,8.50E-06,1.23E-05,0.000155276,7.78E-05,3.66E-05,0.000965755,0.950947892,0.003469768,3.92E-05,0.000964741
1352,question_generation1,314,"In Multimodel differential network , we use exemplars and train them using a triplet loss .",D Ablation Analysis,D.3,question_generation,1,15,1,0,,0.00181397,0,negative,0.049974098,0.000782274,0.20826335,1.47E-05,9.56E-05,0.000325339,0.000103374,8.00E-05,0.007690906,0.731441879,0.000587452,1.74E-05,0.000623589
1353,question_generation1,315,"It is known that using a triplet network , we can learn a representation that accentuates how the image is closer to a supporting exemplar as against the opposing exemplar ( Hoffer and .",D Ablation Analysis,D.3,question_generation,1,16,1,0,,3.51E-05,0,negative,0.006327912,1.20E-05,0.00044557,3.11E-06,7.07E-06,5.80E-05,8.28E-06,9.31E-06,0.000153577,0.991957534,0.000888381,3.43E-06,0.000125849
1354,question_generation1,316,The Joint embedding is obtained between the image and language representations .,D Ablation Analysis,D.3,question_generation,1,17,1,0,,0.003332371,0,negative,0.017290044,0.002892144,0.017774742,1.98E-06,3.92E-05,0.000123091,3.23E-05,0.000116693,0.181230763,0.780356734,5.90E-05,4.54E-06,7.88E-05
1355,question_generation1,317,Therefore the improved representation helps in obtaining an improved context vector .,D Ablation Analysis,D.3,question_generation,1,18,1,0,,0.000497272,0,negative,0.262717636,1.52E-05,0.000600416,5.19E-07,1.42E-05,1.43E-05,9.74E-06,3.23E-06,0.000183334,0.73639221,6.22E-06,3.38E-05,9.18E-06
1356,question_generation1,318,Further we show that this also results in improving VQG .,D Ablation Analysis,D.3,question_generation,1,19,1,0,,0.0002894,0,ablation-analysis,0.59715777,3.49E-05,0.000771849,2.30E-07,5.66E-06,8.09E-06,2.53E-05,2.19E-06,0.000215052,0.401646952,1.08E-05,0.000110569,1.07E-05
1357,question_generation1,319,D.4,D Ablation Analysis,,question_generation,1,20,1,0,,0.000186775,0,negative,0.086122102,3.23E-06,0.000634627,5.67E-05,5.47E-05,0.000375459,3.15E-05,2.05E-05,4.86E-05,0.909488457,5.59E-06,5.82E-06,0.003152725
1358,question_generation1,320,Are exemplars required ?,D Ablation Analysis,D.4,question_generation,1,21,1,0,,7.33E-06,0,negative,0.003073662,8.16E-06,3.78E-05,2.21E-06,7.21E-06,8.20E-05,9.14E-06,2.60E-05,7.52E-05,0.996559623,2.31E-05,1.48E-06,9.43E-05
1359,question_generation1,321,We had similar concerns and validated this point by using random exemplars for the nearest neighbor for MDN .,D Ablation Analysis,D.4,question_generation,1,22,1,0,,1.08E-05,0,negative,0.002686902,3.09E-06,8.87E-05,1.59E-06,1.55E-05,7.08E-05,1.12E-05,9.24E-06,1.42E-05,0.996995402,2.36E-05,1.98E-06,7.77E-05
1360,question_generation1,322,( k=R in table,D Ablation Analysis,D.4,question_generation,1,23,1,0,,5.52E-06,0,negative,0.016814583,1.89E-06,0.000438425,4.68E-08,2.11E-06,1.16E-05,6.50E-06,2.02E-06,2.07E-05,0.982690581,1.19E-06,5.94E-06,4.43E-06
1361,question_generation1,323,6 ) In this case the method is similar to the baseline .,D Ablation Analysis,D.4,question_generation,1,24,1,0,,1.08E-05,0,negative,0.111936662,7.12E-06,0.006176692,1.74E-07,9.91E-06,2.18E-05,2.90E-05,2.33E-06,3.01E-05,0.88172936,2.73E-06,4.02E-05,1.39E-05
1362,question_generation1,324,"This suggests that with random exemplar , the model learns to ignore the cue .",D Ablation Analysis,D.4,question_generation,1,25,1,0,,2.93E-05,0,negative,0.038494187,1.74E-05,0.001088302,3.80E-07,8.04E-06,3.03E-05,9.18E-06,8.12E-06,0.000541517,0.959774218,2.06E-06,3.19E-06,2.31E-05
1363,question_generation1,325,D.5,D Ablation Analysis,,question_generation,1,26,1,0,,0.000163279,0,negative,0.077233138,3.52E-06,0.000591347,4.29E-05,4.99E-05,0.000378027,4.05E-05,2.93E-05,5.25E-05,0.917337257,3.28E-06,6.09E-06,0.004232291
1364,question_generation1,326,Are captions necessary for our method ?,D Ablation Analysis,D.5,question_generation,1,27,1,0,,2.86E-06,0,negative,0.002463754,7.10E-06,4.31E-05,5.69E-06,1.97E-05,0.000125147,1.04E-05,2.40E-05,3.78E-05,0.997081403,8.94E-06,1.44E-06,0.000171452
1365,question_generation1,327,This is not actually necessary .,D Ablation Analysis,D.5,question_generation,1,28,1,0,,1.82E-06,0,negative,0.001820537,4.80E-07,1.02E-05,1.51E-06,1.25E-05,1.80E-05,1.29E-06,2.07E-06,3.12E-06,0.998117126,8.70E-07,5.08E-07,1.18E-05
1366,question_generation1,328,"In our method , we have used an existing image captioning method to generate captions for images that did not have them .",D Ablation Analysis,D.5,question_generation,1,29,1,0,,0.000418899,0,negative,0.047734922,0.000912802,0.022228953,1.76E-05,0.001576181,0.000199772,0.000102929,2.68E-05,0.000636392,0.925624463,0.000135508,3.24E-05,0.000771295
1367,question_generation1,329,"For VQG dataset , captions were available and we have used that , but , for VQA dataset captions were not available and we have generated captions while training .",D Ablation Analysis,D.5,question_generation,1,30,1,0,,0.000160178,0,negative,0.012037621,4.92E-06,0.000647876,3.93E-06,0.002362713,5.45E-05,3.90E-05,1.91E-06,2.12E-06,0.984728398,2.58E-06,1.96E-05,9.49E-05
1368,question_generation1,330,We provide detailed evidence with respect to caption - question pairs to ensure that we are generating novel questions .,D Ablation Analysis,D.5,question_generation,1,31,1,0,,0.000131784,0,negative,0.011378026,1.25E-05,0.000125271,2.10E-05,0.002919804,5.95E-05,8.50E-06,3.72E-06,1.12E-05,0.985408321,4.14E-07,4.22E-06,4.74E-05
1369,question_generation1,331,"While the caption generates scene description , our proposed method generates semantically meaningful and novel questions .",D Ablation Analysis,D.5,question_generation,1,32,1,0,,0.005847451,0,negative,0.313446926,0.000295493,0.002759164,2.31E-06,0.000445883,3.15E-05,0.0001012,6.92E-06,0.00036069,0.682006459,2.29E-05,0.000250858,0.000269725
1370,question_generation1,332,Examples for Figure 1 of main paper :,D Ablation Analysis,D.5,question_generation,1,33,1,0,,1.64E-06,0,negative,0.000543052,9.02E-07,7.64E-05,5.11E-07,9.43E-06,1.83E-05,1.85E-06,1.45E-06,1.87E-05,0.999309037,6.91E-07,5.47E-07,1.91E-05
1371,question_generation1,333,First Image : - Caption -,D Ablation Analysis,D.5,question_generation,1,34,1,0,,0.000142153,0,negative,0.007993925,2.19E-06,0.000231554,1.67E-05,0.000257658,0.000162563,2.46E-05,6.99E-06,2.26E-05,0.990804574,1.09E-06,6.32E-06,0.000469218
1372,question_generation1,334,A young man skateboarding around little cones .,D Ablation Analysis,D.5,question_generation,1,35,1,0,,6.37E-06,0,negative,0.005109432,1.99E-06,0.000257373,3.97E-05,0.00036071,0.00020893,1.10E-05,4.20E-06,8.51E-06,0.993345583,3.84E-06,2.12E-06,0.000646638
1373,question_generation1,335,Our Question - Is this a skateboard competition ?,D Ablation Analysis,D.5,question_generation,1,36,1,0,,2.04E-06,0,negative,0.001352862,6.01E-06,0.000109834,2.77E-05,0.000111945,0.00012675,1.01E-05,1.25E-05,2.22E-05,0.997573785,3.01E-05,1.19E-06,0.000615051
1374,question_generation1,336,Second Image : - Caption - A small child is standing on a pair of skis .,D Ablation Analysis,D.5,question_generation,1,37,1,0,,7.77E-05,0,negative,0.019995651,8.85E-06,0.002044045,1.40E-05,0.001153777,0.000163559,0.000139051,6.51E-06,4.21E-05,0.973797598,3.05E-06,4.12E-05,0.002590642
1375,question_generation1,337,Our Question : - How old is that little girl ?,D Ablation Analysis,D.5,question_generation,1,38,1,0,,3.09E-06,0,negative,0.001317893,5.15E-06,9.66E-05,1.54E-05,5.31E-05,0.000156243,1.40E-05,1.61E-05,2.96E-05,0.997630036,2.84E-05,1.04E-06,0.000636474
1376,question_generation1,338,D.6 Intuition behind Triplet Network :,D Ablation Analysis,D.5,question_generation,1,39,1,0,,0.000122771,0,negative,0.171823515,2.83E-05,0.066813726,2.38E-07,9.41E-06,2.64E-05,0.000108624,2.01E-06,0.000985925,0.759976579,6.79E-06,5.24E-05,0.000166153
1377,question_generation1,339,The intuition behind use of triplet networks is clear through this paper that first advocated its use .,D Ablation Analysis,D.5,question_generation,1,40,1,0,,7.51E-06,0,negative,0.002157156,2.68E-06,9.59E-05,1.19E-06,2.47E-05,3.33E-05,3.55E-06,3.81E-06,1.63E-05,0.997619702,1.09E-06,8.90E-07,3.97E-05
1378,question_generation1,340,"The main idea is that when we learn distance functions thatare close for similar and far from dissimilar representations , it is not clear that close and far are with respect to what measure .",D Ablation Analysis,D.5,question_generation,1,41,1,0,,3.95E-06,0,negative,0.002203601,6.61E-06,0.000173536,3.78E-07,9.50E-06,1.53E-05,2.32E-06,3.06E-06,5.97E-05,0.997505472,7.25E-07,7.79E-07,1.90E-05
1379,question_generation1,341,By incorporating a triplet we learn distance functions that learn that A is more similar to B as compared to C. Learning such measures allows us to bring target image - caption joint embeddings thatare closer to supporting exemplars as compared to contrasting exemplars .,D Ablation Analysis,D.5,question_generation,1,42,1,0,,5.95E-05,0,negative,0.047157827,0.00136152,0.00495941,2.31E-06,0.000139356,4.68E-05,3.63E-05,2.43E-05,0.025813061,0.920270204,2.70E-06,1.63E-05,0.000169839
1380,question_generation1,342,E Analysis of Network E.1 Analysis of Tag Context,D Ablation Analysis,,question_generation,1,43,1,0,,0.006933646,0,negative,0.155101969,1.15E-05,0.003174741,6.32E-06,2.64E-05,0.000153442,0.000319797,1.47E-05,0.000174702,0.8164658,3.81E-05,8.42E-05,0.024428293
1381,question_generation1,343,Tag is language based context .,D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,44,1,0,,0.000576251,0,negative,0.001848332,4.40E-05,0.015155769,5.91E-06,0.000151026,0.000194026,0.000411923,4.30E-05,0.00086449,0.979239096,1.14E-05,0.000107251,0.001923756
1382,question_generation1,344,"These tags are extracted from caption , except question - tags which is fixed as the 7 ' Wh words '",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,45,1,0,,6.33E-06,0,negative,5.34E-05,2.37E-06,1.76E-05,2.50E-07,7.70E-05,3.28E-05,7.44E-06,1.57E-05,7.22E-06,0.999781831,9.67E-09,1.01E-06,3.32E-06
1383,question_generation1,345,"( What , Why ,",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,46,1,0,,1.75E-07,0,negative,8.46E-06,1.60E-07,2.03E-06,6.26E-09,3.10E-07,1.02E-06,3.58E-07,5.83E-07,3.97E-06,0.999982639,2.37E-08,2.39E-07,2.14E-07
1384,question_generation1,346,"Where , Who , When , Which and How ) .",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,47,1,0,,1.57E-06,0,negative,1.25E-05,3.13E-07,3.62E-06,5.83E-09,4.61E-07,8.73E-07,3.57E-07,5.07E-07,7.02E-06,0.999973637,2.51E-08,4.79E-07,1.79E-07
1385,question_generation1,347,"We have experimented with Noun tag , Verb tag and ' Wh - word ' tag as shown in tables .",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,48,1,0,,5.92E-05,0,negative,3.43E-05,5.09E-07,6.37E-06,1.63E-07,3.54E-05,4.22E-05,6.42E-06,8.94E-06,1.38E-06,0.999861286,4.72E-09,9.20E-07,2.15E-06
1386,question_generation1,348,"Also , we have experimented in each tag by varying the number of tags from 1 to 7 .",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,49,1,0,,1.72E-05,0,negative,0.000649421,9.08E-06,2.19E-05,7.65E-08,2.84E-05,2.66E-05,4.86E-05,2.34E-05,1.03E-05,0.99917129,6.91E-09,7.74E-06,3.12E-06
1387,question_generation1,349,"We combined different tags using 1 Dconvolution , concatenation , and addition of all the tags and observed that the concatenation mechanism gives better results .",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,50,1,0,,0.006984936,0,negative,0.00449169,4.37E-06,3.96E-05,1.99E-08,2.76E-06,5.98E-06,2.84E-05,3.49E-06,2.38E-05,0.995340003,6.13E-08,5.64E-05,3.39E-06
1388,question_generation1,350,"As we can see in the table 4 that taking Nouns , Verbs and Wh - Words as context , we achieve significant improvement in the BLEU , METEOR and CIDEr scores from the basic models which only takes the image and the caption respectively .",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,51,1,0,,0.225014507,0,negative,0.449453281,9.46E-06,0.000170725,3.83E-07,1.68E-05,2.86E-05,0.008541277,1.70E-05,5.53E-06,0.498926253,5.60E-07,0.042492459,0.000337605
1389,question_generation1,351,"Taking Nouns generated from the captions and questions of the corresponding training example as context , we achieve an increase of 1.6 % in Bleu Score and 2 % in METEOR and 34.4 % in CIDEr Score from the basic Image model .",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,52,1,0,,0.049704866,0,negative,0.418020746,3.46E-05,0.000188106,7.16E-07,4.70E-05,3.33E-05,0.003960979,2.82E-05,2.97E-05,0.563886344,2.98E-07,0.013306474,0.00046355
1390,question_generation1,352,Similarly taking Verbs as context gives us an increase of 1.3 % in Bleu Score and 2.1 % in METEOR and 33.5 % in CIDEr Score from the basic Image model .,D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,53,1,0,,0.035733136,0,ablation-analysis,0.680631154,1.05E-05,0.000168158,5.72E-07,4.46E-05,2.04E-05,0.001710372,1.13E-05,1.33E-05,0.311320987,8.48E-08,0.005863028,0.000205647
1391,question_generation1,353,And the best result comes when we take 3 Wh- Words as context and apply the Hadamard Model with concatenating the 3 WH - words .,D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,54,1,0,,0.204026216,0,negative,0.039064751,6.95E-06,7.74E-05,2.27E-07,1.11E-05,4.18E-05,0.006429119,4.00E-05,5.39E-06,0.940684657,3.55E-07,0.0133197,0.000318468
1392,question_generation1,354,Also in we have shown the results when we take more than one words as context .,D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,55,1,0,,0.000412351,0,negative,0.000323913,1.27E-07,2.94E-06,5.82E-09,1.37E-06,1.72E-06,8.50E-06,7.72E-07,2.72E-07,0.999626599,4.81E-09,3.33E-05,4.94E-07
1393,question_generation1,355,"Here we show that for 3 words i.e 3 nouns , 3 verbs and 3 Wh-words , the Concatenation model performs the best .",D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,56,1,0,,0.034549695,0,negative,0.05726572,1.77E-05,5.16E-05,3.84E-07,1.29E-05,4.59E-05,0.00243513,5.66E-05,1.98E-05,0.934919265,2.81E-07,0.004997619,0.000176996
1394,question_generation1,356,In this table the conv model is using 1 D convolution to combine the tags and the joint model combine all the tags .,D Ablation Analysis,E Analysis of Network E.1 Analysis of Tag Context,question_generation,1,57,1,0,,0.000167748,0,negative,9.79E-05,8.95E-07,2.50E-05,2.55E-08,6.14E-06,6.61E-06,7.94E-06,3.38E-06,5.03E-06,0.999842011,2.32E-09,3.75E-06,1.27E-06
1395,question_generation1,357,E.2 Analysis of Context : Exemplars,D Ablation Analysis,,question_generation,1,58,1,0,,0.002193804,0,negative,0.059301754,3.11E-06,0.000883295,3.60E-07,4.16E-06,4.34E-05,0.000375949,6.36E-06,4.42E-05,0.929386802,2.04E-06,4.57E-05,0.009902857
1396,question_generation1,358,"In Multimodel Differential Network and Differential Image Network , we use exemplar images ( target , supporting and opposing image ) to obtain the differential context .",D Ablation Analysis,E.2 Analysis of Context : Exemplars,question_generation,1,59,1,0,,0.0001361,0,negative,0.000658126,0.000540583,0.003981554,7.55E-08,5.52E-05,2.55E-05,0.000276454,8.86E-06,0.002430859,0.991966212,3.69E-07,4.21E-05,1.41E-05
1397,question_generation1,359,"We have performed the experiment based on the single exemplar ( K= 1 ) , which is one supporting and one opposing image along with target image , based on two exemplar ( K= 2 ) , i.e. two supporting and two opposing image along with single target image .",D Ablation Analysis,E.2 Analysis of Context : Exemplars,question_generation,1,60,1,0,,3.15E-06,0,negative,7.12E-05,6.63E-06,1.72E-05,6.17E-09,1.50E-05,9.51E-06,9.62E-05,3.37E-06,3.44E-06,0.999769413,3.57E-09,7.45E-06,6.62E-07
1398,question_generation1,360,similarly we have performed experiment for K=3 and K = 4 as shown in table - 6 .,D Ablation Analysis,E.2 Analysis of Context : Exemplars,question_generation,1,61,1,0,,1.08E-05,0,negative,0.000232668,4.26E-07,1.07E-06,1.50E-09,6.54E-07,4.05E-06,0.000143834,1.91E-06,3.84E-07,0.999573738,3.30E-09,4.09E-05,3.85E-07
1399,question_generation1,361,E.3 Mixture Module : Other Variations,D Ablation Analysis,,question_generation,1,62,1,0,,0.026581902,0,negative,0.300395975,3.43E-06,0.016621822,4.59E-08,2.22E-06,1.13E-05,0.000296154,8.27E-07,0.000104864,0.681710654,1.20E-07,7.03E-05,0.000782287
1400,question_generation1,362,Hadamard method uses element - wise multiplication whereas Addition method uses element - wise addition in place of the concatenation operator of the Joint method .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,63,1,0,,7.10E-05,0,negative,4.26E-05,6.45E-06,0.000551601,3.54E-08,2.02E-06,5.81E-06,4.94E-06,6.99E-06,6.14E-05,0.9993063,9.50E-08,8.19E-06,3.54E-06
1401,question_generation1,363,The Hadamard method finds a correlation between image feature and caption feature vector while the Addition method learns a resultant vector .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,64,1,0,,0.000150158,0,negative,5.01E-05,2.93E-06,0.000250041,5.53E-08,2.06E-06,4.54E-06,2.65E-06,4.63E-06,1.66E-05,0.999656423,9.23E-08,6.96E-06,2.94E-06
1402,question_generation1,364,"In the attention method , the output Si is the weighted average of attention probability vector P att and convolutional features G img .",D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,65,1,0,,6.59E-06,0,negative,2.71E-05,6.35E-06,4.58E-05,1.26E-08,8.34E-07,1.06E-06,5.37E-07,5.03E-06,6.30E-05,0.9998478,1.12E-08,2.00E-06,5.22E-07
1403,question_generation1,365,The attention probability vector weights the contribution of each convolutional feature based on the caption vector .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,66,1,0,,0.000195384,0,negative,5.66E-05,2.44E-05,6.28E-05,2.43E-08,1.53E-06,6.93E-06,3.99E-06,7.46E-05,0.000830974,0.998933667,9.80E-09,1.96E-06,2.56E-06
1404,question_generation1,366,This attention method is similar to work stack attention method .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,67,1,0,,0.000158704,0,negative,0.00010121,1.35E-05,0.005136698,1.97E-08,3.04E-06,5.41E-06,1.63E-05,7.45E-06,0.000214821,0.994475022,4.61E-08,1.85E-05,8.02E-06
1405,question_generation1,367,The attention mechanism is given by :,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,68,1,0,,2.76E-05,0,negative,1.81E-05,1.14E-06,5.17E-05,1.27E-09,1.14E-07,5.54E-07,5.18E-07,1.84E-06,6.40E-05,0.999860606,3.47E-09,1.21E-06,2.43E-07
1406,question_generation1,368,where G img is the 14x14x512 - dimensional convolution feature map from the fifth convolution layer of VGG - 19 Net of image X i and f i is the caption context vector .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,69,1,0,,1.58E-06,0,negative,8.80E-06,3.40E-07,2.81E-06,1.39E-08,1.17E-06,3.09E-06,6.66E-07,6.17E-06,8.96E-07,0.999974466,9.58E-10,1.26E-06,3.07E-07
1407,question_generation1,369,The attention probability vector P att is a 196 - dimensional vector .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,70,1,0,,8.55E-06,0,negative,1.81E-05,7.62E-06,8.31E-06,3.35E-08,2.00E-06,4.94E-05,1.54E-05,0.000431823,4.71E-05,0.999413681,5.99E-09,1.88E-06,4.69E-06
1408,question_generation1,370,"WI , WC , WP are the weights and b c , b A , b c is the bias for different layers .",D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,71,1,0,,2.44E-06,0,negative,4.69E-06,9.81E-07,1.78E-06,4.24E-09,2.17E-07,1.48E-06,4.59E-07,1.16E-05,1.15E-05,0.999966652,1.04E-09,4.54E-07,1.95E-07
1409,question_generation1,371,We evaluate the different approaches and provide results for the same .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,72,1,0,,4.49E-06,0,negative,6.86E-06,2.32E-07,9.76E-07,3.94E-09,1.14E-06,8.79E-07,8.62E-07,1.76E-06,1.56E-07,0.999980025,2.85E-10,7.00E-06,1.06E-07
1410,question_generation1,372,Here ?,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,73,1,0,,2.42E-06,0,negative,2.82E-06,1.80E-07,7.29E-07,1.28E-08,2.13E-07,3.36E-06,7.55E-07,7.48E-06,1.39E-06,0.999982009,2.13E-09,6.09E-07,4.37E-07
1411,question_generation1,373,represents element - wise addition .,D Ablation Analysis,E.3 Mixture Module : Other Variations,question_generation,1,74,1,0,,7.64E-05,0,negative,9.25E-06,1.93E-06,4.02E-05,2.29E-08,7.87E-07,5.15E-06,2.94E-06,1.22E-05,5.29E-05,0.999869508,1.16E-08,1.65E-06,3.42E-06
1412,question_generation1,374,E.4 Evaluation Metrics,,,question_generation,1,0,1,0,,0.01529416,0,negative,0.000144186,2.88E-05,7.68E-06,8.71E-08,3.24E-07,1.31E-05,3.71E-05,0.000171084,3.33E-06,0.995668684,0.00063503,0.003289897,7.17E-07
1413,question_generation1,375,Our task is similar to encoder - decoder framework of machine translation .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,1,1,0,,0.001019931,0,negative,0.002993569,0.000236542,0.020139932,0.000128996,3.30E-05,0.000344028,0.000651182,0.000554427,0.000328577,0.907816447,0.011518014,0.045734606,0.009520648
1414,question_generation1,376,we have used same evaluation metric is used in machine translation .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,2,1,0,,1.78E-05,0,negative,0.000143713,1.28E-06,0.000175831,7.13E-07,3.62E-07,4.10E-05,4.42E-05,8.69E-05,1.76E-06,0.996106393,0.000151203,0.003181752,6.48E-05
1415,question_generation1,377,is the first metric to find the correlation between generated question with ground truth question .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,3,1,0,,0.00034362,0,negative,0.003058715,1.10E-05,0.007434629,3.00E-06,3.93E-06,4.02E-05,7.61E-05,8.91E-05,1.57E-05,0.953017356,7.99E-05,0.036037543,0.000132863
1416,question_generation1,378,"BLEU score is used to measure the precision value , i.e",E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,4,1,0,,7.74E-06,0,negative,0.000637877,3.24E-06,0.001324419,4.19E-07,5.28E-07,2.13E-05,1.72E-05,8.81E-05,5.28E-06,0.993927508,1.06E-05,0.00394361,1.99E-05
1417,question_generation1,379,That is how much words in the predicted question is appeared in reference question .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,5,1,0,,2.64E-06,0,negative,0.00023455,1.03E-06,0.000151427,2.44E-07,2.97E-07,7.75E-06,3.70E-06,3.58E-05,3.43E-06,0.998908129,2.40E-06,0.000641691,9.55E-06
1418,question_generation1,380,BLEU -n score measures the n-gram precision for counting cooccurrence on reference sentences .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,6,1,0,,0.001228235,0,negative,0.000281159,5.06E-06,0.003678833,1.05E-06,1.30E-06,7.22E-05,0.000170824,0.000189378,4.89E-06,0.980497962,0.000107561,0.014770912,0.000218882
1419,question_generation1,381,we have evaluated BLEU score from n is 1 to 4 .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,7,1,0,,2.02E-05,0,negative,0.000399655,3.26E-06,0.000185175,1.07E-06,9.02E-07,0.000180318,0.000103539,0.000888654,3.73E-06,0.993348616,1.26E-05,0.004772435,0.000100027
1420,question_generation1,382,"The mechanism of score is similar to BLEU - n , where as , it measures recall value instead of precision value in BLEU .",E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,8,1,0,,1.63E-05,0,negative,0.001095328,1.04E-05,0.034012317,5.09E-07,8.10E-07,2.17E-05,2.11E-05,6.75E-05,5.88E-05,0.962605798,9.95E-06,0.002064439,3.13E-05
1421,question_generation1,383,That is how much words in the reference question is appeared in predicted question .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,9,1,0,,2.63E-06,0,negative,0.000179482,7.79E-07,8.54E-05,1.84E-07,2.61E-07,4.99E-06,2.41E-06,2.57E-05,2.71E-06,0.999190199,1.28E-06,0.000499565,7.02E-06
1422,question_generation1,384,"Another version ROUGE metric is ROUGE - L , which measures longest common sub - sequence present in the generated question .",E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,10,1,0,,0.000147443,0,negative,0.000326243,5.53E-06,0.011677594,2.85E-06,1.70E-06,0.000186456,0.000381586,0.000351449,9.52E-06,0.973423528,0.000177748,0.012419348,0.001036452
1423,question_generation1,385,"METEOR ( Banerjee and Lavie , 2005 ) score is another useful evaluation metric to calculate the similarity between generated question with reference one by considering synonyms , stemming and paraphrases .",E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,11,1,0,,0.000357638,0,negative,0.000436833,7.17E-06,0.001756855,3.33E-06,1.58E-06,9.48E-05,0.000160892,0.000280773,1.27E-05,0.981851796,0.000327091,0.01421848,0.000847707
1424,question_generation1,386,the output of the METEOR score measure the word matches between predicted question and reference question .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,12,1,0,,0.000279598,0,negative,0.001696895,3.09E-06,0.003914302,4.12E-07,1.45E-06,1.88E-05,1.72E-05,4.59E-05,1.50E-05,0.987494697,1.84E-06,0.006760815,2.96E-05
1425,question_generation1,387,"In VQG , it compute the word match score between predicted question with five reference question .",E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,13,1,0,,0.000142623,0,negative,0.003665938,2.80E-05,0.200161406,3.66E-06,5.84E-06,7.69E-05,0.000257977,0.000120172,5.16E-05,0.760021065,0.000222734,0.034533456,0.00085127
1426,question_generation1,388,CI Der score is a consensus based evaluation metric .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,14,1,0,,3.81E-05,0,negative,0.001433561,1.45E-05,0.04830379,3.62E-06,3.37E-06,5.63E-05,0.000133083,0.000133439,4.11E-05,0.931091499,0.000181128,0.017889921,0.000714732
1427,question_generation1,389,"It measure human - likeness , that is the sentence is written by human or not .",E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,15,1,0,,1.58E-05,0,negative,0.000847713,7.09E-06,0.003379333,3.33E-06,5.28E-06,6.65E-05,6.54E-05,0.000154009,1.58E-05,0.988233183,4.03E-05,0.006677749,0.000504346
1428,question_generation1,390,"The consensus is measured , how often n-grams in the predicted question are appeared in the reference question .",E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,16,1,0,,7.55E-06,0,negative,0.000470218,3.06E-06,0.000389703,5.40E-07,1.65E-06,1.15E-05,8.90E-06,5.35E-05,6.76E-06,0.997161325,1.52E-06,0.001868136,2.32E-05
1429,question_generation1,391,If the n-grams in the predicted question sentence is appeared more frequently in reference question then question is less informative and have low CIDer score .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,17,1,0,,1.30E-05,0,negative,0.004712679,9.26E-07,0.000358334,1.39E-07,5.49E-07,3.91E-06,1.07E-05,1.60E-05,1.76E-06,0.970976302,8.53E-07,0.023907573,1.03E-05
1430,question_generation1,392,We provide our results using all these metrics and compare it with existing baselines .,E.4 Evaluation Metrics,E.4 Evaluation Metrics,question_generation,1,18,1,0,,1.40E-05,0,negative,0.0002945,2.64E-06,3.52E-05,5.43E-07,1.53E-06,1.17E-05,1.57E-05,5.53E-05,1.98E-06,0.994784433,5.20E-07,0.004782489,1.35E-05
1431,natural_language_inference44,1,title,,,natural_language_inference,44,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
1432,natural_language_inference44,2,Efficient and Robust Question Answering from Minimal Context over Documents,title,title,natural_language_inference,44,1,1,1,research-problem,0.995250278,1,research-problem,2.44E-08,6.61E-06,3.32E-08,8.99E-08,5.90E-08,5.83E-08,8.09E-07,8.08E-07,3.55E-07,0.001529952,0.998461038,1.22E-07,4.39E-08
1433,natural_language_inference44,3,abstract,,,natural_language_inference,44,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
1434,natural_language_inference44,4,Neural models for question answering ( QA ) over documents have achieved significant performance improvements .,abstract,abstract,natural_language_inference,44,1,1,1,research-problem,0.977014785,1,research-problem,1.07E-08,1.83E-06,1.04E-08,7.58E-08,3.67E-08,3.16E-08,2.90E-07,2.63E-07,9.56E-08,0.00170179,0.998295518,2.31E-08,2.13E-08
1435,natural_language_inference44,5,"Although effective , these models do not scale to large corpora due to their complex modeling of interactions between the document and the question .",abstract,abstract,natural_language_inference,44,2,1,0,,0.015329494,0,research-problem,3.66E-07,6.69E-05,1.07E-07,5.86E-06,4.00E-06,3.65E-06,1.17E-06,1.60E-05,3.93E-06,0.252705636,0.747192035,1.58E-07,1.34E-07
1436,natural_language_inference44,6,"Moreover , recent work has shown that such models are sensitive to adversarial inputs .",abstract,abstract,natural_language_inference,44,3,1,0,,0.005009013,0,research-problem,3.44E-07,8.11E-05,5.36E-08,1.29E-05,4.09E-06,4.13E-06,6.69E-07,2.12E-05,7.01E-06,0.363823522,0.636044717,8.51E-08,1.17E-07
1437,natural_language_inference44,7,"In this paper , we study the minimal context required to answer the question , and find that most questions in existing datasets can be answered with a small set of sentences .",abstract,abstract,natural_language_inference,44,4,1,0,,0.424771994,0,research-problem,5.50E-06,0.040761279,6.98E-06,1.42E-05,0.000233165,1.32E-05,8.46E-06,0.000209642,0.000501312,0.422841623,0.535400423,3.27E-06,9.52E-07
1438,natural_language_inference44,8,"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .",abstract,abstract,natural_language_inference,44,5,1,1,research-problem,0.366405977,0,approach,4.37E-05,0.525096236,0.000280168,2.11E-05,0.00058797,4.49E-05,1.70E-05,0.000492351,0.13941864,0.225717483,0.108267077,9.49E-06,3.90E-06
1439,natural_language_inference44,9,"Our over all system achieves significant reductions in training ( up to 15 times ) and inference times ( up to 13 times ) , with accuracy comparable to or better than the state - of - the - art on SQuAD , News QA , Trivia QA and SQuAD - Open .",abstract,abstract,natural_language_inference,44,6,1,0,,0.019379278,0,research-problem,2.91E-05,0.004194419,1.10E-05,0.000111359,0.000588462,7.18E-05,0.000329456,0.000639737,7.66E-05,0.368635968,0.624992197,0.000297491,2.24E-05
1440,natural_language_inference44,10,"Furthermore , our experimental results and analyses show that our approach is more robust to adversarial inputs .",abstract,abstract,natural_language_inference,44,7,1,0,,0.004144871,0,negative,0.000383581,0.004000897,3.48E-06,2.55E-05,0.000122092,3.27E-05,5.89E-05,0.000475223,0.000116386,0.904567121,0.089914475,0.000296511,3.05E-06
1441,natural_language_inference44,11,Introduction,,,natural_language_inference,44,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
1442,natural_language_inference44,12,"The task of textual question answering ( QA ) , in which a machine reads a document and answers a question , is an important and challenging problem in natural language processing .",Introduction,Introduction,natural_language_inference,44,1,1,0,,0.965522958,1,research-problem,9.95E-07,0.00010504,3.37E-07,1.85E-05,2.92E-05,4.42E-06,2.28E-05,3.89E-06,1.69E-05,0.020764727,0.979028334,1.47E-06,3.38E-06
1443,natural_language_inference44,13,Recent progress in performance of QA models has been largely due to the variety of available QA datasets .,Introduction,Introduction,natural_language_inference,44,2,1,0,,0.705387494,1,research-problem,5.33E-07,0.000114629,1.11E-07,1.20E-06,2.44E-06,3.12E-06,6.74E-06,7.09E-06,5.57E-05,0.049132518,0.950674186,9.77E-07,7.90E-07
1444,natural_language_inference44,14,*,Introduction,Introduction,natural_language_inference,44,3,1,0,,0.004351972,0,negative,1.19E-05,0.001432414,3.09E-06,3.67E-06,6.96E-05,0.000125879,4.32E-05,0.000131918,0.00736616,0.957103538,0.033700984,5.73E-06,1.98E-06
1445,natural_language_inference44,15,All work was done while the author was an intern at Salesforce Research .,Introduction,Introduction,natural_language_inference,44,4,1,0,,0.016276169,0,negative,3.42E-05,0.003966342,8.75E-06,0.009092343,0.020138984,0.001960264,0.000211452,0.000205196,0.000818144,0.919014731,0.044515923,7.98E-06,2.57E-05
1446,natural_language_inference44,16,"Many neural QA models have been proposed for these datasets , the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question .",Introduction,Introduction,natural_language_inference,44,5,1,0,,0.264144197,0,research-problem,8.10E-07,0.000345167,4.00E-07,2.49E-06,5.22E-06,1.30E-05,1.23E-05,2.26E-05,0.000253129,0.116059136,0.883283412,1.21E-06,1.15E-06
1447,natural_language_inference44,17,"Yet , learning the full context over the document is challenging and inefficient .",Introduction,Introduction,natural_language_inference,44,6,1,0,,0.060078464,0,research-problem,2.99E-06,0.000364446,5.24E-07,9.57E-05,9.00E-05,3.43E-05,1.90E-05,2.41E-05,6.55E-05,0.249322932,0.749974495,2.06E-06,3.95E-06
1448,natural_language_inference44,18,"In particular , when the model is given along document , or multiple documents , learning the full context is intractably slow and hence difficult to scale to large corpora .",Introduction,Introduction,natural_language_inference,44,7,1,0,,0.048834279,0,research-problem,6.58E-06,0.001676716,1.31E-06,3.78E-05,6.35E-05,7.61E-05,3.49E-05,0.000115882,0.000670504,0.367722107,0.629582718,5.62E-06,6.27E-06
1449,natural_language_inference44,19,"In addition , show that , given adversarial inputs , such models tend to focus on wrong parts of the context and produce incorrect answers .",Introduction,Introduction,natural_language_inference,44,8,1,0,,0.007130403,0,negative,0.000174263,0.012754675,1.38E-05,3.28E-05,0.001712972,7.30E-05,4.49E-05,6.55E-05,0.004261834,0.970097057,0.010718916,4.79E-05,2.37E-06
1450,natural_language_inference44,20,"In this paper , we aim to develop a QA system that is scalable to large documents as well as robust to adversarial inputs .",Introduction,Introduction,natural_language_inference,44,9,1,1,model,0.937482042,1,research-problem,1.98E-05,0.096281461,3.08E-05,2.06E-05,0.000599414,3.33E-05,0.000127697,0.000108796,0.017175846,0.099129633,0.786414738,4.11E-05,1.68E-05
1451,natural_language_inference44,21,"First , we study the context required to answer the question by sampling examples in the dataset and carefully analyzing them .",Introduction,Introduction,natural_language_inference,44,10,1,1,model,0.527918325,1,approach,0.000249003,0.45235744,0.000121965,0.000557348,0.206742263,0.000339571,0.00015596,0.000250274,0.021470221,0.316068609,0.001624257,4.94E-05,1.37E-05
1452,natural_language_inference44,22,"We find that most questions can be answered using a few sentences , without the consideration of context over entire document .",Introduction,Introduction,natural_language_inference,44,11,1,0,,0.153965442,0,negative,0.000292534,0.024336621,1.06E-05,0.000246312,0.005266715,0.000582569,0.001266182,0.001046126,0.001864372,0.915583549,0.048901984,0.000535915,6.65E-05
1453,natural_language_inference44,23,"In particular , we observe that on the SQuAD dataset , 92 % of answerable questions can be answered using a single sentence .",Introduction,Introduction,natural_language_inference,44,12,1,0,,0.304987489,0,negative,0.000503216,0.022669567,2.15E-05,0.000215824,0.005572354,0.000496408,0.004471314,0.000906561,0.002019615,0.903361009,0.056745894,0.002859452,0.0001573
1454,natural_language_inference44,24,"Second , inspired by this observation , we propose a sentence selector to select the minimal set of sentences to give to the QA model in order to answer the question .",Introduction,Introduction,natural_language_inference,44,13,1,1,model,0.949720772,1,model,2.19E-05,0.15920778,0.000106712,1.56E-06,0.00032087,1.51E-05,1.23E-05,2.67E-05,0.830072027,0.009594937,0.000613892,4.89E-06,1.33E-06
1455,natural_language_inference44,25,"Since the minimum number of sentences depends on the question , our sentence selector chooses a different number of sentences for each question , in contrast with previous models that select a fixed number of sentences .",Introduction,Introduction,natural_language_inference,44,14,1,1,model,0.867757541,1,model,1.47E-05,0.307338981,4.27E-05,8.46E-07,0.000357603,1.50E-05,1.25E-05,4.77E-05,0.671899471,0.01963231,0.000632529,4.85E-06,8.60E-07
1456,natural_language_inference44,26,"Our sentence selector leverages three simple techniques - weight transfer , data modification and score normalization , which we show to be highly effective on the task of sentence selection .",Introduction,Introduction,natural_language_inference,44,15,1,1,model,0.839764448,1,approach,0.000180386,0.682558567,0.000522978,5.01E-05,0.008459498,0.000177753,0.000308019,0.000220635,0.267201195,0.037156779,0.003042684,9.17E-05,2.97E-05
1457,natural_language_inference44,27,We compare the standard QA model given the full document Writers whose papers are in the library are as diverse as Charles Dickens and The papers of which famous English Victorian Beatrix Potter .,Introduction,Introduction,natural_language_inference,44,16,1,0,,0.118669602,0,approach,2.50E-05,0.730096811,0.000211672,8.61E-06,0.003525248,0.000194064,0.000495313,0.000544193,0.10472975,0.148767329,0.011288281,9.46E-05,1.91E-05
1458,natural_language_inference44,28,Illuminated manuscripts in the library dating from author are collected in the library ?,Introduction,Introduction,natural_language_inference,44,17,1,0,,0.00399551,0,negative,3.26E-06,0.001581722,1.42E-06,1.54E-05,0.00027366,0.00010969,2.55E-05,9.72E-05,0.002724808,0.987427756,0.007734424,3.47E-06,1.71E-06
1459,natural_language_inference44,29,Task analyses,Introduction,,natural_language_inference,44,18,1,0,,0.137356995,0,negative,2.69E-05,0.011322202,7.72E-06,2.31E-05,0.001324256,0.000505948,0.000822345,0.000487576,0.005071473,0.956800072,0.023478818,0.00011287,1.67E-05
1460,natural_language_inference44,30,Existing QA models focus on learning the context over different parts in the full document .,Introduction,Task analyses,natural_language_inference,44,19,1,0,,0.028513868,0,negative,6.18E-05,2.24E-05,1.30E-05,8.29E-06,4.93E-06,3.42E-05,0.000140327,2.11E-05,2.20E-05,0.965980664,0.033510988,0.000131706,4.86E-05
1461,natural_language_inference44,31,"Although effective , learning the context within the full document is challenging and inefficient .",Introduction,Task analyses,natural_language_inference,44,20,1,0,,0.000637301,0,negative,6.88E-05,2.85E-06,1.74E-06,3.44E-06,2.12E-06,9.03E-06,2.49E-05,4.24E-06,3.77E-06,0.998362712,0.001404499,0.000107141,4.77E-06
1462,natural_language_inference44,32,"Consequently , we study the minimal context in the document required to answer the question .",Introduction,Task analyses,natural_language_inference,44,21,1,0,,1.16E-05,0,negative,2.88E-05,4.77E-06,3.28E-06,1.41E-08,5.42E-07,4.21E-07,5.33E-07,7.89E-07,3.52E-05,0.999912316,1.37E-06,1.19E-05,3.27E-08
1463,natural_language_inference44,33,Human studies,Introduction,,natural_language_inference,44,22,1,0,,0.069360226,0,negative,9.50E-05,0.010789862,4.88E-05,2.48E-05,0.001446278,0.000520728,0.000672022,0.000516324,0.018124659,0.962618434,0.004989872,0.000128446,2.48E-05
1464,natural_language_inference44,34,"First , we randomly sample 50 examples from the SQuAD development set , and analyze the minimum number of sentences required to answer the question , as shown in .",Introduction,Human studies,natural_language_inference,44,23,1,0,,0.000876871,0,negative,0.000183169,7.91E-05,8.15E-06,1.31E-07,1.29E-05,2.27E-06,6.66E-06,1.36E-05,1.92E-05,0.99933211,1.85E-06,0.000340662,1.19E-07
1465,natural_language_inference44,35,We observed that 98 % of questions are answerable given the document .,Introduction,Human studies,natural_language_inference,44,24,1,0,,0.004708585,0,negative,2.68E-05,3.63E-06,1.57E-06,2.34E-06,6.64E-05,8.69E-06,6.60E-06,7.50E-06,3.89E-06,0.99979615,5.43E-06,7.01E-05,8.57E-07
1466,natural_language_inference44,36,The remaining 2 % of questions are not answerable even given the entire document .,Introduction,Human studies,natural_language_inference,44,25,1,0,,0.000469721,0,negative,0.000174887,4.68E-06,4.32E-06,1.32E-06,8.04E-05,5.11E-06,4.57E-06,4.97E-06,3.10E-06,0.999635349,8.43E-07,8.02E-05,2.43E-07
1467,natural_language_inference44,37,"For instance , in the last example in , the question requires the background knowledge that Charles Dickens is an English Victorian author .",Introduction,Human studies,natural_language_inference,44,26,1,0,,1.84E-06,0,negative,3.13E-06,7.10E-07,4.18E-07,2.82E-06,9.94E-06,5.55E-06,4.99E-07,2.90E-06,3.43E-06,0.99996741,1.91E-06,1.18E-06,9.77E-08
1468,natural_language_inference44,38,"Among the answerable examples , 92 % are answerable with a single sentence , 6 % with two sentences , and 2 % with three or more sentences .",Introduction,Human studies,natural_language_inference,44,27,1,0,,0.006435337,0,negative,2.03E-05,3.94E-06,2.22E-06,2.24E-06,0.00020972,6.02E-06,5.09E-06,5.61E-06,1.92E-06,0.9996928,1.02E-06,4.88E-05,3.68E-07
1469,natural_language_inference44,39,We perform a similar analysis on the TriviaQA ( Wikipedia ) development ( verified ) set .,Introduction,Human studies,natural_language_inference,44,28,1,0,,0.000181785,0,negative,3.51E-05,7.08E-05,3.83E-05,1.04E-07,2.21E-05,1.53E-06,2.94E-06,5.78E-06,2.87E-05,0.999706333,9.18E-07,8.73E-05,1.02E-07
1470,natural_language_inference44,40,"Finding the sentences to answer the question on TriviaQA is more challenging than on SQuAD , since Triv - ia QA documents are much longer than SQuAD documents ( 488 vs 5 sentences per document ) .",Introduction,Human studies,natural_language_inference,44,29,1,0,,0.003276569,0,negative,2.84E-05,1.49E-05,2.73E-06,1.52E-06,1.16E-05,5.70E-06,3.09E-05,1.14E-05,4.40E-06,0.998739355,0.000832128,0.000313827,3.12E-06
1471,natural_language_inference44,41,"Nevertheless , we find that most examples are answerable with one or two sentences - among the 88 % of examples thatare answerable given the full document , 95 % can be answered with one or two sentences .",Introduction,Human studies,natural_language_inference,44,30,1,0,,0.003935639,0,negative,8.99E-05,4.90E-06,2.94E-06,2.00E-06,7.87E-05,6.69E-06,8.67E-06,6.99E-06,2.47E-06,0.999568825,1.53E-06,0.00022593,4.54E-07
1472,natural_language_inference44,42,Analyses on existing QA model,Introduction,Human studies,natural_language_inference,44,31,1,0,,0.000166957,0,negative,8.53E-05,2.05E-05,1.93E-05,4.16E-08,1.49E-06,1.34E-06,2.21E-05,6.44E-06,1.74E-05,0.998514554,0.000124795,0.001186052,6.50E-07
1473,natural_language_inference44,43,"Given that the majority of examples are answerable with a single oracle sentence on SQuAD , we analyze the performance of an existing , competitive QA model when it is given the oracle sentence .",Introduction,Human studies,natural_language_inference,44,32,1,0,,5.02E-05,0,negative,3.18E-05,0.001973578,8.86E-05,6.69E-07,4.95E-05,7.63E-06,2.00E-05,5.42E-05,0.000434548,0.996985352,8.01E-05,0.000272142,1.85E-06
1474,natural_language_inference44,44,"We train DCN + , one of the state - of - the - art models on SQuAD ( details in Section 3.1 ) , on the oracle sentence .",Introduction,Human studies,natural_language_inference,44,33,1,0,,0.008400296,0,negative,0.000123427,0.007974075,0.001147334,5.17E-06,0.000277065,0.000153634,0.000281866,0.001119661,0.002423649,0.986183101,2.05E-05,0.000271824,1.87E-05
1475,natural_language_inference44,45,The model achieves 83.1 F1 when trained and evaluated using the full document and 85.1 F1 when trained and evaluated using the oracle sentence .,Introduction,Human studies,natural_language_inference,44,34,1,0,,0.027282101,0,negative,0.004246166,0.000284678,6.77E-05,2.80E-06,5.41E-05,3.16E-05,0.000796061,0.000305795,0.000154124,0.938810444,1.43E-05,0.055197647,3.45E-05
1476,natural_language_inference44,46,We analyze 50 randomly sampled examples in which the model fails on exact match ( EM ) despite using the oracle sentence .,Introduction,Human studies,natural_language_inference,44,35,1,0,,0.000571995,0,negative,6.22E-05,0.000159669,1.66E-05,7.96E-07,0.000300589,6.73E-06,7.44E-06,3.08E-05,2.75E-05,0.999328162,3.02E-07,5.87E-05,4.16E-07
1477,natural_language_inference44,47,"We classify these errors into 4 categories , as shown in .",Introduction,Human studies,natural_language_inference,44,36,1,0,,6.18E-05,0,negative,6.11E-06,2.71E-06,2.10E-06,3.38E-08,2.62E-06,1.43E-06,4.20E-07,5.30E-06,1.31E-05,0.999960131,7.09E-08,6.01E-06,3.11E-08
1478,natural_language_inference44,48,"In these examples , we observed that 40 % of questions are answerable given the oracle sentence but the model unexpectedly fails to find the answer .",Introduction,Human studies,natural_language_inference,44,37,1,0,,0.000137732,0,negative,5.30E-06,8.38E-07,3.99E-07,7.03E-07,1.73E-05,3.73E-06,1.29E-06,3.59E-06,7.96E-07,0.999957218,4.22E-07,8.26E-06,1.61E-07
1479,natural_language_inference44,49,"58 % are those in which the model 's prediction is correct but does not lexically match the groundtruth answer , as shown in the first example in .",Introduction,Human studies,natural_language_inference,44,38,1,0,,0.001055845,0,negative,0.000212127,5.38E-06,6.98E-06,4.44E-06,0.000252434,9.37E-06,6.76E-06,6.02E-06,3.46E-06,0.999393673,2.25E-07,9.85E-05,6.29E-07
1480,natural_language_inference44,50,2 % are those in which the question is not answerable even given the full document .,Introduction,Human studies,natural_language_inference,44,39,1,0,,0.000147045,0,negative,2.82E-05,4.36E-06,8.56E-06,7.48E-08,9.79E-06,2.19E-06,1.55E-06,6.13E-06,7.57E-06,0.999908093,8.37E-08,2.33E-05,8.36E-08
1481,natural_language_inference44,51,"In addition , we compare predictions by the model trained using the full document ( FULL ) with the model trained on the oracle sentence ( ORACLE ) .",Introduction,Human studies,natural_language_inference,44,40,1,0,,0.0001948,0,negative,7.25E-06,1.79E-05,2.05E-05,9.38E-09,2.06E-06,9.74E-07,1.92E-06,5.56E-06,2.55E-05,0.999892354,8.80E-08,2.59E-05,3.56E-08
1482,natural_language_inference44,52,shows the Venn diagram of the questions answered correctly by FULL and ORACLE on SQuAD and News QA .,Introduction,Human studies,natural_language_inference,44,41,1,0,,1.33E-05,0,negative,4.68E-06,3.66E-06,2.95E-06,7.80E-08,1.63E-06,5.62E-06,6.16E-06,2.01E-05,1.91E-05,0.999903804,1.49E-06,3.02E-05,5.45E-07
1483,natural_language_inference44,53,"ORACLE is able to answer 93 % and 86 % of the questions correctly answered by FULL on SQuAD and NewsQA , respectively .",Introduction,Human studies,natural_language_inference,44,42,1,0,,0.029520462,0,negative,0.000487865,9.46E-05,0.000129526,1.21E-06,4.61E-05,2.25E-05,0.000858456,8.83E-05,5.07E-05,0.958118242,1.43E-05,0.040052117,3.61E-05
1484,natural_language_inference44,54,"These experiments and analyses indicate that if the model can accurately predict the oracle sentence , the model should be able to achieve comparable performance on over all QA task .",Introduction,Human studies,natural_language_inference,44,43,1,0,,0.000274239,0,negative,0.00014746,3.02E-06,1.40E-06,1.17E-08,6.03E-07,5.95E-07,8.13E-06,4.07E-06,4.97E-06,0.998475637,5.44E-07,0.001353434,1.34E-07
1485,natural_language_inference44,55,"Therefore , we aim to create an effective , efficient and robust QA system which only requires a single or a few sentences to answer the question .",Introduction,Human studies,natural_language_inference,44,44,1,0,,0.00130791,0,negative,1.18E-05,0.00013004,7.25E-06,2.65E-06,8.95E-06,1.24E-05,1.78E-05,8.15E-05,0.000115875,0.998195823,0.001339636,6.40E-05,1.21E-05
1486,natural_language_inference44,56,Method,,,natural_language_inference,44,0,1,0,,0.000211124,0,negative,4.12E-05,0.000349831,8.47E-06,1.11E-05,3.83E-06,0.0002083,4.56E-05,0.00372844,0.000354333,0.992353364,0.00284456,4.13E-05,9.63E-06
1487,natural_language_inference44,57,Our over all architecture ( ) consists of a sentence selector and a QA model .,Method,Method,natural_language_inference,44,1,1,0,,0.080305807,0,negative,0.000463371,0.166929022,0.005509559,0.000110075,0.000992835,0.001256525,0.000128974,0.005749554,0.205297176,0.608165381,0.005203556,0.000123785,7.02E-05
1488,natural_language_inference44,58,The sentence selector computes a selection score for each sentence in parallel .,Method,Method,natural_language_inference,44,2,1,0,,0.123493738,0,negative,0.000108512,0.125487777,0.006717913,1.01E-06,4.28E-05,0.000156384,1.74E-05,0.001555374,0.375778586,0.488532721,0.001553986,4.36E-05,3.91E-06
1489,natural_language_inference44,59,We give to the QA model a reduced set of sentences with high selection scores to answer the question .,Method,Method,natural_language_inference,44,3,1,0,,0.018951222,0,negative,0.000226052,0.13495536,0.000386617,1.98E-05,0.001902796,0.000189424,3.01E-05,0.002091772,0.011562363,0.847494959,0.000994659,0.000139595,6.53E-06
1490,natural_language_inference44,60,Neural Question Answering Model,,,natural_language_inference,44,0,1,0,,0.170061135,0,research-problem,7.78E-06,6.68E-05,4.08E-05,2.26E-06,6.65E-07,3.35E-05,0.001099787,0.000295242,1.85E-05,0.053825758,0.943382278,0.001169942,5.67E-05
1491,natural_language_inference44,61,We study two neural QA models that obtain close to state - of - the - art performance on SQuAD .,Neural Question Answering Model,Neural Question Answering Model,natural_language_inference,44,1,1,0,,0.006298973,0,negative,9.73E-05,0.000175506,0.002315871,1.06E-06,1.75E-06,0.000148989,0.006726232,0.000277998,0.000137024,0.97851449,0.000875871,0.010702474,2.54E-05
1492,natural_language_inference44,62,DCN + is one of the start -,Neural Question Answering Model,Neural Question Answering Model,natural_language_inference,44,2,1,0,,1.55E-06,0,negative,3.04E-05,1.52E-06,0.000644057,6.39E-08,1.03E-07,3.10E-05,0.000139958,2.32E-05,0.000110468,0.998843556,2.07E-06,0.000173016,5.96E-07
1493,natural_language_inference44,63,Sentence Selector,Neural Question Answering Model,,natural_language_inference,44,3,1,0,,0.000697213,0,negative,3.48E-05,5.23E-06,0.000226398,1.82E-06,6.97E-07,0.000194896,0.003647,0.000158209,0.000162238,0.989655743,0.000566978,0.005268312,7.77E-05
1494,natural_language_inference44,64,Our sentence selector scores each sentence with respect to the question in parallel .,Neural Question Answering Model,Sentence Selector,natural_language_inference,44,4,1,0,,5.12E-05,0,negative,3.89E-05,1.42E-05,0.002743961,9.12E-08,1.34E-07,9.97E-06,1.58E-05,4.38E-05,0.000983688,0.995934522,7.24E-06,0.000205413,2.21E-06
1495,natural_language_inference44,65,The score indicates whether the question is answerable with this sentence .,Neural Question Answering Model,Sentence Selector,natural_language_inference,44,5,1,0,,1.06E-06,0,negative,1.95E-06,3.27E-07,1.26E-06,2.36E-08,8.31E-09,9.18E-06,5.01E-06,0.000100637,9.12E-06,0.999811884,9.27E-07,5.94E-05,2.53E-07
1496,natural_language_inference44,66,The model architecture is divided into the encoder module and the decoder module .,Neural Question Answering Model,Sentence Selector,natural_language_inference,44,6,1,0,,0.00079866,0,negative,0.000407682,4.07E-05,0.000845233,1.45E-05,1.64E-06,0.000124904,0.000108167,0.0003088,0.004824922,0.992882402,6.12E-05,0.00031971,6.00E-05
1497,natural_language_inference44,67,"The encoder is a shared module with S - Reader , which computes sentence encodings and question encodings from the sentence and the question as inputs .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,7,1,0,,0.000674306,0,negative,0.000105573,4.40E-05,0.001237717,1.64E-06,5.45E-07,6.81E-05,8.01E-05,0.000367221,0.004230809,0.993664466,2.85E-05,0.000146264,2.51E-05
1498,natural_language_inference44,68,"First , the encoder computes sentence embeddings D ?",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,8,1,0,,3.23E-06,0,negative,8.76E-06,5.78E-07,3.42E-05,4.87E-09,5.62E-09,1.21E-06,1.80E-06,1.12E-05,1.69E-05,0.999835569,3.01E-07,8.94E-05,7.24E-08
1499,natural_language_inference44,69,"Rh d L d , question embeddings Q ?",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,9,1,0,,5.13E-07,0,negative,3.93E-06,6.28E-08,3.79E-06,5.54E-08,1.15E-08,2.67E-06,3.48E-06,7.54E-06,1.86E-06,0.999851866,9.23E-07,0.000123492,3.23E-07
1500,natural_language_inference44,70,"Rh d Lq , and question - aware sentence embeddings D q ?",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,10,1,0,,1.84E-06,0,negative,2.39E-06,7.64E-08,1.84E-06,1.75E-08,4.23E-09,2.04E-06,2.80E-06,1.29E-05,1.29E-06,0.999867793,1.10E-06,0.000107611,1.66E-07
1501,natural_language_inference44,71,"Rh d L d , where h d is the dimension of word embeddings , and L d and L q are the sequence length of the document and the question , respectively .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,11,1,0,,6.07E-07,0,negative,1.82E-05,4.59E-07,6.43E-06,9.59E-08,3.92E-08,5.45E-06,7.26E-06,3.70E-05,6.15E-06,0.999643867,1.16E-06,0.000273344,5.79E-07
1502,natural_language_inference44,72,"Specifically , question - aware sentence embeddings are obtained as follows .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,12,1,0,,9.58E-06,0,negative,2.69E-05,1.05E-05,0.000128551,1.46E-08,3.33E-08,1.49E-06,2.81E-06,2.18E-05,0.000289247,0.999396197,8.90E-07,0.000121263,2.65E-07
1503,natural_language_inference44,73,"Here , Di ?",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,13,1,0,,1.48E-07,0,negative,1.26E-06,3.62E-08,6.50E-07,2.98E-09,1.43E-09,7.32E-07,6.43E-07,5.07E-06,1.17E-06,0.999967422,4.07E-08,2.29E-05,2.10E-08
1504,natural_language_inference44,74,Rh d is the hidden state of sentence embedding for the i th word and,Neural Question Answering Model,Sentence Selector,natural_language_inference,44,14,1,0,,2.90E-07,0,negative,3.94E-06,2.60E-07,5.87E-06,3.31E-08,1.25E-08,2.49E-06,2.49E-06,2.22E-05,1.91E-05,0.999901417,3.24E-07,4.16E-05,2.87E-07
1505,natural_language_inference44,75,"Here , ' ; ' denotes the concatenation of two vectors , and h is a hyperparameter of the hidden dimension .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,15,1,0,,9.54E-07,0,negative,3.89E-06,6.49E-07,2.49E-06,7.01E-08,1.59E-08,9.55E-06,3.84E-06,0.000144967,2.87E-05,0.99978626,3.23E-07,1.89E-05,3.49E-07
1506,natural_language_inference44,76,"Next , the decoder is a task - specific module which computes the score for the sentence by calculating bilinear similarities between sentence encodings and question encodings as follows .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,16,1,0,,7.83E-05,0,negative,4.45E-05,1.12E-05,0.000611268,1.23E-07,1.37E-07,7.76E-06,1.43E-05,4.67E-05,0.001761804,0.997389711,3.56E-06,0.000103921,5.02E-06
1507,natural_language_inference44,77,"Here , w ? Rh , W 2 ?",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,17,1,0,,2.72E-07,0,negative,3.32E-05,3.79E-07,9.28E-06,4.30E-08,3.26E-08,6.33E-06,1.06E-05,3.64E-05,5.31E-06,0.999418286,2.09E-07,0.000479531,3.23E-07
1508,natural_language_inference44,78,"R hhh , W 3 ?",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,18,1,0,,2.13E-06,0,negative,1.05E-05,1.11E-07,6.68E-06,2.76E-08,1.47E-08,3.61E-06,7.26E-06,1.44E-05,2.79E-06,0.999624684,2.74E-07,0.000329355,3.50E-07
1509,natural_language_inference44,79,"R h2 , are trainable weight matrices .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,19,1,0,,2.33E-05,0,negative,2.01E-05,5.90E-07,1.43E-05,4.30E-08,3.83E-08,4.15E-06,7.71E-06,4.25E-05,7.88E-06,0.999600844,1.88E-07,0.000301191,4.40E-07
1510,natural_language_inference44,80,Each dimension in score means the question is answerable or nonanswerable given the sentence .,Neural Question Answering Model,Sentence Selector,natural_language_inference,44,20,1,0,,4.58E-07,0,negative,2.77E-06,2.26E-07,1.93E-06,3.64E-08,2.12E-08,8.15E-06,4.43E-06,6.48E-05,9.45E-06,0.999849817,9.44E-08,5.80E-05,3.11E-07
1511,natural_language_inference44,81,We introduce 3 techniques to train the model .,Neural Question Answering Model,Sentence Selector,natural_language_inference,44,21,1,0,,1.84E-05,0,negative,0.000199975,2.54E-06,8.80E-05,5.90E-07,4.56E-07,1.08E-05,1.69E-05,4.19E-05,3.72E-05,0.998984457,2.42E-07,0.000614437,2.54E-06
1512,natural_language_inference44,82,"( i ) As the encoder module of our model is identical to that of S - Reader , we transfer the weights to the encoder module from the QA model trained on the single oracle sentence ( ORACLE ) .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,22,1,0,,1.15E-05,0,negative,0.000593474,1.39E-05,0.000742043,1.74E-07,4.72E-07,5.38E-06,3.74E-05,4.01E-05,7.83E-05,0.995788369,5.20E-07,0.002697687,2.12E-06
1513,natural_language_inference44,83,"( ii ) We modify the training data by treating a sentence as a wrong sentence if the QA model gets 0 F1 , even if the sentence is the oracle sentence .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,23,1,0,,9.46E-05,0,negative,0.000377567,7.17E-06,0.000243458,5.37E-08,1.89E-07,3.28E-06,1.20E-05,3.48E-05,7.54E-05,0.998525439,1.30E-07,0.000719858,6.44E-07
1514,natural_language_inference44,84,"( iii ) After we obtain the score for each sentence , we normalize scores across sentences from the same paragraph , similar to Clark and Gardner ( 2017 ) .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,24,1,0,,1.45E-06,0,negative,3.14E-05,1.50E-06,0.000243149,1.33E-08,4.48E-08,2.08E-06,6.43E-06,1.16E-05,2.84E-05,0.999477724,7.62E-08,0.000197278,2.76E-07
1515,natural_language_inference44,85,"All of these three techniques give substantial improvements in sentence selection accuracy , as shown in .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,25,1,0,,0.06556504,0,results,0.000424085,1.36E-07,7.28E-06,6.32E-08,5.75E-08,2.47E-06,0.000876744,1.28E-05,1.52E-07,0.105375464,6.10E-07,0.893281568,1.86E-05
1516,natural_language_inference44,86,More details including hyperparameters and training procedures are shown in Appendix A.,Neural Question Answering Model,Sentence Selector,natural_language_inference,44,26,1,0,,2.33E-07,0,negative,3.14E-06,1.45E-07,4.28E-07,1.19E-07,2.17E-08,4.85E-06,2.45E-06,4.92E-05,2.67E-06,0.999894907,1.94E-08,4.19E-05,1.64E-07
1517,natural_language_inference44,87,"Because the minimal set of sentences required to answer the question depends on the question , we select the set of sentences by thresholding the sentence scores , where the threshold is a hyperparameter ( details in Appendix A ) .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,27,1,0,,1.03E-06,0,negative,5.09E-06,1.48E-06,3.53E-06,2.14E-08,3.86E-08,4.98E-06,4.44E-06,0.000157856,1.60E-05,0.999768732,3.32E-08,3.75E-05,2.94E-07
1518,natural_language_inference44,88,"This method allows the model to select a variable number of sentences for each question , as opposed to a fixed number of sentences for all questions .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,28,1,0,,9.95E-07,0,negative,1.12E-05,2.65E-06,3.56E-05,1.42E-08,3.66E-08,1.39E-06,2.47E-06,1.87E-05,6.48E-05,0.999798201,8.80E-08,6.45E-05,2.60E-07
1519,natural_language_inference44,89,"Also , by controlling the threshold , the number of sentences can be dynamically controlled during the inference .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,29,1,0,,5.39E-05,0,negative,3.84E-05,2.02E-06,5.50E-06,5.16E-08,5.42E-08,2.07E-06,5.00E-06,4.06E-05,5.33E-05,0.999490546,1.44E-07,0.000361574,7.38E-07
1520,natural_language_inference44,90,"We define Dyn ( for Dynamic ) as this method , and define Top k as the method which simply selects the top -k sentences for each question .",Neural Question Answering Model,Sentence Selector,natural_language_inference,44,30,1,0,,8.82E-07,0,negative,1.88E-06,8.72E-07,5.42E-05,1.74E-08,3.19E-08,5.06E-06,1.29E-05,5.12E-05,1.54E-05,0.999800547,1.17E-07,5.70E-05,7.55E-07
1521,natural_language_inference44,91,Experiments,,,natural_language_inference,44,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
1522,natural_language_inference44,92,Dataset and Evaluation Metrics,,,natural_language_inference,44,0,1,0,,0.003033344,0,negative,2.23E-05,3.58E-05,1.34E-05,1.53E-06,2.67E-06,5.03E-05,0.000229447,0.000449504,8.73E-06,0.991051636,0.00693258,0.001192516,9.60E-06
1523,natural_language_inference44,93,We train and evaluate our model on five different datasets as shown in .,Dataset and Evaluation Metrics,Dataset and Evaluation Metrics,natural_language_inference,44,1,1,0,,0.01134374,0,negative,7.86E-05,7.31E-06,0.000294608,8.55E-08,5.08E-07,4.65E-05,0.002025151,0.000191189,1.51E-06,0.994259857,7.78E-06,0.003084748,2.10E-06
1524,natural_language_inference44,94,SQuAD,Dataset and Evaluation Metrics,,natural_language_inference,44,2,1,0,,0.917110271,1,experiments,0.000634362,1.53E-05,0.020313082,0.000108394,3.21E-05,0.006785316,0.51279095,0.002852413,3.60E-05,0.426001343,0.000432934,0.027516003,0.00248191
1525,natural_language_inference44,95,"( Rajpurkar et al. , 2016 ) is a wellstudied QA dataset on Wikipedia articles that requires each question to be answered from a paragraph .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,3,1,0,,0.01212796,0,negative,0.000140641,4.59E-06,0.00248763,0.000103242,0.001046434,0.00270234,0.378611318,2.79E-05,1.93E-06,0.610899982,3.49E-05,0.001370549,0.002568597
1526,natural_language_inference44,96,"News QA ) is a dataset on news articles that also provides a paragraph for each question , but the paragraphs are longer than those in SQuAD .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,4,1,0,,0.035047834,0,experiments,5.95E-05,2.13E-06,0.000766387,8.52E-05,0.000226093,0.001912425,0.675590963,1.97E-05,7.54E-07,0.3085863,0.000357986,0.001608046,0.010784565
1527,natural_language_inference44,97,"Trivia QA SQuAD - Adversarial ( Jia and Liang , 2017 ) is a variant of SQ uAD .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,5,1,0,,0.014140035,0,experiments,5.47E-05,1.13E-05,0.224433007,1.02E-06,3.27E-06,0.001629406,0.638525703,0.000107964,1.35E-05,0.133222211,4.70E-05,0.000946786,0.001004087
1528,natural_language_inference44,98,"It shares the same training set as SQuAD , but an adversarial sentence is added to each paragraph in a subset of the development set .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,6,1,0,,0.000788027,0,negative,0.000407611,9.55E-06,0.082224026,1.19E-06,2.40E-05,0.001389264,0.316233031,6.56E-05,1.87E-05,0.597817423,2.62E-06,0.001621135,0.000185817
1529,natural_language_inference44,99,We use accuracy ( Acc ) and mean average precision ( MAP ) to evaluate sentence selection .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,7,1,0,,0.014965453,0,negative,6.44E-05,9.34E-06,0.000622512,3.09E-07,7.10E-06,0.000526712,0.088603777,8.98E-05,3.81E-06,0.909086414,3.75E-06,0.000925018,5.71E-05
1530,natural_language_inference44,100,We also measure the average number of selected sentences ( N sent ) to compare the efficiency of our Dyn method and the Top k method .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,8,1,0,,0.003924776,0,negative,7.35E-05,8.81E-07,0.000107313,2.40E-08,8.55E-07,0.000115635,0.018910849,1.50E-05,6.51E-07,0.980275797,2.15E-07,0.00049571,3.58E-06
1531,natural_language_inference44,101,"To evaluate the performance in the task of question answering , we measure F1 and EM ( Exact Match ) , both being standard metrics for evaluating span - based QA .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,9,1,0,,0.004125785,0,negative,4.37E-05,5.14E-06,0.000171986,1.45E-07,2.85E-06,0.000236781,0.03424025,4.40E-05,2.11E-06,0.964549648,2.34E-06,0.000680801,2.03E-05
1532,natural_language_inference44,102,"In addition , we measure training speed ( Train Sp ) and inference speed ( Infer Sp ) relative to the speed of standard QA model ( FULL ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,10,1,0,,0.020627107,0,negative,0.000748492,7.80E-05,0.001332766,1.36E-06,3.22E-05,0.000584778,0.145310525,9.76E-05,3.29E-05,0.848117422,3.32E-06,0.003553493,0.000107143
1533,natural_language_inference44,103,"The speed is measured using a single GPU ( Tesla K80 ) , and includes the training and inference time for the sentence selector .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,11,1,0,,0.050324251,0,experiments,1.95E-05,2.48E-06,0.000154926,1.75E-05,1.09E-05,0.097785472,0.491121924,0.001261159,5.14E-06,0.408932551,3.28E-06,0.000120341,0.000564848
1534,natural_language_inference44,104,SQuAD and NewsQA,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,12,1,0,,0.820345907,1,experiments,4.83E-05,3.30E-07,0.000163356,2.03E-05,2.29E-05,0.002059104,0.87688014,2.76E-05,4.14E-07,0.109523655,7.67E-06,0.002758345,0.008487842
1535,natural_language_inference44,105,"For each QA model , we experiment with three types of inputs .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,13,1,0,,0.022043769,0,negative,2.36E-05,4.44E-06,0.000103688,1.20E-07,3.66E-06,0.001067883,0.124562955,0.000178164,3.22E-06,0.873766023,3.90E-07,0.000257037,2.89E-05
1536,natural_language_inference44,106,"First , we use the full document ( FULL ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,14,1,0,,0.011325011,0,negative,7.45E-05,4.67E-07,0.000457215,1.15E-07,6.18E-06,0.000153836,0.017213748,6.83E-06,5.62E-07,0.981531787,1.08E-07,0.000546735,7.90E-06
1537,natural_language_inference44,107,"Next , we give the model the oracle sentence containing the groundtruth answer span ( ORACLE ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,15,1,0,,0.003079287,0,negative,7.30E-06,1.62E-06,0.000295741,3.51E-08,4.70E-07,0.000163944,0.003551517,2.29E-05,4.47E-05,0.995882946,4.33E-07,2.13E-05,6.98E-06
1538,natural_language_inference44,108,"Finally , we select sentences using our sentence selector ( MINIMAL ) , using both Top k and Dyn .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,16,1,0,,0.174161593,0,negative,0.000565979,3.62E-05,0.024629977,4.86E-07,1.61E-05,0.001830459,0.29709577,0.000166946,6.40E-05,0.67443653,6.34E-07,0.001026606,0.000130337
1539,natural_language_inference44,109,"We also compare this last method with TF - IDF method for sentence selection , which selects sentences using n-gram TF - IDF distance between each sentence and the question .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,17,1,0,,0.691565848,1,negative,0.00012648,3.60E-05,0.012116958,3.20E-07,1.05E-05,0.000719619,0.382739092,6.83E-05,1.61E-05,0.601604468,2.84E-06,0.002425437,0.000133867
1540,natural_language_inference44,110,"Results shows results in the task of sentence selection on SQuAD and New s QA . First , our selector outperforms TF - IDF method and the previous state - of - the - art by large margin ( up to 2.9 % MAP ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,18,1,1,experiments,0.936591865,1,experiments,0.000787028,8.30E-07,9.90E-05,7.35E-08,8.76E-07,5.46E-05,0.735694951,8.64E-06,1.65E-07,0.110389292,1.20E-06,0.152721024,0.000242299
1541,natural_language_inference44,111,"Second , our three training techniques - weight transfer , data modification and score normalization - improve performance by up to 5.6 % MAP .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,19,1,1,experiments,0.961859043,1,experiments,0.013472104,1.66E-06,0.000110388,1.29E-06,4.96E-06,0.000162443,0.808117993,2.71E-05,5.62E-07,0.037975127,5.59E-07,0.138403505,0.001722289
1542,natural_language_inference44,112,"Finally , our Dyn method achieves higher accuracy with less sentences than the Top k method .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,20,1,1,experiments,0.88964332,1,experiments,0.001135394,5.11E-07,2.99E-05,2.02E-07,1.06E-06,6.23E-05,0.795291361,1.63E-05,1.07E-07,0.02990647,6.04E-07,0.17291911,0.00063671
1543,natural_language_inference44,113,"For example , on SQuAD , Top 2 achieves 97.2 accuracy , whereas Dyn achieves 99.3 accuracy with FastQA , which are the model leveraging sentence selection for question answering , and the published state - of - the - art models on SQuAD and NewsQA , respectively .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,21,1,0,,0.202735339,0,negative,5.42E-05,2.15E-07,7.78E-05,5.54E-07,5.21E-06,0.000384669,0.360600934,2.25E-05,1.51E-07,0.623814359,2.66E-06,0.01404679,0.000989917
1544,natural_language_inference44,114,a Numbers on the test set .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,22,1,0,,0.000215379,0,negative,2.77E-06,5.11E-08,1.30E-05,1.86E-09,2.46E-08,8.61E-05,0.003841657,1.12E-05,7.13E-07,0.996021792,5.03E-08,2.13E-05,1.26E-06
1545,natural_language_inference44,115,1.9 sentences per example .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,23,1,0,,0.010931078,0,negative,0.000128388,4.94E-07,7.64E-05,3.83E-07,3.00E-05,0.000132413,0.03887293,9.32E-06,3.92E-07,0.958457037,2.11E-07,0.002196036,9.60E-05
1546,natural_language_inference44,116,"On News QA , Top 4 achieves 92.5 accuracy , whereas Dyn achieves 94.6 accuracy with 3.9 sentences per example .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,24,1,1,experiments,0.66320339,1,experiments,0.000249093,5.22E-07,0.000131519,7.72E-08,1.60E-06,5.07E-05,0.807711873,7.76E-06,6.61E-08,0.074783266,6.39E-07,0.11651967,0.000543206
1547,natural_language_inference44,117,shows that the number of sentences selected by Dyn method vary substantially on both SQuAD and News QA .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,25,1,0,,0.02026549,0,negative,0.001705921,4.81E-07,4.92E-05,8.92E-08,1.54E-06,5.93E-05,0.423941754,1.29E-05,9.91E-08,0.438230417,9.18E-07,0.135832562,0.000164843
1548,natural_language_inference44,118,"This shows that Dyn chooses a different number of sentences depending on the question , which reflects our intuition .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,26,1,0,,0.000361274,0,negative,0.000112761,6.34E-07,4.72E-05,2.10E-08,4.13E-07,5.10E-05,0.007340693,8.45E-06,3.73E-06,0.992272269,5.31E-08,0.000158824,3.91E-06
1549,natural_language_inference44,119,shows results in the task of QA on SQuAD and News QA .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,27,1,0,,0.197024588,0,negative,9.50E-05,3.19E-07,0.000436787,1.68E-07,3.85E-06,8.29E-05,0.466639678,4.57E-06,1.44E-07,0.497232299,4.24E-06,0.034770712,0.00072937
1550,natural_language_inference44,120,MINIMAL is more efficient in training and inference than FULL .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,28,1,0,,0.344475641,0,experiments,0.000828973,1.88E-06,0.000411593,2.70E-07,3.06E-06,0.000241611,0.846403926,2.85E-05,1.06E-06,0.127841513,4.32E-07,0.023647501,0.00058967
1551,natural_language_inference44,121,"On SQuAD , S - Reader achieves 6.7 training and 3.6 inference speedup on SQuAD , and 15.0 training and 6.9 inference speedup on News QA .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,29,1,1,experiments,0.846790903,1,experiments,0.000111542,5.49E-07,2.67E-05,2.82E-07,1.38E-06,0.000218625,0.965924725,3.29E-05,2.70E-07,0.023272225,1.27E-07,0.008997451,0.001413198
1552,natural_language_inference44,122,"In addition to the speedup , MINIMAL achieves comparable result to FULL ( using S-Reader , 79.9 vs 79.8 F1 on SQuAD and 63.8 vs 63.2 F1 on NewsQA ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,30,1,0,,0.848015924,1,experiments,0.000269839,1.53E-07,3.88E-05,3.59E-08,4.95E-07,3.91E-05,0.840883093,5.44E-06,4.81E-08,0.061062001,8.02E-08,0.097366579,0.000334375
1553,natural_language_inference44,123,We compare the predictions from FULL and MINIMAL in How many casualties did British get ?,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,31,1,0,,0.000501458,0,negative,3.43E-06,1.79E-07,4.19E-05,3.48E-08,1.02E-06,0.000131692,0.005576784,8.29E-06,5.33E-07,0.994184222,3.42E-08,4.44E-05,7.53E-06
1554,natural_language_inference44,124,"This book , which influenced the thought of Charles Darwin , successfully promoted the doctrine of uniformitarianism .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,32,1,0,,0.001234068,0,negative,1.08E-05,5.15E-07,2.04E-05,1.79E-06,5.64E-06,0.000612825,0.009144751,2.84E-05,2.42E-06,0.989993996,1.44E-06,4.57E-05,0.000131308
1555,natural_language_inference44,125,This theory states that slow geological processes have occurred throughout the Earth 's history and are still occurring today .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,33,1,0,,0.000319013,0,negative,1.23E-05,1.64E-06,0.000127797,1.73E-06,4.27E-06,0.001436766,0.015089167,0.000107657,2.71E-05,0.982946414,1.98E-06,4.55E-05,0.000197663
1556,natural_language_inference44,126,"In contrast , catastrophism is the theory that Earth 's features formed in single , catastrophic events and remained unchanged thereafter .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,34,1,0,,0.000231595,0,negative,2.26E-05,9.61E-07,0.000181151,1.22E-06,7.80E-06,0.000674462,0.01452085,3.16E-05,5.53E-06,0.984344692,9.67E-07,9.05E-05,0.000117654
1557,natural_language_inference44,127,"Which theory states that slow geological processes are still occuring today , and have occurred throughout Earth 's history ? :",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,35,1,0,,0.000213839,0,negative,3.37E-06,1.86E-07,1.15E-05,1.59E-07,3.95E-07,0.000206994,0.00277744,1.54E-05,1.89E-06,0.996950683,3.60E-07,1.70E-05,1.46E-05
1558,natural_language_inference44,128,Examples on SQuAD .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,36,1,0,,0.135784362,0,negative,1.03E-05,8.91E-08,3.89E-05,2.30E-08,3.20E-07,0.000164396,0.157564622,1.08E-05,1.60E-07,0.840434002,1.27E-07,0.00169982,7.64E-05
1559,natural_language_inference44,129,"Grountruth span ( underlined text ) , the prediction from FULL ( blue text ) and MINIMAL ( red text ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,37,1,0,,0.000575079,0,negative,1.45E-05,1.73E-07,8.60E-05,6.15E-08,6.55E-07,0.000663462,0.089374382,3.25E-05,1.61E-06,0.909541471,7.67E-08,0.000202841,8.22E-05
1560,natural_language_inference44,130,Sentences selected by our selector is denoted with .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,38,1,0,,0.00051304,0,negative,6.85E-07,2.71E-07,6.16E-06,1.22E-08,1.64E-07,0.000163259,0.004421785,3.91E-05,2.07E-06,0.995353195,2.12E-08,6.91E-06,6.33E-06
1561,natural_language_inference44,131,"In the above two examples , MINIMAL correctly answer the question by selecting the oracle sentence .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,39,1,0,,0.000161382,0,negative,1.43E-05,4.92E-08,1.13E-05,1.19E-08,1.11E-06,1.58E-05,0.001899382,9.26E-07,1.02E-07,0.997926959,6.67E-09,0.000127852,2.25E-06
1562,natural_language_inference44,132,"In the last example , MINIMAL fails to answer the question , since the inference over first and second sentences is required to answer the question .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,40,1,0,,0.000837737,0,negative,1.62E-05,4.41E-08,1.10E-05,2.34E-08,1.66E-06,3.28E-05,0.006771702,1.76E-06,9.88E-08,0.992947669,1.39E-08,0.000206857,1.02E-05
1563,natural_language_inference44,133,"selected sentence However , in 1883 - 84 Germany began to build a colonial empire in Africa and the South Pacific , before losing interest in imperialism .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,41,1,0,,0.000192553,0,negative,1.70E-05,1.05E-07,2.74E-05,3.46E-07,8.73E-06,8.22E-05,0.002218519,1.97E-06,3.23E-07,0.997560452,6.57E-08,6.22E-05,2.06E-05
1564,natural_language_inference44,134,"The establishment of the German colonial empire proceeded smoothly , starting with German New Guinea in 1884 .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,42,1,0,,0.003865018,0,negative,1.59E-05,4.37E-07,1.80E-05,7.08E-07,5.60E-06,0.000596593,0.022728434,3.06E-05,1.98E-06,0.976324306,1.94E-07,0.000113778,0.000163452
1565,natural_language_inference44,135,When did Germany found their first settlement ?,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,43,1,0,,6.55E-05,0,negative,1.35E-06,7.07E-08,4.44E-06,1.44E-08,1.09E-07,8.45E-05,0.004100656,9.40E-06,1.01E-06,0.995769455,7.58E-08,1.77E-05,1.12E-05
1566,natural_language_inference44,136,1883-84 1884 1884,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,44,1,0,,0.000682513,0,negative,3.46E-05,1.59E-07,5.99E-05,1.46E-07,3.64E-06,9.02E-05,0.010387167,4.69E-06,7.04E-07,0.989193427,3.88E-08,0.000163404,6.19E-05
1567,natural_language_inference44,137,"In the late 1920s , Tesla also befriended George Sylvester Viereck , a poet , writer , mystic , and later , a Nazi propagandist .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,45,1,0,,0.000599184,0,negative,7.05E-06,1.46E-07,1.75E-05,1.50E-07,4.02E-06,0.000199231,0.011820751,1.10E-05,5.35E-07,0.987813365,2.53E-08,6.00E-05,6.62E-05
1568,natural_language_inference44,138,"In middle age , Tesla became a close friend of Mark Twain ; they spent a lot of time together in his lab and elsewhere .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,46,1,0,,0.000609691,0,negative,1.08E-05,2.13E-07,3.56E-05,2.10E-07,3.66E-06,0.000316953,0.028154105,1.21E-05,8.62E-07,0.971115195,1.15E-07,0.00014234,0.000207826
1569,natural_language_inference44,139,When did Tesla become friends with Viereck ?,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,47,1,0,,6.93E-05,0,negative,1.44E-06,4.69E-08,5.54E-06,8.97E-09,9.30E-08,6.73E-05,0.006084655,6.37E-06,4.81E-07,0.993794269,4.56E-08,2.47E-05,1.50E-05
1570,natural_language_inference44,140,late 1920s middle age late 1920s :,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,48,1,0,,0.005171126,0,negative,4.45E-05,4.72E-07,6.07E-05,7.91E-07,5.66E-06,0.001037809,0.139138325,6.67E-05,3.64E-06,0.858244823,1.38E-07,0.000555065,0.000841354
1571,natural_language_inference44,141,"An example on SQuAD , where the sentences are ordered by the score from our selector .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,49,1,0,,8.40E-05,0,negative,1.33E-06,4.80E-08,1.57E-05,1.57E-08,1.90E-07,8.47E-05,0.010052508,5.00E-06,2.50E-07,0.989752957,4.41E-08,6.07E-05,2.65E-05
1572,natural_language_inference44,142,"Grountruth span ( underlined text ) , the predictions from Top 1 ( blue text ) , Top 2 ( green text ) and Dyn ( red text ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,50,1,0,,0.000681593,0,negative,1.31E-05,7.27E-08,4.35E-05,2.07E-08,7.29E-07,0.000243136,0.07107576,1.07E-05,4.06E-07,0.928299194,7.15E-09,0.000243143,7.02E-05
1573,natural_language_inference44,143,"Sentences selected by Top 1 , Top 2 and Dyn are denoted with , and , respectively .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,51,1,0,,0.000380115,0,negative,7.17E-07,2.58E-07,6.17E-06,1.04E-08,2.48E-07,0.000182453,0.007670474,3.94E-05,2.05E-06,0.992077665,5.83E-09,7.93E-06,1.26E-05
1574,natural_language_inference44,144,and the QA model correctly answers the question .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,52,1,0,,0.000598557,0,negative,5.98E-06,2.11E-08,2.38E-06,4.76E-09,1.66E-07,2.05E-05,0.008426198,2.79E-06,1.02E-07,0.991391741,6.11E-09,0.000136833,1.33E-05
1575,natural_language_inference44,145,"In the last example , our sentence selector fails to choose the oracle sentence , so the QA model can not predict the correct answer .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,53,1,0,,0.000895875,0,negative,8.57E-06,3.67E-08,5.30E-06,5.21E-08,3.42E-06,6.01E-05,0.007007676,3.33E-06,6.12E-08,0.992745779,3.48E-09,0.000134355,3.13E-05
1576,natural_language_inference44,146,"In this case , our selector chooses the second and the third sentences instead of the oracle sentence because the former contains more information relevant to question .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,54,1,0,,0.001337179,0,negative,4.88E-05,7.26E-07,9.70E-05,3.56E-09,9.85E-07,2.48E-05,0.007188891,3.87E-06,2.05E-06,0.992484048,1.19E-09,0.000143634,5.21E-06
1577,natural_language_inference44,147,"In fact , the context over the first and the second sentences is required to correctly answer the question .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,55,1,0,,5.62E-05,0,negative,1.64E-05,9.21E-08,5.38E-06,5.89E-09,6.62E-07,8.10E-06,0.000760761,1.38E-06,5.08E-07,0.999154003,1.79E-09,4.97E-05,2.92E-06
1578,natural_language_inference44,148,"shows an example on SQuAD , which MINIMAL with Dyn correctly answers the question , and MINIMAL with Top k sometimes does not .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,56,1,0,,0.000258292,0,negative,3.72E-06,2.59E-08,1.16E-05,3.11E-08,3.25E-06,6.45E-05,0.005014215,2.01E-06,6.05E-08,0.994838829,1.35E-09,4.21E-05,1.97E-05
1579,natural_language_inference44,149,"Top 1 selects one sentence in the first example , thus fails to choose the oracle sentence .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,57,1,0,,0.000893227,0,negative,0.000338655,5.99E-07,0.00027129,2.58E-08,4.01E-06,3.95E-05,0.044658835,2.92E-06,8.96E-07,0.952455625,6.60E-09,0.002163122,6.45E-05
1580,natural_language_inference44,150,"Top 2 selects two sentences in the second example , which is inefficient as well as leads to the wrong answer .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,58,1,0,,0.001671331,0,negative,0.000346316,5.13E-07,0.000258511,1.64E-08,5.16E-06,4.91E-05,0.063232116,3.46E-06,6.58E-07,0.933906081,3.53E-09,0.002131273,6.68E-05
1581,natural_language_inference44,151,"In both examples , Dyn selects the oracle sentence with minimum number of sentences , and subsequently predicts the answer .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,59,1,0,,0.00054056,0,negative,2.38E-05,4.93E-07,0.000308579,2.46E-09,8.53E-07,1.04E-05,0.005315107,1.01E-06,2.75E-06,0.994221367,1.24E-09,0.000106198,9.40E-06
1582,natural_language_inference44,152,More analyses are shown in Appendix B.,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,60,1,0,,0.000310856,0,negative,4.79E-06,6.39E-08,2.14E-06,5.44E-08,1.37E-06,6.12E-05,0.003142916,8.70E-06,1.83E-07,0.996747168,3.40E-10,2.28E-05,8.64E-06
1583,natural_language_inference44,153,Trivia QA and SQuAD - Open,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,61,1,1,experiments,0.557018038,1,experiments,7.50E-06,4.33E-08,5.00E-05,4.66E-08,1.15E-06,7.38E-05,0.960075364,2.86E-06,2.62E-08,0.033551667,7.06E-09,0.003561958,0.002675584
1584,natural_language_inference44,154,Trivia QA and SQuAD - Open are QA tasks that reason over multiple documents .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,62,1,0,,0.144097387,0,experiments,2.29E-05,5.03E-07,0.000151214,3.12E-06,3.48E-05,0.000558142,0.632351887,1.15E-05,1.15E-07,0.341180962,1.40E-06,0.003413588,0.022269858
1585,natural_language_inference44,155,They do not provide the answer span and only provide the question - answer pairs .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,63,1,0,,0.000136256,0,negative,2.46E-06,6.64E-08,1.26E-05,3.84E-08,1.66E-06,7.56E-05,0.006484742,4.62E-06,1.34E-07,0.993277433,1.03E-08,9.30E-05,4.77E-05
1586,natural_language_inference44,156,"For each QA model , we experiment with two types of inputs .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,64,1,0,,0.010921039,0,negative,6.73E-06,1.35E-06,4.60E-05,1.35E-08,7.30E-06,0.000158683,0.112705785,2.73E-05,5.89E-07,0.886803649,7.69E-10,0.000163866,7.87E-05
1587,natural_language_inference44,157,"First , since TriviaQA and SQuAD - Open have many documents for each question , we first filter paragraphs based on the TF - IDF similarities between the question and the paragraph , and then feed the full paragraphs to the QA model ( FULL ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,65,1,0,,0.006658447,0,negative,0.000158385,1.96E-05,0.004635187,2.25E-08,1.33E-05,0.000103752,0.076433444,9.74E-06,5.32E-05,0.917975801,3.57E-09,0.000363415,0.000234225
1588,natural_language_inference44,158,"On TriviaQA , we choose the top 10 paragraphs for training and inference .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,66,1,0,,0.751889025,1,experiments,8.80E-05,3.60E-06,0.000356742,3.18E-07,0.000103785,0.000335189,0.851852812,2.08E-05,3.21E-07,0.141664593,1.57E-09,0.00309326,0.002480632
1589,natural_language_inference44,159,"On SQuAD - Open , we choose the top 20 paragraphs for training and the top 40 for inferences .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,67,1,0,,0.203563643,0,experiments,2.88E-05,2.61E-06,0.000138842,1.48E-07,3.89E-05,0.000738115,0.770307113,7.19E-05,4.23E-07,0.226569677,5.88E-10,0.00057871,0.00152468
1590,natural_language_inference44,160,"Next , we use our sentence selector with Dyn ( MINIMAL ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,68,1,0,,0.163859734,0,negative,0.000200244,1.92E-06,0.002041256,7.82E-09,2.99E-06,0.00018326,0.447870351,1.38E-05,3.50E-06,0.548761065,5.76E-10,0.000675125,0.000246477
1591,natural_language_inference44,161,"We select 5 - 20 sentences using our sentence selector , from 200 sentences based on TF - IDF .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,69,1,0,,0.144211067,0,negative,1.36E-05,2.09E-06,3.76E-05,1.54E-07,3.24E-05,0.001762935,0.343654231,0.00042283,8.90E-07,0.652739711,4.63E-10,8.24E-05,0.001251087
1592,natural_language_inference44,162,"For training the sentence selector , we use two techniques described in Section 3.2 , weight transfer and score normalization , but we do not use data modification technique , since there are too many sentences to feed each of them to the QA model .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,70,1,0,,0.003676681,0,negative,6.25E-05,1.94E-05,0.001014887,7.42E-08,2.51E-05,0.00034848,0.087601556,6.39E-05,1.57E-05,0.910451132,1.12E-09,9.62E-05,0.00030116
1593,natural_language_inference44,163,"For training the QA model , we transfer the weights from the QA model trained on SQuAD , then finetune . :",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,71,1,0,,0.008251257,0,negative,8.82E-05,2.88E-06,0.000661374,1.97E-08,5.28E-06,0.00013369,0.049135401,2.21E-05,5.25E-06,0.949736762,3.00E-10,9.00E-05,0.000119058
1594,natural_language_inference44,164,Results on the dev-full set of Trivia QA ( Wikipedia ) and the dev set of SQuAD - Open .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,72,1,0,,0.031081016,0,experiments,4.54E-05,5.00E-08,2.07E-05,5.64E-10,6.49E-08,9.92E-06,0.830034731,1.12E-06,2.18E-08,0.154953816,9.72E-10,0.014828035,0.00010616
1595,natural_language_inference44,165,"Full results ( including the dev-verified set on TriviaQA ) are in Appendix C. For training FULL and MINIMAL on TriviaQA , we use 10 paragraphs and 20 sentences , respectively .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,73,1,0,,0.045209314,0,negative,1.96E-05,2.95E-07,1.95E-05,1.40E-08,9.00E-06,5.55E-05,0.140879318,6.75E-06,5.62E-08,0.857728579,1.22E-10,0.001181877,9.95E-05
1596,natural_language_inference44,166,"For training FULL and MINIMAL on SQu AD - Open , we use 20 paragraphs and 20 sentences , respectively .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,74,1,0,,0.185370138,0,experiments,9.16E-06,1.50E-06,2.46E-05,3.57E-08,1.11E-05,0.001134016,0.616888272,0.000340995,5.62E-07,0.38045655,1.84E-10,7.34E-05,0.001059775
1597,natural_language_inference44,167,"For evaluating FULL and MINIMAL , we use 40 paragraphs and 5 - 20 sentences , respectively .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,75,1,0,,0.045764959,0,negative,8.68E-06,8.38E-07,3.00E-05,2.56E-08,1.53E-05,0.000658688,0.378566743,0.000120452,3.46E-07,0.619734903,1.49E-10,0.000106445,0.000757531
1598,natural_language_inference44,168,' n sent ' indicates the number of sentences used during inference .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,76,1,0,,0.000541095,0,negative,8.72E-07,8.46E-08,4.21E-06,4.56E-09,3.89E-07,7.95E-05,0.005448168,1.52E-05,7.46E-07,0.994420738,1.45E-10,5.33E-06,2.48E-05
1599,natural_language_inference44,169,' Acc ' indicates accuracy of whether answer text is contained in selected context .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,77,1,0,,0.000636904,0,negative,1.46E-06,1.29E-07,1.36E-05,1.41E-08,1.22E-06,0.000123295,0.009940648,1.73E-05,8.77E-07,0.989803215,3.55E-10,1.98E-05,7.84E-05
1600,natural_language_inference44,170,' Sp ' indicates inference speed .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,78,1,0,,0.0010006,0,negative,2.37E-06,3.36E-07,1.62E-05,1.40E-08,9.53E-07,0.000174696,0.014009413,4.30E-05,2.76E-06,0.985659319,3.01E-10,1.16E-05,7.93E-05
1601,natural_language_inference44,171,We compare with the results from the sentences selected by TF - IDF method and our selector ( Dyn ) .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,79,1,1,experiments,0.006558782,0,negative,9.93E-06,7.74E-08,1.58E-05,4.43E-10,5.15E-07,6.03E-06,0.042138619,6.52E-07,2.60E-08,0.956846446,6.90E-11,0.000969117,1.27E-05
1602,natural_language_inference44,172,We also compare with published Rank1 - 3 models .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,80,1,1,experiments,0.001217147,0,negative,1.76E-05,1.50E-06,0.000184385,8.42E-09,3.50E-06,7.49E-05,0.064828098,5.96E-06,1.70E-06,0.934589112,3.69E-10,0.000227154,6.61E-05
1603,natural_language_inference44,173,"Results shows results on Trivia QA ( Wikipedia ) and SQuAD - Open. First , MINI - MAL obtains higher F1 and EM over FULL , with the inference speedup of up to 13.8 .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,81,1,1,experiments,0.67167236,1,experiments,0.000173038,9.73E-08,1.00E-05,1.92E-09,3.02E-07,8.94E-06,0.852589588,1.50E-06,1.69E-08,0.113992894,2.76E-10,0.032871776,0.000351831
1604,natural_language_inference44,174,"Second , the model with our sentence selector with Dyn achieves higher F1 and EM over the model with TF - IDF selector .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,82,1,1,experiments,0.919684032,1,experiments,0.001081991,8.04E-08,4.59E-06,1.26E-08,9.11E-07,1.39E-05,0.912013167,3.42E-06,8.55E-09,0.036628527,1.69E-10,0.049156224,0.001097137
1605,natural_language_inference44,175,"For example , on the development - full set , with 5 sentences per question on average , the model with Dyn achieves 59.5 F1 while the model with TF - IDF method achieves 51.9 F1 .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,83,1,0,,0.59067003,1,experiments,0.000455868,4.24E-08,4.75E-06,4.62E-09,7.26E-07,1.29E-05,0.881073133,2.85E-06,6.24E-09,0.073954598,8.75E-11,0.043742561,0.00075261
1606,natural_language_inference44,176,"Third , we outperforms the published state - of - the - art on both dataset .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,84,1,1,experiments,0.790911078,1,experiments,7.99E-05,5.92E-08,4.50E-06,5.26E-08,1.37E-06,4.36E-05,0.94009628,5.76E-06,1.18E-08,0.037301038,2.34E-10,0.020330973,0.002136442
1607,natural_language_inference44,177,SQuAD - Adversarial,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,85,1,1,experiments,0.537703948,1,experiments,6.30E-06,1.64E-07,0.000292083,1.23E-08,5.89E-07,8.13E-05,0.964364531,5.07E-06,2.39E-07,0.029676068,6.12E-10,0.000598146,0.004975487
1608,natural_language_inference44,178,We use the same settings as Section 4.2 .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,86,1,0,,0.000573846,0,negative,8.27E-06,2.11E-07,1.49E-05,1.09E-07,5.14E-06,0.000561456,0.080480868,4.62E-05,1.36E-07,0.918601627,3.48E-11,4.78E-05,0.000233263
1609,natural_language_inference44,179,"We use the model trained on SQuAD , which is exactly same as the model used for .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,87,1,0,,0.000549285,0,negative,3.69E-06,2.15E-07,8.05E-05,2.20E-08,5.15E-06,0.000648647,0.190089276,5.25E-05,2.61E-07,0.808721952,2.80E-11,3.39E-05,0.000363867
1610,natural_language_inference44,180,"For MINI - MAL , we select top 1 sentence from our sentence selector to the QA model .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,88,1,0,,0.058887831,0,negative,1.87E-05,1.60E-06,0.000145242,1.54E-08,1.08E-05,0.000338492,0.246884987,9.58E-05,1.18E-06,0.751686413,3.06E-11,7.26E-05,0.000744183
1611,natural_language_inference44,181,"Results shows that MINIMAL outperforms FULL , achieving the new state - of - the - art by large margin ( + 11.1 and + 11.5 F1 on AddSent and Add OneSent , respectively ) .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,89,1,1,experiments,0.700419427,1,experiments,0.000657885,1.76E-07,1.47E-05,4.20E-09,1.41E-06,1.16E-05,0.77313744,1.81E-06,3.25E-08,0.195903586,7.86E-11,0.029617063,0.000654266
1612,natural_language_inference44,182,compares the predictions by DCN + FULL ( blue ) and MINIMAL ( red ) .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,90,1,0,,0.00070847,0,negative,3.29E-06,2.54E-08,1.54E-05,9.22E-10,8.55E-08,4.82E-05,0.072635473,4.58E-06,2.20E-07,0.927066798,1.63E-10,7.11E-05,0.000154868
1613,natural_language_inference44,183,"While FULL selects the answer from the adversarial sentence , MINIMAL first chooses the oracle sentence , and subsequently predicts the correct answer .",Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,91,1,0,,0.000808251,0,negative,5.37E-05,1.55E-06,0.000961466,2.73E-09,3.00E-06,1.15E-05,0.023950852,1.34E-06,5.06E-06,0.97470176,1.26E-10,0.00019785,0.000111898
1614,natural_language_inference44,184,These experimental results and analyses show that our approach is effective in filtering adversarial sentences and preventing wrong predictions caused by adversarial sentences .,Dataset and Evaluation Metrics,SQuAD,natural_language_inference,44,92,1,0,,0.101954832,0,negative,0.000136218,4.11E-08,3.07E-06,1.26E-09,6.68E-07,8.46E-06,0.260951849,1.21E-06,9.42E-09,0.722846034,4.60E-11,0.015929741,0.000122706
1615,natural_language_inference44,185,Related Work,,,natural_language_inference,44,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
1616,natural_language_inference44,198,Conclusion,,,natural_language_inference,44,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
1617,natural_language_inference44,205,A Models Details,,,natural_language_inference,44,0,1,0,,0.012741382,0,negative,0.000553908,0.000112655,7.70E-05,4.25E-05,2.07E-05,0.000196464,0.000358715,0.000715902,0.000116526,0.995227976,0.001006222,0.001548214,2.32E-05
1618,natural_language_inference44,206,S - Reader,A Models Details,,natural_language_inference,44,1,1,0,,0.050022657,0,negative,0.002807987,5.19E-05,0.007627167,2.43E-05,3.13E-05,0.005047194,0.009689123,0.000929757,0.000624919,0.970558115,0.000835804,0.001258053,0.000514352
1619,natural_language_inference44,207,The model architecture of S - Reader is divided into the encoder module and the decoder module .,A Models Details,S - Reader,natural_language_inference,44,2,1,0,,0.011489312,0,negative,0.001734108,0.003486664,0.090518002,2.20E-05,0.000253692,0.002912511,0.035044014,0.000181927,0.221204729,0.640837844,0.003125199,4.62E-05,0.000633135
1620,natural_language_inference44,208,The encoder module is identical to that of our sentence selector .,A Models Details,S - Reader,natural_language_inference,44,3,1,0,,8.53E-05,0,negative,0.000502472,0.000547695,0.008909788,3.34E-06,2.19E-05,0.00214041,0.011968384,0.000144676,0.059649667,0.915787973,0.000240887,6.04E-06,7.68E-05
1621,natural_language_inference44,209,"It first takes the document and the question as inputs , obtains document embeddings D ?",A Models Details,S - Reader,natural_language_inference,44,4,1,0,,0.000368656,0,negative,0.00028655,0.000180197,0.028390634,5.23E-07,7.70E-06,0.000273604,0.005918401,1.64E-05,0.00645031,0.958274222,0.000175529,9.35E-06,1.66E-05
1622,natural_language_inference44,210,"R L d h d , question embeddings Q ?",A Models Details,S - Reader,natural_language_inference,44,5,1,0,,1.36E-05,0,negative,1.84E-05,1.13E-06,2.64E-05,2.95E-07,1.76E-06,0.000100564,0.000638999,2.23E-06,1.21E-05,0.999140076,5.34E-05,1.54E-06,3.05E-06
1623,natural_language_inference44,211,R Lqh d and question - aware document embeddings D q ?,A Models Details,S - Reader,natural_language_inference,44,6,1,0,,9.81E-05,0,negative,3.09E-05,1.06E-05,3.73E-05,4.41E-07,1.29E-06,0.00023111,0.002772157,1.81E-05,7.62E-05,0.995206223,0.001595092,6.15E-06,1.45E-05
1624,natural_language_inference44,212,"R L d h d , where D q is defined as Equation 1 , and finally obtains document encodings D enc and question encodings Q enc as Equation 3 .",A Models Details,S - Reader,natural_language_inference,44,7,1,0,,7.05E-05,0,negative,0.000346935,1.32E-05,0.002108807,1.26E-07,3.93E-06,7.68E-05,0.002914756,4.08E-06,0.000201054,0.994272191,3.77E-05,1.59E-05,4.52E-06
1625,natural_language_inference44,213,The decoder module obtains the scores for start and end position of the answer span by calculating bilinear similarities between document encodings and question encodings as follows .,A Models Details,S - Reader,natural_language_inference,44,8,1,0,,0.006738254,0,negative,0.000574291,0.000483174,0.07687882,5.79E-07,1.20E-05,0.000395642,0.004904355,2.65E-05,0.110253316,0.80632479,0.000106211,8.64E-06,3.17E-05
1626,natural_language_inference44,214,"The over all architecture is similar to Document Reader in DrQA ( Chen et al. , 2017 ) , except they are different in obtaining embeddings and use different hyperparameters .",A Models Details,S - Reader,natural_language_inference,44,9,1,0,,0.000113145,0,negative,0.000243582,0.001021898,0.08503281,3.50E-06,3.57E-05,0.003092338,0.046378625,0.000207177,0.041770502,0.821699035,0.000321549,1.37E-05,0.000179627
1627,natural_language_inference44,215,"As shown in , our S - Reader obtains F 1 score of 79.9 on SQuAD development data , while Document Reader in DrQA achieves 78.8 .",A Models Details,S - Reader,natural_language_inference,44,10,1,0,,0.116410299,0,experiments,0.014628502,9.21E-06,0.00017521,9.95E-07,1.28E-05,0.000177773,0.814926268,1.88E-05,2.63E-06,0.138994416,6.85E-05,0.030391202,0.000593725
1628,natural_language_inference44,216,Training details,,,natural_language_inference,44,0,1,0,,0.002340091,0,negative,6.05E-05,0.001338931,1.43E-05,0.000123468,5.30E-05,0.00076714,0.000215992,0.00645336,0.000307534,0.988848599,0.00167076,0.000130888,1.56E-05
1629,natural_language_inference44,217,We implement all of our models using PyTorch .,Training details,Training details,natural_language_inference,44,1,1,0,,0.989133615,1,experimental-setup,4.98E-06,2.54E-06,6.06E-06,0.00075618,4.19E-06,0.986354592,0.000857663,0.00943607,2.76E-06,0.002556281,2.47E-06,1.13E-06,1.51E-05
1630,natural_language_inference44,218,"First , the corpus is tokenized using Stanford CoreNLP toolkit .",Training details,Training details,natural_language_inference,44,2,1,0,,0.967613144,1,experimental-setup,0.001709937,0.001053965,0.003221604,0.000340813,0.002434625,0.527476215,0.009704322,0.063716394,0.000239072,0.389154454,5.28E-05,0.000737965,0.000157805
1631,natural_language_inference44,219,"We obtain the embeddings of the document and the question by concatenating 300 - dimensional Glove embeddings pretrained on the 840B Common Crawl corpus , 100 - dimensional character ngram embeddings by , and 300 - dimensional contextualized embeddings pretrained on .",Training details,Training details,natural_language_inference,44,3,1,0,,0.892978424,1,experimental-setup,6.63E-06,1.48E-05,5.67E-06,2.16E-06,1.29E-06,0.718544846,0.000687067,0.272300302,1.14E-05,0.00841286,1.31E-06,4.11E-06,7.50E-06
1632,natural_language_inference44,220,"We do not use handcraft word features such as POS and NER tagging , which is different from Document Reader in DrQA .",Training details,Training details,natural_language_inference,44,4,1,0,,0.047649845,0,negative,0.001098497,0.000939893,0.006203687,0.000164332,0.000688183,0.254920967,0.016296813,0.050773362,0.000136107,0.664066633,0.001223706,0.003149701,0.000338118
1633,natural_language_inference44,221,"Hence , the dimension of the embedding ( d h ) is 600 .",Training details,Training details,natural_language_inference,44,5,1,0,,0.963759989,1,experimental-setup,9.60E-06,1.43E-05,5.48E-06,1.70E-06,1.68E-06,0.70665867,0.001118965,0.280132421,8.79E-06,0.012024237,3.25E-06,9.39E-06,1.15E-05
1634,natural_language_inference44,222,We use the hidden size ( h ) of 200 .,Training details,Training details,natural_language_inference,44,6,1,0,,0.988988392,1,experimental-setup,3.30E-06,6.91E-06,2.07E-06,9.79E-07,3.30E-07,0.614505296,0.00060067,0.382327037,5.85E-06,0.002536256,1.44E-06,2.70E-06,7.16E-06
1635,natural_language_inference44,223,We apply dropout with 0.2 drop rate to encodings and LSTMs for regularization .,Training details,Training details,natural_language_inference,44,7,1,0,,0.994017711,1,experimental-setup,9.12E-06,1.70E-05,4.96E-06,8.12E-06,1.25E-06,0.675626845,0.000860035,0.321740629,9.06E-06,0.00170201,7.98E-07,3.13E-06,1.70E-05
1636,natural_language_inference44,224,"We train the models using ADAM optimizer ( Kingma and Ba , 2014 ) with default hyper - parameters .",Training details,Training details,natural_language_inference,44,8,1,0,,0.994373247,1,experimental-setup,2.93E-06,6.45E-06,1.80E-06,2.13E-06,4.27E-07,0.74324511,0.000413919,0.253908878,4.08E-06,0.002407541,3.83E-07,1.32E-06,5.03E-06
1637,natural_language_inference44,225,"When we train and evaluate the model on the dataset , the document is truncated to the maximum length of min ( 2000 , max ( 1000 , L th ) ) words , where L this the length which covers 90 % of documents in the whole examples .",Training details,Training details,natural_language_inference,44,9,1,0,,0.457712938,0,experimental-setup,7.19E-06,9.45E-06,4.32E-06,8.13E-07,1.37E-06,0.728427639,0.000686889,0.252562665,5.68E-06,0.018281821,1.01E-06,5.60E-06,5.55E-06
1638,natural_language_inference44,226,Selection details,Training details,,natural_language_inference,44,10,1,0,,0.008260588,0,negative,2.92E-05,4.10E-05,2.00E-05,4.81E-05,6.19E-05,0.296865771,0.000990907,0.043642881,2.90E-05,0.658163998,3.57E-05,5.09E-05,2.06E-05
1639,natural_language_inference44,227,"Here , we describe how to dynamically select sentences using Dyn method .",Training details,Selection details,natural_language_inference,44,11,1,0,,0.15651339,0,negative,0.00211182,0.016281051,0.013446438,0.000362817,0.003092894,0.198638348,0.011975413,0.025322182,0.006682618,0.717106123,0.003271927,0.001244706,0.000463664
1640,natural_language_inference44,228,"Given the sentences S all = {s 1 , s 2 , s 3 , ... , s n } , ordered by scores from the sentence selector in descending order , the selected sentences S selected is as follows .",Training details,Selection details,natural_language_inference,44,12,1,0,,0.000622085,0,negative,5.19E-05,6.64E-05,0.00014277,4.14E-06,5.52E-05,0.074297057,0.00083601,0.005967876,8.78E-05,0.918381583,5.71E-05,4.26E-05,9.49E-06
1641,natural_language_inference44,229,"Here , score ( s i ) is the score of sentence s i from the sentence selector , and this a hyperparameter between 0 and 1 .",Training details,Selection details,natural_language_inference,44,13,1,0,,0.000618695,0,negative,2.03E-05,6.31E-05,7.08E-05,4.12E-06,1.15E-05,0.402959156,0.000731669,0.049646604,0.00024925,0.546210734,1.61E-05,6.58E-06,1.01E-05
1642,natural_language_inference44,230,"The number of sentences to select can be dynamically controlled during inference by adjusting th , so that proper number of sentences can be selected depending on the needs of accuracy and speed .",Training details,Selection details,natural_language_inference,44,14,1,0,,0.140229802,0,experimental-setup,0.000272348,0.000319894,7.77E-05,3.30E-05,0.00011332,0.501135764,0.003147099,0.077926169,0.000316856,0.416398006,7.43E-05,0.000119136,6.64E-05
1643,natural_language_inference44,231,"shows the trade - off between the number of sentences and accuracy , as well as the number of selected sentences depending on the threshold th.",Training details,Selection details,natural_language_inference,44,15,1,0,,0.004673366,0,negative,0.000300818,3.87E-05,0.000107289,1.09E-05,9.89E-05,0.087242605,0.001374058,0.004844338,3.48E-05,0.90575083,2.84E-05,0.000154018,1.42E-05
1644,natural_language_inference44,232,B More Analyses,Training details,,natural_language_inference,44,16,1,0,,0.008450494,0,negative,0.000194521,1.18E-05,3.97E-05,2.95E-06,4.56E-06,0.17005561,0.004258552,0.046301385,2.07E-05,0.777733134,6.12E-05,0.001277023,3.87E-05
1645,natural_language_inference44,233,Human studies on TriviaQA,Training details,B More Analyses,natural_language_inference,44,17,1,0,,0.066567152,0,negative,0.003756714,1.34E-06,0.001457954,1.26E-07,5.04E-06,0.000191239,0.074473774,4.73E-06,1.68E-06,0.915832579,1.05E-05,0.004215382,4.89E-05
1646,natural_language_inference44,234,"We randomly sample 50 examples from the TriviaQA ( Wikipedia ) development ( verified ) set , and analyze the minimum number of sentences to answer the question .",Training details,B More Analyses,natural_language_inference,44,18,1,0,,0.001431626,0,negative,7.80E-05,1.58E-05,0.000102641,2.10E-06,0.00058257,0.001496599,0.011582182,3.79E-05,2.11E-06,0.986040935,6.98E-07,2.69E-05,3.14E-05
1647,natural_language_inference44,235,Despite Trivia,Training details,,natural_language_inference,44,19,1,0,,0.002628219,0,negative,0.000147666,9.09E-06,4.11E-05,2.27E-05,5.71E-05,0.069522801,0.00136064,0.007318949,1.46E-05,0.920661518,3.00E-05,0.000785253,2.86E-05
1648,natural_language_inference44,236,"QA having longer documents ( 488 sentences per question ) , most examples are answerable with one or two sentences , as shown in .",Training details,Despite Trivia,natural_language_inference,44,20,1,0,,0.362872786,0,experiments,0.000948653,6.54E-06,0.000521505,8.78E-07,4.56E-05,0.000589954,0.68514903,2.96E-05,9.35E-07,0.276718399,5.35E-05,0.034970325,0.000965084
1649,natural_language_inference44,237,"While 88 % of examples are answerable given the full document , 95 % of them can be answered with one or two sentences .",Training details,Despite Trivia,natural_language_inference,44,21,1,0,,0.037098172,0,negative,0.000542832,2.41E-06,6.82E-05,5.16E-06,0.000720329,0.001177496,0.054909191,2.78E-05,5.62E-07,0.939573925,3.98E-06,0.002776484,0.000191605
1650,natural_language_inference44,238,Error analyses,Training details,,natural_language_inference,44,22,1,0,,0.10243058,0,negative,0.000184324,1.40E-05,8.13E-05,8.65E-05,0.000135554,0.248144018,0.013137004,0.019033856,1.10E-05,0.717072373,8.73E-05,0.001807599,0.000205227
1651,natural_language_inference44,239,We compare the error cases ( in exact match ( EM ) ) of FULL and MINIMAL .,Training details,Error analyses,natural_language_inference,44,23,1,0,,0.010933862,0,negative,0.000293508,4.39E-06,5.29E-05,6.64E-08,2.13E-05,0.000257985,0.028601207,2.93E-06,2.26E-06,0.970733588,2.78E-07,2.48E-05,4.82E-06
1652,natural_language_inference44,240,The left - most Venn diagramin shows that MINIMAL is able to answer correctly to more than 97 % of the questions answered correctly by FULL .,Training details,Error analyses,natural_language_inference,44,24,1,0,,0.09448835,0,negative,0.013619569,6.54E-07,3.77E-05,2.12E-07,2.76E-05,0.000199421,0.373121354,1.28E-06,2.90E-07,0.611132226,8.51E-07,0.001807147,5.17E-05
1653,natural_language_inference44,241,"The other two diagrams in shows the error cases of each model , broken down by the sentence where the model 's prediction is from .",Training details,Error analyses,natural_language_inference,44,25,1,0,,6.04E-05,0,negative,7.17E-05,3.06E-07,6.88E-06,9.29E-08,1.15E-05,0.000226777,0.002984493,7.52E-07,9.32E-07,0.996693209,5.19E-08,1.79E-06,1.52E-06
1654,natural_language_inference44,242,"shows error cases on SQuAD , which MINIMAL fails to answer correctly .",Training details,Error analyses,natural_language_inference,44,26,1,0,,0.000130953,0,negative,0.000160473,7.34E-08,4.66E-06,9.91E-08,2.28E-05,8.19E-05,0.004388797,2.32E-07,6.85E-08,0.995332051,4.15E-08,6.47E-06,2.30E-06
1655,natural_language_inference44,243,In the first International Airport ?,Training details,Error analyses,natural_language_inference,44,27,1,0,,0.000354893,0,negative,3.77E-05,1.72E-07,3.97E-06,1.19E-07,6.13E-06,0.000242084,0.004376047,9.47E-07,8.07E-07,0.995325446,2.49E-07,2.10E-06,4.19E-06
1656,natural_language_inference44,244,"In 1994 , Wet Wet Wet had their biggest hit , a cover version of the troggs ' single "" Love is All Around "" , which",Training details,Error analyses,natural_language_inference,44,28,1,0,,7.18E-05,0,negative,5.28E-05,1.47E-07,8.23E-06,2.51E-07,2.02E-05,0.000159732,0.002621944,2.88E-07,4.20E-07,0.997129239,4.78E-07,1.44E-06,4.81E-06
1657,natural_language_inference44,245,"The song "" Love is All Around "" by was used on the soundtrack to the film Four Weddings and A Funeral .",Training details,Error analyses,natural_language_inference,44,29,1,0,,3.78E-05,0,negative,3.73E-05,5.41E-07,6.84E-06,6.14E-06,0.000483957,0.001236816,0.006242259,1.88E-06,4.26E-07,0.991950354,2.77E-07,1.16E-06,3.20E-05
1658,natural_language_inference44,246,Wet Wet,Training details,,natural_language_inference,44,30,1,0,,0.009511408,0,negative,0.000136711,2.66E-05,0.000116143,1.88E-05,7.68E-05,0.443848798,0.011659075,0.078264156,3.98E-05,0.463822635,9.96E-06,0.00182154,0.000158942
1659,natural_language_inference44,247,Wet featured on the soundtrack for which 1994 film ?,Training details,Wet Wet,natural_language_inference,44,31,1,0,,0.000124083,0,negative,6.48E-06,1.29E-07,2.27E-05,3.84E-07,2.01E-06,0.000599083,0.002325042,1.07E-05,8.15E-07,0.996972559,1.32E-07,6.10E-06,5.38E-05
1660,natural_language_inference44,248,2,Training details,Wet Wet,natural_language_inference,44,32,1,0,,0.000243381,0,negative,1.35E-05,1.95E-07,1.99E-05,7.34E-08,4.76E-07,0.000299709,0.00291083,1.42E-05,3.64E-06,0.996694131,8.27E-08,6.14E-06,3.71E-05
1661,natural_language_inference44,249,28,Training details,Wet Wet,natural_language_inference,44,33,1,0,,0.000213043,0,negative,9.34E-06,1.86E-07,8.92E-06,5.10E-07,2.10E-06,0.000645892,0.002712974,2.14E-05,1.39E-06,0.996543943,6.55E-08,4.25E-06,4.90E-05
1662,natural_language_inference44,250,"Cry Freedom is a 1987 British epic drama film directed by Richard Attenborough , set in late - 1970s apartheid The 1987 film ' Cry Freedom ' is a era South Africa .",Training details,Wet Wet,natural_language_inference,44,34,1,0,,0.00925214,0,negative,0.000101772,2.02E-06,0.000409426,3.39E-05,5.29E-05,0.005940495,0.19567628,5.47E-05,5.28E-06,0.759426402,2.23E-05,0.000282285,0.037992314
1663,natural_language_inference44,251,( ...),Training details,Wet Wet,natural_language_inference,44,35,1,0,,0.00037894,0,negative,6.39E-06,1.18E-07,2.17E-05,2.30E-07,1.29E-06,0.000459539,0.00187683,1.18E-05,1.40E-06,0.997569795,3.48E-08,3.40E-06,4.75E-05
1664,natural_language_inference44,252,The film centres on the real - life events involving black activist Steve biographical drama about which South Aftrican civil rights leader ?,Training details,Wet Wet,natural_language_inference,44,36,1,0,,0.004414688,0,negative,1.69E-05,5.99E-07,6.00E-05,9.33E-06,8.08E-05,0.001169215,0.006792663,1.36E-05,1.65E-06,0.991015841,3.45E-07,1.65E-05,0.000822511
1665,natural_language_inference44,253,"Helen Adams Keller was an American author , political activist , and lecturer .",Training details,Wet Wet,natural_language_inference,44,37,1,0,,0.007492516,0,negative,5.04E-05,1.07E-06,0.000205765,3.74E-06,1.68E-05,0.003236267,0.06755982,7.73E-05,4.97E-06,0.924972403,1.04E-06,7.39E-05,0.003796499
1666,natural_language_inference44,254,(),Training details,Wet Wet,natural_language_inference,44,38,1,0,,0.000374719,0,negative,7.31E-06,2.83E-07,1.93E-05,8.32E-08,4.52E-07,0.000720852,0.004350772,4.38E-05,6.28E-06,0.994790051,6.00E-08,3.04E-06,5.77E-05
1667,natural_language_inference44,255,"The story of how Kellers teacher , Which teacher taught Helen Keller Anne Sullivan , broke through the isolation imposed by a near complete lack of language , allowing the girl to to communicate ?",Training details,Wet Wet,natural_language_inference,44,39,1,0,,0.000994982,0,negative,7.91E-06,1.84E-07,2.83E-05,4.55E-07,2.68E-06,0.000588659,0.004857374,1.26E-05,1.77E-06,0.994260024,2.15E-07,8.92E-06,0.00023092
1668,natural_language_inference44,256,"blossom as she learned to communicate , has become widely known through ( ... )",Training details,Wet Wet,natural_language_inference,44,40,1,0,,0.000211116,0,negative,3.94E-06,9.09E-08,2.26E-05,5.55E-07,7.57E-06,0.000372538,0.001838996,4.46E-06,4.54E-07,0.997639755,6.20E-08,4.83E-06,0.000104128
1669,natural_language_inference44,257,3 ? 4 ( ... ),Training details,Wet Wet,natural_language_inference,44,41,1,0,,0.001590737,0,negative,3.14E-05,1.42E-07,8.50E-05,3.80E-07,6.78E-06,0.000891886,0.01239881,1.13E-05,5.52E-07,0.986388093,3.03E-08,3.06E-05,0.000154959
1670,natural_language_inference44,258,"The equation shows that , as volume increases , the pressure of the gas decreases in proportion .",Training details,Wet Wet,natural_language_inference,44,42,1,0,,0.004371798,0,negative,0.000151465,3.39E-06,0.000272674,2.50E-06,1.69E-05,0.001997811,0.023252165,9.63E-05,6.22E-05,0.973204446,4.06E-07,2.34E-05,0.000916192
1671,natural_language_inference44,259,"Similarly ,",Training details,,natural_language_inference,44,43,1,0,,0.000230269,0,negative,6.51E-05,8.56E-06,4.76E-05,1.70E-06,2.59E-05,0.030911357,0.000726296,0.007510427,2.26E-05,0.960328024,4.07E-07,0.000339224,1.29E-05
1672,natural_language_inference44,260,"Who gave his name to the scientific as volume decreases , the pressure of the gas increases .",Training details,"Similarly ,",natural_language_inference,44,44,1,0,,0.000908703,0,negative,6.76E-05,5.72E-07,0.000447426,4.33E-06,3.00E-05,0.000743506,0.004204633,3.41E-05,4.14E-06,0.989000488,4.89E-07,4.19E-05,0.005420869
1673,natural_language_inference44,261,"The law was named after chemist and physicist law that states that the pressure of a gas Robert Boyle , who published the original law .",Training details,"Similarly ,",natural_language_inference,44,45,1,0,,0.001185736,0,negative,7.57E-05,1.39E-06,0.000435261,6.70E-06,3.83E-05,0.00177335,0.010102894,0.000124549,9.05E-06,0.976625163,9.08E-07,6.15E-05,0.010745196
1674,natural_language_inference44,262,is inversely proportional to its volume at constant temperature ?,Training details,"Similarly ,",natural_language_inference,44,46,1,0,,0.010032496,0,negative,0.000115191,9.06E-07,0.000220799,1.11E-06,8.52E-06,0.000537137,0.005909052,6.36E-05,9.00E-06,0.991019835,3.50E-07,5.08E-05,0.002063746
1675,natural_language_inference44,263,The Buffalo six ( known primarily as Lackawanna Six ) is a group of six Yemeni - American friends who were Colourful State ' and ' the Land of Enchantment ? ',Training details,"Similarly ,",natural_language_inference,44,47,1,0,,0.008153838,0,negative,8.99E-05,9.82E-07,0.003200192,1.74E-06,0.000110674,0.000529336,0.010676271,1.64E-05,2.40E-06,0.981292636,1.12E-07,8.46E-05,0.003994871
1676,natural_language_inference44,264,"Smith also arranged for the publication of a series of etchings of Capricci in his vedette ideal , Canaletto is famous for his landscapes but the returns were not high enough , and in 1746 Canaletto moved to London , to be closer to his market .",Training details,"Similarly ,",natural_language_inference,44,48,1,0,,0.001579648,0,negative,0.000178463,5.83E-07,0.000360562,4.34E-06,7.59E-05,0.000780298,0.00864921,3.17E-05,2.11E-06,0.980432599,1.91E-07,0.000102737,0.009381306
1677,natural_language_inference44,265,of Venice and which other city ?,Training details,"Similarly ,",natural_language_inference,44,49,1,0,,0.000715657,0,negative,3.24E-05,1.53E-07,0.000132764,4.57E-07,1.42E-05,0.000159371,0.001489804,1.04E-05,7.46E-07,0.997686568,1.59E-08,2.05E-05,0.000452612
1678,natural_language_inference44,266,The groundtruth answer text is in red text .,Training details,"Similarly ,",natural_language_inference,44,50,1,0,,0.005653788,0,negative,1.59E-05,1.49E-07,0.000113356,4.35E-07,1.49E-05,0.00041578,0.005458885,2.37E-05,6.20E-07,0.992352161,1.59E-08,3.68E-05,0.001567334
1679,natural_language_inference44,267,Note that the span is not given as the groundtruth .,Training details,"Similarly ,",natural_language_inference,44,51,1,0,,0.000525972,0,negative,3.23E-05,1.87E-07,3.66E-05,3.79E-07,7.24E-06,0.000133388,0.000566477,1.64E-05,9.95E-07,0.99900357,1.15E-08,1.78E-05,0.000184572
1680,natural_language_inference44,268,"In the first example classified into ' N / A ' , the question is not answerable even given whole documents , because there is no word ' corlourful ' or ' enchantment ' in the given documents .",Training details,"Similarly ,",natural_language_inference,44,52,1,0,,0.002719612,0,negative,8.82E-05,1.17E-07,9.27E-05,1.80E-07,3.39E-05,0.000108525,0.00952611,5.96E-06,1.53E-07,0.988416488,2.31E-08,0.000382634,0.001344958
1681,natural_language_inference44,269,"In the next example , the question is also not answerable even given whole documents , because all sentences containing ' London ' does not contain any information about Canaletto 's landscapes .",Training details,"Similarly ,",natural_language_inference,44,53,1,0,,0.002357903,0,negative,0.000237774,1.70E-07,5.80E-05,3.75E-07,3.34E-05,0.000155927,0.016869849,1.00E-05,1.63E-07,0.979427462,5.82E-08,0.000896542,0.002310244
1682,natural_language_inference44,270,"In On the Abrogation of the Private Mass , he condemned as idolatry the idea that the mass is a sacrifice , asserting instead that it is a gift , to be received with thanksgiving by the whole congregation .",Training details,"Similarly ,",natural_language_inference,44,54,1,0,,0.002048827,0,negative,9.16E-05,5.32E-07,0.000154895,6.18E-07,2.11E-05,0.000252651,0.002406637,2.24E-05,2.88E-06,0.995653425,4.90E-08,4.27E-05,0.001350558
1683,natural_language_inference44,271,Who was the Jin dynasty defector who betrayed the location of the Jin army ? :,Training details,"Similarly ,",natural_language_inference,44,55,1,0,,0.00029241,0,negative,2.00E-05,2.61E-07,0.0001003,3.19E-07,2.61E-06,0.000275999,0.003345892,3.23E-05,2.91E-06,0.993852307,1.69E-07,1.66E-05,0.002350312
1684,natural_language_inference44,272,"Examples on SQuAD , which MINIMAL predicts the wrong answer .",Training details,"Similarly ,",natural_language_inference,44,56,1,0,,0.001553321,0,negative,1.78E-05,4.86E-08,6.53E-05,1.15E-07,5.57E-06,0.000139375,0.006090556,6.22E-06,1.28E-07,0.992105162,1.20E-08,7.59E-05,0.001493808
1685,natural_language_inference44,273,"Grountruth span is in underlined text , the prediction from MINIMAL is in red text .",Training details,"Similarly ,",natural_language_inference,44,57,1,0,,0.000959723,0,negative,5.06E-05,6.14E-07,0.000611163,8.25E-07,1.51E-05,0.001682453,0.036780918,0.000112421,5.12E-06,0.952452645,2.88E-08,5.76E-05,0.008230496
1686,natural_language_inference44,274,Sentences selected by our selector is denoted with .,Training details,"Similarly ,",natural_language_inference,44,58,1,0,,0.005740341,0,negative,1.06E-05,1.02E-06,0.000113089,1.07E-07,4.31E-06,0.000433227,0.007974222,0.000115708,5.96E-06,0.989628993,1.59E-08,1.18E-05,0.001700993
1687,natural_language_inference44,275,"In the first example , the model predicts the wrong answer from the oracle sentence .",Training details,"Similarly ,",natural_language_inference,44,59,1,0,,0.000926414,0,negative,5.34E-05,8.15E-08,9.66E-05,3.12E-08,6.00E-06,4.66E-05,0.003621648,3.85E-06,2.71E-07,0.99563629,2.89E-09,9.22E-05,0.000442997
1688,natural_language_inference44,276,"In the second example , the model predicts the answer from the wrong sentence , although it selects the oracle sentence .",Training details,"Similarly ,",natural_language_inference,44,60,1,0,,0.001370733,0,negative,5.28E-05,9.58E-08,0.000151118,4.51E-08,8.46E-06,5.21E-05,0.003203136,3.84E-06,3.47E-07,0.995977946,2.74E-09,7.90E-05,0.000471043
1689,natural_language_inference44,277,"In the last example , the model fails to select the oracle sentence .",Training details,"Similarly ,",natural_language_inference,44,61,1,0,,0.002453271,0,negative,0.000151842,8.94E-08,6.14E-05,1.43E-07,1.58E-05,7.63E-05,0.005093906,5.88E-06,1.89E-07,0.993427292,3.66E-09,0.000144737,0.001022431
1690,natural_language_inference44,278,"two examples , our sentence selector choose the oracle sentence , but the QA model fails to answer correctly , either predicting the wrong answer from the oracle sentence , or predicting the answer from the wrong sentence .",Training details,"Similarly ,",natural_language_inference,44,62,1,0,,0.004083947,0,negative,2.34E-05,7.14E-08,8.49E-05,1.15E-07,8.43E-06,0.00014973,0.008122968,7.65E-06,1.92E-07,0.989415948,8.95E-09,9.62E-05,0.002090368
1691,natural_language_inference44,279,"In the last example , our sentence selector fails to choose the oracle sentence .",Training details,"Similarly ,",natural_language_inference,44,63,1,0,,0.002200096,0,negative,0.000143028,1.12E-07,6.62E-05,3.62E-07,2.75E-05,0.000132013,0.006345131,8.08E-06,1.90E-07,0.991093369,4.94E-09,0.000163781,0.002020176
1692,natural_language_inference44,280,"We conjecture that the selector rather chooses the sentences containing the word ' the Jin dynasty ' , which leads to the failure in selection .",Training details,"Similarly ,",natural_language_inference,44,64,1,0,,0.001342264,0,negative,5.70E-05,2.45E-07,7.44E-05,1.49E-07,3.35E-06,0.000225229,0.004496234,3.00E-05,2.28E-06,0.9932052,1.15E-08,2.92E-05,0.001876721
1693,natural_language_inference44,281,"C Full Results on Trivia QA and SQuAD - Open and show full results on Trivi - a QA ( Wikipedia ) and SQuAD - Open , respectively .",Training details,"Similarly ,",natural_language_inference,44,65,1,0,,0.334677668,0,experiments,0.000414313,2.08E-07,7.59E-05,4.42E-08,1.19E-06,7.42E-05,0.850088803,1.10E-05,1.19E-07,0.121304867,2.15E-08,0.013537131,0.014492291
1694,natural_language_inference44,282,"MINIMAL obtains higher F1 and EM over FULL , with the inference speedup of up to 13.8 .",Training details,"Similarly ,",natural_language_inference,44,66,1,0,,0.811967292,1,experiments,0.002311171,8.31E-07,0.00022346,3.99E-07,1.25E-05,0.000163202,0.812164758,1.84E-05,3.40E-07,0.058295329,1.20E-08,0.011378084,0.11543147
1695,natural_language_inference44,283,"In addition , outperforms the published state - of - the - art on both Trivia QA ( Wikipedia ) and SQuAD - Open , by 5.2 F1 and 4.9 EM , respectively .",Training details,"Similarly ,",natural_language_inference,44,67,1,0,,0.594960508,1,experiments,0.000459433,2.42E-07,9.22E-05,5.49E-07,1.15E-05,0.000161369,0.802308313,1.79E-05,8.81E-08,0.04600345,1.40E-08,0.019179011,0.131766025
1696,natural_language_inference44,284,"D Samples on SQuAD , TriviaQA and",Training details,"Similarly ,",natural_language_inference,44,68,1,0,,0.013314933,0,negative,0.001511275,1.11E-06,0.006796298,3.05E-07,9.54E-05,0.000219643,0.260350279,7.80E-06,1.16E-06,0.695435674,1.02E-08,0.005320186,0.030260839
1697,natural_language_inference44,285,SQuAD - Adversarial shows the full index of samples used for human studies and analyses .,Training details,"Similarly ,",natural_language_inference,44,69,1,0,,0.092422685,0,negative,0.000325672,9.69E-07,0.000817474,0.000773293,0.005042959,0.011110095,0.070756629,0.00011077,1.02E-06,0.654693671,6.20E-09,0.000194747,0.256172698
1698,natural_language_inference44,286,Analysis,Training details,,natural_language_inference,44,70,1,0,,0.001220235,0,negative,2.22E-05,1.39E-05,1.73E-05,9.01E-06,7.52E-05,0.133150706,0.002631732,0.046960801,2.43E-05,0.816821595,1.98E-07,0.000168089,0.000105035
1699,natural_language_inference20,1,title,,,natural_language_inference,20,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
1700,natural_language_inference20,2,Sentence Embeddings in NLI with Iterative Refinement Encoders,title,,natural_language_inference,20,1,1,1,research-problem,0.996658103,1,research-problem,3.86E-08,3.54E-05,1.42E-07,5.28E-08,4.07E-08,2.00E-07,7.16E-07,4.98E-06,1.04E-05,0.003236799,0.996710959,2.02E-07,4.80E-08
1701,natural_language_inference20,3,abstract,,,natural_language_inference,20,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
1702,natural_language_inference20,4,Sentence - level representations are necessary for various NLP tasks .,abstract,abstract,natural_language_inference,20,1,1,1,research-problem,0.442769353,0,research-problem,1.22E-08,3.53E-06,6.22E-09,1.55E-07,6.01E-08,1.16E-07,1.71E-07,1.03E-06,3.42E-07,0.009107432,0.990887103,1.69E-08,1.99E-08
1703,natural_language_inference20,5,Recurrent neural networks have proven to be very effective in learning distributed representations and can be trained efficiently on natural language inference tasks .,abstract,abstract,natural_language_inference,20,2,1,0,,0.298188153,0,research-problem,1.64E-08,5.11E-06,1.54E-08,1.30E-07,4.40E-08,2.65E-07,2.93E-07,2.44E-06,9.89E-07,0.012436968,0.987553678,2.41E-08,2.68E-08
1704,natural_language_inference20,6,We build on top of one such model and propose a hierarchy of BiLSTM and max pooling layers that implements an iterative refinement strategy and yields state of the art results on the SciTail dataset as well as strong results for SNLI and MultiNLI .,abstract,abstract,natural_language_inference,20,3,1,0,,0.057651474,0,approach,0.000144128,0.519811607,0.000353105,0.000319769,0.002251857,0.000225835,4.65E-05,0.00175134,0.08286279,0.330709402,0.061487791,2.05E-05,1.53E-05
1705,natural_language_inference20,7,"We can show that the sentence embeddings learned in this way can be utilized in a wide variety of transfer learning tasks , outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks .",abstract,abstract,natural_language_inference,20,4,1,0,,0.050375175,0,negative,8.85E-05,0.006800527,8.31E-06,6.97E-06,6.23E-05,2.40E-05,6.09E-05,0.000475232,0.000242243,0.655386541,0.336492499,0.000349094,2.85E-06
1706,natural_language_inference20,8,"Furthermore , our model beats the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings ' ability to capture some of the important linguistic properties of sentences .",abstract,abstract,natural_language_inference,20,5,1,0,,0.104497144,0,negative,0.000196578,0.002856504,8.74E-06,0.000269484,0.000555306,0.000204647,0.000441875,0.001511592,4.80E-05,0.677506817,0.31564165,0.000734256,2.46E-05
1707,natural_language_inference20,9,Introduction,,,natural_language_inference,20,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
1708,natural_language_inference20,10,Neural networks have been shown to provide a powerful tool for building representations of natural languages on multiple levels of linguistic abstraction .,Introduction,Introduction,natural_language_inference,20,1,1,0,,0.653093371,1,research-problem,6.86E-07,0.000172283,2.99E-07,7.24E-07,1.86E-06,4.08E-06,7.02E-06,8.35E-06,0.000320596,0.05646,0.943022282,1.09E-06,7.29E-07
1709,natural_language_inference20,11,Perhaps the most widely used representations in natural language processing are word embeddings .,Introduction,Introduction,natural_language_inference,20,2,1,0,,0.005918406,0,research-problem,1.46E-06,0.0006981,1.22E-06,7.36E-06,1.10E-05,5.21E-05,2.29E-05,7.84E-05,0.000856268,0.197803604,0.800463135,1.63E-06,2.77E-06
1710,natural_language_inference20,12,Recently there has been a growing interest in models for sentencelevel representations using a range of different neural network architectures .,Introduction,Introduction,natural_language_inference,20,3,1,0,,0.83002173,1,research-problem,9.42E-07,0.000333999,3.22E-07,1.36E-06,3.27E-06,1.02E-05,1.05E-05,2.27E-05,0.000474361,0.112820086,0.886319925,1.38E-06,9.96E-07
1711,natural_language_inference20,13,"Such sentence embeddings have been generated using unsupervised learning approaches , and supervised learning .",Introduction,Introduction,natural_language_inference,20,4,1,0,,0.034906953,0,negative,1.14E-05,0.015716562,6.20E-06,2.65E-05,0.0003811,0.000175281,3.02E-05,0.000264504,0.011755227,0.808968491,0.16265581,4.82E-06,3.96E-06
1712,natural_language_inference20,14,Supervision typically comes in the form of an underlying semantic task with labeled data to train the model .,Introduction,Introduction,natural_language_inference,20,5,1,0,,0.111337557,0,negative,3.94E-05,0.048460479,2.34E-05,0.000178604,0.002754962,0.000330063,5.56E-05,0.000566727,0.018287293,0.857686851,0.071592434,1.14E-05,1.28E-05
1713,natural_language_inference20,15,The most prominent task for that purpose is natural language inference ( NLI ) that tries to model the inferential relationship between two or more given sentences .,Introduction,Introduction,natural_language_inference,20,6,1,0,,0.895728412,1,research-problem,5.52E-07,0.000102199,2.05E-07,9.40E-06,1.51E-05,5.98E-06,1.18E-05,5.87E-06,2.63E-05,0.057832032,0.941987905,8.18E-07,1.72E-06
1714,natural_language_inference20,16,"In particular , given two sentences - the premise p and the hypothesis h - the task is to determine whether h is entailed by p , whether the sentences are in contradiction with each other or whether there is no inferential relationship between the sentences ( neutral ) .",Introduction,Introduction,natural_language_inference,20,7,1,0,,0.10352108,0,research-problem,1.90E-06,0.002639114,1.59E-06,3.17E-05,0.000135398,2.62E-05,2.03E-05,3.92E-05,0.001550738,0.180732918,0.814813975,2.59E-06,4.32E-06
1715,natural_language_inference20,17,There are two main neural approaches ar Xiv : 1808.08762v2 [ cs. CL ] 3 Jun 2019 to NLI .,Introduction,Introduction,natural_language_inference,20,8,1,0,,0.049057835,0,negative,7.29E-06,0.00386797,8.54E-06,1.87E-05,8.27E-05,0.000123801,6.01E-05,0.000137631,0.003530453,0.525196317,0.466953585,6.84E-06,6.05E-06
1716,natural_language_inference20,18,Sentence encoding - based models focus on building separate embeddings for the premises and the hypothesis and then combine those using a classifier .,Introduction,Introduction,natural_language_inference,20,9,1,0,,0.176055878,0,research-problem,1.09E-05,0.01356761,3.01E-05,2.15E-05,9.41E-05,0.000156877,0.000110838,0.000267265,0.018065151,0.309239705,0.658409974,1.16E-05,1.44E-05
1717,natural_language_inference20,19,Other approaches do not treat the two sentences separately but utilize e.g. cross - sentence attention .,Introduction,Introduction,natural_language_inference,20,10,1,0,,0.006062465,0,negative,1.27E-05,0.005923239,8.10E-06,4.60E-05,0.000121621,0.000231978,9.80E-05,0.00032295,0.005407742,0.594280625,0.393521777,1.32E-05,1.21E-05
1718,natural_language_inference20,20,"With the goal of obtaining general - purpose sentence representations in mind , we opt for the sentence encoding approach .",Introduction,Introduction,natural_language_inference,20,11,1,1,model,0.812841023,1,approach,7.60E-05,0.606325111,0.000183693,3.24E-05,0.002312928,0.000127675,9.78E-05,0.000290734,0.192397524,0.161779542,0.036327371,3.76E-05,1.15E-05
1719,natural_language_inference20,21,Motivated by the success of the InferSent architecture we extend their architecture with a hierarchylike structure of bidirectional LSTM ( BiLSTM ) layers with max pooling .,Introduction,Introduction,natural_language_inference,20,12,1,1,model,0.033515665,0,model,7.79E-05,0.170493634,0.000541008,2.65E-05,0.001512418,0.000180959,7.55E-05,0.000152881,0.789125207,0.035981745,0.001806035,1.50E-05,1.13E-05
1720,natural_language_inference20,22,"All in all , our model improves the previous state of the art for SciTail and achieves strong results for the SNLI and Multi - Genre Natural Language Inference corpus ( MultiNLI ; .",Introduction,Introduction,natural_language_inference,20,13,1,0,,0.052103939,0,negative,0.002585923,0.027966333,8.21E-05,0.000343709,0.004746237,0.000897655,0.028276421,0.00202581,0.002624143,0.849189659,0.057957389,0.022671916,0.000632747
1721,natural_language_inference20,23,"In order to demonstrate the semantic abstractions achieved by our approach , we also apply our model to a number of transfer learning tasks using the SentEval testing library , and show that it outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks , comparing to the scores reported by .",Introduction,Introduction,natural_language_inference,20,14,1,0,,0.065801599,0,negative,0.001752322,0.095951838,3.70E-05,3.69E-05,0.002070097,0.000367052,0.005237173,0.001344444,0.010042419,0.864792696,0.012155177,0.006139576,7.33E-05
1722,natural_language_inference20,24,"Moreover , our model outperforms the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings ' ability to capture some of the important linguistic properties of sentences .",Introduction,Introduction,natural_language_inference,20,15,1,0,,0.154843358,0,negative,0.00399359,0.053706377,0.000110454,0.000976264,0.013742101,0.002406482,0.030654913,0.003710693,0.003334755,0.849392262,0.022407265,0.014779438,0.000785407
1723,natural_language_inference20,25,"This highlights the generalization capability of the proposed model , confirming that its architecture is able to learn sentence representations with strong performance across a wide variety of different NLP tasks .",Introduction,Introduction,natural_language_inference,20,16,1,0,,0.139373893,0,negative,0.001146651,0.030350669,2.38E-05,2.98E-05,0.00220534,0.000148166,0.000548956,0.000219883,0.009701678,0.95007421,0.004873378,0.000667504,9.95E-06
1724,natural_language_inference20,26,Related Work,,,natural_language_inference,20,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
1725,natural_language_inference20,36,Model Architecture,,,natural_language_inference,20,0,1,0,,0.012027743,0,negative,0.002077781,0.004304933,0.001063385,0.001680559,0.000148392,0.002327543,0.001259945,0.007127733,0.020357931,0.924862365,0.03336821,0.000759331,0.000661892
1726,natural_language_inference20,37,Our proposed architecture follows a sentence embedding - based approach for NLI introduced by .,Model Architecture,Model Architecture,natural_language_inference,20,1,1,0,,0.167514453,0,model,4.86E-05,0.061439241,0.000660304,1.73E-06,0.000102481,2.99E-05,1.20E-05,8.57E-05,0.819124466,0.115390203,0.003086109,1.63E-05,2.84E-06
1727,natural_language_inference20,38,"The model illustrated in contains sentence embeddings for the two input sentences , where the output of the sentence embeddings are combined using a heuristic introduced by , putting together the concatenation ( u , v ) , absolute element - wise difference |u ? v| , and element - wise product u * v.",Model Architecture,Model Architecture,natural_language_inference,20,2,1,0,,0.026379934,0,model,6.78E-06,0.004852495,6.84E-05,5.42E-07,7.83E-06,2.83E-05,5.10E-06,0.000103395,0.791800956,0.202338316,0.000784693,1.73E-06,1.41E-06
1728,natural_language_inference20,39,The combined vector is then passed onto a 3 layered multi - layer perceptron ( MLP ) with a 3 - way softmax classifier .,Model Architecture,Model Architecture,natural_language_inference,20,3,1,0,,0.404570312,0,model,1.55E-05,0.007857158,0.00025512,1.64E-07,6.63E-06,2.41E-05,5.47E-06,7.89E-05,0.913908601,0.077722648,0.000123766,1.41E-06,5.40E-07
1729,natural_language_inference20,40,The first two layers of the MLP both utilize dropout and a ReLU activation function .,Model Architecture,Model Architecture,natural_language_inference,20,4,1,0,,0.722223742,1,model,0.000194037,0.033083346,0.000342569,6.62E-05,0.000306878,0.001327626,0.000156208,0.006059231,0.643612255,0.314495728,0.000310879,1.73E-05,2.78E-05
1730,natural_language_inference20,41,"We use a variant of ReLU called Leaky ReLU , defined by :",Model Architecture,Model Architecture,natural_language_inference,20,5,1,0,,0.371778825,0,model,7.69E-05,0.050526937,0.010088442,8.93E-07,6.97E-05,8.37E-05,6.11E-05,0.000221479,0.722338465,0.215805137,0.000699192,2.58E-05,2.15E-06
1731,natural_language_inference20,42,where we set y = 0.01 as the negative slope for x < 0 . This prevents the gradient from dying when x < 0 .,Model Architecture,Model Architecture,natural_language_inference,20,6,1,0,,0.036557457,0,negative,0.000198805,0.005889448,2.85E-05,8.65E-06,8.74E-05,0.001752309,8.91E-05,0.009396267,0.036926981,0.945459171,0.000126911,3.18E-05,4.70E-06
1732,natural_language_inference20,43,Fig .,Model Architecture,,natural_language_inference,20,7,1,0,,0.001184945,0,negative,3.90E-05,0.000620761,1.77E-05,2.53E-06,2.21E-05,4.62E-05,8.40E-06,9.78E-05,0.016962377,0.981777817,0.000383392,2.06E-05,1.32E-06
1733,natural_language_inference20,44,1 . Overall NLI Architecture,Model Architecture,Fig .,natural_language_inference,20,8,1,0,,0.001374789,0,negative,6.49E-05,0.003760237,6.22E-05,2.65E-05,5.07E-05,0.000213959,5.59E-05,0.000707469,0.165450085,0.827014791,0.002480966,7.23E-05,4.00E-05
1734,natural_language_inference20,45,For the sentence representations we first embed the individual words with pretrained word embeddings .,Model Architecture,Fig .,natural_language_inference,20,9,1,0,,0.099477326,0,negative,0.000262927,0.044457868,0.000456598,2.80E-06,6.15E-05,0.000325824,8.65E-05,0.003722571,0.429953762,0.520541855,7.38E-05,4.92E-05,4.81E-06
1735,natural_language_inference20,46,The sequence of the embedded words is then passed onto the sentence encoder which utilizes BiLSTM with max pooling .,Model Architecture,Fig .,natural_language_inference,20,10,1,0,,0.006820785,0,model,1.64E-05,0.006137505,0.000287511,2.21E-07,5.46E-06,2.49E-05,8.85E-06,0.000203885,0.554161115,0.439012403,0.000133533,6.89E-06,1.28E-06
1736,natural_language_inference20,47,"Given a sequence T of words ( w 1 . . . , w T ) , the output of the bi-directional LSTM is a set of vectors ( h 1 , . . . , h T ) , where each ht ? ( h 1 , . . . , h T ) is the concatenation",Model Architecture,Fig .,natural_language_inference,20,11,1,0,,4.72E-05,0,negative,4.15E-06,0.000756009,1.55E-05,3.83E-07,5.22E-06,1.69E-05,3.28E-06,9.50E-05,0.022122944,0.976876542,9.75E-05,6.04E-06,5.59E-07
1737,natural_language_inference20,48,of a forward and backward LSTMs,Model Architecture,Fig .,natural_language_inference,20,12,1,0,,0.000315217,0,negative,3.24E-05,0.001304535,0.00020702,5.29E-07,4.43E-06,1.85E-05,1.28E-05,6.91E-05,0.174896108,0.822049018,0.001349999,5.28E-05,2.79E-06
1738,natural_language_inference20,49,"The max pooling layer produces a vector of the same dimensionality ash t , returning , for each dimension , its maximum value over the hidden units ( h 1 , . . . , h T ) .",Model Architecture,Fig .,natural_language_inference,20,13,1,0,,0.0003886,0,negative,2.89E-05,0.002006553,5.48E-05,1.44E-06,1.22E-05,0.000105164,1.65E-05,0.000732853,0.092005612,0.904992018,3.36E-05,8.46E-06,1.93E-06
1739,natural_language_inference20,50,"Motivated by the strong results of the BiLSTM max pooling network by , we experimented with combining BiLSTM max pooling networks in a hierarchy - like structure .",Model Architecture,Fig .,natural_language_inference,20,14,1,0,,0.002593181,0,negative,0.000184267,0.050200165,0.000875978,4.36E-06,0.000223443,0.000112707,5.52E-05,0.000501246,0.20084136,0.746593093,0.000239056,0.000163369,5.78E-06
1740,natural_language_inference20,51,"1 To improve the BiLSTM layers ' ability to remember the input words , we let each layer of the network re-read the input embeddings instead of stacking the layers in a strict hierarchical model .",Model Architecture,Fig .,natural_language_inference,20,15,1,0,,0.001310629,0,negative,0.000244496,0.006891505,0.000271377,6.85E-06,0.000114782,6.66E-05,1.90E-05,0.000390992,0.062805216,0.929084143,3.20E-05,7.03E-05,2.66E-06
1741,natural_language_inference20,52,"In this way , our model acts as an iterative refinement architecture that reconsiders the input in each layer while being informed by the previous layer through initialis ation .",Model Architecture,Fig .,natural_language_inference,20,16,1,0,,0.001262362,0,model,5.71E-05,0.009201589,0.000574216,7.57E-07,3.07E-05,1.83E-05,9.07E-06,6.95E-05,0.71276018,0.277153793,0.00010094,2.11E-05,2.64E-06
1742,natural_language_inference20,53,This creates a hierarchy of refinement layers and each of them contributes to the NLI classification by max pooling the hidden states .,Model Architecture,Fig .,natural_language_inference,20,17,1,0,,0.0031242,0,negative,0.000199229,0.005274743,0.000722988,8.01E-07,3.97E-05,2.31E-05,1.03E-05,9.08E-05,0.280667935,0.712911932,2.15E-05,3.53E-05,1.68E-06
1743,natural_language_inference20,54,In the following we refer to that architecture with the abbreviation HBMP .,Model Architecture,Fig .,natural_language_inference,20,18,1,0,,2.23E-05,0,negative,3.15E-06,0.000767496,6.23E-05,6.03E-07,9.63E-06,4.23E-05,1.48E-05,0.000122351,0.023881922,0.974921962,0.000162039,8.90E-06,2.45E-06
1744,natural_language_inference20,55,Max pooling is defined in the standard way of taking the highest value over each dimension of the hidden states and the final sentence embedding is the concatenation of those vectors coming from each BiLSTM layer .,Model Architecture,Fig .,natural_language_inference,20,19,1,0,,0.000270581,0,negative,5.63E-05,0.006353361,0.000206153,7.87E-06,4.29E-05,0.000344237,6.01E-05,0.002937939,0.083225913,0.906684515,5.15E-05,2.12E-05,8.02E-06
1745,natural_language_inference20,56,The over all architecture is illustrated in .,Model Architecture,Fig .,natural_language_inference,20,20,1,0,,0.004912151,0,negative,6.27E-05,0.002809828,0.000222667,7.71E-06,5.76E-05,9.38E-05,3.28E-05,0.000292151,0.268043213,0.72807451,0.000231293,5.32E-05,1.85E-05
1746,natural_language_inference20,57,To summarize the differences between our model and traditional stacked BiLSTM architectures we can list the following three main aspects : setup that does not transfer knowledge between layers but also combines information from three separate BiLSTM layers for the final classification .,Model Architecture,Fig .,natural_language_inference,20,21,1,0,,0.000169726,0,negative,8.86E-05,0.00077206,3.84E-05,1.01E-06,1.78E-05,9.62E-06,4.06E-06,4.16E-05,0.003132253,0.995750912,5.45E-05,8.87E-05,5.60E-07
1747,natural_language_inference20,58,The second model ( BiLSTM - Ens - Train ) adds a trainable initialization to each layer to study the impact of the hierarchical initialization that we propose in our architecture .,Model Architecture,Fig .,natural_language_inference,20,22,1,0,,0.058238645,0,negative,0.000236773,0.017369401,0.01658612,1.25E-06,9.66E-05,0.000111761,0.000144453,0.0003125,0.476911575,0.488092019,4.05E-05,8.81E-05,8.95E-06
1748,natural_language_inference20,59,The third model ( BiLSTM - Ens - Tied ) connects the three layers by tying parameters to each other .,Model Architecture,Fig .,natural_language_inference,20,23,1,0,,0.048436497,0,model,0.00013312,0.005882643,0.006613009,1.10E-06,3.15E-05,7.06E-05,7.23E-05,0.00020116,0.700806626,0.286097374,4.51E-05,3.54E-05,1.01E-05
1749,natural_language_inference20,60,"Finally , the fourth model ( BiLSTM - Stack ) implements a standard hierarchical network with stacked layers that do not re-read the original input .",Model Architecture,Fig .,natural_language_inference,20,24,1,0,,0.040651748,0,model,0.00020251,0.006323796,0.018434544,2.20E-06,8.62E-05,0.000129661,0.000154383,0.000207234,0.521138962,0.453136664,6.68E-05,9.63E-05,2.07E-05
1750,natural_language_inference20,61,We apply the standard SNLI data for the comparison of these different architectures ( see Section 5 for more information about the SNLI benchmark ) .,Model Architecture,Fig .,natural_language_inference,20,25,1,0,,2.35E-05,0,negative,4.01E-06,0.000230956,1.24E-05,2.84E-07,4.90E-05,2.87E-05,1.27E-05,0.000110128,0.000466503,0.999051952,1.45E-06,3.17E-05,2.90E-07
1751,natural_language_inference20,62,"The results show that HBMP performs better than each of the other models , which supports the use of our setup in favor of alternative architectures .",Model Architecture,Fig .,natural_language_inference,20,26,1,0,,0.277039532,0,negative,0.004454659,0.000708592,3.27E-05,1.92E-05,0.000112993,0.00036234,0.002629137,0.002380591,0.000635333,0.87400298,0.00012215,0.114454214,8.51E-05
1752,natural_language_inference20,63,"Furthermore , we can see that the different components all contribute to the final score .",Model Architecture,Fig .,natural_language_inference,20,27,1,0,,0.003015299,0,negative,0.000810545,0.000713094,1.62E-05,1.21E-06,1.80E-05,2.03E-05,1.76E-05,0.000173237,0.010101102,0.987839245,8.11E-06,0.000280157,1.24E-06
1753,natural_language_inference20,64,Ensembling information from three separate BiLSTM layers ( with independent parameters ) improves the performance as we can see in the comparison between BiLSTM - Ens and BiLSTM - Ens - Tied .,Model Architecture,Fig .,natural_language_inference,20,28,1,0,,0.591774576,1,negative,0.119283001,0.0016198,0.00020899,2.33E-05,0.000223813,0.000333492,0.005567813,0.001976592,0.001870383,0.575892876,7.91E-05,0.292727125,0.000193698
1754,natural_language_inference20,65,Trainable initialization does not seem to add to the model 's capacity and indicates that the hierarchical initialization that we propose is indeed beneficial .,Model Architecture,Fig .,natural_language_inference,20,29,1,0,,0.007319007,0,negative,0.017998473,0.001405917,1.79E-05,5.75E-05,0.000330671,0.000335911,0.00032259,0.002460421,0.001971031,0.968619194,2.48E-05,0.006422939,3.27E-05
1755,natural_language_inference20,66,"Finally , feeding the same input embeddings to all Bi - LSTMs of HBMP leads to an improvement over the stacked model that does not re-read the input information .",Model Architecture,Fig .,natural_language_inference,20,30,1,0,,0.641374856,1,negative,0.08196844,0.004468014,0.000285453,2.39E-05,0.000345196,0.00029379,0.005938463,0.002229924,0.003184562,0.583786299,0.000105655,0.317171106,0.000199239
1756,natural_language_inference20,67,"Using these initial findings , we will now look at a more detailed analyses of the performance of HBMP on various datasets and tasks .",Model Architecture,Fig .,natural_language_inference,20,31,1,0,,4.82E-05,0,negative,1.97E-05,0.000133664,7.46E-06,1.31E-07,8.21E-06,5.68E-06,6.19E-06,2.48E-05,0.001655557,0.998060956,2.97E-06,7.44E-05,2.87E-07
1757,natural_language_inference20,68,"But before , we first give some more details about the implementation of the model and the training procedures we use .",Model Architecture,Fig .,natural_language_inference,20,32,1,0,,1.64E-06,0,negative,1.28E-06,2.30E-05,3.66E-07,4.38E-07,4.18E-06,7.84E-06,7.98E-07,3.20E-05,0.000175253,0.99975173,5.10E-07,2.46E-06,8.75E-08
1758,natural_language_inference20,69,"Note , that the same specifications also apply to the experiments that we already discussed above .",Model Architecture,Fig .,natural_language_inference,20,33,1,0,,4.75E-06,0,negative,3.26E-06,1.15E-05,3.73E-07,5.15E-07,7.15E-06,5.98E-06,6.61E-07,2.06E-05,7.16E-05,0.999872859,2.02E-07,5.33E-06,5.44E-08
1759,natural_language_inference20,70,Training Details,,,natural_language_inference,20,0,1,0,,0.002340091,0,negative,6.05E-05,0.001338931,1.43E-05,0.000123468,5.30E-05,0.00076714,0.000215992,0.00645336,0.000307534,0.988848599,0.00167076,0.000130888,1.56E-05
1760,natural_language_inference20,71,The architecture was implemented using PyTorch .,Training Details,Training Details,natural_language_inference,20,1,1,1,experimental-setup,0.988745688,1,experimental-setup,5.23E-06,4.45E-06,1.53E-05,8.94E-05,4.20E-06,0.969918763,0.001567806,0.022512446,5.56E-06,0.005853724,4.86E-06,2.69E-06,1.56E-05
1761,natural_language_inference20,72,We have published our code in GitHub : https://github.com/Helsinki-NLP/HBMP.,Training Details,Training Details,natural_language_inference,20,2,1,1,code,0.982765636,1,code,1.79E-05,1.25E-06,3.09E-06,0.829295585,0.000116865,0.16464515,0.000128571,0.000259206,2.34E-06,0.005494337,4.65E-06,3.08E-07,3.08E-05
1762,natural_language_inference20,73,"For all of our models we used a gradient descent optimization algorithm based on the Adam update rule , which is pre-implemented in PyTorch .",Training Details,Training Details,natural_language_inference,20,3,1,1,experimental-setup,0.990890625,1,experimental-setup,5.17E-06,1.19E-05,4.62E-06,8.50E-06,1.15E-06,0.823506912,0.000629176,0.172730957,6.40E-06,0.003083423,1.01E-06,2.15E-06,8.65E-06
1763,natural_language_inference20,74,We used a learning rate of 5e - 4 for all our models .,Training Details,Training Details,natural_language_inference,20,4,1,1,experimental-setup,0.991881577,1,experimental-setup,2.81E-06,5.73E-06,1.80E-06,1.53E-06,3.59E-07,0.700060341,0.000524282,0.297107687,4.49E-06,0.002280792,1.31E-06,2.07E-06,6.81E-06
1764,natural_language_inference20,75,The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve .,Training Details,Training Details,natural_language_inference,20,5,1,1,experimental-setup,0.989715311,1,experimental-setup,1.27E-05,1.39E-05,4.65E-06,2.46E-06,8.87E-07,0.635274917,0.000590429,0.359530037,1.35E-05,0.004539056,2.07E-06,3.96E-06,1.14E-05
1765,natural_language_inference20,76,We used a batch size of 64 .,Training Details,Training Details,natural_language_inference,20,6,1,1,experimental-setup,0.981892546,1,experimental-setup,3.24E-06,4.49E-06,2.68E-06,2.65E-06,6.44E-07,0.819315735,0.00091811,0.176643053,3.57E-06,0.003091447,1.15E-06,3.80E-06,9.43E-06
1766,natural_language_inference20,77,The models were evaluated with the development data after each epoch and training was stopped if the development loss increased for more than 3 epochs .,Training Details,Training Details,natural_language_inference,20,7,1,0,,0.402041205,0,experimental-setup,2.28E-05,2.48E-05,2.47E-05,1.82E-06,2.34E-06,0.738818909,0.001047018,0.227401825,2.42E-05,0.032607343,3.17E-06,1.17E-05,9.33E-06
1767,natural_language_inference20,78,The model with the highest development accuracy was selected for testing .,Training Details,Training Details,natural_language_inference,20,8,1,0,,0.12033177,0,experimental-setup,5.12E-05,3.62E-05,4.95E-05,2.13E-06,8.90E-06,0.463083558,0.0013823,0.191328181,2.64E-05,0.34392696,6.59E-06,8.80E-05,1.01E-05
1768,natural_language_inference20,79,"We use pre-trained Glo Ve word embeddings of size 300 dimensions ( Glo Ve 840B 300D ; , which were fine - tuned during training .",Training Details,Training Details,natural_language_inference,20,9,1,1,experimental-setup,0.99541189,1,experimental-setup,2.73E-06,6.31E-06,2.16E-06,2.04E-06,5.56E-07,0.756430966,0.000756211,0.24078595,4.34E-06,0.001997046,5.83E-07,2.43E-06,8.68E-06
1769,natural_language_inference20,80,"The sentence embeddings have hidden size of 600 for both direction ( except for SentEval test , where we test models with 600D and 1200D per direction ) and the 3 - layer multilayer perceptron ( MLP ) have the size of 600 dimensions .",Training Details,Training Details,natural_language_inference,20,10,1,1,experimental-setup,0.991923505,1,experimental-setup,2.66E-06,5.76E-06,3.11E-06,1.21E-06,3.53E-07,0.714461463,0.000754318,0.282492396,6.14E-06,0.002261278,8.75E-07,2.19E-06,8.25E-06
1770,natural_language_inference20,81,We use a dropout of 0.1 between the MLP layers ( except just before the final layer ) .,Training Details,Training Details,natural_language_inference,20,11,1,1,experimental-setup,0.993748985,1,experimental-setup,9.40E-06,1.64E-05,4.29E-06,7.16E-06,1.27E-06,0.656301285,0.000948087,0.340948137,1.02E-05,0.001727624,7.86E-07,4.12E-06,2.13E-05
1771,natural_language_inference20,82,Our models were trained using one NVIDIA Tesla P100 GPU .,Training Details,Training Details,natural_language_inference,20,12,1,1,experimental-setup,0.993203494,1,experimental-setup,1.68E-06,1.28E-06,1.46E-06,8.50E-05,2.69E-06,0.981468266,0.000940127,0.016061755,9.72E-07,0.001421056,4.41E-07,1.61E-06,1.36E-05
1772,natural_language_inference20,83,Evaluation Benchmarks,,,natural_language_inference,20,0,1,0,,0.013343394,0,negative,4.39E-06,1.38E-05,2.20E-06,4.65E-08,2.11E-07,3.18E-05,3.75E-05,0.000578397,2.87E-06,0.998903917,0.0001977,0.000226775,3.48E-07
1773,natural_language_inference20,84,"To further study the performance of HBMP , we train our architecture with three common NLI datasets :",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,1,1,0,,0.002413565,0,negative,0.000111558,9.03E-07,0.041891427,2.00E-07,3.64E-07,0.00017896,0.003249288,0.000337707,1.29E-07,0.936990877,2.99E-06,0.017219979,1.56E-05
1774,natural_language_inference20,85,"the Stanford Natural Language Inference ( SNLI ) corpus , the Multi - Genre Natural Language Inference ( MultiNLI ) corpus , the Textual Entailment Dataset from Science Question Answering ( SciTail ) .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,2,1,0,,0.52373683,1,negative,0.000438228,2.31E-06,0.128806353,3.06E-05,1.12E-05,0.001576906,0.071107969,0.000849287,8.93E-07,0.73583257,0.000122063,0.058281287,0.002940261
1775,natural_language_inference20,86,"Note that we treat them as separate tasks and do not mix any of the training , development and test data in our NLI experiments .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,3,1,0,,0.000356584,0,negative,3.71E-05,3.51E-07,0.00047809,6.37E-07,1.36E-07,0.000279677,7.25E-05,0.000626183,6.78E-07,0.998314045,3.36E-07,0.000186543,3.74E-06
1776,natural_language_inference20,87,We further perform additional linguistic error analyses using the MultiNLI Annotation Dataset and the Breaking NLI dataset .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,4,1,0,,0.041100313,0,negative,0.004665583,1.27E-05,0.026747772,3.31E-06,5.28E-06,0.000311701,0.003938077,0.000796879,2.04E-06,0.899164778,2.45E-06,0.064250051,9.94E-05
1777,natural_language_inference20,88,"Finally , in order to test the ability of the model to learn generalpurpose representations , we apply the downstream tasks thatare bundled in the SentEval package for sentence embedding evaluation .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,5,1,0,,0.01788033,0,negative,0.000700739,3.35E-06,0.034510968,9.21E-07,1.30E-06,0.000296232,0.003117617,0.000635489,8.04E-07,0.947283,1.25E-06,0.013415409,3.29E-05
1778,natural_language_inference20,89,Note that we combine SNLI and MultiNLI data in those experiments in order to be compatible with related work .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,6,1,0,,0.000506942,0,negative,0.000148083,3.43E-07,0.002570325,2.57E-07,1.94E-07,0.00015715,0.000121574,0.000310425,4.17E-07,0.995774867,1.01E-07,0.000912252,4.01E-06
1779,natural_language_inference20,90,Below we provide a few more details about each of the evaluation frameworks .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,7,1,0,,0.000161405,0,negative,2.83E-05,4.93E-07,0.000138095,4.78E-07,1.01E-07,0.000287573,8.41E-05,0.001248205,7.49E-07,0.997913695,2.20E-07,0.00029354,4.51E-06
1780,natural_language_inference20,91,SNLI :,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,8,1,0,,0.875677292,1,baselines,0.000166184,2.47E-07,0.948763294,9.82E-08,5.08E-08,4.30E-05,0.001319947,5.24E-05,2.98E-07,0.036669889,2.22E-06,0.012938695,4.36E-05
1781,natural_language_inference20,92,"The Stanford Natural Language Inference ( SNLI ) corpus ) is a dataset of 570 k human- written sentence pairs manually labeled with the gold labels entailment , contradiction , and neutral .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,9,1,0,,0.040150714,0,negative,0.00069368,5.26E-06,0.109361292,0.000139446,7.33E-05,0.001846811,0.032759127,0.000712947,2.03E-06,0.829159692,3.45E-05,0.022334338,0.002877554
1782,natural_language_inference20,93,"The dataset is divided into training ( 550,152 pairs ) , development ( 10,000 pairs ) and test sets ( 10,000 pairs ) .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,10,1,0,,0.015290903,0,negative,0.000220861,2.17E-06,0.010634747,1.67E-06,5.60E-06,0.00034089,0.001098114,0.000797475,1.05E-06,0.985394045,2.90E-07,0.001466161,3.69E-05
1783,natural_language_inference20,94,The source for the premise sentences in SNLI were image captions taken from the Flickr30 k corpus .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,11,1,0,,0.008367666,0,negative,5.16E-05,2.28E-06,0.004498711,1.18E-05,1.04E-05,0.002994148,0.003213889,0.003900132,1.36E-06,0.984318376,1.02E-06,0.000869341,0.000126847
1784,natural_language_inference20,95,"MultiNLI : The Multi-Genre Natural Language Inference ( MultiNLI ) corpus ) is a broad - coverage corpus for natural language inference , consisting of 433 k human- written sentence pairs labeled with entailment , contradiction and neutral .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,12,1,0,,0.889511883,1,negative,0.001186357,9.55E-06,0.271389066,0.000259581,8.73E-05,0.002827872,0.099566982,0.001056405,3.19E-06,0.54555598,6.49E-05,0.065599627,0.012393185
1785,natural_language_inference20,96,"Unlike the SNLI corpus , which draws the premise sentence from image captions , MultiNLI consists of sentence pairs from ten distinct genres of both written and spoken English .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,13,1,0,,0.006620454,0,negative,0.000252929,4.22E-06,0.022477213,2.98E-05,4.80E-05,0.000690926,0.004108883,0.000551173,1.62E-06,0.967689371,2.50E-06,0.00386561,0.000277706
1786,natural_language_inference20,97,"The dataset is divided into training ( 392,702 pairs ) , development ( 20,000 pairs ) and test sets ( 20,000 pairs ) .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,14,1,0,,0.026687908,0,negative,0.000245184,2.38E-06,0.01199677,1.91E-06,8.26E-06,0.000305394,0.001136776,0.000696999,1.12E-06,0.983589737,2.16E-07,0.001966584,4.87E-05
1787,natural_language_inference20,98,Only five genres are included in the training set .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,15,1,0,,0.020530921,0,negative,9.36E-05,8.58E-07,0.001574686,7.60E-07,2.13E-06,0.000527426,0.00094223,0.002530025,4.95E-07,0.992844871,8.86E-08,0.001445238,3.76E-05
1788,natural_language_inference20,99,"The development and test sets have been divided into matched and mismatched , where the former includes only sentences from the same genres as the training data , and the latter includes sentences from the remaining genres not present in the training data .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,16,1,0,,0.002511428,0,negative,0.000109281,8.38E-07,0.009564482,1.86E-07,8.25E-07,0.000110928,0.000619142,0.000457791,4.48E-07,0.986528776,8.77E-08,0.002594542,1.27E-05
1789,natural_language_inference20,100,"In addition to the training , development and test sets , MultiNLI provides a smaller annotation dataset , which contains approximately 1000 sentence pairs annotated with linguistic properties of the sentences and is split between the matched and mismatched datasets .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,17,1,0,,0.032682075,0,negative,0.000290531,2.81E-06,0.022835309,1.89E-05,4.26E-05,0.000678301,0.004082388,0.000680289,1.85E-06,0.965457331,6.56E-07,0.005558006,0.00035094
1790,natural_language_inference20,101,2,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,18,1,0,,0.000302273,0,negative,2.43E-05,1.12E-07,0.000398067,7.28E-08,2.97E-08,7.83E-05,7.04E-05,0.000314209,5.88E-07,0.998706123,1.38E-07,0.000400732,6.88E-06
1791,natural_language_inference20,102,This dataset provides a simple way to assess what kind of sentence pairs an NLI system is able to predict correctly and where it makes errors .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,19,1,0,,0.003845254,0,negative,0.000489499,1.89E-06,0.024501609,2.38E-05,6.56E-05,0.000290028,0.00306974,0.000178645,1.00E-06,0.954243686,1.38E-06,0.016712578,0.000420534
1792,natural_language_inference20,103,We use the annotation dataset to perform linguistic error analysis of our model and compare the results to results obtained with InferSent .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,20,1,0,,0.056516249,0,negative,0.00043756,5.77E-06,0.019054078,6.28E-06,2.37E-05,0.000306902,0.004178534,0.000658071,1.25E-06,0.956108971,6.88E-07,0.018924296,0.000293925
1793,natural_language_inference20,104,For our experiment with the annotation dataset we use the annotations for the MultiNLI mismatched dataset .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,21,1,0,,0.014304413,0,negative,6.02E-05,5.23E-07,0.009325031,1.74E-07,1.28E-06,0.000153726,0.001130809,0.000400159,2.04E-07,0.983825539,3.85E-08,0.005083821,1.85E-05
1794,natural_language_inference20,105,SciTail : SciTail ) is an NLI dataset created from multiple - choice science exams consisting of 27 k sentence pairs .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,22,1,0,,0.227020545,0,negative,0.000841738,8.22E-06,0.119622235,0.000260925,0.00011996,0.00405839,0.06084403,0.002037761,3.19E-06,0.758109297,1.18E-05,0.041133071,0.012949336
1795,natural_language_inference20,106,Each question and the correct answer choice have been converted into an assertive statement to form the hypothesis .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,23,1,0,,0.001080369,0,negative,8.89E-05,6.73E-07,0.009348576,8.68E-08,4.55E-07,6.35E-05,0.000157794,0.000257795,2.04E-06,0.988982459,5.92E-08,0.001081037,1.67E-05
1796,natural_language_inference20,107,"The dataset is divided into training ( 23,596 pairs ) , development ( 1,304 pairs ) and test sets ( 2,126 pairs ) .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,24,1,0,,0.040048624,0,negative,0.000366927,2.31E-06,0.01018821,3.19E-06,1.81E-05,0.000198033,0.000979873,0.00047641,1.07E-06,0.984531971,7.66E-08,0.003159868,7.39E-05
1797,natural_language_inference20,108,"Unlike the SNLI and MultiNLI datasets , SciTail uses only two labels : entailment and neutral .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,25,1,0,,0.014439442,0,negative,0.000698695,6.21E-06,0.148623819,1.50E-06,1.03E-05,0.000199123,0.005074517,0.000518421,1.54E-06,0.802818778,5.77E-07,0.041838508,0.00020805
1798,natural_language_inference20,109,Breaking NLI : Breaking NLI is a test set which is constructed by taking premises from the SNLI training set and constructing several hypotheses from them by changing at most one word within the premise .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,26,1,0,,0.424757031,0,negative,0.000186185,2.40E-06,0.12581311,1.30E-06,1.07E-06,0.000299572,0.011956021,0.000766512,1.04E-06,0.752836187,3.35E-05,0.106776913,0.001326245
1799,natural_language_inference20,110,It was constructed to highlight how poorly current neural network models for NLI can handle lexical meaning .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,27,1,0,,0.004473123,0,negative,0.000176667,5.35E-06,0.010745262,9.38E-06,6.01E-06,0.000339674,0.000696048,0.00088071,5.36E-06,0.984211092,4.86E-06,0.002505682,0.000413905
1800,natural_language_inference20,111,SentEval : SentEval ) is a library for evaluating the quality of sentence embeddings .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,28,1,0,,0.92642148,1,baselines,0.001136688,8.83E-06,0.490780423,0.000161548,2.53E-05,0.004692329,0.164906743,0.003465912,5.80E-06,0.199256686,1.82E-05,0.094113386,0.041428128
1801,natural_language_inference20,112,3,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,29,1,0,,0.000373517,0,negative,3.00E-05,7.62E-08,0.000184919,6.34E-08,4.58E-08,3.92E-05,5.25E-05,0.000204067,3.83E-07,0.998790725,3.22E-08,0.000688928,8.96E-06
1802,natural_language_inference20,113,It contains 17 downstream tasks as well as 10 probing tasks .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,30,1,0,,0.109858766,0,negative,0.001299821,1.86E-05,0.058445928,3.00E-05,4.00E-05,0.002172365,0.015419216,0.006708537,1.40E-05,0.895685267,7.98E-07,0.016269229,0.003896279
1803,natural_language_inference20,114,"The downstream datasets included in the tests were MR movie reviews , CR product reviews , SUBJ subjectivity status , MPQA opinion - polarity , SST binary sentiment analysis , TREC question - type classification , MRPC paraphrase detection , SICK - Relatedness ( SICK - R ) semantic textual similarity , SICK - Entailment ( SICK - E ) natural language inference and STS14 semantic textual similarity .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,31,1,0,,0.158979472,0,negative,7.22E-05,1.70E-06,0.008964299,1.99E-06,1.16E-05,0.000233272,0.004100014,0.000492026,4.76E-07,0.974036941,3.98E-07,0.01170723,0.000377758
1804,natural_language_inference20,115,The probing tasks evaluate how well the sentence encodings are able to capture the following linguistic properties :,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,32,1,0,,0.01027432,0,negative,8.38E-05,4.76E-07,0.002617448,3.90E-08,1.67E-07,2.39E-05,0.000249797,0.000156856,2.47E-07,0.985217191,1.31E-07,0.011633029,1.69E-05
1805,natural_language_inference20,116,"Length prediction , Word Content analysis ,",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,33,1,0,,0.269655882,0,negative,0.000349882,1.85E-06,0.04186104,1.16E-06,2.28E-06,0.000201758,0.028063844,0.000495375,1.10E-06,0.603876977,1.55E-05,0.323060861,0.002068372
1806,natural_language_inference20,117,"Tree depth prediction , Top Constituents prediction , Word order analysis , Verb tense prediction , Subject number prediction , Object number prediction , Semantic odd man out and Coordination Inversion .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,34,1,0,,0.836941615,1,negative,0.000281679,3.16E-06,0.135257454,2.88E-06,1.69E-06,0.000757993,0.033296153,0.001184329,4.17E-06,0.737856581,3.34E-05,0.08704742,0.004273105
1807,natural_language_inference20,118,"For the SentEval tasks we trained our model on NLI data consisting of the concatenation of the SNLI and MultiNLI training sets consisting of 942,854 sentence pairs in total .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,35,1,0,,0.075570665,0,negative,0.000168483,4.55E-06,0.009570709,2.60E-06,2.62E-05,0.000292137,0.003303245,0.000999062,8.71E-07,0.972002146,1.12E-07,0.013284246,0.000345653
1808,natural_language_inference20,119,This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data .,Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,36,1,0,,0.005217263,0,negative,4.46E-05,5.38E-08,0.000597621,8.29E-09,5.45E-08,9.47E-06,8.38E-05,3.46E-05,5.27E-08,0.993246013,5.92E-09,0.005981457,2.23E-06
1809,natural_language_inference20,120,"have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks , compared to training the model only on SNLI data .",Evaluation Benchmarks,Evaluation Benchmarks,natural_language_inference,20,37,1,0,,0.153963012,0,negative,0.000575078,1.96E-07,0.006848848,9.28E-08,3.17E-07,2.32E-05,0.002088897,9.56E-05,1.13E-07,0.698252593,4.06E-07,0.291988441,0.000126196
1810,natural_language_inference20,121,Model Performance on the NLI task,Evaluation Benchmarks,,natural_language_inference,20,38,1,0,,0.027981606,0,results,9.78E-05,8.83E-08,0.005281375,5.27E-08,1.69E-07,1.40E-05,0.013084356,4.78E-05,2.99E-08,0.09939931,3.55E-07,0.881717783,0.000356811
1811,natural_language_inference20,122,"In this section , we discuss the performance of the proposed sentence - encoding approach in common natural language inference benchmarks .",Evaluation Benchmarks,Model Performance on the NLI task,natural_language_inference,20,39,1,0,,0.259623529,0,negative,0.000137357,5.15E-08,3.74E-05,2.59E-08,3.73E-08,1.45E-06,0.000224419,3.83E-06,5.13E-08,0.963938054,2.54E-08,0.035560593,9.67E-05
1812,natural_language_inference20,123,"From the experiments , we can conclude that the model provides strong results on all of the three NLI datasets .",Evaluation Benchmarks,Model Performance on the NLI task,natural_language_inference,20,40,1,0,,0.960755527,1,results,0.00089979,2.92E-09,3.30E-06,2.11E-08,6.23E-09,1.02E-06,0.001745181,2.78E-06,3.99E-09,0.082402467,9.50E-09,0.914193084,0.000752338
1813,natural_language_inference20,124,It clearly outperforms the similar but non-hierarchical BiLSTM models reported in the literature and fares well in comparison to other state of the art architectures in the sentence encoding category .,Evaluation Benchmarks,Model Performance on the NLI task,natural_language_inference,20,41,1,1,results,0.953692018,1,results,0.000363226,7.14E-09,1.98E-05,6.33E-08,1.09E-08,2.51E-06,0.003224555,4.25E-06,1.26E-08,0.1023419,3.37E-08,0.890932065,0.003111594
1814,natural_language_inference20,125,"In particular , our results are close to the current state of the art on SNLI in this category and strong on both , the matched and mismatched test sets of MultiNLI .",Evaluation Benchmarks,Model Performance on the NLI task,natural_language_inference,20,42,1,1,results,0.934247943,1,results,0.000372019,1.59E-08,1.21E-05,8.20E-08,2.62E-08,3.06E-06,0.001820042,7.40E-06,3.01E-08,0.378227427,4.09E-08,0.617783591,0.001774159
1815,natural_language_inference20,126,"Finally , on SciTail , we achieve the new state of the art with an accuracy of 86.0 % .",Evaluation Benchmarks,Model Performance on the NLI task,natural_language_inference,20,43,1,1,results,0.97455434,1,results,0.000421875,5.00E-09,1.82E-05,1.56E-08,9.22E-09,5.49E-07,0.001453751,1.07E-06,5.56E-09,0.075464385,7.31E-09,0.921252899,0.001387242
1816,natural_language_inference20,127,"Below , we provide additional details on our results for each of the benchmarks .",Evaluation Benchmarks,Model Performance on the NLI task,natural_language_inference,20,44,1,0,,0.013487083,0,negative,1.25E-05,4.16E-09,2.32E-06,4.51E-08,1.76E-08,1.16E-06,1.01E-05,3.64E-06,2.90E-08,0.999140436,7.48E-10,0.000794524,3.53E-05
1817,natural_language_inference20,128,"We compare our model only with other state - of - the - art sentence encoding models and exclude cross - sentence attention models , except for SciTail where previous sentence encoding model - based results have not been published .",Evaluation Benchmarks,Model Performance on the NLI task,natural_language_inference,20,45,1,0,,0.136861209,0,negative,5.47E-05,2.02E-08,0.000134709,2.24E-08,6.62E-08,1.27E-06,6.09E-05,1.95E-06,4.11E-08,0.995869864,2.32E-09,0.003826814,4.97E-05
1818,natural_language_inference20,129,SNLI,Evaluation Benchmarks,,natural_language_inference,20,46,1,0,,0.020867138,0,negative,0.000256664,3.82E-07,0.003012054,6.10E-07,8.46E-07,0.00019992,0.00333838,0.001321501,1.45E-06,0.939847087,1.25E-07,0.051133173,0.00088781
1819,natural_language_inference20,130,"For the SNLI dataset , our model provides the test accuracy of 86.6 % after 4 epochs of training .",Evaluation Benchmarks,SNLI,natural_language_inference,20,47,1,1,results,0.953549563,1,results,0.000348279,2.57E-09,8.39E-06,1.28E-08,4.22E-09,3.54E-07,0.000819311,1.52E-06,3.70E-09,0.068104864,2.33E-09,0.927701098,0.003016163
1820,natural_language_inference20,131,"The comparison of our results with the previous state of the art and selected other sentence embedding based results are reported in and c by Yoon , Lee , and Lee ( 2018 ) .",Evaluation Benchmarks,SNLI,natural_language_inference,20,48,1,0,,0.039319559,0,negative,1.48E-05,2.68E-09,1.70E-05,2.26E-08,1.19E-08,7.76E-07,2.77E-05,1.88E-06,1.54E-08,0.994684703,1.98E-09,0.004811391,0.000441685
1821,natural_language_inference20,132,MultiNLI,Evaluation Benchmarks,,natural_language_inference,20,49,1,0,,0.002881754,0,negative,2.11E-05,1.32E-07,0.001467655,7.80E-08,1.32E-07,0.000160708,0.000645198,0.000944378,7.93E-07,0.994696824,1.59E-08,0.001977299,8.57E-05
1822,natural_language_inference20,133,"For the MultiNLI matched test set ( MultiNLI - m ) our model achieves a test accuracy of 73.7 % after 3 epochs of training , which is 0.8 % points lower than the state of the art 74.5 % by .",Evaluation Benchmarks,MultiNLI,natural_language_inference,20,50,1,1,results,0.941182734,1,results,0.000306923,4.69E-09,3.01E-05,1.64E-08,2.16E-08,1.56E-06,0.013381415,1.24E-06,4.86E-09,0.100843704,3.03E-09,0.87289727,0.012537778
1823,natural_language_inference20,134,"For the mismatched test set ( MultiNLI - mm ) our model achieves a test accuracy of 73.0 % after 3 epochs of training , which is 0.6 % points lower than the state of the art 73.6 % by Chen , Zhu , Ling , Wei , Jiang , and Inkpen ( 2017 b ) .",Evaluation Benchmarks,MultiNLI,natural_language_inference,20,51,1,1,results,0.926093605,1,results,0.000357808,4.13E-09,2.25E-05,1.49E-08,1.59E-08,1.46E-06,0.012305942,1.38E-06,4.83E-09,0.092501868,2.94E-09,0.884832196,0.009976824
1824,natural_language_inference20,135,A comparison of our results with the previous state of the art and selected other approaches are reported in .,Evaluation Benchmarks,MultiNLI,natural_language_inference,20,52,1,0,,0.023214534,0,negative,3.11E-05,3.33E-09,1.47E-05,5.21E-08,4.36E-08,2.71E-06,0.000204511,1.42E-06,2.03E-08,0.992284065,1.64E-09,0.006040004,0.001421399
1825,natural_language_inference20,136,"Although we did not achieve state of the art results for the MultiNLI dataset , we believe that a systematic study of different BiLSTM max pooling structures could reveal an architecture providing the needed improvement .",Evaluation Benchmarks,MultiNLI,natural_language_inference,20,53,1,0,,0.139384539,0,negative,0.000124246,3.47E-09,2.02E-05,1.34E-08,1.07E-08,3.00E-06,0.001041717,2.37E-06,2.58E-08,0.963038645,3.50E-09,0.0343117,0.001458095
1826,natural_language_inference20,137,SciTail,Evaluation Benchmarks,,natural_language_inference,20,54,1,0,,0.029711942,0,negative,0.000506368,2.44E-07,0.004826382,9.40E-07,1.16E-06,8.78E-05,0.000968845,0.000288434,7.54E-07,0.967633389,2.97E-08,0.025024926,0.00066077
1827,natural_language_inference20,138,"On the SciTail dataset we compared our model also against non-sentence embedding - based models , as no results have been previously published which are based on independent sentence embeddings .",Evaluation Benchmarks,SciTail,natural_language_inference,20,55,1,1,results,0.891520202,1,negative,0.000113184,2.44E-08,0.000107547,5.23E-09,9.66E-08,4.75E-06,0.006250943,1.36E-06,3.26E-08,0.981034199,1.37E-09,0.012211607,0.000276247
1828,natural_language_inference20,139,"We obtain a score of 86.0 % after 4 epochs of training , which is + 2.7 % points absolute improvement on the previous published state of the art by .",Evaluation Benchmarks,SciTail,natural_language_inference,20,56,1,1,results,0.882350886,1,results,0.002684392,6.89E-08,0.000103722,1.01E-07,4.01E-07,1.71E-05,0.139204698,7.78E-06,1.23E-07,0.383642019,8.33E-09,0.404733042,0.069606547
1829,natural_language_inference20,140,Our model also outperforms In - fer Sent which achieves an accuracy of 85.1 % in our experiments .,Evaluation Benchmarks,SciTail,natural_language_inference,20,57,1,1,results,0.885969986,1,results,0.001055861,1.39E-08,3.96E-05,1.49E-07,1.75E-07,1.61E-05,0.123708035,6.30E-06,1.52E-08,0.157057913,6.65E-09,0.637947797,0.080168043
1830,natural_language_inference20,141,The comparison of our results with the previous state of the art results are reported in .,Evaluation Benchmarks,SciTail,natural_language_inference,20,58,1,0,,0.024780051,0,negative,1.71E-05,3.11E-09,1.33E-05,1.79E-08,9.08E-08,3.52E-06,0.000258197,8.86E-07,1.92E-08,0.998734806,5.99E-10,0.000621763,0.000350279
1831,natural_language_inference20,142,The results achieved by our proposed model are significantly higher than the previously published results .,Evaluation Benchmarks,SciTail,natural_language_inference,20,59,1,1,results,0.884915571,1,negative,0.000498421,1.05E-08,1.71E-05,1.47E-08,4.21E-08,7.00E-06,0.076369188,4.35E-06,2.14E-08,0.576111505,3.64E-09,0.335955707,0.011036643
1832,natural_language_inference20,143,It has been argued that the lexical similarity of the sentences in SciTail sentence pairs make it a particularly difficult dataset ) .,Evaluation Benchmarks,SciTail,natural_language_inference,20,60,1,0,,0.003201866,0,negative,7.91E-06,3.30E-09,2.01E-05,4.80E-08,7.60E-08,5.30E-06,0.000256375,1.01E-06,1.95E-08,0.997735172,1.26E-08,0.000345758,0.001628213
1833,natural_language_inference20,144,"If this is the case , we hypothesize that our model is indeed better at identifying entailment relations beyond focusing on the lexical similarity of the sentences .",Evaluation Benchmarks,SciTail,natural_language_inference,20,61,1,0,,0.002939854,0,negative,2.04E-05,1.08E-08,1.17E-05,1.21E-08,3.76E-08,4.07E-06,0.00014999,4.61E-06,2.15E-07,0.999427692,4.89E-10,0.000106342,0.000274937
1834,natural_language_inference20,145,Error Analysis of NLI Predictions,Evaluation Benchmarks,,natural_language_inference,20,62,1,0,,0.071202822,0,negative,0.000152331,6.32E-07,0.004386503,5.95E-07,7.60E-07,8.82E-05,0.030065325,0.000424117,6.53E-07,0.655241221,2.75E-06,0.300919382,0.008717524
1835,natural_language_inference20,146,"To better understand what kind of inferential relationships our model is able to identify , we conducted an error analysis for the three datasets .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,63,1,0,,0.001815958,0,negative,0.000153038,3.01E-08,2.40E-06,2.47E-09,1.48E-07,5.98E-07,0.000567704,2.82E-07,5.96E-08,0.998754017,2.60E-10,0.00050658,1.51E-05
1836,natural_language_inference20,147,We report the results below .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,64,1,0,,0.001434779,0,negative,2.54E-05,2.93E-09,7.13E-07,2.15E-08,2.29E-07,8.15E-07,0.000137314,2.07E-07,1.02E-08,0.999641089,1.01E-10,0.000161582,3.27E-05
1837,natural_language_inference20,148,shows the accuracy of predictions per label ( in terms of F-scores ) for the HBMP model and compares them to the InferSent model .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,65,1,0,,0.010984653,0,negative,2.82E-05,3.11E-09,2.97E-06,8.00E-10,2.34E-08,4.84E-07,0.001333619,1.61E-07,1.33E-08,0.997766135,2.78E-10,0.000826658,4.17E-05
1838,natural_language_inference20,149,This analysis shows that our model leads to a significant improvement over the outcome of the nonhierarchical model from previous work in almost all categories on all the three benchmarks .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,66,1,0,,0.947067402,1,negative,0.012081566,6.61E-08,7.23E-06,4.45E-08,1.46E-07,4.60E-06,0.245599254,2.34E-06,6.26E-08,0.423775495,5.15E-09,0.313215349,0.005313852
1839,natural_language_inference20,150,"The only exception is the entailment score on SciTail , which is slightly below the performance of InferSent .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,67,1,0,,0.43215367,0,negative,0.003188271,1.50E-08,5.66E-06,3.72E-08,3.47E-07,3.84E-06,0.101685711,1.10E-06,1.02E-08,0.698573816,2.88E-09,0.193010663,0.003530522
1840,natural_language_inference20,151,"To see in more detail how our HBMP model is able to classify sentence pairs with different labels and what kind of errors it makes , we summarize error statistics as confusion matrices for the different datasets .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,68,1,0,,0.003130434,0,negative,1.83E-05,5.02E-09,4.02E-07,6.45E-10,3.02E-08,2.87E-07,0.000321897,1.80E-07,1.03E-08,0.999330044,7.90E-11,0.000321578,7.22E-06
1841,natural_language_inference20,152,They highlight the HBMP model 's strong performance across all the labels .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,69,1,0,,0.369976533,0,negative,0.001279316,7.57E-09,2.70E-06,3.15E-08,1.91E-07,1.60E-06,0.002300107,4.35E-07,1.89E-08,0.991398617,3.16E-10,0.004817804,0.000199175
1842,natural_language_inference20,153,On the SNLI dataset our model clearly outperforms Infer Sent on all labels in terms of precision and recall .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,70,1,0,,0.964213748,1,results,0.004286391,2.67E-08,9.39E-06,2.79E-08,9.96E-08,3.77E-06,0.240620426,1.22E-06,2.07E-08,0.274203688,3.81E-09,0.470610562,0.010264373
1843,natural_language_inference20,154,contains the confusion matrices for that dataset comparing HBMP to InferSent .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,71,1,0,,0.00037261,0,negative,7.44E-06,2.36E-09,1.52E-06,4.16E-09,1.05E-07,9.30E-07,0.00016058,2.56E-07,1.44E-08,0.9997592,4.42E-11,4.07E-05,2.92E-05
1844,natural_language_inference20,155,"The precision on contradiction exceeds 90 % for our model and reaches high recall values for both , entailment and contradiction .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,72,1,0,,0.847526318,1,negative,0.006867649,3.03E-08,7.88E-06,2.08E-08,2.35E-07,3.26E-06,0.133960205,1.35E-06,2.47E-08,0.566049224,2.28E-09,0.284200631,0.008909489
1845,natural_language_inference20,156,"The performance is lower for neutral and the confusion of that label with both , contradiction and entailment is higher .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,73,1,0,,0.456132066,0,negative,0.002821574,5.16E-09,2.54E-06,7.62E-09,1.13E-07,9.72E-07,0.02032501,4.30E-07,5.46E-09,0.886792592,3.83E-10,0.089545237,0.000511509
1846,natural_language_inference20,157,"However , HBMP still outperforms",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,74,1,0,,0.380530545,0,negative,0.009709874,2.92E-08,2.80E-05,2.08E-08,1.88E-07,3.60E-06,0.132558067,8.21E-07,3.01E-08,0.626278836,2.14E-09,0.226263813,0.005156718
1847,natural_language_inference20,158,Infer Sent by a similar margin as for the other two labels .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,75,1,0,,0.000886543,0,negative,0.000260597,2.06E-08,4.30E-05,4.69E-09,1.39E-07,7.03E-07,0.000513299,2.30E-07,2.53E-07,0.9985427,1.43E-10,0.000521214,0.000117848
1848,natural_language_inference20,159,"Unlike for the SNLI and both of the MultiNLI datasets , on the SciTail dataset our model is most accurate on sentence pairs labeled neutral , having an F- score 88.9 % compared to pairs marked with entailment , where the F- score was 81.0 % .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,76,1,0,,0.940419051,1,negative,0.005114026,3.14E-08,7.21E-06,4.04E-08,2.09E-07,4.09E-06,0.199694032,2.13E-06,2.19E-08,0.399646009,1.58E-09,0.385066084,0.010466115
1849,natural_language_inference20,160,"Infer Sent has slightly higher accuracy on entailment , whereas HBMP outperforms InferSent on neutral .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,77,1,0,,0.765568088,1,negative,0.004486094,2.08E-08,1.05E-05,1.95E-08,1.91E-07,3.65E-06,0.267418237,1.11E-06,1.22E-08,0.399356657,8.74E-10,0.317982652,0.010740812
1850,natural_language_inference20,161,contains the confusion matrices for the SciTail dataset comparing the HBMP to InferSent .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,78,1,0,,0.004081555,0,negative,2.19E-05,2.01E-09,2.18E-06,5.29E-09,9.88E-08,1.24E-06,0.000813395,3.26E-07,1.02E-08,0.998807066,4.54E-11,0.000205752,0.000148071
1851,natural_language_inference20,162,This analysis reveals that our model mainly suffers in recall on entailment detection whereas it performs well for neutral with respect to recall .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,79,1,0,,0.691720665,1,negative,0.023799595,5.23E-08,5.89E-06,2.12E-08,2.52E-07,2.12E-06,0.040811991,1.11E-06,4.57E-08,0.865814413,5.84E-10,0.06821139,0.001353124
1852,natural_language_inference20,163,It is difficult to say what the reason might be for the mismatch between the two systems but the over all performance of our architecture suggests that it is superior to the InferSent model even though the balance between precision and recall on individual labels is different .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,80,1,0,,0.000557051,0,negative,0.000153706,5.72E-09,8.82E-07,4.42E-09,3.55E-08,1.38E-06,0.001736779,7.13E-07,2.52E-08,0.997215451,1.71E-10,0.000650322,0.000240691
1853,natural_language_inference20,164,The error analysis of the MultiNLI dataset is not standard as it can not be based on test data .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,81,1,0,,0.001558464,0,negative,1.56E-05,1.33E-09,1.17E-06,1.59E-09,3.15E-08,3.70E-07,0.000533062,1.07E-07,3.27E-09,0.998697627,2.43E-10,0.000646485,0.00010559
1854,natural_language_inference20,165,"As the labeled test data is not openly available for MultiNLI , we analyzed the error statistics for this dataset based on the development data .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,82,1,0,,0.000968029,0,negative,3.27E-05,7.09E-09,1.73E-06,1.77E-09,3.10E-07,3.66E-07,0.000330871,1.41E-07,6.92E-09,0.999394856,1.13E-11,0.000222433,1.66E-05
1855,natural_language_inference20,166,For the matched dataset ( MultiNLI - m ) our model had a development accuracy of 74.1 % .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,83,1,0,,0.941672934,1,negative,0.003081971,2.67E-08,1.76E-05,2.51E-08,3.09E-07,5.59E-06,0.177173515,1.63E-06,4.36E-08,0.668795731,3.76E-10,0.135853233,0.015070351
1856,natural_language_inference20,167,"For MultiNLI -m our model has the best accuracy on sentence pairs labeled with entailment , having an F- score of 77.2 % .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,84,1,0,,0.965183141,1,negative,0.004131937,2.73E-08,1.52E-05,3.34E-08,3.32E-07,4.59E-06,0.286175514,1.61E-06,1.58E-08,0.378755961,3.96E-10,0.311656005,0.019258753
1857,natural_language_inference20,168,"The model is also almost as accurate in predicting contradictions , with an F- score of 75.3 % .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,85,1,0,,0.92446358,1,negative,0.002072339,2.55E-08,5.89E-06,4.09E-08,2.25E-07,6.65E-06,0.308885498,3.26E-06,2.44E-08,0.488785409,6.77E-10,0.16504431,0.035196324
1858,natural_language_inference20,169,"Similar to SNLI , our model is less effective on sentence pairs labeled with neutral , having an F- score of 68.2 % but , again , the HBMP model outperforms the InferSent on all the labels . HBMP to InferSent .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,86,1,0,,0.902464275,1,experiments,0.002449167,1.12E-08,3.47E-06,2.37E-08,7.22E-08,4.73E-06,0.381613801,2.67E-06,1.06E-08,0.368024829,5.77E-10,0.226803806,0.021097403
1859,natural_language_inference20,170,"Our model improves upon InferSent in all values of precision and recall , in some cases by a wide margin .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,87,1,0,,0.939101555,1,negative,0.006497156,3.64E-08,8.18E-06,8.11E-08,1.75E-07,9.75E-06,0.344830417,3.98E-06,4.10E-08,0.403890315,5.36E-10,0.204242727,0.040517142
1860,natural_language_inference20,171,For the MultiNLI mismatched dataset ( MultiNLI - mm ) our model had a development accuracy of 73.7 % .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,88,1,0,,0.958022467,1,negative,0.002396628,2.21E-08,1.61E-05,1.67E-08,1.96E-07,4.58E-06,0.231789278,1.34E-06,2.92E-08,0.585978249,2.80E-10,0.161794009,0.018019529
1861,natural_language_inference20,172,"or MultiNLI - mm our model has very similar performance as with the MultiNLI -m dataset , having the best accuracy on sentence pars labeled with entailment , having an F- score of 77.9 % .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,89,1,0,,0.934199354,1,negative,0.001201981,8.42E-09,8.96E-06,4.93E-09,7.97E-08,1.65E-06,0.203360076,5.19E-07,5.47E-09,0.516593521,2.23E-10,0.273142207,0.005690979
1862,natural_language_inference20,173,"The model is also almost as accurate in predicting contradictions , with an F- score of 75.6 % .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,90,1,0,,0.920400842,1,negative,0.001789919,2.04E-08,5.29E-06,3.10E-08,1.97E-07,5.96E-06,0.313349512,2.96E-06,1.92E-08,0.504593409,3.51E-10,0.142553622,0.037699058
1863,natural_language_inference20,174,"Our model is less effective on sentence pairs labeled with neutral , having an F- score of 68.6 % .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,91,1,0,,0.647608766,1,negative,0.006461071,1.60E-08,5.71E-06,1.09E-08,1.62E-07,2.37E-06,0.146333544,1.21E-06,1.09E-08,0.615291034,1.56E-10,0.22134808,0.010556784
1864,natural_language_inference20,175,contains the confusion matrices for the MultiNLI Mismatched dataset comparing the HBMP to InferSent and the picture is similar to the result of the matched dataset .,Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,92,1,0,,0.001446085,0,negative,2.28E-05,7.09E-10,1.11E-06,2.98E-10,1.27E-08,2.66E-07,0.000915686,7.59E-08,5.44E-09,0.998760549,7.48E-12,0.000236527,6.29E-05
1865,natural_language_inference20,176,"Substantial improvements can be seen again , in particular in the precision of contradiction detection .",Evaluation Benchmarks,Error Analysis of NLI Predictions,natural_language_inference,20,93,1,0,,0.333606444,0,negative,0.002049099,5.00E-09,2.89E-06,1.07E-08,6.81E-08,1.42E-06,0.03842909,6.09E-07,1.09E-08,0.892805105,1.33E-10,0.062529997,0.004181703
1866,natural_language_inference20,177,Evaluation of Linguistic Abstractions,,,natural_language_inference,20,0,1,0,,0.24841044,0,research-problem,5.52E-05,0.000102134,2.22E-05,2.31E-06,1.26E-06,3.19E-05,0.001259348,0.000416296,1.42E-05,0.27356225,0.714219408,0.010275005,3.86E-05
1867,natural_language_inference20,178,"The most interesting part of the sentence encoder approach to NLI is the ability of the system to learn generic sentence embeddings that capture abstractions , which can be useful for other downstream tasks as well .",Evaluation of Linguistic Abstractions,Evaluation of Linguistic Abstractions,natural_language_inference,20,1,1,0,,0.00171443,0,negative,9.38E-05,4.29E-06,3.77E-05,1.55E-06,1.81E-07,1.33E-05,0.000247892,3.45E-05,6.19E-06,0.991187229,0.004277157,0.004061756,3.45E-05
1868,natural_language_inference20,179,In order to understand the capabilities of our model we first look at the type of linguistic reasoning that the NLI system is able to learn using the MultiNLI annotation set and the Breaking NLI test set .,Evaluation of Linguistic Abstractions,Evaluation of Linguistic Abstractions,natural_language_inference,20,2,1,0,,2.52E-05,0,negative,0.000164498,7.57E-07,1.30E-05,1.60E-08,7.93E-08,1.35E-06,3.62E-05,4.26E-06,9.38E-07,0.997358562,5.58E-07,0.002419588,2.39E-07
1869,natural_language_inference20,180,"Thereafter , we evaluate downstream tasks using the SentEval library to study the use of our NLI - based sentence embeddings in transfer learning .",Evaluation of Linguistic Abstractions,Evaluation of Linguistic Abstractions,natural_language_inference,20,3,1,0,,0.004391371,0,negative,0.000578894,5.47E-06,0.000186565,3.56E-07,2.92E-06,7.07E-06,0.000637262,1.62E-05,2.55E-06,0.984097704,1.97E-06,0.014459178,3.84E-06
1870,natural_language_inference20,181,Linguistic Error Analysis of NLI Classifications,Evaluation of Linguistic Abstractions,,natural_language_inference,20,4,1,0,,0.048634729,0,negative,0.000144433,6.61E-06,7.10E-05,1.94E-06,2.33E-07,3.80E-05,0.008205366,0.000120115,9.45E-06,0.919225684,0.018292065,0.05353544,0.00034967
1871,natural_language_inference20,182,The MultiNLI annotation set makes it possible to conduct a detailed analysis of different linguistic phenomena when predicting inferential relationships .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,5,1,0,,0.00333467,0,negative,0.000775614,2.82E-06,5.74E-05,8.57E-05,0.000231271,0.00011017,0.000959571,1.39E-05,7.54E-06,0.994923419,6.71E-06,0.002798171,2.78E-05
1872,natural_language_inference20,183,We use this to compare our model to InferSent with respect to the type of linguistic properties .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,6,1,0,,4.89E-05,0,negative,1.40E-05,1.08E-06,4.51E-06,4.96E-08,1.63E-07,5.20E-06,2.12E-05,9.29E-06,4.19E-06,0.999854664,5.16E-07,8.49E-05,1.78E-07
1873,natural_language_inference20,184,MultiNLI - mismatched confusion matrices for HBMP and InferSent .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,7,1,0,,0.001462811,0,negative,0.000164064,8.20E-06,0.000469833,4.63E-08,1.01E-07,6.53E-06,0.000701819,1.21E-05,5.46E-05,0.987295001,0.000297159,0.010985976,4.63E-06
1874,natural_language_inference20,185,thatare present in the given sentence pairs .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,8,1,0,,0.000140541,0,negative,2.88E-05,2.81E-07,2.44E-06,6.52E-08,2.08E-07,4.66E-06,1.53E-05,5.65E-06,2.94E-06,0.999846444,4.32E-07,9.26E-05,2.26E-07
1875,natural_language_inference20,186,contains the comparison for the MultiNLI - mm dataset .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,9,1,0,,0.00084498,0,negative,5.71E-05,1.34E-07,1.13E-05,2.58E-08,1.31E-07,4.01E-06,0.000128779,5.08E-06,4.47E-07,0.998649645,2.71E-07,0.001142707,3.50E-07
1876,natural_language_inference20,187,"The analysis shows that our HBMP model outperforms InferSent with antonyms , coreference links , modality , negation , paraphrases and tense differences .",Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,10,1,0,,0.871036761,1,results,0.023132334,1.81E-06,1.93E-05,5.80E-07,1.26E-06,1.25E-05,0.008211906,2.64E-05,1.32E-06,0.311486493,5.49E-06,0.657059417,4.12E-05
1877,natural_language_inference20,188,It also produces improved scores for most of the other categories in entailment detection .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,11,1,0,,0.794660032,1,results,0.003237544,6.71E-07,1.33E-05,5.19E-07,6.19E-07,1.45E-05,0.010390736,3.12E-05,6.00E-07,0.225585803,6.88E-06,0.760652141,6.55E-05
1878,natural_language_inference20,189,Infer Sent gains especially with conditionals in contradiction and in the word overlap catehory for entailments .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,12,1,0,,0.002070823,0,negative,0.000806155,1.28E-06,4.88E-05,1.25E-06,1.40E-06,1.36E-05,0.001259059,8.89E-06,2.48E-06,0.982018014,0.000286314,0.015510188,4.25E-05
1879,natural_language_inference20,190,This seems to suggest that InferSent relies a lot on matching words to find entailment and specific constructions indicating contradictions .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,13,1,0,,0.000203969,0,negative,5.01E-05,1.63E-07,1.30E-06,5.08E-08,1.20E-07,3.62E-06,2.42E-05,4.27E-06,1.36E-06,0.999754287,1.30E-06,0.000158903,3.67E-07
1880,natural_language_inference20,191,HBMP does not seem to use word overlap as an indication for entailment that much and is better on detecting neutral sentences in this category .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,14,1,0,,0.000203682,0,negative,0.004018731,7.50E-07,6.00E-05,1.91E-07,1.40E-06,7.39E-06,0.002957636,9.19E-06,6.57E-07,0.738827935,6.43E-06,0.254091552,1.81E-05
1881,natural_language_inference20,192,This outcome may indicate that our model works with stronger lexical abstractions than InferSent .,Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,15,1,0,,0.000149726,0,negative,8.75E-05,3.24E-07,2.45E-06,2.39E-08,9.96E-08,2.92E-06,3.09E-05,5.12E-06,3.32E-06,0.999584533,3.93E-07,0.000282142,2.38E-07
1882,natural_language_inference20,193,"However , due to the small number of examples per annotation category and small differences in the scores in general , it is hard to draw reliable conclusions from this experiment .",Evaluation of Linguistic Abstractions,Linguistic Error Analysis of NLI Classifications,natural_language_inference,20,16,1,0,,0.000101549,0,negative,7.43E-05,1.20E-07,1.19E-06,2.89E-08,1.65E-07,3.20E-06,7.40E-05,4.62E-06,3.17E-07,0.998469815,2.80E-07,0.001371604,3.90E-07
1883,natural_language_inference20,194,Tests with the Breaking NLI dataset,Evaluation of Linguistic Abstractions,,natural_language_inference,20,17,1,0,,0.329297542,0,results,0.000383749,4.16E-07,7.23E-05,5.51E-08,1.20E-07,3.08E-06,0.010026586,8.06E-06,1.26E-07,0.232858704,4.09E-06,0.756607243,3.55E-05
1884,natural_language_inference20,195,In the second experiment we conducted testing of the proposed sentence embedding architecture using the Breaking NLI test set recently published by .,Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,18,1,0,,0.076494613,0,negative,0.000230663,1.34E-07,6.79E-05,8.54E-08,1.01E-07,1.77E-06,0.000389349,4.35E-06,1.84E-07,0.891913535,3.48E-07,0.107279629,0.000111974
1885,natural_language_inference20,196,The test set is designed to highlight the lack of lexical reasoning capability of NLI systems .,Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,19,1,0,,0.007543699,0,negative,6.33E-05,5.21E-08,2.83E-05,2.22E-07,1.60E-07,1.82E-06,3.34E-05,6.66E-06,2.23E-07,0.993176153,8.74E-08,0.00659967,8.99E-05
1886,natural_language_inference20,197,"For the Breaking NLI experiment , we trained our HBMP model and the InferSent model using the SNLI training data .",Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,20,1,0,,0.174911534,0,negative,0.000378778,1.62E-07,0.000218076,1.62E-07,1.97E-07,4.07E-06,0.000496392,1.15E-05,2.97E-07,0.940247759,1.13E-07,0.058516733,0.000125728
1887,natural_language_inference20,198,We compare our results with the results published by and to results obtained with InferSent sentence encoder ( our implementation ) .,Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,21,1,0,,0.15958228,0,negative,7.31E-05,6.39E-08,3.47E-05,3.50E-08,2.47E-08,9.88E-07,0.000122049,3.37E-06,1.48E-07,0.957028656,1.45E-07,0.042699268,3.74E-05
1888,natural_language_inference20,199,"The results show that our HBMP model outperforms the InferSent model in 7 out of 14 categories , receiving an over all score of 65.1 % ( InferSent : 65.6 % ) .",Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,22,1,0,,0.971425937,1,results,0.000426356,5.08E-09,2.97E-06,2.63E-08,4.58E-09,2.48E-07,0.000739265,8.73E-07,4.59E-09,0.017710939,3.94E-08,0.980888701,0.000230569
1889,natural_language_inference20,200,"Our model is especially strong with handling antonyms , which shows a good level of semantic abstraction on the lexical level .",Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,23,1,0,,0.728743396,1,negative,0.006870736,9.86E-07,0.001665183,1.16E-06,2.88E-07,5.04E-06,0.000309318,1.48E-05,1.83E-05,0.733488439,1.49E-06,0.256459435,0.00116484
1890,natural_language_inference20,201,"Infer Sent fares well in narrow categories like drinks , instruments and planets , which may indicate a problem of overfitting to prominent examples in the training data .",Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,24,1,0,,0.925699064,1,results,0.000491743,1.27E-08,9.38E-06,1.80E-07,1.13E-08,1.34E-06,0.00098472,3.80E-06,2.08E-08,0.037840513,1.94E-07,0.959610473,0.001057613
1891,natural_language_inference20,202,The strong result on the synonyms class may also come from a significant representation of related examples in training .,Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,25,1,0,,0.017452059,0,results,0.002788673,4.73E-08,1.37E-05,2.32E-07,5.48E-08,2.91E-06,0.000828542,1.21E-05,1.56E-07,0.469734903,1.79E-07,0.526147878,0.000470708
1892,natural_language_inference20,203,"However , more detailed investigations are necessary to verify this hypothesis .",Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,26,1,0,,0.003480219,0,negative,9.12E-05,2.76E-08,7.45E-06,4.25E-08,1.13E-08,1.16E-06,1.64E-05,6.01E-06,4.41E-07,0.99523457,8.41E-08,0.004599809,4.28E-05
1893,natural_language_inference20,204,"Our model also compares well against the other models , outperforming Decomposable Attention model ( 51.90 % ) and Residual Encoders ( 62.20 % ) in the over all score .",Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,27,1,0,,0.961901495,1,results,0.000290223,4.40E-09,2.71E-06,6.50E-08,4.78E-09,4.74E-07,0.00116458,1.56E-06,5.79E-09,0.022972878,3.62E-08,0.974993019,0.000574443
1894,natural_language_inference20,205,"As these models are not based purely on sentence embeddings , the obtained result highlights that sentence embedding approaches can be competitive when handling inferences requiring lexical information .",Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,28,1,0,,0.74030743,1,results,0.002965523,3.66E-08,1.67E-05,3.12E-08,1.66E-08,4.94E-07,0.000317259,1.71E-06,8.29E-08,0.367492182,9.39E-08,0.62914496,6.09E-05
1895,natural_language_inference20,206,The results of the comparison are summarized in .,Evaluation of Linguistic Abstractions,Tests with the Breaking NLI dataset,natural_language_inference,20,29,1,0,,0.006346946,0,negative,6.34E-05,5.48E-09,6.73E-06,1.59E-08,1.10E-08,4.08E-07,2.49E-05,1.13E-06,4.66E-08,0.986106502,1.29E-08,0.013776967,1.99E-05
1896,natural_language_inference20,207,Transfer Learning,Evaluation of Linguistic Abstractions,,natural_language_inference,20,30,1,0,,0.499502481,0,results,0.001856057,1.42E-06,0.000614605,2.11E-06,3.06E-06,2.81E-05,0.028399675,3.82E-05,1.40E-06,0.467626938,3.39E-06,0.500819493,0.000605574
1897,natural_language_inference20,208,"In this section , we focus on transfer learning experiments that apply sentence embeddings trained on NLI to other downstream tasks .",Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,31,1,0,,0.001445581,0,negative,0.000210221,6.39E-06,0.000109006,8.71E-07,2.46E-06,1.62E-05,0.001196582,2.63E-05,6.74E-06,0.994615036,8.17E-07,0.003709362,1.00E-04
1898,natural_language_inference20,209,"In order to better understand how well the sentence encoding model generalizes to different tasks , we conducted various tests implemented in the SentEval sentence embedding evaluation library ) and compared our results to the results published for In - fer Sent and SkipThought .",Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,32,1,0,,0.001467619,0,negative,3.48E-05,7.57E-08,4.05E-06,1.20E-08,7.77E-08,2.51E-06,0.000626745,3.04E-06,1.45E-07,0.996640052,2.38E-08,0.002683565,4.93E-06
1899,natural_language_inference20,210,"We used the SentEval library with the default settings recommended on their website , with a logistic regression classifier , Adam optimizer with learning rate of 0.001 , batch size of 64 and epoch size of 4 . lists the transfer learning results for our models with 600D and 1200D hidden dimensionality and compares it to the InferSent and SkipThought scores reported by .",Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,33,1,0,,0.004482116,0,negative,0.000144293,2.22E-07,2.39E-05,3.97E-07,6.04E-07,4.85E-05,0.005574047,3.82E-05,5.12E-07,0.990944323,3.39E-08,0.003133127,9.19E-05
1900,natural_language_inference20,211,Our 1200D model outperforms the InferSent model on 7 out of 10 tasks .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,34,1,0,,0.845464074,1,results,0.001749739,7.90E-08,8.07E-06,3.02E-07,1.39E-07,1.08E-05,0.060352635,1.60E-05,8.27E-08,0.0863537,1.21E-07,0.849641334,0.001867059
1901,natural_language_inference20,212,"The model achieves higher score on 8 out of 9 tasks reported for SkipThought , having equal score on the SUBJ dataset .",Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,35,1,0,,0.437706843,0,results,0.001251472,2.58E-08,5.51E-06,4.06E-08,4.51E-08,1.80E-06,0.021310637,2.93E-06,2.47E-08,0.064882068,3.82E-08,0.912167526,0.000377884
1902,natural_language_inference20,213,No MRPC results have been reported for SkipThought .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,36,1,0,,0.000380006,0,negative,2.54E-05,2.31E-08,7.64E-06,2.30E-08,4.51E-08,2.80E-06,0.000767645,3.01E-06,8.59E-08,0.994637344,1.31E-07,0.004521298,3.45E-05
1903,natural_language_inference20,214,"To study in more detail the linguistic properties of our proposed model , we also ran the recently published SentEval probing tasks .",Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,37,1,0,,0.000978605,0,negative,0.000104085,1.68E-07,1.29E-05,9.80E-08,1.18E-06,5.76E-06,0.001525476,4.49E-06,2.23E-07,0.991959183,5.32E-08,0.006331062,5.53E-05
1904,natural_language_inference20,215,Breaking NLI scores ( accuracy % ) .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,38,1,0,,0.000861632,0,negative,0.000103911,9.98E-09,5.55E-06,9.38E-09,2.52E-08,1.16E-06,0.000584508,9.76E-07,7.40E-08,0.990751801,2.37E-08,0.008534007,1.79E-05
1905,natural_language_inference20,216,Results marked with * as reported by .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,39,1,0,,1.68E-05,0,negative,7.66E-06,1.05E-08,1.04E-06,1.14E-08,1.53E-08,1.85E-06,0.000206517,3.42E-06,1.46E-07,0.999376138,9.86E-09,0.000395509,7.67E-06
1906,natural_language_inference20,217,InferSent results obtained with our implementation using the training set - up described in .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,40,1,0,,0.000427182,0,negative,0.000266783,3.63E-08,1.53E-05,4.37E-09,2.95E-08,1.32E-06,0.006684052,1.29E-06,6.58E-08,0.919923141,2.91E-08,0.073081537,2.64E-05
1907,natural_language_inference20,218,"To remain consistent with other work using SentEval , we report the accuracies as they are provided by the SentEval library .",Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,41,1,0,,4.02E-05,0,negative,1.59E-05,2.80E-08,3.96E-06,4.58E-08,2.33E-07,2.57E-06,0.000237105,2.93E-06,9.99E-08,0.998593132,5.48E-09,0.001137197,6.77E-06
1908,natural_language_inference20,219,model outperforms the InferSent model in 8 out of 10 probing tasks .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,42,1,0,,0.395832891,0,results,0.004376735,1.16E-07,4.05E-05,1.42E-07,2.21E-07,5.87E-06,0.040931606,6.45E-06,1.62E-07,0.225681403,4.95E-08,0.727996983,0.000959765
1909,natural_language_inference20,220,The results are listed in .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,43,1,0,,0.000155871,0,negative,3.37E-05,6.19E-09,2.91E-06,3.60E-09,3.12E-08,4.51E-07,0.000145814,4.95E-07,3.67E-08,0.996991367,2.81E-09,0.002822246,2.97E-06
1910,natural_language_inference20,221,Looking at both the downstream and the probing tasks we can observe strong results of our model compared to the InferSent model that already demonstrated good general abstractions on the sentence level according to the original publication by .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,44,1,0,,0.581688138,1,results,0.002457019,2.93E-08,4.34E-06,2.11E-08,4.69E-08,1.56E-06,0.019424953,2.24E-06,3.33E-08,0.180102998,2.40E-08,0.79780486,0.000201876
1911,natural_language_inference20,222,"Hence , HBMP does not only provide competitive NLI .",Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,45,1,0,,0.000430733,0,negative,0.003017909,1.65E-07,2.50E-05,3.55E-08,1.28E-07,1.81E-06,0.002991676,2.60E-06,3.11E-07,0.857819611,6.42E-08,0.13607846,6.22E-05
1912,natural_language_inference20,223,scores but also produces improved sentence embeddings thatare useful for other tasks .,Evaluation of Linguistic Abstractions,Transfer Learning,natural_language_inference,20,46,1,0,,0.001047378,0,negative,0.000955454,1.84E-08,5.20E-06,3.44E-08,1.14E-07,1.63E-06,0.006772639,2.47E-06,4.56E-08,0.670266897,1.71E-08,0.321801438,0.000194038
1913,natural_language_inference20,224,Conclusion,,,natural_language_inference,20,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
1914,sentiment_analysis30,1,title,,,sentiment_analysis,30,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
1915,sentiment_analysis30,2,Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment Analysis,title,title,sentiment_analysis,30,1,1,1,research-problem,0.999070216,1,research-problem,4.58E-08,9.45E-06,1.15E-07,1.11E-07,6.38E-08,1.19E-07,1.63E-06,1.75E-06,1.19E-06,0.001401576,0.998583613,2.44E-07,9.29E-08
1916,sentiment_analysis30,3,abstract,,,sentiment_analysis,30,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
1917,sentiment_analysis30,4,"While neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task .",abstract,abstract,sentiment_analysis,30,1,1,1,research-problem,0.936755164,1,research-problem,3.00E-08,3.68E-06,1.26E-08,9.49E-07,2.77E-07,1.19E-07,4.16E-07,4.97E-07,1.15E-07,0.004837757,0.995156078,2.25E-08,4.60E-08
1918,sentiment_analysis30,5,"Motivated by recent advances in memoryaugmented models for machine reading , we propose a novel architecture , utilising external "" memory chains "" with a delayed memory update mechanism to track entities .",abstract,abstract,sentiment_analysis,30,2,1,0,,0.668279773,1,research-problem,1.65E-05,0.038352434,0.000155342,1.93E-05,0.000127999,2.28E-05,1.73E-05,0.000124452,0.014850762,0.096885839,0.849417666,5.31E-06,4.23E-06
1919,sentiment_analysis30,6,"On a TABSA task , the proposed model demonstrates substantial improvements over state - of the - art approaches , including those using external knowledge bases .",abstract,abstract,sentiment_analysis,30,3,1,0,,0.025638025,0,negative,0.000160739,0.002266084,5.59E-06,1.37E-05,4.38E-05,3.20E-05,0.0001804,0.000662227,5.75E-05,0.512225496,0.483208733,0.001135639,8.22E-06
1920,sentiment_analysis30,7,1,abstract,abstract,sentiment_analysis,30,4,1,0,,4.40E-05,0,negative,5.61E-07,0.00022147,1.11E-07,4.50E-06,5.50E-06,1.28E-05,4.24E-07,7.14E-05,0.000186073,0.989156581,0.010340387,7.93E-08,5.96E-08
1921,sentiment_analysis30,8,Introduction,,,sentiment_analysis,30,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
1922,sentiment_analysis30,9,Targeted aspect - based sentiment analysis ( TABSA ) is the task of identifying fine - grained opinion polarity towards a specific aspect associated with a given target .,Introduction,Introduction,sentiment_analysis,30,1,1,0,,0.950462764,1,research-problem,4.81E-06,0.000442067,2.70E-06,7.15E-05,0.000147413,1.25E-05,6.63E-05,7.69E-06,6.35E-05,0.025567731,0.973598061,5.07E-06,1.06E-05
1923,sentiment_analysis30,10,"The task requires classification of opinions on different entities across a range of different attributes , with the expectation that there will be no overt opinion expressed on a given entity for many attributes .",Introduction,Introduction,sentiment_analysis,30,2,1,0,,0.782803214,1,research-problem,4.03E-06,0.001798804,3.43E-06,0.000817202,0.002569661,8.70E-05,6.51E-05,3.51E-05,0.000246199,0.223100794,0.77125162,4.01E-06,1.71E-05
1924,sentiment_analysis30,11,"This can be seen in Example ( 1 ) , e.g. , where opinions on the aspects SAFETY and PRICE are expressed for entity LOC1 but not entity LOC2 : 2 ( 1 ) LOC1 is your best bet for secure although expensive and LOC2 is too far .",Introduction,Introduction,sentiment_analysis,30,3,1,0,,0.006948543,0,negative,5.53E-06,0.004300034,2.98E-06,6.85E-06,0.000222134,6.21E-05,1.43E-05,5.92E-05,0.010289197,0.936365872,0.04866647,4.11E-06,1.18E-06
1925,sentiment_analysis30,12,2,Introduction,Introduction,sentiment_analysis,30,4,1,0,,0.001825101,0,negative,8.89E-06,0.00296869,2.36E-06,4.41E-06,7.28E-05,8.89E-05,1.49E-05,0.000114066,0.02060759,0.964680501,0.011432993,2.77E-06,1.06E-06
1926,sentiment_analysis30,13,"Note that in our dataset , all entity mentions have been pre-nomalised to LOCn , where n is an index .",Introduction,Introduction,sentiment_analysis,30,5,1,0,,0.268861435,0,negative,7.22E-05,0.043299362,4.46E-05,0.000363929,0.058182356,0.000780663,8.70E-05,0.000349497,0.010413692,0.883203775,0.003179285,1.55E-05,8.12E-06
1927,sentiment_analysis30,14,"The earliest work on ( T ) ABSA relied heavily on feature engineering , but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect - specific word and sentence representations .",Introduction,Introduction,sentiment_analysis,30,6,1,0,,0.711140541,1,research-problem,3.85E-06,0.001177,2.63E-06,1.89E-05,0.000107546,1.48E-05,3.41E-05,1.66E-05,0.00014812,0.109322169,0.889145818,4.41E-06,4.05E-06
1928,sentiment_analysis30,15,"Despite these successes , keeping track of multiple entity - aspect pairs remains a difficult task , even for an LSTM .",Introduction,Introduction,sentiment_analysis,30,7,1,0,,0.40506981,0,research-problem,1.31E-06,0.000193759,3.07E-07,1.38E-05,2.09E-05,1.37E-05,1.43E-05,1.31E-05,5.48E-05,0.155036606,0.844633823,1.56E-06,2.10E-06
1929,sentiment_analysis30,16,"As reported in , a target - dependent biLSTM is ineffective , both in terms of aspect detection and sentiment classification , compared to a simple logistic regression model with n-gram features .",Introduction,Introduction,sentiment_analysis,30,8,1,0,,0.110659028,0,negative,0.014691414,0.048631072,0.000128457,9.29E-05,0.002131026,0.000273843,0.002798118,0.000548452,0.006630024,0.858288884,0.061736503,0.003987102,6.22E-05
1930,sentiment_analysis30,17,"Intuitively , we would expect that a model which better captures linguistic structure via the original word sequencing should perform better , which provides the motivation for this research .",Introduction,Introduction,sentiment_analysis,30,9,1,0,,0.041904886,0,negative,1.02E-05,0.009509116,2.55E-06,1.24E-05,0.000129545,0.000138766,4.75E-05,0.000354722,0.010553934,0.823003952,0.156220077,1.30E-05,4.23E-06
1931,sentiment_analysis30,18,"More recently , successful works in ( T ) ABSA have explored the idea of leveraging external memory .",Introduction,Introduction,sentiment_analysis,30,10,1,0,,0.533073176,1,research-problem,5.55E-06,0.001573479,2.53E-06,1.22E-05,2.69E-05,5.41E-05,4.06E-05,6.76E-05,0.001344615,0.280816157,0.716047036,5.38E-06,3.91E-06
1932,sentiment_analysis30,19,"Their models are largely based on memory networks , originally developed for reasoning - focused machine reading comprehension tasks .",Introduction,Introduction,sentiment_analysis,30,11,1,0,,0.035286145,0,negative,1.21E-05,0.006721264,1.23E-05,4.27E-05,0.000215242,0.000406003,9.52E-05,0.000365909,0.015237305,0.813148359,0.163725878,8.11E-06,9.59E-06
1933,sentiment_analysis30,20,"In contrast to memory networks , where each input sentence / word occupies a memory slot and is then accessed via attention independently , recent advances in machine reading suggest that processing inputs sequentially is beneficial to over all performance .",Introduction,Introduction,sentiment_analysis,30,12,1,0,,0.472322009,0,research-problem,3.36E-06,0.001647432,1.53E-06,4.64E-06,1.93E-05,1.69E-05,2.83E-05,3.34E-05,0.001326043,0.237298819,0.75961152,6.18E-06,2.51E-06
1934,sentiment_analysis30,21,"However , successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the two tasks : on the Children 's Book Test corpus ( CBT ) , for example , competitive models take as input a window of text , centred around candidate entities , with crucial information contained within that window .",Introduction,Introduction,sentiment_analysis,30,13,1,0,,0.345498767,0,research-problem,1.64E-06,0.000633433,1.15E-06,8.38E-06,4.56E-05,1.41E-05,3.06E-05,1.45E-05,0.000155055,0.193627558,0.805461531,3.82E-06,2.62E-06
1935,sentiment_analysis30,22,"In TABSA , given the fine - grained nature of the task , it is common practice for models to operate at the word - rather than chunk / sentencelevel .",Introduction,Introduction,sentiment_analysis,30,14,1,0,,0.224048185,0,negative,2.71E-05,0.024078493,2.29E-05,2.88E-05,0.000286856,0.00015155,9.28E-05,0.00021185,0.013289059,0.698492433,0.263287758,2.05E-05,9.88E-06
1936,sentiment_analysis30,23,"It is not uncommon to see examples like Example ( 1 ) , where the sentence starts with LOC1 , but the negative PRICE sentiment towards the entity is not expressed until much later .",Introduction,Introduction,sentiment_analysis,30,15,1,0,,0.002527952,0,negative,3.42E-06,0.001528284,1.39E-06,3.40E-05,0.000535969,9.38E-05,2.96E-05,6.72E-05,0.000886954,0.977083014,0.019728618,5.28E-06,2.41E-06
1937,sentiment_analysis30,24,"Moreover , phrases such as best bet and although play the role of triggers , indicating that succeeding tokens bear aspect / sentiment signal .",Introduction,Introduction,sentiment_analysis,30,16,1,0,,0.015155046,0,negative,0.002119947,0.064785157,0.000162692,0.000120111,0.019942474,0.000492743,0.00025515,0.000244265,0.033030796,0.878039089,0.000676544,0.000123659,7.37E-06
1938,sentiment_analysis30,25,This key difference necessitates the ability to model the delayed activation of memory updates .,Introduction,Introduction,sentiment_analysis,30,17,1,0,,0.050422214,0,negative,5.85E-05,0.02125908,1.78E-05,2.55E-05,0.000668477,8.98E-05,2.61E-05,0.000114938,0.062698929,0.907244959,0.007779974,1.35E-05,2.50E-06
1939,sentiment_analysis30,26,"In this work , we propose a novel model architecture for TABSA , augmented with multiple "" memory chains "" , and equipped with a delayed memory update mechanism , to keep track of numerous entities independently .",Introduction,Introduction,sentiment_analysis,30,18,1,1,model,0.968331488,1,model,3.31E-05,0.176264024,0.000227152,2.27E-06,0.000269022,2.35E-05,6.09E-05,3.47E-05,0.801406244,0.014114089,0.007543156,1.69E-05,4.99E-06
1940,sentiment_analysis30,27,"We evaluate the effectiveness of the proposed model over the task of TABSA , and achieve substantial improvements over a number of baselines , including one incorporating external knowledge bases , setting a new state of the art in both sentiment classification and aspect detection .",Introduction,Introduction,sentiment_analysis,30,19,1,0,,0.021815014,0,negative,0.000728615,0.46889538,9.96E-05,3.92E-05,0.008421156,0.000259184,0.003191223,0.000930412,0.02038368,0.488836142,0.005901608,0.002256756,5.71E-05
1941,sentiment_analysis30,28,Methodology,,,sentiment_analysis,30,0,1,0,,0.043255215,0,negative,1.91E-05,0.000159627,1.45E-05,1.37E-06,4.99E-07,0.000139902,0.000211372,0.00286638,0.000149264,0.913404109,0.082542361,0.000474296,1.72E-05
1942,sentiment_analysis30,29,Task description .,Methodology,,sentiment_analysis,30,1,1,0,,1.01E-05,0,negative,7.27E-07,4.26E-06,2.27E-06,5.53E-09,3.06E-08,3.16E-06,1.57E-06,6.04E-05,1.46E-06,0.999899709,1.12E-05,1.52E-05,1.75E-08
1943,sentiment_analysis30,30,"In TABSA , a sentence s typically consists of a sequence of words : {w 1 , . . . , w i , . . . , w m } where w i denotes words interleaved with one or more targets ( t ) , which we assume to be pre-identified as with LOC1 and LOC2 in Example ( 1 ) .",Methodology,Task description .,sentiment_analysis,30,2,1,0,,3.05E-06,0,negative,8.45E-07,1.15E-05,8.08E-07,2.08E-07,7.54E-07,3.07E-05,4.86E-06,2.89E-05,1.91E-05,0.999613809,0.000286548,1.67E-06,1.99E-07
1944,sentiment_analysis30,31,"Following , we frame the task as a 3 - class classification problem : given a sentence s , a pre-identified set of target entities T and fixed set of aspects A , predict the sentiment polarity y ?",Methodology,Task description .,sentiment_analysis,30,3,1,0,,2.17E-05,0,negative,1.75E-06,2.90E-05,1.25E-06,5.96E-07,6.04E-07,3.72E-05,1.10E-05,5.54E-05,3.63E-05,0.996490464,0.003330116,5.83E-06,5.78E-07
1945,sentiment_analysis30,32,"{ positive , negative , none } over the full set of target - aspect pairs { ( t , a ) : t ? T , a ?",Methodology,Task description .,sentiment_analysis,30,4,1,0,,1.22E-06,0,negative,3.32E-06,7.12E-06,1.34E-06,2.31E-08,2.72E-07,2.78E-05,3.30E-06,4.52E-05,6.82E-06,0.999892559,6.56E-06,5.68E-06,3.51E-08
1946,sentiment_analysis30,33,"A }. For example , ( LOC1 , SAFETY ) has goldstandard polarity positive , while ( LOC1 , TRANSIT - LOCATION ) has polarity none .",Methodology,Task description .,sentiment_analysis,30,5,1,0,,4.29E-07,0,negative,9.36E-07,1.32E-06,7.34E-07,1.41E-08,1.15E-07,1.20E-05,1.60E-06,1.08E-05,5.59E-06,0.999953801,1.13E-05,1.70E-06,3.60E-08
1947,sentiment_analysis30,34,Proposed model .,,,sentiment_analysis,30,0,1,0,,0.002687153,0,negative,2.56E-05,0.001042119,4.75E-05,3.11E-06,1.90E-06,0.000177662,4.93E-05,0.004625859,0.002409504,0.970891097,0.020626865,8.44E-05,1.51E-05
1948,sentiment_analysis30,35,"To this end , we design a neural network architecture , capable of tracking and updating the states of entities at the right time with external memory , making it a natural fit for the task .",Proposed model .,Proposed model .,sentiment_analysis,30,1,1,0,,0.024950588,0,negative,0.00041657,0.098848516,0.000891037,3.70E-05,0.000163298,0.000248486,4.26E-05,0.009636699,0.202764177,0.683264059,0.002963903,0.000662616,6.10E-05
1949,sentiment_analysis30,36,"Specifically , our model maintains a number of "" memory chains "" h j , one for each entity with the key k j and dynamically updates the states ( h j ) of them as it progresses through the sentence with the help of the delay recurrence d j , taking previous activations into account .",Proposed model .,Proposed model .,sentiment_analysis,30,2,1,0,,0.000838056,0,negative,8.39E-05,0.021124831,0.000458022,1.87E-06,1.83E-05,6.82E-05,7.54E-06,0.003389617,0.193654036,0.780730066,0.000387208,6.87E-05,7.76E-06
1950,sentiment_analysis30,37,An illustration of our model is provided in . :,Proposed model .,Proposed model .,sentiment_analysis,30,3,1,0,,0.00018449,0,negative,5.26E-05,0.000516467,0.000136161,2.49E-06,8.17E-06,3.32E-05,4.19E-06,0.00045673,0.005324026,0.993305565,7.07E-05,8.62E-05,3.43E-06
1951,sentiment_analysis30,38,"Illustration of our model with a single memory chain at time i. ? , ?",Proposed model .,Proposed model .,sentiment_analysis,30,4,1,0,,2.39E-05,0,negative,2.06E-05,0.00060002,4.27E-05,1.19E-06,2.90E-06,8.68E-05,1.41E-05,0.002459432,0.002260677,0.99340819,0.000844338,0.000252707,6.44E-06
1952,sentiment_analysis30,39,"and GRU represent Equations , and , while circled nodes L , C , and + depict the location , content terms , Hadamard product , and addition , resp .",Proposed model .,Proposed model .,sentiment_analysis,30,5,1,0,,4.39E-05,0,negative,1.08E-05,0.000648398,4.65E-05,7.08E-07,4.11E-06,4.53E-05,3.14E-06,0.000900769,0.002673021,0.995604279,2.71E-05,3.49E-05,9.99E-07
1953,sentiment_analysis30,40,Delayed memory update .,Proposed model .,,sentiment_analysis,30,6,1,0,,0.000929995,0,negative,8.65E-05,0.000845808,0.002493286,1.30E-07,9.72E-07,3.04E-05,5.78E-06,0.000784233,0.009272757,0.985950663,0.000111884,0.000416105,1.44E-06
1954,sentiment_analysis30,41,"Update of each memory chain is controlled by a gating mechanism , consisting of three components : the "" content "" term w i h j i ?1 , the "" location "" term w i k j and the "" delay "" term vd j i where d j i carries knowledge regarding previous activation of the gate and v is a trainable parameter vector .",Proposed model .,Delayed memory update .,sentiment_analysis,30,7,1,0,,1.42E-05,0,negative,1.65E-05,0.000699845,6.83E-05,4.29E-07,9.12E-07,1.92E-05,1.81E-06,0.000846925,0.018570907,0.979713927,4.88E-05,1.06E-05,1.78E-06
1955,sentiment_analysis30,42,"All three terms may lead to the activation of g j i , but differ in how they turn the gate on .",Proposed model .,Delayed memory update .,sentiment_analysis,30,8,1,0,,2.78E-07,0,negative,2.20E-05,1.34E-05,2.76E-06,2.92E-08,7.75E-08,1.35E-06,1.67E-07,3.63E-05,9.57E-05,0.999804575,1.02E-06,2.25E-05,4.86E-08
1956,sentiment_analysis30,43,"While the "" location "" term causes the gate to open for memory chains whose keys ( k j ) match the input , the "" content "" term triggers the activation when the content of the entities ( h j i?1 ) matches the input .",Proposed model .,Delayed memory update .,sentiment_analysis,30,9,1,0,,1.89E-06,0,negative,1.82E-05,5.04E-05,1.72E-05,2.46E-08,1.06E-07,2.58E-06,2.24E-07,0.000123596,0.000999185,0.998777406,1.21E-06,9.81E-06,9.74E-08
1957,sentiment_analysis30,44,The delay term models how and when the gate was turned on in the past with a GRU and how past activations should influence the current one .,Proposed model .,Delayed memory update .,sentiment_analysis,30,10,1,0,,7.69E-05,0,negative,3.99E-05,0.000230452,4.75E-05,2.62E-07,5.24E-07,2.46E-05,2.27E-06,0.001321798,0.008816528,0.989489474,7.94E-06,1.75E-05,1.18E-06
1958,sentiment_analysis30,45,"More formally , with arrows denoting processing direction , the update gate is defined as :",Proposed model .,Delayed memory update .,sentiment_analysis,30,11,1,0,,9.40E-07,0,negative,3.26E-06,2.39E-05,1.19E-05,1.23E-08,4.65E-08,1.16E-06,1.13E-07,4.07E-05,0.000476538,0.999437362,1.39E-06,3.53E-06,5.26E-08
1959,sentiment_analysis30,46,?,Proposed model .,Delayed memory update .,sentiment_analysis,30,12,1,0,,3.40E-07,0,negative,1.57E-06,1.85E-06,2.85E-07,1.78E-08,2.45E-08,2.14E-06,1.75E-07,4.23E-05,1.56E-05,0.999929479,8.25E-07,5.73E-06,3.59E-08
1960,sentiment_analysis30,47,"g j i is the update gate value for the j - th memory at time i , 3 k j is the embedding for the jth entity ( key ) , ? ? h j i?1 is the hidden memory representation responsible for keeping track of the state of the j - th entity ( content ) , and ?",Proposed model .,Delayed memory update .,sentiment_analysis,30,13,1,0,,4.62E-07,0,negative,5.25E-06,2.23E-05,1.98E-06,3.93E-08,1.46E-07,2.40E-06,2.47E-07,9.49E-05,7.12E-05,0.999787388,8.67E-07,1.33E-05,5.48E-08
1961,sentiment_analysis30,48,is the sigmoid activation function .,Proposed model .,Delayed memory update .,sentiment_analysis,30,14,1,0,,8.19E-05,0,negative,1.13E-05,0.000260765,3.90E-06,1.00E-06,5.93E-07,0.000149807,6.26E-06,0.022673774,0.001292339,0.975569936,1.46E-05,1.24E-05,3.37E-06
1962,sentiment_analysis30,49,The delay recurrence,Proposed model .,,sentiment_analysis,30,15,1,0,,0.001135781,0,negative,0.000487468,0.001203922,0.001159949,2.24E-06,1.15E-05,6.78E-05,5.40E-05,0.001620711,0.00760108,0.97979415,0.000378809,0.007595673,2.26E-05
1963,sentiment_analysis30,50,"where is the Hadamard product , and ? ?",Proposed model .,The delay recurrence,sentiment_analysis,30,16,1,0,,2.32E-07,0,negative,1.69E-06,3.80E-06,8.61E-07,1.92E-08,7.00E-08,6.71E-07,1.98E-07,1.12E-05,4.80E-05,0.999929169,1.52E-06,2.74E-06,4.42E-08
1964,sentiment_analysis30,51,h j i is the unnormalised memory representation for the j - th entity .,Proposed model .,The delay recurrence,sentiment_analysis,30,17,1,0,,1.87E-07,0,negative,6.57E-06,1.75E-05,1.16E-05,6.63E-09,8.34E-08,3.42E-07,1.63E-07,7.11E-06,0.000422677,0.999527383,1.03E-06,5.59E-06,3.98E-08
1965,sentiment_analysis30,52,"Essentially , gate ? ?",Proposed model .,The delay recurrence,sentiment_analysis,30,18,1,0,,8.03E-07,0,negative,3.46E-06,2.97E-06,2.76E-06,9.97E-09,4.46E-08,3.43E-07,1.43E-07,5.04E-06,7.48E-05,0.999905872,8.41E-07,3.68E-06,3.57E-08
1966,sentiment_analysis30,53,"g j i determines how much the j - th memory should be updated , factoring in three elements : ( 1 ) how similar the current input w i is to the entity being tracked ( k j ) ; ( 2 ) how related the current input w i is to the state of the j - th entity ( ? ? h j i?1 ) ; and how past activation should influence the current one .",Proposed model .,The delay recurrence,sentiment_analysis,30,19,1,0,,5.37E-07,0,negative,2.48E-05,4.52E-05,8.39E-06,4.56E-08,3.94E-07,6.31E-07,2.40E-07,1.82E-05,0.000815717,0.999075788,9.17E-07,9.56E-06,8.36E-08
1967,sentiment_analysis30,54,Update of the memory of an entity is only triggered when the gate is activated .,Proposed model .,The delay recurrence,sentiment_analysis,30,20,1,0,,2.04E-05,0,negative,7.27E-05,0.000111656,6.51E-05,6.26E-08,8.37E-07,7.60E-07,4.94E-07,1.53E-05,0.004604951,0.995083961,5.30E-06,3.86E-05,3.18E-07
1968,sentiment_analysis30,55,Normalis ation .,Proposed model .,,sentiment_analysis,30,21,1,0,,0.000428267,0,negative,2.78E-05,0.00038859,0.000115671,5.12E-07,7.36E-06,5.72E-05,1.08E-05,0.001511504,0.000626358,0.996891394,7.96E-06,0.000351821,3.09E-06
1969,sentiment_analysis30,56,"Following the update , the model performs a normalis ation step , allowing the memory to forget :",Proposed model .,Normalis ation .,sentiment_analysis,30,22,1,0,,4.64E-06,0,negative,8.35E-05,8.41E-05,7.67E-05,4.78E-08,6.17E-07,6.37E-06,1.49E-06,0.000106539,0.000749807,0.998810218,7.66E-07,7.95E-05,4.05E-07
1970,sentiment_analysis30,57,As all information stored in ? ?,Proposed model .,Normalis ation .,sentiment_analysis,30,23,1,0,,7.63E-07,0,negative,1.73E-06,2.56E-06,1.13E-06,1.51E-08,1.06E-07,1.44E-06,2.90E-07,1.86E-05,1.88E-05,0.999940034,3.48E-07,1.50E-05,6.70E-08
1971,sentiment_analysis30,58,"h j i is constrained to be of unit length , when new information ? ?",Proposed model .,Normalis ation .,sentiment_analysis,30,24,1,0,,3.64E-07,0,negative,7.77E-06,8.26E-06,1.85E-06,7.54E-08,3.29E-07,6.06E-06,4.95E-07,0.00011338,4.41E-05,0.999796867,1.52E-07,2.06E-05,1.36E-07
1972,sentiment_analysis30,59,"h j i is added to the existing memory ? ? h j i?1 , the cosine distance between the original and updated memory decreases , allowing the model to forget information deemed out - of - date .",Proposed model .,Normalis ation .,sentiment_analysis,30,25,1,0,,3.63E-06,0,negative,0.000457178,3.52E-05,2.15E-05,1.04E-07,1.46E-06,5.35E-06,2.40E-06,8.24E-05,0.000102162,0.998589843,6.43E-07,0.000701287,4.74E-07
1973,sentiment_analysis30,60,Bi-directionality .,Proposed model .,,sentiment_analysis,30,26,1,0,,8.37E-05,0,negative,4.88E-05,0.000391496,0.000311614,1.88E-07,3.56E-06,2.17E-05,9.31E-06,0.000609079,0.002444942,0.995173189,1.36E-05,0.000969875,2.74E-06
1974,sentiment_analysis30,61,"We apply the above steps both forward and backward over the sentence , enabling the model to capture sentiment terms appearing before and after its associated entity .",Proposed model .,Bi-directionality .,sentiment_analysis,30,27,1,0,,4.57E-05,0,negative,3.42E-05,0.000343891,5.57E-05,3.28E-08,8.81E-07,2.06E-06,5.35E-07,7.80E-05,0.003472395,0.995979058,4.05E-07,3.27E-05,2.40E-07
1975,sentiment_analysis30,62,The memory representation incorporating contexts from both directions is obtained by,Proposed model .,Bi-directionality .,sentiment_analysis,30,28,1,0,,1.21E-06,0,negative,1.21E-05,5.32E-05,6.14E-06,4.01E-08,3.54E-07,2.29E-06,3.93E-07,9.91E-05,0.000601626,0.999204431,7.38E-07,1.94E-05,2.29E-07
1976,sentiment_analysis30,63,Final classifier .,Proposed model .,,sentiment_analysis,30,29,1,0,,0.00141364,0,negative,9.08E-06,0.00016797,1.42E-05,9.91E-08,1.43E-06,2.40E-05,3.13E-06,0.001580129,0.001580576,0.996537748,2.04E-06,7.81E-05,1.41E-06
1977,sentiment_analysis30,64,Our model predicts the sentiment polarity ?,Proposed model .,Final classifier .,sentiment_analysis,30,30,1,0,,0.001456085,0,negative,4.31E-06,2.43E-05,1.62E-05,2.12E-07,8.18E-07,1.82E-05,1.70E-06,0.000498074,0.000314954,0.999090387,1.70E-06,2.52E-05,3.90E-06
1978,sentiment_analysis30,65,to the given target t and aspect a embeddings by incorporating the states of all tracked entities in the form of a weighted sum u:,Proposed model .,Final classifier .,sentiment_analysis,30,31,1,0,,2.20E-06,0,negative,2.51E-05,0.000150408,0.000143453,6.60E-08,9.82E-07,6.55E-06,1.33E-06,0.000256847,0.000456786,0.998871752,1.59E-06,8.39E-05,1.21E-06
1979,sentiment_analysis30,66,"where [ ] denotes concatenation , m is sentence length , and Watt is a trainable weight matrix .",Proposed model .,Final classifier .,sentiment_analysis,30,32,1,0,,2.70E-06,0,negative,3.08E-06,3.69E-05,4.36E-06,2.49E-07,6.26E-07,2.62E-05,1.19E-06,0.001814644,0.000159598,0.997944923,4.07E-07,6.60E-06,1.18E-06
1980,sentiment_analysis30,67,"Here , the values of both t and a take the embedding values of their corresponding words ( i.e. t and a are drawn from the same embedding matrix as are the input words w i ) .",Proposed model .,Final classifier .,sentiment_analysis,30,33,1,0,,6.11E-07,0,negative,2.40E-06,1.85E-05,3.78E-06,3.24E-08,2.60E-07,8.32E-06,4.13E-07,0.000454381,0.000119116,0.999388097,4.16E-08,4.45E-06,1.96E-07
1981,sentiment_analysis30,68,"In the case of multi-word aspect expressions ( e.g. TRANSIT - LOCATION ) , we take the mean of the embeddings of the constituent words .",Proposed model .,Final classifier .,sentiment_analysis,30,34,1,0,,5.12E-06,0,negative,9.00E-06,7.70E-05,4.29E-05,6.12E-08,7.59E-07,3.13E-05,2.27E-06,0.00126889,0.000227769,0.998316078,1.76E-07,2.29E-05,8.62E-07
1982,sentiment_analysis30,69,We then transform u to get : ? = softmax ( R ? ( Hu + a ) ),Proposed model .,Final classifier .,sentiment_analysis,30,35,1,0,,4.59E-06,0,negative,3.95E-05,2.37E-05,6.99E-05,9.75E-08,1.70E-06,1.13E-05,2.04E-06,0.00016946,6.97E-05,0.999484443,1.49E-07,0.000126997,1.01E-06
1983,sentiment_analysis30,70,Training is carried out based on cross entropy loss .,Proposed model .,Final classifier .,sentiment_analysis,30,36,1,0,,0.003515919,0,negative,3.40E-05,0.002324176,5.42E-05,3.35E-06,1.27E-05,0.001160968,4.66E-05,0.199609578,0.002663229,0.794017437,8.24E-07,3.05E-05,4.23E-05
1984,sentiment_analysis30,71,"L = CrossEntropy ( y , ? )",Proposed model .,Final classifier .,sentiment_analysis,30,37,1,0,,3.60E-06,0,negative,9.73E-06,7.38E-06,1.95E-05,2.29E-07,1.07E-06,2.84E-05,2.77E-06,0.000506769,3.37E-05,0.999331999,1.41E-07,5.63E-05,2.01E-06
1985,sentiment_analysis30,72,Comparision with EntNet .,Proposed model .,,sentiment_analysis,30,38,1,0,,0.000372985,0,negative,0.000105014,0.00019224,0.00012066,4.10E-08,1.68E-06,1.29E-05,6.49E-05,0.000696331,0.000191961,0.973785478,4.93E-06,0.024820341,3.57E-06
1986,sentiment_analysis30,73,"While our model is largely inspired by Recurrent Entity Networks ( EntNets : Henaff et al. ) , it differs in three main respects .",Proposed model .,Comparision with EntNet .,sentiment_analysis,30,39,1,0,,1.70E-05,0,negative,0.000128571,0.000156332,0.000351068,1.49E-07,3.77E-06,7.38E-06,4.57E-06,5.95E-05,0.000135335,0.998234628,6.72E-07,0.000916307,1.76E-06
1987,sentiment_analysis30,74,"First , we explicitly model the delay of activation of the update gates g j with the GRU in Equations and as opposed to making h j i implicitly assume the same responsibility in EntNets .",Proposed model .,Comparision with EntNet .,sentiment_analysis,30,40,1,0,,7.25E-06,0,negative,0.000613646,0.000223276,0.000259032,7.21E-08,2.37E-06,3.99E-06,2.94E-06,5.14E-05,0.000585295,0.997610487,1.31E-07,0.000646731,6.69E-07
1988,sentiment_analysis30,75,"Admittedly , for EntNets on bAbI and CBT , given the coarse - grained nature and the difference in the granularity of inputs ( sentences vs. words ) , the demand for modelling delayed memory update is less obvious .",Proposed model .,Comparision with EntNet .,sentiment_analysis,30,41,1,0,,2.11E-05,0,negative,9.44E-05,2.30E-06,6.38E-06,2.05E-08,1.62E-07,2.93E-06,1.97E-06,3.10E-05,2.93E-06,0.998303161,3.02E-07,0.00155392,4.94E-07
1989,sentiment_analysis30,76,"With this delayed gate activation mechanism , we essentially decouple the duty of capturing transitions of activations between steps from the task of entity state tracking .",Proposed model .,Comparision with EntNet .,sentiment_analysis,30,42,1,0,,5.43E-05,0,negative,0.000271585,0.000380892,0.000130011,1.66E-07,3.18E-06,3.77E-06,2.62E-06,6.94E-05,0.000913603,0.997185251,8.47E-07,0.001036699,1.94E-06
1990,sentiment_analysis30,77,"That is , h j t is now dedicated to keeping track of the state of the j - th entity only and released from the burden of monitoring the activation of the update gate .",Proposed model .,Comparision with EntNet .,sentiment_analysis,30,43,1,0,,5.35E-06,0,negative,1.38E-05,1.84E-05,3.60E-05,6.00E-09,1.89E-07,7.64E-07,3.13E-07,1.62E-05,0.00020129,0.999678876,3.59E-08,3.40E-05,1.75E-07
1991,sentiment_analysis30,78,"Second , tailoring to the task of TABSA , we incorporate not only the target t but also the aspect a when trying to determine the attention in the softmax function .",Proposed model .,Comparision with EntNet .,sentiment_analysis,30,44,1,0,,6.52E-05,0,negative,0.000587483,0.000531158,0.000326508,1.13E-07,4.25E-06,5.85E-06,5.13E-06,0.000118363,0.000643742,0.996793187,1.33E-07,0.00098268,1.40E-06
1992,sentiment_analysis30,79,"Third , the proposed model is bi-directional .",Proposed model .,,sentiment_analysis,30,45,1,0,,0.000125954,0,negative,0.00019854,0.003219534,0.000503287,1.60E-06,6.03E-05,2.77E-05,1.13E-05,0.000979045,0.006616385,0.987709838,3.09E-06,0.000661486,7.85E-06
1993,sentiment_analysis30,80,Experiments,,,sentiment_analysis,30,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
1994,sentiment_analysis30,81,Experimental Setup,,,sentiment_analysis,30,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
1995,sentiment_analysis30,82,Dataset .,Experimental Setup,,sentiment_analysis,30,1,1,0,,0.004097405,0,negative,3.29E-05,1.42E-06,0.001874031,1.95E-06,7.53E-07,0.007652949,0.001544156,0.00811495,2.00E-06,0.979076732,5.54E-05,0.001630355,1.24E-05
1996,sentiment_analysis30,83,"To test the effectiveness of our model , we use Sentihood , a dataset constructed by for the purpose of detecting aspects and identifying sentiments for each targetaspect pair , consisting of 5 , 215 sentences , 3 , 862 of which contain a single target , and the remainder multiple targets .",Experimental Setup,Dataset .,sentiment_analysis,30,2,1,0,,0.00817299,0,negative,0.000213174,8.68E-05,0.000866433,2.52E-05,0.003028005,0.004815638,0.000546019,0.000311103,2.53E-06,0.989695922,8.86E-06,0.000391414,8.88E-06
1997,sentiment_analysis30,84,"Each sentence is annotated with a list of tuples { ( t , a , y ) } with each identifying the sentiment polarity y towards a specific aspect a of : Performance on Sentihood .",Experimental Setup,Dataset .,sentiment_analysis,30,3,1,0,,0.002030817,0,negative,0.000101054,0.000120082,0.000543247,6.02E-06,0.000269727,0.004725284,9.81E-05,0.000750841,4.19E-05,0.993191085,1.81E-05,0.000127153,7.37E-06
1998,sentiment_analysis30,85,"We take the results reported in and , resp ; Bold = best performance ; "" - "" = not reported ; = average performance over 5 runs .",Experimental Setup,Dataset .,sentiment_analysis,30,4,1,0,,0.000394411,0,negative,8.76E-06,8.56E-07,8.64E-06,9.09E-08,5.24E-07,0.008295182,7.22E-05,0.001979453,5.79E-07,0.989587027,1.99E-06,4.42E-05,5.32E-07
1999,sentiment_analysis30,86,a given target tin s .,Experimental Setup,Dataset .,sentiment_analysis,30,5,1,0,,0.003917102,0,negative,0.00020292,3.45E-06,9.08E-05,4.53E-08,4.09E-07,0.000402819,1.83E-05,0.000211761,2.19E-06,0.998613565,8.69E-06,0.000444605,4.52E-07
2000,sentiment_analysis30,87,"Ultimately , given a sentence s , we are interested in both detecting the mention of an aspect a for target t ( a label other than none ) , and also identifying the specific sentiment y w.r.t. the target - aspect pair .",Experimental Setup,Dataset .,sentiment_analysis,30,6,1,0,,0.00657836,0,negative,0.00017465,0.000377961,0.00024435,6.70E-06,3.57E-05,0.002688527,0.000120254,0.001340239,0.000111858,0.99309667,0.001297593,0.000481531,2.39E-05
2001,sentiment_analysis30,88,A detailed description of the task is presented in Section 2 .,Experimental Setup,Dataset .,sentiment_analysis,30,7,1,0,,0.000112873,0,negative,1.59E-05,8.90E-06,1.27E-05,2.75E-06,1.86E-05,0.001272337,1.67E-05,0.00019696,4.24E-06,0.998407625,1.49E-05,2.63E-05,2.07E-06
2002,sentiment_analysis30,89,Model configuration .,,,sentiment_analysis,30,0,1,0,,0.005979084,0,negative,1.23E-05,0.000412117,8.24E-06,1.43E-06,1.40E-06,0.000265648,3.10E-05,0.006895895,0.000287888,0.991535593,0.000485836,6.04E-05,2.30E-06
2003,sentiment_analysis30,90,"We initialise our model with GloVe ( 300 - D , trained on 42B tokens , 1.9 M vocab , not updated during training : ) 4 and pre-process the corpus with tokenisation using NLTK ) and case folding .",Model configuration .,Model configuration .,sentiment_analysis,30,1,1,1,hyperparameters,0.773229456,1,hyperparameters,0.000129953,0.000148833,0.000693299,3.37E-05,9.87E-06,0.212710676,0.002904552,0.717733077,9.91E-05,0.065382692,1.31E-05,7.04E-05,7.07E-05
2004,sentiment_analysis30,91,Training is carried out over 800 epochs with the FTRL optimiser and a batch size of 128 and learning rate of 0.05 .,Model configuration .,Model configuration .,sentiment_analysis,30,2,1,1,hyperparameters,0.929036383,1,hyperparameters,2.07E-05,5.88E-05,4.03E-05,1.26E-05,1.63E-06,0.113671676,0.001238927,0.868552715,3.87E-05,0.016268587,2.06E-05,2.18E-05,5.29E-05
2005,sentiment_analysis30,92,"We use the following hyper - parameters for weight matrices in both directions : R ? R 3003 , H , U , V , Ware all matrices of size R 300300 , v ? R 300 , and hidden size of the GRU in Equation is 300 .",Model configuration .,Model configuration .,sentiment_analysis,30,3,1,1,hyperparameters,0.753437905,1,hyperparameters,1.30E-05,2.92E-05,1.76E-05,1.69E-06,4.93E-07,0.064084578,0.0003903,0.918853849,1.97E-05,0.016552232,7.60E-06,1.82E-05,1.15E-05
2006,sentiment_analysis30,93,Dropout is applied to the output of ? in the final classifier ( Equation ) with a rate of 0.2 .,Model configuration .,Model configuration .,sentiment_analysis,30,4,1,1,hyperparameters,0.738042905,1,hyperparameters,1.82E-05,4.56E-05,3.73E-05,4.19E-06,6.56E-07,0.047933373,0.000317219,0.94330869,4.83E-05,0.008254335,4.49E-06,7.21E-06,2.05E-05
2007,sentiment_analysis30,94,"Moreover , we employ the technique introduced by where the same dropout mask is applied to the input w i at every step with a rate of 0.2 .",Model configuration .,Model configuration .,sentiment_analysis,30,5,1,0,,0.025094402,0,negative,0.009658226,0.004388813,0.069157225,5.22E-05,0.000102732,0.019646392,0.001435547,0.129437093,0.001609218,0.761700028,6.93E-05,0.002612873,0.000130394
2008,sentiment_analysis30,95,"Lastly , to curb overfitting , we regularise the last layer ( Equation ) with an L 2 penalty on its weights : ?",Model configuration .,Model configuration .,sentiment_analysis,30,6,1,1,hyperparameters,0.037576228,0,negative,0.00850528,0.000581877,0.019060864,1.06E-05,2.05E-05,0.013876903,0.000545233,0.095579964,0.000433615,0.8586788,1.86E-05,0.00264443,4.34E-05
2009,sentiment_analysis30,96,R where ? = 0.001 .,Model configuration .,Model configuration .,sentiment_analysis,30,7,1,0,,0.076413757,0,negative,0.000268078,2.22E-05,0.000219417,6.67E-06,6.76E-06,0.015973842,0.000265386,0.091656025,2.52E-05,0.89109225,1.55E-05,0.000432036,1.66E-05
2010,sentiment_analysis30,97,"We empirically set the number of memory chains to 6 , with the keys of two of them set to the same embeddings as the target words LOC1 and LOC2 , resp. , and the other 4 chains with free key embeddings which are updated during training , and therefore free to capture any entities .",Model configuration .,Model configuration .,sentiment_analysis,30,8,1,1,hyperparameters,0.640060017,1,hyperparameters,2.97E-05,8.53E-05,5.08E-05,3.25E-06,1.63E-06,0.06774147,0.000547469,0.902515066,6.02E-05,0.028913012,5.17E-06,2.52E-05,2.17E-05
2011,sentiment_analysis30,98,5 4 http://nlp.stanford.edu/data/glove.,Model configuration .,Model configuration .,sentiment_analysis,30,9,1,0,,0.010200616,0,negative,0.000651138,2.09E-05,0.000405398,0.046377368,0.000586136,0.174678722,0.001287393,0.024927842,4.28E-05,0.75067276,1.56E-05,0.000155131,0.000178856
2012,sentiment_analysis30,99,42B.300 d.zip,Model configuration .,Model configuration .,sentiment_analysis,30,10,1,0,,0.000355666,0,negative,0.001521915,2.72E-05,0.001766294,0.000812734,7.99E-05,0.025758349,0.001211399,0.017052495,0.000102672,0.950409277,4.67E-05,0.001018826,0.00019223
2013,sentiment_analysis30,100,5,Model configuration .,Model configuration .,sentiment_analysis,30,11,1,0,,0.000131389,0,negative,9.61E-05,6.65E-06,4.06E-05,3.98E-06,2.62E-06,0.002112956,4.78E-05,0.009676489,2.63E-05,0.987817087,3.34E-06,0.000159898,6.19E-06
2014,sentiment_analysis30,101,"In line with the findings of that tying key vectors damages model performance , we observed similar performance deterioration when using tied keys only .",Model configuration .,Model configuration .,sentiment_analysis,30,12,1,0,,0.628070605,1,negative,0.031831127,4.86E-05,0.001135927,5.51E-06,1.83E-05,0.001335385,0.001205428,0.008714189,1.40E-05,0.667539394,3.01E-05,0.288054457,6.76E-05
2015,sentiment_analysis30,102,"While we also experimented with various configurations ( all Consistent with , we tackle the data unbalanced problem ( none positive + negative ) by sampling the same number of training instances within a batch randomly from each class .",Model configuration .,Model configuration .,sentiment_analysis,30,13,1,0,,0.016779058,0,negative,0.002433441,0.002669558,0.006090689,3.56E-05,0.000199328,0.01098222,0.000991419,0.075466504,0.000410771,0.896769803,1.68E-05,0.003852913,8.09E-05
2016,sentiment_analysis30,103,Evaluation .,,,sentiment_analysis,30,0,1,0,,0.001811462,0,negative,1.01E-05,8.49E-05,2.11E-06,6.33E-07,7.51E-07,0.000101449,1.89E-05,0.001409143,3.86E-05,0.997945076,0.000354392,3.32E-05,7.85E-07
2017,sentiment_analysis30,104,We benchmark against baseline systems presented in the works of and : ( 1 ) LR : a logistic regression classifier with n-gram and POS tag features ; ( 2 ) LSTM - Final : a biLSTM taking the final states as representations ; ( 3 ) LSTM - Loc : a biLSTM taking the states at the location where target t is mentioned as representations ; ( 4 ) LSTM + TA + SA : a biLSTM equipped with complex target and sentence - level attention mechanisms ; ( 5 ) SenticLSTM : an improved version of ( 4 ) incorporating the SenticNet external knowledge base .,Evaluation .,Evaluation .,sentiment_analysis,30,1,1,0,,0.692281239,1,negative,0.000919573,0.003471753,0.378906075,3.21E-05,7.69E-05,0.008641275,0.013797051,0.043058128,0.000436061,0.543665512,0.001977723,0.004583672,0.000434169
2018,sentiment_analysis30,105,We additionally implement a bi-directional EntNet with the same hyper - parameter settings and Glo Ve embeddings as our model .,Evaluation .,Evaluation .,sentiment_analysis,30,2,1,0,,0.52039854,1,hyperparameters,0.012571815,0.009686325,0.336200034,0.000223477,0.000132525,0.044383398,0.011459957,0.374651418,0.008652853,0.198732626,0.000400927,0.001280447,0.001624197
2019,sentiment_analysis30,106,"In terms of evaluation , we adopt the standard 70/10/20 train / validation / test split , and report the test performance corresponding to the model with the best validation score .",Evaluation .,Evaluation .,sentiment_analysis,30,3,1,0,,0.001996516,0,negative,0.000390911,0.000430929,0.001340979,2.96E-06,8.59E-06,0.00428155,0.000641727,0.069597515,6.70E-05,0.922625978,4.03E-05,0.000549586,2.20E-05
2020,sentiment_analysis30,107,"Following , we consider the top 4 aspects only ( GENERAL , PRICE , TRANSIT - LOCATION , and SAFETY ) and employ the following evaluation metrics : macro -average F 1 and AUC for aspect detection ignoring the none class , and accuracy and macro -average AUC for sentiment classification .",Evaluation .,Evaluation .,sentiment_analysis,30,4,1,0,,0.009660672,0,negative,0.00217782,0.002036648,0.011385185,8.18E-06,8.79E-05,0.000944498,0.000937945,0.010163171,0.000108882,0.966744106,9.56E-05,0.005265772,4.43E-05
2021,sentiment_analysis30,108,"Following , we also report strict accuracy for aspect detection , as the fraction of sentences where all aspects are detected correctly .",Evaluation .,Evaluation .,sentiment_analysis,30,5,1,0,,0.000664361,0,negative,0.004730826,0.000196915,0.002409945,4.23E-05,0.000174711,0.000594341,0.000794861,0.002584521,2.60E-05,0.977285569,0.000115274,0.010989103,5.56E-05
2022,sentiment_analysis30,109,"tied vs. all free ) , this hybrid setup results in the best performance on the validation set . :",Evaluation .,Evaluation .,sentiment_analysis,30,6,1,0,,0.915623325,1,results,0.054900524,0.000802951,0.173211879,3.68E-05,8.17E-05,0.002216927,0.018253537,0.011868658,0.000113005,0.317471778,0.000643167,0.419502502,0.000896549
2023,sentiment_analysis30,110,"Example of the gate value gt averaged across memory chains , forward and backward , in EntNet vs. our model .",Evaluation .,Evaluation .,sentiment_analysis,30,7,1,0,,0.000120692,0,negative,0.000551572,9.49E-05,0.001697377,1.33E-05,9.99E-06,0.006411909,0.000900283,0.042587778,0.000156606,0.946305751,0.00031252,0.000796328,0.000161725
2024,sentiment_analysis30,111,Results,,,sentiment_analysis,30,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
2025,sentiment_analysis30,112,The experimental results are presented in .,Results,,sentiment_analysis,30,1,1,0,,0.099220365,0,negative,0.003981173,2.93E-06,0.000266544,1.70E-06,2.66E-06,3.02E-05,0.000615024,9.94E-05,3.60E-06,0.822823579,1.27E-05,0.172076176,8.43E-05
2026,sentiment_analysis30,113,State - of - the - art results .,Results,,sentiment_analysis,30,2,1,0,,0.6411328,1,negative,0.001303244,3.85E-06,0.000874537,1.30E-06,1.04E-06,4.86E-05,0.001545041,0.000159051,2.95E-06,0.771499695,0.000198555,0.224102496,0.000259668
2027,sentiment_analysis30,114,Our model achieves state - of - the - art results for both aspect detection and sentiment classification .,Results,State - of - the - art results .,sentiment_analysis,30,3,1,1,results,0.805925389,1,results,0.000313136,1.28E-07,0.000171971,3.49E-07,3.80E-08,3.25E-06,0.000145166,7.62E-06,9.38E-08,0.040374231,5.25E-06,0.957978048,0.001000723
2028,sentiment_analysis30,115,"It is impressive that the proposed model , equipped only with domainindependent general - purpose GloVe embeddings , outperforms Sentic LSTM , an approach heavily reliant on external knowledge bases and domainspecific embeddings .",Results,State - of - the - art results .,sentiment_analysis,30,4,1,1,results,0.943070411,1,results,0.001188922,5.66E-08,2.49E-05,1.79E-07,1.97E-08,1.90E-06,9.26E-05,7.16E-06,5.38E-08,0.028156098,9.35E-07,0.970383596,0.000143583
2029,sentiment_analysis30,116,Ent Net vs. our model .,Results,,sentiment_analysis,30,5,1,1,results,0.476365396,0,negative,0.023687655,2.06E-05,0.04427082,1.11E-05,4.70E-06,0.000210383,0.003637618,0.000732277,9.59E-05,0.60153172,0.000100478,0.324671579,0.001025228
2030,sentiment_analysis30,117,"We see consistent performance gains for our model in both aspect detection and sentiment classification , compared to EntNet , esp. for aspect detection , underlining the benefit of delayed update gate activation .",Results,Ent Net vs. our model .,sentiment_analysis,30,6,1,1,results,0.947360398,1,results,0.000474101,2.69E-08,1.09E-05,9.07E-08,7.64E-09,4.34E-07,6.85E-05,1.45E-06,2.37E-08,0.011104298,3.76E-07,0.988146011,0.000193825
2031,sentiment_analysis30,118,Discussion,Results,,sentiment_analysis,30,7,1,0,,0.002094312,0,negative,0.00611291,2.35E-05,0.000368927,4.73E-05,1.04E-05,0.000263241,0.000742946,0.000998984,0.000121953,0.975331804,9.35E-05,0.015161976,0.000722535
2032,sentiment_analysis30,119,"To better understand what the model has learned , we visualise the average gate value gt in , where colour intensity indicates how much memory is updated .",Results,Discussion,sentiment_analysis,30,8,1,0,,2.91E-06,0,negative,3.70E-05,0.000199233,9.36E-06,5.60E-08,1.87E-06,6.53E-06,2.34E-07,3.57E-05,0.000318543,0.99934699,4.15E-05,2.95E-06,2.49E-08
2033,sentiment_analysis30,120,"Observe that , while updated less by the mention of LOC1 , our model carries out memory updates upon seeing lovely town and plenty of restaurants , key phrases associated with aspects such as GENERAL and DINNING .",Results,Discussion,sentiment_analysis,30,9,1,0,,3.19E-06,0,negative,0.013930783,0.001172764,0.000181123,5.64E-07,2.83E-05,6.24E-06,2.05E-06,2.03E-05,0.001507744,0.982859839,0.000108452,0.000181621,2.48E-07
2034,sentiment_analysis30,121,"Perhaps even more importantly , despite the distance between LOC1 and the final portion of the sentence , our model recognises the relevance to : Sensitivity study of model performance to # of memory chains n.",Results,Discussion,sentiment_analysis,30,10,1,0,,2.01E-06,0,negative,0.001708024,0.000168581,3.30E-05,2.39E-07,4.84E-06,1.71E-06,6.65E-07,5.54E-06,7.10E-05,0.997667846,0.000261449,7.71E-05,6.44E-08
2035,sentiment_analysis30,122,Note that we report average performance over 5 runs with standard deviation .,Results,Discussion,sentiment_analysis,30,11,1,0,,3.24E-07,0,negative,1.11E-05,3.23E-05,4.00E-07,3.20E-07,3.36E-06,1.30E-05,5.06E-07,5.75E-05,1.31E-05,0.999839438,2.64E-05,2.56E-06,3.88E-08
2036,sentiment_analysis30,123,"that the activation rate of gt tends to drop , a tendency not so apparent with our model .",Results,Discussion,sentiment_analysis,30,12,1,0,,1.76E-05,0,negative,0.004351214,6.68E-05,1.22E-05,1.24E-06,2.15E-05,4.95E-06,9.50E-07,1.15E-05,2.41E-05,0.9953919,6.71E-05,4.64E-05,1.05E-07
2037,sentiment_analysis30,124,"In , we further study the sensitivity of model performance to the number of memory chains n ( 2 of which are constrained to track LOC1 and LOC2 , the rest are unconstrained chains ) .",Results,Discussion,sentiment_analysis,30,13,1,0,,2.06E-05,0,negative,0.002022812,0.012561251,4.24E-05,1.18E-06,4.69E-05,9.70E-06,1.94E-06,0.000108218,0.002106073,0.982842406,0.000153359,0.00010353,2.56E-07
2038,sentiment_analysis30,125,"Observe that , when n < 5 , the model suffers from insufficient capacity ( not enough memory chains ) to capture the various aspects required by the task , with aspect detection F 1 remaining below 78 .",Results,Discussion,sentiment_analysis,30,14,1,0,,0.006990774,0,negative,0.090032298,0.000345581,1.54E-05,8.83E-06,6.96E-05,3.06E-05,1.91E-05,0.000110577,4.98E-05,0.907414189,0.000277258,0.001624855,1.90E-06
2039,sentiment_analysis30,126,"In particular , when n = 2 ( no unconstrained chains ) , model performance drops substantially to a F 1 of 76.6 0.4 . Once n ? 5 , aspect detection F 1 increases to around 78 , and is quite stable even with as many as n = 10 chains .",Results,Discussion,sentiment_analysis,30,15,1,0,,0.008331779,0,negative,0.051102806,0.002125068,2.31E-05,2.56E-05,0.000101694,0.000114642,5.06E-05,0.000858175,0.000520263,0.942080504,0.001369297,0.001617728,1.05E-05
2040,sentiment_analysis30,127,Conclusion,,,sentiment_analysis,30,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
2041,relation_extraction5,1,title,,,relation_extraction,5,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
2042,relation_extraction5,2,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,title,,relation_extraction,5,1,1,1,research-problem,0.994247574,1,research-problem,8.39E-08,6.87E-06,1.26E-07,6.83E-08,4.71E-08,9.06E-08,1.06E-06,1.12E-06,9.56E-07,0.00273913,0.997249967,4.25E-07,5.90E-08
2043,relation_extraction5,3,abstract,,,relation_extraction,5,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
2044,relation_extraction5,4,Dependency trees help relation extraction models capture long - range relations between words .,abstract,abstract,relation_extraction,5,1,1,0,,0.646509141,1,research-problem,8.48E-09,2.27E-06,1.96E-08,4.17E-08,2.30E-08,5.07E-08,1.58E-07,3.78E-07,4.78E-07,0.003588511,0.996408031,1.52E-08,1.46E-08
2045,relation_extraction5,5,"However , existing dependency - based models either neglect crucial information ( e.g. , negation ) by pruning the dependency trees too aggressively , or are computationally inefficient because it is difficult to parallelize over different tree structures .",abstract,abstract,relation_extraction,5,2,1,0,,0.006277251,0,research-problem,1.25E-07,3.91E-05,4.74E-08,2.87E-06,1.17E-06,1.55E-06,5.54E-07,8.03E-06,2.80E-06,0.118732115,0.88121156,4.98E-08,7.07E-08
2046,relation_extraction5,6,"We propose an extension of graph convolutional networks that is tailored for relation extraction , which pools information over arbitrary dependency structures efficiently in parallel .",abstract,abstract,relation_extraction,5,3,1,0,,0.513094172,1,research-problem,1.13E-05,0.057366283,0.000144175,6.27E-06,6.14E-05,1.25E-05,1.24E-05,0.00013438,0.011279664,0.084646003,0.846316583,6.65E-06,2.39E-06
2047,relation_extraction5,7,"To incorporate relevant information while maximally removing irrelevant content , we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold .",abstract,abstract,relation_extraction,5,4,1,0,,0.102089261,0,approach,7.22E-05,0.663056916,8.84E-05,2.55E-05,0.000526217,6.49E-05,1.16E-05,0.0011651,0.082831147,0.229747156,0.022401018,7.35E-06,2.45E-06
2048,relation_extraction5,8,"The resulting model achieves state - of - the - art performance on the large - scale TACRED dataset , outperforming existing sequence and dependency - based neural models .",abstract,abstract,relation_extraction,5,5,1,0,,0.017435168,0,research-problem,0.000110681,0.023789544,5.57E-05,2.19E-05,0.000157116,5.88E-05,0.000108609,0.000974959,0.001734331,0.445422465,0.527230617,0.000325569,9.72E-06
2049,relation_extraction5,9,"We also show through detailed analysis that this model has complementary strengths to sequence models , and combining them further improves the state of the art .",abstract,abstract,relation_extraction,5,6,1,0,,0.014084749,0,negative,0.000400208,0.011717066,2.20E-05,4.34E-05,0.000569704,3.26E-05,2.20E-05,0.000181641,0.000811976,0.958813343,0.02727698,0.00010726,1.92E-06
2050,relation_extraction5,10,* Equal contribution .,abstract,abstract,relation_extraction,5,7,1,0,,9.92E-05,0,negative,1.62E-06,0.000203257,6.03E-07,0.000146054,3.99E-05,3.79E-05,2.72E-06,7.72E-05,0.000105861,0.950400078,0.04898419,1.90E-07,4.08E-07
2051,relation_extraction5,11,The order of authorship was decided by a tossed coin .,abstract,abstract,relation_extraction,5,8,1,0,,0.001739062,0,negative,4.58E-07,0.001321337,3.72E-07,3.26E-05,9.54E-05,7.13E-05,2.41E-06,0.000347405,0.000318209,0.975410597,0.022399515,1.75E-07,2.38E-07
2052,relation_extraction5,12,Introduction,,,relation_extraction,5,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
2053,relation_extraction5,13,"Relation extraction involves discerning whether a relation exists between two entities in a sentence ( often termed subject and object , respectively ) .",Introduction,Introduction,relation_extraction,5,1,1,0,,0.440464918,0,research-problem,1.30E-06,0.000116962,5.90E-07,7.89E-05,7.19E-05,1.07E-05,1.52E-05,4.34E-06,3.19E-05,0.040038628,0.959624374,1.06E-06,4.30E-06
2054,relation_extraction5,14,"Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale , such as question answering , knowledge base population , and biomedical knowledge discovery .",Introduction,Introduction,relation_extraction,5,2,1,0,,0.493081213,0,research-problem,6.01E-07,8.04E-05,1.89E-07,9.17E-06,1.08E-05,3.69E-06,8.18E-06,3.44E-06,1.81E-05,0.0272174,0.972646054,6.64E-07,1.44E-06
2055,relation_extraction5,15,"Models making use of dependency parses of the input sentences , or dependency - based models , : An example modified from the TAC KBP challenge corpus .",Introduction,Introduction,relation_extraction,5,3,1,0,,0.009687811,0,research-problem,5.24E-06,0.002016065,1.17E-05,2.76E-06,2.65E-05,4.21E-05,3.09E-05,4.60E-05,0.013362705,0.476128376,0.508319915,5.12E-06,2.74E-06
2056,relation_extraction5,16,"A subtree of the original UD dependency tree between the subject ( "" he "" ) and object ( "" Mike Cane "" ) is also shown , where the shortest dependency path between the entities is highlighted in bold .",Introduction,Introduction,relation_extraction,5,4,1,0,,0.023364244,0,negative,1.13E-05,0.027318483,2.43E-05,7.70E-06,0.000190211,0.000302761,5.80E-05,0.000410193,0.295323457,0.617091803,0.059249722,6.95E-06,5.16E-06
2057,relation_extraction5,17,"Note that negation ( "" not "" ) is off the dependency path .",Introduction,Introduction,relation_extraction,5,5,1,0,,0.004651632,0,negative,1.29E-05,0.00499999,5.65E-06,2.12E-05,0.000380189,0.000252106,2.70E-05,0.000176464,0.016175188,0.967700781,0.010241877,4.39E-06,2.28E-06
2058,relation_extraction5,18,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations thatare obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",Introduction,Introduction,relation_extraction,5,6,1,0,,0.187205689,0,research-problem,3.11E-06,0.000347856,9.33E-07,2.60E-05,5.27E-05,3.11E-05,2.26E-05,2.19E-05,0.00011983,0.213486925,0.785881387,2.48E-06,3.27E-06
2059,relation_extraction5,19,Traditional feature - based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees .,Introduction,Introduction,relation_extraction,5,7,1,0,,0.381925625,0,research-problem,5.75E-05,0.039376347,7.77E-05,1.20E-05,0.000171613,7.98E-05,6.68E-05,0.0001329,0.053022411,0.418083275,0.488892597,2.06E-05,6.59E-06
2060,relation_extraction5,20,"However , these models face the challenge of sparse feature spaces and are brittle to lexical variations .",Introduction,Introduction,relation_extraction,5,8,1,0,,0.037621543,0,negative,1.06E-05,0.001314494,2.94E-06,9.37E-05,0.000175385,9.54E-05,3.99E-05,6.72E-05,0.000464801,0.542710655,0.455012939,5.48E-06,6.54E-06
2061,relation_extraction5,21,More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees .,Introduction,Introduction,relation_extraction,5,9,1,0,,0.022671323,0,research-problem,5.67E-06,0.004863372,1.09E-05,1.08E-05,4.25E-05,0.000173413,6.35E-05,0.00018544,0.027406603,0.454225237,0.513000973,5.02E-06,6.51E-06
2062,relation_extraction5,22,One common approach to leverage dependency information is to perform bottom - up or top - down computation along the parse tree or the subtree below the lowest common ancestor ( LCA ) of the entities .,Introduction,Introduction,relation_extraction,5,10,1,0,,0.092841056,0,negative,1.67E-05,0.006588351,1.16E-05,6.41E-05,0.000246712,0.000131732,5.46E-05,0.000104533,0.003402685,0.51021093,0.479149366,9.04E-06,9.70E-06
2063,relation_extraction5,23,"Another popular approach , inspired by , is to reduce the parse tree to the shortest dependency path between the entities .",Introduction,Introduction,relation_extraction,5,11,1,0,,0.012064102,0,negative,9.24E-06,0.003717459,1.01E-05,5.35E-05,0.000160788,0.000204637,7.69E-05,0.000160196,0.002784643,0.642614048,0.350191302,7.91E-06,9.39E-06
2064,relation_extraction5,24,"However , these models suffer from several drawbacks .",Introduction,Introduction,relation_extraction,5,12,1,0,,0.023886762,0,negative,1.03E-05,0.001316532,2.12E-06,2.53E-05,8.51E-05,4.32E-05,2.91E-05,4.09E-05,0.000735208,0.673553951,0.324149491,5.89E-06,2.96E-06
2065,relation_extraction5,25,"Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient , because aligning trees for efficient batch training is usually nontrivial .",Introduction,Introduction,relation_extraction,5,13,1,0,,0.020401336,0,research-problem,3.41E-06,0.002414078,2.70E-06,1.69E-05,3.42E-05,0.000120294,5.88E-05,0.00017421,0.003186546,0.444489105,0.549487746,5.25E-06,6.82E-06
2066,relation_extraction5,26,"Models based on the shortest dependency path between the subject and object are computationally more efficient , but this simplifying assumption has major limitations as well .",Introduction,Introduction,relation_extraction,5,14,1,0,,0.027321353,0,negative,1.38E-05,0.004063216,5.73E-06,1.45E-05,0.000115472,9.11E-05,4.48E-05,0.000106164,0.004138428,0.866459813,0.124934015,9.45E-06,3.55E-06
2067,relation_extraction5,27,"shows a real - world example where crucial information ( i.e. , negation ) would be excluded when the model is restricted to only considering the dependency path .",Introduction,Introduction,relation_extraction,5,15,1,0,,0.015065963,0,negative,8.10E-05,0.009790137,4.75E-05,1.83E-05,0.006262328,0.000106102,7.26E-05,4.95E-05,0.009823593,0.97164908,0.002060049,3.70E-05,2.72E-06
2068,relation_extraction5,28,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .",Introduction,Introduction,relation_extraction,5,16,1,1,model,0.960246135,1,model,5.47E-05,0.247730239,0.000403247,5.72E-06,0.000524997,4.59E-05,8.35E-05,6.77E-05,0.730300353,0.015237019,0.005517155,2.13E-05,8.15E-06
2069,relation_extraction5,29,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .",Introduction,Introduction,relation_extraction,5,17,1,1,model,0.952777117,1,model,5.37E-06,0.038754033,3.94E-05,3.41E-07,5.20E-05,7.19E-06,5.11E-06,1.21E-05,0.956735975,0.004129857,0.000256618,1.25E-06,7.12E-07
2070,relation_extraction5,30,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",Introduction,Introduction,relation_extraction,5,18,1,1,model,0.841070976,1,approach,0.000205039,0.590735812,0.000133526,2.04E-05,0.002271219,8.81E-05,9.51E-05,0.000213526,0.377187848,0.028316812,0.000688503,3.64E-05,7.75E-06
2071,relation_extraction5,31,"We test our model on the popular SemEval 2010 Task 8 dataset and the more recent , larger TAC - RED dataset .",Introduction,Introduction,relation_extraction,5,19,1,0,,0.018239647,0,negative,0.000113521,0.401708042,0.000101512,0.000120975,0.06365068,0.001051552,0.001380505,0.001487406,0.011043605,0.517417348,0.001600265,0.000281593,4.30E-05
2072,relation_extraction5,32,"On both datasets , our model not only outperforms existing dependency - based neural models by a significant margin when combined with the new pruning technique , but also achieves a 10 - 100x speedup over existing tree - based models .",Introduction,Introduction,relation_extraction,5,20,1,0,,0.163697361,0,negative,0.007922719,0.101648901,0.000301802,0.000326922,0.006512177,0.001657379,0.05196296,0.003549952,0.015951102,0.767831661,0.015055203,0.026228238,0.001050985
2073,relation_extraction5,33,"On TACRED , our model further achieves the state - of - the - art performance , surpassing a competitive neural sequence model baseline .",Introduction,Introduction,relation_extraction,5,21,1,0,,0.060981816,0,negative,0.008597102,0.080969876,0.000415901,0.000496335,0.011385031,0.001794157,0.082148812,0.003545592,0.009411088,0.738027161,0.010160898,0.051385887,0.001662158
2074,relation_extraction5,34,"This model also exhibits complementary strengths to sequence models on TACRED , and combining these two model types through simple prediction interpolation further improves the state of the art .",Introduction,Introduction,relation_extraction,5,22,1,0,,0.198590304,0,negative,0.010294805,0.108634238,0.001670208,0.000402161,0.037109417,0.001122885,0.009501244,0.000477981,0.056068057,0.762379544,0.005174651,0.006938339,0.000226471
2075,relation_extraction5,35,"To recap , our main contributions are : ( i ) we propose a neural model for relation extraction based on graph convolutional networks , which allows it to efficiently pool information over arbitrary dependency structures ; ( ii ) we present a new pathcentric pruning technique to help dependencybased models maximally remove irrelevant information without damaging crucial content to improve their robustness ; ( iii ) we present detailed analysis on the model and the pruning technique , and show that dependency - based models have complementary strengths with sequence models .",Introduction,Introduction,relation_extraction,5,23,1,0,,0.47490067,0,approach,0.000177923,0.520981785,0.000172426,4.06E-05,0.002954511,0.000123518,0.000157198,0.000223532,0.322902947,0.142160821,0.010005472,8.21E-05,1.72E-05
2076,relation_extraction5,36,Models,,,relation_extraction,5,0,1,0,,0.013378693,0,negative,0.001769058,0.000281584,0.001312991,0.00016118,0.000150821,0.001858418,0.032853537,0.00468419,0.00015464,0.882857056,0.025147519,0.048083361,0.000685644
2077,relation_extraction5,37,"In this section , we first describe graph convolutional networks ( GCNs ) over dependency tree structures , and then we introduce an architecture that uses GCNs at its core for relation extraction .",Models,Models,relation_extraction,5,1,1,0,,0.191172775,0,baselines,0.000809426,0.000301348,0.615727221,2.90E-06,7.98E-06,0.000276362,0.046784871,0.000124555,0.003975363,0.330468874,0.000218646,0.00099826,0.000304199
2078,relation_extraction5,38,Graph Convolutional Networks over Dependency Trees,Models,,relation_extraction,5,2,1,0,,0.484725937,0,negative,0.0003348,3.58E-05,0.149505498,1.63E-06,4.91E-07,0.000482238,0.365219709,0.00024844,0.001257108,0.471076717,0.006101928,0.003410919,0.002324691
2079,relation_extraction5,39,The graph convolutional network is an adaptation of the convolutional neural network for encoding graphs .,Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,3,1,0,,0.037071813,0,negative,0.000119962,3.09E-05,0.125817314,8.29E-07,4.04E-07,6.72E-05,0.000310271,0.00022831,0.002792197,0.869978987,5.57E-05,0.00055825,3.96E-05
2080,relation_extraction5,40,"Given a graph with n nodes , we can represent the graph structure with an n n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j.",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,4,1,0,,3.98E-05,0,negative,9.04E-07,8.56E-07,1.31E-05,1.56E-08,7.57E-09,1.69E-06,1.32E-06,1.60E-05,9.30E-05,0.999853539,4.05E-06,1.53E-05,2.70E-07
2081,relation_extraction5,41,"In an L-layer GCN , if we denote by h ( l?1 ) i the input vector and h ( l ) i the output vector of node i at the l - th layer , a graph convolution operation can be written as",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,5,1,0,,3.67E-05,0,negative,3.07E-06,1.20E-06,5.47E-05,2.66E-08,1.09E-08,1.58E-06,2.38E-06,1.32E-05,6.92E-05,0.999802102,6.46E-06,4.57E-05,4.40E-07
2082,relation_extraction5,42,"where W ( l ) is a linear transformation , b ( l ) a bias term , and ?",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,6,1,0,,1.73E-05,0,negative,1.11E-05,4.46E-07,2.96E-05,2.23E-08,1.06E-08,1.50E-06,2.33E-06,1.51E-05,1.06E-05,0.999838008,6.68E-07,9.04E-05,1.78E-07
2083,relation_extraction5,43,"a nonlinear function ( e.g. , ReLU ) .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,7,1,0,,8.89E-05,0,negative,6.19E-06,4.86E-07,6.52E-05,3.62E-08,1.11E-08,2.78E-06,3.49E-06,2.26E-05,5.65E-05,0.999778416,2.58E-06,6.08E-05,8.68E-07
2084,relation_extraction5,44,"Intuitively , during each graph convolution , each node gathers and summarizes information from its neighboring nodes in the graph .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,8,1,0,,0.001058795,0,negative,1.36E-05,5.09E-06,0.000200628,1.84E-08,3.88E-08,7.47E-07,1.27E-06,6.28E-06,0.000396644,0.999328951,1.10E-06,4.54E-05,2.58E-07
2085,relation_extraction5,45,"We adapt the graph convolution operation to model dependency trees by converting each tree into its corresponding adjacency matrix A , where A ij = 1 if there is a dependency edge between tokens i and j .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,9,1,0,,0.000400572,0,negative,0.000140761,8.62E-05,0.002256574,1.48E-07,2.60E-07,6.72E-06,2.39E-05,7.68E-05,0.003107244,0.993952112,9.51E-06,0.00033696,2.70E-06
2086,relation_extraction5,46,"However , naively applying the graph convolution operation in Equation ( 1 ) could lead to node representations with drastically different magnitudes , since the degree of a token varies a lot .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,10,1,0,,0.000204562,0,negative,6.93E-05,1.88E-07,5.49E-06,7.49E-09,6.87E-09,3.77E-07,1.81E-06,2.81E-06,2.18E-06,0.998928178,7.08E-07,0.000988863,1.20E-07
2087,relation_extraction5,47,This could bias our sentence representation towards favoring high - degree nodes regardless of the information carried in the node ( see details in Section 2.2 ) .,Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,11,1,0,,4.20E-05,0,negative,9.16E-06,4.17E-07,9.77E-06,2.15E-08,1.37E-08,9.15E-07,8.63E-07,7.47E-06,2.56E-05,0.999911189,2.10E-07,3.42E-05,1.44E-07
2088,relation_extraction5,48,"Furthermore , the information in h ( l?1 ) i is never carried over to hi , since nodes never connect to themselves in a dependency tree .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,12,1,0,,4.45E-05,0,negative,3.17E-05,3.48E-07,3.00E-05,3.99E-09,7.29E-09,2.22E-07,7.55E-07,2.39E-06,1.30E-05,0.999715868,2.16E-07,0.00020539,6.64E-08
2089,relation_extraction5,49,"We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity , and adding self - loops to each node in the graph :",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,13,1,0,,0.000364334,0,negative,0.000672659,1.65E-05,0.000662188,2.06E-07,3.67E-07,4.17E-06,1.51E-05,3.29E-05,0.000249604,0.997645217,1.01E-06,0.000698673,1.48E-06
2090,relation_extraction5,50,"where = A + I with I being then n identity matrix , and d i = n j=1 ij is the degree of token i in the resulting graph . :",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,14,1,0,,1.09E-05,0,negative,8.28E-06,1.97E-07,9.00E-06,4.60E-08,1.79E-08,1.47E-06,1.98E-06,8.71E-06,8.55E-06,0.999890394,2.28E-07,7.09E-05,2.56E-07
2091,relation_extraction5,51,Relation extraction with a graph convolutional network .,Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,15,1,0,,0.001598358,0,negative,0.000119269,6.31E-06,0.001717785,6.14E-07,1.80E-07,8.21E-06,0.000323626,3.00E-05,5.91E-05,0.965278094,0.001274576,0.031072803,0.00010939
2092,relation_extraction5,52,"The left side shows the over all architecture , while on the right side , we only show the detailed graph convolution computation for the word "" relative "" for clarity .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,16,1,0,,3.03E-05,0,negative,3.72E-06,2.99E-07,5.21E-06,3.86E-08,1.90E-08,1.21E-06,1.72E-06,1.25E-05,1.80E-05,0.999922642,1.24E-07,3.43E-05,2.34E-07
2093,relation_extraction5,53,A full unlabeled dependency parse of the sentence is also provided for reference .,Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,17,1,0,,0.000257288,0,negative,6.55E-06,1.13E-07,4.36E-06,6.01E-08,5.99E-08,1.37E-06,2.38E-06,1.02E-05,3.11E-06,0.999855312,2.93E-08,0.000116252,2.57E-07
2094,relation_extraction5,54,Stacking this operation over,Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,18,1,0,,1.57E-05,0,negative,1.23E-05,2.18E-07,2.51E-05,2.35E-08,1.81E-08,7.16E-07,2.71E-06,3.24E-06,2.28E-05,0.999758401,2.74E-07,0.000173537,6.22E-07
2095,relation_extraction5,55,"L layers gives us a deep GCN network , where we set h Moreover , the propagation of information between tokens occurs in parallel , and the runtime does not depend on the depth of the dependency tree .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,19,1,0,,0.00028509,0,negative,0.000121664,4.88E-06,0.000300999,4.08E-07,3.40E-07,1.60E-05,4.54E-05,0.000126771,0.000446716,0.998570219,1.89E-06,0.000352986,1.18E-05
2096,relation_extraction5,56,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph .,Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,20,1,0,,3.21E-05,0,negative,9.02E-06,6.70E-07,1.78E-05,7.92E-09,1.07E-08,3.91E-07,8.50E-07,5.81E-06,3.15E-05,0.999885427,8.72E-08,4.83E-05,8.13E-08
2097,relation_extraction5,57,"We also experimented with : ( 1 ) using different transformation matrices W for topdown , bottom - up , and self - loop edges ; and ( 2 ) adding dependency relation - specific parameters for edge - wise gating , similar to .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,21,1,0,,0.005838593,0,negative,0.000183965,3.90E-06,0.000156872,6.17E-08,2.01E-07,4.67E-06,3.45E-05,2.77E-05,2.40E-05,0.998172268,1.63E-07,0.001391,7.79E-07
2098,relation_extraction5,58,"We found that modeling directions does not lead to improvement , 1 and adding edgewise gating further hurts performance .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,22,1,0,,0.300621927,0,negative,0.002394495,6.51E-07,3.56E-05,3.28E-07,2.05E-07,9.73E-06,0.00029618,6.48E-05,3.98E-06,0.852408062,8.68E-07,0.144769779,1.53E-05
2099,relation_extraction5,59,"We hypothesize that this is because the presented GCN model is usually already able to capture dependency edge patterns thatare informative for classifying relations , and modeling edge directions and types does not offer additional discriminative power to the network before it leads to overfitting .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,23,1,0,,5.52E-05,0,negative,7.63E-06,5.03E-07,3.53E-06,3.30E-08,2.28E-08,1.25E-06,1.82E-06,1.92E-05,1.72E-05,0.999878936,2.09E-07,6.93E-05,4.12E-07
2100,relation_extraction5,60,"For example , the relations entailed by "" A 's son , B "" and "" B 's son , A "" can be readily distinguished with "" 's "" attached to different entities , even when edge directionality is not considered .",Models,Graph Convolutional Networks over Dependency Trees,relation_extraction,5,24,1,0,,1.22E-05,0,negative,1.67E-06,6.70E-08,1.61E-06,7.83E-09,5.46E-09,3.19E-07,6.48E-07,2.19E-06,3.63E-06,0.99993414,1.65E-07,5.54E-05,1.10E-07
2101,relation_extraction5,61,Encoding Relations with GCN,Models,,relation_extraction,5,25,1,0,,0.589006984,1,negative,0.000148917,2.30E-05,0.030385867,4.38E-07,4.66E-07,0.000169274,0.406749787,0.000171776,0.000718525,0.554300006,0.000293642,0.003931837,0.00310648
2102,relation_extraction5,62,We now formally define the task of relation extraction .,Models,Encoding Relations with GCN,relation_extraction,5,26,1,0,,2.02E-06,0,negative,6.48E-06,1.07E-06,1.36E-05,2.65E-07,2.84E-08,2.10E-06,5.41E-06,4.37E-05,1.79E-05,0.999218257,1.03E-05,0.000666969,1.39E-05
2103,relation_extraction5,63,"Let X = [ x 1 , ... , x n ] denote a sentence , where xi is the i th token .",Models,Encoding Relations with GCN,relation_extraction,5,27,1,0,,4.75E-07,0,negative,4.52E-07,9.51E-08,2.03E-06,5.72E-09,3.20E-09,4.28E-07,5.84E-07,1.48E-05,4.72E-06,0.999942317,5.94E-08,3.41E-05,3.76E-07
2104,relation_extraction5,64,A subject entity and an object entity are identified and correspond to two spans in the sentence :,Models,Encoding Relations with GCN,relation_extraction,5,28,1,0,,9.97E-07,0,negative,8.82E-06,9.54E-08,1.61E-05,9.64E-08,3.72E-08,6.91E-07,1.37E-06,8.26E-06,3.67E-06,0.999627887,6.57E-08,0.00033093,2.01E-06
2105,relation_extraction5,65,.,Models,Encoding Relations with GCN,relation_extraction,5,29,1,0,,1.32E-07,0,negative,1.11E-06,1.62E-08,5.88E-07,1.83E-09,7.53E-10,1.04E-07,1.43E-07,2.99E-06,1.49E-06,0.999970283,6.94E-09,2.32E-05,7.11E-08
2106,relation_extraction5,66,"Given X , X s , and X o , the goal of relation extraction is to predict a relation r ?",Models,Encoding Relations with GCN,relation_extraction,5,30,1,0,,5.00E-07,0,negative,5.04E-06,4.28E-07,1.33E-05,1.74E-07,1.91E-08,1.52E-06,4.47E-06,3.53E-05,9.87E-06,0.999338648,6.67E-06,0.000570332,1.42E-05
2107,relation_extraction5,67,"R ( a predefined relation set ) that holds between the entities or "" no relation "" otherwise .",Models,Encoding Relations with GCN,relation_extraction,5,31,1,0,,3.36E-07,0,negative,1.22E-06,9.00E-08,1.26E-05,2.96E-09,3.24E-09,1.96E-07,4.99E-07,6.95E-06,5.41E-06,0.99991426,3.18E-08,5.84E-05,3.05E-07
2108,relation_extraction5,68,"After applying an L-layer GCN over word vectors , we obtain hidden representations of each token thatare directly influenced by its neighbors no more than L edges apart in the dependency tree .",Models,Encoding Relations with GCN,relation_extraction,5,32,1,0,,5.74E-05,0,negative,7.21E-05,2.24E-06,0.000301336,1.04E-08,3.54E-08,2.95E-07,2.93E-06,7.67E-06,0.000120045,0.998321293,9.21E-08,0.00117057,1.37E-06
2109,relation_extraction5,69,"To make use of these word representations for relation extraction , we first obtain a sentence representation as follows ( see also left ) :",Models,Encoding Relations with GCN,relation_extraction,5,33,1,0,,2.34E-06,0,negative,4.47E-06,1.69E-07,0.000133777,1.65E-09,4.15E-09,1.42E-07,6.53E-07,2.54E-06,2.07E-05,0.999712326,2.77E-08,0.00012489,3.49E-07
2110,relation_extraction5,70,"where h ( l ) denotes the collective hidden representations at layer l of the GCN , and f : R dn ?",Models,Encoding Relations with GCN,relation_extraction,5,34,1,0,,7.59E-07,0,negative,6.28E-06,2.30E-07,8.25E-06,1.01E-08,6.76E-09,5.15E-07,1.92E-06,1.95E-05,3.25E-06,0.999487957,7.53E-08,0.000471333,6.96E-07
2111,relation_extraction5,71,Rd is a max pooling function that maps from n output vectors to the sentence vector .,Models,Encoding Relations with GCN,relation_extraction,5,35,1,0,,6.53E-06,0,negative,6.68E-06,2.87E-06,0.004175267,8.20E-08,5.03E-08,5.25E-06,2.71E-05,0.000129868,0.000289033,0.995159943,1.40E-06,0.000171333,3.11E-05
2112,relation_extraction5,72,We also observe that information close to entity tokens in the dependency tree is often central to relation classification .,Models,Encoding Relations with GCN,relation_extraction,5,36,1,0,,0.000889341,0,negative,0.000712371,7.36E-07,6.42E-05,4.86E-08,9.15E-08,7.09E-07,3.27E-05,1.21E-05,3.18E-06,0.919964345,1.94E-07,0.079202635,6.69E-06
2113,relation_extraction5,73,"Therefore , we also obtain a subject representation h s from h ( L ) as follows",Models,Encoding Relations with GCN,relation_extraction,5,37,1,0,,3.96E-07,0,negative,5.69E-06,5.30E-08,1.72E-05,7.86E-10,2.06E-09,4.28E-08,2.64E-07,1.02E-06,3.13E-06,0.999802162,5.40E-09,0.000170339,7.36E-08
2114,relation_extraction5,74,as well as an object representation ho similarly .,Models,Encoding Relations with GCN,relation_extraction,5,38,1,0,,5.94E-07,0,negative,1.43E-05,5.00E-08,2.15E-05,6.62E-09,6.68E-09,2.08E-07,1.02E-06,3.04E-06,2.96E-06,0.999238089,2.94E-08,0.000717889,8.26E-07
2115,relation_extraction5,75,"Inspired by recent work on relational learning between entities , we obtain the final representation used for classification by concatenating the sentence and the entity representations , and feeding them through a feed - forward neural network ( FFNN ) :",Models,Encoding Relations with GCN,relation_extraction,5,39,1,0,,2.50E-05,0,negative,2.16E-05,1.50E-06,0.001818448,5.79E-09,2.31E-08,3.06E-07,2.79E-06,6.41E-06,0.000137802,0.997812529,4.71E-08,0.00019691,1.61E-06
2116,relation_extraction5,76,This h final representation is then fed into a linear layer followed by a softmax operation to obtain a probability distribution over relations .,Models,Encoding Relations with GCN,relation_extraction,5,40,1,0,,1.54E-05,0,negative,1.42E-05,7.83E-07,0.000351587,8.57E-09,1.68E-08,4.52E-07,2.55E-06,1.48E-05,0.000242067,0.999276401,2.90E-08,9.40E-05,3.06E-06
2117,relation_extraction5,77,Contextualized GCN,Models,,relation_extraction,5,41,1,0,,0.14247808,0,experiments,0.0001519,5.58E-06,0.073140704,3.44E-07,8.42E-07,0.000167771,0.463537082,0.000101661,0.000403146,0.454492787,4.92E-06,0.002519487,0.005473777
2118,relation_extraction5,78,"The network architecture introduced so far learns effective representations for relation extraction , but it also leaves a few issues inadequately addressed .",Models,Contextualized GCN,relation_extraction,5,42,1,0,,3.69E-06,0,negative,2.07E-05,1.14E-07,1.09E-05,6.36E-08,1.62E-08,6.46E-07,6.35E-06,9.60E-06,2.02E-06,0.998157252,3.44E-07,0.00177685,1.51E-05
2119,relation_extraction5,79,"First , the input word vectors do not contain contextual information about word order or dis ambiguation .",Models,Contextualized GCN,relation_extraction,5,43,1,0,,4.63E-06,0,negative,8.09E-05,2.46E-07,3.54E-05,8.66E-08,4.20E-08,3.28E-07,1.88E-06,5.27E-06,4.47E-06,0.999124807,2.77E-08,0.00074407,2.46E-06
2120,relation_extraction5,80,"Second , the GCN highly depends on a correct parse tree to extract crucial information from the sentence ( especially when pruning is performed ) , while existing parsing algorithms produce imperfect trees in many cases .",Models,Contextualized GCN,relation_extraction,5,44,1,0,,3.71E-05,0,negative,2.05E-05,2.66E-07,3.94E-05,1.28E-07,2.71E-08,7.93E-07,1.04E-05,1.26E-05,3.59E-06,0.998031476,6.90E-07,0.00184855,3.15E-05
2121,relation_extraction5,81,"To resolve these issues , we further apply a Contextualized GCN ( C - GCN ) model , where the input word vectors are first fed into a bi-directional long short - term memory ( LSTM ) network to generate contextualized representations , which are then used ash ( 0 ) in the original model .",Models,Contextualized GCN,relation_extraction,5,45,1,0,,0.006890517,0,negative,9.53E-05,9.78E-06,0.004688406,2.20E-08,1.17E-07,5.38E-07,1.87E-05,1.71E-05,0.000534945,0.993915177,6.59E-08,0.000704254,1.56E-05
2122,relation_extraction5,82,This BiL - STM contextualization layer is trained jointly with the rest of the network .,Models,Contextualized GCN,relation_extraction,5,46,1,0,,2.04E-05,0,negative,5.05E-05,2.07E-06,0.000703595,6.12E-08,6.30E-08,9.46E-07,1.06E-05,5.21E-05,0.00066839,0.998369417,2.51E-08,0.000103009,3.93E-05
2123,relation_extraction5,83,We show empirically in Section 5 that this augmentation substantially improves the performance over the original model .,Models,Contextualized GCN,relation_extraction,5,47,1,0,,0.000225683,0,negative,0.003267181,9.59E-07,6.51E-05,6.03E-08,1.10E-07,5.63E-07,7.14E-05,2.61E-05,8.81E-06,0.943869478,9.57E-09,0.05267459,1.56E-05
2124,relation_extraction5,84,"We note that this relation extraction model is conceptually similar to graph kernel - based models , in that it aims to utilize local dependency tree patterns to inform relation classification .",Models,Contextualized GCN,relation_extraction,5,48,1,0,,8.04E-06,0,negative,3.16E-05,1.38E-06,0.001141224,8.14E-09,3.14E-08,1.27E-07,2.99E-06,2.76E-06,0.000134337,0.998255776,3.19E-08,0.000426042,3.67E-06
2125,relation_extraction5,85,"Our model also incorporates crucial off - path information , which greatly improves its robustness compared to shortest dependency pathbased approaches .",Models,Contextualized GCN,relation_extraction,5,49,1,0,,0.0019851,0,negative,0.000521037,3.20E-06,0.000391978,1.39E-07,1.68E-07,7.75E-07,2.32E-05,2.44E-05,0.000166754,0.993982401,2.70E-08,0.004853888,3.21E-05
2126,relation_extraction5,86,"Compared to tree - structured models ( e.g. , Tree - LSTM",Models,Contextualized GCN,relation_extraction,5,50,1,0,,0.000189771,0,negative,8.72E-05,1.98E-07,0.001335721,4.93E-09,1.41E-08,1.83E-07,4.19E-05,3.16E-06,3.22E-06,0.981714346,3.10E-08,0.016806576,7.40E-06
2127,relation_extraction5,87,"( Tai et al. , 2015 ) ) , it not only is able to capture more global information through the use of pooling functions , but also achieves substantial speedup by not requiring recursive operations thatare difficult to parallelize .",Models,Contextualized GCN,relation_extraction,5,51,1,0,,1.50E-05,0,negative,1.79E-05,9.27E-08,0.000188898,1.38E-08,2.13E-08,2.00E-07,4.66E-06,2.62E-06,3.07E-06,0.998409347,1.48E-08,0.001364706,8.49E-06
2128,relation_extraction5,88,"For example , we observe that on a Titan Xp GPU , training a Tree - LSTM model over a minibatch of 50 examples takes 6.54 seconds on average , while training the original GCN model takes only 0.07 seconds , and the C - GCN model 0.08 seconds .",Models,Contextualized GCN,relation_extraction,5,52,1,0,,0.000296343,0,negative,7.11E-05,3.01E-07,9.10E-06,3.39E-07,1.33E-07,7.53E-06,0.000261335,0.000257852,4.15E-06,0.990496833,2.22E-08,0.008391675,0.000499589
2129,relation_extraction5,89,Incorporating Off - path Information with Path - centric Pruning,Models,Contextualized GCN,relation_extraction,5,53,1,0,,0.001293445,0,negative,6.57E-05,1.85E-06,0.000655402,9.79E-09,2.05E-08,2.53E-07,3.84E-05,6.33E-06,8.82E-05,0.989098211,3.56E-07,0.010021362,2.39E-05
2130,relation_extraction5,90,"Dependency trees provide rich structures that one can exploit in relation extraction , but most of the information pertinent to relations is usually contained within the subtree rooted at the lowest common ancestor ( LCA ) of the two entities .",Models,Contextualized GCN,relation_extraction,5,54,1,0,,0.000169105,0,negative,2.18E-05,5.15E-07,0.000138981,3.62E-07,8.24E-08,2.26E-06,3.31E-05,1.75E-05,1.19E-05,0.996780594,1.69E-06,0.002656242,0.00033496
2131,relation_extraction5,91,Previous studies have shown that removing tokens outside this scope helps relation extraction by eliminating irrelevant information from the sentence .,Models,Contextualized GCN,relation_extraction,5,55,1,0,,3.26E-05,0,negative,2.54E-05,1.79E-07,2.29E-05,8.42E-08,2.93E-08,6.95E-07,2.26E-05,1.39E-05,1.80E-06,0.994909626,6.27E-07,0.00491899,8.31E-05
2132,relation_extraction5,92,It is therefore desirable to combine our GCN models with tree pruning strategies to further improve performance .,Models,Contextualized GCN,relation_extraction,5,56,1,0,,0.000157452,0,negative,8.56E-05,1.42E-07,1.72E-05,2.01E-08,1.55E-08,2.22E-07,3.40E-06,7.24E-06,5.98E-06,0.998329943,5.74E-09,0.001545793,4.44E-06
2133,relation_extraction5,93,"However , pruning too aggressively ( e.g. , keeping only the dependency path ) could lead to loss of crucial information and conversely hurt robustness .",Models,Contextualized GCN,relation_extraction,5,57,1,0,,6.68E-06,0,negative,0.000107915,2.81E-08,4.65E-06,8.65E-09,6.02E-09,8.93E-08,1.67E-06,2.10E-06,6.79E-07,0.997643749,2.40E-09,0.002237568,1.53E-06
2134,relation_extraction5,94,"For instance , the negation in is neglected when a model is restricted to only looking at the dependency path between the entities .",Models,Contextualized GCN,relation_extraction,5,58,1,0,,2.44E-06,0,negative,1.22E-05,2.00E-08,9.36E-06,3.50E-09,4.04E-09,6.23E-08,6.69E-07,1.87E-06,8.32E-07,0.999704863,2.03E-09,0.000269185,9.31E-07
2135,relation_extraction5,95,"Similarly , in the sentence "" She was diagnosed with cancer last year , and succumbed this June "" , the dependency path She?diagnosed ?",Models,Contextualized GCN,relation_extraction,5,59,1,0,,1.79E-07,0,negative,8.86E-07,6.69E-09,2.62E-06,4.51E-09,5.26E-09,6.70E-08,2.64E-07,1.39E-06,6.15E-07,0.999966043,3.74E-10,2.75E-05,6.02E-07
2136,relation_extraction5,96,cancer is not sufficient to establish that cancer is the cause of death for the subject unless the conjunction dependency to succumbed is also present .,Models,Contextualized GCN,relation_extraction,5,60,1,0,,6.63E-07,0,negative,1.26E-05,1.70E-08,3.54E-06,1.58E-08,1.71E-08,1.52E-07,1.56E-06,4.08E-06,8.71E-07,0.999484811,9.94E-10,0.000489445,2.94E-06
2137,relation_extraction5,97,"Motivated by these observations , we propose path - centric pruning , a novel technique to incorporate information off the dependency path .",Models,Contextualized GCN,relation_extraction,5,61,1,0,,0.000255894,0,negative,0.000524438,2.53E-05,0.001585587,2.70E-07,7.76E-07,1.42E-06,4.58E-05,4.70E-05,0.000378751,0.994295124,3.96E-08,0.00302471,7.09E-05
2138,relation_extraction5,98,This is achieved by including tokens thatare up to distance K away from the dependency path in the LCA subtree .,Models,Contextualized GCN,relation_extraction,5,62,1,0,,3.97E-06,0,negative,2.68E-05,7.31E-07,0.000149369,3.61E-09,3.40E-08,9.39E-08,2.21E-06,5.52E-06,5.32E-05,0.999574945,1.11E-09,0.000184511,2.60E-06
2139,relation_extraction5,99,"K = 0 , corresponds to pruning the tree down to the path , K = 1 keeps all nodes thatare directly attached to the path , and K = ?",Models,Contextualized GCN,relation_extraction,5,63,1,0,,6.05E-07,0,negative,2.61E-06,9.33E-08,9.51E-06,1.97E-09,6.52E-09,2.42E-07,3.78E-06,1.89E-05,3.17E-06,0.99984993,7.95E-10,0.000109701,2.06E-06
2140,relation_extraction5,100,retains the entire LCA subtree .,Models,Contextualized GCN,relation_extraction,5,64,1,0,,1.22E-05,0,negative,6.30E-06,7.86E-08,0.000305669,1.27E-09,6.44E-09,5.57E-08,3.34E-06,2.07E-06,1.85E-05,0.999455829,1.61E-09,0.000204523,3.65E-06
2141,relation_extraction5,101,"We combine this pruning strategy with our GCN model , by directly feeding the pruned trees into the graph convolutional layers .",Models,Contextualized GCN,relation_extraction,5,65,1,0,,5.36E-05,0,negative,7.05E-05,2.83E-06,0.0004128,1.91E-08,8.96E-08,2.56E-07,6.96E-06,1.43E-05,0.000123648,0.999045473,2.01E-09,0.000313191,1.00E-05
2142,relation_extraction5,102,"We show that pruning with K = 1 achieves the best balance between including relevant information ( e.g. , negation and conjunction ) and keeping irrelevant content out of the resulting pruned tree as much as possible .",Models,Contextualized GCN,relation_extraction,5,66,1,0,,0.000305542,0,negative,0.000261234,2.46E-06,3.38E-05,4.15E-07,1.71E-07,3.81E-06,0.000203461,0.00048443,2.77E-05,0.983277825,1.55E-08,0.015452418,0.00025227
2143,relation_extraction5,103,Related Work,,,relation_extraction,5,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
2144,relation_extraction5,121,Experiments,,,relation_extraction,5,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
2145,relation_extraction5,122,Baseline Models,,,relation_extraction,5,0,1,0,,0.01144179,0,negative,7.95E-05,0.000152953,0.000324645,2.06E-06,2.38E-06,0.00054363,0.001153762,0.004225388,0.000103339,0.986423474,0.003492121,0.003477786,1.90E-05
2146,relation_extraction5,123,We compare our models with several competitive dependency - based and neural sequence models .,Baseline Models,Baseline Models,relation_extraction,5,1,1,0,,0.221877655,0,baselines,2.73E-05,1.15E-05,0.84881896,3.29E-07,3.00E-07,5.56E-05,0.000951485,0.000536572,6.50E-06,0.148955386,6.70E-06,0.000623096,6.27E-06
2147,relation_extraction5,124,Dependency - based models .,Baseline Models,Baseline Models,relation_extraction,5,2,1,1,baselines,0.730821074,1,baselines,3.18E-06,4.73E-07,0.977558801,1.15E-08,3.42E-09,3.74E-06,0.0002465,4.05E-05,2.27E-06,0.021761727,4.21E-05,0.00033731,3.37E-06
2148,relation_extraction5,125,In our main experiments we compare with three types of dependency - based models .,Baseline Models,Baseline Models,relation_extraction,5,3,1,0,,0.545492382,1,baselines,8.96E-06,3.74E-06,0.946959389,3.77E-08,6.03E-08,6.24E-06,0.000255361,6.32E-05,2.15E-06,0.052401876,3.25E-06,0.00029421,1.55E-06
2149,relation_extraction5,126,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,Baseline Models,Baseline Models,relation_extraction,5,4,1,1,baselines,0.995675175,1,baselines,2.99E-07,2.60E-08,0.999596918,1.23E-09,6.53E-10,1.90E-07,7.01E-06,6.96E-07,1.28E-07,0.000391486,1.09E-07,2.96E-06,1.75E-07
2150,relation_extraction5,127,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .",Baseline Models,Baseline Models,relation_extraction,5,5,1,1,baselines,0.998340181,1,baselines,9.48E-08,1.38E-08,0.999828217,5.40E-10,2.78E-10,1.23E-07,6.98E-06,4.88E-07,7.86E-08,0.000162526,6.09E-08,1.29E-06,1.29E-07
2151,relation_extraction5,128,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .",Baseline Models,Baseline Models,relation_extraction,5,6,1,1,baselines,0.990697584,1,baselines,7.23E-08,1.53E-08,0.999719941,4.67E-10,2.34E-10,1.71E-07,8.49E-06,9.54E-07,1.27E-07,0.000268915,6.34E-08,1.13E-06,1.23E-07
2152,relation_extraction5,129,"We investigate the child - sum variant of Tree - LSTM , and apply it to the dependency tree ( or part of it ) .",Baseline Models,Baseline Models,relation_extraction,5,7,1,0,,0.462109507,0,baselines,5.92E-06,4.77E-06,0.989622671,5.43E-08,4.06E-08,4.17E-06,0.000150972,4.86E-05,3.91E-06,0.010037496,4.92E-06,0.000113877,2.63E-06
2153,relation_extraction5,130,"In practice , we find that modifying this model by concatenating dependency label embeddings to the input of forget gates improves its performance on relation extraction , and therefore use this variant in our experiments .",Baseline Models,Baseline Models,relation_extraction,5,8,1,0,,0.421668816,0,results,0.005001409,7.85E-06,0.310867305,1.48E-06,1.26E-06,5.98E-05,0.006284592,0.000638227,2.99E-06,0.273445724,2.00E-05,0.403599404,7.00E-05
2154,relation_extraction5,131,"Earlier , our group compared and with sequence models , and we report these results ; for ( 3 ) we report results with our own implementation .",Baseline Models,Baseline Models,relation_extraction,5,9,1,0,,0.085145129,0,negative,1.69E-05,1.60E-06,0.394206076,9.21E-07,3.54E-07,8.73E-05,0.001803778,0.000466532,3.62E-06,0.601731995,4.15E-05,0.001602972,3.65E-05
2155,relation_extraction5,132,Neural sequence model .,Baseline Models,Baseline Models,relation_extraction,5,10,1,1,baselines,0.860366177,1,baselines,3.19E-06,8.29E-07,0.958271098,1.49E-07,2.79E-08,2.69E-05,0.000763511,0.000271128,1.62E-05,0.040393592,2.58E-05,0.000198886,2.87E-05
2156,relation_extraction5,133,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .",Baseline Models,Baseline Models,relation_extraction,5,11,1,1,baselines,0.986022715,1,baselines,1.40E-06,1.90E-07,0.996419782,1.15E-08,7.37E-09,9.05E-07,5.18E-05,4.93E-06,5.80E-07,0.003476022,1.25E-06,4.17E-05,1.43E-06
2157,relation_extraction5,134,"We compare with this strong baseline , and use its open implementation in further analysis .",Baseline Models,Baseline Models,relation_extraction,5,12,1,0,,0.034432652,0,negative,1.52E-05,3.53E-06,0.173082113,4.14E-07,5.32E-07,4.87E-05,0.000459673,0.000520931,5.65E-06,0.825306602,7.58E-07,0.000550165,5.75E-06
2158,relation_extraction5,135,3,Baseline Models,Baseline Models,relation_extraction,5,13,1,0,,0.013686623,0,negative,3.46E-05,1.23E-06,0.024175861,3.74E-07,1.55E-07,2.76E-05,0.000148995,0.000378469,1.23E-05,0.974772646,1.11E-06,0.000438166,8.42E-06
2159,relation_extraction5,136,Experimental Setup,,,relation_extraction,5,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
2160,relation_extraction5,137,We conduct experiments on two relation extraction datasets :,Experimental Setup,Experimental Setup,relation_extraction,5,1,1,0,,0.051755489,0,negative,0.000406882,0.00027782,0.096271027,4.81E-05,0.000151201,0.00582757,0.036868077,0.008644807,1.43E-05,0.820975458,0.001573213,0.028704208,0.000237282
2161,relation_extraction5,138,"( 1 ) TACRED : Introduced in , TACRED contains over 106 k mention pairs drawn from the yearly TAC KBP 4 challenge .",Experimental Setup,Experimental Setup,relation_extraction,5,2,1,0,,0.569029242,1,baselines,0.002100181,0.000189831,0.65070729,0.000450293,0.000451916,0.019964296,0.068943317,0.006174792,2.74E-05,0.224384553,0.000541214,0.025194976,0.000869927
2162,relation_extraction5,139,It represents 41 relation types and a special no relation class when the mention pair does not have a relation between them within these categories .,Experimental Setup,Experimental Setup,relation_extraction,5,3,1,0,,0.376428162,0,negative,0.000213623,0.000148006,0.14893257,1.41E-05,2.77E-05,0.04037364,0.006452505,0.067850401,0.000173291,0.734738979,5.62E-05,0.000923721,9.53E-05
2163,relation_extraction5,140,"Mentions in TACRED are typed , with subjects categorized into person and organization , and objects into 16 fine - grained types ( e.g. , date and location ) .",Experimental Setup,Experimental Setup,relation_extraction,5,4,1,0,,0.14933059,0,negative,0.000258759,0.000125887,0.015280936,0.000675421,0.00052891,0.07294448,0.006744902,0.025604085,5.65E-05,0.876901026,9.69E-05,0.000635864,0.000146401
2164,relation_extraction5,141,We report micro-averaged F 1 scores on this dataset as is conventional .,Experimental Setup,Experimental Setup,relation_extraction,5,5,1,0,,0.010544402,0,negative,8.83E-05,6.50E-06,0.00295795,5.63E-07,2.36E-06,0.001756836,0.000591742,0.003699928,1.67E-06,0.98727324,4.58E-06,0.003613566,2.79E-06
2165,relation_extraction5,142,"For fair comparisons on the TACRED dataset , we follow the evaluation protocol used in by selecting the model with the median dev F 1 from 5 independent runs and reporting its test F 1 .",Experimental Setup,Experimental Setup,relation_extraction,5,6,1,0,,0.026578694,0,negative,3.44E-05,7.71E-06,0.000657863,2.37E-07,4.93E-07,0.004079724,0.000350267,0.014476606,2.40E-06,0.979766035,3.04E-06,0.00061991,1.29E-06
2166,relation_extraction5,143,"We also use the same "" entity mask "" strategy where we replace each subject ( and object similarly ) entity with a special SUBJ - < NER > token .",Experimental Setup,Experimental Setup,relation_extraction,5,7,1,0,,0.275680036,0,baselines,0.001381771,0.000213701,0.819521536,1.09E-05,1.12E-05,0.005788618,0.002403441,0.009253518,0.000283978,0.159124229,3.31E-05,0.001870778,0.000103335
2167,relation_extraction5,144,"For all models , we also adopt the "" multichannel "" strategy by concatenating the input word embeddings with POS and NER embeddings .",Experimental Setup,Experimental Setup,relation_extraction,5,8,1,0,,0.938081728,1,hyperparameters,0.000380717,0.000535066,0.069583764,1.23E-05,4.80E-06,0.12070902,0.006846924,0.627761662,0.000633761,0.173067712,2.10E-05,0.000320637,0.000122648
2168,relation_extraction5,145,"Traditionally , evaluation on SemEval is conducted without entity mentions masked .",Experimental Setup,Experimental Setup,relation_extraction,5,9,1,0,,0.041973674,0,negative,9.09E-05,3.13E-05,0.007144752,7.24E-06,6.49E-06,0.002783375,0.0103206,0.007761678,2.72E-06,0.953405362,0.00594279,0.012399072,0.00010367
2169,relation_extraction5,146,"However , as we will discuss in Section 6.4 , this method encourages models to overfit to these mentions and fails to test their actual ability to generalize .",Experimental Setup,Experimental Setup,relation_extraction,5,10,1,0,,0.014510635,0,negative,0.001882971,2.22E-05,0.00594713,8.97E-06,8.09E-06,0.002511114,0.000786671,0.004273321,8.89E-06,0.97415113,5.33E-05,0.010320212,2.59E-05
2170,relation_extraction5,147,"We therefore report results with two evaluation protocols : ( 1 ) with- mention , where mentions are kept for comparison with previous work ; and ( 2 ) maskmention , where they are masked to test the generalization of our model in a more realistic setting .",Experimental Setup,Experimental Setup,relation_extraction,5,11,1,0,,0.310466067,0,negative,0.000378768,0.000180083,0.040652487,3.32E-06,2.06E-05,0.001932624,0.002102728,0.005134266,2.16E-05,0.942494471,1.27E-05,0.007046825,1.96E-05
2171,relation_extraction5,148,"Due to space limitations , we report model training details in the supplementary material .",Experimental Setup,Experimental Setup,relation_extraction,5,12,1,0,,0.057324185,0,negative,5.91E-05,1.71E-05,0.000275123,7.67E-05,8.74E-06,0.01396464,0.000429977,0.029611383,2.15E-05,0.955236338,5.75E-06,0.000279363,1.43E-05
2172,relation_extraction5,149,Results on the TACRED Dataset,Experimental Setup,,relation_extraction,5,13,1,1,results,0.968652491,1,results,0.000324914,6.80E-06,0.008611277,3.62E-07,6.28E-07,0.000572709,0.013242171,0.002784327,1.01E-06,0.098849732,6.05E-05,0.875493523,5.20E-05
2173,relation_extraction5,150,We present our main results on the TACRED test set in .,Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,14,1,0,,0.786147376,1,results,0.000410474,7.07E-08,4.39E-05,9.39E-08,1.98E-08,8.31E-07,5.49E-05,2.83E-06,8.41E-08,0.310338484,3.30E-07,0.689077988,7.00E-05
2174,relation_extraction5,151,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,15,1,1,results,0.96581498,1,results,0.000180924,3.56E-09,7.96E-06,2.77E-08,2.23E-09,2.20E-07,8.22E-05,6.39E-07,4.93E-09,0.013432522,4.05E-08,0.986193268,0.000102149
2175,relation_extraction5,152,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,16,1,1,results,0.97896026,1,results,0.000169459,2.37E-09,5.96E-06,2.79E-08,1.24E-09,1.70E-07,7.31E-05,4.94E-07,3.79E-09,0.005888359,3.47E-08,0.993730885,0.000131522
2176,relation_extraction5,153,"In addition , we find our model improves upon other dependencybased models in both precision and recall .",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,17,1,1,results,0.976190414,1,results,0.000185065,4.23E-09,3.53E-06,1.37E-07,2.47E-09,5.91E-07,0.000119634,1.68E-06,1.06E-08,0.010144825,8.09E-08,0.989220125,0.000324322
2177,relation_extraction5,154,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,18,1,1,results,0.978282399,1,results,0.001261314,1.21E-08,8.91E-06,3.18E-08,4.14E-09,3.15E-07,8.76E-05,1.96E-06,1.54E-08,0.033286574,5.78E-08,0.965301827,5.14E-05
2178,relation_extraction5,155,We hypothesize that this is because the C - GCN is more robust to parse errors by capturing local word patterns ( see also Section 6.2 ) .,Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,19,1,0,,0.008167057,0,negative,7.81E-05,1.60E-07,0.000114192,2.10E-07,2.14E-08,4.25E-06,9.19E-06,3.55E-05,2.61E-06,0.990682526,3.25E-07,0.008952691,0.000120231
2179,relation_extraction5,156,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,20,1,1,results,0.941182424,1,results,0.000593986,7.72E-09,5.39E-06,6.85E-08,4.41E-09,6.12E-07,8.83E-05,2.36E-06,1.81E-08,0.036888546,4.78E-08,0.96230399,0.000116694
2180,relation_extraction5,157,"To leverage this result , we experiment with a simple interpolation strategy to combine these models .",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,21,1,0,,0.014345016,0,negative,0.000941327,9.14E-07,0.001693731,1.06E-07,5.77E-08,1.63E-06,1.31E-05,1.02E-05,7.62E-06,0.960829326,2.84E-07,0.036466828,3.48E-05
2181,relation_extraction5,158,"Given the output probabilities PG ( r|x ) from a GCN model and PS ( r|x ) from the sequence model for any relation r , we calculate the interpolated probability as",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,22,1,0,,0.001143805,0,negative,3.64E-05,3.22E-08,6.03E-05,1.97E-08,4.52E-09,5.08E-07,1.70E-06,4.04E-06,5.48E-07,0.993391362,4.95E-08,0.0064956,9.46E-06
2182,relation_extraction5,159,where ? ?,Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,23,1,0,,0.00042111,0,negative,1.50E-05,1.89E-08,1.28E-05,8.07E-08,3.52E-09,1.27E-06,1.88E-06,6.44E-06,3.77E-07,0.997398232,6.80E-08,0.002540946,2.29E-05
2183,relation_extraction5,160,"[ 0 , 1 ] is chosen on the dev set and set to 0.6 .",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,24,1,0,,0.694463914,1,negative,0.000368295,2.08E-06,9.46E-05,6.17E-06,1.87E-07,0.000503692,0.001301483,0.017116359,1.15E-05,0.937018093,1.27E-06,0.038425752,0.005150531
2184,relation_extraction5,161,"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .",Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,25,1,1,results,0.902192113,1,results,0.001589307,3.61E-08,7.26E-05,7.80E-08,1.24E-08,6.04E-07,8.92E-05,2.67E-06,1.38E-07,0.077268999,7.08E-08,0.92076098,0.000215331
2185,relation_extraction5,162,An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .,Experimental Setup,Results on the TACRED Dataset,relation_extraction,5,26,1,1,results,0.971568298,1,results,0.008317803,2.82E-08,2.60E-05,2.24E-07,1.87E-08,1.05E-06,0.00013267,4.83E-06,1.29E-07,0.084726223,3.89E-08,0.906524591,0.000266387
2186,relation_extraction5,163,Results on the SemEval Dataset,Experimental Setup,,relation_extraction,5,27,1,1,results,0.977429715,1,results,0.000179015,3.22E-06,0.002328629,2.10E-07,6.12E-07,0.00027084,0.011672002,0.001356458,5.44E-07,0.066053277,8.72E-06,0.918074246,5.22E-05
2187,relation_extraction5,164,"To study the generalizability of our proposed model , we also trained and evaluated our best C - GCN model on the SemEval test set ) .",Experimental Setup,Results on the SemEval Dataset,relation_extraction,5,28,1,0,,0.027706311,0,negative,0.000110087,1.78E-08,3.80E-05,1.63E-08,1.78E-08,6.09E-07,3.02E-05,2.49E-06,2.99E-08,0.931015423,1.12E-08,0.068796954,6.13E-06
2188,relation_extraction5,165,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .",Experimental Setup,Results on the SemEval Dataset,relation_extraction,5,29,1,1,results,0.971562169,1,results,9.15E-05,4.59E-09,2.66E-06,1.58E-08,1.89E-09,2.79E-07,0.000118168,1.56E-06,4.79E-09,0.029091693,2.80E-08,0.970645596,4.85E-05
2189,relation_extraction5,166,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .",Experimental Setup,Results on the SemEval Dataset,relation_extraction,5,30,1,1,results,0.982817361,1,results,0.000382729,3.66E-09,3.69E-06,5.74E-08,2.16E-09,2.80E-07,0.000120877,1.01E-06,6.25E-09,0.014054546,1.64E-08,0.985322273,0.000114509
2190,relation_extraction5,167,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .",Experimental Setup,Results on the SemEval Dataset,relation_extraction,5,31,1,1,results,0.978681215,1,results,0.000149664,2.23E-09,2.00E-06,1.75E-08,1.41E-09,1.61E-07,0.000135758,8.04E-07,2.14E-09,0.011661847,1.06E-08,0.98796842,8.13E-05
2191,relation_extraction5,168,Effect of Path - centric,Experimental Setup,Results on the SemEval Dataset,relation_extraction,5,32,1,0,,0.568086794,1,results,0.005194224,2.58E-08,3.73E-05,8.32E-08,1.52E-08,6.02E-07,0.000136402,2.03E-06,6.42E-08,0.136530241,5.15E-08,0.858018845,8.01E-05
2192,relation_extraction5,169,Pruning,Experimental Setup,,relation_extraction,5,33,1,0,,0.174747288,0,negative,0.000156392,4.48E-05,0.002109081,2.82E-06,6.06E-06,0.01311827,0.001223608,0.105795105,0.000122073,0.876184404,3.17E-06,0.001162464,7.17E-05
2193,relation_extraction5,170,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .",Experimental Setup,Pruning,relation_extraction,5,34,1,1,results,0.881919986,1,negative,0.001492228,0.000200689,0.001264273,5.08E-07,1.67E-05,0.008480937,0.001774981,0.006424298,2.65E-05,0.976097365,1.43E-06,0.004210247,9.84E-06
2194,relation_extraction5,171,"We experimented with K ? { 0 , 1 , 2 , ?} on the TACRED dev set , and also include results when the full tree is used .",Experimental Setup,Pruning,relation_extraction,5,35,1,0,,0.191519764,0,negative,3.89E-05,3.78E-05,9.01E-05,1.15E-06,1.38E-05,0.182713285,0.002116171,0.075619044,1.01E-05,0.739183005,6.22E-07,0.000162463,1.36E-05
2195,relation_extraction5,172,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .",Experimental Setup,Pruning,relation_extraction,5,36,1,1,results,0.978313715,1,results,0.048147031,2.96E-05,0.000791112,9.77E-06,8.65E-05,0.006523324,0.019871377,0.003604688,4.43E-06,0.234621199,3.67E-06,0.68598951,0.00031776
2196,relation_extraction5,173,This confirms our hypothesis in Section 3 that incorporating off - path information is crucial to relation extraction .,Experimental Setup,Pruning,relation_extraction,5,37,1,0,,0.633677102,1,negative,0.012728867,2.17E-05,0.000402797,1.38E-06,4.38E-05,0.002373038,0.002307565,0.000905282,6.25E-06,0.928193244,2.11E-06,0.052986827,2.71E-05
2197,relation_extraction5,174,Miwa and Bansal ( 2016 ) reported that a Tree - LSTM achieves similar performance when the dependency path and the LCA subtree are used respectively .,Experimental Setup,Pruning,relation_extraction,5,38,1,0,,0.361822906,0,negative,0.000157326,1.22E-05,0.000666659,1.02E-06,4.94E-06,0.011175075,0.004757683,0.00583699,8.03E-06,0.970155017,4.02E-05,0.007107576,7.73E-05
2198,relation_extraction5,175,"Our experiments confirm this , and further show that the result can be improved by path - centric pruning with K = 1 .",Experimental Setup,Pruning,relation_extraction,5,39,1,0,,0.620502962,1,negative,0.007757749,4.91E-05,0.000513678,1.58E-06,2.87E-05,0.006263239,0.002663808,0.002798618,1.47E-05,0.945757323,2.20E-06,0.034109572,3.98E-05
2199,relation_extraction5,176,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .",Experimental Setup,Pruning,relation_extraction,5,40,1,1,results,0.98249801,1,negative,0.029732487,0.000112757,0.000397239,5.28E-05,0.000295894,0.0888389,0.027304749,0.055114988,1.78E-05,0.650044843,7.38E-06,0.147282582,0.000797637
2200,relation_extraction5,177,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .",Experimental Setup,Pruning,relation_extraction,5,41,1,1,results,0.963550225,1,results,0.213165121,9.14E-05,0.001335995,3.80E-05,0.000406601,0.018375359,0.028759796,0.007390323,1.16E-05,0.336128904,3.87E-06,0.393637091,0.000655999
2201,relation_extraction5,178,Analysis & Discussion,Experimental Setup,,relation_extraction,5,42,1,0,,0.016607493,0,negative,0.000765769,1.46E-05,0.005364936,1.70E-05,3.03E-05,0.004946773,0.002746706,0.010149024,3.66E-05,0.96683395,1.51E-06,0.008871006,0.00022185
2202,relation_extraction5,179,Ablation Study,,,relation_extraction,5,0,1,0,,0.006818876,0,negative,0.046739945,0.000279772,0.002808551,0.000142874,8.76E-05,0.000276199,0.001210224,0.00046093,0.000147782,0.934438164,0.002965146,0.010376843,6.60E-05
2203,relation_extraction5,180,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .",Ablation Study,Ablation Study,relation_extraction,5,1,1,0,,0.871175267,1,ablation-analysis,0.95486633,7.30E-06,0.000881381,5.29E-07,2.19E-05,8.78E-06,2.14E-05,4.37E-07,2.67E-05,0.044124955,5.11E-06,5.67E-06,2.95E-05
2204,relation_extraction5,181,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,Ablation Study,Ablation Study,relation_extraction,5,2,1,1,ablation-analysis,0.975201185,1,ablation-analysis,0.994960779,8.85E-07,2.53E-05,3.23E-07,1.81E-06,3.45E-06,1.05E-05,4.73E-07,5.51E-06,0.004962986,3.78E-07,2.40E-06,2.52E-05
2205,relation_extraction5,182,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .",Ablation Study,Ablation Study,relation_extraction,5,3,1,1,ablation-analysis,0.983884406,1,ablation-analysis,0.999246495,6.79E-08,4.92E-06,1.15E-07,4.73E-07,3.92E-07,1.67E-06,2.04E-08,2.32E-07,0.000739661,7.95E-08,6.27E-07,5.25E-06
2206,relation_extraction5,183,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .",Ablation Study,Ablation Study,relation_extraction,5,4,1,1,ablation-analysis,0.983851908,1,ablation-analysis,0.999558593,8.63E-08,3.49E-06,1.01E-07,3.77E-07,4.33E-07,2.68E-06,3.17E-08,2.67E-07,0.000425244,5.78E-08,6.42E-07,7.99E-06
2207,relation_extraction5,184,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .",Ablation Study,Ablation Study,relation_extraction,5,5,1,1,ablation-analysis,0.967427896,1,ablation-analysis,0.999177385,9.12E-08,5.67E-06,1.01E-07,6.46E-07,5.92E-07,4.55E-06,3.44E-08,2.48E-07,0.000796442,8.12E-08,1.29E-06,1.29E-05
2208,relation_extraction5,185,Complementary Strengths of GCNs and PA - LSTMs,Ablation Study,Ablation Study,relation_extraction,5,6,1,0,,0.758163748,1,ablation-analysis,0.840295963,2.27E-05,0.005849487,1.83E-07,8.98E-07,1.57E-05,0.000134826,1.72E-06,0.000925902,0.151556563,0.000785156,4.17E-05,0.000369253
2209,relation_extraction5,186,"To understand what the GCN models are capturing and how they differ from a sequence model such as the PA - LSTM , we compared their performance :",Ablation Study,Ablation Study,relation_extraction,5,7,1,0,,0.007600295,0,negative,0.084604149,7.98E-06,0.002939876,1.99E-07,1.02E-05,2.23E-05,3.75E-05,1.36E-06,3.62E-05,0.912292949,1.17E-05,6.27E-06,2.94E-05
2210,relation_extraction5,187,The three dependency edges that contribute the most to the classification of different relations in the TACRED dev set .,Ablation Study,Ablation Study,relation_extraction,5,8,1,0,,0.013476955,0,negative,0.405329205,4.54E-06,0.001637757,6.41E-07,8.36E-06,4.37E-05,4.39E-05,3.78E-06,0.000112593,0.59267026,6.20E-06,7.43E-06,0.000131602
2211,relation_extraction5,188,"For clarity , we removed edges which 1 ) connect to common punctuation ( i.e. , commas , periods , and quotation marks ) , 2 ) connect to common prepositions ( i.e. , of , to , by ) , and 3 ) connect between tokens within the same entity .",Ablation Study,Ablation Study,relation_extraction,5,9,1,0,,0.020780801,0,negative,0.08274441,3.56E-05,0.002342321,1.66E-06,3.53E-05,0.000261335,5.23E-05,2.49E-05,0.001261307,0.913021209,1.17E-05,1.31E-06,0.000206608
2212,relation_extraction5,189,"We use PER , ORG for entity types of PERSON , ORGANIZATION .",Ablation Study,Ablation Study,relation_extraction,5,10,1,0,,0.017899809,0,negative,0.040757049,4.27E-05,0.003329322,6.44E-05,0.001362918,0.002459961,0.000278608,0.000101432,0.000259613,0.946943337,9.67E-06,3.46E-06,0.004387537
2213,relation_extraction5,190,"We use S - and O - to denote subject and object entities , respectively .",Ablation Study,Ablation Study,relation_extraction,5,11,1,0,,0.025237216,0,negative,0.01344757,8.94E-05,0.00111922,4.99E-06,4.47E-05,0.00033741,6.80E-05,7.32E-05,0.001892636,0.982018212,3.71E-05,1.14E-06,0.000866302
2214,relation_extraction5,191,We also include edges for more relations in the supplementary material .,Ablation Study,Ablation Study,relation_extraction,5,12,1,0,,0.075586898,0,negative,0.05929706,2.39E-05,0.001571616,4.68E-06,4.42E-05,0.000315506,6.61E-05,4.41E-05,0.000767723,0.937482153,4.06E-06,1.39E-06,0.000377514
2215,relation_extraction5,192,over examples in the TACRED dev set .,Ablation Study,Ablation Study,relation_extraction,5,13,1,0,,0.008669029,0,negative,0.092809847,3.51E-06,0.000288319,1.67E-07,8.98E-06,4.06E-05,6.15E-05,6.82E-06,1.70E-05,0.90660777,3.68E-06,4.79E-06,0.000147105
2216,relation_extraction5,193,"Specifically , for each model , we trained it for 5 independent runs with different seeds , and for each example we evaluated the model 's accuracy over these 5 runs .",Ablation Study,Ablation Study,relation_extraction,5,14,1,0,,0.025777354,0,negative,0.177109284,8.12E-05,0.000662393,8.34E-06,0.000519277,0.000419685,0.000409923,4.88E-05,0.00014611,0.81872072,4.29E-06,5.45E-06,0.001864547
2217,relation_extraction5,194,"For instance , if a model correctly classifies an example for 3 out of 5 times , it achieves an accuracy of 60 % on this example .",Ablation Study,Ablation Study,relation_extraction,5,15,1,0,,0.011648247,0,ablation-analysis,0.810328059,1.10E-06,8.71E-05,3.32E-07,2.26E-05,7.43E-06,8.83E-05,4.96E-07,1.93E-06,0.189240484,3.28E-06,5.76E-05,0.000161348
2218,relation_extraction5,195,"We observe that on 847 ( 3.7 % ) dev examples , our C - GCN model achieves an accuracy at least 60 % higher than that of the PA - LSTM , while on 629 ( 2.8 % ) examples the PA - LSTM achieves 60 % higher .",Ablation Study,Ablation Study,relation_extraction,5,16,1,0,,0.794978415,1,ablation-analysis,0.986492198,1.01E-06,4.40E-05,6.33E-07,1.70E-05,4.06E-06,0.000195519,3.15E-07,8.00E-07,0.012634441,1.27E-06,9.30E-05,0.000515808
2219,relation_extraction5,196,This complementary performance explains the gain we see in when the two models are combined .,Ablation Study,Ablation Study,relation_extraction,5,17,1,0,,0.00672619,0,ablation-analysis,0.597125068,4.46E-06,0.000626269,5.45E-07,7.23E-06,1.83E-05,3.66E-05,1.54E-06,0.000166356,0.401862116,4.64E-06,9.70E-06,0.000137182
2220,relation_extraction5,197,"We further show that this difference is due to each model 's competitive advantage : dependency - based models are better at handling sentences with entities farther apart , while sequence models can better leverage local word patterns regardless of parsing quality ( see also .",Ablation Study,Ablation Study,relation_extraction,5,18,1,0,,0.005356092,0,ablation-analysis,0.94443896,2.59E-06,0.000181718,7.81E-08,2.28E-06,2.15E-06,2.36E-05,2.27E-07,1.10E-05,0.05530741,7.99E-07,1.37E-05,1.54E-05
2221,relation_extraction5,198,We include further analysis in the supplementary material .,Ablation Study,Ablation Study,relation_extraction,5,19,1,0,,0.002726653,0,negative,0.030547847,1.57E-06,6.86E-05,7.27E-06,6.59E-05,5.20E-05,7.95E-06,2.72E-06,1.56E-05,0.969132197,6.20E-07,9.45E-07,9.69E-05
2222,relation_extraction5,199,Understanding Model Behavior,,,relation_extraction,5,0,1,0,,0.001839369,0,negative,6.66E-05,5.99E-05,9.85E-06,6.53E-06,2.09E-06,5.05E-05,0.000385931,0.000500325,2.63E-05,0.791846111,0.205772931,0.001240229,3.27E-05
2223,relation_extraction5,200,"To gain more insights into the C - GCN model 's behavior , we visualized the partial dependency tree it is processing and how much each token 's final representation contributed to h sent ( ) .",Understanding Model Behavior,Understanding Model Behavior,relation_extraction,5,1,1,0,,0.000312246,0,negative,0.00037115,2.03E-06,6.36E-06,1.28E-08,1.92E-07,1.61E-05,3.13E-05,1.22E-05,3.84E-06,0.999453656,1.38E-06,0.000101758,4.49E-08
2224,relation_extraction5,201,"We find that the model often focuses on the dependency path , but sometimes also incorporates offpath information to help reinforce its prediction .",Understanding Model Behavior,Understanding Model Behavior,relation_extraction,5,2,1,0,,0.041800243,0,negative,0.009299,1.64E-05,1.06E-05,3.83E-07,1.70E-06,8.83E-05,0.000269286,0.000115656,1.01E-05,0.987724882,1.50E-05,0.002447436,1.21E-06
2225,relation_extraction5,202,"The model also learns to ignore determiners ( e.g. , "" the "" ) as they rarely affect relation prediction .",Understanding Model Behavior,Understanding Model Behavior,relation_extraction,5,3,1,0,,0.005098224,0,negative,0.00084211,0.000107127,0.000701794,1.66E-07,9.63E-07,8.56E-05,4.49E-05,0.000115637,0.003363589,0.994679768,1.73E-05,3.99E-05,1.03E-06
2226,relation_extraction5,203,"To further understand what dependency edges contribute most to the classification of different relations , we scored each dependency edge by summing up the number of dimensions each of its connected nodes contributed to h sent .",Understanding Model Behavior,Understanding Model Behavior,relation_extraction,5,4,1,0,,0.000763753,0,negative,0.000242064,2.85E-06,9.12E-06,7.07E-09,2.59E-07,1.34E-05,1.89E-05,1.18E-05,5.40E-06,0.999645246,3.69E-07,5.06E-05,2.51E-08
2227,relation_extraction5,204,We present the top scoring edges in .,Understanding Model Behavior,Understanding Model Behavior,relation_extraction,5,5,1,0,,0.000475154,0,negative,5.17E-05,1.83E-06,2.51E-06,7.46E-08,4.95E-07,2.65E-05,7.11E-06,2.23E-05,1.23E-05,0.999866711,3.46E-07,8.04E-06,6.58E-08
2228,relation_extraction5,205,"As can be seen in the table , most of these edges are associated with indicative nouns or verbs of each relation .",Understanding Model Behavior,Understanding Model Behavior,relation_extraction,5,6,1,0,,0.002265736,0,negative,0.000757298,9.72E-07,3.70E-06,1.58E-06,1.47E-05,0.000126229,6.35E-05,1.89E-05,1.96E-06,0.998959242,5.31E-07,5.10E-05,4.05E-07
2229,relation_extraction5,206,5,Understanding Model Behavior,Understanding Model Behavior,relation_extraction,5,7,1,0,,1.21E-05,0,negative,3.73E-05,2.26E-07,3.71E-07,8.47E-08,1.58E-07,2.22E-05,7.79E-06,1.24E-05,1.85E-06,0.99990861,4.73E-07,8.48E-06,7.02E-08
2230,relation_extraction5,207,Entity Bias in the SemEval Dataset,Understanding Model Behavior,,relation_extraction,5,8,1,0,,0.365116057,0,negative,0.002614224,1.85E-05,0.000205143,4.52E-07,8.84E-07,8.21E-05,0.027650559,9.72E-05,8.65E-06,0.883878574,0.009384071,0.075978043,8.17E-05
2231,relation_extraction5,208,"In our study , we observed a high correlation between the entity mentions in a sentence and its relation label in the SemEval dataset .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,9,1,0,,0.002807365,0,negative,0.002353064,4.82E-06,8.57E-05,9.73E-07,2.26E-06,1.14E-05,0.001077965,2.58E-05,5.16E-06,0.754346581,1.79E-05,0.242020988,4.75E-05
2232,relation_extraction5,209,"We experimented with PA - LSTM models to analyze this dependency tree corresponding to K = 1 in path-centric pruning is shown , and the shortest dependency path is thickened .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,10,1,0,,3.49E-05,0,negative,0.000158269,6.03E-07,7.69E-05,5.25E-08,1.78E-07,5.90E-06,0.000149898,1.09E-05,3.61E-06,0.99728231,7.56E-07,0.002309066,1.52E-06
2233,relation_extraction5,210,We omit edges to punctuation for clarity .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,11,1,0,,0.000167432,0,negative,1.56E-05,9.03E-07,4.71E-06,3.21E-07,7.16E-08,1.35E-05,3.57E-05,5.34E-05,2.17E-05,0.999638006,2.80E-06,0.000210868,2.43E-06
2234,relation_extraction5,211,"The first example shows that the C - GCN is effective at leveraging long - range dependencies while reducing noise with the help of pruning ( while the PA - LSTM predicts no relation twice , org : alternate names twice , and org : parents once in this case ) .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,12,1,0,,6.55E-05,0,negative,0.003706718,4.41E-07,3.71E-05,5.27E-08,1.28E-07,1.95E-06,0.000549674,4.96E-06,8.44E-07,0.806422402,1.25E-06,0.18927041,4.05E-06
2235,relation_extraction5,212,"The second example shows that the PA - LSTM is better at leveraging the proximity of the word "" migrated "" regardless of attachment errors in the parse ( while the C - GCN is misled to predict per :country of birth three times , and no relation twice ) .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,13,1,0,,8.51E-05,0,negative,0.002738777,1.99E-07,2.02E-05,5.19E-08,1.18E-07,1.79E-06,0.000819979,4.24E-06,2.76E-07,0.551624362,1.19E-06,0.444782053,6.76E-06
2236,relation_extraction5,213,phenomenon .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,14,1,0,,1.45E-05,0,negative,2.41E-05,3.93E-07,7.22E-06,2.23E-07,8.04E-08,4.26E-06,1.95E-05,1.57E-05,6.51E-06,0.999404828,2.79E-06,0.000509815,4.54E-06
2237,relation_extraction5,214,"We started by simplifying every sentence in the SemEval training and dev sets to "" subject and object "" , where subject and object are the actual entities in the sentence .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,15,1,0,,9.61E-05,0,negative,0.000109589,3.98E-06,2.91E-05,6.07E-07,2.60E-06,2.16E-05,0.000144492,5.15E-05,9.71E-06,0.998854991,1.13E-06,0.000763552,7.22E-06
2238,relation_extraction5,215,"Surprisingly , a trained PA - LSTM model on this data is able to achieve 65.1 F 1 on the dev set if Glo Ve is used to initialize word vectors , and 47.9 dev F 1 even without GloVe initialization .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,16,1,0,,0.074297394,0,results,0.003492799,3.31E-07,1.08E-05,1.58E-07,1.60E-07,4.01E-06,0.002586708,1.65E-05,3.71E-07,0.224191148,9.90E-07,0.769663018,3.31E-05
2239,relation_extraction5,216,"To further evaluate the model in a more realistic setting , we trained one model with the original SemEval training set ( unmasked ) and one with mentions masked in the training set , following what we have done for TACRED ( masked ) .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,17,1,0,,0.000267219,0,negative,0.00034359,2.89E-06,0.00017161,1.63E-07,9.71E-07,1.01E-05,0.000498184,2.66E-05,8.14E-06,0.995531738,2.48E-07,0.00340135,4.39E-06
2240,relation_extraction5,217,"While the unmasked model achieves a 83.6 F 1 on the original SemEval dev set , F 1 drops drastically to 62.4 if we replace dev set entity mentions with a special < UNK > token to simulate the presence of unseen entities .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,18,1,0,,0.318274581,0,results,0.023640006,6.28E-07,1.74E-05,5.04E-07,4.01E-07,5.49E-06,0.003508885,2.33E-05,6.36E-07,0.115953199,1.40E-06,0.856745474,0.000102746
2241,relation_extraction5,218,"In contrast , the masked model is unaffected by unseen entity mentions and achieves a stable dev F 1 of 74.7 .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,19,1,0,,0.397512169,0,results,0.017403355,3.00E-07,1.54E-05,1.55E-07,1.62E-07,2.26E-06,0.002579293,1.04E-05,3.28E-07,0.096696489,4.82E-07,0.883256048,3.53E-05
2242,relation_extraction5,219,This suggests that models trained without entities masked generalize poorly to new examples with unseen entities .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,20,1,0,,0.000246016,0,negative,0.000111891,1.73E-07,4.99E-06,2.51E-08,5.52E-08,9.07E-07,1.55E-05,3.14E-06,1.65E-06,0.998575344,2.57E-07,0.001285508,5.62E-07
2243,relation_extraction5,220,Our findings call for more careful evaluation that takes dataset biases into account in future relation extraction studies .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation_extraction,5,21,1,0,,0.005091975,0,negative,0.001244659,1.41E-06,5.30E-06,4.93E-07,5.27E-07,6.22E-06,0.000147054,2.54E-05,6.25E-06,0.983145443,1.76E-06,0.015405433,1.00E-05
2244,relation_extraction5,221,Conclusion,,,relation_extraction,5,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
2245,relation_extraction5,225,A Experimental Details,,,relation_extraction,5,0,1,0,,0.000282337,0,negative,2.34E-05,0.000174683,5.43E-06,1.49E-05,1.02E-05,0.000110654,6.45E-05,0.001024105,5.58E-05,0.997656658,0.000784076,7.14E-05,4.34E-06
2246,relation_extraction5,226,A.1 Hyperparameters TACRED,,,relation_extraction,5,0,1,0,,0.405203042,0,negative,0.000224854,0.00025361,3.76E-05,1.18E-05,2.52E-06,0.000251396,0.000239693,0.003483471,0.00024852,0.985434215,0.008701084,0.001075202,3.60E-05
2247,relation_extraction5,227,We set LSTM hidden size to 200 in all neural models .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,1,1,0,,0.068494393,0,hyperparameters,0.000157064,0.002628041,0.000128612,5.76E-05,1.56E-05,0.030671723,0.001209104,0.629350986,0.004145359,0.329185491,0.002265343,8.41E-05,0.000100937
2248,relation_extraction5,228,We also use hidden size 200 for the output feedforward layers in the GCN model .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,2,1,0,,0.037270466,0,hyperparameters,0.000210049,0.004195617,0.000184857,5.77E-05,2.44E-05,0.029184736,0.00108475,0.58404516,0.006480287,0.372871914,0.00149049,7.82E-05,9.18E-05
2249,relation_extraction5,229,We use 2 GCN layers and 2 feedforward ( FFNN ) layers in our experiments .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,3,1,0,,0.063027215,0,negative,0.000694738,0.007056397,0.000696867,0.000599927,0.000161587,0.059471269,0.003917799,0.416344146,0.007610172,0.500996999,0.001851265,0.000236928,0.000361906
2250,relation_extraction5,230,We employ the ReLU function for all nonlinearities in the GCN layers and the standard max pooling operations in all pooling layers .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,4,1,0,,0.031875608,0,negative,0.000678602,0.013749879,0.000557451,0.000194853,8.81E-05,0.027311862,0.001277073,0.379415815,0.018188207,0.557687178,0.000631543,0.000113707,0.000105732
2251,relation_extraction5,231,"For the Tree - LSTM model , we find a 2 - layer architecture works substantially better than the vanilla 1 - layer model , and use it in all our experiments .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,5,1,0,,0.076904324,0,negative,0.003015393,0.000264299,7.31E-05,8.73E-05,2.36E-05,0.005087576,0.002043402,0.034391235,0.000140391,0.936196698,0.002281827,0.016265699,0.000129497
2252,relation_extraction5,232,"For both the Tree - LSTM and our models , we apply path - centric pruning with K = 1 , as we find that this generates best results for all models ( also see ) .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,6,1,0,,0.053831639,0,negative,0.002909085,0.006782389,0.000465444,3.17E-05,5.42E-05,0.011088652,0.00106739,0.139420126,0.003455995,0.832878341,0.000531295,0.001270198,4.52E-05
2253,relation_extraction5,233,"We use the pretrained 300 - dimensional Glo Ve vectors to initialize word embeddings , and we use embedding size of 30 for all other embeddings ( i.e. , POS , NER ) .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,7,1,0,,0.120128293,0,hyperparameters,0.000101792,0.00325748,0.000110003,3.36E-05,1.80E-05,0.030746431,0.00106618,0.580613334,0.003252464,0.380030098,0.000620141,7.91E-05,7.14E-05
2254,relation_extraction5,234,"We use the dependency parse trees , POS and NER sequences as included in the original release of the dataset , which was generated with Stanford CoreNLP .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,8,1,0,,6.32E-05,0,negative,3.27E-05,8.46E-05,0.000113537,2.14E-05,0.000720714,0.000309706,5.12E-05,0.000284643,2.98E-05,0.998311202,1.25E-05,2.64E-05,1.56E-06
2255,relation_extraction5,235,For regularization we apply dropout with p = 0.5 to all LSTM layers and all but the last GCN layers .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation_extraction,5,9,1,0,,0.093940769,0,hyperparameters,0.000641184,0.006408373,0.000228947,0.00050352,0.000149543,0.046743297,0.002106763,0.621653081,0.004364802,0.316450763,0.00033059,0.000171104,0.000248034
2256,relation_extraction5,236,Sem Eval,A.1 Hyperparameters TACRED,,relation_extraction,5,10,1,0,,6.13E-05,0,negative,1.96E-05,1.63E-05,4.27E-06,2.66E-07,5.81E-07,1.03E-05,6.44E-06,0.000131187,5.59E-05,0.999435814,0.000270276,4.85E-05,5.89E-07
2257,relation_extraction5,237,We use LSTM hidden size of 100 and use 1 GCN layer for the SemEval dataset .,A.1 Hyperparameters TACRED,Sem Eval,relation_extraction,5,11,1,0,,0.047200538,0,negative,0.000776054,0.000345249,0.000423366,8.12E-05,1.74E-05,0.016445124,0.006174502,0.093109722,0.001507586,0.878557734,0.000254589,0.000425557,0.001881928
2258,relation_extraction5,238,"We preprocess the dataset with Stanford CoreNLP to generate the dependency parse trees , POS and NER annotations .",A.1 Hyperparameters TACRED,Sem Eval,relation_extraction,5,12,1,0,,0.002582209,0,negative,8.13E-05,6.55E-06,7.73E-05,1.97E-05,0.000212411,0.00010224,3.00E-05,3.91E-05,1.27E-05,0.999330142,1.83E-06,6.67E-05,2.00E-05
2259,relation_extraction5,239,All other hyperparameters are set to be the same .,A.1 Hyperparameters TACRED,Sem Eval,relation_extraction,5,13,1,0,,0.000740078,0,negative,0.000201073,0.000132256,4.29E-05,4.63E-06,2.21E-06,0.001236381,0.000387558,0.023682491,0.000790383,0.973280348,4.81E-05,0.000108825,8.29E-05
2260,relation_extraction5,240,"For both datasets , we work with the Universal Dependencies v 1 formalism .",A.1 Hyperparameters TACRED,Sem Eval,relation_extraction,5,14,1,0,,0.000766468,0,negative,0.000291804,0.000151582,0.004712736,1.24E-06,4.22E-06,8.18E-05,0.000113901,0.000240938,0.000871929,0.99326203,1.68E-05,0.000234409,1.66E-05
2261,relation_extraction5,241,A.2 Training,,,relation_extraction,5,0,1,0,,0.071310185,0,negative,0.000955927,0.000573902,0.000435276,1.14E-06,3.40E-06,8.15E-05,0.000103404,0.000884336,0.000155452,0.990752821,0.001941914,0.004103027,7.89E-06
2262,relation_extraction5,242,For training we use Stochastic Gradient Descent with an initial learning rate of 1.0 .,A.2 Training,A.2 Training,relation_extraction,5,1,1,0,,0.169950966,0,negative,0.004455067,0.001765909,0.000504801,0.000399884,6.53E-05,0.033445998,0.001916417,0.391397429,0.002889467,0.556697091,0.000436719,0.000476627,0.005549321
2263,relation_extraction5,243,We use a cutoff of 5 for gradient clipping .,A.2 Training,A.2 Training,relation_extraction,5,2,1,0,,0.004426036,0,negative,0.008573933,0.00215659,0.000473687,0.000280264,7.28E-05,0.015280379,0.001055079,0.34466366,0.003898991,0.618500352,0.000337908,0.000758679,0.003947705
2264,relation_extraction5,244,"For GCN models , we train every model for 100 epochs on the TAC - RED dataset , and from epoch 5 we start to anneal the learning rate by a factor of 0.9 every time the F 1 score on the dev set does not increase after an epoch .",A.2 Training,A.2 Training,relation_extraction,5,3,1,0,,0.006348632,0,negative,0.020552277,0.001597918,0.000547251,0.000168306,0.00011061,0.006236639,0.001098118,0.061877301,0.001645496,0.901556247,0.000171629,0.001695443,0.002742766
2265,relation_extraction5,245,For Tree - LSTM models we find 30 total epochs to be enough .,A.2 Training,A.2 Training,relation_extraction,5,4,1,0,,0.018525474,0,negative,0.013356551,0.000479688,0.000183182,0.000113473,6.33E-05,0.004713737,0.000836619,0.036999092,0.000777459,0.935029289,0.000316795,0.002185167,0.004945639
2266,relation_extraction5,246,"Due to the small size of the SemEval dataset , we train all models for 150 epochs , and use an initial learning rate of 0.5 with a decay rate of 0.95 .",A.2 Training,A.2 Training,relation_extraction,5,5,1,0,,0.046622957,0,negative,0.01153117,0.002118801,0.000460319,0.00051307,0.000151113,0.018869559,0.00189057,0.221198002,0.00245389,0.730582154,0.000295145,0.000973782,0.008962423
2267,relation_extraction5,247,"In our experiments we found that the output vector h sent tends to have large magnitude , and : Aggregated 5 - run difference compared to PA - LSTM on the TACRED dev set .",A.2 Training,A.2 Training,relation_extraction,5,6,1,0,,9.31E-05,0,negative,0.173188682,1.82E-05,0.000130666,3.71E-06,8.70E-06,6.27E-05,0.000112271,0.000218267,1.76E-05,0.777740521,3.75E-05,0.048354913,0.000106295
2268,relation_extraction5,248,"For each example , if X out of 5 GCN models predicted its label correctly and Y PA - LSTM models did , it is aggregated in the bar labeled X ?",A.2 Training,A.2 Training,relation_extraction,5,7,1,0,,2.03E-06,0,negative,3.84E-05,1.68E-06,7.73E-06,2.08E-07,4.41E-07,5.17E-06,8.28E-07,3.32E-05,5.49E-06,0.999861384,3.60E-06,3.83E-05,3.57E-06
2269,relation_extraction5,249,Y .,A.2 Training,,relation_extraction,5,8,1,0,,2.19E-06,0,negative,0.000363581,1.43E-06,6.53E-06,8.73E-08,1.05E-07,2.26E-06,7.54E-07,2.16E-05,1.36E-05,0.999498512,4.70E-06,8.37E-05,3.13E-06
2270,relation_extraction5,250,""" 0 "" is omitted due to redundancy .",A.2 Training,Y .,relation_extraction,5,9,1,0,,3.29E-05,0,negative,0.000116392,1.01E-05,3.69E-05,1.36E-06,9.82E-07,6.04E-05,3.13E-06,0.00042477,9.71E-05,0.999187266,6.20E-06,2.97E-05,2.57E-05
2271,relation_extraction5,251,therefore adding the following regularization term to the cross entropy loss of each example improves the results :,A.2 Training,Y .,relation_extraction,5,10,1,0,,0.000761013,0,negative,0.037784573,6.48E-05,0.000956848,7.57E-07,2.99E-06,2.27E-05,9.83E-06,0.000186651,0.000232728,0.958570036,7.96E-06,0.002143003,1.71E-05
2272,relation_extraction5,252,"Here , reg functions as an l 2 regularization on the learned sentence representations .",A.2 Training,Y .,relation_extraction,5,11,1,0,,0.000279787,0,negative,0.001806021,0.00023125,0.002591216,2.44E-06,5.38E-06,5.09E-05,5.69E-06,0.000897593,0.001714939,0.99257828,6.25E-06,7.12E-05,3.88E-05
2273,relation_extraction5,253,?,A.2 Training,Y .,relation_extraction,5,12,1,0,,6.91E-07,0,negative,5.38E-05,7.03E-07,7.66E-06,1.03E-07,1.36E-07,5.86E-06,5.45E-07,2.87E-05,6.97E-06,0.999880194,1.22E-06,1.13E-05,2.81E-06
2274,relation_extraction5,254,controls the regularization strength and we set ? = 0.003 .,A.2 Training,Y .,relation_extraction,5,13,1,0,,0.006074558,0,negative,0.004981514,0.000539245,0.000152054,8.28E-05,4.35E-05,0.006838026,0.000383066,0.172038675,0.000609836,0.811915518,3.61E-05,0.000415865,0.001963773
2275,relation_extraction5,255,We empirically found this to be more effective than applying l 2 regularization on the convolutional weights .,A.2 Training,Y .,relation_extraction,5,14,1,0,,1.58E-05,0,negative,0.004678828,3.96E-05,4.73E-05,9.98E-06,1.11E-05,0.00031683,2.16E-05,0.002040871,5.37E-05,0.992237244,5.07E-06,0.000399334,0.000138696
2276,relation_extraction5,256,B Comparing GCN models and PA - LSTM on TACRED,A.2 Training,Y .,relation_extraction,5,15,1,0,,0.00267322,0,negative,0.009463186,3.08E-05,0.003398274,3.40E-07,1.28E-06,2.11E-05,0.000285944,0.000117699,1.90E-05,0.808151314,0.000194877,0.178088223,0.00022803
2277,relation_extraction5,257,We compared the performance of both GCN models with the PA - LSTM on the TACRED dev set .,A.2 Training,Y .,relation_extraction,5,16,1,0,,0.001602409,0,negative,0.000510558,7.19E-06,0.000174771,4.64E-08,7.49E-07,8.99E-06,8.43E-06,4.25E-05,4.95E-06,0.998157304,1.47E-06,0.001079034,4.00E-06
2278,relation_extraction5,258,"To minimize randomness that is not inherent to these models , we accumulate statistics over 5 independent runs of each model , and report them in .",A.2 Training,Y .,relation_extraction,5,17,1,0,,1.45E-05,0,negative,0.000797764,1.52E-05,6.34E-05,7.81E-07,1.54E-05,2.75E-05,5.91E-06,0.000151563,1.22E-05,0.998765598,3.16E-07,0.000134955,9.49E-06
2279,relation_extraction5,259,"As is shown in the figure , both GCN models capture very different examples from the PA - LSTM model .",A.2 Training,Y .,relation_extraction,5,18,1,0,,0.002774206,0,negative,0.017799765,3.38E-06,5.82E-05,1.84E-07,1.45E-06,7.68E-06,3.28E-05,4.68E-05,2.45E-06,0.947217317,3.99E-06,0.03479916,2.68E-05
2280,relation_extraction5,260,"In the entire dev set of 22,631 examples , 1,450 had at least 3 more GCN models predicting the label correctly compared to the PA - LSTM , and 1,550 saw an improvement from using the PA - LSTM .",A.2 Training,Y .,relation_extraction,5,19,1,0,,0.001050069,0,negative,0.115293058,2.16E-05,0.000673194,1.39E-05,0.000233475,6.77E-05,0.00010387,8.81E-05,1.66E-05,0.858181202,4.08E-06,0.024821424,0.0004817
2281,relation_extraction5,261,"The C - GCN , on the other hand , outperformed the PA - LSTM by at least 3 models on a total of 847 examples , and lost by a margin of at least 3 on another 629 examples , as reported in the main text .",A.2 Training,Y .,relation_extraction,5,20,1,0,,0.040753619,0,results,0.180511697,1.84E-05,0.000573956,1.70E-05,5.38E-05,0.000112629,0.001324841,0.00024213,7.43E-06,0.399381588,1.87E-05,0.413852898,0.003884951
2282,relation_extraction5,262,This smaller difference is also reflected in the diminished gain from ensembling with the PA - LSTM shown in .,A.2 Training,Y .,relation_extraction,5,21,1,0,,4.43E-06,0,negative,0.047777822,4.41E-06,0.000326457,3.42E-07,3.52E-06,4.95E-06,6.40E-06,1.88E-05,1.33E-05,0.949103197,6.76E-07,0.002731245,8.90E-06
2283,relation_extraction5,263,We hypoth -,A.2 Training,Y .,relation_extraction,5,22,1,0,,5.89E-07,0,negative,0.000111715,1.57E-06,1.04E-05,1.56E-07,4.62E-07,3.94E-06,4.87E-07,3.33E-05,1.46E-05,0.999800071,4.00E-07,1.70E-05,5.88E-06
2284,natural_language_inference33,1,title,,,natural_language_inference,33,1,1,0,,0.000701755,0,negative,8.67E-05,0.000310204,3.41E-06,0.00010693,6.41E-06,0.000664696,5.35E-05,0.005762623,0.000365664,0.991721448,0.000883565,2.01E-05,1.47E-05
2285,natural_language_inference33,2,Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS VIA LARGE SCALE MULTI - TASK LEARNING,,,natural_language_inference,33,2,1,1,research-problem,0.980600559,1,research-problem,1.29E-05,9.25E-05,1.33E-05,5.66E-06,9.81E-07,2.20E-05,0.000512448,0.000193909,1.57E-05,0.037509442,0.961165237,0.000417892,3.81E-05
2286,natural_language_inference33,3,abstract,,,natural_language_inference,33,3,1,0,,0.004275135,0,negative,0.000172564,0.000566819,6.30E-06,0.000186221,1.07E-05,0.001072161,0.000114264,0.009973721,0.000790861,0.985126726,0.001897692,4.59E-05,3.61E-05
2287,natural_language_inference33,4,A lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,,,natural_language_inference,33,4,1,1,research-problem,0.942906877,1,research-problem,1.22E-05,3.72E-05,3.70E-06,4.88E-06,9.08E-07,1.58E-05,0.000202263,0.000105112,4.74E-06,0.075633014,0.92381441,0.000145262,2.05E-05
2288,natural_language_inference33,5,These representations are typically used as general purpose features for words across a range of NLP problems .,,,natural_language_inference,33,5,1,0,,0.03096572,0,negative,3.04E-05,0.000229324,2.12E-05,8.65E-06,3.05E-06,0.000202519,0.000117888,0.001693959,0.000126696,0.961831184,0.035613405,0.000106843,1.49E-05
2289,natural_language_inference33,6,"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem .",,,natural_language_inference,33,6,1,1,research-problem,0.027362132,0,negative,1.30E-05,4.73E-05,2.95E-06,2.02E-06,6.91E-07,3.75E-05,5.70E-05,0.000383094,9.77E-06,0.918846129,0.080472047,0.000124008,4.56E-06
2290,natural_language_inference33,7,Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed - length sentence representations .,,,natural_language_inference,33,7,1,0,,0.244754562,0,negative,3.27E-05,0.00013082,7.28E-06,3.30E-06,1.42E-06,5.12E-05,0.00015785,0.000689897,1.66E-05,0.739810315,0.258832332,0.000253699,1.26E-05
2291,natural_language_inference33,8,"In this work , we present a simple , effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model .",,,natural_language_inference,33,8,1,0,,0.895529982,1,negative,0.002699159,0.174595315,0.00379761,0.000166607,0.000579074,0.000416945,0.002065508,0.006317272,0.008880524,0.49835231,0.28363921,0.018064665,0.0004258
2292,natural_language_inference33,9,We train this model on several data sources with multiple training objectives on over 100 million sentences .,,,natural_language_inference,33,9,1,0,,0.113103479,0,negative,0.000329252,0.012430731,0.000113161,3.34E-05,0.000248897,0.003445398,0.001002936,0.050387772,0.000840949,0.929845446,0.000344871,0.000904502,7.27E-05
2293,natural_language_inference33,10,Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods .,,,natural_language_inference,33,10,1,0,,0.163425638,0,negative,0.0029158,0.000754416,5.95E-05,2.39E-06,8.99E-06,4.66E-05,0.000425748,0.001042103,4.10E-05,0.897767667,0.002468621,0.094440712,2.65E-05
2294,natural_language_inference33,11,We present substantial improvements in the context of transfer learning and low - resource settings using our learned general - purpose representations .,,,natural_language_inference,33,11,1,0,,0.245353908,0,negative,0.006870235,0.000490989,4.52E-05,1.08E-05,2.20E-05,0.000105248,0.001863862,0.00189857,3.88E-05,0.591993305,0.001181684,0.395360338,0.000119062
2295,natural_language_inference33,12,1,,,natural_language_inference,33,12,1,0,,4.70E-05,0,negative,1.68E-05,3.53E-05,9.31E-07,9.06E-07,8.71E-07,2.59E-05,3.81E-06,0.000320079,7.57E-05,0.999477931,2.16E-05,1.95E-05,6.41E-07
2296,natural_language_inference33,13,INTRODUCTION,,,natural_language_inference,33,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
2297,natural_language_inference33,14,Transfer learning has driven a number of recent successes in computer vision and NLP .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,1,1,0,,0.326275908,0,research-problem,1.35E-06,0.000316335,7.68E-07,4.28E-06,1.26E-05,4.80E-06,1.47E-05,6.99E-06,8.00E-05,0.052338863,0.947215746,1.97E-06,1.59E-06
2298,natural_language_inference33,15,"Computer vision tasks like image captioning and visual question answering typically use CNNs pretrained on ImageNet to extract representations of the image , while several natural language tasks such as reading comprehension and sequence labeling have benefited from pretrained word embeddings thatare either fine - tuned for a specific task or held fixed .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,2,1,0,,0.112346229,0,research-problem,1.12E-06,0.000227499,3.28E-07,1.29E-05,2.10E-05,1.40E-05,1.22E-05,1.62E-05,6.94E-05,0.116188996,0.883433041,1.22E-06,2.02E-06
2299,natural_language_inference33,16,"Many neural NLP systems are initialized with pretrained word embeddings but learn their representations of words in context from scratch , in a task - specific manner from supervised learning signals .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,3,1,0,,0.503585062,1,research-problem,1.64E-06,0.001269016,9.20E-07,7.58E-06,2.25E-05,2.20E-05,1.36E-05,4.18E-05,0.000529652,0.150921688,0.847165783,1.77E-06,2.00E-06
2300,natural_language_inference33,17,"However , learning these representations reliably from scratch is not always feasible , especially in low - resource settings , where we believe that using general purpose sentence representations will be beneficial .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,4,1,0,,0.196982846,0,research-problem,3.00E-06,0.000651058,4.60E-07,1.59E-05,3.76E-05,1.90E-05,1.28E-05,2.96E-05,0.000139675,0.270153259,0.728933302,2.32E-06,1.93E-06
2301,natural_language_inference33,18,Some recent work has addressed this by learning general - purpose sentence representations .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,5,1,1,research-problem,0.248484362,0,research-problem,1.49E-06,0.000368946,4.94E-07,7.16E-06,1.59E-05,1.24E-05,1.12E-05,1.76E-05,0.000130543,0.120819748,0.878611541,1.31E-06,1.66E-06
2302,natural_language_inference33,19,"However , there exists no clear consensus yet on what training objective or methodology is best suited to this goal .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,6,1,0,,0.004367533,0,negative,4.01E-06,0.002917864,1.65E-06,1.87E-05,0.000100904,5.42E-05,1.70E-05,9.58E-05,0.002697508,0.693626747,0.300459993,3.35E-06,2.34E-06
2303,natural_language_inference33,20,Understanding the inductive biases of distinct neural models is important for guiding progress in representation learning .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,7,1,0,,0.780379872,1,research-problem,9.18E-07,0.000923811,4.51E-07,7.80E-06,1.53E-05,1.79E-05,1.16E-05,3.45E-05,0.000830909,0.136596374,0.861557609,1.21E-06,1.61E-06
2304,natural_language_inference33,21,and demonstrate that neural ma -chine translation ( NMT ) systems appear to capture morphology and some syntactic properties .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,8,1,0,,0.501337485,1,research-problem,3.15E-06,0.001188672,2.98E-06,2.78E-06,2.38E-05,6.32E-06,2.59E-05,9.59E-06,0.000755768,0.089473246,0.908500974,4.88E-06,1.96E-06
2305,natural_language_inference33,22,also present evidence that sequence - to - sequence parsers more strongly encode source language syntax .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,9,1,0,,0.004799489,0,negative,3.25E-05,0.003105637,1.67E-05,0.000198816,0.002746893,0.00039906,0.00010154,0.000130363,0.001925381,0.967079524,0.024240506,1.31E-05,9.96E-06
2306,natural_language_inference33,23,"Similarly , probe representations extracted by sequence autoencoders , word embedding averages , and skip - thought vectors with a multi - layer perceptron ( MLP ) classifier to study whether sentence characteristics such as length , word content and word order are encoded .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,10,1,0,,0.499069377,0,negative,0.000153806,0.273668614,0.000500657,1.69E-05,0.006928175,0.000130025,0.000177225,0.000134231,0.172969534,0.525158297,0.020070535,8.45E-05,7.46E-06
2307,natural_language_inference33,24,"To generalize across a diverse set of tasks , it is important to build representations that encode several aspects of a sentence .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,11,1,0,,0.342884091,0,negative,1.20E-05,0.016108658,2.74E-06,9.57E-06,9.54E-05,3.87E-05,2.93E-05,0.000124604,0.011230358,0.49440375,0.477929711,1.18E-05,3.43E-06
2308,natural_language_inference33,25,"Neural approaches to tasks such as skip - thoughts , machine translation , natural language inference , and constituency parsing likely have different inductive biases .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,12,1,0,,0.106074077,0,research-problem,1.30E-06,0.000703624,6.30E-07,2.98E-06,1.42E-05,1.27E-05,1.69E-05,2.03E-05,0.00039347,0.2659396,0.732889874,3.13E-06,1.37E-06
2309,natural_language_inference33,26,"Our work exploits this in the context of a simple one - to - many multi -task learning ( MTL ) framework , wherein a single recurrent sentence encoder is shared across multiple tasks .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,13,1,1,approach,0.90567155,1,approach,5.34E-05,0.513100088,0.000218964,1.47E-05,0.000873471,7.25E-05,9.45E-05,0.000145395,0.378662683,0.061593562,0.045126574,3.25E-05,1.17E-05
2310,natural_language_inference33,27,"We hypothesize that sentence representations learned by training on a reasonably large number of weakly related tasks will generalize better to novel tasks unseen during training , since this process encodes the inductive biases of multiple models .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,14,1,0,,0.141463388,0,model,1.39E-05,0.244498452,2.71E-05,2.03E-05,0.001061172,0.000229499,5.51E-05,0.000733756,0.410729796,0.324629141,0.017981492,1.21E-05,8.20E-06
2311,natural_language_inference33,28,This hypothesis is based on the theoretical work of .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,15,1,0,,0.010955856,0,negative,1.02E-05,0.008800506,5.35E-06,1.91E-05,0.000238175,0.000430736,6.10E-05,0.000574829,0.04100429,0.935032853,0.013811891,5.79E-06,5.29E-06
2312,natural_language_inference33,29,"While our work aims at learning fixed - length distributed sentence representations , it is not always practical to assume that the entire "" meaning "" of a sentence can be encoded into a fixed - length vector .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,16,1,1,approach,0.848399265,1,approach,4.19E-05,0.392183383,4.79E-05,2.48E-05,0.000754735,8.67E-05,9.33E-05,0.000336319,0.140262152,0.289828848,0.17628479,4.34E-05,1.17E-05
2313,natural_language_inference33,30,We merely hope to capture some of its characteristics that could be of use in a variety of tasks .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,17,1,0,,0.002334044,0,negative,1.46E-05,0.003828919,3.70E-06,7.86E-05,0.003041434,0.000204718,2.63E-05,0.000127474,0.003466805,0.988664139,0.000536243,5.45E-06,1.71E-06
2314,natural_language_inference33,31,The primary contribution of our work is to combine the benefits of diverse sentence - representation learning objectives into a single multi-task framework .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,18,1,1,approach,0.470954743,0,approach,0.000108668,0.659781364,0.000108925,6.40E-05,0.003130474,0.000162342,0.000123662,0.000352392,0.183921138,0.131753471,0.020427588,4.80E-05,1.80E-05
2315,natural_language_inference33,32,"To the best of our knowledge , this is the first large - scale reusable sentence representation model obtained by combining a set of training objectives with the level of diversity explored here , i.e. multi-lingual NMT , natural language inference , constituency parsing and skip - thought vectors .",INTRODUCTION,INTRODUCTION,natural_language_inference,33,19,1,0,,0.795046789,1,approach,0.000162641,0.618144653,0.000862123,0.000116706,0.02432878,0.000256084,0.000800249,0.000235072,0.16502055,0.1677337,0.022022543,0.00025753,5.94E-05
2316,natural_language_inference33,33,We demonstrate through extensive experimentation that representations learned in this way lead to improved performance across a diverse set of novel tasks not used in the learning of our representations .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,20,1,0,,0.68514251,1,approach,0.000370822,0.481638725,4.36E-05,2.02E-05,0.002132706,0.000162495,0.000396972,0.000723897,0.139967399,0.369560197,0.004681129,0.000284975,1.70E-05
2317,natural_language_inference33,34,Such representations facilitate low - resource learning as exhibited by significant improvements to model performance for new tasks in the low labelled data regime - achieving comparable performance to a few models trained from scratch using only 6 % of the available training set on the Quora duplicate question dataset .,INTRODUCTION,INTRODUCTION,natural_language_inference,33,21,1,0,,0.172650995,0,negative,0.000307496,0.248759253,0.000143784,0.000260735,0.013476477,0.000670787,0.0005426,0.000768224,0.165255114,0.562649962,0.006878576,0.000233664,5.33E-05
2318,natural_language_inference33,35,RELATED WORK,,,natural_language_inference,33,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
2319,natural_language_inference33,77,TRAINING OBJECTIVES & EVALUATION,,,natural_language_inference,33,0,1,0,,0.0063601,0,negative,2.52E-05,0.000188759,8.61E-06,2.66E-06,3.79E-06,5.69E-05,9.29E-05,0.00097904,3.98E-05,0.987338594,0.010996615,0.000259238,7.91E-06
2320,natural_language_inference33,78,Our motivation for multi-task training stems from theoretical insights presented in .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,1,1,0,,0.000965046,0,negative,1.55E-05,2.12E-05,1.84E-05,1.96E-06,2.03E-06,8.89E-05,7.21E-05,0.000168296,1.82E-05,0.99013757,0.009435487,1.67E-05,3.70E-06
2321,natural_language_inference33,79,"We refer readers to that work for a detailed discussion of results , but the conclusions most relevant to this discussion are ( i ) that learning multiple related tasks jointly results in good generalization as measured by the number of training examples required per task ; and ( ii ) that inductive biases learned on sufficiently many training tasks are likely to be good for learning novel tasks drawn from the same environment .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,2,1,0,,0.02416466,0,negative,0.000128374,9.50E-06,1.92E-05,7.29E-07,1.84E-06,6.42E-05,0.000142332,0.000152881,4.42E-06,0.998940695,0.000250132,0.00028419,1.51E-06
2322,natural_language_inference33,80,We select the following training objectives to learn general - purpose sentence embeddings .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,3,1,0,,0.00598256,0,negative,0.000111563,0.001764482,0.000294699,1.99E-06,8.89E-06,0.00101358,0.000232998,0.014290765,0.001666959,0.980474755,8.23E-05,5.24E-05,4.61E-06
2323,natural_language_inference33,81,"Our desiderata for the task collection were : sufficient diversity , existence of fairly large datasets for training , and success as standalone training objectives for sentence representations .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,4,1,0,,0.001191567,0,negative,0.000102493,1.54E-05,2.64E-05,5.38E-06,3.02E-05,8.29E-05,8.75E-05,5.32E-05,6.30E-06,0.99929137,0.000172651,0.000124203,2.06E-06
2324,natural_language_inference33,82,Skip - thought vectors,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,5,1,0,,0.302582727,0,negative,0.00066834,0.000768946,0.227134733,1.04E-06,2.80E-06,0.000542493,0.003371537,0.00107924,0.005574741,0.753656108,0.004758411,0.002404369,3.72E-05
2325,natural_language_inference33,83,Skip - thought vectors are an extension of skip - gram word embedding models to sentences .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,6,1,0,,0.313266662,0,baselines,0.000193823,0.001507825,0.726264498,3.21E-06,9.44E-06,0.000876615,0.003836653,0.001322932,0.005215287,0.255879072,0.004516968,0.000289145,8.45E-05
2326,natural_language_inference33,84,"The task typically requires a corpus of contiguous sentences , for which we use the BookCorpus .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,7,1,0,,0.001733519,0,negative,3.52E-05,2.72E-05,4.65E-05,2.36E-05,0.000133126,0.000133443,0.000312703,8.89E-05,5.32E-06,0.996445224,0.002685234,4.81E-05,1.54E-05
2327,natural_language_inference33,85,The learning objective is to simultaneously predict the next and previous sentences from the current sentence .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,8,1,0,,0.015437444,0,negative,9.07E-05,0.006779599,0.002003513,2.40E-06,1.45E-05,0.000237534,0.000189225,0.00207435,0.026615707,0.958276194,0.003616761,7.82E-05,2.13E-05
2328,natural_language_inference33,86,The encoder for the current sentence and the decoders for the previous ( STP ) and next sentence ( STN ) are typically parameterized as separate RNNs .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,9,1,0,,0.001763716,0,negative,6.82E-05,0.000413435,0.001317568,1.61E-06,4.12E-06,0.000324742,0.000140635,0.001074993,0.009408845,0.986924565,0.000298835,1.35E-05,8.94E-06
2329,natural_language_inference33,87,"We also consider training skip - thoughts by predicting only the next sentence given the current , motivated by results in where it is demonstrated that predicting the next sentence alone leads to comparable performance .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,10,1,0,,0.003455976,0,negative,0.001028702,0.003439543,0.013790369,1.52E-06,2.71E-05,0.000162949,0.000377634,0.000449644,0.004860558,0.97500145,0.0002301,0.000621417,9.03E-06
2330,natural_language_inference33,88,and demonstrated that NMT can be formulated as a sequence - to - sequence learning problem where the input is a sentence in the source language and the output is its corresponding translation in the target language .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,11,1,0,,0.009372591,0,negative,2.00E-05,8.01E-05,0.000179559,2.56E-06,2.51E-06,0.000153914,0.000502906,0.000568698,0.000105908,0.954431834,0.043837516,8.28E-05,3.17E-05
2331,natural_language_inference33,89,We use a parallel corpus of around 4.5 million English - German ( De ) sentence pairs from WMT15 and 40 million English - French ( Fr ) sentence pairs from WMT14 .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,12,1,0,,0.003039234,0,negative,0.000192224,0.000281335,0.001130606,0.000269279,0.018302059,0.00175585,0.003148752,0.000463759,4.04E-05,0.974055793,5.09E-05,0.000233824,7.52E-05
2332,natural_language_inference33,90,We train our representations using multiple target languages motivated by improvements demonstrated by .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,13,1,0,,0.013557391,0,negative,0.000275478,0.003064059,0.001111059,8.01E-06,0.000155024,0.002635347,0.001831615,0.011587564,0.000775588,0.978383409,1.73E-05,0.000135346,2.02E-05
2333,natural_language_inference33,91,Neural Machine Translation,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,14,1,0,,0.904917849,1,experiments,0.000738967,0.000137008,0.003697397,6.52E-05,9.58E-05,0.001641147,0.48112232,0.001443801,3.61E-05,0.348764243,0.086759183,0.068579612,0.006919123
2334,natural_language_inference33,92,Constituency Parsing ( linearized parse tree construction ) demonstrated that a sequence - to - sequence approach to constituency parsing is viable .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,15,1,0,,0.094046799,0,negative,0.000126835,6.33E-05,0.000471903,5.61E-06,6.48E-06,0.000146718,0.002699,0.000202022,4.72E-05,0.841388306,0.154159117,0.000562834,0.000120672
2335,natural_language_inference33,93,The input to the encoder is the sentence itself and the decoder produces its linearized parse tree .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,16,1,0,,0.012317723,0,negative,2.64E-05,0.000353016,0.000726474,5.95E-07,4.85E-06,0.00017626,8.68E-05,0.000862379,0.006253988,0.99145397,4.04E-05,8.20E-06,6.72E-06
2336,natural_language_inference33,94,We train on 3 million weakly labeled parses obtained by parsing a random subset of the 1 - billion word corpus with the Puck GPU parser 2 along with gold parses from sections 0 - 21 of the WSJ section of Penn Treebank .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,17,1,0,,0.043502066,0,negative,0.000152451,0.001883343,0.000744599,2.40E-05,0.00043452,0.010981703,0.007920785,0.029016319,0.000384573,0.948173366,2.14E-05,0.00015742,0.000105495
2337,natural_language_inference33,95,Gold parses are duplicated 5 times and shuffled in with the weakly labeled parses to have a roughly 1:5 ratio of gold to noisy parses .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,18,1,0,,0.009073053,0,negative,0.00012206,0.000125775,0.000502813,3.84E-06,0.000293253,0.00129547,0.001662828,0.001907632,4.73E-05,0.993950262,1.86E-06,7.42E-05,1.27E-05
2338,natural_language_inference33,96,Natural Language Inference Natural language inference ( NLI ) is a 3 - way classification problem .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,19,1,0,,0.633609763,1,negative,0.000394135,0.000226382,0.002638021,4.08E-05,5.87E-05,0.000443381,0.028932318,0.000490893,4.91E-05,0.733840151,0.224389057,0.007095657,0.001401326
2339,natural_language_inference33,97,"Given a premise and a hypothesis sentence , the objective is to classify their relationship as either entailment , contradiction , or neutral .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,20,1,0,,0.000121855,0,negative,5.17E-05,0.000411741,0.000214741,1.65E-05,9.77E-05,0.000132931,0.000155121,0.000352517,0.000189571,0.996759935,0.001501897,8.65E-05,2.91E-05
2340,natural_language_inference33,98,"In contrast to the previous tasks , we do not formulate this as sequence - to - sequence learning .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,21,1,0,,7.00E-05,0,negative,7.70E-05,0.000143187,0.000181583,9.97E-07,1.93E-05,3.16E-05,2.94E-05,8.76E-05,9.95E-05,0.999250369,3.12E-05,4.61E-05,2.14E-06
2341,natural_language_inference33,99,"We use the shared recurrent sentence encoder to encode both the premise and hypothesis into fixed length vectors u and v , respectively .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,22,1,0,,0.04220615,0,negative,0.000209159,0.004182586,0.003213793,1.03E-05,4.69E-05,0.002734892,0.001565027,0.024082136,0.048322345,0.915407533,7.29E-05,3.29E-05,0.000119579
2342,natural_language_inference33,100,"We then feed the vector [ u ; v ; | u ? v| ; u * v ] , which is a concatenation of the premise and hypothesis vectors and their respective absolute difference and hadamard product , to an MLP that performs the 3 - way classification .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,23,1,0,,0.002355042,0,negative,1.96E-05,0.000213367,0.000613979,2.21E-07,3.45E-06,0.000244553,0.000109323,0.001078635,0.003550293,0.994149349,6.37E-06,7.13E-06,3.75E-06
2343,natural_language_inference33,101,This is the same classification strategy adopted by .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,24,1,0,,8.34E-05,0,negative,1.56E-05,1.45E-05,0.000300185,7.40E-08,1.59E-06,2.54E-05,2.33E-05,4.70E-05,0.000118068,0.999442508,2.66E-06,8.41E-06,7.24E-07
2344,natural_language_inference33,102,We train on a collection of about 1 million sentence pairs from the SNLI and MultiNLI corpora .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,25,1,0,,0.018808408,0,negative,0.000243257,0.001768215,0.001329206,4.05E-05,0.005443521,0.001862973,0.004194615,0.002729548,0.000121086,0.981625603,6.54E-06,0.000522037,0.000112912
2345,natural_language_inference33,103,use periodic task alternations with equal training ratios for every task .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,26,1,0,,0.001069381,0,negative,0.000178977,5.78E-05,0.001556025,2.30E-06,1.60E-05,0.000792834,0.000786373,0.001453165,0.000144454,0.994800787,9.16E-06,0.000173953,2.81E-05
2346,natural_language_inference33,104,"In contrast , alter the training ratios for each task based on the size of their respective training sets .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,27,1,0,,4.53E-05,0,negative,0.000142297,3.60E-05,0.000171228,6.16E-07,1.02E-05,5.82E-05,3.75E-05,0.000151913,7.45E-05,0.999264356,1.09E-06,5.02E-05,1.79E-06
2347,natural_language_inference33,105,"Specifically , the training ratio for a particular task , ? i , is the fraction of the number of training examples in that task to the total number of training samples across all tasks .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,28,1,0,,4.69E-05,0,negative,5.37E-06,2.61E-05,2.62E-05,1.06E-07,1.51E-06,2.79E-05,1.43E-05,0.000169231,7.83E-05,0.999641195,2.75E-06,6.29E-06,6.76E-07
2348,natural_language_inference33,106,The authors then perform ?,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,29,1,0,,0.00041049,0,negative,1.45E-05,1.21E-06,3.80E-06,6.46E-07,4.17E-06,3.43E-05,9.38E-06,3.05E-05,2.90E-06,0.999891644,2.52E-07,6.25E-06,4.41E-07
2349,natural_language_inference33,107,"i * N parameter updates on task i before selecting a new task at random proportional to the training ratios , where N is a predetermined constant .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,30,1,0,,0.000280248,0,negative,0.0003446,5.98E-05,0.000608878,3.06E-07,1.08E-05,4.60E-05,0.000139116,0.000193297,0.000165192,0.998269939,1.72E-06,0.000156162,4.15E-06
2350,natural_language_inference33,108,We take a simpler approach and pick a new sequence - to - sequence task to train on after every parameter update sampled uniformly .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,31,1,0,,0.000551714,0,negative,0.00078468,0.001804531,0.001889689,1.85E-06,6.67E-05,7.76E-05,0.00018548,0.000396312,0.002039594,0.992546205,4.50E-06,0.000191694,1.12E-05
2351,natural_language_inference33,109,An NLI minibatch is interspersed after every ten parameter updates on sequence - to - sequence tasks ( this was chosen so as to complete roughly 6 epochs of the dataset after 7 days of training ) .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,32,1,0,,0.001003587,0,negative,0.00026992,0.000764458,0.001698904,3.00E-05,0.000519643,0.011780389,0.011119574,0.029512081,0.000511407,0.94345523,1.98E-06,0.00010603,0.000230404
2352,natural_language_inference33,110,Our approach is described formally in the Algorithm below .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,33,1,0,,0.000428021,0,negative,3.47E-05,0.000173051,0.000103675,2.99E-06,3.03E-05,0.000168861,8.41E-05,0.000527166,0.000893459,0.997940094,5.84E-06,2.07E-05,1.50E-05
2353,natural_language_inference33,111,Model details can be found in section 7 in the Appendix .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,34,1,0,,0.000133816,0,negative,2.27E-05,8.52E-06,1.31E-05,1.67E-06,1.43E-05,0.000169119,8.40E-05,0.000355819,2.04E-05,0.999281084,2.32E-07,2.71E-05,1.93E-06
2354,natural_language_inference33,112,"Require : A set of k tasks with a common source language , a shared encoder E across all tasks and a set of k task specific decoders D1 . . .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,35,1,0,,0.000303256,0,negative,0.000176074,0.00048859,0.000536399,6.84E-07,3.17E-05,5.46E-05,0.000206693,0.000267526,0.000459586,0.997453525,8.81E-06,0.000304722,1.11E-05
2355,natural_language_inference33,113,D k .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,36,1,0,,7.53E-05,0,negative,2.73E-05,1.29E-06,8.90E-06,1.28E-07,1.40E-06,2.26E-05,2.76E-05,3.50E-05,5.06E-06,0.999833645,3.96E-07,3.58E-05,9.30E-07
2356,natural_language_inference33,114,Let ?,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,37,1,0,,4.25E-05,0,negative,5.38E-06,9.10E-07,3.19E-06,6.75E-08,7.37E-07,2.72E-05,1.54E-05,5.46E-05,4.36E-06,0.999880611,2.15E-07,6.95E-06,4.30E-07
2357,natural_language_inference33,115,"denote each model 's parameters , ? a probability vector ( p 1 . . . pk ) denoting the probability of sampling a task such that ?",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,38,1,0,,4.17E-05,0,negative,4.71E-06,1.29E-05,9.44E-06,1.23E-07,2.05E-06,5.07E-05,2.37E-05,0.000303785,6.07E-05,0.999524858,3.00E-07,5.76E-06,9.30E-07
2358,natural_language_inference33,116,"k i pi = 1 , datasets for each task IP1 . . .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,39,1,0,,7.63E-05,0,negative,0.000272878,2.20E-05,0.000148436,5.79E-08,6.45E-06,9.30E-06,5.06E-05,2.89E-05,2.33E-05,0.999109228,4.51E-07,0.000327132,1.16E-06
2359,natural_language_inference33,117,IP k and a loss function L.,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,40,1,0,,0.000170514,0,negative,0.000190171,1.37E-05,8.93E-05,2.46E-07,8.57E-06,1.77E-05,4.65E-05,7.50E-05,1.63E-05,0.999380797,3.82E-07,0.00015941,1.93E-06
2360,natural_language_inference33,118,while ?,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,41,1,0,,6.79E-05,0,negative,1.25E-05,2.21E-06,1.87E-05,2.13E-07,1.76E-06,9.81E-05,0.000123721,0.000135191,9.60E-06,0.999558609,8.79E-07,3.45E-05,3.90E-06
2361,natural_language_inference33,119,has not converged do,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,42,1,0,,0.000236685,0,negative,3.93E-05,3.42E-06,6.60E-05,1.27E-07,3.81E-06,5.52E-05,7.52E-05,7.25E-05,1.38E-05,0.999614497,2.12E-07,5.39E-05,2.16E-06
2362,natural_language_inference33,120,end : T - SNE visualizations of our sentence representations on 3 different datasets .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,43,1,0,,0.00110887,0,negative,4.50E-05,4.42E-06,2.00E-05,9.07E-07,3.70E-06,0.000293919,0.001530907,0.000597792,8.40E-06,0.996889635,5.31E-06,0.0005608,3.93E-05
2363,natural_language_inference33,121,"SUBJ ( left ) , TREC ( middle ) , DBpedia ( right ) .",TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,44,1,0,,0.001782254,0,negative,6.27E-05,3.99E-06,0.000261703,6.92E-07,2.32E-05,0.000180423,0.003556012,0.000171236,6.07E-06,0.993754228,2.70E-06,0.001882224,9.48E-05
2364,natural_language_inference33,122,Dataset details are presented in the Appendix .,TRAINING OBJECTIVES & EVALUATION,TRAINING OBJECTIVES & EVALUATION,natural_language_inference,33,45,1,0,,0.000254565,0,negative,7.94E-06,5.00E-06,6.86E-06,4.10E-06,4.27E-05,0.000180012,6.67E-05,0.000336903,6.75E-06,0.999318977,9.57E-08,2.05E-05,3.52E-06
2365,natural_language_inference33,123,"EVALUATION STRATEGIES , EXPERIMENTAL RESULTS & DISCUSSION",TRAINING OBJECTIVES & EVALUATION,,natural_language_inference,33,46,1,0,,0.002413745,0,negative,1.91E-05,1.41E-06,1.30E-05,1.38E-08,3.95E-07,1.98E-05,0.000184717,4.79E-05,5.88E-06,0.999576161,5.73E-07,0.000127458,3.59E-06
2366,natural_language_inference33,124,"In this section , we describe our approach to evaluate the quality of our learned representations , present the results of our evaluation and discuss our findings .",TRAINING OBJECTIVES & EVALUATION,"EVALUATION STRATEGIES , EXPERIMENTAL RESULTS & DISCUSSION",natural_language_inference,33,47,1,0,,3.11E-06,0,negative,0.000168127,9.52E-05,4.45E-05,1.36E-06,0.0003452,1.96E-05,2.81E-05,1.34E-05,2.93E-05,0.999213142,2.27E-07,3.51E-05,6.77E-06
2367,natural_language_inference33,125,EVALUATION STRATEGY,,,natural_language_inference,33,0,1,0,,0.007504942,0,negative,1.17E-05,6.49E-05,5.98E-06,1.43E-07,2.05E-07,3.84E-05,1.16E-05,0.000976737,5.58E-05,0.998102311,0.000659453,7.23E-05,6.66E-07
2368,natural_language_inference33,126,We follow a similar evaluation protocol to those presented in ; ; which is to use our learned representations as features for a low complexity classifier ( typically linear ) on a novel supervised task / domain unseen during training without updating the parameters of our sentence representation model .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,1,1,0,,2.58E-05,0,negative,0.000173555,0.000141366,0.007780331,1.48E-06,7.29E-06,4.28E-05,6.89E-05,0.000724411,1.22E-05,0.99036855,2.14E-05,0.000653204,4.53E-06
2369,natural_language_inference33,127,We also consider such a transfer learning evaluation in an artificially constructed low - resource setting .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,2,1,0,,5.94E-05,0,negative,0.000294387,0.000334609,0.00608049,5.77E-07,1.91E-06,2.66E-05,2.31E-05,0.001035403,3.76E-05,0.99112353,6.50E-05,0.000973055,3.70E-06
2370,natural_language_inference33,128,"In addition , we also evaluate the quality of our learned individual word representations using standard benchmarks .",EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,3,1,0,,0.000655405,0,negative,0.000151146,0.000179251,0.000403577,6.81E-07,2.81E-06,3.04E-05,2.87E-05,0.001318818,1.02E-05,0.997113526,7.76E-06,0.000750948,2.20E-06
2371,natural_language_inference33,129,The choice of transfer tasks and evaluation framework 3 are borrowed largely from .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,4,1,0,,5.07E-06,0,negative,1.45E-05,1.97E-05,6.18E-05,1.22E-06,4.65E-07,4.99E-05,8.34E-06,0.0015299,6.81E-06,0.998247109,2.98E-05,2.82E-05,2.22E-06
2372,natural_language_inference33,130,We provide a condensed summary of the tasks in section 10 in the Appendix but refer readers to their paper for a more detailed description .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,5,1,0,,2.95E-06,0,negative,5.78E-05,5.99E-06,2.31E-05,3.09E-05,4.32E-06,5.92E-05,6.48E-06,0.000380851,5.46E-06,0.999395761,4.29E-06,2.31E-05,2.74E-06
2373,natural_language_inference33,131,presents the results of training logistic regression on 10 different supervised transfer tasks using different fixed - length sentence representation .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,6,1,0,,0.011578121,0,negative,0.001254429,7.30E-05,0.0229103,1.25E-05,1.64E-05,7.86E-05,0.000644483,0.001001443,1.35E-05,0.963267192,0.001066588,0.009512528,0.000149047
2374,natural_language_inference33,132,Supervised approaches trained from scratch on some of these tasks are also presented for comparison .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,7,1,0,,0.00021714,0,negative,4.37E-05,5.28E-05,0.002810962,2.33E-07,7.91E-07,2.55E-05,1.88E-05,0.000705029,8.47E-06,0.996078492,1.63E-05,0.000236776,2.19E-06
2375,natural_language_inference33,133,We present performance ablations when adding more tasks and increasing the number of hidden units in our GRU ( + L ) .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,8,1,0,,0.011611894,0,negative,0.057033954,0.000194405,0.004917582,8.90E-06,2.25E-05,6.04E-05,0.000144293,0.000888468,3.34E-05,0.906657554,3.08E-05,0.029978379,2.94E-05
2376,natural_language_inference33,134,Ablation specifics are presented in section 9 of the Appendix .,EVALUATION STRATEGY,EVALUATION STRATEGY,natural_language_inference,33,9,1,0,,5.49E-06,0,negative,0.000636081,1.15E-05,0.000243148,8.93E-06,3.85E-06,7.09E-05,1.27E-05,0.000661605,1.67E-05,0.998197535,1.69E-06,0.000130309,4.97E-06
2377,natural_language_inference33,135,EXPERIMENTAL RESULTS & DISCUSSION,,,natural_language_inference,33,0,1,0,,0.004755534,0,negative,9.80E-05,4.33E-05,9.11E-06,4.78E-07,5.87E-07,5.40E-05,0.000328967,0.00054096,1.56E-05,0.985744828,0.007654977,0.005502312,6.99E-06
2378,natural_language_inference33,136,It is evident from that adding more tasks improves the transfer performance of our model .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,1,1,1,results,0.905625833,1,results,0.029820062,6.63E-07,1.31E-05,3.30E-07,1.78E-07,5.63E-06,0.000583112,2.11E-05,2.34E-07,0.064158581,1.19E-05,0.90537359,1.15E-05
2379,natural_language_inference33,137,Increasing the capacity our sentence encoder with more hidden units ( + L ) as well as an additional layer ( + 2L ) also lead to improved transfer performance .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,2,1,1,results,0.689667794,1,results,0.202887181,2.78E-06,6.15E-05,1.79E-06,7.88E-07,2.87E-05,0.000726631,8.58E-05,2.57E-06,0.149636654,1.80E-05,0.646519742,2.78E-05
2380,natural_language_inference33,138,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,3,1,1,results,0.941307828,1,results,0.007774033,2.15E-06,6.30E-05,5.11E-07,4.01E-07,5.82E-06,0.00092968,2.64E-05,5.16E-07,0.055470888,1.70E-05,0.935678379,3.12E-05
2381,natural_language_inference33,139,"We demonstrate substantial gains on TREC ( 6 % over Infersent and roughly 2 % over the CNN - LSTM ) , outperforming even a competitive supervised baseline .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,4,1,1,results,0.823810965,1,results,0.003475658,1.60E-06,4.04E-05,5.10E-07,3.60E-07,6.37E-06,0.000719947,2.35E-05,5.69E-07,0.068827493,1.21E-05,0.926862175,2.94E-05
2382,natural_language_inference33,140,"We see similar gains ( 2.3 % ) on paraphrase identification ( MPRC ) , closing the gap on supervised approaches trained from scratch .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,5,1,1,results,0.897909499,1,results,0.002614251,7.82E-07,6.71E-05,1.55E-07,1.92E-07,2.19E-06,0.00060688,7.96E-06,1.67E-07,0.059615463,1.68E-05,0.937051902,1.62E-05
2383,natural_language_inference33,141,The addition of constituency parsing improves performance on sentence relatedness ( SICK - R ) and entailment ( SICK - E ) consistent with observations made by .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,6,1,1,results,0.940190934,1,results,0.003985371,1.12E-06,7.30E-05,2.07E-07,2.17E-07,3.29E-06,0.000739996,1.04E-05,2.66E-07,0.055889265,2.80E-05,0.939250704,1.82E-05
2384,natural_language_inference33,142,"In , we show that simply training an MLP on top of our fixed sentence representations outperforms several strong & complex supervised approaches that use attention mechanisms , even on this fairly large dataset .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,7,1,1,results,0.412101324,0,results,0.010592353,6.02E-06,6.34E-05,4.29E-07,5.71E-07,1.02E-05,0.00047102,4.60E-05,1.69E-06,0.194560552,1.50E-05,0.794219322,1.35E-05
2385,natural_language_inference33,143,"For example , we observe a 0.2-0.5 % improvement over the decomposable attention model .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,8,1,1,results,0.750908554,1,results,0.047611084,1.07E-06,4.07E-05,5.22E-07,4.31E-07,5.53E-06,0.000616194,2.42E-05,7.02E-07,0.070207938,2.61E-06,0.881466086,2.29E-05
2386,natural_language_inference33,144,"When using only a small fraction of the training data , indicated by the columns 1 k - 25 k , we are able to outperform the Siamese and Multi - Perspective CNN using roughly 6 % of the available training set .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,9,1,1,results,0.755038545,1,results,0.009527386,1.81E-06,3.81E-05,5.14E-07,4.57E-07,7.55E-06,0.000725989,3.16E-05,6.81E-07,0.061015625,6.43E-06,0.928613648,3.02E-05
2387,natural_language_inference33,145,We also outperform the Deconv LVM model proposed by in this low - resource setting .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,10,1,1,results,0.888537304,1,results,0.004813909,1.18E-06,2.56E-05,9.26E-07,3.35E-07,1.03E-05,0.00105608,4.07E-05,5.01E-07,0.033634927,8.66E-06,0.960361627,4.52E-05
2388,natural_language_inference33,146,"Unlike , who use pretrained Glo Ve word embeddings , we learn our word embeddings from scratch .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,11,1,0,,0.006711221,0,negative,0.003339924,0.000182059,0.000536865,3.08E-06,1.02E-05,5.43E-05,7.36E-05,0.000329714,0.000105003,0.992368087,2.98E-06,0.00298843,5.76E-06
2389,natural_language_inference33,147,"Somewhat surprisingly , in we observe that the learned word embeddings are competitive with popular methods such as GloVe , word2vec , and fasttext on the benchmarks presented by and .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,12,1,1,results,0.789948775,1,results,0.003716393,2.15E-06,1.56E-05,8.25E-07,6.25E-07,1.62E-05,0.00113254,7.56E-05,6.28E-07,0.125845053,9.28E-06,0.869154397,3.08E-05
2390,natural_language_inference33,148,"In , we probe our sentence representations to determine if certain sentence characteristics and syntactic properties can be inferred following work by and .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,13,1,0,,0.001673526,0,negative,0.009206109,9.60E-05,0.001229263,7.28E-06,2.70E-05,3.21E-05,0.000129384,6.02E-05,8.09E-05,0.968246849,1.41E-05,0.020854907,1.59E-05
2391,natural_language_inference33,149,We observe that syntactic properties are better encoded with the addition of multi-lingual NMT and parsing .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,14,1,0,,0.945945817,1,results,0.064337779,1.36E-05,5.70E-05,5.93E-06,3.11E-06,4.96E-05,0.001183401,0.000249067,1.54E-05,0.272293565,2.42E-05,0.661675594,9.19E-05
2392,natural_language_inference33,150,Representations learned solely from NLI do appear to encode syntax but incorporation into our multi-task framework does not amplify this signal .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,15,1,1,results,0.00420878,0,negative,0.021920732,6.93E-06,0.000124445,5.40E-07,1.48E-06,9.55E-06,0.00017935,3.92E-05,6.68E-06,0.633119103,2.52E-05,0.344547509,1.93E-05
2393,natural_language_inference33,151,"Similarly , we observe that sentence characteristics such as length and word order are better encoded with the addition of parsing .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,16,1,1,results,0.895565253,1,results,0.130566366,1.15E-05,7.91E-05,2.96E-06,2.96E-06,2.44E-05,0.000686165,0.000125983,1.08E-05,0.299651804,7.61E-06,0.568793238,3.70E-05
2394,natural_language_inference33,152,"In Appendix , we note that our sentence representations outperform skip - thoughts and are on par with Infersent for image - caption retrieval .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,17,1,0,,0.892472284,1,results,0.003430045,5.42E-07,1.09E-05,1.23E-07,1.44E-07,2.43E-06,0.000461136,1.04E-05,1.42E-07,0.056697349,2.16E-06,0.939373834,1.08E-05
2395,natural_language_inference33,153,We also observe that comparing sentences using For MRPC and STSB we consider only the F 1 score and Spearman correlations respectively and we also multiply the SICK - R scores by 100 to have all differences in the same scale .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,18,1,0,,0.000813843,0,results,0.02255757,1.21E-06,3.19E-05,8.15E-08,3.37E-07,3.08E-06,0.000257509,1.39E-05,4.60E-07,0.328337984,1.74E-06,0.648790302,3.93E-06
2396,natural_language_inference33,154,Bold numbers indicate the best performing transfer model on a given task .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,19,1,0,,0.000179951,0,negative,5.31E-05,2.83E-06,8.53E-06,2.33E-07,1.72E-07,2.69E-05,2.86E-05,0.000177082,1.57E-05,0.998770425,2.67E-06,0.000910825,2.87E-06
2397,natural_language_inference33,155,Underlines are used for each task to indicate both our best performing model as well as the best performing transfer model that is n't ours .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,20,1,0,,0.000163212,0,negative,4.85E-05,4.80E-06,1.44E-05,1.84E-07,2.30E-07,3.40E-05,2.24E-05,0.000239672,2.69E-05,0.999131181,1.26E-06,0.000474571,1.97E-06
2398,natural_language_inference33,156,cosine similarities correlates reasonably well with their relatedness on semantic textual similarity benchmarks ( Appendix ) .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,21,1,0,,0.270287081,0,results,0.001439954,6.78E-07,2.16E-05,3.38E-07,2.97E-07,1.32E-05,0.000764865,4.22E-05,6.07E-07,0.356595428,1.31E-05,0.641074353,3.33E-05
2399,natural_language_inference33,157,We also present qualitative analysis of our learned representations by visualizations using dimensionality reduction techniques ) and nearest neighbor exploration ( Appendix ) .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,22,1,0,,0.002176608,0,negative,0.00113988,3.06E-06,1.79E-05,4.95E-06,4.93E-06,2.11E-05,2.70E-05,3.40E-05,5.61E-06,0.995988715,4.39E-07,0.002749187,3.28E-06
2400,natural_language_inference33,158,"shows t- sne plots of our sentence representations on three different datasets - SUBJ , TREC and DBpedia .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,23,1,0,,0.002210347,0,negative,0.00092992,9.97E-07,3.68E-05,1.02E-07,3.00E-07,7.43E-06,0.000159808,2.64E-05,9.61E-07,0.888932849,3.18E-06,0.1098933,7.99E-06
2401,natural_language_inference33,159,DBpedia is a large corpus of sentences from Wikipedia labeled by category and used by .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,24,1,0,,0.010247581,0,negative,0.00292824,7.36E-05,0.001345393,0.002452441,0.00214803,0.001147236,0.002210723,0.000445759,5.71E-05,0.969193253,4.58E-05,0.016190975,0.001761516
2402,natural_language_inference33,160,Sentences appear to cluster reasonably well according to their labels .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,25,1,0,,0.078692148,0,negative,0.005772597,2.41E-06,1.68E-05,1.32E-06,1.16E-06,1.91E-05,0.000571957,8.73E-05,2.11E-06,0.543213505,5.38E-06,0.450248129,5.82E-05
2403,natural_language_inference33,161,The clustering also appears better than that demonstrated in of on TREC and SUBJ .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,26,1,0,,0.084693099,0,results,0.003616799,1.44E-06,1.17E-05,3.02E-06,9.26E-07,5.12E-05,0.002144343,0.000215237,1.33E-06,0.241124367,5.66E-06,0.75267626,0.000147659
2404,natural_language_inference33,162,Appendix contains sentences from the BookCorpus and their nearest neighbors .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,27,1,0,,0.000359704,0,negative,0.00064191,1.24E-06,1.17E-05,6.51E-05,9.36E-06,6.67E-05,5.46E-05,4.91E-05,4.56E-06,0.997213328,1.45E-06,0.001865411,1.56E-05
2405,natural_language_inference33,163,Sentences with some lexical overlap and similar discourse structure appear to be clustered together .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,28,1,0,,0.069626525,0,negative,0.013182665,5.13E-06,8.05E-05,4.72E-07,2.46E-06,5.86E-06,9.40E-05,2.39E-05,5.24E-06,0.874402859,1.51E-06,0.11218473,1.06E-05
2406,natural_language_inference33,164,and respectively .,EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,29,1,0,,0.001435788,0,negative,8.36E-05,4.19E-07,3.59E-06,1.32E-08,5.63E-08,1.02E-06,1.96E-06,7.60E-06,2.04E-06,0.999388682,9.29E-08,0.000510719,1.95E-07
2407,natural_language_inference33,165,"We also report QVEC benchmarks Model Accuracy 1 k 5 k 10 k 25 k All ( 400 k ) , the next 5 from and The last 4 rows are our experiments using Infersent and our models .",EXPERIMENTAL RESULTS & DISCUSSION,EXPERIMENTAL RESULTS & DISCUSSION,natural_language_inference,33,30,1,0,,0.003434007,0,negative,0.000761275,1.03E-06,1.68E-05,7.49E-07,4.76E-06,1.31E-05,0.000135676,2.44E-05,6.59E-07,0.971070365,2.12E-07,0.027965424,5.51E-06
2408,natural_language_inference33,166,CONCLUSION & FUTURE WORK,,,natural_language_inference,33,0,1,0,,0.000937136,0,negative,4.16E-05,6.90E-05,5.87E-06,8.07E-07,4.53E-07,9.78E-05,4.95E-05,0.000796725,9.14E-05,0.994591543,0.004078466,0.000172344,4.48E-06
2409,natural_language_inference33,176,APPENDIX 7 MODEL TRAINING,,,natural_language_inference,33,0,1,0,,0.005113676,0,negative,0.000155993,7.51E-05,8.53E-06,6.08E-06,3.01E-06,9.31E-05,0.000128589,0.001411949,4.82E-05,0.996055904,0.000916367,0.001089358,7.81E-06
2410,natural_language_inference33,177,We present some architectural specifics and training details of our multi-task framework .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,1,1,0,,2.85E-05,0,negative,7.73E-05,0.000317555,7.29E-05,1.48E-05,2.98E-05,0.000181296,4.10E-05,0.000687422,0.000238984,0.998262612,4.46E-05,2.73E-05,4.51E-06
2411,natural_language_inference33,178,Our shared encoder uses a common word embedding lookup table and GRU .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,2,1,0,,0.015289647,0,negative,0.001894645,0.009359909,0.271301008,1.71E-05,7.42E-05,0.001052457,0.001375538,0.00364963,0.048762777,0.661278185,0.00080955,0.000317224,0.000107801
2412,natural_language_inference33,179,"We experiment with unidirectional , bidirectional and 2 layer bidirectional GRUs ( details in Appendix section 9 ) .",APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,3,1,0,,0.035391927,0,negative,0.001137514,0.005901489,0.024445158,6.04E-05,0.000288227,0.0071169,0.008732059,0.016815738,0.001957347,0.932423008,0.000277362,0.00071806,0.000126751
2413,natural_language_inference33,180,"For each task , every decoder has its separate word embedding lookups , conditional GRUs and fully connected layers that project the GRU hidden states to the target vocabularies .",APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,4,1,0,,0.0127191,0,negative,0.00252862,0.003814854,0.066568031,8.72E-06,5.62E-05,0.000437409,0.000603108,0.001767735,0.02001977,0.903610513,0.000242041,0.000297029,4.60E-05
2414,natural_language_inference33,181,The last hidden state of the encoder is used as the initial hidden state of the decoder and is also presented as input to all the gates of the GRU at every time step .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,5,1,0,,0.001463918,0,negative,8.48E-05,0.000240451,0.000972253,2.72E-07,1.59E-06,0.000135718,4.99E-05,0.001401104,0.001330092,0.995726488,2.58E-05,3.01E-05,1.46E-06
2415,natural_language_inference33,182,"For natural language inference , the same encoder is used to encode both the premise and hypothesis and a concatenation of their representations along with the absolute difference and hadamard product ( as described in ) are given to a single layer MLP with a dropout ) rate of 0.3 .",APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,6,1,0,,0.032805774,0,negative,0.000251118,0.001851723,0.00237134,2.35E-05,3.40E-05,0.016544045,0.002629948,0.108151414,0.003385158,0.864517868,0.000113502,5.20E-05,7.44E-05
2416,natural_language_inference33,183,All models use word embeddings of 512 dimensions and GRUs with either 1500 or 2048 hidden units .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,7,1,0,,0.660348087,1,hyperparameters,0.000261787,0.000784336,0.000812897,0.000279004,5.16E-05,0.255240639,0.029261421,0.525704765,0.000823692,0.18575085,0.000162781,0.000145357,0.000720838
2417,natural_language_inference33,184,We used minibatches of 48 examples and the Adam optimizer with a learning rate of 0.002 .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,8,1,0,,0.71321041,1,hyperparameters,0.000208714,0.000877384,0.000233828,0.000168529,5.61E-05,0.159742611,0.014243163,0.602380571,0.000561179,0.220701378,0.000157404,0.000130103,0.000539079
2418,natural_language_inference33,185,Models were trained for 7 days on an Nvidia Tesla P100 - SXM2 - 16GB GPU .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,9,1,0,,0.843813455,1,experimental-setup,0.00041642,0.000240129,0.000502886,0.015353943,0.000653039,0.620612894,0.041087604,0.054795542,0.000197008,0.263935787,0.000175655,0.000197582,0.001831511
2419,natural_language_inference33,186,"While report close to a month of training , we only train for 7 days , made possible by advancements in GPU hardware and software ( cuDNN RNNs ) .",APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,10,1,0,,0.00091448,0,negative,0.00134476,6.93E-05,0.00052751,0.000120788,0.000951966,0.000853664,0.000533683,0.000326168,2.46E-05,0.99490858,1.67E-05,0.000288486,3.38E-05
2420,natural_language_inference33,187,We did not tune any of the architectural details and hyperparameters owing to the fact that we were unable to identify any clear criterion on which to tune them .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,11,1,0,,4.54E-05,0,negative,0.000193429,8.24E-05,5.34E-05,3.52E-05,6.02E-05,0.001847165,0.000136933,0.00201587,6.19E-05,0.995457308,5.31E-06,4.35E-05,7.33E-06
2421,natural_language_inference33,188,Gains in performance on a specific task do not often translate to better transfer performance .,APPENDIX 7 MODEL TRAINING,APPENDIX 7 MODEL TRAINING,natural_language_inference,33,12,1,0,,0.001267476,0,negative,0.000144224,4.74E-06,2.83E-05,4.61E-07,1.45E-06,1.25E-05,7.55E-05,5.70E-05,2.10E-06,0.998886053,0.000149573,0.000635462,2.76E-06
2422,natural_language_inference33,189,VOCABULARY EXPANSION & REPRESENTATION POOLING,APPENDIX 7 MODEL TRAINING,,natural_language_inference,33,13,1,0,,0.023523211,0,negative,0.004050049,0.000280511,0.049434341,8.83E-05,7.19E-05,0.001572321,0.023348932,0.002165237,0.000703877,0.892924722,0.005150965,0.017927746,0.002281141
2423,natural_language_inference33,190,"In addition to performing 10 - fold cross -validation to determine the L2 regularization penalty on the logistic regression models , we also tune the way in which our sentence representations are generated from the hidden states corresponding to words in a sentence .",APPENDIX 7 MODEL TRAINING,VOCABULARY EXPANSION & REPRESENTATION POOLING,natural_language_inference,33,14,1,0,,0.035147191,0,negative,0.001430411,0.004345236,0.000239187,6.45E-06,0.000184605,0.001148305,0.000829489,0.004934345,0.001114577,0.985462104,8.88E-06,0.000289366,7.04E-06
2424,natural_language_inference33,191,"For example , use the last hidden state while perform max - pooling across all of the hidden states .",APPENDIX 7 MODEL TRAINING,VOCABULARY EXPANSION & REPRESENTATION POOLING,natural_language_inference,33,15,1,0,,1.13E-05,0,negative,2.49E-05,1.59E-05,1.55E-05,9.15E-07,2.82E-06,0.000175255,2.49E-05,0.000203024,4.71E-05,0.999472795,7.66E-06,8.56E-06,6.55E-07
2425,natural_language_inference33,192,We consider both of these approaches and pick the one with better performance on the validation set .,APPENDIX 7 MODEL TRAINING,VOCABULARY EXPANSION & REPRESENTATION POOLING,natural_language_inference,33,16,1,0,,7.51E-05,0,negative,2.54E-05,9.40E-05,3.52E-05,2.48E-07,3.67E-06,0.000179656,4.02E-05,0.000720516,0.000161185,0.998726347,1.93E-06,1.13E-05,3.43E-07
2426,natural_language_inference33,193,"We note that max - pooling works best on sentiment tasks such as MR , CR , SUBJ and MPQA , while the last hidden state works better on all other tasks .",APPENDIX 7 MODEL TRAINING,VOCABULARY EXPANSION & REPRESENTATION POOLING,natural_language_inference,33,17,1,0,,0.553770251,1,negative,0.04790417,0.000104358,0.000110348,5.96E-05,0.000106897,0.002026873,0.028040608,0.003784799,2.64E-05,0.721830538,0.000229284,0.195559242,0.000216882
2427,natural_language_inference33,194,We also employ vocabulary expansion on all tasks as in by training a linear regression to map from the space of pre-trained word embeddings ( GloVe ) to our model 's word embeddings .,APPENDIX 7 MODEL TRAINING,VOCABULARY EXPANSION & REPRESENTATION POOLING,natural_language_inference,33,18,1,0,,0.056401331,0,negative,0.002749777,0.011456708,0.016016875,6.15E-06,0.000158847,0.001022213,0.001437192,0.00249919,0.027837982,0.936516341,2.80E-05,0.000250749,1.99E-05
2428,natural_language_inference33,195,MULTI - TASK MODEL DETAILS,APPENDIX 7 MODEL TRAINING,,natural_language_inference,33,19,1,0,,0.010658892,0,negative,0.00022097,0.00139547,0.003097528,2.20E-05,5.16E-05,0.001248633,0.002355386,0.005671576,0.006272791,0.978328949,0.000428065,0.000600682,0.000306272
2429,natural_language_inference33,196,This section describes the specifics our multi-task ablations in the experiments section .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,20,1,0,,4.49E-05,0,negative,3.10E-05,4.86E-05,3.99E-05,2.85E-05,0.000135445,0.00044115,3.71E-05,0.000457877,2.81E-05,0.998722894,4.45E-06,2.15E-05,3.36E-06
2430,natural_language_inference33,197,These definitions hold for all tables except for 3 and 5 .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,21,1,0,,3.24E-05,0,negative,1.31E-05,1.05E-05,1.14E-05,2.47E-06,9.66E-06,0.000524552,3.01E-05,0.000582403,1.25E-05,0.998777975,9.56E-07,2.35E-05,7.72E-07
2431,natural_language_inference33,198,"We refer to skip - thought next as STN , French and German NMT as Fr and De , natural language inference as NLI , skip - thought previous as STP and parsing as Par .",APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,22,1,0,,0.000563438,0,negative,1.07E-05,6.83E-05,0.000177591,5.12E-06,2.30E-05,0.002072814,0.000590062,0.002550216,5.91E-05,0.994289463,5.41E-05,8.49E-05,1.45E-05
2432,natural_language_inference33,199,"+ STN + Fr + De : The sentence representation h x is the concatenation of the final hidden vectors from a forward GRU with 1500 - dimensional hidden vectors and a bidirectional GRU , also with 1500 - dimensional hidden vectors .",APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,23,1,0,,0.000600185,0,negative,0.000899965,0.000121467,0.06023712,2.86E-06,5.54E-05,0.001730438,0.003285554,0.001659071,0.000131233,0.92915713,2.51E-05,0.00263999,5.46E-05
2433,natural_language_inference33,200,+ STN + Fr + De + NLI :,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,24,1,0,,0.00018413,0,negative,0.001194998,4.78E-05,0.002914027,3.10E-06,3.46E-05,0.000227336,0.00043287,0.000295444,4.68E-05,0.991471182,3.91E-05,0.003275597,1.71E-05
2434,natural_language_inference33,201,The sentence representation h x is the concatenation of the final hidden vectors from a bidirectional GRU with 1500 - dimensional hidden vectors and another bidirectional GRU with 1500 - dimensional hidden vectors trained without NLI .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,25,1,0,,0.002147434,0,negative,2.47E-05,0.000158371,0.000441622,3.41E-06,2.21E-05,0.023076762,0.002838084,0.081830524,0.000177742,0.891283233,1.26E-05,8.88E-05,4.20E-05
2435,natural_language_inference33,202,+ STN + Fr + De + NLI +L :,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,26,1,0,,0.000153172,0,negative,0.000701764,2.98E-05,0.001027468,2.01E-06,2.27E-05,0.000198624,0.00037383,0.00028698,3.05E-05,0.994392358,3.16E-05,0.002888889,1.35E-05
2436,natural_language_inference33,203,The sentence representation h x is the concatenation of the final hidden vectors from a bidirectional GRU with 2048 - dimensional hidden vectors and another bidirectional GRU with 2048 - dimensional hidden vectors trained without NLI .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,27,1,0,,0.00152943,0,negative,2.16E-05,0.000151813,0.0007688,2.17E-06,2.01E-05,0.009999942,0.001820421,0.034097144,0.000222844,0.952768644,1.15E-05,8.24E-05,3.26E-05
2437,natural_language_inference33,204,+ STN + Fr + De + NLI +L + STP :,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,28,1,0,,0.000263344,0,negative,0.000744258,4.77E-05,0.002657258,1.67E-06,2.35E-05,0.000187111,0.000429809,0.00027289,5.33E-05,0.992096873,3.11E-05,0.003438512,1.61E-05
2438,natural_language_inference33,205,The sentence representation h x is the concatenation of the final hidden vectors from a bidirectional GRU with 2048 - dimensional hidden vectors and another bidirectional GRU with 2048 - dimensional hidden vectors trained without STP .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,29,1,0,,0.00147057,0,negative,2.50E-05,0.000167587,0.000883436,2.16E-06,2.30E-05,0.009822441,0.001993958,0.034817999,0.000266842,0.951863701,9.07E-06,8.78E-05,3.70E-05
2439,natural_language_inference33,206,+ STN + Fr + De + NLI +2L + STP :,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,30,1,0,,0.000191237,0,negative,0.000913349,4.48E-05,0.001178342,3.01E-06,4.75E-05,0.000358321,0.000872178,0.000482374,3.69E-05,0.99118234,2.80E-05,0.004821901,3.08E-05
2440,natural_language_inference33,207,The sentence representation h x is the concatenation of the final hidden vectors from a 2 - layer bidirectional GRU with 2048 - dimensional hidden vectors and a 1 - layer bidirectional GRU with 2048 - dimensional hidden vectors trained without STP .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,31,1,0,,0.001481881,0,negative,2.65E-05,0.000196816,0.000604809,2.48E-06,2.28E-05,0.010768611,0.001659844,0.046900121,0.000340765,0.939357108,7.35E-06,7.20E-05,4.08E-05
2441,natural_language_inference33,208,+ STN + Fr + De + NLI +L + STP + Par :,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,32,1,0,,0.000194431,0,negative,0.001093543,4.48E-05,0.003966188,2.00E-06,4.60E-05,0.000179263,0.000459584,0.000194666,4.58E-05,0.98981939,1.22E-05,0.004117375,1.92E-05
2442,natural_language_inference33,209,The sentence representation h x is the concatenation of the final hidden vectors from a bidirectional GRU with 2048 - dimensional hidden vectors and another bidirectional GRU with 2048 - dimensional hidden vectors trained without Par.,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,33,1,0,,0.001121051,0,negative,2.21E-05,0.000137085,0.000814409,1.78E-06,2.51E-05,0.007420542,0.00178606,0.027047464,0.000229057,0.962394747,4.65E-06,8.02E-05,3.68E-05
2443,natural_language_inference33,210,In tables 3 and 5 we do not concatenate the representations of multiple models .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,34,1,0,,7.48E-05,0,negative,3.55E-05,7.93E-06,4.23E-05,3.95E-07,1.01E-05,0.000322242,0.000108707,0.000585154,5.72E-06,0.998670368,3.51E-07,0.000209904,1.34E-06
2444,natural_language_inference33,211,and provide a detailed description of tasks thatare typically used to evaluate sentence representations .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,35,1,0,,8.18E-05,0,negative,7.94E-06,3.56E-06,1.95E-05,7.82E-06,7.06E-05,0.000116397,2.53E-05,8.03E-05,2.18E-06,0.999629267,2.66E-06,3.12E-05,3.30E-06
2445,natural_language_inference33,212,We provide a condensed summary and refer readers to their work for a more thorough description .,APPENDIX 7 MODEL TRAINING,MULTI - TASK MODEL DETAILS,natural_language_inference,33,36,1,0,,3.10E-05,0,negative,1.09E-05,1.15E-05,1.25E-05,2.01E-05,4.35E-05,0.000474916,2.90E-05,0.000496964,1.05E-05,0.998862096,1.32E-06,2.30E-05,3.72E-06
2446,natural_language_inference33,213,DESCRIPTION OF EVALUATION TASKS,,,natural_language_inference,33,0,1,0,,0.000672069,0,negative,9.47E-06,9.99E-05,3.84E-06,4.93E-07,9.69E-07,3.39E-05,2.57E-05,0.000673918,3.68E-05,0.99570311,0.003295937,0.000114114,1.85E-06
2447,natural_language_inference33,214,TEXT CLASSIFICATION,DESCRIPTION OF EVALUATION TASKS,,natural_language_inference,33,1,1,0,,0.519878814,1,negative,0.000197229,5.31E-05,0.000246587,4.61E-05,3.37E-05,0.001574863,0.016455308,0.00218564,1.95E-05,0.95016549,0.020505717,0.007513475,0.001003394
2448,natural_language_inference33,215,"We evaluate on text classification benchmarks - sentiment classification on movie reviews ( MR ) , product reviews ( CR ) and Stanford sentiment ( SST ) , question type classification ( TREC ) , subjectivity / objectivity classification ( SUBJ ) and opinion polarity ( MPQA ) .",DESCRIPTION OF EVALUATION TASKS,TEXT CLASSIFICATION,natural_language_inference,33,2,1,0,,0.004107244,0,negative,0.000138772,0.000415723,0.000533854,5.59E-05,0.000802195,0.000932514,0.019366584,0.000162033,2.92E-05,0.971495504,0.005237458,0.000720576,0.000109661
2449,natural_language_inference33,216,Representations are used to train a logistic regression classifier with 10 - fold cross validation to tune the L2 weight penalty .,DESCRIPTION OF EVALUATION TASKS,TEXT CLASSIFICATION,natural_language_inference,33,3,1,0,,0.000194013,0,negative,2.05E-05,0.000192131,3.04E-05,7.32E-07,2.64E-06,0.015711654,0.001469505,0.01500421,0.00081889,0.966726291,1.52E-05,5.26E-06,2.72E-06
2450,natural_language_inference33,217,The evaluation metric for all these tasks is classification accuracy .,DESCRIPTION OF EVALUATION TASKS,TEXT CLASSIFICATION,natural_language_inference,33,4,1,0,,3.38E-06,0,negative,4.04E-07,1.98E-06,3.84E-07,1.27E-08,5.76E-08,4.63E-05,2.92E-05,4.48E-05,3.84E-06,0.999804422,6.69E-05,1.64E-06,7.37E-08
2451,natural_language_inference33,218,PARAPHRASE IDENTIFICATION,DESCRIPTION OF EVALUATION TASKS,,natural_language_inference,33,5,1,0,,0.000528823,0,negative,0.009217785,0.000154451,0.001593267,0.007752682,0.002516,0.0035695,0.286013088,0.001073268,2.46E-05,0.548225012,0.035084871,0.084572476,0.020202979
2452,natural_language_inference33,219,We also evaluate on pairwise text classification tasks such as paraphrase identification on the Microsoft Research Paraphrase Corpus ( MRPC ) corpus .,DESCRIPTION OF EVALUATION TASKS,PARAPHRASE IDENTIFICATION,natural_language_inference,33,6,1,0,,0.004706136,0,negative,0.000588367,0.002111502,0.000685335,1.37E-05,0.001158351,0.002221921,0.142420328,0.000225767,0.000208565,0.848412154,0.000269542,0.001547982,0.000136527
2453,natural_language_inference33,220,This is a binary classification problem to identify if two sentences are paraphrases of each other .,DESCRIPTION OF EVALUATION TASKS,PARAPHRASE IDENTIFICATION,natural_language_inference,33,7,1,0,,0.00104165,0,negative,1.47E-05,6.94E-05,2.73E-05,2.17E-05,3.78E-05,0.001071762,0.005319384,7.15E-05,9.87E-05,0.965672023,0.027452033,2.11E-05,0.000122634
2454,natural_language_inference33,221,The evaluation metric is classification accuracy and F1 .,DESCRIPTION OF EVALUATION TASKS,PARAPHRASE IDENTIFICATION,natural_language_inference,33,8,1,0,,6.80E-06,0,negative,3.33E-06,7.29E-06,1.17E-06,7.08E-08,1.64E-06,0.000331023,0.000604939,4.60E-05,1.69E-05,0.998975017,9.00E-06,3.06E-06,5.61E-07
2455,natural_language_inference33,222,ENTAILMENT AND SEMANTIC RELATEDNESS,,,natural_language_inference,33,0,1,0,,0.223799899,0,research-problem,5.06E-05,0.000100142,2.55E-05,1.49E-05,5.37E-06,3.58E-05,0.001141426,0.000186733,1.39E-05,0.116478874,0.880787797,0.001083338,7.56E-05
2456,text_summarization12,1,title,,,text_summarization,12,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
2457,text_summarization12,2,Selective Encoding for Abstractive Sentence Summarization,title,,text_summarization,12,1,1,1,research-problem,0.998546278,1,research-problem,2.68E-07,1.40E-05,3.19E-07,1.53E-06,1.10E-06,3.67E-07,1.02E-05,1.81E-06,3.74E-07,0.002762781,0.997205886,9.78E-07,4.40E-07
2458,text_summarization12,3,abstract,,,text_summarization,12,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
2459,text_summarization12,4,We propose a selective encoding model to extend the sequence - to - sequence framework for abstractive sentence summarization .,abstract,abstract,text_summarization,12,1,1,0,,0.573257247,1,research-problem,8.57E-06,0.039236518,7.18E-05,4.27E-06,3.19E-05,1.16E-05,7.93E-06,0.00016913,0.011853355,0.083491762,0.865108002,3.58E-06,1.63E-06
2460,text_summarization12,5,"It consists of a sentence encoder , a selective gate network , and an attention equipped decoder .",abstract,abstract,text_summarization,12,2,1,0,,0.535717895,1,negative,9.23E-05,0.178153582,0.001410893,5.41E-05,0.000706942,0.00015806,4.70E-05,0.000804126,0.230621862,0.406730489,0.181197015,1.11E-05,1.25E-05
2461,text_summarization12,6,The sentence encoder and decoder are built with recurrent neural networks .,abstract,abstract,text_summarization,12,3,1,0,,0.689309721,1,negative,1.97E-05,0.139692387,0.000156684,0.000101035,0.000239222,0.001314762,5.07E-05,0.014792161,0.233219542,0.542865086,0.067535421,2.03E-06,1.12E-05
2462,text_summarization12,7,The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder .,abstract,abstract,text_summarization,12,4,1,0,,0.386907656,0,model,5.14E-05,0.113438774,0.001152213,9.55E-06,0.000204997,6.45E-05,1.74E-05,0.000373313,0.403451533,0.372214451,0.109011233,6.69E-06,4.02E-06
2463,text_summarization12,8,"The second level representation is tailored for sentence summarization task , which leads to better performance .",abstract,abstract,text_summarization,12,5,1,1,research-problem,0.09100243,0,negative,0.000279967,0.157801007,0.000511784,1.47E-05,0.00047933,5.94E-05,3.46E-05,0.000676876,0.030486013,0.662551771,0.147041411,5.95E-05,3.58E-06
2464,text_summarization12,9,"We evaluate our model on the English Gigaword , DUC 2004 and MSR abstractive sentence summarization datasets .",abstract,abstract,text_summarization,12,6,1,0,,0.007821893,0,negative,1.04E-05,0.087001338,1.32E-05,2.04E-05,0.000809505,8.00E-05,4.57E-05,0.001364405,0.000655419,0.831805918,0.078166721,2.48E-05,2.31E-06
2465,text_summarization12,10,The experimental results show that the proposed selective encoding model outperforms the state - of the - art baseline models .,abstract,abstract,text_summarization,12,7,1,0,,0.01492469,0,negative,0.000343911,0.004004548,4.29E-06,1.32E-05,6.46E-05,4.47E-05,0.000132249,0.001116198,9.43E-05,0.870150712,0.122984593,0.001041655,4.98E-06
2466,text_summarization12,11,Introduction,,,text_summarization,12,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
2467,text_summarization12,12,Sentence summarization aims to shorten a given sentence and produce a brief summary of it .,Introduction,Introduction,text_summarization,12,1,1,0,,0.621666779,1,research-problem,2.00E-06,0.000106391,7.19E-07,0.000148838,0.000143301,1.68E-05,3.33E-05,5.80E-06,2.25E-05,0.046043229,0.953466692,1.90E-06,8.55E-06
2468,text_summarization12,13,"This is different from document level summarization task since it is hard to apply existing techniques in extractive methods , such as extracting sentence level features and ranking sentences .",Introduction,Introduction,text_summarization,12,2,1,0,,0.202301462,0,research-problem,1.12E-06,0.000205152,4.13E-07,3.85E-05,6.97E-05,8.95E-06,1.20E-05,6.75E-06,3.56E-05,0.083047735,0.916570281,1.25E-06,2.55E-06
2469,text_summarization12,14,"Early works propose using rule - based methods , syntactic tree pruning methods , statistical machine translation techniques and soon for this task .",Introduction,Introduction,text_summarization,12,3,1,0,,0.02676358,0,research-problem,6.95E-06,0.000652099,1.91E-06,3.95E-05,0.000122723,7.54E-05,3.52E-05,5.03E-05,0.00032521,0.391359228,0.607322777,3.76E-06,4.98E-06
2470,text_summarization12,15,We focus on abstractive sentence summarization task in this paper .,Introduction,Introduction,text_summarization,12,4,1,0,,0.53916257,1,research-problem,1.48E-05,0.081218435,2.04E-05,3.41E-05,0.000932677,6.18E-05,0.000168416,0.000222054,0.007768433,0.150639284,0.758868771,3.08E-05,2.00E-05
2471,text_summarization12,16,"Recently , neural network models have been applied in this task .",Introduction,Introduction,text_summarization,12,5,1,0,,0.065425918,0,research-problem,2.12E-06,0.000716302,9.49E-07,4.03E-06,1.06E-05,2.82E-05,1.87E-05,4.72E-05,0.001224303,0.20285245,0.795091044,2.22E-06,1.93E-06
2472,text_summarization12,17,use autoconstructed sentence - headline pairs to train a neu - * Contribution during internship at Microsoft Research .,Introduction,Introduction,text_summarization,12,6,1,0,,0.010990272,0,research-problem,2.33E-05,0.007038659,3.43E-05,2.05E-05,0.00025388,7.00E-05,9.56E-05,7.43E-05,0.006078052,0.350375743,0.635897458,2.61E-05,1.21E-05
2473,text_summarization12,18,ral network summarization model .,Introduction,Introduction,text_summarization,12,7,1,0,,0.229593837,0,research-problem,3.31E-06,0.00201541,4.60E-06,1.36E-06,1.68E-05,1.37E-05,7.42E-05,3.66E-05,0.003403768,0.117175052,0.877230345,1.93E-05,5.54E-06
2474,text_summarization12,19,They use a Convolutional Neural Network ( CNN ) encoder and feed - forward neural network language model decoder for this task .,Introduction,Introduction,text_summarization,12,8,1,0,,0.710227413,1,negative,9.10E-05,0.077892992,0.000533341,0.000155807,0.001196843,0.000951726,0.000393105,0.00078848,0.256479068,0.531211998,0.130216147,3.51E-05,5.44E-05
2475,text_summarization12,20,extend their work by replacing the decoder with Recurrent Neural Network ( RNN ) .,Introduction,Introduction,text_summarization,12,9,1,0,,0.002399476,0,negative,3.71E-05,0.00570659,4.79E-05,0.000198256,0.000962563,0.000645429,0.000116707,0.000274363,0.006871876,0.912298349,0.072809404,1.20E-05,1.94E-05
2476,text_summarization12,21,follow this line and change the encoder to RNN to make it a full RNN based sequence - tosequence model .,Introduction,Introduction,text_summarization,12,10,1,0,,0.001838057,0,negative,2.99E-05,0.009150207,2.87E-05,3.84E-05,0.000489469,0.00048127,7.86E-05,0.000283703,0.038842846,0.944498261,0.006064523,8.23E-06,5.95E-06
2477,text_summarization12,22,the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country .,Introduction,Introduction,text_summarization,12,11,1,0,,0.012531802,0,negative,3.43E-05,0.005124625,7.60E-06,8.75E-05,0.0034141,0.000246059,8.59E-05,0.000113311,0.003813106,0.960801464,0.026250494,1.33E-05,8.26E-06
2478,text_summarization12,23,sri lanka closes schools as war escalates :,Introduction,Introduction,text_summarization,12,12,1,0,,0.003908181,0,negative,3.41E-05,0.002322006,1.15E-05,3.56E-05,0.000853924,0.000183611,0.000126305,6.67E-05,0.002293658,0.955289416,0.038752116,2.46E-05,6.46E-06
2479,text_summarization12,24,An abstractive sentence summarization system may produce the output summary by distilling the salient information from the highlight to generate a fluent sentence .,Introduction,Introduction,text_summarization,12,13,1,0,,0.092970331,0,research-problem,4.79E-06,0.001603182,2.40E-06,4.81E-05,0.000218617,4.04E-05,4.16E-05,4.29E-05,0.000596947,0.290050359,0.707336084,6.79E-06,7.88E-06
2480,text_summarization12,25,We model the distilling process with selective encoding .,Introduction,Introduction,text_summarization,12,14,1,0,,0.893621526,1,model,6.70E-06,0.036440552,2.35E-05,4.61E-07,3.19E-05,1.12E-05,7.50E-06,2.78E-05,0.955064677,0.007470665,0.000912522,1.71E-06,8.92E-07
2481,text_summarization12,26,"All the above works fall into the encodingdecoding paradigm , which first encodes the input sentence to an abstract representation and then decodes the intended output sentence based on the encoded information .",Introduction,Introduction,text_summarization,12,15,1,0,,0.041113413,0,negative,6.00E-06,0.012120173,1.13E-05,3.39E-05,0.000143373,0.000341695,0.000104468,0.000460077,0.026373113,0.641923361,0.318462722,7.69E-06,1.21E-05
2482,text_summarization12,27,"As an extension of the encoding - decoding framework , attentionbased approach has been broadly used : the encoder produces a list of vectors for all tokens in the input , and the decoder uses an attention mechanism to dynamically extract encoded information and align with the output tokens .",Introduction,Introduction,text_summarization,12,16,1,0,,0.038081388,0,negative,2.62E-05,0.038497168,6.26E-05,7.01E-05,0.000457012,0.000571173,0.000171778,0.000692999,0.090302906,0.746859811,0.122248388,1.78E-05,2.20E-05
2483,text_summarization12,28,"This approach achieves huge success in tasks like machine translation , where alignment between all parts of the input and output are required .",Introduction,Introduction,text_summarization,12,17,1,0,,0.155618921,0,research-problem,4.68E-06,0.001030775,2.64E-06,5.28E-05,0.000108426,7.19E-05,7.55E-05,5.00E-05,0.000305862,0.329878199,0.668402617,6.84E-06,9.62E-06
2484,text_summarization12,29,"However , in abstractive sentence summarization , there is no explicit alignment relationship between the input sentence and the summary ex-cept for the extracted common words .",Introduction,Introduction,text_summarization,12,18,1,0,,0.129866013,0,research-problem,4.51E-06,0.001008062,2.38E-06,9.78E-05,0.000243176,5.44E-05,9.90E-05,3.65E-05,0.000120971,0.213443412,0.784870813,6.42E-06,1.26E-05
2485,text_summarization12,30,"The challenge here is not to infer the alignment , but to select the highlights while filtering out secondary information in the input .",Introduction,Introduction,text_summarization,12,19,1,0,,0.019734938,0,negative,2.46E-05,0.036898619,1.12E-05,0.000299612,0.002568481,0.000219431,5.21E-05,0.000183292,0.018265956,0.889088633,0.052364174,1.40E-05,9.86E-06
2486,text_summarization12,31,"A desired work - flow for abstractive sentence summarization is encoding , selection , and decoding .",Introduction,Introduction,text_summarization,12,20,1,0,,0.682792708,1,research-problem,4.29E-06,0.001810008,3.74E-06,8.31E-05,0.000230902,4.95E-05,0.000128603,4.26E-05,0.000306387,0.178684452,0.818630786,9.31E-06,1.62E-05
2487,text_summarization12,32,"After selecting the important information from an encoded sentence , the decoder produces the output summary using the selected information .",Introduction,Introduction,text_summarization,12,21,1,0,,0.790607541,1,model,1.56E-05,0.105688696,5.00E-05,3.85E-07,0.000194016,8.85E-06,1.43E-05,2.32E-05,0.83708245,0.055989686,0.000924615,7.39E-06,8.39E-07
2488,text_summarization12,33,"For example , in 1 , given the input sentence , the summarization system first selects the important information , and then rephrases or paraphrases to produce a well - organized summary .",Introduction,Introduction,text_summarization,12,22,1,0,,0.008712214,0,negative,7.99E-06,0.007258937,1.57E-05,0.000194679,0.003144785,0.000322016,9.80E-05,0.000159499,0.00519428,0.949839395,0.033738541,1.14E-05,1.48E-05
2489,text_summarization12,34,"Although this is implicitly modeled in the encoding - decoding framework , we argue that abstractive sentence summarization shall benefit from explicitly modeling this selection process .",Introduction,Introduction,text_summarization,12,23,1,0,,0.273510899,0,negative,7.68E-05,0.143899075,2.90E-05,1.26E-05,0.000509892,6.64E-05,4.79E-05,0.000167587,0.219277849,0.602003835,0.033874626,2.93E-05,5.11E-06
2490,text_summarization12,35,In this paper we propose Selective Encoding for Abstractive Sentence Summarization ( SEASS ) .,Introduction,Introduction,text_summarization,12,24,1,1,model,0.963339406,1,approach,0.000341104,0.500459749,0.000948201,4.79E-05,0.002459661,0.000230833,0.001925175,0.000376503,0.199248478,0.144182993,0.149335193,0.000343863,0.00010034
2491,text_summarization12,36,"We treat the sentence summarization as a threephase task : encoding , selection , and decoding .",Introduction,Introduction,text_summarization,12,25,1,1,model,0.877364062,1,model,2.45E-05,0.408259975,9.80E-05,8.96E-06,0.000844719,7.65E-05,0.000136928,0.00017511,0.500346314,0.073911102,0.016069483,3.24E-05,1.60E-05
2492,text_summarization12,37,"It consists of a sentence encoder , a selective gate network , and a summary decoder .",Introduction,Introduction,text_summarization,12,26,1,1,model,0.95395814,1,model,3.11E-05,0.057655137,0.000328311,2.22E-06,0.000405135,3.72E-05,4.71E-05,3.85E-05,0.9298102,0.011407336,0.000226042,6.29E-06,5.39E-06
2493,text_summarization12,38,"First , the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation .",Introduction,Introduction,text_summarization,12,27,1,1,model,0.952354288,1,model,1.75E-05,0.048664353,0.000150781,4.26E-07,0.000170672,1.36E-05,2.07E-05,1.48E-05,0.941825277,0.009043469,7.44E-05,2.78E-06,1.28E-06
2494,text_summarization12,39,Then the selective gate network selects the encoded information to construct the second level sentence representation .,Introduction,Introduction,text_summarization,12,28,1,1,model,0.90938992,1,model,8.68E-06,0.020401578,7.75E-05,1.56E-07,5.19E-05,6.68E-06,8.99E-06,1.08E-05,0.962576698,0.016764352,9.05E-05,1.68E-06,4.99E-07
2495,text_summarization12,40,"The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information , which helps improve encoding effectiveness and release the burden of the decoder .",Introduction,Introduction,text_summarization,12,29,1,1,model,0.959116181,1,model,1.31E-05,0.044086523,8.18E-05,2.74E-07,8.87E-05,7.88E-06,1.19E-05,1.16E-05,0.94879161,0.006844571,5.91E-05,2.20E-06,6.82E-07
2496,text_summarization12,41,"Finally , the attention - equipped decoder generates the summary using the second level sentence representation .",Introduction,Introduction,text_summarization,12,30,1,1,model,0.944861497,1,model,5.28E-06,0.019522979,5.26E-05,5.70E-08,2.35E-05,4.44E-06,7.14E-06,7.13E-06,0.973378077,0.006965969,3.16E-05,8.31E-07,3.36E-07
2497,text_summarization12,42,"We conduct experiments on English Gigaword , DUC 2004 and Microsoft Research Abstractive Text Compression test sets .",Introduction,Introduction,text_summarization,12,31,1,0,,0.017029057,0,approach,9.38E-05,0.687781759,0.000104613,4.71E-05,0.033358764,0.000473401,0.001624368,0.000940165,0.015192132,0.259689668,0.000415474,0.000240302,3.84E-05
2498,text_summarization12,43,"Our SEASS model achieves 17.54 ROUGE - 2 F1 , 9.56 ROUGE - 2 recall and 10.63 ROUGE - 2 F1 on these test sets respectively , which improves performance compared to the state - of - the - art methods .",Introduction,Introduction,text_summarization,12,32,1,0,,0.133563759,0,negative,0.004112242,0.062381812,0.000177978,0.000333739,0.01423888,0.00179947,0.126261221,0.003693763,0.004222202,0.7291798,0.002892113,0.048843491,0.00186329
2499,text_summarization12,44,Related Work,,,text_summarization,12,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
2500,text_summarization12,80,Model,,,text_summarization,12,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
2501,text_summarization12,81,"As shown in , our model consists of a sentence encoder using the Gated Recurrent Unit ( GRU ) , a selective gate network and an attention - equipped GRU decoder .",Model,Model,text_summarization,12,1,1,0,,0.192886924,0,model,0.000651306,0.004289875,0.010732139,1.40E-05,4.16E-05,0.000192601,0.000216769,0.001140919,0.62168015,0.359742368,0.000638018,0.000505295,0.000155033
2502,text_summarization12,82,"First , the bidirectional GRU encoder reads the input words x = ( x 1 , x 2 , . . . , x n ) and builds its representation ( h 1 , h 2 , . . . , h n ) .",Model,Model,text_summarization,12,2,1,0,,0.013196963,0,negative,0.000230286,0.000688592,0.005453984,5.96E-07,3.54E-06,2.60E-05,3.94E-05,0.00019324,0.07973989,0.913240743,7.02E-05,0.000306827,6.67E-06
2503,text_summarization12,83,Then the selective gate selects and filters the word representations according to the sentence meaning representation to produce a tailored sentence word representation for abstractive sentence summarization task .,Model,Model,text_summarization,12,3,1,0,,0.3109309,0,negative,0.000334578,0.001510376,0.011829276,1.58E-06,1.01E-05,4.22E-05,5.58E-05,0.000326783,0.372846548,0.61259945,0.000105855,0.000318231,1.92E-05
2504,text_summarization12,84,"Lastly , the GRU decoder produces the output summary with attention to the tailored representation .",Model,Model,text_summarization,12,4,1,0,,0.188921643,0,negative,0.000560109,0.001419438,0.010737742,1.14E-06,7.21E-06,4.27E-05,7.08E-05,0.000317364,0.261285245,0.724947369,6.27E-05,0.000532567,1.56E-05
2505,text_summarization12,85,"In the following sections , we introduce the sentence encoder , the selective mechanism , and the summary decoder respectively .",Model,Model,text_summarization,12,5,1,0,,0.017864349,0,negative,0.000170752,0.003252313,0.000595991,4.79E-06,1.93E-05,0.00010548,7.43E-05,0.001469833,0.171691778,0.822014543,0.000221181,0.000348428,3.13E-05
2506,text_summarization12,86,Sentence Encoder,Model,,text_summarization,12,6,1,0,,0.244690813,0,negative,0.000259802,0.001589651,0.007321383,1.24E-05,1.57E-05,0.000458015,0.001219024,0.003901064,0.289325863,0.689373513,0.002317112,0.003638975,0.000567556
2507,text_summarization12,87,The role of the sentence encoder is to read the input sentence and construct the basic sentence representation .,Model,Sentence Encoder,text_summarization,12,7,1,0,,0.001046826,0,negative,2.02E-05,0.000158055,0.000390051,1.13E-07,4.00E-07,3.73E-06,3.28E-06,8.41E-05,0.008407088,0.990854635,1.33E-05,6.38E-05,1.24E-06
2508,text_summarization12,88,"Here we employ a bidirectional GRU ( BiGRU ) as the recurrent unit , where GRU is defined as :",Model,Sentence Encoder,text_summarization,12,8,1,0,,0.039133793,0,negative,2.00E-05,0.000131786,0.007475366,7.72E-08,2.80E-07,7.27E-06,1.58E-05,9.09E-05,0.005342417,0.986715608,2.76E-05,0.000171206,1.67E-06
2509,text_summarization12,89,"where W z , W rand W hare weight matrices .",Model,Sentence Encoder,text_summarization,12,9,1,0,,4.87E-05,0,negative,3.55E-06,2.02E-06,2.03E-06,2.92E-08,2.15E-08,2.46E-06,9.27E-07,6.35E-05,5.60E-05,0.999847687,5.29E-07,2.11E-05,6.77E-08
2510,text_summarization12,90,The BiGRU consists of a forward GRU and a backward GRU .,Model,Sentence Encoder,text_summarization,12,10,1,0,,0.02626422,0,negative,5.10E-05,0.00040365,0.05542452,4.85E-07,1.97E-06,3.31E-05,0.000154102,0.000279178,0.007819543,0.935073239,7.32E-05,0.000671353,1.47E-05
2511,text_summarization12,91,"The forward GRU reads the input sentence word embeddings from left to right and gets a sequence of hidden states , ( h 1 , h 2 , . . . , h n ) .",Model,Sentence Encoder,text_summarization,12,11,1,0,,0.000311785,0,negative,7.23E-06,5.52E-05,0.000510798,6.47E-08,1.28E-07,4.86E-06,4.97E-06,9.71E-05,0.008482779,0.990794384,1.19E-05,2.91E-05,1.39E-06
2512,text_summarization12,92,"The backward GRU reads the input sentence embeddings reversely , from right to left , and results in another sequence of hidden states , ( h 1 , h 2 , . . . , h n ) :",Model,Sentence Encoder,text_summarization,12,12,1,0,,5.37E-05,0,negative,5.30E-06,8.74E-06,0.000171014,1.04E-08,3.31E-08,9.65E-07,1.33E-06,1.72E-05,0.000755995,0.999010193,1.33E-06,2.78E-05,1.51E-07
2513,text_summarization12,93,"The initial states of the BiGRU are set to zero vectors , i.e. , h 1 = 0 and h n =",Model,Sentence Encoder,text_summarization,12,13,1,0,,0.001399016,0,negative,1.14E-05,4.92E-05,9.09E-06,1.67E-07,1.84E-07,0.000189514,5.51E-05,0.012135647,0.000624187,0.986858779,2.07E-06,6.32E-05,1.49E-06
2514,text_summarization12,94,"0 . After reading the sentence , the forward and backward hidden states are concatenated , i.e. , hi = [ hi ; hi ] , to get the basic sentence representation .",Model,Sentence Encoder,text_summarization,12,14,1,0,,6.51E-05,0,negative,7.14E-06,4.54E-06,3.53E-05,1.78E-08,5.45E-08,1.73E-06,1.33E-06,3.22E-05,0.000213787,0.999661459,2.97E-07,4.20E-05,1.24E-07
2515,text_summarization12,95,Selective Mechanism,Model,,text_summarization,12,15,1,0,,0.001156003,0,negative,0.000330184,0.00025778,0.00025404,2.64E-05,2.12E-05,0.000343381,0.000417978,0.002021522,0.023531748,0.970450416,0.000127334,0.002058739,0.000159292
2516,text_summarization12,96,"In the sequence - to - sequence machine translation ( MT ) model , the encoder and decoder are responsible for mapping input sentence information to a list of vectors and decoding the sentence representation vectors to generate an output sentence .",Model,Selective Mechanism,text_summarization,12,16,1,0,,0.009007829,0,negative,2.25E-05,0.001399259,0.000233234,5.34E-06,5.73E-06,3.37E-05,4.64E-05,0.000379502,0.004794502,0.985003041,0.007976697,8.37E-05,1.64E-05
2517,text_summarization12,97,Some previous works apply this framework to summarization generation tasks .,Model,Selective Mechanism,text_summarization,12,17,1,0,,2.87E-05,0,negative,6.01E-06,7.99E-05,7.67E-06,6.81E-07,5.25E-07,1.24E-05,1.21E-05,0.000159486,0.000106411,0.997422978,0.002156072,3.45E-05,1.25E-06
2518,text_summarization12,98,"However , abstractive sentence summarization is different from MT in two ways .",Model,Selective Mechanism,text_summarization,12,18,1,0,,0.00015316,0,negative,6.62E-05,0.000229131,4.34E-05,6.30E-06,6.35E-06,1.63E-05,0.000196969,0.000104749,7.59E-05,0.891983313,0.106254886,0.000998244,1.83E-05
2519,text_summarization12,99,"First , there is no explicit alignment relationship between the input sentence and the output summary except for the common words .",Model,Selective Mechanism,text_summarization,12,19,1,0,,2.22E-05,0,negative,6.20E-05,0.000106659,1.38E-05,5.39E-07,4.21E-06,3.25E-06,1.34E-06,3.34E-05,0.000124959,0.999604617,9.07E-06,3.60E-05,1.40E-07
2520,text_summarization12,100,"Second , summarization task needs to keep the highlights and remove the unnecessary information , while MT needs to keep all information literally .",Model,Selective Mechanism,text_summarization,12,20,1,0,,2.44E-05,0,negative,1.00E-05,3.66E-05,3.24E-06,3.15E-06,1.15E-06,7.56E-06,1.09E-05,6.80E-05,4.23E-05,0.996523092,0.003230951,6.07E-05,2.21E-06
2521,text_summarization12,101,"Herein , we propose a selective mechanism to model the selection process for abstractive sentence summarization .",Model,Selective Mechanism,text_summarization,12,21,1,0,,0.001829621,0,negative,0.000322142,0.023449779,0.001564261,2.63E-06,4.24E-05,2.55E-05,5.37E-05,0.000408122,0.051144109,0.921122681,0.001191117,0.000665155,8.34E-06
2522,text_summarization12,102,The selective mechanism extends the sequence - to - sequence model by constructing a tailored representation for abstractive sentence summarization task .,Model,Selective Mechanism,text_summarization,12,22,1,0,,0.001878475,0,negative,7.64E-05,0.001278424,0.002986281,1.10E-07,3.16E-06,5.37E-06,1.01E-05,5.81E-05,0.038696951,0.956769625,2.54E-05,8.90E-05,1.12E-06
2523,text_summarization12,103,"Concretely , the selective gate network in our model takes two vector inputs , the sentence word vector hi and the sentence representation vector s .",Model,Selective Mechanism,text_summarization,12,23,1,0,,2.29E-05,0,negative,7.43E-06,0.000298344,4.57E-05,1.04E-07,8.11E-07,7.51E-06,2.14E-06,0.000192071,0.016689894,0.982747787,3.86E-06,3.98E-06,4.06E-07
2524,text_summarization12,104,The sentence word vector hi is the output of the BiGRU encoder and represents the meaning and context information of word x i .,Model,Selective Mechanism,text_summarization,12,24,1,0,,3.49E-06,0,negative,2.53E-06,5.42E-05,7.52E-06,2.17E-08,2.20E-07,2.54E-06,7.62E-07,7.57E-05,0.001481181,0.998372371,5.69E-07,2.40E-06,6.73E-08
2525,text_summarization12,105,The sentence vector sis used to represent the meaning of the sentence .,Model,Selective Mechanism,text_summarization,12,25,1,0,,3.98E-05,0,negative,3.75E-06,0.000141801,2.61E-05,4.51E-08,4.76E-07,9.99E-06,2.24E-06,0.000326292,0.004315721,0.995168924,1.22E-06,3.20E-06,2.50E-07
2526,text_summarization12,106,"For each word x i , the selective gate network generates agate vector s Gate i using hi and s , then the tailored representation is constructed , i.e. , hi .",Model,Selective Mechanism,text_summarization,12,26,1,0,,3.43E-06,0,negative,7.82E-06,3.13E-05,1.89E-05,1.03E-08,3.03E-07,7.33E-07,4.92E-07,1.34E-05,0.000501612,0.999415463,3.85E-07,9.54E-06,3.44E-08
2527,text_summarization12,107,"In detail , we concatenate the last forward hidden state h n and backward hidden state h 1 as the sentence representation s:",Model,Selective Mechanism,text_summarization,12,27,1,0,,1.29E-06,0,negative,6.36E-06,2.98E-05,1.64E-05,1.25E-08,2.45E-07,1.13E-06,5.60E-07,1.84E-05,0.000518225,0.999403413,2.73E-07,5.13E-06,4.22E-08
2528,text_summarization12,108,"For each time step i , the selective gate takes the sentence representation sand BiGRU hidden hi as inputs to compute the gate vector s Gate i :",Model,Selective Mechanism,text_summarization,12,28,1,0,,4.45E-06,0,negative,1.58E-05,4.87E-05,6.29E-05,7.62E-09,2.49E-07,7.15E-07,6.20E-07,1.14E-05,0.000772839,0.999072222,3.23E-07,1.42E-05,3.51E-08
2529,text_summarization12,109,"where W sand U s are weight matrices , b is the bias vector , ? denotes sigmoid activation function , and is element - wise multiplication .",Model,Selective Mechanism,text_summarization,12,29,1,0,,1.37E-06,0,negative,4.83E-06,2.79E-05,2.36E-06,1.08E-07,4.05E-07,6.39E-06,1.61E-06,0.000198141,0.000223328,0.999530126,5.37E-07,4.13E-06,1.38E-07
2530,text_summarization12,110,"After the selective gate network , we obtain another sequence of vectors ( h 1 , h 2 , . . . , h n ) .",Model,Selective Mechanism,text_summarization,12,30,1,0,,3.17E-06,0,negative,6.05E-06,1.43E-05,4.63E-06,8.03E-09,2.09E-07,1.08E-06,7.61E-07,1.83E-05,0.000339203,0.999608002,1.97E-07,7.13E-06,3.45E-08
2531,text_summarization12,111,This new sequence is then used as the input sentence representation for the decoder to generate the summary .,Model,Selective Mechanism,text_summarization,12,31,1,0,,3.37E-05,0,negative,1.51E-06,7.64E-05,1.38E-05,1.34E-08,3.66E-07,1.42E-06,1.02E-06,4.09E-05,0.003160216,0.996699632,8.37E-07,3.82E-06,1.41E-07
2532,text_summarization12,112,Summary Decoder,Model,,text_summarization,12,32,1,0,,0.011991231,0,negative,0.000225364,4.01E-05,0.000158847,3.99E-06,7.65E-06,7.27E-05,0.000265029,0.000457048,0.009128937,0.987767989,7.34E-06,0.001771861,9.32E-05
2533,text_summarization12,113,"On top of the sentence encoder and the selective gate network , we use GRU with attention as the decoder to produce the output summary .",Model,Summary Decoder,text_summarization,12,33,1,0,,0.017566077,0,negative,7.64E-05,0.001021938,0.003905363,4.35E-07,5.34E-06,2.02E-05,9.01E-05,0.000442288,0.075434268,0.918782096,2.51E-06,0.000202579,1.66E-05
2534,text_summarization12,114,"At each decoding time step t , the GRU reads the previous word embedding w t?1 and previous context vector c t ?1 as inputs to compute the new hidden state st .",Model,Summary Decoder,text_summarization,12,34,1,0,,0.000110066,0,negative,3.50E-06,2.92E-05,3.85E-05,1.94E-08,1.68E-07,1.17E-06,2.43E-06,3.91E-05,0.002747389,0.997123185,4.57E-07,1.45E-05,4.25E-07
2535,text_summarization12,115,"To initialize the GRU hidden state , we use a linear layer with the last backward encoder hidden state h 1 as input :",Model,Summary Decoder,text_summarization,12,35,1,0,,9.10E-05,0,negative,1.08E-05,4.70E-05,1.95E-05,1.78E-07,6.19E-07,1.93E-05,1.45E-05,0.001046617,0.0027272,0.996091217,3.19E-07,2.08E-05,1.88E-06
2536,text_summarization12,116,where W d is the weight matrix and b is the bias vector .,Model,Summary Decoder,text_summarization,12,36,1,0,,6.38E-06,0,negative,6.22E-06,5.40E-06,4.85E-06,5.58E-08,1.52E-07,2.18E-06,2.62E-06,8.75E-05,0.000211436,0.999652173,1.47E-07,2.68E-05,4.51E-07
2537,text_summarization12,117,"The context vector ct for current time step t is computed through the concatenate attention mechanism , which matches the current decoder state st with each encoder hidden state hi to get an importance score .",Model,Summary Decoder,text_summarization,12,37,1,0,,5.10E-05,0,negative,4.36E-06,4.26E-05,1.90E-05,2.56E-08,2.34E-07,2.32E-06,2.47E-06,0.000140798,0.003769098,0.996008927,1.36E-07,9.60E-06,4.05E-07
2538,text_summarization12,118,The importance scores are then normalized to get the current context vector by weighted sum :,Model,Summary Decoder,text_summarization,12,38,1,0,,5.13E-05,0,negative,8.65E-06,6.67E-06,2.69E-05,5.17E-09,7.56E-08,5.65E-07,9.26E-07,1.67E-05,0.00050143,0.99942207,2.55E-08,1.59E-05,7.31E-08
2539,text_summarization12,119,"We then combine the previous word embedding w t ?1 , the current context vector ct , and the decoder state st to construct the readout state rt .",Model,Summary Decoder,text_summarization,12,39,1,0,,3.67E-05,0,negative,2.75E-06,6.91E-06,1.65E-05,2.17E-08,1.14E-07,1.05E-06,1.57E-06,3.04E-05,0.001430121,0.998503273,8.88E-08,6.82E-06,3.98E-07
2540,text_summarization12,120,The readout state is then passed through a maxout hidden layer to predict the next word with a softmax layer over the decoder vocabulary .,Model,Summary Decoder,text_summarization,12,40,1,0,,0.000222821,0,negative,4.33E-06,1.74E-05,3.23E-05,3.29E-08,1.71E-07,3.02E-06,3.64E-06,0.00012565,0.007257499,0.992548434,1.37E-07,6.46E-06,9.80E-07
2541,text_summarization12,121,"where W a , U a , W r , Ur , V rand W o are weight matrices .",Model,Summary Decoder,text_summarization,12,41,1,0,,8.70E-06,0,negative,2.55E-06,2.66E-06,1.95E-06,2.86E-08,1.02E-07,1.35E-06,1.25E-06,5.93E-05,0.000132644,0.999783028,2.75E-08,1.50E-05,1.56E-07
2542,text_summarization12,122,"Readout state rt is a 2 d - dimensional vector , and the maxout layer ( Equation 16 ) picks the max value for every two numbers in rt and produces a d-dimensional vector mt .",Model,Summary Decoder,text_summarization,12,42,1,0,,4.62E-05,0,negative,1.46E-06,2.14E-05,1.36E-05,2.79E-08,2.25E-07,5.17E-06,5.75E-06,0.000226107,0.002302723,0.997413272,1.89E-07,9.28E-06,8.47E-07
2543,text_summarization12,123,Objective Function,Model,,text_summarization,12,43,1,0,,0.001414549,0,negative,1.72E-05,8.64E-05,4.70E-05,8.71E-07,2.40E-06,4.42E-05,9.72E-05,0.001995102,0.006399335,0.990793702,5.16E-06,0.000475661,3.58E-05
2544,text_summarization12,124,Our goal is to maximize the output summary probability given the input sentence .,Model,Objective Function,text_summarization,12,44,1,0,,2.41E-05,0,negative,7.86E-06,0.000150543,2.31E-05,2.73E-07,1.04E-06,1.06E-05,4.17E-06,0.002012848,0.000468235,0.997055118,3.12E-06,0.000256531,6.51E-06
2545,text_summarization12,125,"Therefore , we optimize the negative log - likelihood loss function :",Model,Objective Function,text_summarization,12,45,1,0,,1.03E-05,0,negative,2.49E-05,1.57E-05,2.30E-05,1.40E-08,2.38E-07,2.73E-06,9.60E-07,0.000394697,6.59E-05,0.999367754,1.34E-08,0.000103841,2.66E-07
2546,text_summarization12,126,where D denotes a set of parallel sentencesummary pairs and ?,Model,Objective Function,text_summarization,12,46,1,0,,3.18E-07,0,negative,1.45E-06,1.19E-06,3.08E-06,3.88E-09,4.28E-08,5.95E-07,2.93E-07,6.05E-05,4.04E-06,0.999877284,2.90E-08,5.14E-05,8.86E-08
2547,text_summarization12,127,is the model parameter .,Model,Objective Function,text_summarization,12,47,1,0,,3.57E-05,0,negative,4.98E-06,2.37E-05,2.98E-06,4.29E-07,4.33E-07,4.83E-05,7.78E-06,0.02074773,0.000116018,0.978982441,2.21E-07,6.01E-05,4.90E-06
2548,text_summarization12,128,We use Stochastic Gradient Descent ( SGD ) with minibatch to learn the model parameter ?.,Model,Objective Function,text_summarization,12,48,1,0,,0.027621878,0,negative,2.67E-05,0.000394454,6.00E-05,4.37E-06,4.36E-06,0.002113255,0.000301354,0.482663504,0.000725491,0.51349798,3.10E-07,0.000106385,0.000101841
2549,text_summarization12,129,Experiments,,,text_summarization,12,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
2550,text_summarization12,130,"In this section we introduce the dataset we use , the evaluation metric , the implementation details , the baselines we compare to , and the performance of our system .",Experiments,Experiments,text_summarization,12,1,1,0,,0.000409484,0,negative,7.74E-05,0.000150131,8.67E-05,2.17E-05,4.19E-05,0.001653289,0.000503261,0.010019367,2.45E-05,0.98700144,6.38E-05,0.000306324,5.02E-05
2551,text_summarization12,131,Dataset,Experiments,,text_summarization,12,2,1,0,,0.001227854,0,negative,0.000149424,2.80E-05,0.000120755,9.94E-05,1.11E-05,0.010739737,0.00300146,0.036189362,5.44E-05,0.947360084,0.00060668,0.001136078,0.000503635
2552,text_summarization12,132,Training Set,,,text_summarization,12,0,1,0,,0.002725553,0,negative,8.59E-06,0.000435388,1.61E-05,4.98E-07,1.23E-06,0.000215904,5.62E-05,0.014548482,0.000181702,0.983504828,0.000940822,8.69E-05,3.33E-06
2553,text_summarization12,133,"For our training set , we use a parallel corpus which is constructed from the Annotated English Gigaword dataset as mentioned in .",Training Set,Training Set,text_summarization,12,1,1,0,,0.157723818,0,negative,0.000102859,0.000225148,0.003293241,1.83E-05,8.32E-05,0.006040498,0.003087738,0.059000799,2.60E-05,0.927346442,1.25E-05,0.000697596,6.58E-05
2554,text_summarization12,134,The parallel corpus is produced by pairing the first sentence and the headline in the news article with some heuristic rules .,Training Set,Training Set,text_summarization,12,2,1,0,,0.080724755,0,negative,6.36E-05,7.30E-05,0.001260818,5.38E-05,0.000113996,0.010863273,0.002214153,0.057498203,2.25E-05,0.927592414,6.01E-06,0.000171788,6.64E-05
2555,text_summarization12,135,We use the script 1 released by to pre-process and extract the training and development datasets .,Training Set,Training Set,text_summarization,12,3,1,0,,0.052246161,0,negative,2.02E-05,2.45E-05,5.86E-05,8.41E-05,5.27E-06,0.04233685,0.000668277,0.352638233,2.16E-05,0.60407415,4.70E-06,2.37E-05,3.99E-05
2556,text_summarization12,136,"For the test set , we use the same randomly heldout test set of 2000 sentence - summary pairs as .",Training Set,Training Set,text_summarization,12,4,1,0,,0.049689468,0,negative,5.02E-05,4.80E-05,0.000483293,3.91E-06,2.11E-05,0.002633323,0.00140771,0.04034535,6.05E-06,0.954414711,3.13E-06,0.000559397,2.39E-05
2557,text_summarization12,137,"We also find that except for the empty titles , this test set has some invalid lines like the input sentence containing only one word .",Training Set,Training Set,text_summarization,12,5,1,0,,0.036196809,0,negative,0.001299987,2.79E-06,5.77E-05,7.06E-06,7.39E-06,0.001163316,0.000905066,0.005235107,6.76E-07,0.976384021,3.40E-06,0.014906152,2.74E-05
2558,text_summarization12,138,"Therefore , we further sample 2000 pairs as our internal test set and release it for future works 3 .",Training Set,Training Set,text_summarization,12,6,1,0,,0.044592898,0,negative,4.60E-05,2.36E-05,7.10E-05,5.68E-07,1.95E-06,0.003311595,0.000161073,0.100464202,8.50E-06,0.895829559,5.42E-07,7.52E-05,6.23E-06
2559,text_summarization12,139,DUC 2004,Training Set,Training Set,text_summarization,12,7,1,1,results,0.37876521,0,negative,0.000277193,5.85E-05,0.008255338,2.34E-05,1.12E-05,0.007347245,0.167009441,0.072568773,1.20E-05,0.374651028,0.001753774,0.363064149,0.004967972
2560,text_summarization12,140,Test Set,Training Set,,text_summarization,12,8,1,0,,0.057321196,0,negative,1.36E-05,9.35E-06,0.000365844,1.27E-07,2.88E-07,0.001390167,0.001051909,0.03830331,2.91E-06,0.955847657,1.26E-05,0.002987962,1.43E-05
2561,text_summarization12,141,We employ DUC 2004 data for tasks 1 & 2 in our experiments as one of the test sets since it is too small to train a neural network model on .,Training Set,Test Set,text_summarization,12,9,1,0,,0.010339349,0,negative,3.31E-05,7.28E-07,0.000473532,2.84E-07,4.58E-07,0.000172135,0.000109362,0.001461449,2.21E-07,0.997157191,2.07E-07,0.000568908,2.24E-05
2562,text_summarization12,142,The dataset pairs each document with 4 different human - written reference summaries which are capped at 75 bytes .,Training Set,Test Set,text_summarization,12,10,1,0,,0.129962589,0,negative,0.000107602,1.27E-06,0.004713699,8.79E-06,6.44E-06,0.000407716,0.000201626,0.000766465,8.48E-07,0.993236404,2.89E-07,0.000407579,0.000141266
2563,text_summarization12,143,It has 500 input sentences with each sentence paired with 4 summaries .,Training Set,Test Set,text_summarization,12,11,1,0,,0.204607646,0,negative,6.89E-05,2.72E-06,0.006064008,4.05E-06,3.07E-06,0.001339376,0.000836082,0.006854785,1.85E-06,0.982848226,8.01E-07,0.001218078,0.000758103
2564,text_summarization12,144,release a new dataset for sentence summarization task by crowdsourcing .,Training Set,Test Set,text_summarization,12,12,1,0,,0.032209021,0,negative,0.000276495,6.70E-07,0.002163443,1.36E-05,1.00E-06,0.000291169,0.000497014,0.000973346,1.05E-06,0.981842677,2.75E-05,0.011028223,0.002883848
2565,text_summarization12,145,"This dataset contains approximately 6,000 source text sentences with multiple manually - created summaries ( about 26,000 sentence - summary pairs in total ) .",Training Set,Test Set,text_summarization,12,13,1,0,,0.123041485,0,negative,0.000155314,1.67E-06,0.002872386,2.88E-05,2.93E-05,0.000362067,0.000279372,0.000559335,8.79E-07,0.993968877,5.88E-07,0.00125894,0.000482552
2566,text_summarization12,146,"provide a standard split of the data into training , development , and test sets , with 4,936 , 448 and 785 input sentences respectively .",Training Set,Test Set,text_summarization,12,14,1,0,,0.007078705,0,negative,1.97E-05,8.09E-08,0.000271653,2.75E-07,3.40E-07,6.14E-05,1.88E-05,0.000252772,8.87E-08,0.999179812,2.76E-08,0.000182038,1.30E-05
2567,text_summarization12,147,"Since the training set is too small , we only use the test set as one of our test sets .",Training Set,Test Set,text_summarization,12,15,1,0,,0.007271355,0,negative,0.000135276,9.13E-07,0.000498445,9.16E-08,2.15E-07,3.92E-05,1.81E-05,0.000986751,2.71E-07,0.997841022,3.83E-08,0.000474763,4.97E-06
2568,text_summarization12,148,We denote this dataset as MSR - ATC ( Microsoft Research Abstractive Text Compression ) test set in the following .,Training Set,Test Set,text_summarization,12,16,1,0,,0.271059588,0,negative,2.54E-05,7.90E-07,0.007229871,1.19E-07,3.70E-07,4.10E-05,0.000181568,0.000266935,1.95E-07,0.987693025,4.52E-07,0.004510699,4.96E-05
2569,text_summarization12,149,summarizes the statistic information of the three datasets we used .,Training Set,Test Set,text_summarization,12,17,1,0,,0.006165986,0,negative,2.01E-05,4.17E-08,5.87E-05,1.75E-07,5.78E-08,5.23E-05,8.78E-06,0.00037735,1.21E-07,0.999242423,3.04E-08,0.000232215,7.69E-06
2570,text_summarization12,150,MSR - ATC,Training Set,Test Set,text_summarization,12,18,1,0,,0.141249941,0,negative,0.000642937,1.30E-06,0.129310865,4.98E-06,9.98E-07,0.000696654,0.002645241,0.002022405,4.58E-06,0.793562269,8.44E-06,0.0657015,0.005397832
2571,text_summarization12,151,Test Set,Training Set,,text_summarization,12,19,1,0,,0.082347366,0,negative,1.48E-05,7.89E-06,0.000283136,1.10E-07,4.24E-07,0.000893092,0.001098034,0.028702065,3.01E-06,0.964566045,3.43E-06,0.004408756,1.92E-05
2572,text_summarization12,152,Evaluation Metric,,,text_summarization,12,0,1,0,,0.001623015,0,negative,7.01E-06,6.67E-05,6.21E-06,1.46E-07,2.24E-07,6.28E-05,5.24E-05,0.001698319,3.66E-05,0.993103832,0.004752658,0.000211489,1.68E-06
2573,text_summarization12,153,We employ ROUGE as our evaluation metric .,Evaluation Metric,Evaluation Metric,text_summarization,12,1,1,0,,0.006954653,0,negative,8.16E-06,5.42E-06,8.78E-05,4.73E-08,4.33E-08,0.000259483,3.67E-05,0.005219127,1.26E-06,0.994200359,1.39E-06,0.000179513,6.42E-07
2574,text_summarization12,154,"ROUGE measures the quality of summary by computing overlapping lexical units , such as unigram , bigram , trigram , and longest common subsequence ( LCS ) .",Evaluation Metric,Evaluation Metric,text_summarization,12,2,1,0,,0.228015356,0,negative,7.81E-05,1.44E-05,0.01286057,3.20E-06,1.01E-06,0.000291601,0.00211096,0.001336085,2.62E-06,0.971238827,0.003375203,0.008551885,0.000135506
2575,text_summarization12,155,It becomes the standard evaluation metric for DUC shared tasks and popular for summarization evaluation .,Evaluation Metric,Evaluation Metric,text_summarization,12,3,1,0,,0.008817435,0,negative,1.37E-05,2.56E-06,0.000143631,9.32E-07,8.35E-08,6.37E-05,0.000154827,0.000667976,1.03E-06,0.996639983,0.001517244,0.000778042,1.63E-05
2576,text_summarization12,156,"Following previous work , we use ROUGE - 1 ( unigram ) , ROUGE - 2 ( bi - gram ) and ROUGE - L ( LCS ) as the evaluation metrics in the reported experimental results .",Evaluation Metric,Evaluation Metric,text_summarization,12,4,1,0,,0.003074757,0,negative,1.87E-06,9.26E-07,3.25E-05,1.23E-08,1.14E-08,4.31E-05,9.58E-06,0.000669028,2.90E-07,0.999158946,1.68E-06,8.20E-05,1.85E-07
2577,text_summarization12,157,Data Set,Evaluation Metric,,text_summarization,12,5,1,0,,4.36E-05,0,negative,3.08E-06,1.31E-07,6.98E-05,7.90E-09,6.13E-09,1.94E-05,1.95E-05,0.000170306,1.78E-07,0.999236885,2.08E-06,0.000478156,5.15E-07
2578,text_summarization12,158,Implementation Details,,,text_summarization,12,0,1,0,,0.045796543,0,negative,4.47E-05,0.001220595,9.47E-06,3.20E-05,1.55E-05,0.000465852,0.000169332,0.007430528,0.000344669,0.986360185,0.003666023,0.000225373,1.58E-05
2579,text_summarization12,159,Model Parameters,,,text_summarization,12,0,1,0,,0.003198277,0,negative,2.98E-05,0.000650471,2.01E-05,2.57E-06,1.89E-06,0.000697696,0.000104692,0.019956979,0.000976383,0.976362224,0.001081142,0.000107468,8.57E-06
2580,text_summarization12,160,"The input and output vocabularies are collected from the training data , which have 119,504 and 68,883 word types respectively .",Model Parameters,Model Parameters,text_summarization,12,1,1,0,,0.21137629,0,negative,4.63E-05,0.00019251,0.00022091,1.42E-05,3.42E-05,0.020603637,0.000433379,0.291117412,9.92E-05,0.687098707,5.97E-06,0.000119658,1.39E-05
2581,text_summarization12,161,We set the word embedding size to 300 and all GRU hidden state sizes to 512 .,Model Parameters,Model Parameters,text_summarization,12,2,1,0,,0.981625097,1,hyperparameters,6.67E-06,4.11E-05,1.65E-05,1.68E-06,2.87E-07,0.029877696,0.000272551,0.961264441,3.67E-05,0.008453897,6.20E-06,1.06E-05,1.16E-05
2582,text_summarization12,162,We use dropout with probability p = 0.5 .,Model Parameters,Model Parameters,text_summarization,12,3,1,0,,0.969817417,1,hyperparameters,1.17E-05,6.42E-05,2.82E-05,7.04E-06,6.66E-07,0.039055315,0.000378146,0.954363273,5.14E-05,0.006005349,4.24E-06,9.39E-06,2.11E-05
2583,text_summarization12,163,Model Training,,,text_summarization,12,0,1,0,,0.040316081,0,negative,0.000796383,0.003292277,0.000530447,0.000329045,0.000173516,0.00370929,0.005741766,0.049118948,0.001813108,0.9034652,0.025859436,0.004515954,0.00065463
2584,text_summarization12,164,We initialize model parameters randomly using a Gaussian distribution with Xavier scheme .,Model Training,Model Training,text_summarization,12,1,1,1,hyperparameters,0.24070396,0,hyperparameters,4.07E-05,8.81E-05,1.84E-05,2.57E-06,9.53E-07,0.308807228,0.003506205,0.550127384,0.000382564,0.136978448,7.10E-06,9.08E-06,3.13E-05
2585,text_summarization12,165,We use Adam as our optimizing algorithm .,Model Training,Model Training,text_summarization,12,2,1,1,hyperparameters,0.692156434,1,hyperparameters,3.44E-05,9.08E-05,1.47E-05,3.68E-06,1.07E-06,0.333580653,0.005432571,0.575893819,0.000336769,0.084533527,1.12E-05,9.58E-06,5.72E-05
2586,text_summarization12,166,"For the hyperparameters of Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .",Model Training,Model Training,text_summarization,12,3,1,1,hyperparameters,0.528020488,1,hyperparameters,4.04E-05,3.76E-05,6.76E-06,1.01E-05,2.31E-06,0.439600251,0.007753868,0.49534636,6.75E-05,0.057033674,5.25E-06,2.04E-05,7.56E-05
2587,text_summarization12,167,"During training , we test the model performance ( ROUGE - 2 F1 ) on development set for every 2,000 batches .",Model Training,Model Training,text_summarization,12,4,1,1,hyperparameters,0.001938455,0,negative,0.000206916,0.000105755,2.70E-05,1.44E-06,4.37E-06,0.051012687,0.006259785,0.085195998,0.000128566,0.856894286,6.86E-06,0.0001267,2.97E-05
2588,text_summarization12,168,We halve the Adam learning rate ? if the ROUGE - 2 F1 score drops for twelve consecutive tests on development set .,Model Training,Model Training,text_summarization,12,5,1,0,,0.008524222,0,negative,0.000171272,4.22E-05,1.97E-05,3.82E-06,2.03E-06,0.135966135,0.005066058,0.180776714,0.000161011,0.677634605,2.46E-05,7.03E-05,6.16E-05
2589,text_summarization12,169,"We also apply gradient clipping with range [ ? 5 , 5 ] during training .",Model Training,Model Training,text_summarization,12,6,1,1,hyperparameters,0.589700068,1,hyperparameters,0.000693571,0.000479261,0.000207422,2.51E-05,1.54E-05,0.407178377,0.029835018,0.483079171,0.000766294,0.077326689,6.52E-06,9.99E-05,0.000287251
2590,text_summarization12,170,"To both speedup the training and converge quickly , we use mini-batch size 64 by grid search .",Model Training,Model Training,text_summarization,12,7,1,1,hyperparameters,0.629616685,1,hyperparameters,7.33E-05,7.25E-05,1.91E-05,1.12E-05,3.35E-06,0.45326882,0.018197064,0.480361462,0.000124372,0.047597471,6.87E-06,3.19E-05,0.000232613
2591,text_summarization12,171,Beam Search,Model Training,,text_summarization,12,8,1,0,,0.748210782,1,negative,0.003092193,0.000137166,0.006558538,0.000199733,0.000139721,0.120287097,0.36323255,0.021631416,0.000757992,0.464742808,0.00052453,0.014359652,0.004336606
2592,text_summarization12,172,We use beam search to generate multiple summary candidates to get better results .,Model Training,Beam Search,text_summarization,12,9,1,0,,0.81606022,1,negative,0.00014089,0.000184728,0.271982188,1.51E-06,3.80E-06,0.00883412,0.003457494,0.010217434,0.001633675,0.703400986,1.03E-05,5.51E-05,7.78E-05
2593,text_summarization12,173,"To avoid favoring shorter outputs , we average the ranking score along the beam path by dividing it by the number of generated words .",Model Training,Beam Search,text_summarization,12,10,1,0,,0.008511092,0,negative,8.55E-05,1.14E-05,0.009810067,6.17E-08,3.22E-07,0.001270339,0.000363405,0.001771942,7.45E-05,0.98657814,7.21E-07,3.14E-05,2.19E-06
2594,text_summarization12,174,"To both decode fast and get better results , we set the beam size to 12 in our experiments .",Model Training,Beam Search,text_summarization,12,11,1,0,,0.931417113,1,negative,7.35E-05,1.96E-05,0.000280511,3.13E-06,1.33E-06,0.244477129,0.017522993,0.358414922,3.50E-05,0.378966708,5.25E-06,7.92E-05,0.000120679
2595,text_summarization12,175,Baseline,,,text_summarization,12,0,1,0,,0.002222059,0,negative,0.000119302,0.000103989,5.37E-05,1.30E-06,2.04E-06,0.000110759,0.000377266,0.001181693,0.000104203,0.990679366,0.002532326,0.004720987,1.30E-05
2596,text_summarization12,176,We compare SEASS model with the following state - of - the - art baselines : use an attentive CNN encoder and NNLM decoder to do the sentence summarization task .,Baseline,Baseline,text_summarization,12,1,1,0,,0.648452592,1,baselines,0.000104572,0.000541264,0.642046215,1.10E-06,2.68E-06,5.72E-05,0.001202996,0.001099573,0.000105012,0.353067846,0.000156886,0.001601477,1.32E-05
2597,text_summarization12,177,We trained this baseline model with the released code 1 and evaluate it with our internal English Gigaword test set and MSR - ATC test set .,Baseline,Baseline,text_summarization,12,2,1,0,,0.061119189,0,negative,6.19E-05,8.63E-05,0.041414719,1.13E-06,1.49E-06,0.000264553,0.0009158,0.00427891,5.25E-05,0.952358025,1.04E-05,0.000547366,7.00E-06
2598,text_summarization12,178,"ABS + Based on ABS model , further with two - layer LSTMs for the encoder - decoder with 500 hidden units in each layer implemented in .",Baseline,Baseline,text_summarization,12,3,1,1,baselines,0.933573809,1,baselines,6.11E-05,3.82E-05,0.928838573,1.00E-05,2.82E-06,0.000623319,0.011407013,0.002667334,6.35E-05,0.05514191,8.48E-05,0.000866976,0.000194397
2599,text_summarization12,179,s 2 s+ att,Baseline,Baseline,text_summarization,12,4,1,1,baselines,0.001277554,0,negative,0.000447529,2.30E-05,0.008931173,4.67E-06,1.25E-06,0.000107909,0.000449604,0.001039097,9.07E-05,0.986861347,8.61E-05,0.00192437,3.33E-05
2600,text_summarization12,180,"We also implement a sequence - to sequence model with attention as our baseline and denote it as "" s2 s + att "" .",Baseline,Baseline,text_summarization,12,5,1,1,baselines,0.20090751,0,baselines,5.06E-05,8.03E-05,0.909670253,2.52E-07,4.27E-07,2.70E-05,0.000314749,0.000393458,0.000263646,0.089071078,1.14E-05,0.000111073,5.69E-06
2601,text_summarization12,181,Results,,,text_summarization,12,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
2602,text_summarization12,182,"We report ROUGE F1 , ROUGE recall and ROUGE F1 for English Gigaword , DUC 2004 and MSR - ATC test sets respectively .",Results,Results,text_summarization,12,1,1,0,,0.08144383,0,negative,0.003108362,2.24E-05,0.00034752,3.58E-06,7.83E-06,4.41E-05,0.002631595,0.000228748,4.02E-06,0.650355248,6.65E-05,0.342996126,0.000183958
2603,text_summarization12,183,We use the official ROUGE script ( version 1.5.5 ) 4 to evaluate the summarization quality in our experiments .,Results,Results,text_summarization,12,2,1,0,,0.823329214,1,negative,0.001918242,5.48E-05,0.000819082,2.43E-05,2.72E-05,0.000960475,0.003178127,0.003519382,3.22E-05,0.968112941,4.94E-05,0.020738768,0.000564988
2604,text_summarization12,184,"For English Gigaword 5 and MSR - ATC 6 test sets , the outputs have different lengths so we evaluate the system with F1 metric .",Results,Results,text_summarization,12,3,1,0,,0.057402626,0,negative,0.007204327,1.06E-05,0.001905358,1.45E-06,4.74E-06,6.77E-05,0.002094627,0.000229734,3.79E-06,0.795539018,1.27E-05,0.192863263,6.28E-05
2605,text_summarization12,185,"As for the DUC 2004 test set 7 , the task requires the system to produce a fixed length summary ( 75 bytes ) , therefore we employ ROUGE recall as the evaluation metric .",Results,Results,text_summarization,12,4,1,0,,0.036875986,0,negative,0.002356695,9.74E-06,0.000684209,1.97E-06,6.41E-06,5.98E-05,0.000921116,0.000216126,3.87E-06,0.950750763,8.53E-06,0.04492652,5.42E-05
2606,text_summarization12,186,"To satisfy the length requirement , we decode the output summary to a roughly expected length following .",Results,Results,text_summarization,12,5,1,0,,0.196059587,0,negative,0.104638325,0.000621522,0.03172667,7.25E-05,0.000111973,0.000592877,0.002125358,0.004025653,0.001613176,0.777334654,6.68E-05,0.075541607,0.001528891
2607,text_summarization12,187,English Gigaword,Results,,text_summarization,12,6,1,0,,0.026221203,0,results,0.027245463,1.68E-05,0.003926572,0.001028259,0.000105272,0.00098503,0.041345331,0.000712295,4.94E-05,0.200632421,0.000496025,0.665468651,0.057988489
2608,text_summarization12,188,"We acquire the test set from so we can make fair comparisons to the baselines . In , we report the ROUGE F1 score of our model and the baseline methods .",Results,English Gigaword,text_summarization,12,7,1,0,,0.000209775,0,negative,0.000121373,1.95E-08,2.60E-05,9.28E-08,3.26E-08,1.38E-06,1.26E-05,2.73E-06,8.74E-08,0.98540115,4.27E-08,0.014393603,4.09E-05
2609,text_summarization12,189,Our SEASS model with beam search outperforms all baseline models by a large margin .,Results,English Gigaword,text_summarization,12,8,1,1,results,0.924331273,1,results,0.000331874,4.58E-09,9.42E-06,6.66E-08,2.22E-09,4.15E-07,0.000138389,8.06E-07,8.48E-09,0.008885464,1.18E-07,0.990208228,0.000425208
2610,text_summarization12,190,"Even for greedy search , our model still performs better than other methods which used beam search .",Results,English Gigaword,text_summarization,12,9,1,1,results,0.817625365,1,results,0.000349375,7.30E-09,4.46E-06,1.90E-07,4.09E-09,1.58E-06,0.000174093,3.86E-06,2.24E-08,0.030033234,3.02E-07,0.968511162,0.000921706
2611,text_summarization12,191,"For the popular ROUGE - 2 metric , our SEASS model achieves 17.54 F1 score and performs better than the previous works .",Results,English Gigaword,text_summarization,12,10,1,1,results,0.822957524,1,results,0.000137755,1.44E-09,2.21E-06,1.18E-08,7.52E-10,1.28E-07,9.80E-05,3.43E-07,1.42E-09,0.007519804,4.16E-08,0.992079028,0.000162684
2612,text_summarization12,192,"Compared to the ABS model , our model has a 6.22 ROUGE - 2 F1 relative gain .",Results,English Gigaword,text_summarization,12,11,1,1,results,0.772461845,1,results,0.002308176,8.19E-09,9.34E-06,8.06E-08,4.79E-09,4.01E-07,0.000152783,1.15E-06,1.88E-08,0.018956146,6.10E-08,0.978190112,0.000381712
2613,text_summarization12,193,"Compared to the highest CAs 2s baseline , our model achieves 1.57 ROUGE - 2 F1 improvement and passes the significant test according to the official ROUGE script .",Results,English Gigaword,text_summarization,12,12,1,1,results,0.80592435,1,results,0.000326655,3.06E-09,3.64E-06,4.36E-08,2.07E-09,2.19E-07,0.000159383,6.08E-07,4.93E-09,0.011154221,5.40E-08,0.987830511,0.00052465
2614,text_summarization12,194,summarizes our results on our internal test set using ROUGE F1 evaluation metrics .,Results,English Gigaword,text_summarization,12,13,1,0,,0.013180917,0,negative,0.00017095,8.34E-09,2.32E-05,5.88E-08,1.16E-08,1.14E-06,4.09E-05,2.38E-06,3.17E-08,0.784611297,1.02E-07,0.214952843,0.000197001
2615,text_summarization12,195,"The performance on our internal test set is comparable to our development set , which achieves 24.58 ROUGE - 2 F1 and outperforms the baselines .",Results,English Gigaword,text_summarization,12,14,1,0,,0.432086711,0,results,7.47E-05,2.23E-09,3.76E-06,2.24E-08,2.10E-09,3.13E-07,0.000149798,8.04E-07,3.63E-09,0.044418333,4.87E-08,0.955031804,0.000320399
2616,text_summarization12,196,DUC 2004,Results,English Gigaword,text_summarization,12,15,1,1,results,0.011723047,0,results,0.000157401,3.89E-08,0.00019279,3.35E-07,1.90E-08,3.37E-06,0.000830306,5.08E-06,8.41E-08,0.121122056,3.48E-06,0.864938713,0.01274632
2617,text_summarization12,197,We evaluate our model using the ROUGE recall score since the reference summaries of the DUC 2004 test set are capped at 75 bytes .,Results,English Gigaword,text_summarization,12,16,1,0,,0.00057601,0,negative,0.00011774,3.15E-08,1.22E-05,2.78E-08,1.02E-08,2.52E-06,3.98E-05,1.17E-05,7.93E-08,0.97188787,5.01E-08,0.027880606,4.73E-05
2618,text_summarization12,198,"Therefore , we decode the summary to a fixed length 18 to ensure that the generated summary satisfies the minimum length requirement .",Results,English Gigaword,text_summarization,12,17,1,0,,0.001715453,0,negative,0.000760099,4.20E-07,0.000615025,6.08E-08,2.84E-08,1.71E-06,1.14E-05,1.56E-05,6.96E-06,0.984639052,1.81E-07,0.013900647,4.88E-05
2619,text_summarization12,199,"As summarized in , our SEASS outperforms all the baseline methods and achieves 29.21 , 9.56 and 25.51 for ROUGE 1 , 2 and L recall .",Results,English Gigaword,text_summarization,12,18,1,1,results,0.812522625,1,results,0.000136398,2.62E-09,3.04E-06,2.52E-08,1.51E-09,2.14E-07,0.000178784,5.47E-07,2.81E-09,0.011908051,5.08E-08,0.987316639,0.000456242
2620,text_summarization12,200,"Compared to the ABS + model which is tuned using DUC 2003 data , our model performs significantly better by 1.07 ROUGE - 2 recall score and is trained only with English Gigaword sentence - summary data without being tuned using DUC data .",Results,English Gigaword,text_summarization,12,19,1,1,results,0.59795499,1,results,0.000267283,2.15E-09,2.08E-06,2.50E-08,1.34E-09,1.99E-07,0.000121051,6.18E-07,3.49E-09,0.013400926,3.28E-08,0.985822895,0.000384879
2621,text_summarization12,201,Figure 3 : First derivative heat map of the output with respect to the selective gate .,Results,English Gigaword,text_summarization,12,20,1,0,,0.000938961,0,negative,0.000716636,1.55E-07,0.000249232,3.64E-06,5.11E-08,1.32E-05,5.54E-05,2.38E-05,8.38E-06,0.964533043,1.25E-06,0.028536976,0.005858215
2622,text_summarization12,202,"The important words are selected in the input sentence , such as "" europe "" , "" slammed "" and "" unacceptable "" .",Results,English Gigaword,text_summarization,12,21,1,0,,7.26E-05,0,negative,5.52E-05,5.99E-08,2.63E-05,5.94E-08,2.26E-08,2.08E-06,4.76E-06,2.05E-05,6.63E-07,0.997502844,2.24E-08,0.002313872,7.36E-05
2623,text_summarization12,203,"The output summary of our system is "" council of europe slams french prison conditions "" and the true summary is "" council of europe again slams french prison conditions "" .",Results,English Gigaword,text_summarization,12,22,1,0,,0.000256488,0,negative,0.000176742,9.04E-09,1.20E-05,1.39E-06,1.19E-07,5.05E-06,1.77E-05,3.00E-06,1.48E-07,0.986068743,3.73E-08,0.012756817,0.000958236
2624,text_summarization12,204,Discussion,Results,,text_summarization,12,23,1,0,,0.001269198,0,negative,0.004595333,1.33E-05,0.000242867,2.73E-05,8.49E-06,0.000140437,0.000549777,0.000757619,6.75E-05,0.979530234,1.71E-05,0.013217284,0.00083283
2625,text_summarization12,205,"In this section , we first compare the performance of SEASS with the s 2s+ att baseline model to illustrate that the proposed method succeeds in selecting information and building tailored representation for abstractive sentence summarization .",Results,Discussion,text_summarization,12,24,1,0,,0.002630372,0,negative,0.000963527,0.004956567,8.54E-05,2.73E-07,4.34E-05,7.48E-06,1.47E-05,6.78E-05,0.000278111,0.992439686,0.000133918,0.001008783,4.05E-07
2626,text_summarization12,206,We then analyze selective encoding by visualizing the heat map .,Results,Discussion,text_summarization,12,25,1,0,,7.05E-06,0,negative,0.000243407,0.000371792,2.90E-05,1.14E-07,1.40E-05,1.47E-06,2.76E-07,6.90E-06,0.000437519,0.998877943,5.53E-06,1.20E-05,4.22E-08
2627,text_summarization12,207,Effectiveness of Selective Encoding,Results,,text_summarization,12,26,1,0,,0.263889268,0,results,0.040172769,7.90E-06,0.000951711,9.33E-06,8.01E-06,4.00E-05,0.005190015,0.000152065,1.42E-05,0.360774794,2.65E-05,0.591321991,0.00133071
2628,text_summarization12,208,"We further test the SEASS model with different sentence lengths on English Gigaword test sets , which are merged from the test set and our internal test set .",Results,Effectiveness of Selective Encoding,text_summarization,12,27,1,0,,0.006888509,0,negative,0.011610297,1.28E-05,0.000420518,5.38E-07,6.39E-06,1.00E-05,0.000164524,1.94E-05,8.95E-06,0.975013462,5.98E-07,0.012074402,0.000658026
2629,text_summarization12,209,The length of sentences in the test sets ranges from 10 to 80 .,Results,Effectiveness of Selective Encoding,text_summarization,12,28,1,0,,0.007424724,0,negative,0.004220641,5.49E-06,9.44E-05,2.94E-06,7.55E-06,0.0002796,0.00046629,0.000990708,1.31E-05,0.979248393,4.78E-07,0.001151002,0.013519427
2630,text_summarization12,210,We group the sentences with an interval of 4 and get 18 different groups and we draw the first 14 groups .,Results,Effectiveness of Selective Encoding,text_summarization,12,29,1,0,,0.00136033,0,negative,0.005500706,1.40E-06,7.39E-05,1.73E-06,1.33E-05,3.95E-05,6.89E-05,5.13E-05,3.31E-06,0.988933052,1.02E-07,0.001048425,0.004264376
2631,text_summarization12,211,We find that the performance curve of our SEASS model always appears to be on the top of that of s 2s + att with a certain margin .,Results,Effectiveness of Selective Encoding,text_summarization,12,30,1,0,,0.105235088,0,negative,0.214790421,1.29E-06,9.98E-05,7.12E-07,1.34E-06,1.06E-05,0.000527602,1.51E-05,2.50E-06,0.624176884,7.50E-07,0.158218239,0.002154826
2632,text_summarization12,212,"For the groups of 16 , 20 , 24 , 32 , 56 and 60 , the SEASS model obtains big improvements compared to the s 2s+ att model .",Results,Effectiveness of Selective Encoding,text_summarization,12,31,1,0,,0.753787436,1,results,0.405475594,3.13E-07,6.49E-05,4.15E-07,4.27E-07,5.00E-06,0.001028473,5.17E-06,6.62E-07,0.173620984,3.71E-07,0.415418282,0.00437946
2633,text_summarization12,213,"Overall , these improvements on all groups indicate that the selective encoding method benefits the abstractive sentence summarization task .",Results,Effectiveness of Selective Encoding,text_summarization,12,32,1,0,,0.444138187,0,results,0.166129423,4.83E-07,4.01E-05,7.62E-07,6.74E-07,8.93E-06,0.001835674,8.97E-06,7.43E-07,0.321364584,8.24E-07,0.500557026,0.010051797
2634,text_summarization12,214,Saliency Heat Map of Selective Gate,Results,,text_summarization,12,33,1,0,,0.263082931,0,results,0.029887319,4.30E-05,0.012113234,4.85E-05,2.35E-05,0.000131576,0.005940462,0.000760933,0.000208946,0.456868146,4.79E-05,0.479301615,0.014624933
2635,text_summarization12,215,"Since the output of the selective gate network is a high dimensional vector , it is hard to visualize all the gate values .",Results,Saliency Heat Map of Selective Gate,text_summarization,12,34,1,0,,4.33E-05,0,negative,0.00027041,3.44E-07,3.30E-06,9.91E-08,9.47E-08,1.93E-06,1.68E-06,3.01E-06,9.22E-06,0.999542148,7.18E-07,0.000103807,6.32E-05
2636,text_summarization12,216,"We use the method in to visualize the contribution of the selective gate to the final output , which can be approximated by the first derivative .",Results,Saliency Heat Map of Selective Gate,text_summarization,12,35,1,0,,8.37E-06,0,negative,0.001236965,4.57E-07,1.80E-05,8.12E-08,4.80E-07,9.92E-07,6.93E-07,1.32E-06,1.01E-05,0.998672672,2.38E-08,5.22E-05,6.04E-06
2637,text_summarization12,217,"Given sentence words x with associated output summary y , the trained model associates the pair ( x , y) with a score S y ( x ) .",Results,Saliency Heat Map of Selective Gate,text_summarization,12,36,1,0,,3.51E-06,0,negative,0.000383493,1.03E-06,2.96E-05,1.98E-08,1.55E-07,4.13E-07,6.79E-07,1.09E-06,5.47E-05,0.999457215,7.99E-08,6.30E-05,8.49E-06
2638,text_summarization12,218,The goal is to decide which gate g associated with a specific word makes the most significant contribution to S y ( x ) .,Results,Saliency Heat Map of Selective Gate,text_summarization,12,37,1,0,,5.94E-06,0,negative,0.000467906,1.66E-06,9.57E-06,8.39E-08,2.65E-07,9.35E-07,8.93E-07,2.88E-06,4.72E-05,0.999363199,3.25E-07,7.07E-05,3.45E-05
2639,text_summarization12,219,We approximate the S y ( g ) by computing the first - order Taylor expansion since the score S y ( x ) is a highly non-linear function in the deep neural network models :,Results,Saliency Heat Map of Selective Gate,text_summarization,12,38,1,0,,6.98E-06,0,negative,0.002935324,6.00E-07,3.29E-05,4.05E-08,2.72E-07,1.13E-06,1.74E-06,1.84E-06,1.12E-05,0.996884553,3.26E-08,0.000116703,1.37E-05
2640,text_summarization12,220,where w ( g ) is first the derivative of S y with respect to the gate g:,Results,Saliency Heat Map of Selective Gate,text_summarization,12,39,1,0,,1.52E-06,0,negative,0.001516986,1.28E-07,1.00E-05,2.12E-08,1.14E-07,5.89E-07,9.28E-07,5.86E-07,3.20E-06,0.998362084,3.10E-08,9.79E-05,7.40E-06
2641,text_summarization12,221,We then draw the Euclidean norm of the first derivative of the output y with respect to the selective gate g associated with each input words .,Results,Saliency Heat Map of Selective Gate,text_summarization,12,40,1,0,,1.59E-06,0,negative,0.000368213,1.35E-07,6.69E-06,3.56E-09,5.26E-08,2.01E-07,4.17E-07,3.96E-07,4.40E-06,0.999583818,5.24E-09,3.43E-05,1.34E-06
2642,text_summarization12,222,"shows an example of the first derivative heat map , in which most of the important words are selected by the selective gate such as "" europe "" , "" slammed "" , "" unacceptable "" , "" conditions "" , and "" france "" .",Results,Saliency Heat Map of Selective Gate,text_summarization,12,41,1,0,,7.20E-06,0,negative,0.000351009,2.84E-08,6.96E-06,2.24E-08,2.31E-07,8.63E-07,1.23E-06,5.51E-07,8.80E-07,0.999557459,6.08E-09,5.75E-05,2.33E-05
2643,text_summarization12,223,"We can observe that the selective gate determines the importance of each word before decoder , which releases the burden of it by providing tailored sentence encoding .",Results,Saliency Heat Map of Selective Gate,text_summarization,12,42,1,0,,0.000152452,0,negative,0.176308364,5.08E-07,1.21E-05,5.83E-08,2.68E-07,7.46E-07,7.83E-06,1.08E-06,1.29E-05,0.822791432,1.75E-08,0.0008399,2.48E-05
2644,text_summarization12,224,Conclusion,,,text_summarization,12,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
2645,natural_language_inference32,1,title,,,natural_language_inference,32,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
2646,natural_language_inference32,2,FLOWQA : GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION,title,title,natural_language_inference,32,1,1,1,research-problem,0.99894827,1,research-problem,1.12E-06,3.72E-05,1.24E-06,0.000121943,4.09E-05,9.67E-06,0.000111837,1.28E-05,9.14E-07,0.009033048,0.990622062,3.12E-06,4.19E-06
2647,natural_language_inference32,3,abstract,,,natural_language_inference,32,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
2648,natural_language_inference32,4,"Conversational machine comprehension requires the understanding of the conversation history , such as previous question / answer pairs , the document context and the current question .",abstract,abstract,natural_language_inference,32,1,1,0,,0.855620854,1,research-problem,1.13E-08,2.05E-06,7.67E-09,4.91E-07,1.34E-07,1.01E-07,3.49E-07,4.28E-07,1.41E-07,0.004338873,0.995657361,1.57E-08,3.70E-08
2649,natural_language_inference32,5,"To enable traditional , single - turn models to encode the history comprehensively , we introduce FLOW , a mechanism that can incorporate intermediate representations generated during the process of answering previous questions , through an alternating parallel processing structure .",abstract,abstract,natural_language_inference,32,2,1,0,,0.492274955,0,approach,6.48E-05,0.368596407,0.00027814,5.53E-05,0.000675532,8.23E-05,2.88E-05,0.000732258,0.092963063,0.282926025,0.253579149,1.16E-05,6.59E-06
2650,natural_language_inference32,6,"Compared to approaches that concatenate previous questions / answers as input , FLOW integrates the latent semantics of the conversation history more deeply .",abstract,abstract,natural_language_inference,32,3,1,0,,0.193960693,0,negative,0.000145073,0.132080927,0.00031971,1.99E-05,0.000462699,4.25E-05,2.95E-05,0.000386503,0.01364513,0.617169408,0.235632174,6.30E-05,3.51E-06
2651,natural_language_inference32,7,"Our model , FLOWQA , shows superior performance on two recently proposed conversational challenges ( + 7.2 % F 1 on CoQA and + 4.0 % on QuAC ) .",abstract,abstract,natural_language_inference,32,4,1,0,,0.019363612,0,research-problem,5.07E-05,0.007794976,6.08E-05,1.17E-05,0.000115884,2.54E-05,0.000187789,0.000343881,0.00018207,0.261668851,0.72887947,0.000668921,9.59E-06
2652,natural_language_inference32,8,The effectiveness of FLOW also shows in other tasks .,abstract,abstract,natural_language_inference,32,5,1,0,,0.0269535,0,negative,0.000184785,0.000330971,2.39E-06,1.67E-05,5.93E-05,2.36E-05,8.00E-05,0.000188711,1.09E-05,0.86540711,0.133336042,0.000357019,2.37E-06
2653,natural_language_inference32,9,"By reducing sequential instruction understanding to conversational machine comprehension , FLOWQA outperforms the best models on all three domains in SCONE , with + 1.8 % to + 4.4 % improvement in accuracy .",abstract,abstract,natural_language_inference,32,6,1,0,,0.117006764,0,research-problem,0.000150885,0.003015382,4.74E-05,7.00E-05,0.00036775,4.86E-05,0.000764021,0.000425878,5.02E-05,0.297228959,0.69570459,0.0020899,3.63E-05
2654,natural_language_inference32,10,*,abstract,abstract,natural_language_inference,32,7,1,0,,0.000135684,0,negative,9.15E-07,0.000133621,2.67E-07,2.35E-06,7.23E-06,1.75E-05,2.19E-06,0.000106721,8.65E-05,0.949155646,0.050486564,3.89E-07,1.39E-07
2655,natural_language_inference32,11,Work done during internship at Allen Institute for Artificial Intelligence .,abstract,abstract,natural_language_inference,32,8,1,0,,0.000632105,0,negative,1.20E-06,9.01E-05,5.87E-07,0.000288415,0.000134759,4.55E-05,9.59E-06,4.18E-05,1.72E-05,0.582604951,0.416764463,3.71E-07,1.00E-06
2656,natural_language_inference32,12,"1 We use "" reasoning "" to refer to the model process of finding the answer .",abstract,abstract,natural_language_inference,32,9,1,0,,0.000274041,0,negative,4.81E-07,0.000410639,4.28E-07,0.000144461,0.000137692,0.000111961,2.89E-06,0.000184389,8.28E-05,0.979111563,0.019812185,1.26E-07,4.23E-07
2657,natural_language_inference32,13,"We present FLOWQA , a model designed for conversational machine comprehension .",abstract,abstract,natural_language_inference,32,10,1,1,model,0.895426004,1,research-problem,2.95E-05,0.018009278,0.00025162,0.000341576,0.003207451,7.42E-05,0.000312843,0.000212883,0.000461382,0.126968368,0.850053843,4.94E-05,2.76E-05
2658,natural_language_inference32,14,FLOWQA consists of two main components : a base neural model for single - turn MC and a FLOW mechanism that encodes the conversation history .,abstract,abstract,natural_language_inference,32,11,1,1,model,0.443231585,0,research-problem,0.000106262,0.321388395,0.002902247,0.000166332,0.003714654,0.000158306,0.000162397,0.000622936,0.034048677,0.286220833,0.350425583,5.17E-05,3.16E-05
2659,natural_language_inference32,15,"Instead of using the shallow history , i.e. , previous questions and answers , we feed the model with the entire hidden representations generated during the process of answering previous questions .",abstract,abstract,natural_language_inference,32,12,1,1,model,0.155326107,0,negative,2.13E-05,0.280212705,0.000181266,5.78E-06,0.000241691,5.16E-05,6.95E-06,0.000741352,0.210944099,0.496112164,0.011476348,3.61E-06,1.19E-06
2660,natural_language_inference32,16,"These hidden representations potentially capture related information , such as phrases and facts in the context , for answering the previous questions , and hence provide additional clues on what the current conversation is revolving around .",abstract,abstract,natural_language_inference,32,13,1,0,,0.106410367,0,negative,7.08E-06,0.021348498,1.52E-05,1.63E-05,0.000172046,4.47E-05,4.54E-06,0.00027115,0.035423841,0.909075681,0.033617931,2.09E-06,9.40E-07
2661,natural_language_inference32,17,"This FLOW mechanism is also remarkably effective at tracking the world states for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",abstract,abstract,natural_language_inference,32,14,1,1,model,0.137311193,0,research-problem,1.52E-06,0.002946757,5.48E-06,7.42E-06,2.04E-05,2.08E-05,6.74E-06,0.000157681,0.002462726,0.453266093,0.541101929,1.48E-06,1.01E-06
2662,natural_language_inference32,18,"The FLOW mechanism can be viewed as stacking single - turn QA models along the dialog progression ( i.e. , the question turns ) and building information flow along the dialog .",abstract,abstract,natural_language_inference,32,15,1,1,model,0.235153192,0,negative,1.47E-05,0.14243561,0.00011685,7.56E-06,0.000210327,3.23E-05,6.97E-06,0.000379466,0.139751275,0.649251331,0.067787451,4.56E-06,1.69E-06
2663,natural_language_inference32,19,"This information transfer happens for each context word , allowing rich information in the reasoning process to flow .",abstract,abstract,natural_language_inference,32,16,1,1,model,0.121680862,0,negative,3.40E-05,0.056936178,0.000120693,7.35E-06,0.000288343,2.59E-05,6.14E-06,0.000273577,0.057871896,0.869732885,0.014695429,6.52E-06,1.10E-06
2664,natural_language_inference32,20,"The design is analogous to recurrent neural networks , where each single update unit is now an entire question answering process .",abstract,abstract,natural_language_inference,32,17,1,0,,0.046956558,0,negative,1.17E-05,0.209902804,0.000105464,1.73E-05,0.000352068,6.01E-05,8.00E-06,0.000612213,0.166129585,0.600061104,0.022734819,2.90E-06,1.98E-06
2665,natural_language_inference32,21,"Because there are two recurrent structures in our modeling , one in the context for each question and the other in the conversation progression , a naive implementation leads to a highly unparallelizable structure .",abstract,abstract,natural_language_inference,32,18,1,0,,0.006090694,0,negative,4.02E-05,0.008042426,3.99E-06,9.08E-06,0.000164228,2.46E-05,4.78E-06,0.000218925,0.001022212,0.976265517,0.01419692,6.55E-06,5.09E-07
2666,natural_language_inference32,22,"To handle this issue , we propose an alternating parallel processing structure , which alternates between sequentially processing one dimension in parallel of the other dimension , and thus speeds up training significantly .",abstract,abstract,natural_language_inference,32,19,1,1,model,0.307784265,0,approach,4.97E-05,0.607000747,0.000255341,5.52E-05,0.001585062,8.07E-05,2.53E-05,0.000590914,0.205320243,0.171673453,0.013345519,1.06E-05,7.28E-06
2667,natural_language_inference32,23,Machine Reasoning,abstract,,natural_language_inference,32,20,1,0,,0.225007242,0,research-problem,9.86E-07,0.00020922,7.62E-07,3.54E-05,4.96E-05,1.95E-05,7.14E-05,7.55E-05,1.01E-05,0.122681855,0.876836274,5.67E-06,3.73E-06
2668,natural_language_inference32,24,The young girl and her dog set out a trip into the woods one day .,abstract,Machine Reasoning,natural_language_inference,32,21,1,0,,0.000114295,0,negative,5.36E-06,7.34E-06,1.08E-06,1.14E-06,3.56E-05,0.000109282,4.77E-05,5.68E-06,3.23E-05,0.999741925,1.07E-05,1.45E-06,3.91E-07
2669,natural_language_inference32,25,Upon entering the woods the girl and her dog found that the woods were dark and cold .,abstract,Machine Reasoning,natural_language_inference,32,22,1,0,,0.000319895,0,negative,5.14E-06,7.21E-06,9.44E-07,2.16E-06,4.76E-05,0.000148078,5.11E-05,6.30E-06,3.33E-05,0.999682138,1.42E-05,1.34E-06,4.69E-07
2670,natural_language_inference32,26,"The girl was a little scared and was thinking of turning back , but yet they went on .",abstract,Machine Reasoning,natural_language_inference,32,23,1,0,,2.84E-05,0,negative,2.69E-06,1.87E-06,3.77E-07,3.13E-07,6.97E-06,6.85E-05,2.88E-05,3.19E-06,1.21E-05,0.999870571,3.55E-06,8.90E-07,9.25E-08
2671,natural_language_inference32,27,Context : : An illustration of conversational machine comprehension with an example from the Conversational Question Answering Challenge dataset ( CoQA ) .,abstract,Machine Reasoning,natural_language_inference,32,24,1,0,,0.021244502,0,negative,5.08E-05,4.66E-05,2.39E-05,0.000173101,0.000933551,0.000655269,0.009922749,1.74E-05,2.24E-05,0.964805946,0.023099735,0.000105023,0.000143444
2672,natural_language_inference32,28,"Humans seek information in a conversational manner , by asking follow - up questions for additional information based on what they have already learned .",abstract,Machine Reasoning,natural_language_inference,32,25,1,0,,0.000508279,0,negative,2.83E-06,2.00E-05,2.22E-06,1.83E-05,5.73E-05,0.0002634,0.00018459,1.12E-05,2.63E-05,0.996198563,0.003207803,2.58E-06,4.88E-06
2673,natural_language_inference32,29,Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,abstract,Machine Reasoning,natural_language_inference,32,26,1,1,research-problem,0.689643115,1,negative,1.75E-05,3.43E-05,4.79E-06,2.19E-05,4.40E-05,0.000190688,0.002183022,1.52E-05,1.30E-05,0.894644304,0.102771561,2.48E-05,3.49E-05
2674,natural_language_inference32,30,They consist of a sequence of question / answer pairs where questions can only be understood along with the conversation history .,abstract,Machine Reasoning,natural_language_inference,32,27,1,0,,0.002777808,0,negative,2.16E-05,0.000200784,0.000105638,4.99E-06,0.001461994,0.000124537,0.000336032,5.16E-06,0.000162125,0.997437399,0.000125281,1.13E-05,3.14E-06
2675,natural_language_inference32,31,illustrates this new challenge .,abstract,Machine Reasoning,natural_language_inference,32,28,1,0,,4.29E-05,0,negative,3.13E-06,2.62E-06,1.04E-06,1.71E-07,6.32E-06,1.36E-05,1.31E-05,1.12E-06,1.97E-05,0.999934243,3.42E-06,1.46E-06,6.71E-08
2676,natural_language_inference32,32,Existing approaches take a single - turn MC model and augment the current question and context with the previous questions and answers .,abstract,Machine Reasoning,natural_language_inference,32,29,1,0,,0.009059441,0,negative,1.12E-05,0.000112296,1.52E-05,5.67E-06,2.44E-05,0.00037222,0.000406556,4.02E-05,0.000146846,0.997771508,0.001082463,5.90E-06,5.54E-06
2677,natural_language_inference32,33,"However , this offers only a partial solution , ignoring previous reasoning 1 processes performed by the model .",abstract,Machine Reasoning,natural_language_inference,32,30,1,0,,0.000137164,0,negative,5.73E-06,6.98E-06,6.38E-07,7.27E-07,5.46E-06,3.05E-05,1.87E-05,3.07E-06,2.20E-05,0.999843922,6.04E-05,1.67E-06,1.91E-07
2678,natural_language_inference32,34,"FLOWQA achieves strong empirical results on conversational machine comprehension tasks , and improves the state of the art on various datasets ( from 67.8 % to 75.0 % on CoQA and 60.1 % to 64.1 % on QuAC ) .",abstract,Machine Reasoning,natural_language_inference,32,31,1,0,,0.032328517,0,negative,0.000775347,0.000161185,9.20E-05,7.58E-06,0.000301178,0.000401754,0.137994613,4.70E-05,1.80E-05,0.845922306,0.00054313,0.013528509,0.000207427
2679,natural_language_inference32,35,"While designed for conversational machine comprehension , FLOWQA also shows superior performance on a seemingly different task - understanding a sequence of natural language instructions ( framed previously as a sequential semantic parsing problem ) .",abstract,Machine Reasoning,natural_language_inference,32,32,1,0,,0.035062693,0,negative,0.000394818,4.82E-05,5.40E-05,2.29E-06,0.000167266,0.00015488,0.045494808,1.52E-05,6.91E-06,0.945065136,0.000233087,0.00832575,3.77E-05
2680,natural_language_inference32,36,"When tested on SCONE , FLOWQA outperforms all existing systems in three domains , resulting in a range of accuracy improvement from + 1.8 % to + 4.4 % .",abstract,Machine Reasoning,natural_language_inference,32,33,1,0,,0.174346373,0,negative,0.005391923,0.000110765,6.90E-05,8.71E-06,0.000231109,0.00070319,0.308349035,0.000104847,1.35E-05,0.62791539,6.72E-05,0.056773267,0.000262101
2681,natural_language_inference32,37,Our code can be found in https://github.com/momohuang/FlowQA.,abstract,Machine Reasoning,natural_language_inference,32,34,1,1,code,0.584077644,1,code,8.63E-06,9.87E-07,9.54E-07,0.980259793,0.001226258,0.004186544,4.78E-05,3.15E-06,1.22E-06,0.014252388,6.24E-07,6.31E-08,1.16E-05
2682,natural_language_inference32,38,BACKGROUND : MACHINE COMPREHENSION,abstract,Machine Reasoning,natural_language_inference,32,35,1,0,,0.006589171,0,negative,7.47E-05,9.13E-05,3.57E-05,1.83E-05,7.88E-05,0.000733085,0.024604524,4.25E-05,6.25E-05,0.961908082,0.012063879,0.000178479,0.000108114
2683,natural_language_inference32,39,"In this section , we introduce the task formulations of machine comprehension in both single - turn and conversational settings , and discuss the main ideas of state - of - the - art MC models .",abstract,Machine Reasoning,natural_language_inference,32,36,1,0,,0.00535611,0,negative,9.08E-06,0.000521979,9.49E-06,9.42E-07,6.31E-05,9.64E-05,0.000397114,1.71E-05,0.000323997,0.998160786,0.000378035,1.97E-05,2.27E-06
2684,natural_language_inference32,40,TASK FORMULATION,abstract,Machine Reasoning,natural_language_inference,32,37,1,0,,0.000423852,0,negative,3.81E-06,0.000164973,4.29E-06,4.11E-06,2.75E-05,0.000462952,0.000378071,0.000118104,0.000727166,0.9979535,0.000142654,7.33E-06,5.55E-06
2685,natural_language_inference32,41,"Given an evidence document ( context ) and a question , the task is to find the answer to the question based on the context .",abstract,Machine Reasoning,natural_language_inference,32,38,1,0,,0.014768394,0,negative,4.38E-06,6.16E-05,3.85E-06,8.62E-06,5.75E-05,0.000160613,0.000273317,1.95E-05,7.29E-05,0.997706139,0.001616658,5.77E-06,9.07E-06
2686,natural_language_inference32,42,"The context C = {c 1 , c 2 , . . . cm } is described as a sequence of m words and the question Q = {q 1 , q 2 . . . q n } a sequence of n words .",abstract,Machine Reasoning,natural_language_inference,32,39,1,0,,0.000141935,0,negative,1.15E-06,3.56E-05,2.36E-06,1.01E-07,7.09E-06,8.11E-05,5.50E-05,1.62E-05,0.000675609,0.999120451,3.96E-06,9.78E-07,4.13E-07
2687,natural_language_inference32,43,"In the extractive setting , the answer A must be a span in the context .",abstract,Machine Reasoning,natural_language_inference,32,40,1,0,,0.000121331,0,negative,1.26E-06,6.41E-06,5.10E-07,1.47E-07,4.59E-06,2.06E-05,1.67E-05,2.86E-06,2.65E-05,0.999908381,1.09E-05,1.03E-06,1.61E-07
2688,natural_language_inference32,44,"Conversational machine comprehension is a generalization of the singleturn setting : the agent needs to answer multiple , potentially inter-dependent questions in sequence .",abstract,Machine Reasoning,natural_language_inference,32,41,1,0,,0.004657162,0,negative,2.44E-05,0.000125974,3.43E-05,3.24E-05,0.00012869,0.000515564,0.00782073,3.23E-05,6.33E-05,0.974383457,0.01663847,7.29E-05,0.000127613
2689,natural_language_inference32,45,"The meaning of the current question may depend on the conversation history ( e.g. , in , the third question such as ' Where ? ' can not be answered in isolation ) .",abstract,Machine Reasoning,natural_language_inference,32,42,1,0,,4.85E-05,0,negative,1.00E-06,3.12E-06,3.50E-07,1.70E-07,3.66E-06,3.22E-05,1.34E-05,3.51E-06,2.03E-05,0.999919939,1.56E-06,7.42E-07,1.05E-07
2690,natural_language_inference32,46,"Thus , previous conversational history ( i.e. , question / answer pairs ) is provided as an input in addition to the context and the current question .",abstract,Machine Reasoning,natural_language_inference,32,43,1,0,,0.001297226,0,negative,9.35E-06,0.000378712,3.52E-05,6.09E-08,2.60E-05,2.36E-05,4.11E-05,6.17E-06,0.006442247,0.993033077,1.24E-06,2.94E-06,2.84E-07
2691,natural_language_inference32,47,MODEL ARCHITECTURE,abstract,Machine Reasoning,natural_language_inference,32,44,1,0,,0.001221062,0,negative,7.47E-05,0.000257177,0.00014346,5.68E-05,0.000237198,0.002221521,0.001458344,0.000105769,0.009953401,0.985364107,6.29E-05,1.20E-05,5.25E-05
2692,natural_language_inference32,48,"For single - turn MC , many top - performing models share a similar architecture , consisting of four major components : ( 1 ) question encoding , ( 2 ) context encoding , ( 3 ) reasoning , and finally ( 4 ) answer prediction .",abstract,Machine Reasoning,natural_language_inference,32,45,1,0,,0.002952137,0,negative,8.47E-06,6.37E-05,2.54E-05,8.42E-07,1.50E-05,0.000128534,0.000236766,9.93E-06,0.000326077,0.999128489,4.92E-05,4.49E-06,3.12E-06
2693,natural_language_inference32,49,"Initially the word embeddings ( e.g. , of question tokens Q and context tokens C are taken as input and fed into contextual integration layers , such as LSTMs or self attentions , to encode the question and context .",abstract,Machine Reasoning,natural_language_inference,32,46,1,0,,0.000571342,0,negative,1.43E-05,0.000175627,1.35E-05,1.23E-07,1.45E-05,8.60E-05,0.000111017,3.83E-05,0.00263393,0.996910702,3.39E-07,1.13E-06,4.96E-07
2694,natural_language_inference32,50,"Multiple integration layers provide contextualized representations of context , and are often inter-weaved with attention , which inject question information .",abstract,Machine Reasoning,natural_language_inference,32,47,1,0,,0.000533716,0,negative,8.43E-06,1.81E-05,4.83E-06,2.21E-07,4.95E-06,4.28E-05,5.21E-05,4.81E-06,0.000209397,0.99964484,6.53E-06,2.46E-06,6.02E-07
2695,natural_language_inference32,51,The context integration layers thus produce a series of query - aware hidden vectors for each word in the context .,abstract,Machine Reasoning,natural_language_inference,32,48,1,0,,0.005465542,0,negative,2.56E-05,0.000470558,4.31E-05,4.02E-07,2.71E-05,0.000135893,0.000181211,3.72E-05,0.035602496,0.963469683,1.90E-06,2.00E-06,2.83E-06
2696,natural_language_inference32,52,"Together , the context integration layers can be viewed as conducting implicit reasoning to find the answer span .",abstract,Machine Reasoning,natural_language_inference,32,49,1,0,,0.001258535,0,negative,1.96E-05,0.000227669,2.03E-05,2.86E-07,1.87E-05,5.10E-05,6.17E-05,1.01E-05,0.012675748,0.986910169,1.58E-06,2.07E-06,1.10E-06
2697,natural_language_inference32,53,The final sequence of context vectors is fed into the answer prediction layer to select < Author went to his father 's funeral >,abstract,Machine Reasoning,natural_language_inference,32,50,1,0,,0.018224109,0,negative,9.84E-06,0.000331785,6.71E-05,3.17E-08,8.83E-06,4.05E-05,0.000123106,1.07E-05,0.011925483,0.987478725,1.04E-06,2.01E-06,8.15E-07
2698,natural_language_inference32,54,"After he passed away , I stayed in his apartment .",abstract,Machine Reasoning,natural_language_inference,32,51,1,0,,1.81E-05,0,negative,1.97E-06,1.37E-06,4.78E-07,3.85E-08,3.92E-06,2.27E-05,3.34E-05,2.04E-06,7.66E-06,0.999925388,1.74E-07,6.63E-07,1.38E-07
2699,natural_language_inference32,55,I was lonely . < Author found a cat outside the apartment >,abstract,Machine Reasoning,natural_language_inference,32,52,1,0,,3.26E-05,0,negative,2.01E-06,1.42E-06,5.29E-07,3.25E-06,6.06E-05,0.000237605,7.08E-05,6.33E-06,2.51E-06,0.999612693,5.26E-07,7.38E-07,9.71E-07
2700,natural_language_inference32,56,I felt pity of him and brougut him inside .,abstract,Machine Reasoning,natural_language_inference,32,53,1,0,,1.45E-05,0,negative,1.46E-06,9.39E-07,3.30E-07,4.36E-07,1.66E-05,8.45E-05,2.40E-05,3.00E-06,2.93E-06,0.999864997,1.26E-07,4.55E-07,2.20E-07
2701,natural_language_inference32,57,It has been five years since my father died .,abstract,Machine Reasoning,natural_language_inference,32,54,1,0,,1.11E-05,0,negative,7.99E-07,7.71E-07,2.19E-07,1.94E-07,1.02E-05,3.99E-05,1.80E-05,2.63E-06,2.69E-06,0.999923964,1.71E-07,3.04E-07,1.77E-07
2702,natural_language_inference32,58,"Over the years , people commented on how nice I was to save the cat .",abstract,Machine Reasoning,natural_language_inference,32,55,1,0,,2.83E-05,0,negative,7.15E-07,4.72E-07,1.82E-07,2.50E-08,2.51E-06,1.30E-05,1.77E-05,1.09E-06,1.90E-06,0.999961655,1.85E-07,5.36E-07,6.51E-08
2703,natural_language_inference32,59,But I know we saved each other .,abstract,Machine Reasoning,natural_language_inference,32,56,1,0,,7.28E-06,0,negative,7.05E-07,9.14E-07,2.51E-07,9.39E-08,2.60E-06,5.34E-05,3.41E-05,4.53E-06,5.19E-06,0.999897452,1.55E-07,4.24E-07,1.60E-07
2704,natural_language_inference32,60,Context :,abstract,Machine Reasoning,natural_language_inference,32,57,1,0,,2.52E-05,0,negative,3.85E-06,2.12E-05,5.05E-06,1.84E-07,1.44E-05,5.85E-05,0.000104227,6.39E-06,8.47E-05,0.999696889,1.07E-06,2.73E-06,6.89E-07
2705,natural_language_inference32,61,Question :,abstract,Machine Reasoning,natural_language_inference,32,58,1,0,,6.49E-06,0,negative,4.91E-07,1.71E-06,2.39E-07,9.73E-08,1.79E-06,3.80E-05,2.02E-05,6.13E-06,1.21E-05,0.999918182,6.08E-07,2.81E-07,2.08E-07
2706,natural_language_inference32,62,What did he feel ?,abstract,Machine Reasoning,natural_language_inference,32,59,1,0,,6.15E-06,0,negative,5.11E-07,8.07E-07,1.54E-07,3.87E-08,1.58E-06,4.95E-05,3.99E-05,5.21E-06,4.34E-06,0.999897223,9.60E-08,4.59E-07,1.41E-07
2707,natural_language_inference32,63,FLOWQA,abstract,,natural_language_inference,32,60,1,0,,0.705461184,1,negative,2.93E-05,0.006693257,0.000202202,0.20123967,0.155421844,0.10870671,0.00683893,0.028673372,0.000326664,0.490608502,0.000403381,6.42E-05,0.000791991
2708,natural_language_inference32,64,Our model aims to incorporate the conversation history more comprehensively via a conceptually simple FLOW mechanism .,abstract,FLOWQA,natural_language_inference,32,61,1,0,,0.216158147,0,model,3.54E-05,0.078921148,0.000288575,1.97E-06,0.001619004,0.000304014,0.000331665,6.78E-05,0.697549907,0.220851814,1.90E-05,4.92E-06,4.80E-06
2709,natural_language_inference32,65,"We first introduce the concept of FLOW ( Section 3.1 ) , propose the INTEGRATION - FLOW layers ( Section 3.2 ) , and present an end - to - end architecture for conversational machine comprehension , FLOWQA ( Section 3.3 ) .",abstract,FLOWQA,natural_language_inference,32,62,1,0,,0.00990386,0,negative,5.32E-05,0.166228141,0.000618971,1.78E-05,0.031584307,0.000998306,0.001518986,0.000136007,0.119058986,0.679687558,5.33E-05,2.35E-05,2.10E-05
2710,natural_language_inference32,66,CONCEPT OF FLOW,abstract,FLOWQA,natural_language_inference,32,63,1,0,,0.00364308,0,negative,5.23E-06,0.001463543,5.40E-05,1.83E-06,0.000458866,0.000726704,0.000699641,5.60E-05,0.011360268,0.985099662,6.47E-05,5.46E-06,4.11E-06
2711,natural_language_inference32,67,Successful conversational MC models should grasp how the conversation flows .,abstract,FLOWQA,natural_language_inference,32,64,1,0,,0.082601998,0,negative,4.10E-06,0.000486856,4.77E-06,4.76E-06,0.000303904,0.000363901,0.001453888,4.71E-05,0.000177662,0.994952198,0.002176954,1.39E-05,1.00E-05
2712,natural_language_inference32,68,"This includes knowing the main topic currently being discussed , as well as the relevant events and facts .",abstract,FLOWQA,natural_language_inference,32,65,1,0,,0.000787938,0,negative,1.88E-06,0.000114561,1.23E-06,4.41E-06,0.001103804,0.000164267,3.00E-05,1.16E-05,9.89E-05,0.998466525,1.91E-06,5.03E-07,3.73E-07
2713,natural_language_inference32,69,shows a simplified CoQA example where such conversation flow is crucial .,abstract,FLOWQA,natural_language_inference,32,66,1,0,,0.000652266,0,negative,8.70E-06,0.000217179,3.09E-05,4.25E-06,0.021329003,0.000347208,0.00038666,7.36E-06,7.36E-05,0.977587789,1.32E-06,4.30E-06,1.70E-06
2714,natural_language_inference32,70,"As the conversation progresses , the topic being discussed changes overtime .",abstract,FLOWQA,natural_language_inference,32,67,1,0,,0.003056481,0,negative,4.68E-06,0.001214261,1.38E-05,7.57E-07,0.001765883,9.47E-05,7.49E-05,1.02E-05,0.00182676,0.994990257,1.81E-06,1.51E-06,4.61E-07
2715,natural_language_inference32,71,"Since the conversation is about the context C , we consider FLOW to be a sequence of latent representations based on the context tokens ( the middle part of .",abstract,FLOWQA,natural_language_inference,32,68,1,0,,0.002026524,0,negative,1.14E-05,0.011053958,9.26E-05,1.22E-06,0.001980015,0.000305705,0.000205942,6.78E-05,0.085038915,0.901237061,1.53E-06,2.00E-06,1.78E-06
2716,natural_language_inference32,72,"Depending on the current topic , the answer to the same question may differ significantly .",abstract,FLOWQA,natural_language_inference,32,69,1,0,,0.000160402,0,negative,3.36E-07,3.12E-05,4.27E-07,1.45E-07,6.44E-05,3.21E-05,1.90E-05,3.23E-06,4.34E-05,0.99980372,1.38E-06,4.17E-07,7.60E-08
2717,natural_language_inference32,73,"For example , when the dialog is about the author 's father 's funeral , the answer to the question What did he feel ?",abstract,FLOWQA,natural_language_inference,32,70,1,0,,0.000146661,0,negative,2.09E-07,2.08E-05,5.32E-07,5.18E-07,0.000319229,0.000125812,3.74E-05,6.16E-06,2.05E-05,0.999467255,1.12E-06,2.27E-07,1.98E-07
2718,natural_language_inference32,74,"would be lonely , but when the conversation topic changes to five years after the death of the author 's father , the answer becomes we saved each other .",abstract,FLOWQA,natural_language_inference,32,71,1,0,,4.09E-05,0,negative,5.62E-07,8.32E-06,5.90E-07,1.56E-06,0.000780801,0.000161052,2.87E-05,3.66E-06,9.17E-06,0.999005026,2.35E-07,1.83E-07,1.57E-07
2719,natural_language_inference32,75,2,abstract,FLOWQA,natural_language_inference,32,72,1,0,,4.46E-05,0,negative,5.87E-07,3.49E-05,5.36E-07,1.89E-07,8.08E-05,0.000126202,4.16E-05,1.33E-05,0.000148282,0.99955287,2.45E-07,2.03E-07,1.67E-07
2720,natural_language_inference32,76,"Our model integrates both previous question / answer pairs and FLOW , the intermediate context representation from conversation history ( See ) .",abstract,FLOWQA,natural_language_inference,32,73,1,0,,0.3062648,0,model,5.77E-05,0.050373051,0.002045214,2.60E-06,0.00402849,0.000575556,0.001109354,6.25E-05,0.619181946,0.322524747,9.63E-06,8.94E-06,2.03E-05
2721,natural_language_inference32,77,"In MC models , the intermediate reasoning process is captured in several context integration layers ( often BiLSTMs ) , which locate the answer candidates in the context .",abstract,FLOWQA,natural_language_inference,32,74,1,0,,0.157094908,0,negative,6.58E-06,0.00455444,7.69E-05,6.26E-06,0.000787159,0.001273104,0.000906837,0.000145686,0.009444412,0.982724751,5.47E-05,3.02E-06,1.61E-05
2722,natural_language_inference32,78,"Our model considers these intermediate representations , Ch i , generated during the h -th context integration layer of the reasoning component for the i - th question .",abstract,FLOWQA,natural_language_inference,32,75,1,0,,0.006419047,0,negative,9.78E-06,0.022565537,0.000227583,5.03E-07,0.000924802,0.000240234,0.000274913,6.02E-05,0.467718765,0.507969319,2.52E-06,1.63E-06,4.18E-06
2723,natural_language_inference32,79,"FLOW builds information flow from the intermediate representation Ch 1 , . . . , Ch i?1 generated for the previous question Q 1 , . . . , Q i?1 to the current process for answering Q i , for every hand i .",abstract,FLOWQA,natural_language_inference,32,76,1,0,,0.008194691,0,negative,1.67E-05,0.006112628,0.000258195,1.53E-06,0.001871404,0.000343732,0.000353317,4.42E-05,0.054725375,0.936263379,2.81E-06,2.76E-06,4.02E-06
2724,natural_language_inference32,80,INTEGRATION-FLOW LAYER,abstract,FLOWQA,natural_language_inference,32,77,1,0,,0.043631947,0,negative,0.000105808,0.004823781,0.000764154,2.54E-05,0.005231018,0.005271611,0.008804341,0.000393477,0.041077264,0.933346315,9.09E-06,2.64E-05,0.000121382
2725,natural_language_inference32,81,A naive implementation of FLOW would pass the output hidden vectors from each integration layer during the ( i ? 1 ) - th question turn to the corresponding integration layer for Q i .,abstract,FLOWQA,natural_language_inference,32,78,1,0,,0.000304397,0,negative,1.43E-05,0.002756892,6.50E-05,5.43E-07,0.000931476,0.000638803,0.000650774,7.32E-05,0.006799659,0.988063538,6.18E-07,2.42E-06,2.78E-06
2726,natural_language_inference32,82,"This is highly unparalleled , as the contexts have to be read in order , and the question turns have to be processed sequentially .",abstract,FLOWQA,natural_language_inference,32,79,1,0,,0.000868443,0,negative,1.97E-06,0.000200581,5.42E-06,6.11E-07,0.000382445,0.000109211,8.44E-05,5.78E-06,0.00018295,0.99902292,2.09E-06,1.10E-06,5.26E-07
2727,natural_language_inference32,83,"To achieve better parallelism , we alternate between them : context integration , processing sequentially in context , in parallel of question turns ; and flow , processing sequentially in question turns , in parallel of context words ( see ) .",abstract,FLOWQA,natural_language_inference,32,80,1,0,,0.105642745,0,negative,0.000121811,0.082836625,0.003032726,3.92E-06,0.022616975,0.002649799,0.005829582,0.000179088,0.099200719,0.783494609,7.12E-07,1.39E-05,1.95E-05
2728,natural_language_inference32,84,This architecture significantly improves efficiency during training .,abstract,FLOWQA,natural_language_inference,32,81,1,0,,0.033428664,0,negative,0.000775201,0.006105284,0.000785192,4.89E-06,0.010012057,0.000721834,0.005659538,4.67E-05,0.013575751,0.962204072,1.39E-06,8.72E-05,2.09E-05
2729,natural_language_inference32,85,"Below we describe the implementation of an INTEGRATION - FLOW ( IF ) layer , which is composed of a context integration layer and a FLOW component .",abstract,FLOWQA,natural_language_inference,32,82,1,0,,0.063520559,0,negative,6.74E-05,0.070126231,0.000834888,3.49E-06,0.009741771,0.000645185,0.001496272,0.000105125,0.250220502,0.666728137,1.92E-06,1.07E-05,1.84E-05
2730,natural_language_inference32,86,Context Integration,abstract,,natural_language_inference,32,83,1,0,,0.009703034,0,negative,1.89E-05,0.009098607,6.47E-05,1.42E-05,0.000602566,0.000437715,0.003508934,0.003016766,0.0021188,0.978957843,0.001555501,0.000496647,0.000108838
2731,natural_language_inference32,87,We pass the current context representation Ch i for each question i into a BiLSTM layer .,abstract,Context Integration,natural_language_inference,32,84,1,0,,1.13E-05,0,negative,1.48E-05,0.002929251,5.82E-05,5.70E-07,0.00037522,0.000120816,8.54E-05,5.74E-05,0.036069729,0.960285042,2.09E-07,1.11E-06,2.28E-06
2732,natural_language_inference32,88,All question i ( 1 ? i ? t ) are processed in parallel during training .,abstract,Context Integration,natural_language_inference,32,85,1,0,,8.08E-06,0,negative,4.73E-06,0.00087519,2.12E-05,3.07E-08,0.000163055,1.95E-05,2.43E-05,7.33E-06,0.00615866,0.992724869,4.60E-08,8.32E-07,2.02E-07
2733,natural_language_inference32,89,Q u e st ion,abstract,Context Integration,natural_language_inference,32,86,1,0,,1.51E-05,0,negative,1.95E-06,3.49E-05,3.19E-06,1.29E-06,0.000170639,0.000257168,8.96E-05,1.74E-05,0.0001038,0.999318046,2.33E-07,7.08E-07,1.07E-06
2734,natural_language_inference32,90,"Tu r n s FLOW After the integration , we have t context sequences of length m , one for each question .",abstract,Context Integration,natural_language_inference,32,87,1,0,,1.44E-05,0,negative,1.99E-05,0.00050209,3.54E-05,1.56E-07,0.001071728,7.26E-05,0.000211645,9.05E-06,0.000646461,0.997425384,1.07E-07,4.58E-06,9.08E-07
2735,natural_language_inference32,91,"We reshape it to become m sequences of length t , one for each context word .",abstract,Context Integration,natural_language_inference,32,88,1,0,,2.71E-05,0,negative,8.23E-06,0.002493429,1.74E-05,5.82E-07,0.001502493,0.000335527,0.000232946,0.000117676,0.004376133,0.99091203,6.91E-08,9.69E-07,2.47E-06
2736,natural_language_inference32,92,We then pass each sequence into a GRU 3 so the entire intermediate representation for answering the previous questions can be used when processing the current question .,abstract,Context Integration,natural_language_inference,32,89,1,0,,3.78E-05,0,negative,1.04E-05,0.003186348,0.000169249,1.95E-07,0.000327893,8.00E-05,0.000118723,2.35E-05,0.069781994,0.926297749,1.69E-07,1.26E-06,2.55E-06
2737,natural_language_inference32,93,We only consider the forward direction since we do not know the ( i + 1 ) - th question when answering the i - th question .,abstract,Context Integration,natural_language_inference,32,90,1,0,,7.07E-06,0,negative,1.33E-05,0.00214706,1.37E-05,4.71E-08,0.000108715,2.90E-05,5.03E-05,1.91E-05,0.005269791,0.992347072,5.42E-08,1.62E-06,2.90E-07
2738,natural_language_inference32,94,All context word j,abstract,Context Integration,natural_language_inference,32,91,1,0,,1.12E-06,0,negative,4.57E-07,2.52E-05,1.17E-06,1.40E-07,8.04E-05,8.36E-05,2.05E-05,1.27E-05,7.79E-05,0.999697375,2.91E-08,2.37E-07,2.54E-07
2739,natural_language_inference32,95,"We reshape the outputs from the FLOW layer to be sequential to context tokens , and concatenate them to the output of the integration layer .",abstract,Context Integration,natural_language_inference,32,92,1,0,,0.00136778,0,negative,5.47E-05,0.026773767,0.000123481,3.86E-06,0.003381619,0.001182668,0.000747251,0.00071501,0.070722858,0.896278704,8.59E-08,1.92E-06,1.41E-05
2740,natural_language_inference32,96,"In summary , this process takes Ch i and generates C h+1 i , which will be used for further contextualization to predict the start and end answer span tokens .",abstract,Context Integration,natural_language_inference,32,93,1,0,,4.02E-05,0,negative,4.27E-06,0.000420403,7.60E-06,3.27E-08,9.40E-05,2.20E-05,2.93E-05,8.80E-06,0.002471451,0.996941347,1.49E-08,5.56E-07,3.06E-07
2741,natural_language_inference32,97,"When FLOW is removed , the IF layer becomes a regular context integration layer and in this case , a single layer of BiLSTM .",abstract,Context Integration,natural_language_inference,32,94,1,0,,7.04E-05,0,negative,0.000231628,0.003671758,0.000231541,5.85E-07,0.001247341,0.000183621,0.000339541,4.08E-05,0.016497881,0.977545959,4.85E-08,6.53E-06,2.73E-06
2742,natural_language_inference32,98,FULL ARCHITECTURE OF FLOWQA,abstract,Context Integration,natural_language_inference,32,95,1,0,,0.000500873,0,negative,1.28E-05,0.001773927,0.000220998,1.76E-06,0.000435974,0.00085446,0.002958025,0.000112456,0.021233869,0.972304637,3.72E-06,1.62E-05,7.12E-05
2743,natural_language_inference32,99,"We construct our conversation MC model , FLOWQA , based on the single - turn MC structure ( Sec. 2.2 ) with fully - aware attention .",abstract,Context Integration,natural_language_inference,32,96,1,0,,0.002731788,0,negative,4.15E-05,0.132852235,0.001757549,2.04E-06,0.010130921,0.000965569,0.00477114,0.000306934,0.09299741,0.756105258,8.23E-07,1.85E-05,5.01E-05
2744,natural_language_inference32,100,The full architecture is shown in .,abstract,Context Integration,natural_language_inference,32,97,1,0,,3.98E-05,0,negative,3.87E-06,0.000502544,2.16E-05,2.58E-06,0.000432243,0.000594658,0.000207183,7.81E-05,0.005851327,0.992296152,2.57E-07,7.44E-07,8.73E-06
2745,natural_language_inference32,101,"In this section , we describe its main components : initial encoding , reasoning and answer prediction .",abstract,Context Integration,natural_language_inference,32,98,1,0,,1.64E-05,0,negative,3.48E-06,0.000833526,2.56E-05,3.99E-07,0.000898339,7.07E-05,6.05E-05,1.17E-05,0.0026256,0.995468145,3.59E-08,7.68E-07,1.22E-06
2746,natural_language_inference32,102,QUESTION / CONTEXT ENCODING,abstract,Context Integration,natural_language_inference,32,99,1,0,,0.00012164,0,negative,1.94E-06,0.000261152,3.83E-05,7.04E-08,7.64E-05,0.000133406,0.000589928,2.52E-05,0.001859078,0.997004955,5.35E-07,4.11E-06,4.88E-06
2747,natural_language_inference32,103,Word Embedding,abstract,,natural_language_inference,32,100,1,0,,0.005760316,0,negative,2.53E-05,0.015594358,0.000387838,5.80E-05,0.002132745,0.002433849,0.001375502,0.013333391,0.019510012,0.94490277,1.04E-05,3.61E-05,0.000199781
2748,natural_language_inference32,104,"We embed the context into a sequence of vectors , C = {c 1 , . . . , cm } with pretrained GloVe and ELMo embeddings .",abstract,Word Embedding,natural_language_inference,32,101,1,0,,0.000121209,0,negative,2.54E-06,0.001532123,7.18E-06,6.39E-07,0.000581646,0.00095534,0.000254205,0.000877244,0.003572589,0.992209365,5.64E-09,3.76E-07,6.75E-06
2749,natural_language_inference32,105,"Similarly , each question at the i - th turn is embedded into a sequence of vectors Q i = {q i , 1 , . . . , q i , n } , where n is the maximum question length for all questions in the conversation .",abstract,Word Embedding,natural_language_inference,32,102,1,0,,1.56E-06,0,negative,3.58E-07,0.000152872,1.88E-06,1.26E-08,2.88E-05,2.64E-05,1.08E-05,1.57E-05,0.001617881,0.998144978,1.84E-09,8.41E-08,2.16E-07
2750,natural_language_inference32,106,"Attention ( on Question ) Following DrQA , for each question , we compute attention in the word level to enhance context word embeddings with question .",abstract,Word Embedding,natural_language_inference,32,103,1,0,,0.001708618,0,negative,3.11E-05,0.002446943,0.000558581,2.28E-07,0.00047882,0.000184608,0.000885211,3.44E-05,0.004705328,0.990647751,9.44E-08,1.18E-05,1.52E-05
2751,natural_language_inference32,107,The generated question - specific context input representation is denoted as C 0 i .,abstract,Word Embedding,natural_language_inference,32,104,1,0,,6.19E-06,0,negative,9.20E-08,7.27E-05,9.63E-07,3.63E-09,1.00E-05,2.01E-05,1.35E-05,1.93E-05,0.000433119,0.999429957,2.62E-09,1.08E-07,2.27E-07
2752,natural_language_inference32,108,"For completeness , a restatement of this representation can be found in Appendix C.1 .",abstract,Word Embedding,natural_language_inference,32,105,1,0,,8.18E-07,0,negative,6.63E-08,4.11E-06,2.34E-07,4.89E-07,9.40E-05,9.08E-05,5.09E-06,9.94E-06,6.63E-06,0.999788427,6.22E-10,3.97E-08,1.34E-07
2753,natural_language_inference32,109,Question Integration with QHierRNN,abstract,,natural_language_inference,32,106,1,0,,0.015320604,0,negative,8.42E-06,0.003775442,9.65E-05,1.77E-06,0.000322052,0.000313741,0.005547538,0.001956931,0.001046777,0.986359125,9.76E-05,0.000354543,0.000119498
2754,natural_language_inference32,110,"Similar to many MC models , contextualized embeddings for the questions are obtained using multiple layers of BiLSTM ( we used two layers ) .",abstract,Question Integration with QHierRNN,natural_language_inference,32,107,1,0,,3.49E-05,0,negative,1.12E-05,0.000662434,0.000115021,7.20E-08,6.53E-05,4.32E-05,0.000336641,6.00E-05,0.002534352,0.996160817,2.43E-09,4.35E-06,6.52E-06
2755,natural_language_inference32,111,We build a pointer vector for each question to be used in the answer prediction layer by first taking a weighted sum of each word vectors in the question .,abstract,Question Integration with QHierRNN,natural_language_inference,32,108,1,0,,9.06E-06,0,negative,9.75E-06,0.000630457,5.32E-05,1.82E-08,4.15E-05,1.78E-05,0.000137468,3.26E-05,0.010429784,0.98864224,1.44E-09,2.28E-06,3.04E-06
2756,natural_language_inference32,112,where w is a trainable vector .,abstract,Question Integration with QHierRNN,natural_language_inference,32,109,1,0,,1.94E-07,0,negative,6.77E-07,3.28E-06,2.92E-07,3.36E-09,1.42E-06,4.31E-06,1.34E-05,1.42E-05,1.69E-05,0.999944844,2.08E-10,6.32E-07,1.72E-07
2757,natural_language_inference32,113,We then encode question history hierarchically with LSTMs to generate history - aware question vectors ( QHierRNN ) .,abstract,Question Integration with QHierRNN,natural_language_inference,32,110,1,0,,0.000238121,0,negative,1.00E-05,0.000752507,0.000301215,1.12E-08,3.49E-05,9.93E-06,0.000260252,8.45E-06,0.011115231,0.987494708,4.23E-09,7.48E-06,5.32E-06
2758,natural_language_inference32,114,"The final vectors , p 1 , . . . , pt , will be used in the answer prediction layer .",abstract,Question Integration with QHierRNN,natural_language_inference,32,111,1,0,,9.26E-07,0,negative,4.82E-07,5.96E-06,6.27E-07,2.52E-09,1.89E-06,3.00E-06,1.03E-05,8.66E-06,8.36E-05,0.999885037,7.58E-11,2.67E-07,1.70E-07
2759,natural_language_inference32,115,REASONING,abstract,,natural_language_inference,32,112,1,0,,0.001210002,0,negative,5.78E-06,0.001395525,4.75E-05,0.000256431,0.025471072,0.003093199,0.002176409,0.004160143,8.09E-05,0.963154216,1.54E-06,4.14E-05,0.000115995
2760,natural_language_inference32,116,"The reasoning component has several IF layers on top of the context encoding , inter-weaved with attention ( first on question , then on context itself ) .",abstract,REASONING,natural_language_inference,32,113,1,0,,0.000250163,0,negative,2.71E-05,2.98E-05,0.000180783,9.60E-08,1.59E-05,4.24E-05,0.000275497,5.05E-05,0.000676151,0.998647406,8.22E-10,4.44E-06,4.99E-05
2761,natural_language_inference32,117,"We use fully - aware attention , which concatenates all layers of hidden vectors and uses S (x , y) = ReLU ( Ux ) TD ReLU ( Uy ) to compute the attention score between x , y , where U , Dare trainable parameters and Dis a diagonal matrix .",abstract,REASONING,natural_language_inference,32,114,1,0,,2.92E-05,0,negative,1.23E-05,0.000104294,0.000116616,2.89E-08,1.29E-05,0.00012375,0.001060804,0.000336067,0.000331255,0.997863899,2.04E-10,3.74E-06,3.44E-05
2762,natural_language_inference32,118,Below we give the details of each layer ( from bottom to top ) .,abstract,REASONING,natural_language_inference,32,115,1,0,,7.33E-07,0,negative,2.71E-07,1.69E-07,1.16E-07,3.00E-09,6.01E-07,3.18E-06,4.65E-06,7.56E-06,7.44E-07,0.999982244,3.80E-12,2.81E-07,1.82E-07
2763,natural_language_inference32,119,"Integration - Flow 2 First , we take the question - augmented context representation C 0 i and pass it to two IF layers .",abstract,REASONING,natural_language_inference,32,116,1,0,,2.93E-05,0,negative,2.80E-05,7.88E-06,0.000156628,7.93E-10,1.31E-06,2.54E-06,0.000190918,2.99E-06,7.73E-05,0.999510227,7.00E-11,2.00E-05,2.22E-06
2764,natural_language_inference32,120,C,abstract,REASONING,natural_language_inference,32,117,1,0,,8.83E-07,0,negative,1.76E-07,1.72E-08,5.81E-08,1.74E-10,3.69E-08,1.18E-06,9.21E-06,1.63E-06,1.47E-07,0.999986872,7.09E-12,5.50E-07,1.11E-07
2765,natural_language_inference32,121,Attention ( on Question ),abstract,REASONING,natural_language_inference,32,118,1,0,,2.13E-05,0,negative,6.59E-06,3.50E-07,3.28E-05,1.46E-09,1.24E-06,4.48E-06,0.000292281,1.81E-06,2.30E-06,0.999625494,7.32E-11,2.93E-05,3.33E-06
2766,natural_language_inference32,122,"After contextualizing the context representation , we perform fully - aware attention on the question for each context words .",abstract,REASONING,natural_language_inference,32,119,1,0,,0.000697036,0,negative,7.99E-05,5.72E-05,0.000137955,2.01E-09,6.90E-06,4.51E-06,0.000352524,8.37E-06,0.000318604,0.999009939,5.83E-11,1.79E-05,6.17E-06
2767,natural_language_inference32,123,Integration - Flow,abstract,,natural_language_inference,32,120,1,0,,0.005563347,0,negative,3.09E-05,0.004488257,0.001044454,7.56E-06,0.00238757,0.00086939,0.00168086,0.001634876,0.002561693,0.98516627,2.71E-07,5.05E-05,7.74E-05
2768,natural_language_inference32,124,"We concatenate the output from the previous IF layer with the attended question vector , and pass it as an input .",abstract,Integration - Flow,natural_language_inference,32,121,1,0,,4.98E-05,0,negative,1.12E-06,2.03E-05,3.43E-06,1.15E-08,1.68E-05,1.89E-05,1.58E-05,1.33E-05,0.000287831,0.999621762,5.67E-11,6.08E-08,7.98E-07
2769,natural_language_inference32,125,Attention ( on Context ),abstract,Integration - Flow,natural_language_inference,32,122,1,0,,0.000337607,0,negative,4.86E-06,1.68E-05,7.53E-05,7.88E-09,1.15E-05,1.21E-05,0.000273476,2.12E-06,0.000285052,0.999307984,3.22E-09,3.99E-06,6.72E-06
2770,natural_language_inference32,126,We apply fully - aware attention on the context itself ( self - attention ) .,abstract,Integration - Flow,natural_language_inference,32,123,1,0,,0.014084064,0,negative,9.28E-05,0.004067037,0.001386198,7.15E-08,0.000534766,8.44E-05,0.001220808,2.39E-05,0.013954383,0.978600375,2.61E-09,6.21E-06,2.91E-05
2771,natural_language_inference32,127,Integration,abstract,,natural_language_inference,32,124,1,0,,0.000803906,0,negative,7.44E-06,0.001080284,5.70E-05,9.10E-06,0.001746706,0.001008287,0.000862894,0.002374018,0.000381976,0.992412605,1.02E-07,2.23E-05,3.73E-05
2772,natural_language_inference32,128,"We concatenate the output from the the previous IF layer with the attention vector , and feed it to the last BiLSTM layer .",abstract,Integration,natural_language_inference,32,125,1,0,,0.000580292,0,negative,4.14E-06,0.009199009,2.74E-05,1.34E-06,0.002243635,0.001265505,0.000180216,0.00391363,0.009103283,0.974050779,3.14E-09,2.44E-07,1.08E-05
2773,natural_language_inference32,129,ANSWER PREDICTION,abstract,Integration,natural_language_inference,32,126,1,0,,0.000284067,0,negative,1.23E-05,0.001230552,0.000111599,0.00036976,0.076587064,0.019526209,0.075700478,0.002262273,2.51E-05,0.822243757,2.06E-06,0.000178778,0.001750001
2774,natural_language_inference32,130,"We use the same answer span selection method to estimate the start and end probabilities PS i , j , PE i , j of the j - th context token for the i - th question .",abstract,Integration,natural_language_inference,32,127,1,0,,2.30E-05,0,negative,2.40E-06,0.016128352,0.000105498,7.62E-08,0.000999509,9.11E-05,7.57E-05,0.000161274,0.012810614,0.969623126,5.89E-09,5.41E-07,1.82E-06
2775,natural_language_inference32,131,"Since there are unanswerable questions , we also calculate the no answer probabilities P ?",abstract,Integration,natural_language_inference,32,128,1,0,,6.84E-06,0,negative,2.59E-07,0.000227803,1.41E-06,1.20E-08,0.00027082,1.97E-05,4.87E-06,3.21E-05,0.000120452,0.999322445,2.41E-10,9.17E-08,6.25E-08
2776,natural_language_inference32,132,i for the i - th question .,abstract,Integration,natural_language_inference,32,129,1,0,,7.16E-07,0,negative,6.48E-08,2.63E-05,2.93E-07,1.02E-08,4.89E-05,1.81E-05,3.43E-06,2.24E-05,1.25E-05,0.999867986,3.99E-10,6.12E-08,4.85E-08
2777,natural_language_inference32,133,"For completeness , the equations for answer span selection is in Appendix C.1 .",abstract,Integration,natural_language_inference,32,130,1,0,,1.27E-06,0,negative,1.78E-07,8.86E-05,5.02E-07,2.18E-06,0.001387575,0.000184077,5.07E-06,0.000112095,2.27E-05,0.998196792,4.12E-10,4.35E-08,2.20E-07
2778,natural_language_inference32,134,EXPERIMENTS : CONVERSATIONAL MACHINE COMPREHENSION,abstract,Integration,natural_language_inference,32,131,1,0,,0.074525336,0,dataset,6.10E-05,0.002121401,0.000401678,0.001101078,0.612040327,0.01446769,0.078889363,0.00065913,1.13E-05,0.288531745,2.19E-07,0.000326907,0.001388256
2779,natural_language_inference32,135,"In this section , we evaluate FLOWQA on recently released conversational MC datasets .",abstract,Integration,natural_language_inference,32,132,1,0,,0.016404979,0,negative,1.11E-05,0.046104432,0.00040196,8.74E-06,0.229861718,0.001594685,0.006735967,0.000969642,0.000215117,0.714000275,1.83E-08,4.68E-05,4.96E-05
2780,natural_language_inference32,136,Data and Evaluation Metric,,,natural_language_inference,32,0,1,0,,0.00208167,0,negative,2.32E-05,5.51E-05,8.37E-06,1.28E-06,1.57E-06,0.000105448,9.43E-05,0.001022993,2.29E-05,0.995904879,0.002507252,0.00024883,3.91E-06
2781,natural_language_inference32,137,We experiment with the QuAC and,Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,1,1,0,,0.005907824,0,negative,0.000305362,5.10E-05,0.105606666,4.42E-07,3.68E-06,0.000309552,0.001557218,0.000576614,8.59E-06,0.889942316,3.11E-05,0.001600193,7.30E-06
2782,natural_language_inference32,138,CoQA datasets .,Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,2,1,0,,0.599961033,1,negative,0.000285222,1.03E-05,0.018601669,4.45E-07,3.13E-06,0.000160053,0.014005132,0.000274548,5.73E-07,0.863484939,0.000442736,0.102691296,4.00E-05
2783,natural_language_inference32,139,"While both datasets follow the conversational setting ( Section 2.1 ) , QuAC asked crowdworkers to highlight answer spans from the context and CoQA asked for free text as an answer to encourage natural dialog .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,3,1,0,,0.000328997,0,negative,3.26E-05,5.90E-06,0.000744405,1.36E-05,0.000101719,0.000158784,0.000228453,4.47E-05,7.47E-07,0.998524021,5.69E-05,8.21E-05,6.17E-06
2784,natural_language_inference32,140,"While this may call for a generation approach , shows that the an extractive approach which can handle Yes / No answers has a high upper-bound - 97.8 % F 1 .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,4,1,0,,0.002212126,0,negative,0.00132902,2.04E-06,0.000635442,7.63E-08,4.57E-07,2.17E-05,0.00015186,5.37E-05,3.91E-07,0.989905611,3.65E-05,0.007861705,1.52E-06
2785,natural_language_inference32,141,"Following this observation , we apply the extractive approach to CoQA .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,5,1,0,,0.007413982,0,negative,0.000453497,0.001166302,0.068616971,2.53E-06,2.24E-05,0.000250583,0.000931354,0.000743087,7.03E-05,0.923967054,0.00157811,0.002166546,3.13E-05
2786,natural_language_inference32,142,"We handle the Yes / No questions by computing P Y i , P Ni , the probability for answering yes and no , using the same equation as P ? i ( Eq. 17 ) , and find a span in the context for other questions .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,6,1,0,,9.77E-05,0,negative,0.000136158,3.37E-05,0.007349879,3.86E-08,5.88E-07,4.04E-05,2.53E-05,0.000108066,4.16E-05,0.992176207,1.52E-06,8.61E-05,3.82E-07
2787,natural_language_inference32,143,"The main evaluation metric is F 1 , the harmonic mean of precision and recall at the word level .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,7,1,0,,0.000288046,0,negative,1.30E-05,1.02E-05,0.000127758,1.28E-07,4.14E-07,8.49E-05,2.94E-05,0.000425604,4.17E-06,0.999257005,9.50E-06,3.72E-05,6.31E-07
2788,natural_language_inference32,144,"5 In CoQA , we report the performance for each context domain ( children 's story , literature from Project Gutenberg , middle and high school English exams , news articles from CNN , Wikipedia , AI2 Science Questions , Reddit articles ) and the over all performance .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,8,1,0,,0.00020871,0,negative,1.30E-05,1.67E-06,0.000112806,2.71E-07,6.54E-06,5.50E-05,5.76E-05,4.78E-05,2.57E-07,0.999574218,6.45E-07,0.000129785,4.69E-07
2789,natural_language_inference32,145,"For QuAC , we use its original evaluation metrics : F 1 and Human Equivalence Score ( HEQ ) .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,9,1,0,,0.000112538,0,negative,3.77E-05,5.77E-05,0.006695349,1.18E-07,1.48E-06,0.000113787,0.000206695,0.000506969,6.34E-06,0.99199865,1.71E-05,0.000356564,1.57E-06
2790,natural_language_inference32,146,"HEQ - Q is the accuracy of each question , where the answer is considered correct when the model 's F 1 score is higher than the average human F 1 score .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,10,1,0,,4.14E-05,0,negative,8.36E-06,2.50E-06,0.000231476,2.84E-07,3.30E-06,0.000117877,0.000106446,0.000158061,7.38E-07,0.999289302,8.67E-06,7.13E-05,1.73E-06
2791,natural_language_inference32,147,"Similarly , HEQ - D is the accuracy of each dialog - it is considered correct if all the questions in the dialog satisfy HEQ .",Data and Evaluation Metric,Data and Evaluation Metric,natural_language_inference,32,11,1,0,,2.43E-05,0,negative,1.39E-05,4.83E-06,0.000616487,8.14E-08,7.37E-07,4.81E-05,3.72E-05,0.000128124,2.94E-06,0.999065822,6.57E-06,7.41E-05,1.13E-06
2792,natural_language_inference32,148,Comparison Systems,,,natural_language_inference,32,0,1,0,,0.001243708,0,negative,3.34E-05,5.92E-05,0.000129977,1.54E-07,3.91E-07,9.35E-05,0.000169058,0.001079532,3.57E-05,0.995998069,0.000871314,0.001527937,1.75E-06
2793,natural_language_inference32,149,"We compare FLOWQA with baseline models previously tested on CoQA and QuAC . presented PGNet ( Seq2 Seq with copy mechanism ) , DrQA and DrQA + PGNet ( PGNet on predictions from DrQA ) to address abstractive answers .",Comparison Systems,Comparison Systems,natural_language_inference,32,1,1,0,,0.563588088,1,baselines,5.69E-05,4.34E-06,0.964064333,2.66E-06,4.58E-07,3.75E-05,0.00153432,0.000160864,9.23E-07,0.027644655,0.000132768,0.006104263,0.000256005
2794,natural_language_inference32,150,"To incorporate dialog history , CoQA baselines append the most recent previous question and answer to the current question .",Comparison Systems,Comparison Systems,natural_language_inference,32,2,1,0,,0.778078514,1,baselines,7.12E-05,3.34E-06,0.863992672,1.64E-06,1.48E-07,4.57E-05,0.000459665,0.000447655,2.24E-06,0.131564795,0.000445981,0.002729364,0.000235584
2795,natural_language_inference32,151,"applied BiDAF ++ , a strong extractive QA model to QuAC dataset .",Comparison Systems,Comparison Systems,natural_language_inference,32,3,1,1,baselines,0.244049911,0,baselines,0.000156679,1.93E-06,0.935032802,1.72E-06,4.11E-07,3.07E-05,0.00273833,0.000125373,4.59E-07,0.0345873,0.000142789,0.026845178,0.000336308
2796,natural_language_inference32,152,They append a feature vector encoding the turn number to the question embedding and a feature vector encoding previous N answer locations to the context embeddings ( denoted as N - ctx ) .,Comparison Systems,Comparison Systems,natural_language_inference,32,4,1,0,,0.77951687,1,baselines,1.59E-05,3.86E-07,0.982295398,2.19E-08,5.65E-09,2.14E-06,8.33E-06,3.36E-05,2.11E-06,0.017607261,3.97E-07,3.28E-05,1.66E-06
2797,natural_language_inference32,153,"Empirically , this performs better than just concatenating previous question / answer pairs .",Comparison Systems,Comparison Systems,natural_language_inference,32,5,1,0,,0.179842852,0,negative,0.002966732,1.43E-06,0.148218161,8.38E-07,2.16E-07,7.80E-05,0.001058853,0.000941192,8.25E-07,0.511292435,1.00E-05,0.335340468,9.08E-05
2798,natural_language_inference32,154,"Yatskar ( 2018 ) applied the same model to CoQA by modifying the system to first make a Yes / No decision , and output an answer span only if Yes / No was not selected .",Comparison Systems,Comparison Systems,natural_language_inference,32,6,1,0,,0.221362605,0,baselines,3.78E-05,1.05E-06,0.765079966,6.46E-07,6.83E-08,4.87E-05,0.000320096,0.000518568,3.25E-06,0.232880916,4.48E-05,0.000932017,0.000132123
2799,natural_language_inference32,155,"FLOWQA ( N - Ans ) is our model : similar to BiDAF ++ ( N - ctx ) , we append the binary feature vector encoding previous N answer spans to the context embeddings .",Comparison Systems,Comparison Systems,natural_language_inference,32,7,1,0,,0.940269878,1,baselines,5.30E-07,4.28E-08,0.999482576,3.94E-09,1.02E-09,4.83E-07,9.19E-06,3.04E-06,7.94E-08,0.000494566,1.39E-07,8.25E-06,1.10E-06
2800,natural_language_inference32,156,"Here we briefly describe the ablated systems : "" - FLOW "" removes the flow component from IF layer ( Eq. 2 in Section 3.2 ) , "" - QHIER - RNN "" removes the hierarchical LSTM layers on final question vectors ( Eq. 7 in Section 3.3 ) .",Comparison Systems,Comparison Systems,natural_language_inference,32,8,1,1,baselines,0.53332005,1,baselines,0.000105706,5.00E-07,0.830390109,2.10E-07,6.46E-08,7.05E-06,2.82E-05,6.60E-05,2.96E-06,0.169127813,4.39E-07,0.000262454,8.52E-06
2801,natural_language_inference32,157,"Results Tables 1 and 2 report model performance on CoQA and QuAC , respectively .",Comparison Systems,Comparison Systems,natural_language_inference,32,9,1,0,,0.912493814,1,results,0.001178887,1.90E-06,0.238234256,8.17E-07,3.51E-07,3.30E-05,0.003541402,0.00026643,5.55E-07,0.235393295,2.23E-05,0.521182584,0.000144223
2802,natural_language_inference32,158,"FLOWQA yields substantial improvement over existing models on both datasets ( + 7.2 % F 1 on CoQA , + 4.0 % F 1 on QuAC ) .",Comparison Systems,Comparison Systems,natural_language_inference,32,10,1,1,results,0.968071784,1,results,0.000990598,2.90E-07,0.021671151,5.46E-07,1.27E-07,6.66E-06,0.001672627,6.13E-05,8.29E-08,0.013763522,3.68E-06,0.961577496,0.000251954
2803,natural_language_inference32,159,"The larger gain on CoQA , which contains longer dialog chains , 7 suggests that our FLOW architecture can capture long - range conversation history more effectively .",Comparison Systems,Comparison Systems,natural_language_inference,32,11,1,0,,0.719001973,1,results,0.007227786,4.70E-07,0.160970336,3.91E-07,1.82E-07,1.11E-05,0.000776688,1.00E-04,3.47E-07,0.158195071,3.11E-06,0.672631425,8.32E-05
2804,natural_language_inference32,160,"shows the contributions of three components : ( 1 ) QHierRNN , the hierarchical LSTM layers for encoding past questions , ( 2 ) FLOW , augmenting the intermediate representation from the machine reasoning process in the conversation history , and ( 3 ) N - Ans , marking the gold answers to the previous N questions in the context .",Comparison Systems,Comparison Systems,natural_language_inference,32,12,1,0,,0.932543623,1,baselines,0.000117515,3.17E-07,0.974906118,4.50E-07,5.15E-08,3.28E-06,3.03E-05,2.32E-05,3.05E-06,0.024490245,8.16E-07,0.000391208,3.34E-05
2805,natural_language_inference32,161,We find that FLOW is a critical component .,Comparison Systems,Comparison Systems,natural_language_inference,32,13,1,1,results,0.922242637,1,negative,0.060154285,1.65E-05,0.243373924,5.10E-05,4.38E-06,0.000277528,0.001037968,0.005148738,4.53E-05,0.632069642,7.57E-06,0.057109812,0.00070332
2806,natural_language_inference32,162,"Removing QHier - RNN has a minor impact ( 0.1 % on both datasets ) , while removing FLOW results in a substantial performance drop , with or without using QHierRNN ( 2 - 3 % on QuAC , 4.1 % on CoQA ) .",Comparison Systems,Comparison Systems,natural_language_inference,32,14,1,1,results,0.972206971,1,ablation-analysis,0.44444379,4.04E-06,0.085702427,1.97E-05,1.98E-06,4.28E-05,0.002102183,0.000366601,3.68E-06,0.071650554,5.12E-06,0.395091181,0.000565957
2807,natural_language_inference32,163,"Without both components , our model performs comparably to the BiDAF ++ model ( 1.0 % gain ) .",Comparison Systems,Comparison Systems,natural_language_inference,32,15,1,0,,0.977074718,1,results,0.008555458,4.38E-07,0.023212861,7.19E-07,1.55E-07,8.77E-06,0.001209334,0.000115077,2.46E-07,0.024697223,1.49E-06,0.942005191,0.000193044
2808,natural_language_inference32,164,8,Comparison Systems,Comparison Systems,natural_language_inference,32,16,1,0,,0.001388688,0,negative,2.94E-05,2.89E-07,0.007469688,1.65E-07,3.47E-08,1.65E-05,1.57E-05,0.000342559,2.02E-06,0.991890172,2.50E-07,0.000224904,8.29E-06
2809,natural_language_inference32,165,Our model exploits the entire conversation history while prior models could leverage up to three previous turns .,Comparison Systems,Comparison Systems,natural_language_inference,32,17,1,0,,0.625158639,1,baselines,8.82E-05,2.77E-06,0.923322814,1.01E-07,6.56E-08,5.35E-06,2.53E-05,0.00011172,1.55E-05,0.076045007,5.51E-07,0.000367699,1.49E-05
2810,natural_language_inference32,166,"By comparing 0 - Ans and 1 - Ans on two datasets , we can see that providing gold answers is more crucial for QuAC .",Comparison Systems,Comparison Systems,natural_language_inference,32,18,1,1,results,0.923470422,1,results,0.003433787,3.17E-07,0.009008955,1.35E-07,1.11E-07,4.28E-06,0.000524539,7.75E-05,1.06E-07,0.062210614,1.31E-06,0.924697252,4.11E-05
2811,natural_language_inference32,167,We hypothesize that QuAC contains more open - ended questions with multiple valid answer spans because the questioner can not see the text .,Comparison Systems,Comparison Systems,natural_language_inference,32,19,1,0,,0.062170758,0,negative,0.000138764,1.92E-06,0.06088538,1.26E-06,8.04E-07,2.53E-05,0.000100468,0.000572647,3.85E-06,0.93481241,3.07E-06,0.00333521,0.000118964
2812,natural_language_inference32,168,The semantics of follow - up questions may change based on the answer span selected by the teacher among many valid answer spans .,Comparison Systems,Comparison Systems,natural_language_inference,32,20,1,0,,0.005816373,0,negative,6.47E-05,1.19E-06,0.053589584,5.98E-07,1.86E-07,1.13E-05,3.17E-05,0.000192611,3.50E-06,0.944554838,5.36E-06,0.001497288,4.71E-05
2813,natural_language_inference32,169,Knowing the selected answer span is thus important .,Comparison Systems,Comparison Systems,natural_language_inference,32,21,1,0,,0.052404021,0,negative,0.00043475,7.52E-07,0.013178933,1.24E-06,3.86E-07,2.18E-05,6.80E-05,0.000352438,2.07E-06,0.975747519,2.43E-06,0.010093026,9.67E-05
2814,natural_language_inference32,170,"We also measure the speedup of our proposed alternating parallel processing structure ) over the naive implementation of FLOW , where each question is processed in sequence .",Comparison Systems,Comparison Systems,natural_language_inference,32,22,1,0,,0.027981719,0,negative,0.000147425,2.44E-06,0.093688106,1.16E-07,1.35E-07,1.24E-05,0.000119034,0.000277938,1.73E-06,0.900639232,6.20E-07,0.005100697,1.02E-05
2815,natural_language_inference32,171,"Based on the training time each epoch takes ( i.e. , time needed for passing through the data once ) , the speedup is 8.1x on CoQA and 4.2 x on QuAC .",Comparison Systems,Comparison Systems,natural_language_inference,32,23,1,1,results,0.898009902,1,negative,0.004363115,2.97E-05,0.213743559,2.25E-05,7.32E-06,0.000726026,0.010103088,0.022239613,2.83E-05,0.598405369,6.12E-06,0.146408475,0.003916862
2816,natural_language_inference32,172,"The higher speedup on CoQA is due to the fact that CoQA has longer dialog sequences , compared to those in QuAC .",Comparison Systems,Comparison Systems,natural_language_inference,32,24,1,0,,0.071436271,0,results,0.002246976,6.26E-07,0.102637949,4.61E-07,4.39E-07,1.61E-05,0.001503626,0.000151226,3.79E-07,0.228654722,1.85E-06,0.664580986,0.000204672
2817,natural_language_inference32,173,EXPERIMENTS : SEQUENTIAL INSTRUCTION UNDERSTANDING,Comparison Systems,Comparison Systems,natural_language_inference,32,25,1,0,,0.640190686,1,results,0.003164908,6.32E-06,0.244065772,0.000229464,1.60E-05,0.000197978,0.040580231,0.000571286,4.11E-06,0.072482827,0.00015347,0.605315801,0.033211807
2818,natural_language_inference32,174,"In this section , we consider the task of understanding a sequence of natural language instructions .",Comparison Systems,Comparison Systems,natural_language_inference,32,26,1,0,,0.151191048,0,negative,0.000326417,2.81E-05,0.249808461,1.91E-05,1.05E-05,7.69E-05,0.001023425,0.00121834,1.50E-05,0.726512274,5.74E-05,0.019434949,0.001469186
2819,natural_language_inference32,175,We reduce this problem to a conversational MC task and apply FLOWQA .,Comparison Systems,Comparison Systems,natural_language_inference,32,27,1,0,,0.249922304,0,baselines,0.000381821,1.76E-05,0.848439202,1.27E-06,1.85E-06,1.26E-05,0.000380161,0.00020751,5.32E-06,0.137298846,4.72E-06,0.013081176,0.000167962
2820,natural_language_inference32,176,gives a simplified example of this task and our reduction .,Comparison Systems,Comparison Systems,natural_language_inference,32,28,1,0,,0.002746625,0,negative,5.86E-05,3.01E-07,0.036814322,3.96E-07,1.93E-07,8.76E-06,2.22E-05,9.90E-05,1.98E-06,0.962353117,1.71E-07,0.000608492,3.24E-05
2821,natural_language_inference32,177,"Task Given a sequence of instructions , where the meaning of each instruction may depend on the entire history and world state , the task is to understand the instructions and modify the world accordingly .",Comparison Systems,Comparison Systems,natural_language_inference,32,29,1,0,,0.008443245,0,negative,0.000561163,1.02E-05,0.108585168,0.000119665,4.04E-06,0.000275581,0.001521325,0.002750156,2.74E-05,0.852842785,0.00032485,0.023120602,0.009856987
2822,natural_language_inference32,178,"More formally , given the initial world state W 0 and a sequence of natural language instructions { I 1 , . . . , I K } , the model has to perform the correct sequence of actions on W 0 , to obtain { W 1 , . . . , W K } , the correct world states after each instruction .",Comparison Systems,Comparison Systems,natural_language_inference,32,30,1,0,,0.007757542,0,negative,3.90E-05,2.69E-06,0.035212572,4.69E-07,1.83E-07,1.61E-05,3.39E-05,0.000838841,3.05E-05,0.963101745,1.71E-06,0.000641457,8.08E-05
2823,natural_language_inference32,179,Reduction,Comparison Systems,,natural_language_inference,32,31,1,0,,0.04242704,0,negative,0.00086389,1.57E-06,0.123595475,4.10E-06,6.05E-07,3.31E-05,0.000134109,0.000395013,1.73E-05,0.871916355,8.96E-07,0.002752179,0.000285409
2824,natural_language_inference32,180,We reduce sequential instruction understanding to machine comprehension as follows .,Comparison Systems,Reduction,natural_language_inference,32,32,1,0,,0.00034838,0,negative,0.000686828,0.000181493,0.093973312,1.46E-05,2.04E-05,8.99E-05,0.000322344,0.000395718,0.000119636,0.899360346,0.000611028,0.003321953,0.000902446
2825,natural_language_inference32,181,Context Ci :,Comparison Systems,Reduction,natural_language_inference,32,33,1,0,,1.54E-05,0,negative,9.53E-05,9.92E-06,0.225105049,4.02E-08,3.47E-07,4.11E-06,2.15E-05,2.77E-05,3.82E-05,0.772886732,7.61E-06,0.001788284,1.53E-05
2826,natural_language_inference32,182,We encode the current world state W i?1 as a sequence of tokens .,Comparison Systems,Reduction,natural_language_inference,32,34,1,0,,8.40E-06,0,negative,3.47E-05,6.43E-06,0.002080803,9.96E-08,3.03E-07,8.64E-06,2.57E-06,0.000156751,0.000137062,0.997551562,2.37E-07,1.53E-05,5.54E-06
2827,natural_language_inference32,183,Question Q i :,Comparison Systems,Reduction,natural_language_inference,32,35,1,0,,1.08E-06,0,negative,9.91E-06,4.24E-07,3.72E-05,4.52E-08,7.43E-08,2.92E-06,1.33E-06,3.65E-05,1.30E-06,0.999860394,8.03E-07,4.73E-05,1.71E-06
2828,natural_language_inference32,184,We simply treat each natural language instruction I i as a question .,Comparison Systems,Reduction,natural_language_inference,32,36,1,0,,5.78E-07,0,negative,1.48E-05,6.44E-07,0.000175885,1.90E-08,1.30E-07,1.47E-06,4.02E-07,1.41E-05,3.50E-06,0.999777413,2.84E-08,1.12E-05,4.15E-07
2829,natural_language_inference32,185,Answer A i : We encode the world state change from W i?1 to W i as a sequence of tokens .,Comparison Systems,Reduction,natural_language_inference,32,37,1,0,,3.64E-06,0,negative,2.38E-05,2.12E-07,0.000314175,1.59E-08,7.33E-08,3.15E-06,1.75E-06,1.55E-05,1.04E-06,0.999569824,9.34E-08,6.89E-05,1.49E-06
2830,natural_language_inference32,186,"At each time step i , the current context Ci and question Q i are given to the system , which outputs the answer A i .",Comparison Systems,Reduction,natural_language_inference,32,38,1,0,,3.52E-06,0,negative,2.03E-05,2.32E-06,0.000407024,1.65E-08,1.15E-07,9.20E-07,6.58E-07,2.15E-05,1.33E-05,0.999504651,1.06E-07,2.81E-05,9.65E-07
2831,natural_language_inference32,187,"Given A i , the next world state C i + 1 is automatically mapped from the reduction rules .",Comparison Systems,Reduction,natural_language_inference,32,39,1,0,,1.05E-06,0,negative,5.19E-05,1.54E-06,0.001243761,1.14E-08,1.26E-07,6.67E-07,6.03E-07,9.93E-06,1.10E-05,0.99864286,4.10E-08,3.68E-05,7.25E-07
2832,natural_language_inference32,188,We encode the history of instructions explicitly by concatenating preceding questions and the current one and by marking previous answers in the current context similar to N - Ans in conversational MC tasks .,Comparison Systems,Reduction,natural_language_inference,32,40,1,0,,7.58E-05,0,negative,0.00024035,8.95E-05,0.06348401,1.34E-07,1.43E-06,6.26E-06,1.21E-05,7.33E-05,0.000582822,0.935254452,1.43E-06,0.000240798,1.34E-05
2833,natural_language_inference32,189,"Further , we simplify FLOWQA to prevent overfitting .",Comparison Systems,Reduction,natural_language_inference,32,41,1,0,,0.030052027,0,negative,0.021568544,8.86E-05,0.020892314,1.83E-06,1.35E-05,2.25E-05,4.38E-05,0.000207102,0.000107566,0.954544033,1.19E-07,0.002474301,3.57E-05
2834,natural_language_inference32,190,Appendix C.2 contains the details on : Illustration on reducing sequential instruction understanding to conversational MC .,Comparison Systems,Reduction,natural_language_inference,32,42,1,0,,1.93E-06,0,negative,6.99E-05,1.36E-07,7.91E-05,1.17E-06,1.61E-06,6.76E-06,1.93E-06,1.24E-05,4.28E-07,0.999752023,6.34E-08,6.97E-05,4.72E-06
2835,natural_language_inference32,191,The corresponding units in the semantic parsing approach are shown to the right .,Comparison Systems,Reduction,natural_language_inference,32,43,1,0,,1.52E-06,0,negative,1.08E-05,5.55E-07,0.000139759,3.99E-08,1.21E-07,3.73E-06,1.26E-06,4.52E-05,5.91E-06,0.999776295,3.11E-08,1.42E-05,2.15E-06
2836,natural_language_inference32,192,This simplified example is from .,Comparison Systems,Reduction,natural_language_inference,32,44,1,0,,5.93E-07,0,negative,6.19E-06,7.91E-08,3.35E-05,1.55E-08,1.29E-07,7.09E-07,2.64E-07,4.78E-06,3.92E-07,0.999943444,6.94E-09,1.02E-05,2.57E-07
2837,natural_language_inference32,193,Sce. Tan. Alc.,Comparison Systems,,natural_language_inference,32,45,1,0,,0.01440585,0,negative,0.000229188,8.84E-07,0.047621972,7.39E-07,4.58E-07,1.86E-05,0.00033073,0.000627787,2.85E-06,0.923820944,9.51E-07,0.026531067,0.000813818
2838,natural_language_inference32,194,Suhr & Artzi,Comparison Systems,,natural_language_inference,32,46,1,0,,0.035029729,0,negative,0.00055938,4.34E-07,0.224033002,5.19E-07,3.23E-07,1.12E-05,0.00037817,0.000188633,3.16E-06,0.743386227,2.94E-07,0.030883827,0.000554786
2839,natural_language_inference32,195,RESULTS,,,natural_language_inference,32,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
2840,natural_language_inference32,196,"We evaluate our model on the sequential instruction understanding dataset , SCONE , which contains three domains ( SCENE , TANGRAMS , ALCHEMY ) .",RESULTS,RESULTS,natural_language_inference,32,1,1,0,,0.40338733,0,negative,0.006624068,0.000308529,0.003904121,3.30E-05,7.14E-05,0.00026157,0.006693866,0.001074915,6.13E-05,0.72311337,0.000255047,0.256614588,0.000984241
2841,natural_language_inference32,197,Each domain has a different environment setting ( see Appendix C.2 ) .,RESULTS,RESULTS,natural_language_inference,32,2,1,0,,0.00534389,0,negative,0.006037557,1.31E-05,0.001469322,3.97E-06,6.79E-06,3.96E-05,0.000198886,0.000171148,2.22E-05,0.953640551,1.15E-05,0.038323433,6.19E-05
2842,natural_language_inference32,198,"We compare our approaches with previous semantic parsing approaches , which map each instruction into a logical form , and then execute the logical form to update the world state , and , which maps each instruction into actions .",RESULTS,RESULTS,natural_language_inference,32,3,1,0,,0.619020999,1,negative,0.011903866,0.000966129,0.034943079,4.06E-05,6.09E-05,0.000395317,0.00376493,0.001442209,0.000423232,0.828612302,0.000474614,0.116217915,0.000754853
2843,natural_language_inference32,199,The model performance is evaluated by the correctness of the final world state after five instructions .,RESULTS,RESULTS,natural_language_inference,32,4,1,0,,0.092912736,0,negative,0.005803399,1.88E-05,0.00048879,3.90E-06,5.25E-06,0.000142674,0.001578478,0.001052855,1.83E-05,0.900103126,2.39E-05,0.090519837,0.000240618
2844,natural_language_inference32,200,"Our learning set - up is similar to that of , where the supervision is the change in world states ( i.e. , analogous to logical form ) , while that of and used world states as a supervision .",RESULTS,RESULTS,natural_language_inference,32,5,1,0,,0.00173773,0,negative,0.006015795,0.000600045,0.018477218,8.66E-05,7.95E-05,0.000402927,0.000882316,0.002503792,0.001290921,0.958219152,0.000161345,0.010406028,0.000874361
2845,natural_language_inference32,201,The development and test set results are reported in .,RESULTS,RESULTS,natural_language_inference,32,6,1,0,,0.036199467,0,negative,0.004836507,1.51E-06,0.000147321,7.48E-07,1.74E-06,1.08E-05,0.000843222,4.57E-05,8.56E-07,0.584262012,4.33E-06,0.40979189,5.34E-05
2846,natural_language_inference32,202,"Even without FLOW , our model ( FLOWQA - FLOW ) achieves comparable results in two domains ( Tangrams and Alchemy ) since we still encode the history explicitly .",RESULTS,RESULTS,natural_language_inference,32,7,1,0,,0.960683521,1,results,0.002242298,6.71E-07,7.72E-05,8.74E-07,3.99E-07,7.35E-06,0.00297703,4.01E-05,2.47E-07,0.015437612,8.77E-06,0.978920605,0.000286901
2847,natural_language_inference32,203,"When augmented with FLOW , our FLOWQA model gains decent improvements and outperforms the state - of - the - art models for all three domains .",RESULTS,RESULTS,natural_language_inference,32,8,1,0,,0.95472897,1,results,0.016924072,1.77E-06,7.94E-05,4.61E-06,1.29E-06,1.55E-05,0.005129032,8.88E-05,8.63E-07,0.013400868,8.11E-06,0.963500734,0.000844893
2848,natural_language_inference32,204,RELATED WORK,RESULTS,RESULTS,natural_language_inference,32,9,1,0,,0.000479618,0,negative,0.001738122,1.07E-05,0.00054307,9.86E-06,6.11E-06,0.000164044,0.00088991,0.000471871,2.33E-05,0.969872586,0.00011722,0.025480367,0.000672778
2849,natural_language_inference32,205,"Sequential question answering has been studied in the knowledge base setting , often framed as a semantic parsing problem .",RESULTS,RESULTS,natural_language_inference,32,10,1,0,,0.774325774,1,negative,0.003900344,0.000129664,0.002456177,0.00038905,3.78E-05,0.000629307,0.024393739,0.001408877,9.91E-05,0.611843489,0.154284656,0.14723017,0.053197626
2850,natural_language_inference32,206,"Recent datasets enabled studying it in the textual setting , where the information source used to answer questions is a given article .",RESULTS,RESULTS,natural_language_inference,32,11,1,0,,0.069465843,0,negative,0.003998892,3.48E-05,0.000799438,0.000453305,2.88E-05,0.000705566,0.004319243,0.001386366,7.96E-05,0.911580543,0.00927248,0.05174307,0.01559797
2851,natural_language_inference32,207,"Existing approaches attempted on these datasets are often extensions of strong single - turn models , such as BiDAF and DrQA , with some manipulation of the input .",RESULTS,RESULTS,natural_language_inference,32,12,1,0,,0.005728375,0,negative,0.003219722,3.06E-05,0.001411422,0.000205428,3.90E-05,0.00030951,0.004698486,0.000542876,3.24E-05,0.8589934,0.002067253,0.122292375,0.006157501
2852,natural_language_inference32,208,"In contrast , we propose a new architecture suitable for multi-turn MC tasks by passing the hidden model representations of preceding questions using the FLOW design .",RESULTS,RESULTS,natural_language_inference,32,13,1,0,,0.604802839,1,negative,0.073378932,0.002749965,0.227573165,9.40E-05,0.000116232,0.000331178,0.002208454,0.001514533,0.009250698,0.558514102,0.001022088,0.119927592,0.003319021
2853,natural_language_inference32,209,Dialog response generation requires reasoning about the conversation history as in conversational MC .,RESULTS,RESULTS,natural_language_inference,32,14,1,0,,0.056119046,0,negative,0.004841348,0.000124034,0.002845707,0.000416602,4.18E-05,0.0005814,0.019000853,0.001565102,0.000135792,0.557449613,0.045104478,0.288768844,0.079124379
2854,natural_language_inference32,210,"This has been studied in social chit - chats ( e.g. , and goal - oriented dialogs ( e.g. , .",RESULTS,RESULTS,natural_language_inference,32,15,1,0,,0.006453983,0,negative,0.001514803,2.73E-05,0.000853069,2.78E-05,8.18E-06,0.000167821,0.001725082,0.000657244,4.25E-05,0.951435706,0.002027311,0.038700159,0.002813018
2855,natural_language_inference32,211,Prior work also modeled hierarchical representation of the conversation history .,RESULTS,RESULTS,natural_language_inference,32,16,1,0,,0.002623726,0,negative,0.003039587,7.61E-05,0.003268586,7.53E-05,2.33E-05,0.000485413,0.001297939,0.001915385,0.00023362,0.962841175,0.000853743,0.021981529,0.003908394
2856,natural_language_inference32,212,"While these tasks target reasoning with the knowledge base or exclusively on the conversation history , the main challenge in conversational MC lies in reasoning about context based on the conversation history , which is the main focus in our work .",RESULTS,RESULTS,natural_language_inference,32,17,1,0,,0.186649536,0,negative,0.004778938,7.09E-05,0.001194586,0.000312832,3.08E-05,0.000373832,0.00744598,0.001134644,7.80E-05,0.750831737,0.014352942,0.192561924,0.02683288
2857,natural_language_inference32,213,CONCLUSION,,,natural_language_inference,32,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
2858,text_summarization8,1,title,,,text_summarization,8,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
2859,text_summarization8,2,Bottom - Up Abstractive Summarization,title,,text_summarization,8,1,1,1,research-problem,0.988133454,1,research-problem,2.88E-06,6.77E-05,3.57E-06,5.25E-05,4.85E-05,4.71E-06,0.000134771,8.49E-06,1.01E-06,0.009365433,0.990296043,1.00E-05,4.42E-06
2860,text_summarization8,3,abstract,,,text_summarization,8,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
2861,text_summarization8,4,"Neural network - based methods for abstractive summarization produce outputs that are more fluent than other techniques , but perform poorly at content selection .",abstract,abstract,text_summarization,8,1,1,0,,0.171015277,0,research-problem,5.30E-08,3.74E-06,3.98E-08,2.93E-07,1.67E-07,9.04E-08,9.93E-07,5.40E-07,9.83E-08,0.003676248,0.996317542,1.30E-07,6.03E-08
2862,text_summarization8,5,This work proposes a simple technique for addressing this issue : use a data- efficient content selector to over - determine phrases in a source document that should be part of the summary .,abstract,abstract,text_summarization,8,2,1,0,,0.172173393,0,research-problem,4.38E-05,0.083135087,7.14E-05,0.000127193,0.000891923,3.54E-05,1.87E-05,0.000191966,0.001480193,0.349512284,0.564481378,6.79E-06,3.99E-06
2863,text_summarization8,6,We use this selector as a bottom - up attention step to constrain the model to likely phrases .,abstract,abstract,text_summarization,8,3,1,0,,0.141769298,0,negative,1.63E-05,0.178426279,0.000139735,1.13E-05,0.000127657,0.000135862,8.23E-06,0.002323991,0.279559117,0.513728885,0.025519077,1.70E-06,1.85E-06
2864,text_summarization8,7,"We show that this approach improves the ability to compress text , while still generating fluent summaries .",abstract,abstract,text_summarization,8,4,1,0,,0.210432756,0,negative,0.000189465,0.019288332,2.09E-05,6.08E-06,9.94E-05,1.14E-05,1.82E-05,0.000210867,0.000938411,0.744575909,0.234498647,0.000140663,1.67E-06
2865,text_summarization8,8,"This two - step process is both simpler and higher performing than other end - toend content selection models , leading to significant improvements on ROUGE for both the CNN - DM and NYT corpus .",abstract,abstract,text_summarization,8,5,1,0,,0.030340734,0,negative,0.000404709,0.062904535,5.92E-05,5.83E-05,0.000934344,3.96E-05,8.71E-05,0.000481712,0.001825626,0.654256541,0.278663548,0.000275226,9.65E-06
2866,text_summarization8,9,"Furthermore , the content selector can be trained with as little as 1,000 sentences , making it easy to transfer a trained summarizer to a new domain .",abstract,abstract,text_summarization,8,6,1,0,,0.143528748,0,negative,1.99E-05,0.018656612,7.59E-06,8.28E-05,0.000296599,0.00012831,3.05E-05,0.001186422,0.001450737,0.602605277,0.375517207,1.25E-05,5.53E-06
2867,text_summarization8,10,Introduction,,,text_summarization,8,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
2868,text_summarization8,11,Text summarization systems aim to generate natural language summaries that compress the information in a longer text .,Introduction,Introduction,text_summarization,8,1,1,1,research-problem,0.551490707,1,research-problem,8.18E-07,7.21E-05,2.60E-07,1.24E-05,2.03E-05,4.84E-06,1.27E-05,3.92E-06,1.84E-05,0.032608657,0.96724219,1.05E-06,2.40E-06
2869,text_summarization8,12,Approaches using neural networks have shown promising results on this task with end - to - end models that encode a source document and then decode it into an abstractive summary .,Introduction,Introduction,text_summarization,8,2,1,0,,0.051104353,0,research-problem,1.27E-06,0.000203598,4.57E-07,3.40E-06,7.07E-06,1.03E-05,1.21E-05,1.42E-05,0.000145054,0.103155089,0.896444092,1.88E-06,1.44E-06
2870,text_summarization8,13,Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .,Introduction,Introduction,text_summarization,8,3,1,1,research-problem,0.310824283,0,research-problem,2.80E-06,0.000379842,1.78E-06,1.76E-05,4.28E-05,1.33E-05,2.62E-05,1.06E-05,8.93E-05,0.063585229,0.935824596,2.40E-06,3.57E-06
2871,text_summarization8,14,"These end - to - end models produce fluent abstractive summaries but have had mixed success in content selection , i.e. deciding what to summarize , compared to fully extractive models .",Introduction,Introduction,text_summarization,8,4,1,0,,0.154652589,0,research-problem,4.90E-06,0.00042387,1.93E-06,7.01E-05,0.00016979,5.26E-05,4.62E-05,2.58E-05,0.000100092,0.266695185,0.73239937,4.17E-06,6.09E-06
2872,text_summarization8,15,"There is an appeal to end - to - end models from a modeling perspective ; however , there is evidence that when summarizing people follow a two - step Source Document german chancellor angela merkel [ did ] not too pleased about the weather during her easter holiday as britain in and temperatures of up to 21 c , mrs merkel and her husband [ , chemistry professor joachim sauer , ] had to settle for a measly 12 degrees .",Introduction,Introduction,text_summarization,8,5,1,0,,0.01689867,0,negative,5.75E-06,0.000343096,1.47E-06,4.03E-05,0.000301585,5.70E-05,1.91E-05,1.99E-05,0.000237882,0.876776385,0.122192828,2.78E-06,1.95E-06
2873,text_summarization8,16,"the chancellor and her have been spending easter on the small island of ischia , near naples in the mediterranean for over a [ not so sunny : ] angela merkel her husband are spotted on their easter trip to the island of ischia near naples [ .",Introduction,Introduction,text_summarization,8,6,1,0,,0.004041444,0,negative,9.98E-06,0.002259771,3.05E-06,9.36E-05,0.00308452,0.000158672,2.75E-05,5.67E-05,0.001268316,0.979289405,0.013741641,3.95E-06,2.85E-06
2874,text_summarization8,17,"the ] couple spend their holiday at the fivestar miramare spa hotel on the south of the island [ , which comes ] with it s own private beach [ , and balconies overlooking the ] ocean ...",Introduction,Introduction,text_summarization,8,7,1,0,,0.001570002,0,negative,1.21E-05,0.00286413,4.93E-06,0.000328673,0.008307675,0.000385897,3.98E-05,8.97E-05,0.000955887,0.980776702,0.006225969,3.58E-06,4.99E-06
2875,text_summarization8,18,Reference angela merkel and husband spotted while on italian island holiday .,Introduction,Introduction,text_summarization,8,8,1,0,,0.001260931,0,negative,7.01E-06,0.000587967,1.54E-06,7.73E-05,0.000550566,0.000161225,2.45E-05,6.09E-05,0.000754946,0.988880842,0.008888706,2.76E-06,1.78E-06
2876,text_summarization8,19,. . .,Introduction,Introduction,text_summarization,8,9,1,0,,0.002037853,0,negative,8.10E-06,0.002095784,2.79E-06,5.64E-06,7.96E-05,0.000103225,1.29E-05,9.05E-05,0.014388023,0.979457533,0.003752473,2.53E-06,9.20E-07
2877,text_summarization8,20,Baseline Approach,,,text_summarization,8,0,1,0,,0.000481606,0,negative,3.56E-05,0.000271906,8.38E-05,5.62E-06,2.45E-06,0.000560494,0.000339481,0.004029007,0.000228211,0.983963183,0.010016897,0.000440963,2.24E-05
2878,text_summarization8,21,"angela merkel and her husband , chemistry professor joachim sauer , are spotted on their annual easter trip to the island of ischia , near naples .",Baseline Approach,Baseline Approach,text_summarization,8,1,1,0,,0.003314099,0,negative,1.41E-05,0.000243363,0.000704413,5.26E-05,5.44E-05,0.000101637,0.000100257,0.000518528,9.71E-05,0.99690909,0.001139893,5.87E-05,5.93E-06
2879,text_summarization8,22,. . .,Baseline Approach,Baseline Approach,text_summarization,8,2,1,0,,0.000217244,0,negative,3.71E-06,4.32E-05,0.000260491,5.02E-07,5.69E-07,2.27E-05,5.84E-06,0.00032328,0.000111129,0.999183059,3.46E-05,1.06E-05,2.69E-07
2880,text_summarization8,23,Bottom - Up Summarization,Baseline Approach,,text_summarization,8,3,1,0,,0.639459693,1,research-problem,0.000274306,0.002892087,0.136775808,5.85E-05,0.000157175,0.000230392,0.066257909,0.001862008,0.000127474,0.277696243,0.387390334,0.125520093,0.000757671
2881,text_summarization8,24,"angela merkel and her husband are spotted on their easter trip to the island of ischia , near naples .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,4,1,0,,4.01E-05,0,negative,2.74E-05,6.23E-07,3.39E-05,2.44E-06,3.82E-07,2.55E-05,1.68E-05,2.06E-05,5.45E-06,0.999625725,4.00E-06,0.0002318,5.52E-06
2882,text_summarization8,25,. . . approach of first selecting important phrases and then paraphrasing them .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,5,1,0,,7.40E-06,0,negative,0.000126055,1.13E-06,0.000267084,7.39E-05,1.22E-06,7.26E-05,2.73E-05,1.86E-05,2.26E-05,0.999100637,3.40E-05,0.000218449,3.65E-05
2883,text_summarization8,26,A similar argument has been made for image captioning .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,6,1,0,,2.66E-05,0,negative,8.22E-06,2.90E-07,3.86E-05,1.26E-07,2.20E-08,7.98E-06,1.64E-05,1.78E-05,1.55E-06,0.999362194,8.43E-05,0.000458465,4.04E-06
2884,text_summarization8,27,develop a state - of - the - art model with a two - step approach that first pre-computes bounding boxes of segmented objects and then applies attention to these regions .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,7,1,0,,0.00071564,0,negative,0.000131113,1.37E-05,0.006549432,6.14E-06,1.07E-06,0.000100952,0.000182845,0.000158087,7.48E-05,0.988725048,0.000175085,0.003704216,0.000177514
2885,text_summarization8,28,This so - called bottom - up attention is inspired by neuroscience research describing attention based on properties in - herent to a stimulus .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,8,1,0,,4.72E-05,0,negative,1.03E-05,2.49E-06,0.001179601,2.29E-07,5.82E-08,2.02E-05,2.69E-05,4.35E-05,6.88E-05,0.998391324,5.84E-05,0.000189426,8.71E-06
2886,text_summarization8,29,"Motivated by this approach , we consider bottom - up attention for neural abstractive summarization .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,9,1,1,approach,0.019962729,0,negative,0.001006,0.00053242,0.054466758,2.09E-05,1.02E-05,0.000144114,0.001769999,0.000303746,0.000296433,0.882038447,0.004595953,0.054311415,0.000503536
2887,text_summarization8,30,Our approach first selects a selection mask for the source document and then constrains a standard neural model by this mask .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,10,1,1,approach,0.004740619,0,negative,0.00102736,0.000187494,0.006667495,3.67E-06,4.04E-06,3.44E-05,7.39E-05,0.000172795,0.000410865,0.988032861,3.02E-05,0.003330335,2.46E-05
2888,text_summarization8,31,"This approach can better decide which phrases a model should include in a summary , without sacrificing the fluency advantages of neural abstractive summarizers .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,11,1,0,,0.000432383,0,negative,0.000322739,5.01E-06,0.000776683,1.44E-07,2.18E-07,3.53E-06,1.74E-05,1.04E-05,2.76E-05,0.991053864,4.42E-06,0.007775624,2.32E-06
2889,text_summarization8,32,"Furthermore , it requires much fewer data to train , which makes it more adaptable to new domains .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,12,1,0,,6.36E-05,0,negative,0.000262407,4.03E-06,0.000400686,6.66E-07,4.61E-07,1.07E-05,2.49E-05,3.04E-05,1.29E-05,0.997586975,6.34E-06,0.001652138,7.33E-06
2890,text_summarization8,33,Our full model incorporates a separate content selection system to decide on relevant aspects of the source document .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,13,1,1,approach,0.000232722,0,negative,0.000168214,2.05E-05,0.007179292,5.12E-07,5.40E-07,2.10E-05,3.95E-05,6.82E-05,0.001487765,0.990723848,7.35E-06,0.000265277,1.80E-05
2891,text_summarization8,34,"We frame this selection task as a sequence - tagging problem , with the objective of identifying tokens from a document that are part of its summary .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,14,1,1,approach,0.000619541,0,negative,0.000177682,0.000120311,0.002397315,3.81E-06,3.34E-06,2.54E-05,8.34E-05,9.28E-05,0.000183577,0.990326286,0.000222967,0.006312288,5.09E-05
2892,text_summarization8,35,"We show that a content selection model that builds on contextual word embeddings can identify correct tokens with a recall of over 60 % , and a precision of over 50 % .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,15,1,0,,0.100494553,0,negative,0.001594242,1.79E-05,0.000712358,7.02E-07,9.78E-07,1.46E-05,0.000343351,7.26E-05,2.41E-05,0.807474205,2.16E-05,0.18969276,3.06E-05
2893,text_summarization8,36,"To incorporate bottom - up attention into abstractive summarization models , we employ masking to constrain copying words to the selected parts of the text , which produces grammatical outputs .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,16,1,1,approach,0.01901135,0,negative,0.001545364,0.000406909,0.026213977,7.48E-06,1.08E-05,8.64E-05,0.000734694,0.000258081,0.000639589,0.94761839,0.000183741,0.022170252,0.000124326
2894,text_summarization8,37,"We additionally experiment with multiple methods to incorporate similar constraints into the training process of more complex end - to - end abstractive summarization models , either through multi-task learning or through directly incorporating a fully differentiable mask .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,17,1,0,,0.001828371,0,negative,0.000270424,0.000107947,0.003380541,9.71E-07,2.76E-06,3.10E-05,0.000188385,0.000118998,7.85E-05,0.991067125,6.14E-06,0.004733799,1.34E-05
2895,text_summarization8,38,Our experiments compare bottom - up attention with several other state - of - the - art abstractive systems .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,18,1,0,,0.001354359,0,negative,6.40E-05,1.23E-05,0.001036005,3.23E-07,1.64E-06,1.34E-05,0.000111588,4.15E-05,9.94E-06,0.995521924,1.36E-06,0.003180215,5.84E-06
2896,text_summarization8,39,Compared to our baseline models of bottom - up attention leads to an improvement in ROUGE - L score on the CNN - Daily Mail ( CNN - DM ) corpus from 36.4 to 38.3 while being simpler to train .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,19,1,0,,0.46954109,0,results,0.002017044,2.59E-07,6.01E-05,1.50E-07,1.08E-07,2.92E-06,0.000942245,1.10E-05,1.98E-07,0.067427134,6.00E-07,0.929508205,3.00E-05
2897,text_summarization8,40,We also see comparable or better results than recent reinforcement - learning based methods with our MLE trained system .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,20,1,0,,0.06600621,0,results,0.00038505,2.85E-07,4.64E-05,2.01E-07,9.86E-08,7.64E-06,0.001063587,2.46E-05,3.17E-07,0.236032177,1.31E-06,0.762407469,3.09E-05
2898,text_summarization8,41,"Furthermore , we find that the content selection model is very data - efficient and can be trained with less than 1 % of the original training data .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,21,1,0,,0.344423131,0,negative,0.003499316,1.37E-05,0.000145322,3.25E-06,1.62E-06,5.50E-05,0.001870189,0.00038478,1.40E-05,0.5802026,7.99E-06,0.413681435,0.000120786
2899,text_summarization8,42,This provides opportunities for domain - transfer and lowresource summarization .,Baseline Approach,Bottom - Up Summarization,text_summarization,8,22,1,0,,0.000576606,0,negative,0.000175924,1.07E-06,0.000273961,2.80E-07,4.43E-07,2.96E-06,2.26E-05,4.41E-06,4.94E-06,0.989242756,1.96E-06,0.010262059,6.62E-06
2900,text_summarization8,43,"We show that a summarization model trained on CNN - DM and evaluated on the NYT corpus can be improved by over 5 points in ROUGE - L with a content selector trained on only 1,000 in - domain sentences .",Baseline Approach,Bottom - Up Summarization,text_summarization,8,23,1,0,,0.094739998,0,results,0.001314784,6.53E-06,0.00031869,5.71E-07,6.27E-07,1.36E-05,0.001100366,7.00E-05,5.99E-06,0.447047938,6.30E-06,0.550044617,6.99E-05
2901,text_summarization8,44,Related Work,,,text_summarization,8,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
2902,text_summarization8,71,Background : Neural Summarization,,,text_summarization,8,0,1,0,,0.082138877,0,research-problem,2.54E-05,4.38E-05,5.08E-05,2.76E-06,9.44E-07,1.37E-05,0.000886302,7.16E-05,4.76E-06,0.04457786,0.953123272,0.001166659,3.21E-05
2903,text_summarization8,162,Data and Experiments,,,text_summarization,8,0,1,0,,0.001363023,0,negative,5.29E-06,7.12E-06,1.67E-06,1.19E-07,3.13E-07,1.88E-05,1.28E-05,0.000155735,3.28E-06,0.9995443,0.00019946,5.09E-05,3.06E-07
2904,text_summarization8,163,"We evaluate our approach on the CNN - DM corpus , and the NYT corpus , which are both standard corpora for news summarization .",Data and Experiments,Data and Experiments,text_summarization,8,1,1,0,,0.039530717,0,negative,0.000240009,0.000135489,0.006887716,1.65E-06,7.63E-06,0.00066606,0.00247811,0.002683534,7.88E-06,0.984065714,2.71E-05,0.002774626,2.45E-05
2905,text_summarization8,164,"The summaries for the CNN - DM corpus are bullet points for the articles shown on their respective websites , whereas the NYT corpus contains summaries written by library scientists .",Data and Experiments,Data and Experiments,text_summarization,8,2,1,0,,0.001961697,0,negative,5.94E-05,2.73E-06,0.000933384,7.93E-06,6.78E-05,0.000657873,0.00037293,0.000267323,1.06E-06,0.997522018,1.79E-06,9.67E-05,9.10E-06
2906,text_summarization8,165,"CNN - DM summaries are full sentences , with on average 66 tokens (? = 26 ) and 4.9 bullet points .",Data and Experiments,Data and Experiments,text_summarization,8,3,1,0,,0.191259258,0,negative,0.000358116,1.95E-05,0.010041843,9.88E-05,0.000456574,0.005500316,0.007155857,0.002097218,5.08E-06,0.972916587,8.67E-06,0.001088867,0.000252573
2907,text_summarization8,166,"NYT summaries are not always complete sentences and are shorter , with on average 40 tokens (? = 27 ) and 1.9 bullet points .",Data and Experiments,Data and Experiments,text_summarization,8,4,1,0,,0.020910778,0,negative,0.000231063,1.19E-06,0.001436538,3.63E-05,9.30E-05,0.000542557,0.002159317,9.74E-05,4.28E-07,0.993794359,3.88E-05,0.001453809,0.000115365
2908,text_summarization8,167,"Following , we use the non-anonymized version of the CNN - DM corpus and truncate source documents to 400 tokens and the target summaries to 100 tokens in training and validation sets .",Data and Experiments,Data and Experiments,text_summarization,8,5,1,0,,0.031358993,0,negative,0.000794537,8.49E-05,0.010917802,2.88E-06,1.66E-05,0.002075115,0.002886504,0.008705811,1.41E-05,0.973622588,2.46E-06,0.000842023,3.47E-05
2909,text_summarization8,168,"For experiments with the NYT corpus , we use the preprocessing described by , and additionally remove author information and truncate source documents to 400 tokens instead of 800 .",Data and Experiments,Data and Experiments,text_summarization,8,6,1,0,,0.029851093,0,negative,0.001180498,3.19E-05,0.007835765,2.66E-06,1.44E-05,0.002348137,0.003273396,0.004674012,6.41E-06,0.978742715,2.16E-06,0.001856147,3.18E-05
2910,text_summarization8,169,"These changes lead to an average of 326 tokens per article , a decrease from the 549 tokens with 800 token truncated articles .",Data and Experiments,Data and Experiments,text_summarization,8,7,1,0,,0.00790939,0,negative,0.083666353,2.12E-05,0.026508929,1.49E-05,7.87E-05,0.000454123,0.002146362,0.000464315,2.71E-05,0.873783235,6.30E-06,0.012681593,0.000146936
2911,text_summarization8,170,"The target ( non - copy ) vocabulary is limited to 50,000 tokens for all models .",Data and Experiments,Data and Experiments,text_summarization,8,8,1,0,,0.454975164,0,hyperparameters,8.76E-05,7.72E-05,0.001414236,4.29E-06,2.20E-06,0.061394956,0.006542957,0.644335316,6.21E-05,0.285790118,1.13E-05,0.000115782,0.000161811
2912,text_summarization8,171,"The content selection model uses pre-trained GloVe embeddings of size 100 , and ELMo with size 1024 .",Data and Experiments,Data and Experiments,text_summarization,8,9,1,0,,0.823136077,1,hyperparameters,8.61E-05,7.16E-05,0.002667664,1.25E-05,2.38E-06,0.143213142,0.016576054,0.723694393,5.57E-05,0.113159023,1.00E-05,0.000103584,0.000347904
2913,text_summarization8,172,The bi - LSTM has two layers and a hidden size of 256 .,Data and Experiments,Data and Experiments,text_summarization,8,10,1,0,,0.618845063,1,hyperparameters,0.000207178,9.13E-05,0.012233694,5.55E-05,6.79E-06,0.200269024,0.054561113,0.649646402,0.000138082,0.080746083,2.32E-05,0.000234972,0.001786646
2914,text_summarization8,173,"Dropout is set to 0.5 , and the model is trained with Adagrad , an initial learning rate of 0.15 , and an initial accumulator value of 0.1 .",Data and Experiments,Data and Experiments,text_summarization,8,11,1,0,,0.908024781,1,hyperparameters,0.000166387,7.59E-05,0.000392042,1.91E-05,3.34E-06,0.070159122,0.007468835,0.872813687,5.54E-05,0.048315105,8.12E-06,0.00010521,0.000417661
2915,text_summarization8,174,"We limit the number of training examples to 100,000 on either corpus , which only has a small impact on performance .",Data and Experiments,Data and Experiments,text_summarization,8,12,1,0,,0.012062817,0,negative,0.004443456,8.56E-05,0.001893922,1.05E-05,2.83E-05,0.010012972,0.007698407,0.062178944,2.52E-05,0.903821296,7.96E-06,0.009435789,0.000357804
2916,text_summarization8,175,"For the jointly trained content selection models , we use the same configuration as the abstractive model .",Data and Experiments,Data and Experiments,text_summarization,8,13,1,0,,0.03850182,0,negative,0.000699777,0.000197986,0.014514118,9.17E-06,1.00E-05,0.048507013,0.019289999,0.257035815,0.000136686,0.658645921,5.25E-06,0.000660781,0.000287431
2917,text_summarization8,176,"For the base model , we re-implemented the Pointer - Generator model as described by .",Data and Experiments,Data and Experiments,text_summarization,8,14,1,0,,0.043361694,0,negative,0.001073736,0.000102183,0.175972145,2.96E-06,6.08E-06,0.003267158,0.002891139,0.010929954,0.000550754,0.804430619,9.28E-06,0.000582881,0.000181109
2918,text_summarization8,177,"To have a comparable number of parameters to previous work , we use an encoder with 256 hidden states for both directions in the one - layer LSTM , and 512 for the one - layer decoder .",Data and Experiments,Data and Experiments,text_summarization,8,15,1,0,,0.07020496,0,hyperparameters,0.000961855,0.000374325,0.00803547,7.03E-05,3.68E-05,0.091302827,0.02451425,0.521715001,0.000399996,0.350345983,1.68E-05,0.000491598,0.001734886
2919,text_summarization8,178,The embedding size is set to 128 .,Data and Experiments,Data and Experiments,text_summarization,8,16,1,0,,0.904286843,1,hyperparameters,8.51E-05,4.43E-05,0.000315874,8.13E-06,1.85E-06,0.060614784,0.00798517,0.844323356,3.34E-05,0.086071423,5.90E-06,0.000131866,0.000378853
2920,text_summarization8,179,The model is trained with the same Adagrad configuration as the content selector .,Data and Experiments,Data and Experiments,text_summarization,8,17,1,0,,0.044770664,0,negative,0.000427938,0.000142852,0.014710874,8.38E-06,6.90E-06,0.030382026,0.006413373,0.274548998,0.000254998,0.672618971,2.34E-06,0.000149835,0.000332519
2921,text_summarization8,180,"Additionally , the learning rate halves after each epoch once the validation perplexity does not decrease after an epoch .",Data and Experiments,Data and Experiments,text_summarization,8,18,1,0,,0.149919915,0,negative,0.00540067,0.000341974,0.035360609,1.33E-05,2.19E-05,0.00549201,0.00375048,0.080730291,0.000514959,0.866069251,7.31E-06,0.001773847,0.00052334
2922,text_summarization8,181,We do not use dropout and use gradient - clipping with a maximum norm of 2 .,Data and Experiments,Data and Experiments,text_summarization,8,19,1,0,,0.7472576,1,hyperparameters,0.001342741,0.000300586,0.005402167,0.000911831,0.000105201,0.176699828,0.03103856,0.562952667,0.000120885,0.217456044,5.61E-06,0.000480118,0.003183761
2923,text_summarization8,182,"We found that increasing model size or using the Transformer ( Vaswani et al. , 40.43 18.00 37.10 Key information guide network 38.95 17.12 35.68 Inconsistency loss 40.68 17.97 37.13",Data and Experiments,Data and Experiments,text_summarization,8,20,1,0,,0.001634638,0,negative,0.005271872,2.20E-06,0.001932022,1.64E-06,4.89E-06,0.000377339,0.000931089,0.000815479,2.32E-06,0.980466135,1.51E-06,0.010141671,5.18E-05
2924,text_summarization8,183,"Sentence Rewriting 40 2017 ) can lead to slightly improved performance , but at the cost of increased training time and parameters .",Data and Experiments,Data and Experiments,text_summarization,8,21,1,0,,0.054111961,0,negative,0.002407107,7.34E-06,0.085778722,2.76E-06,6.22E-06,0.000196357,0.020241531,0.000297374,1.91E-06,0.718755874,0.000789403,0.170334139,0.001181265
2925,text_summarization8,184,"We report numbers of a Transformer with copy-attention , which we denote CopyTransformer .",Data and Experiments,Data and Experiments,text_summarization,8,22,1,0,,0.00030997,0,negative,0.000311329,1.03E-05,0.00904836,8.45E-07,1.87E-05,8.34E-05,0.000360679,0.000145419,4.21E-06,0.988610095,9.01E-07,0.001387767,1.80E-05
2926,text_summarization8,185,"In this model , we randomly choose one of the attention - heads as the copy- distribution , and otherwise follow the parameters of the big Transformer by .",Data and Experiments,Data and Experiments,text_summarization,8,23,1,0,,0.000882726,0,negative,0.000152418,0.000107784,0.012443362,2.05E-06,4.72E-06,0.001464485,0.000567711,0.024475949,0.000429894,0.960168769,1.87E-06,9.23E-05,8.86E-05
2927,text_summarization8,186,All inference parameters are tuned on a 200 example subset of the validation set .,Data and Experiments,Data and Experiments,text_summarization,8,24,1,1,experimental-setup,0.030902414,0,negative,0.000171312,7.28E-05,0.000402058,5.01E-06,8.21E-06,0.026794021,0.004625682,0.336327341,3.48E-05,0.631142946,8.69E-07,0.000209176,0.000205802
2928,text_summarization8,187,"Length penalty parameter ? and copy mask differ across models , with ? ranging from 0.6 to 1.4 , and ranging from 0.1 to 0.2 .",Data and Experiments,Data and Experiments,text_summarization,8,25,1,1,experimental-setup,0.014928492,0,negative,0.000319017,3.58E-05,0.000624369,4.34E-06,5.12E-06,0.014176399,0.002833289,0.145574564,3.08E-05,0.835821127,8.43E-07,0.00044393,0.000130412
2929,text_summarization8,188,The minimum length of the generated summary is set to 35 for CNN - DM and 6 for NYT .,Data and Experiments,Data and Experiments,text_summarization,8,26,1,1,experimental-setup,0.740005173,1,hyperparameters,1.00E-04,5.29E-05,0.000425402,3.99E-06,2.82E-06,0.037753048,0.008569083,0.74014208,3.84E-05,0.212282242,2.03E-06,0.00020629,0.000421724
2930,text_summarization8,189,"While the Pointer - Generator uses a beam size of 5 and does not improve with a larger beam , we found that bottom - up attention requires a larger beam size of 10 .",Data and Experiments,Data and Experiments,text_summarization,8,27,1,0,,0.106612088,0,negative,0.01853848,2.03E-05,0.003039411,3.57E-06,1.88E-05,0.001721595,0.014498312,0.006242146,8.30E-06,0.7301109,3.97E-06,0.225239521,0.000554689
2931,text_summarization8,190,"The coverage penalty parameter ? is set to 10 , and the copy attention normalization parameter ? to 2 for both approaches .",Data and Experiments,Data and Experiments,text_summarization,8,28,1,1,experimental-setup,0.661808376,1,hyperparameters,0.000203802,7.77E-05,0.000407967,6.82E-06,4.35E-06,0.032654531,0.010006668,0.778283545,4.50E-05,0.177483915,1.91E-06,0.000278275,0.000545598
2932,text_summarization8,191,"We use AllenNLP for the content selector , and Open NMT - py for the abstractive models .",Data and Experiments,Data and Experiments,text_summarization,8,29,1,1,experimental-setup,0.401566557,0,negative,0.00061757,7.17E-05,0.01495349,0.000544262,0.000399928,0.043822765,0.019654385,0.03343305,3.62E-05,0.883634449,1.75E-06,0.000778589,0.002051881
2933,text_summarization8,192,"3 . shows our main results on the CNN - DM corpus , with abstractive models shown in the top , and bottom - up attention methods at the bottom .",Data and Experiments,Data and Experiments,text_summarization,8,30,1,1,results,0.277659572,0,negative,0.003520281,2.24E-06,0.003571769,1.20E-06,1.89E-05,7.75E-05,0.002302829,0.000141559,7.00E-07,0.845600693,5.62E-07,0.144656392,0.000105328
2934,text_summarization8,193,"We first observe that using a coverage inference penalty scores the same as a full coverage mechanism , without requiring any additional model parameters or model fine - tuning .",Data and Experiments,Data and Experiments,text_summarization,8,31,1,1,results,0.556331975,1,negative,0.017641936,6.92E-05,0.003506786,2.81E-06,1.29E-05,0.000218288,0.010584657,0.001567734,1.80E-05,0.492605791,5.19E-06,0.473327022,0.000439725
2935,text_summarization8,194,"The results with the CopyTransformer and coverage penalty indicate a slight improvement across all three scores , but we observe no significant difference between Pointer - Generator and CopyTransformer with bottom - up attention .",Data and Experiments,Data and Experiments,text_summarization,8,32,1,1,results,0.875180883,1,results,0.010395302,8.06E-07,0.000285988,3.54E-07,1.62E-06,2.98E-05,0.007255214,0.000137853,1.86E-07,0.040122109,4.41E-07,0.941593251,0.000177128
2936,text_summarization8,195,Results,,,text_summarization,8,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
2937,text_summarization8,196,"We found that none of our end - to - end models lead to improvements , indicating that it is difficult to apply the masking during training without hurting the training process .",Results,Results,text_summarization,8,1,1,0,,0.927088172,1,results,0.098629644,4.33E-06,0.000111563,1.30E-05,4.26E-06,7.64E-05,0.003155905,0.000258562,3.61E-06,0.105086224,2.67E-05,0.792216016,0.000413741
2938,text_summarization8,197,The Mask Only model with increased supervision on the copy mechanism performs very similar to the Multi - Task model .,Results,Results,text_summarization,8,2,1,0,,0.975117333,1,results,0.011413939,8.53E-07,6.99E-05,2.29E-06,6.65E-07,2.29E-05,0.003984254,0.000120646,5.05E-07,0.018192852,1.06E-05,0.965881379,0.000299118
2939,text_summarization8,198,"On the other hand , bottom - up attention leads to a major improvement across all three scores .",Results,Results,text_summarization,8,3,1,0,,0.982492577,1,results,0.368032574,3.47E-06,0.000126722,1.41E-05,2.44E-06,2.69E-05,0.00464544,0.00012067,3.15E-06,0.015450592,8.10E-06,0.611007911,0.00055799
2940,text_summarization8,199,"While we would expect better content selection to primarily improve ROUGE - 1 , the fact all three increase hints that the fluency is not being hurt specifically .",Results,Results,text_summarization,8,4,1,0,,0.745985406,1,results,0.112598016,5.69E-06,0.000160434,4.10E-05,7.16E-06,9.23E-05,0.00766604,0.000307224,7.85E-06,0.05488166,2.83E-05,0.823015355,0.001188957
2941,text_summarization8,200,Our cross - entropy trained ap - .,Results,Results,text_summarization,8,5,1,0,,0.099464021,0,negative,0.015332661,3.53E-05,0.074042507,2.73E-06,3.09E-06,6.46E-05,0.000373251,0.00031248,0.000178134,0.849379334,4.68E-05,0.060064769,0.000164336
2942,text_summarization8,201,We compare to their DCA model on the NYT corpus . :,Results,Results,text_summarization,8,6,1,0,,0.226761854,0,results,0.005247763,8.77E-06,0.004008811,1.11E-06,2.21E-06,3.10E-05,0.002382747,9.08E-05,3.27E-06,0.455086984,3.42E-05,0.532971814,0.000130531
2943,text_summarization8,202,"Results on the NYT corpus , where we compare to RL trained models .",Results,Results,text_summarization,8,7,1,0,,0.283434302,0,results,0.002461411,1.00E-06,0.000230987,1.93E-07,2.01E-07,5.95E-06,0.001962573,3.10E-05,3.89E-07,0.110779094,1.27E-05,0.884452071,6.24E-05
2944,text_summarization8,203,"* marks models and results by , and results by proach even outperforms all of the reinforcementlearning based approaches in ROUGE - 1 and 2 , while the highest reported ROUGE - L score by falls within the 95 % confidence interval of our results .",Results,Results,text_summarization,8,8,1,0,,0.711389801,1,results,0.008056412,1.59E-06,0.000145747,2.50E-05,4.72E-06,5.58E-05,0.008400666,0.000137384,1.88E-06,0.074423004,2.72E-05,0.906777128,0.00194342
2945,text_summarization8,204,shows experiments with the same systems on the NYT corpus .,Results,Results,text_summarization,8,9,1,0,,0.045173158,0,negative,0.007462085,3.37E-06,0.0015691,4.27E-06,8.86E-06,4.26E-05,0.001892165,9.55E-05,3.88E-06,0.789276309,1.53E-05,0.199352587,0.000274002
2946,text_summarization8,205,We see that the 2 point improvement compared to the baseline Pointer - Generator maximum - likelihood approach carries over to this dataset .,Results,Results,text_summarization,8,10,1,0,,0.708928637,1,results,0.024878303,1.35E-06,6.75E-05,1.58E-06,1.07E-06,1.00E-05,0.003115176,5.90E-05,8.02E-07,0.028407322,3.41E-06,0.943150524,0.000303902
2947,text_summarization8,206,"Here , the model outperforms the RL based model by in ROUGE - 1 and 2 , but not L , and is comparable to the results of except for ROUGE - L .",Results,Results,text_summarization,8,11,1,0,,0.902421759,1,results,0.008586474,9.29E-07,4.32E-05,3.22E-06,1.15E-06,1.44E-05,0.006035112,7.12E-05,4.96E-07,0.021896579,5.61E-06,0.962869254,0.000472444
2948,text_summarization8,207,The same can be observed when comparing ML and our Pointer - Generator .,Results,Results,text_summarization,8,12,1,0,,0.12019949,0,negative,0.006844246,3.67E-06,0.000209278,1.76E-06,1.71E-06,6.26E-05,0.001432111,0.000342774,4.56E-06,0.75968318,1.67E-05,0.231207991,0.000189385
2949,text_summarization8,208,"We suspect that a difference in summary lengths due to our inference parameter choices leads to this difference , but did not have access to their models or summaries to investigate this claim .",Results,Results,text_summarization,8,13,1,0,,0.001320282,0,negative,0.001252134,7.43E-06,0.000163974,9.24E-06,5.93E-06,0.000131967,0.000268749,0.000583326,2.27E-05,0.986968124,1.59E-05,0.010366002,0.000204572
2950,text_summarization8,209,This shows that a bottom - up approach achieves competitive results even to models that are trained on summary - specific objectives .,Results,Results,text_summarization,8,14,1,0,,0.742757271,1,results,0.015541436,1.17E-06,6.85E-05,6.27E-07,8.48E-07,7.27E-06,0.001499376,4.04E-05,6.65E-07,0.086851438,2.83E-06,0.895892187,9.33E-05
2951,text_summarization8,210,The main benefit of bottom - up summarization seems to be from the reduction of mistakenly copied words .,Results,Results,text_summarization,8,15,1,0,,0.898126765,1,results,0.030644031,8.98E-06,0.000634562,5.16E-06,4.29E-06,2.05E-05,0.003357794,6.42E-05,4.37E-06,0.145131524,0.000132146,0.819252145,0.000740386
2952,text_summarization8,211,"With the best Pointer - Generator models , the precision of copied words is 50.0 % compared to the reference .",Results,Results,text_summarization,8,16,1,0,,0.963822433,1,results,0.014253033,1.16E-06,0.000101146,9.35E-07,1.06E-06,7.74E-06,0.003159525,4.51E-05,5.82E-07,0.034395687,1.89E-06,0.947740865,0.000291311
2953,text_summarization8,212,"This precision increases to 52.8 % , which mostly drives the increase in R1 .",Results,Results,text_summarization,8,17,1,0,,0.822150612,1,ablation-analysis,0.639648087,5.17E-06,0.000194721,2.06E-05,6.63E-06,2.03E-05,0.001962454,0.000102112,9.83E-06,0.050987391,2.70E-06,0.306392098,0.000647928
2954,text_summarization8,213,An independent - samples t- test shows that this improvement is statistically significant with t= 14.7 ( p < 10 ?5 ) .,Results,Results,text_summarization,8,18,1,0,,0.733104399,1,results,0.180192926,3.74E-06,9.43E-05,4.14E-06,4.68E-06,2.49E-05,0.004279984,0.000157502,3.05E-06,0.105014978,2.59E-06,0.70975758,0.000459635
2955,text_summarization8,214,We also observe a decrease in average sentence length of summaries from 13 to 12 words when adding content selection compared to the Pointer - Generator while holding all other inference parameters constant .,Results,Results,text_summarization,8,19,1,0,,0.85004988,1,results,0.37985071,4.90E-06,0.000137386,7.58E-06,4.22E-06,1.44E-05,0.002762458,9.01E-05,3.73E-06,0.046103252,2.17E-06,0.570442649,0.000576388
2956,text_summarization8,215,Domain Transfer,Results,,text_summarization,8,20,1,0,,0.839384542,1,results,0.137553951,6.70E-05,0.044232386,0.00047748,6.08E-05,0.000480746,0.009351534,0.000836835,0.000516242,0.292022781,0.000142345,0.48466963,0.029588238
2957,text_summarization8,216,"While end - to - end training has become common , there are benefits to a twostep method .",Results,Domain Transfer,text_summarization,8,21,1,0,,0.000964432,0,negative,0.00139052,1.17E-07,0.000201765,1.66E-07,2.94E-08,1.41E-06,8.63E-06,3.02E-06,7.88E-07,0.950534256,1.22E-06,0.047329031,0.000529056
2958,text_summarization8,217,"Since the content selector only needs to solve a binary tagging problem with pretrained vectors , it performs well even with very limited training data .",Results,Domain Transfer,text_summarization,8,22,1,0,,0.307780057,0,results,0.006233722,6.13E-08,1.57E-05,3.92E-07,3.27E-08,1.99E-06,0.000158345,7.06E-06,1.99E-07,0.145009165,2.37E-07,0.845928607,0.002644492
2959,text_summarization8,218,"As shown in , with only 1,000 sentences , the model achieves an AUC of over 74 .",Results,Domain Transfer,text_summarization,8,23,1,0,,0.453236723,0,results,0.001589557,1.23E-08,1.12E-05,5.35E-08,1.36E-08,6.24E-07,0.000186029,1.98E-06,3.31E-08,0.092555392,5.59E-08,0.903979016,0.001676001
2960,text_summarization8,219,"Beyond that size , the AUC of the model increases only slightly with increasing training data .",Results,Domain Transfer,text_summarization,8,24,1,0,,0.022095488,0,results,0.095255007,1.22E-07,3.99E-05,3.99E-07,8.11E-08,2.58E-06,0.000193422,1.14E-05,8.62E-07,0.286871304,1.94E-07,0.615729422,0.001895296
2961,text_summarization8,220,"To further evaluate the content selection , we consider an application to domain transfer .",Results,Domain Transfer,text_summarization,8,25,1,0,,0.004502666,0,negative,0.002352564,3.05E-06,0.000961424,7.17E-07,4.74E-07,4.48E-06,7.87E-05,2.06E-05,1.10E-05,0.904320631,1.82E-06,0.09079821,0.001446351
2962,text_summarization8,221,"In this experiment , we apply the Pointer - Generator trained on CNN - DM to the NYT corpus .",Results,Domain Transfer,text_summarization,8,26,1,0,,0.079046919,0,negative,0.001217745,4.87E-07,0.000359512,5.11E-08,6.74E-08,2.35E-06,5.22E-05,2.10E-05,1.30E-06,0.977339831,5.23E-08,0.020856139,0.00014925
2963,text_summarization8,222,"In addition , we train three content selectors on 1 , 10 , and 100 thousand sentences of the NYT set , and use these in the bottom - up summarization .",Results,Domain Transfer,text_summarization,8,27,1,0,,0.114888594,0,negative,0.003507207,4.27E-06,0.000755643,1.45E-06,1.25E-06,1.05E-05,0.000250052,6.19E-05,6.95E-06,0.909601469,3.98E-07,0.081709071,0.004089798
2964,text_summarization8,223,"The results , shown in , demonstrates that even a model trained on the smallest subset leads to an improvement of almost 5 points over the model without bottom - up attention .",Results,Domain Transfer,text_summarization,8,28,1,0,,0.661208214,1,results,0.022889885,4.09E-08,1.28E-05,8.17E-08,2.21E-08,7.97E-07,0.000235007,3.69E-06,1.13E-07,0.169822162,4.72E-08,0.80606156,0.000973799
2965,text_summarization8,224,This improvement increases with the larger subsets to up to 7 points .,Results,Domain Transfer,text_summarization,8,29,1,0,,0.128751482,0,results,0.019829778,5.34E-08,2.57E-05,4.47E-07,4.73E-08,1.98E-06,0.000204773,6.34E-06,3.66E-07,0.271539299,8.05E-08,0.703543639,0.004847551
2966,text_summarization8,225,"While this approach does not reach a comparable performance to models trained directly on the NYT dataset , it still represents a significant increase over the not-augmented CNN - DM model and produces summaries that are quite readable .",Results,Domain Transfer,text_summarization,8,30,1,0,,0.246486465,0,results,0.001788556,3.58E-09,3.58E-06,1.05E-08,3.12E-09,1.28E-07,6.49E-05,3.90E-07,8.48E-09,0.10047374,1.24E-08,0.897374273,0.000294435
2967,text_summarization8,226,We show two example summaries in Appendix A .,Results,Domain Transfer,text_summarization,8,31,1,0,,1.44E-05,0,negative,0.000101601,4.87E-09,2.38E-06,8.97E-08,2.53E-08,5.23E-07,1.02E-06,1.54E-06,8.52E-08,0.998646506,1.93E-09,0.001195306,5.09E-05
2968,text_summarization8,227,This technique could be used for low - resource domains and for problems with limited data availability .,Results,Domain Transfer,text_summarization,8,32,1,0,,0.000971886,0,negative,0.000803187,7.85E-08,4.24E-05,4.15E-07,4.62E-08,2.54E-06,8.52E-06,7.38E-06,2.10E-06,0.984522962,1.21E-07,0.013408282,0.001201997
2969,text_summarization8,228,Analysis and Discussion,Results,,text_summarization,8,33,1,0,,0.002096298,0,negative,0.064304304,6.31E-06,0.001417667,4.04E-05,2.01E-05,7.84E-05,0.00102816,0.000193287,3.51E-05,0.850860655,3.74E-06,0.080461871,0.001549943
2970,text_summarization8,229,Extractive Summary by Content Selection ?,Results,Analysis and Discussion,text_summarization,8,34,1,0,,0.019248884,0,negative,0.077749965,0.000127537,0.000606265,1.95E-05,0.000411634,0.000387016,0.003842683,1.45E-05,5.90E-05,0.902170786,0.008298013,0.000130904,0.006182151
2971,text_summarization8,230,"Given that the content selector is effective in conjunction with the abstractive model , it is interesting to know whether it has learned an effective extractive summarization system on its own .",Results,Analysis and Discussion,text_summarization,8,35,1,0,,2.62E-05,0,negative,0.012611201,4.51E-06,1.88E-05,1.18E-07,1.75E-05,2.25E-05,2.33E-05,1.25E-06,1.07E-05,0.987279027,1.53E-06,3.97E-06,5.63E-06
2972,text_summarization8,231,shows experiments comparing content selection to extractive baselines .,Results,Analysis and Discussion,text_summarization,8,36,1,0,,0.000106411,0,negative,0.010488259,3.00E-06,0.00014399,1.49E-06,0.000641431,4.17E-05,4.41E-05,6.09E-07,2.26E-06,0.988587271,3.25E-06,4.33E-06,3.83E-05
2973,text_summarization8,232,The LEAD - 3 baseline is a commonly used baseline in news summarization that extracts the first three sentences from an article .,Results,Analysis and Discussion,text_summarization,8,37,1,0,,0.064766856,0,negative,0.037603857,0.001415495,0.198912788,2.20E-06,0.000580844,0.000973316,0.007229744,4.32E-05,0.001051579,0.750035675,5.49E-05,8.70E-05,0.002009504
2974,text_summarization8,233,Top - 3 shows the performance when we extract the top three sentences by average copy probability from the selector .,Results,Analysis and Discussion,text_summarization,8,38,1,0,,0.00812565,0,negative,0.032289956,1.98E-05,0.000265453,1.78E-06,0.000302139,0.000221979,0.000614474,1.07E-05,1.47E-05,0.966007237,1.80E-06,3.10E-05,0.000218961
2975,text_summarization8,234,"Interestingly , with this method , only 7.1 % of the top three sentences are not within the first three , further reinforcing the strength of the LEAD - 3 baseline .",Results,Analysis and Discussion,text_summarization,8,39,1,0,,0.001552429,0,ablation-analysis,0.903444828,5.58E-06,3.78E-05,3.36E-07,0.000135464,1.18E-05,0.000209325,5.31E-07,1.43E-06,0.096043706,2.81E-07,8.32E-05,2.57E-05
2976,text_summarization8,235,Our naive sentence - extractor performs slightly worse than the highest reported extractive score by that is specifically trained to score combinations of sentences .,Results,Analysis and Discussion,text_summarization,8,40,1,0,,0.016502655,0,ablation-analysis,0.807316539,2.50E-05,0.000220256,5.13E-06,0.000376239,0.000209317,0.007069217,8.78E-06,7.23E-06,0.181026636,5.40E-06,0.001292558,0.002437724
2977,text_summarization8,236,The final entry shows the performance when all the words above a threshold are extracted such that the resulting summaries are approximately the length of reference summaries .,Results,Analysis and Discussion,text_summarization,8,41,1,0,,0.000297552,0,negative,0.004604846,2.71E-06,2.10E-05,3.89E-07,0.000218115,4.20E-05,4.41E-05,1.45E-06,1.95E-06,0.995050721,8.43E-08,4.46E-06,8.16E-06
2978,text_summarization8,237,"The oracle score represents the results if our model had a perfect accuracy , and shows that the content selector , while yielding competitive results , has room for further improvements in future work .",Results,Analysis and Discussion,text_summarization,8,42,1,0,,1.82E-05,0,negative,0.0116872,9.40E-06,5.51E-05,1.48E-07,4.31E-05,5.36E-05,4.71E-05,3.49E-06,1.87E-05,0.988071053,1.56E-07,3.43E-06,7.63E-06
2979,text_summarization8,238,This result shows that the model is quite effective at finding important words ( ROUGE - 1 ) but less effective at chaining them together ( ROUGE - 2 ) .,Results,Analysis and Discussion,text_summarization,8,43,1,0,,0.084074479,0,ablation-analysis,0.721593275,3.94E-06,1.32E-05,2.20E-07,2.82E-05,2.61E-05,0.000980674,2.34E-06,1.47E-06,0.277036312,5.45E-07,0.000250042,6.37E-05
2980,text_summarization8,239,"Similar to , we find that the decrease in ROUGE - 2 indicates alack of fluency and grammaticality of the generated summaries .",Results,Analysis and Discussion,text_summarization,8,44,1,0,,0.005320901,0,ablation-analysis,0.838767858,7.74E-06,2.24E-05,1.55E-06,0.000178291,3.46E-05,0.000221435,2.17E-06,3.77E-06,0.160677079,1.96E-07,3.08E-05,5.22E-05
2981,text_summarization8,240,A Bottom - Up Attention 0.5 53.3 24.8 6.5 : % Novel shows the percentage of words in a summary that are not in the source document .,Results,Analysis and Discussion,text_summarization,8,45,1,0,,1.49E-05,0,negative,0.022931213,1.52E-05,0.001412057,1.18E-07,3.05E-05,9.04E-05,0.000114712,3.01E-06,0.000100539,0.975265997,4.34E-07,4.25E-06,3.16E-05
2982,text_summarization8,241,The last three columns show the part - of - speech tag distribution of the novel words in generated summaries .,Results,Analysis and Discussion,text_summarization,8,46,1,0,,0.000175498,0,negative,0.003155961,3.64E-06,2.04E-05,2.91E-07,0.00024606,3.40E-05,2.08E-05,1.38E-06,4.24E-06,0.996505878,3.16E-08,1.30E-06,5.98E-06
2983,text_summarization8,242,typical example looks like this :,Results,Analysis and Discussion,text_summarization,8,47,1,0,,6.51E-07,0,negative,6.56E-05,9.17E-07,1.46E-05,1.28E-07,2.93E-05,1.90E-05,4.17E-06,6.25E-07,4.95E-06,0.999854532,2.14E-07,1.35E-07,5.81E-06
2984,text_summarization8,243,a man food his first hamburger wrongfully for 36 years .,Results,Analysis and Discussion,text_summarization,8,48,1,0,,4.72E-06,0,negative,0.000629786,2.01E-06,1.88E-05,7.78E-07,0.000212732,4.38E-05,1.03E-05,1.10E-06,3.37E-06,0.999050581,2.54E-07,3.03E-07,2.62E-05
2985,text_summarization8,244,"michael hanline , 69 , was convicted of murder for the shooting of truck driver jt mcgarry in 1980 on judge charges .",Results,Analysis and Discussion,text_summarization,8,49,1,0,,3.49E-05,0,negative,0.005308787,7.61E-06,7.50E-05,1.02E-06,0.000188002,9.67E-05,8.56E-05,3.42E-06,1.77E-05,0.994090079,1.18E-06,1.96E-06,0.000122973
2986,text_summarization8,245,This particular ungrammatical example has a ROUGE - 1 of 29.3 .,Results,Analysis and Discussion,text_summarization,8,50,1,0,,5.20E-06,0,negative,0.001200298,3.27E-07,6.80E-06,5.91E-07,0.000696705,2.86E-05,1.15E-05,3.05E-07,1.94E-07,0.998047743,1.85E-08,8.96E-07,6.08E-06
2987,text_summarization8,246,This further highlights the benefit of the combined approach where bottomup predictions are chained together fluently by the abstractive system .,Results,Analysis and Discussion,text_summarization,8,51,1,0,,0.001745194,0,negative,0.125141578,1.19E-05,0.000113828,2.41E-07,3.56E-05,2.11E-05,4.73E-05,8.42E-07,5.19E-05,0.874546407,1.94E-07,1.12E-05,1.80E-05
2988,text_summarization8,247,"However , we also note that the abstractive system requires access to the full source document .",Results,Analysis and Discussion,text_summarization,8,52,1,0,,1.99E-05,0,negative,0.002181455,4.61E-07,8.93E-06,1.28E-07,3.17E-05,1.19E-05,3.12E-06,2.34E-07,1.12E-06,0.997758904,2.97E-08,3.77E-07,1.68E-06
2989,text_summarization8,248,Distillation experiments in which we tried to use the output of the contentselection as training - input to abstractive models showed a drastic decrease in model performance .,Results,Analysis and Discussion,text_summarization,8,53,1,0,,0.005228513,0,negative,0.005802013,1.48E-06,2.72E-05,4.76E-07,0.000108219,3.76E-05,4.16E-05,6.14E-07,1.28E-06,0.99391542,8.91E-07,3.06E-06,6.01E-05
2990,text_summarization8,249,"Analysis of Copying While Pointer - Generator models have the ability to abstract in summary , the use of a copy mechanism causes the summaries to be mostly extractive .",Results,Analysis and Discussion,text_summarization,8,54,1,0,,0.000144034,0,negative,0.016265974,8.01E-05,0.000870912,1.10E-07,1.82E-05,4.31E-05,0.000304839,2.31E-06,0.000247284,0.982046774,1.33E-05,1.88E-05,8.83E-05
2991,text_summarization8,250,"shows that with copying the percentage of generated words that are not in the source document decreases from 6.6 % to 2.2 % , while reference summaries are much more abstractive with 14.8 % novel words .",Results,Analysis and Discussion,text_summarization,8,55,1,0,,0.010510818,0,ablation-analysis,0.865223083,1.81E-06,3.29E-05,1.76E-07,0.000106406,7.56E-06,0.00022074,2.99E-07,8.51E-07,0.134335168,4.03E-08,3.99E-05,3.11E-05
2992,text_summarization8,251,Bottom - up attention leads to a further reduction to only a half percent .,Results,Analysis and Discussion,text_summarization,8,56,1,0,,0.064998999,0,ablation-analysis,0.991550128,1.63E-06,1.64E-05,3.49E-07,3.61E-05,7.52E-06,0.000162133,3.44E-07,1.07E-06,0.008156735,6.97E-09,9.87E-06,5.77E-05
2993,text_summarization8,252,"However , since generated summaries are typically not longer than 40 - 50 words , the difference between an abstractive system with and without bottom - up attention is less than one novel word per summary .",Results,Analysis and Discussion,text_summarization,8,57,1,0,,1.17E-05,0,negative,0.004489516,1.88E-06,1.58E-05,3.89E-08,1.57E-05,7.46E-06,2.24E-05,4.53E-07,1.30E-06,0.995431703,4.19E-07,3.35E-06,1.00E-05
2994,text_summarization8,253,This shows that the benefit of abstractive models has been less in their ability to produce better paraphrasing but more in the ability to create fluent summaries from a mostly extractive process .,Results,Analysis and Discussion,text_summarization,8,58,1,0,,0.000108254,0,negative,0.014698201,6.07E-07,2.74E-05,1.78E-08,1.46E-05,3.62E-06,2.68E-05,9.81E-08,6.66E-07,0.985216985,9.42E-08,6.85E-06,4.02E-06
2995,text_summarization8,254,"also shows the part - of - speech - tags of the novel generated words , and we can observe an interesting effect .",Results,Analysis and Discussion,text_summarization,8,59,1,0,,2.51E-05,0,negative,0.0888707,2.79E-06,7.00E-05,7.35E-08,4.49E-05,1.50E-05,9.71E-05,6.68E-07,3.99E-06,0.910868168,2.46E-08,1.67E-05,9.90E-06
2996,text_summarization8,255,"Application of bottom - up attention leads to a sharp decrease in novel adjectives and nouns , whereas the fraction of novel words that are verbs sharply increases .",Results,Analysis and Discussion,text_summarization,8,60,1,0,,0.142911348,0,ablation-analysis,0.975652917,4.19E-06,1.31E-05,1.55E-07,3.96E-05,6.24E-06,0.000177303,4.47E-07,1.58E-06,0.024050957,1.81E-08,1.16E-05,4.18E-05
2997,text_summarization8,256,"When looking at the novel verbs that are being generated , we notice a very high percentage of tense or number changes , indicated by variation of the word "" say "" , for example "" said "" or "" says "" , while novel nouns are mostly morphological variants of words in the source .",Results,Analysis and Discussion,text_summarization,8,61,1,0,,0.000129729,0,negative,0.027244807,1.15E-06,1.43E-05,8.10E-07,0.001292124,3.19E-05,8.32E-05,4.62E-07,2.94E-07,0.971294201,2.00E-08,6.21E-06,3.04E-05
2998,text_summarization8,257,shows the length of the phrases that are being copied .,Results,Analysis and Discussion,text_summarization,8,62,1,0,,8.70E-06,0,negative,0.000632802,4.15E-06,4.15E-05,7.92E-08,1.63E-05,5.55E-05,2.29E-05,3.76E-06,3.78E-05,0.999157012,2.75E-08,2.08E-07,2.80E-05
2999,text_summarization8,258,"While most copied phrases in the reference summaries are in groups of 1 to 5 words , the Pointer - Generator copies many very long sequences and full sentences of over 11 words .",Results,Analysis and Discussion,text_summarization,8,63,1,0,,0.000543774,0,negative,0.050385076,1.44E-05,0.000145335,8.08E-07,0.002021991,0.000107041,0.00033773,2.04E-06,4.89E-06,0.946854768,3.50E-08,1.46E-05,0.000111304
3000,text_summarization8,259,"Since the content selection mask interrupts most long copy sequences , the model has to either generate the unselected words using only the generation probability or use a different word instead .",Results,Analysis and Discussion,text_summarization,8,64,1,0,,2.47E-05,0,negative,0.013011501,2.35E-05,0.000150573,4.97E-08,2.47E-05,9.28E-06,1.54E-05,8.46E-07,0.000146088,0.986607153,5.87E-08,1.83E-06,9.08E-06
3001,text_summarization8,260,"While we observed both cases quite frequently in generated summaries , the fraction of very long copied phrases decreases .",Results,Analysis and Discussion,text_summarization,8,65,1,0,,0.00058961,0,negative,0.424321238,2.22E-06,2.04E-05,2.29E-07,0.000182079,1.19E-05,0.000102806,4.43E-07,7.17E-07,0.575318427,1.75E-08,1.51E-05,2.45E-05
3002,text_summarization8,261,"However , either with or without bottom - up attention , the distribution of the length of copied phrases is still quite different from the reference .",Results,Analysis and Discussion,text_summarization,8,66,1,0,,2.04E-05,0,negative,0.02105724,6.08E-06,2.84E-05,3.44E-08,3.48E-05,7.76E-06,7.53E-05,5.66E-07,2.66E-06,0.978762978,8.08E-08,1.24E-05,1.17E-05
3003,text_summarization8,262,Inference Penalty Analysis,Results,,text_summarization,8,67,1,0,,0.020216478,0,negative,0.004679337,4.76E-06,0.000382551,1.94E-05,8.70E-06,5.22E-05,0.000854552,0.000418219,1.89E-05,0.965375901,3.58E-07,0.020817629,0.007367444
3004,text_summarization8,263,We next analyze the effect of the inference - time loss functions .,Results,Inference Penalty Analysis,text_summarization,8,68,1,0,,0.000182802,0,negative,0.0030177,8.54E-06,4.84E-06,2.49E-08,6.16E-06,1.14E-05,5.12E-05,8.31E-06,9.50E-06,0.996803244,2.70E-09,7.77E-05,1.31E-06
3005,text_summarization8,264,Table 6 presents the marginal improvements over the simple Pointer - Generator when adding one penalty at a time .,Results,Inference Penalty Analysis,text_summarization,8,69,1,0,,0.005638424,0,negative,0.007376162,5.62E-06,8.03E-06,1.08E-08,4.20E-06,1.19E-05,0.001106467,7.55E-06,2.69E-06,0.989153004,8.94E-09,0.002317676,6.70E-06
3006,text_summarization8,265,"We observe that all three penalties improve all three scores , even when added on top of the other two .",Results,Inference Penalty Analysis,text_summarization,8,70,1,0,,0.146714453,0,negative,0.139145737,1.51E-05,1.40E-05,2.40E-06,8.05E-05,0.000365928,0.009846699,0.000169935,6.50E-06,0.841650168,5.42E-08,0.0082713,0.000431641
3007,text_summarization8,266,"This further indicates that the unmodified Pointer - Generator model has already learned an appropriate representation of the abstractive summarization problem , but is limited by its ineffective content selection and inference methods . :",Results,Inference Penalty Analysis,text_summarization,8,71,1,0,,7.12E-05,0,negative,0.001758593,1.36E-06,1.63E-05,1.51E-08,1.15E-05,5.59E-06,1.50E-05,7.45E-07,3.30E-06,0.998158261,1.82E-09,2.86E-05,7.39E-07
3008,text_summarization8,267,Results on CNN - DM when adding one inference penalty at a time .,Results,Inference Penalty Analysis,text_summarization,8,72,1,0,,0.000237213,0,negative,0.00067779,2.77E-06,4.40E-05,1.21E-09,4.77E-07,4.17E-06,0.000308951,1.41E-06,6.77E-06,0.998528523,2.17E-08,0.000422133,2.95E-06
3009,text_summarization8,268,Conclusion,,,text_summarization,8,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
3010,text-classification9,1,title,,,text-classification,9,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
3011,text-classification9,2,Translations as Additional Contexts for Sentence Classification,title,,text-classification,9,1,1,1,research-problem,0.99822031,1,research-problem,1.47E-07,3.12E-05,2.17E-07,2.59E-07,6.30E-07,3.27E-07,3.52E-06,4.07E-06,1.43E-06,0.006163876,0.993793071,1.07E-06,1.69E-07
3012,text-classification9,3,abstract,,,text-classification,9,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
3013,text-classification9,4,"In sentence classification tasks , additional contexts , such as the neighboring sentences , may improve the accuracy of the classifier .",abstract,abstract,text-classification,9,1,1,0,,0.072431382,0,research-problem,4.87E-08,1.52E-05,1.01E-08,1.52E-06,5.06E-07,5.41E-07,2.98E-07,3.91E-06,7.65E-07,0.045360563,0.954616503,2.95E-08,4.98E-08
3014,text-classification9,5,"However , such contexts are domain - dependent and thus can not be used for another classification task with an inappropriate domain .",abstract,abstract,text-classification,9,2,1,0,,0.002229588,0,negative,5.67E-07,0.000241202,1.65E-07,2.33E-06,1.00E-05,2.00E-06,4.51E-07,1.59E-05,1.57E-05,0.771702163,0.228009129,2.34E-07,4.73E-08
3015,text-classification9,6,"In contrast , we propose the use of translated sentences as domain - free context that is always available regardless of the domain .",abstract,abstract,text-classification,9,3,1,0,,0.117585869,0,negative,8.87E-05,0.263425803,0.000158571,5.17E-05,0.000896264,7.37E-05,1.54E-05,0.000652048,0.014862014,0.630872191,0.088890696,9.96E-06,2.90E-06
3016,text-classification9,7,"We find that naive feature expansion of translations gains only marginal improvements and may decrease the performance of the classifier , due to possible inaccurate translations thus producing noisy sentence vectors .",abstract,abstract,text-classification,9,4,1,0,,0.111398792,0,negative,0.00154751,0.011671691,1.86E-05,0.000100293,0.000233145,0.000121152,0.00021738,0.002148836,0.00028117,0.673408112,0.309459897,0.000774018,1.82E-05
3017,text-classification9,8,"To this end , we present multiple context fixing attachment ( MCFA ) , a series of modules attached to multiple sentence vectors to fix the noise in the vectors using the other sentence vectors as context .",abstract,abstract,text-classification,9,5,1,0,,0.667181446,1,research-problem,6.46E-05,0.1931741,0.000544778,4.84E-05,0.000598477,4.93E-05,4.98E-05,0.000330114,0.024075036,0.154574983,0.626457821,2.23E-05,1.04E-05
3018,text-classification9,9,"We show that our method performs competitively compared to previous models , achieving best classification performance on multiple data sets .",abstract,abstract,text-classification,9,6,1,0,,0.006659619,0,negative,6.86E-05,0.00871005,5.12E-06,1.33E-05,8.87E-05,3.51E-05,6.63E-05,0.000767059,0.000194907,0.74756524,0.242153625,0.000328159,3.71E-06
3019,text-classification9,10,We are the first to use translations as domainfree contexts for sentence classification .,abstract,abstract,text-classification,9,7,1,0,,0.483329422,0,research-problem,2.58E-05,0.079998132,9.59E-05,3.82E-05,0.001296963,3.42E-05,3.67E-05,0.000271983,0.001665522,0.430618058,0.48588777,2.61E-05,4.76E-06
3020,text-classification9,11,Introduction,,,text-classification,9,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
3021,text-classification9,12,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .",Introduction,Introduction,text-classification,9,1,1,1,research-problem,0.850323686,1,research-problem,7.41E-07,8.58E-05,2.78E-07,6.18E-05,3.73E-05,1.07E-05,1.34E-05,5.32E-06,2.25E-05,0.041011642,0.958746362,7.22E-07,3.49E-06
3022,text-classification9,13,"This task is important as it is widely used in almost all subare as of NLP such as sentiment classification for sentiment analysis and question type classification for question answering , to name a few .",Introduction,Introduction,text-classification,9,2,1,0,,0.52053351,1,research-problem,5.13E-07,0.0001114,2.01E-07,3.20E-05,2.80E-05,9.53E-06,1.01E-05,7.33E-06,2.66E-05,0.061886149,0.937885357,6.33E-07,2.14E-06
3023,text-classification9,14,"While past methods require feature engineering , recent methods enjoy neural - based methods to automatically encode the sentences into low - dimensional dense vectors .",Introduction,Introduction,text-classification,9,3,1,0,,0.174842032,0,negative,2.41E-05,0.007532648,6.00E-06,0.000103013,0.000390392,9.74E-05,3.25E-05,0.000107587,0.001799792,0.550917178,0.438976538,6.70E-06,6.13E-06
3024,text-classification9,15,"Despite the success of these methods , the major challenge in this task is that extracting features from a single sentence limits the performance .",Introduction,Introduction,text-classification,9,4,1,0,,0.092476215,0,research-problem,1.89E-06,0.000169847,3.20E-07,3.32E-05,3.32E-05,1.66E-05,1.25E-05,1.19E-05,4.45E-05,0.183146472,0.816525922,1.38E-06,2.30E-06
3025,text-classification9,16,"To overcome this limitation , recent works attempted to augment different kinds of features to the sentence , such as the neighboring sentences and the topics of the sentences .",Introduction,Introduction,text-classification,9,5,1,0,,0.015812489,0,research-problem,1.23E-05,0.00291629,2.11E-06,4.39E-05,0.000115793,8.00E-05,2.75E-05,0.000112555,0.001459084,0.471815341,0.523405255,5.19E-06,4.78E-06
3026,text-classification9,17,"However , these methods used domain - dependent contexts thatare only effective when the domain of the task is appropriate .",Introduction,Introduction,text-classification,9,6,1,0,,0.010979965,0,negative,1.58E-05,0.004198941,3.97E-06,7.69E-05,0.000391509,0.000216688,5.68E-05,0.00020193,0.00183407,0.724998698,0.267989619,7.78E-06,7.20E-06
3027,text-classification9,18,"For one thing , neighboring sentences may not be available in some tasks such as question type classification .",Introduction,Introduction,text-classification,9,7,1,0,,0.012200844,0,negative,3.81E-06,0.0015061,9.28E-07,3.24E-05,0.000138021,4.76E-05,2.05E-05,6.66E-05,0.000412409,0.519635908,0.478128619,3.76E-06,3.38E-06
3028,text-classification9,19,"Moreover , topics inferred using topic models may produce less useful topics when the data set is domain - specific such as movie review sentiment classification .",Introduction,Introduction,text-classification,9,8,1,0,,0.021422609,0,negative,1.22E-05,0.002735864,1.33E-06,2.43E-05,9.89E-05,4.25E-05,2.91E-05,8.38E-05,0.000769665,0.517127032,0.479061579,9.85E-06,3.88E-06
3029,text-classification9,20,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts thatare always available no matter what the task domain is .",Introduction,Introduction,text-classification,9,9,1,0,,0.930083566,1,approach,0.00014489,0.76255326,0.00018044,4.33E-05,0.005439297,0.000114586,0.000110259,0.000198163,0.167135807,0.053620773,0.010399,4.82E-05,1.20E-05
3030,text-classification9,21,We observe two opportunities when using translations .,Introduction,Introduction,text-classification,9,10,1,0,,0.052564937,0,negative,8.58E-05,0.007468126,1.60E-05,5.45E-05,0.001681135,0.000152733,5.38E-05,9.35E-05,0.004339855,0.976746606,0.00928011,2.45E-05,3.28E-06
3031,text-classification9,22,"First , each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class .",Introduction,Introduction,text-classification,9,11,1,0,,0.017856698,0,negative,8.92E-05,0.017961276,2.23E-05,9.88E-05,0.00435393,0.000101366,2.45E-05,6.64E-05,0.006456225,0.961506525,0.009304516,1.26E-05,2.38E-06
3032,text-classification9,23,contrasts the sentence vectors of the original English sentences and their Arabictranslated sentences in the question type classification task .,Introduction,Introduction,text-classification,9,12,1,0,,0.03279919,0,negative,5.19E-05,0.111701504,0.000113202,2.69E-05,0.002664799,9.13E-05,0.000550011,0.000222782,0.012462329,0.538324751,0.333571413,0.00018975,2.93E-05
3033,text-classification9,24,A yellow circle signifies a clear separation of a class .,Introduction,Introduction,text-classification,9,13,1,0,,0.017288265,0,negative,1.48E-05,0.021441913,1.65E-05,5.05E-06,0.000149834,0.000374935,7.50E-05,0.000691314,0.287140351,0.678752724,0.01132162,1.10E-05,4.88E-06
3034,text-classification9,25,"For example , the green class , or the numeric question type , is circled in the Arabic space as it is clearly separated from other classes , while such separation can not be observed in English .",Introduction,Introduction,text-classification,9,14,1,0,,0.002178034,0,negative,3.42E-05,0.00140667,4.06E-06,1.39E-05,0.000991194,9.43E-05,5.90E-05,4.97E-05,0.000627991,0.989779723,0.006906336,3.11E-05,1.81E-06
3035,text-classification9,26,"Meanwhile , location type questions ( in orange ) are better classified in English .",Introduction,Introduction,text-classification,9,15,1,0,,0.010854542,0,negative,9.07E-06,0.001788455,2.78E-06,1.80E-05,0.000817072,0.000133457,4.73E-05,9.60E-05,0.000818234,0.985725814,0.010531577,9.71E-06,2.50E-06
3036,text-classification9,27,"Second , the original sentences may include languagespecific ambiguity , which maybe resolved when presented with its translations .",Introduction,Introduction,text-classification,9,16,1,0,,0.007382934,0,negative,2.02E-05,0.002561129,2.59E-06,2.44E-05,0.000427024,9.42E-05,1.92E-05,7.81E-05,0.001692627,0.990037391,0.005034677,7.01E-06,1.41E-06
3037,text-classification9,28,"Consider the example English sentence "" The movie is terribly amazing "" for the sentiment classification task .",Introduction,Introduction,text-classification,9,17,1,0,,0.010615167,0,negative,4.04E-06,0.012104999,5.58E-06,1.93E-05,0.00199942,0.000194082,8.62E-05,0.000231714,0.006276152,0.946836505,0.032221563,1.19E-05,8.50E-06
3038,text-classification9,29,"In this case , terribly can be used in both positive and negative sense , thus introduces ambiguity in the sentence .",Introduction,Introduction,text-classification,9,18,1,0,,0.002818971,0,negative,3.18E-05,0.002144073,4.87E-06,2.20E-05,0.001502779,0.000106125,2.89E-05,5.88E-05,0.001653773,0.993249929,0.001184335,1.11E-05,1.45E-06
3039,text-classification9,30,"When translated to Korean , it becomes "" ? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ? "" which means "" The movie is greatly magnificent "" , removing the ambiguity .",Introduction,Introduction,text-classification,9,19,1,0,,0.006930074,0,negative,7.36E-06,0.003039697,3.59E-06,0.000225955,0.003285282,0.000751735,8.70E-05,0.000416289,0.001766887,0.988317017,0.002085347,5.57E-06,8.28E-06
3040,text-classification9,31,The above two observations hold only when translations are supported for ( nearly ) arbitrary language pairs with sufficiently high quality .,Introduction,Introduction,text-classification,9,20,1,0,,0.082843288,0,negative,3.18E-05,0.017093578,3.92E-06,2.13E-05,0.000229305,0.000161382,9.54E-05,0.00046991,0.013625292,0.941650857,0.026580807,3.07E-05,5.73E-06
3041,text-classification9,32,"Thankfully , translation services ( e.g. Google Translate )",Introduction,Introduction,text-classification,9,21,1,0,,0.017145713,0,negative,2.58E-05,0.001303982,5.09E-06,9.38E-05,0.002545742,9.32E-05,0.000103772,4.13E-05,0.000313485,0.973408387,0.022037539,2.09E-05,6.90E-06
3042,text-classification9,33,"Moreover , recent research on neural machine translation ( NMT ) improved the efficiency and even enabled zero - shot translation of models for languages with no parallel data .",Introduction,Introduction,text-classification,9,22,1,0,,0.651999476,1,research-problem,5.22E-06,0.001139514,2.66E-06,3.85E-05,0.00011402,9.70E-05,0.000138513,7.58E-05,0.000351348,0.47154407,0.52647177,9.70E-06,1.19E-05
3043,text-classification9,34,"This provides an opportunity to leverage on as many languages as possible to any domain , providing a much wider context compared to the limited contexts provided by past studies .",Introduction,Introduction,text-classification,9,23,1,0,,0.441333361,0,negative,0.000100652,0.081034831,3.97E-05,3.72E-05,0.007693211,0.000138084,0.000100816,0.000133806,0.038071915,0.870858687,0.001719324,6.72E-05,4.67E-06
3044,text-classification9,35,"However , despite the maturity of translation , naively concatenating their vectors to the original sentence vector may introduce more noise than signals .",Introduction,Introduction,text-classification,9,24,1,0,,0.002548062,0,negative,2.84E-05,0.001766457,3.24E-06,2.37E-05,0.000481247,5.05E-05,4.35E-05,3.28E-05,0.000300985,0.976688605,0.020561003,1.67E-05,2.75E-06
3045,text-classification9,36,The unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable .,Introduction,Introduction,text-classification,9,25,1,0,,0.061977355,0,negative,7.11E-05,0.063627585,9.30E-05,1.28E-05,0.001298184,0.000371065,0.000269578,0.00042992,0.220412159,0.710227197,0.00312135,5.50E-05,1.11E-05
3046,text-classification9,37,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .",Introduction,Introduction,text-classification,9,26,1,0,,0.921978656,1,approach,0.000215953,0.780434406,0.000187214,0.000114589,0.008574152,0.000213525,0.000279079,0.000350971,0.094968281,0.108039756,0.006498721,9.15E-05,3.19E-05
3047,text-classification9,38,Suppose there are two translated sentences a and b with slight errors .,Introduction,Introduction,text-classification,9,27,1,0,,0.000997134,0,negative,5.27E-06,0.011966689,2.73E-06,1.83E-06,0.000312819,6.20E-05,5.12E-05,0.00022913,0.0089258,0.975151721,0.003277528,1.10E-05,2.35E-06
3048,text-classification9,39,"We posit that a can be used to fix b when a is used as a context of b , and vice versa",Introduction,Introduction,text-classification,9,28,1,0,,0.003751541,0,negative,6.84E-06,0.041355647,1.18E-05,1.35E-05,0.000414619,0.000370922,5.25E-05,0.000880503,0.242596782,0.712816407,0.001470107,5.11E-06,5.24E-06
3049,text-classification9,40,1 .,Introduction,Introduction,text-classification,9,29,1,0,,0.001344794,0,negative,7.13E-06,0.002467595,2.05E-06,8.68E-06,0.000164272,0.000232537,3.00E-05,0.000290056,0.009022496,0.987470659,0.000300188,2.73E-06,1.63E-06
3050,text-classification9,41,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",Introduction,Introduction,text-classification,9,30,1,0,,0.001152159,0,negative,1.23E-05,0.0043724,7.89E-06,0.000119796,0.005961894,0.000628379,9.34E-05,0.000264284,0.002417087,0.985841478,0.000267156,7.13E-06,6.82E-06
3051,text-classification9,42,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",Introduction,Introduction,text-classification,9,31,1,1,model,0.956238701,1,approach,7.79E-05,0.485207368,0.000739185,1.05E-05,0.001621369,0.000100805,0.000498047,0.00015194,0.471481031,0.03418794,0.00581718,7.12E-05,3.56E-05
3052,text-classification9,43,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .",Introduction,Introduction,text-classification,9,32,1,1,model,0.939499326,1,negative,4.67E-05,0.185458477,0.001324042,0.000172252,0.008976226,0.001274883,0.00176971,0.00086771,0.119070986,0.613966054,0.066731408,9.70E-05,0.000244581
3053,text-classification9,44,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .",Introduction,Introduction,text-classification,9,33,1,1,model,0.862769969,1,model,9.84E-05,0.394105432,0.000121654,2.06E-05,0.00359748,0.000244737,0.000107464,0.000552701,0.464306803,0.136681611,0.000143564,1.17E-05,7.84E-06
3054,text-classification9,45,Noises from translation may cause adverse effects to the vector itself ( e.g. when a noisy vector is directly used for the task ) and relatively to other vectors ( e.g. when a noisy vector is used to fix another noisy vector ) .,Introduction,Introduction,text-classification,9,34,1,0,,0.014937818,0,negative,0.000104513,0.005770309,2.08E-05,1.25E-05,0.00225761,0.000105133,0.000111827,4.47E-05,0.002005944,0.989124616,0.000381264,5.79E-05,2.85E-06
3055,text-classification9,46,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,Introduction,Introduction,text-classification,9,35,1,1,model,0.939722544,1,approach,6.89E-05,0.738410203,0.000898131,1.70E-05,0.00589749,0.000198108,0.001336665,0.000281746,0.106931959,0.137335143,0.008377534,0.00019603,5.11E-05
3056,text-classification9,47,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",Introduction,Introduction,text-classification,9,36,1,1,model,0.922251929,1,model,1.80E-05,0.122008088,0.000426365,2.90E-07,0.000132891,5.39E-05,0.00013062,7.91E-05,0.749313704,0.127094126,0.000724402,1.52E-05,3.26E-06
3057,text-classification9,48,Listed below are the three main strengths of the MCFA attachment .,Introduction,Introduction,text-classification,9,37,1,0,,0.003564514,0,negative,2.83E-05,0.003722363,1.17E-05,0.000161728,0.002755364,0.000405854,8.46E-05,0.000204002,0.003576141,0.988737067,0.000294897,1.06E-05,7.40E-06
3058,text-classification9,49,"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .",Introduction,Introduction,text-classification,9,38,1,0,,0.922595575,1,approach,0.001925293,0.667789229,0.000996405,0.000270298,0.052014519,0.000677348,0.001520082,0.000452373,0.054230748,0.219344211,0.000440051,0.000288507,5.09E-05
3059,text-classification9,50,( 2 ) MCFA is extensible and improves the accuracy as the number of translated sentences increases .,Introduction,Introduction,text-classification,9,39,1,0,,0.280537045,0,negative,0.00898086,0.112413078,0.000628277,0.000185636,0.011375165,0.001392165,0.073022917,0.001502281,0.007423277,0.753517062,0.003004176,0.025861039,0.000694067
3060,text-classification9,51,"( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .",Introduction,Introduction,text-classification,9,40,1,0,,0.919906818,1,approach,0.000459712,0.59150411,0.000636166,1.00E-05,0.003061603,0.000126182,0.000642592,0.000176487,0.263375277,0.139442655,0.000403947,0.00014711,1.41E-05
3061,text-classification9,52,"Results show that a convolutional neural network ( CNN ) attached with MCFA significantly improves the classification performance of CNN , achieving state of the 1 Hereon , we mean to "" fix "" as to "" correct , repair , or alter . "" art performance over multiple data sets .",Introduction,Introduction,text-classification,9,41,1,0,,0.059531467,0,negative,0.004375592,0.135392397,0.000210121,3.61E-05,0.006469251,0.00049351,0.031943937,0.000943012,0.011503393,0.792641185,0.001238552,0.014498211,0.000254753
3062,text-classification9,53,Preliminaries,Introduction,,text-classification,9,42,1,0,,0.001049505,0,negative,5.64E-06,0.004272269,8.26E-06,7.95E-05,0.001045594,0.000748243,0.000151179,0.000558407,0.005839405,0.986919957,0.000355308,4.83E-06,1.14E-05
3063,text-classification9,54,Problem : Translated Sentences as Context,Introduction,,text-classification,9,43,1,0,,0.021109051,0,negative,2.40E-05,0.020336949,3.20E-05,7.10E-06,0.000145406,0.000150679,0.004024038,0.00033009,0.011590562,0.736701701,0.226053253,0.000505066,9.92E-05
3064,text-classification9,55,"In this paper , the ultimate task that we solve is the sentence classification task where given a sentence and a list of classes , one is task to classify which class ( e.g. positive or negative sentiment ) among the list of classes does the sentence belong .",Introduction,Problem : Translated Sentences as Context,text-classification,9,44,1,0,,0.003773824,0,negative,1.19E-05,0.000119498,1.29E-05,2.61E-05,4.83E-05,8.03E-05,0.000126175,7.58E-05,9.64E-05,0.99853335,0.000454649,0.000326964,8.76E-05
3065,text-classification9,56,"However , the main challenge that we tackle is the task on how to utilize translated sentences as additional context in order to improve the performance of the classifier .",Introduction,Problem : Translated Sentences as Context,text-classification,9,45,1,0,,0.000747283,0,negative,1.34E-05,3.75E-05,1.77E-06,8.08E-07,4.57E-06,6.38E-06,7.77E-06,1.11E-05,5.21E-05,0.999687345,1.87E-05,0.000156344,2.16E-06
3066,text-classification9,57,"Specifically , the problem states : given the original sentence s , the goal is to use t 1 , t 2 , ... , tn , or sentences in other languages which are translated from s , as additional context .",Introduction,Problem : Translated Sentences as Context,text-classification,9,46,1,0,,3.76E-05,0,negative,2.74E-06,4.62E-05,2.26E-06,5.05E-07,3.10E-06,9.21E-06,5.26E-06,4.11E-05,0.000340022,0.999523654,3.45E-06,2.02E-05,2.21E-06
3067,text-classification9,58,Base Model : Convolutional Neural Network .,Introduction,,text-classification,9,47,1,0,,0.274960845,0,model,0.000114572,0.040057601,0.001302659,0.000181707,0.002430534,0.007024592,0.00625736,0.004598096,0.523181027,0.413786491,0.000517881,6.51E-05,0.000482343
3068,text-classification9,59,The base model used is the convolutional neural network ( CNN ) for sentences .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,48,1,0,,0.015035758,0,negative,1.90E-05,0.000373599,0.000598197,1.67E-06,3.91E-05,0.000189256,9.76E-05,0.0004282,0.005940868,0.992268647,1.75E-06,2.52E-05,1.69E-05
3069,text-classification9,60,It is a simple variation of the original CNN for texts to be used on sentences .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,49,1,0,,1.46E-05,0,negative,1.81E-05,0.000226151,0.001617575,1.24E-07,8.65E-06,1.22E-05,1.32E-05,2.15E-05,0.005795013,0.992258876,1.37E-06,2.52E-05,2.12E-06
3070,text-classification9,61,Let xi ?,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,50,1,0,,7.27E-07,0,negative,1.18E-06,8.76E-07,3.62E-07,2.17E-09,1.04E-07,5.37E-07,2.47E-07,1.73E-06,1.32E-05,0.999977345,2.79E-08,4.43E-06,8.51E-09
3071,text-classification9,62,Rd be the d-dimensional word vector of the i - th word in a sentence of length n .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,51,1,0,,3.05E-06,0,negative,4.77E-07,4.91E-06,3.65E-07,4.58E-09,1.89E-07,1.39E-06,4.08E-07,1.02E-05,4.87E-05,0.999931154,6.43E-08,2.11E-06,2.25E-08
3072,text-classification9,63,A convolution operation involves applying a filter matrix W ?,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,52,1,0,,3.04E-06,0,negative,1.48E-06,4.18E-06,2.47E-06,2.71E-09,1.72E-07,1.00E-06,3.95E-07,3.52E-06,0.000117315,0.999867531,4.65E-08,1.87E-06,2.36E-08
3073,text-classification9,64,R hd to a window of h words and producing a new feature vector c i using the equation,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,53,1,0,,1.48E-06,0,negative,7.77E-06,5.20E-06,5.86E-06,1.95E-08,1.23E-06,1.83E-06,1.06E-06,4.05E-06,6.75E-05,0.999891878,6.72E-08,1.35E-05,7.34E-08
3074,text-classification9,65,bias vector and f ( . ) is a non-linear function .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,54,1,0,,8.50E-06,0,negative,2.59E-06,4.57E-05,2.35E-06,6.25E-08,1.04E-06,2.03E-05,4.56E-06,0.000302012,0.001104765,0.99851334,3.21E-07,2.51E-06,4.60E-07
3075,text-classification9,66,"By doing this on all possible windows of words we produce a feature map c = [ c 1 , c 2 , ... ] .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,55,1,0,,3.09E-06,0,negative,1.09E-06,1.27E-05,1.26E-06,7.52E-09,4.26E-07,1.30E-06,3.86E-07,7.16E-06,0.000496892,0.99947707,4.25E-08,1.59E-06,5.53E-08
3076,text-classification9,67,We then apply a max - over - time pooling operation over the feature map and take the maximum value as the feature vector of the filter .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,56,1,0,,0.00021739,0,negative,2.17E-05,9.84E-05,5.08E-05,1.54E-08,1.66E-06,3.60E-06,2.58E-06,1.44E-05,0.003447884,0.996353023,5.53E-08,5.64E-06,2.13E-07
3077,text-classification9,68,We do this on all feature vectors and concatenate all the feature vectors to obtain the final feature vector v.,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,57,1,0,,2.48E-05,0,negative,2.78E-06,3.38E-05,2.89E-06,1.11E-08,9.40E-07,2.16E-06,9.52E-07,1.33E-05,0.000692702,0.999247998,3.34E-08,2.25E-06,1.04E-07
3078,text-classification9,69,We can then use this vector as input features to train a classifier such as logistic regression .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,58,1,0,,2.83E-06,0,negative,3.49E-07,3.33E-06,7.20E-07,2.98E-09,1.80E-07,1.41E-06,2.93E-07,7.58E-06,0.000124955,0.999860565,1.15E-08,5.84E-07,2.50E-08
3079,text-classification9,70,"We use CNN to create sentence vectors for all sentences s , t 1 , t 2 , ... , tn .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,59,1,0,,3.62E-05,0,negative,1.97E-06,0.000135518,1.66E-05,3.81E-08,2.80E-06,1.87E-05,6.60E-06,0.000153175,0.005543926,0.994117439,1.21E-07,2.16E-06,9.57E-07
3080,text-classification9,71,"From hereon , we refer to these vectors as v s , v t1 , v t2 , ... , v tn , respectively .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,60,1,0,,1.10E-06,0,negative,8.24E-08,1.44E-06,1.02E-07,2.69E-09,7.67E-08,1.34E-06,2.16E-07,7.37E-06,3.16E-05,0.99995726,5.90E-09,4.68E-07,1.03E-08
3081,text-classification9,72,We refer to them collectively as V .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,61,1,0,,1.02E-06,0,negative,1.42E-07,1.83E-06,2.04E-07,3.61E-09,1.77E-07,9.51E-07,1.87E-07,4.63E-06,3.08E-05,0.999960425,4.92E-09,6.26E-07,1.12E-08
3082,text-classification9,73,Baseline 1 : Naive Concatenation .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,62,1,0,,0.002191043,0,negative,2.20E-06,1.27E-05,0.000118507,2.15E-08,8.32E-07,2.05E-05,3.72E-05,3.41E-05,0.000361995,0.99937846,1.91E-07,3.17E-05,1.58E-06
3083,text-classification9,74,A simple method in order to use the translated sentences as additional context is to naively concatenate their vectors with the vector of the original sentence .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,63,1,0,,6.87E-06,0,negative,3.08E-06,1.12E-05,1.46E-06,3.77E-08,8.75E-07,3.46E-06,9.95E-07,1.34E-05,7.27E-05,0.999886881,1.86E-07,5.47E-06,1.82E-07
3084,text-classification9,75,"That is , we create a wide vectorv = [ v s ; v t1 ; ... ; v tn ] , and use this as the input feature vector of the sentence to the classifier .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,64,1,0,,2.84E-06,0,negative,3.86E-07,1.21E-05,9.31E-07,4.10E-09,4.35E-07,1.69E-06,4.49E-07,1.63E-05,0.00027263,0.999694435,7.93E-09,6.18E-07,4.35E-08
3085,text-classification9,76,This method works fine if the translated sentences are translated properly .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,65,1,0,,3.72E-05,0,negative,2.16E-05,7.45E-06,2.53E-06,4.04E-08,2.58E-06,3.55E-06,3.79E-06,9.80E-06,1.42E-05,0.999820312,1.57E-07,0.000113689,3.17E-07
3086,text-classification9,77,"However , sentences translated using machine translation models usually contain incorrect translation .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,66,1,0,,0.000853966,0,negative,2.69E-06,3.32E-06,9.69E-07,1.96E-06,7.84E-06,1.71E-05,8.83E-06,1.61E-05,3.62E-06,0.999899142,5.62E-06,2.98E-05,3.06E-06
3087,text-classification9,78,"In effect , this method will have adverse effects on the over all performance of the classifier .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,67,1,0,,1.82E-06,0,negative,1.09E-05,9.65E-07,4.51E-07,9.12E-09,6.57E-07,5.62E-07,3.28E-07,1.18E-06,4.43E-06,0.999967453,5.36E-09,1.30E-05,2.02E-08
3088,text-classification9,79,This will especially be very evident if the number of additional sentences increases .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,68,1,0,,1.32E-05,0,negative,3.72E-05,1.56E-06,8.77E-07,2.38E-08,9.90E-07,1.66E-06,9.23E-07,3.01E-06,1.28E-05,0.999924467,9.56E-09,1.64E-05,5.47E-08
3089,text-classification9,80,Baseline 2 : L2 Regularization .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,69,1,0,,0.023175036,0,negative,6.85E-06,1.24E-05,0.000112389,2.04E-08,1.55E-06,1.43E-05,3.28E-05,2.88E-05,0.000150083,0.99959547,2.96E-08,4.42E-05,1.20E-06
3090,text-classification9,81,"In order to alleviate the problems above , we can use L2 regularization to automatically select useful features by weakening the appropriate weights .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,70,1,0,,0.000255358,0,negative,1.56E-05,3.61E-05,2.16E-06,1.89E-08,1.50E-06,1.66E-06,9.95E-07,1.59E-05,0.000176101,0.999739901,9.55E-09,9.98E-06,1.01E-07
3091,text-classification9,82,The main problem of this method occurs when almost all of the weights coming from the vectors of the translated sentence are weakened .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,71,1,0,,5.21E-06,0,negative,6.35E-06,8.78E-06,1.02E-06,5.34E-07,2.68E-06,1.18E-05,3.11E-06,2.31E-05,2.05E-05,0.999903033,5.73E-07,1.76E-05,1.01E-06
3092,text-classification9,83,This leads to making the additional context vectors useless and to having a similar performance when there are no additional context .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,72,1,0,,1.07E-05,0,negative,2.12E-05,1.77E-06,9.82E-07,3.52E-09,3.92E-07,3.08E-07,2.35E-07,8.55E-07,1.79E-05,0.999946998,1.63E-09,9.24E-06,1.21E-08
3093,text-classification9,84,"Ultimately , this method does not make use of the full potential of the additional context .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,73,1,0,,1.41E-06,0,negative,1.45E-06,1.29E-06,5.78E-07,4.00E-08,8.71E-07,2.11E-06,5.18E-07,3.19E-06,5.99E-06,0.999978349,3.35E-08,5.49E-06,8.36E-08
3094,text-classification9,85,usability usability ( a ) Self and relative usability modules,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,74,1,0,,0.000241576,0,negative,6.56E-05,1.83E-05,4.45E-05,4.32E-08,5.56E-06,5.86E-06,0.000121783,1.07E-05,3.72E-05,0.997155813,2.74E-07,0.002530093,4.27E-06
3095,text-classification9,86,Model,,,text-classification,9,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
3096,text-classification9,87,"To solve the problems of the baselines discussed above , we introduce an attention - based neural multiple context fixing attachment ( MCFA ) 2 , a series of modules attached to the sentence vectors V .",Model,Model,text-classification,9,1,1,0,,0.147629286,0,model,0.001935019,0.012226434,0.050552187,2.06E-05,9.64E-05,0.000260605,0.000491632,0.00167834,0.648295228,0.282085987,0.000768322,0.001402284,0.000186951
3097,text-classification9,88,"MCFA attachment is used to fix the sentence vectors , by slightly modifying the per-dimension values of the vector , before concatenating them into the final feature vector .",Model,Model,text-classification,9,2,1,0,,0.093485655,0,negative,0.001674495,0.011352477,0.036952316,2.18E-05,8.11E-05,0.000482427,0.000578765,0.005933488,0.295174199,0.646736052,0.000141045,0.000789661,8.22E-05
3098,text-classification9,89,"The sentence vectors are altered using other sentence vectors as context ( e.g. v t 1 is altered using v s , v t2 , ... , v tn ) .",Model,Model,text-classification,9,3,1,0,,0.011314546,0,negative,9.72E-05,0.000239268,0.000536139,3.27E-07,1.31E-06,3.67E-05,2.30E-05,0.000427367,0.018677964,0.979801432,1.03E-05,0.000146824,2.17E-06
3099,text-classification9,90,This results to moving the vectors in the same vector space .,Model,Model,text-classification,9,4,1,0,,0.036339623,0,negative,0.000112384,0.000233447,0.000467251,6.48E-07,1.63E-06,2.81E-05,1.61E-05,0.000237181,0.027203533,0.971553626,2.71E-05,0.0001153,3.74E-06
3100,text-classification9,91,The full architecture is shown in .,Model,Model,text-classification,9,5,1,0,,0.027000236,0,negative,0.000256023,0.001003174,0.000812619,2.21E-05,3.12E-05,0.000312167,0.000228873,0.001242106,0.201274171,0.793652589,0.000489414,0.000493581,0.000181912
3101,text-classification9,92,Self Usability Module,Model,,text-classification,9,6,1,0,,0.020478098,0,negative,0.002283052,0.000554831,0.002841208,3.64E-05,5.07E-05,0.000445133,0.001688365,0.001648064,0.019431366,0.938963869,0.000442485,0.031089577,0.000524976
3102,text-classification9,93,"To fix a source sentence vector 3 , we use the other sentence vectors as guide to know which dimensions to fix and to what extent do we need to fix them .",Model,Self Usability Module,text-classification,9,7,1,0,,2.17E-06,0,negative,2.05E-05,7.95E-06,1.20E-05,7.44E-09,1.17E-07,5.01E-06,2.13E-06,2.83E-05,4.14E-05,0.999872289,2.20E-07,1.01E-05,1.77E-08
3103,text-classification9,94,"However , other vectors might also contain errors which may reflect to the fixing of the source sentence vector .",Model,Self Usability Module,text-classification,9,8,1,0,,8.78E-07,0,negative,1.06E-05,2.38E-07,1.98E-07,7.95E-09,3.53E-08,7.73E-07,2.89E-07,1.94E-06,1.91E-06,0.999979111,2.61E-07,4.60E-06,7.17E-09
3104,text-classification9,95,"In order to cope with this , we introduce self usability modules .",Model,Self Usability Module,text-classification,9,9,1,0,,9.19E-05,0,negative,0.000547821,0.000364186,0.000402041,1.23E-07,2.18E-06,7.73E-06,8.89E-06,3.32E-05,0.00483475,0.993658169,5.96E-06,0.00013442,5.31E-07
3105,text-classification9,96,"A self usability module contains the self usability of the vector ? i ( a ) , which measures how confident sentence a is for the task at hand .",Model,Self Usability Module,text-classification,9,10,1,0,,3.78E-06,0,negative,3.36E-05,5.44E-05,0.000264389,3.72E-08,3.34E-07,5.77E-06,2.82E-06,2.27E-05,0.001638507,0.997958364,4.43E-06,1.45E-05,2.37E-07
3106,text-classification9,97,"For example , an ambiguous sentence ( e.g. "" The movie is terribly amazing "" ) may receive a low self usability , while a clear and definite sentence ( e.g. "" The movie is very good "" ) may receive a high self usability .",Model,Self Usability Module,text-classification,9,11,1,0,,6.64E-07,0,negative,2.73E-06,6.05E-07,3.21E-07,9.02E-09,4.86E-08,9.63E-07,3.97E-07,5.07E-06,4.85E-06,0.999979312,5.07E-07,5.17E-06,1.45E-08
3107,text-classification9,98,"Mathematically , we calculate the self usability of the vector vi of sentence i , denoted as ? i ( v i ) , using the equation",Model,Self Usability Module,text-classification,9,12,1,0,,3.78E-07,0,negative,8.67E-06,2.23E-06,4.96E-06,1.89E-09,2.47E-08,4.90E-07,3.19E-07,2.74E-06,1.39E-05,0.999955816,4.55E-07,1.04E-05,6.15E-09
3108,text-classification9,99,is a matrix to be learned .,Model,Self Usability Module,text-classification,9,13,1,0,,7.79E-06,0,negative,3.44E-06,1.37E-05,1.16E-06,6.20E-08,8.87E-08,8.46E-06,1.35E-06,0.000146518,0.000168001,0.999653173,1.88E-06,2.07E-06,8.64E-08
3109,text-classification9,100,The produced value is a single real number from 0 to 1 .,Model,Self Usability Module,text-classification,9,14,1,0,,9.46E-06,0,negative,1.98E-06,1.02E-05,1.20E-06,2.29E-08,5.68E-08,6.08E-05,8.49E-06,0.000873027,0.000119964,0.998919783,1.13E-06,3.22E-06,1.38E-07
3110,text-classification9,101,We pre-calculate the self usability of all sentence vectors vi ?,Model,Self Usability Module,text-classification,9,15,1,0,,1.25E-06,0,negative,4.05E-06,2.98E-06,1.97E-06,2.56E-09,2.23E-08,1.06E-06,3.04E-07,9.40E-06,2.65E-05,0.999951426,1.42E-07,2.08E-06,9.17E-09
3111,text-classification9,102,V .,Model,,text-classification,9,16,1,0,,0.000650572,0,negative,0.000115959,2.43E-05,2.37E-05,1.17E-06,1.57E-06,3.87E-05,2.43E-05,0.000306461,0.001385896,0.997838332,2.71E-06,0.000233086,3.79E-06
3112,text-classification9,103,"These are used in the next module , the relative usability module .",Model,V .,text-classification,9,17,1,0,,1.94E-05,0,negative,2.84E-05,6.28E-05,0.000214381,3.22E-08,4.99E-07,2.44E-06,8.18E-07,3.82E-05,0.00580639,0.993823946,1.57E-06,2.01E-05,4.18E-07
3113,text-classification9,104,Relative Usability,Model,,text-classification,9,18,1,0,,0.03437652,0,negative,0.000438225,0.000108775,0.000287556,7.12E-06,2.28E-05,0.00024333,0.002820682,0.002287746,0.000814956,0.934966851,4.97E-05,0.057807507,0.000144729
3114,text-classification9,105,Module,Model,,text-classification,9,19,1,0,,0.005164616,0,negative,0.000340046,0.000427286,0.001690034,6.67E-06,1.55E-05,0.000159066,0.000283993,0.000939815,0.122387475,0.871673997,9.96E-05,0.001755881,0.000220646
3115,text-classification9,106,"Relative usability ? r ( a , b ) measures how useful a can be when fixing b , relative to other sentences .",Model,Module,text-classification,9,20,1,0,,0.000103867,0,negative,1.73E-06,1.21E-05,1.12E-05,6.53E-08,5.21E-08,3.90E-06,2.48E-06,0.000172919,5.42E-05,0.999562199,8.52E-05,9.26E-05,1.36E-06
3116,text-classification9,107,"There are two main differences between ? i ( a ) and ? r ( a , b ) .",Model,Module,text-classification,9,21,1,0,,8.87E-08,0,negative,8.17E-06,2.23E-06,8.26E-06,4.02E-09,2.04E-08,3.24E-07,1.27E-07,7.88E-06,1.01E-05,0.999904171,4.34E-07,5.83E-05,2.43E-08
3117,text-classification9,108,"First , ? i ( a ) is calculated before a knows about b while ? r ( a , b ) is calculated when a knows about b.",Model,Module,text-classification,9,22,1,0,,1.49E-07,0,negative,1.57E-05,5.78E-06,2.06E-05,2.81E-09,3.58E-08,1.91E-07,6.52E-08,6.06E-06,6.80E-05,0.99985735,9.63E-08,2.62E-05,1.55E-08
3118,text-classification9,109,"Second , ? r ( a , b ) can below even though ? i ( a ) is not .",Model,Module,text-classification,9,23,1,0,,2.20E-07,0,negative,2.30E-05,1.22E-06,3.50E-06,1.85E-08,4.46E-08,6.88E-07,2.07E-07,1.41E-05,9.79E-06,0.999865309,2.48E-07,8.18E-05,5.90E-08
3119,text-classification9,110,This means that a is notable to help in fixing the wrong information in b .,Model,Module,text-classification,9,24,1,0,,6.59E-07,0,negative,2.97E-05,2.05E-06,2.90E-06,2.30E-08,9.34E-08,4.30E-07,1.22E-07,1.02E-05,1.82E-05,0.999895851,7.66E-08,4.03E-05,4.94E-08
3120,text-classification9,111,"Here , we extend the additive attention module and use it as a method to calculate the relative usability of two sentences of different languages .",Model,Module,text-classification,9,25,1,0,,0.000140544,0,negative,0.000151746,0.001799346,0.000966262,5.00E-07,2.96E-06,6.16E-06,6.02E-06,0.000191788,0.00642349,0.989681361,9.41E-05,0.000671403,4.87E-06
3121,text-classification9,112,"To better visualize the original attention mechanism , we present the equations below .",Model,Module,text-classification,9,26,1,0,,9.44E-08,0,negative,4.89E-06,4.53E-06,2.46E-06,2.10E-08,4.61E-08,8.03E-07,1.03E-07,2.62E-05,0.000176128,0.999778564,1.02E-07,6.05E-06,6.58E-08
3122,text-classification9,113,One major challenge in using the attention mechanism in our problem is that the sentence vectors do not belong to the same vector space .,Model,Module,text-classification,9,27,1,0,,9.46E-06,0,negative,4.88E-05,4.24E-05,5.34E-06,4.88E-07,4.98E-07,3.06E-06,1.11E-06,8.43E-05,7.14E-05,0.999498468,1.83E-05,0.000224696,1.12E-06
3123,text-classification9,114,"Moreover , one characteristic of our problem is that the sentence vectors can be both a source and a context vector ( e.g. v scan be both sand ti in Equation 1 ) .",Model,Module,text-classification,9,28,1,0,,7.37E-07,0,negative,6.97E-06,6.13E-05,5.84E-06,1.14E-07,2.66E-07,1.65E-06,2.97E-07,6.73E-05,0.000299498,0.999521893,3.40E-06,3.11E-05,3.04E-07
3124,text-classification9,115,"Because of these , we can not directly use the additive attention module .",Model,Module,text-classification,9,29,1,0,,5.91E-07,0,negative,5.95E-05,1.93E-06,4.27E-06,1.38E-08,6.26E-08,4.58E-07,1.34E-07,8.33E-06,1.48E-05,0.999833378,8.25E-08,7.69E-05,4.87E-08
3125,text-classification9,116,We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ?,Model,Module,text-classification,9,30,1,0,,7.38E-07,0,negative,2.18E-05,2.18E-05,3.64E-05,1.43E-08,1.13E-07,8.10E-07,2.24E-07,2.52E-05,0.000398502,0.999460681,2.21E-07,3.41E-05,1.35E-07
3126,text-classification9,117,"R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .",Model,Module,text-classification,9,31,1,0,,6.56E-07,0,negative,0.000105038,1.36E-05,4.25E-05,3.12E-08,2.77E-07,9.99E-07,8.49E-07,2.97E-05,4.19E-05,0.999317086,3.28E-07,0.000447405,2.43E-07
3127,text-classification9,118,"Finally , we incorporate the self usability function ?",Model,Module,text-classification,9,32,1,0,,0.000182672,0,negative,4.87E-05,7.56E-06,2.51E-05,1.15E-07,3.09E-07,3.31E-06,7.55E-07,8.41E-05,0.000182875,0.99960975,6.05E-08,3.69E-05,4.99E-07
3128,text-classification9,119,i ( v k ) to reflect the self usability of a sentence .,Model,Module,text-classification,9,33,1,0,,4.41E-07,0,negative,9.82E-06,2.35E-06,1.78E-05,3.10E-09,4.24E-08,2.45E-07,9.30E-08,7.00E-06,2.32E-05,0.999899593,3.87E-08,3.98E-05,3.34E-08
3129,text-classification9,120,"Ultimately , the relative usability denoted as ? r ( v i , v j ) is calculated using the equations below , where is the multiplication of a vector and a scalar through broadcasting .",Model,Module,text-classification,9,34,1,0,,2.22E-07,0,negative,3.31E-06,2.77E-06,5.46E-06,2.94E-09,2.43E-08,3.02E-07,6.79E-08,1.14E-05,7.70E-05,0.999891799,3.45E-08,7.78E-06,3.19E-08
3130,text-classification9,121,Vector Fixing Module,Model,,text-classification,9,35,1,0,,0.009714213,0,negative,0.000449121,0.00018607,0.002576872,2.68E-06,1.24E-05,6.73E-05,0.000400437,0.000403846,0.034208837,0.956650684,1.39E-05,0.004861152,0.000166747
3131,text-classification9,122,The vector fixing module applies the attention weights to the sentence vectors and creates an integrated context vector .,Model,Vector Fixing Module,text-classification,9,36,1,0,,0.000113905,0,negative,0.000101249,1.54E-05,0.000911808,4.34E-08,1.59E-07,8.96E-07,2.62E-06,2.51E-05,0.004467297,0.994401585,1.87E-07,6.66E-05,7.04E-06
3132,text-classification9,123,We then use this vector alongside with the source sentence vector to create a weighted gate vector .,Model,Vector Fixing Module,text-classification,9,37,1,0,,2.13E-06,0,negative,6.84E-06,3.21E-06,2.32E-05,1.26E-08,3.83E-08,9.63E-07,9.82E-07,5.00E-05,0.000659966,0.999243577,3.12E-08,9.69E-06,1.46E-06
3133,text-classification9,124,The weighted gate vector is used to determine to what extent should a dimension of the source sentence vector be altered .,Model,Vector Fixing Module,text-classification,9,38,1,0,,3.57E-06,0,negative,2.38E-05,4.79E-06,5.77E-05,6.73E-09,3.97E-08,1.93E-07,3.39E-07,9.64E-06,0.000625331,0.999254609,2.33E-08,2.31E-05,3.61E-07
3134,text-classification9,125,The common way to apply the attention weights to the context vectors and create an integrated context vector c i is to directly do weighted sum of all the context vectors .,Model,Vector Fixing Module,text-classification,9,39,1,0,,3.81E-07,0,negative,1.42E-05,1.93E-06,1.34E-05,6.11E-08,4.40E-08,5.33E-07,5.88E-07,1.75E-05,8.31E-05,0.999834666,2.84E-07,3.17E-05,2.07E-06
3135,text-classification9,126,"However , this is not possible because the context vectors are not on the same space .",Model,Vector Fixing Module,text-classification,9,40,1,0,,3.54E-07,0,negative,8.40E-06,3.01E-07,1.53E-06,1.93E-08,1.15E-08,2.27E-07,4.00E-07,7.35E-06,9.82E-06,0.999872354,1.61E-07,9.85E-05,9.21E-07
3136,text-classification9,127,"Thus , we use a projection matrix U k ?",Model,Vector Fixing Module,text-classification,9,41,1,0,,7.00E-07,0,negative,8.30E-05,1.35E-06,3.18E-05,9.75E-09,5.87E-08,2.64E-07,6.90E-07,6.22E-06,4.17E-05,0.999695168,1.09E-08,0.000139402,3.44E-07
3137,text-classification9,128,R dd to linearly project the sentence vector v k to transform the sentence vectors into a common vector space .,Model,Vector Fixing Module,text-classification,9,42,1,0,,8.35E-07,0,negative,4.23E-05,5.37E-07,4.82E-05,8.46E-09,3.30E-08,1.89E-07,6.31E-07,4.30E-06,2.36E-05,0.999757632,1.41E-08,0.000121947,6.64E-07
3138,text-classification9,129,The integrated context vector c i is then calculated as,Model,Vector Fixing Module,text-classification,9,43,1,0,,1.31E-07,0,negative,1.43E-05,3.01E-07,6.84E-06,6.71E-09,1.52E-08,1.20E-07,2.40E-07,3.48E-06,2.38E-05,0.999900257,1.22E-08,5.03E-05,3.67E-07
3139,text-classification9,130,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ?",Model,Vector Fixing Module,text-classification,9,44,1,0,,3.87E-07,0,negative,3.17E-05,9.30E-07,2.35E-05,2.65E-09,2.31E-08,1.14E-07,3.87E-07,3.39E-06,5.07E-05,0.999819,5.51E-09,7.01E-05,1.83E-07
3140,text-classification9,131,R 2dd is a trainable parameter and ?,Model,Vector Fixing Module,text-classification,9,45,1,0,,2.03E-07,0,negative,2.14E-05,4.01E-06,6.24E-06,2.58E-07,1.69E-07,7.47E-06,1.08E-05,0.000747491,7.46E-05,0.999004508,9.33E-08,0.000105781,1.71E-05
3141,text-classification9,132,is the element - wise multiplication procedure .,Model,Vector Fixing Module,text-classification,9,46,1,0,,9.64E-06,0,negative,1.14E-05,8.62E-07,1.60E-05,3.31E-08,5.73E-08,4.76E-07,7.20E-07,1.59E-05,5.95E-05,0.999873509,1.96E-08,1.96E-05,1.96E-06
3142,text-classification9,133,The weighted gate vector is a vector of real numbers between 0 and 1 to modify the intensity of per-dimension values of the sentence vector .,Model,Vector Fixing Module,text-classification,9,47,1,0,,3.15E-05,0,negative,1.52E-05,1.26E-05,2.91E-05,1.43E-07,2.40E-07,8.48E-06,8.25E-06,0.001193956,0.001041518,0.997652047,4.45E-08,2.36E-05,1.48E-05
3143,text-classification9,134,This causes the vector to move in the same vector space towards the correct direction .,Model,Vector Fixing Module,text-classification,9,48,1,0,,4.29E-07,0,negative,1.29E-05,4.75E-07,1.13E-05,5.55E-09,2.05E-08,9.86E-08,1.50E-07,3.20E-06,9.54E-05,0.999861661,5.41E-09,1.43E-05,3.91E-07
3144,text-classification9,135,"An alternative approach to do vector correction is using a residual - style correction , where instead of multiplying agate vector , a residual vector is added to the original vector .",Model,Vector Fixing Module,text-classification,9,49,1,0,,1.75E-06,0,negative,4.38E-05,3.44E-06,0.000142233,2.50E-07,2.50E-07,2.18E-06,4.20E-06,2.97E-05,9.97E-05,0.999505758,5.25E-07,0.00014111,2.68E-05
3145,text-classification9,136,"However , this approach makes the correction not interpretable ; it is hard to explain what does adding a value to a specific dimension mean .",Model,Vector Fixing Module,text-classification,9,50,1,0,,6.54E-08,0,negative,9.17E-06,2.42E-07,1.73E-06,6.31E-08,2.67E-08,4.49E-07,4.15E-07,8.83E-06,8.47E-06,0.999918382,6.40E-08,5.03E-05,1.88E-06
3146,text-classification9,137,One major advantage of MCFA is that the corrections in the vectors are interpretable ; the weights in the gate vector correspond to the importance of the per-dimension features of the vector .,Model,Vector Fixing Module,text-classification,9,51,1,0,,2.06E-06,0,negative,7.44E-05,1.14E-05,0.000109448,1.74E-07,3.03E-07,8.99E-07,4.14E-06,1.36E-05,0.000141738,0.998364279,1.13E-06,0.001264016,1.45E-05
3147,text-classification9,138,"The altered vector ? v s , ... , v tn are then concatenated and fed directly as an input vector to the logistic regression classifier for training .",Model,Vector Fixing Module,text-classification,9,52,1,0,,6.73E-07,0,negative,5.57E-06,4.49E-07,4.20E-06,6.14E-09,2.37E-08,2.10E-07,3.20E-07,1.25E-05,4.83E-05,0.999917482,1.74E-09,1.04E-05,5.38E-07
3148,text-classification9,139,Experiments,,,text-classification,9,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
3149,text-classification9,140,Experimental Setting,,,text-classification,9,0,1,0,,0.000550602,0,negative,6.94E-06,0.000223775,5.37E-06,1.49E-06,4.59E-07,0.00036809,4.68E-05,0.006768399,0.000329329,0.987130295,0.005063478,5.12E-05,4.36E-06
3150,text-classification9,141,We test our model on four different data sets as listed below and summarized in .,Experimental Setting,Experimental Setting,text-classification,9,1,1,0,,0.0560195,0,negative,1.08E-05,0.000314508,0.00011547,3.03E-06,1.33E-05,0.000306678,9.26E-05,0.010672167,1.42E-05,0.98808261,0.000184207,0.000188691,1.80E-06
3151,text-classification9,142,( a ) MR 4 : Movie reviews data where the task is to classify whether the review sentence has positive or negative sentiment .,Experimental Setting,Experimental Setting,text-classification,9,2,1,0,,0.253669654,0,negative,0.000105027,0.002584805,0.038494425,7.51E-05,0.000217058,0.000803395,0.014364053,0.015562108,7.45E-05,0.685819312,0.223073307,0.018359754,0.000467182
3152,text-classification9,143,( b ) SUBJ : Subjectivity data where the task is to classify whether the sentence is subjective or objective .,Experimental Setting,Experimental Setting,text-classification,9,3,1,0,,0.475655076,0,negative,0.000142852,0.001852647,0.049388367,0.000147016,0.00144223,0.001369592,0.005189768,0.015411381,8.87E-05,0.916319119,0.003222977,0.005279989,0.000145395
3153,text-classification9,144,( c ) CR 5 : Customer reviews where,Experimental Setting,Experimental Setting,text-classification,9,4,1,0,,0.155439117,0,negative,9.29E-05,0.000543693,0.003150987,5.04E-05,7.06E-05,0.000344469,0.001031654,0.00698537,5.31E-05,0.936160524,0.047022646,0.004414698,7.89E-05
3154,text-classification9,145,The task is to classify whether the review sentence is positive or negative .,Experimental Setting,Experimental Setting,text-classification,9,5,1,0,,0.119199568,0,negative,7.08E-06,0.001703961,0.000101355,5.36E-05,1.79E-05,0.000319239,0.000155896,0.036470159,0.000236026,0.890019806,0.0708026,7.36E-05,3.87E-05
3155,text-classification9,146,( d ) TREC 6 : TREC question data set the task is to classify the type of question .,Experimental Setting,Experimental Setting,text-classification,9,6,1,0,,0.220629381,0,negative,6.50E-05,0.000606871,0.006379029,0.0002336,0.000655955,0.001288024,0.006685218,0.010352343,3.63E-05,0.951669833,0.017065527,0.004772515,0.000189784
3156,text-classification9,147,All our data sets are in English .,Experimental Setting,Experimental Setting,text-classification,9,7,1,0,,0.320677182,0,negative,2.71E-06,0.000447951,7.73E-05,2.95E-05,7.57E-05,0.000713015,0.000115666,0.030148901,4.49E-05,0.968125954,0.000183382,3.06E-05,4.46E-06
3157,text-classification9,148,"For the additional contexts , we use ten other languages , selected based on their diversity and their performance on prior experiments : Arabic , Finnish , French , Italian , Korean , Mongolian , Norwegian , Polish , Russian , and Ukranian .",Experimental Setting,Experimental Setting,text-classification,9,8,1,0,,0.625829146,1,negative,4.16E-05,0.0012509,0.000704711,0.000288067,0.001718053,0.00214607,0.000565513,0.036647527,0.000102356,0.956332326,6.95E-05,0.000116011,1.74E-05
3158,text-classification9,149,We translate the data sets using Google Translate .,Experimental Setting,Experimental Setting,text-classification,9,9,1,0,,0.864282931,1,negative,7.07E-05,0.002829387,0.000806514,0.001706633,0.001504816,0.020666954,0.001499026,0.181857905,0.000362812,0.788276336,0.000202622,0.000127792,8.85E-05
3159,text-classification9,150,Tokenization is done using the polyglot library 7 .,Experimental Setting,Experimental Setting,text-classification,9,10,1,0,,0.978313975,1,hyperparameters,6.30E-05,0.002038454,0.001260516,0.000196551,6.83E-05,0.032327419,0.001574836,0.768107953,0.000689826,0.193338176,0.000130623,0.000110944,9.35E-05
3160,text-classification9,151,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,Experimental Setting,Experimental Setting,text-classification,9,11,1,0,,0.970975594,1,hyperparameters,3.83E-05,0.004499543,0.000177759,7.90E-06,2.25E-05,0.00573715,0.000419188,0.788558162,0.000366377,0.19997438,5.15E-05,0.000127137,2.02E-05
3161,text-classification9,152,"For N = 1 , we only show the accuracy of the best classifier for conciseness .",Experimental Setting,Experimental Setting,text-classification,9,12,1,0,,0.094723871,0,negative,6.34E-06,0.000151028,3.17E-05,6.42E-07,2.83E-06,0.000187877,1.85E-05,0.021439024,2.73E-05,0.977995958,1.84E-05,0.000119649,7.37E-07
3162,text-classification9,153,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .",Experimental Setting,Experimental Setting,text-classification,9,13,1,1,hyperparameters,0.98095725,1,hyperparameters,4.82E-06,0.000514955,2.27E-05,1.07E-05,3.69E-06,0.009466629,0.000241159,0.975313693,8.43E-05,0.014299946,1.20E-05,8.52E-06,1.69E-05
3163,text-classification9,154,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .",Experimental Setting,Experimental Setting,text-classification,9,14,1,1,hyperparameters,0.978885583,1,hyperparameters,4.29E-06,0.000788608,2.81E-05,1.80E-06,2.09E-06,0.004649874,0.000108412,0.961102052,0.000322742,0.032960254,1.28E-05,9.44E-06,9.48E-06
3164,text-classification9,155,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,Experimental Setting,Experimental Setting,text-classification,9,15,1,1,hyperparameters,0.995275989,1,hyperparameters,5.29E-06,0.000369339,1.58E-05,1.83E-05,2.82E-06,0.00709759,0.000202116,0.987377434,7.63E-05,0.004799772,6.56E-06,5.59E-06,2.30E-05
3165,text-classification9,156,"We also use an l 2 constraint of 3 , following for accurate comparisons .",Experimental Setting,Experimental Setting,text-classification,9,16,1,0,,0.970800984,1,hyperparameters,1.33E-05,0.001006592,5.21E-05,2.22E-06,3.38E-06,0.005397804,0.000128514,0.9221557,0.00027149,0.070922495,8.78E-06,2.90E-05,8.59E-06
3166,text-classification9,157,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,Experimental Setting,Experimental Setting,text-classification,9,17,1,0,,0.978704326,1,hyperparameters,1.80E-06,0.000431416,1.20E-05,1.87E-06,1.24E-06,0.005273086,0.000157475,0.979197942,8.76E-05,0.014808583,9.59E-06,8.61E-06,8.74E-06
3167,text-classification9,158,"During training , we use mini-batch size of 50 .",Experimental Setting,Experimental Setting,text-classification,9,18,1,1,hyperparameters,0.993626399,1,hyperparameters,1.51E-06,0.000226086,4.43E-06,2.71E-06,9.89E-07,0.00330381,7.98E-05,0.988554159,5.13E-05,0.007754684,6.21E-06,4.89E-06,9.40E-06
3168,text-classification9,159,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,Experimental Setting,Experimental Setting,text-classification,9,19,1,1,hyperparameters,0.985290638,1,hyperparameters,8.70E-06,0.002406425,3.36E-05,1.60E-05,9.03E-06,0.003994705,0.000115199,0.959444339,0.000402607,0.033530206,9.44E-06,8.76E-06,2.10E-05
3169,text-classification9,160,We perform early stopping using a random 10 % of the training set as the development set .,Experimental Setting,Experimental Setting,text-classification,9,20,1,1,hyperparameters,0.98661418,1,hyperparameters,4.27E-06,0.00039555,8.05E-06,4.13E-06,2.10E-06,0.003486645,8.72E-05,0.982676491,8.34E-05,0.013228206,4.39E-06,8.12E-06,1.14E-05
3170,text-classification9,161,"We present several competing models , listed below to compare the performance of our model .",Experimental Setting,Experimental Setting,text-classification,9,21,1,0,,0.079192884,0,negative,9.20E-06,0.000301942,0.000533143,6.10E-06,1.77E-05,0.000298915,6.66E-05,0.013740297,0.000129917,0.984625568,9.28E-05,0.000169937,7.83E-06
3171,text-classification9,162,uses topics as additional contexts and changes the CNN architecture .,Experimental Setting,Experimental Setting,text-classification,9,22,1,0,,0.457079254,0,negative,0.001997799,0.001773787,0.083534554,6.60E-05,0.000357945,0.00102805,0.000834306,0.017680902,0.000747561,0.885982304,0.00011099,0.005781201,0.000104626
3172,text-classification9,163,TopCNN uses two types of topics : word- specific topic and sentence - specific topic ; and ( D ) CNN+ B1 and CNN +,Experimental Setting,Experimental Setting,text-classification,9,23,1,0,,0.943369136,1,baselines,0.00027034,0.006253502,0.607284216,9.34E-06,0.000128796,0.001578329,0.00435209,0.04687789,0.000975689,0.320988811,0.00021008,0.010879243,0.000191673
3173,text-classification9,164,B2 are the two baselines presented in this paper .,Experimental Setting,Experimental Setting,text-classification,9,24,1,0,,0.048116336,0,negative,0.000100794,0.003785787,0.148501935,2.93E-06,5.01E-05,0.000413988,0.00055334,0.019483998,0.001131224,0.82349896,0.000136554,0.002304846,3.56E-05
3174,text-classification9,165,We do not show results from RNN models because they were shown to be less effective in sentence classification in our prior experiments .,Experimental Setting,Experimental Setting,text-classification,9,25,1,0,,0.029453291,0,negative,9.00E-06,0.000180895,5.35E-05,4.18E-06,2.70E-05,0.000203765,5.88E-05,0.014187112,2.57E-05,0.984777944,1.06E-05,0.000458332,3.30E-06
3175,text-classification9,166,"For models with additional context , we further use an ensemble classification model using a commonly used method by averaging the class probability scores generated by the multiple variants ( in our model 's case , N = 1 and N = 10 models ) , following .",Experimental Setting,Experimental Setting,text-classification,9,26,1,0,,0.475381689,0,negative,0.000188738,0.040390836,0.079909899,6.47E-06,0.000132231,0.001124192,0.000511401,0.101182497,0.010121836,0.765708827,6.08E-05,0.000610571,5.17E-05
3176,text-classification9,167,Results and Discussion,,,text-classification,9,0,1,0,,0.005562826,0,negative,0.000289338,0.000152875,1.75E-05,1.27E-06,2.39E-06,4.28E-05,0.000328215,0.000502027,2.34E-05,0.983648035,0.005859673,0.009125785,6.73E-06
3177,text-classification9,168,We report the classification accuracy of the competing models in .,Results and Discussion,Results and Discussion,text-classification,9,1,1,0,,0.021103697,0,negative,0.004609832,1.09E-05,0.000130207,1.54E-06,4.93E-06,1.78E-05,0.000244682,6.68E-05,6.97E-06,0.931616574,1.95E-05,0.063254943,1.54E-05
3178,text-classification9,169,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,Results and Discussion,Results and Discussion,text-classification,9,2,1,1,results,0.911412406,1,results,0.011446835,6.97E-06,0.000101414,2.36E-06,1.51E-06,1.76E-05,0.002052482,8.22E-05,2.51E-06,0.046785136,3.94E-05,0.939317728,0.000143864
3179,text-classification9,170,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",Results and Discussion,Results and Discussion,text-classification,9,3,1,1,results,0.97080022,1,results,0.080845809,6.46E-06,0.000122126,3.28E-06,2.28E-06,1.69E-05,0.002463166,8.44E-05,3.05E-06,0.028119262,2.07E-05,0.88810639,0.000206139
3180,text-classification9,171,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .",Results and Discussion,Results and Discussion,text-classification,9,4,1,1,results,0.969031111,1,results,0.011936622,1.42E-06,5.76E-05,9.47E-07,5.82E-07,7.66E-06,0.002271301,3.43E-05,5.07E-07,0.017667163,1.65E-05,0.967890533,0.000114767
3181,text-classification9,172,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .",Results and Discussion,Results and Discussion,text-classification,9,5,1,1,results,0.960362672,1,results,0.018646004,2.10E-06,4.74E-05,4.79E-06,1.36E-06,1.48E-05,0.00378644,5.12E-05,9.86E-07,0.013903879,1.69E-05,0.963256,0.000268166
3182,text-classification9,173,"We emphasize that we only use the basic CNN as our sentence encoder for our experiments , yet still achieve state of the art performance :",Results and Discussion,Results and Discussion,text-classification,9,6,1,0,,0.783975908,1,results,0.070666352,3.30E-05,0.000442398,6.06E-06,7.91E-06,3.02E-05,0.002386263,0.000117348,1.21E-05,0.193981735,4.66E-05,0.732120788,0.000149218
3183,text-classification9,174,Classification accuracies of competing models .,Results and Discussion,Results and Discussion,text-classification,9,7,1,0,,0.007536338,0,negative,0.001710994,3.41E-06,0.000120439,5.42E-07,8.47E-07,1.70E-05,0.000239096,6.62E-05,6.36E-06,0.928109614,6.35E-05,0.069639626,2.24E-05
3184,text-classification9,175,"C refers to the additional context , N refers to the number of translations .",Results and Discussion,Results and Discussion,text-classification,9,8,1,0,,0.001829166,0,negative,0.001501974,5.09E-05,0.000164682,6.24E-06,3.56E-06,6.85E-05,7.08E-05,0.000859727,0.00038896,0.995063942,2.73E-05,0.001738851,5.45E-05
3185,text-classification9,176,"In TopCNN , word refers to using word - specific topic while sentence refers to using sentence - specific topic .",Results and Discussion,Results and Discussion,text-classification,9,9,1,0,,0.009689611,0,negative,0.001448088,0.000276782,0.003971145,1.75E-05,2.03E-05,0.000224272,0.000343144,0.001161861,0.000591544,0.983134812,0.000518019,0.007885335,0.000407162
3186,text-classification9,177,Accuracies colored red are accuracies that perform worse than CNN .,Results and Discussion,Results and Discussion,text-classification,9,10,1,0,,0.004219941,0,negative,0.001030026,2.04E-05,0.000183367,4.25E-06,3.50E-06,0.000189008,0.000355242,0.001087355,8.46E-05,0.985759481,3.44E-05,0.011162357,8.60E-05
3187,text-classification9,178,Previous state of the art results and the results of our best model are bold - faced .,Results and Discussion,Results and Discussion,text-classification,9,11,1,0,,0.003787128,0,negative,0.000475149,9.65E-06,7.00E-05,1.20E-05,4.67E-06,7.79E-05,0.000180141,0.000208199,3.75E-05,0.98915231,6.51E-05,0.009600061,0.000107306
3188,text-classification9,179,The winning result is underlined .,Results and Discussion,Results and Discussion,text-classification,9,12,1,0,,0.00137725,0,negative,0.000755289,1.98E-05,7.05E-05,7.04E-06,3.00E-06,8.48E-05,0.000147834,0.000437878,0.00010709,0.992985996,5.59E-05,0.005219301,0.000105553
3189,text-classification9,180,"The number inside the parenthesis indicates the increase from the base model , CNN . on most data sets .",Results and Discussion,Results and Discussion,text-classification,9,13,1,0,,0.007065513,0,negative,0.018346349,6.72E-06,9.18E-05,1.00E-05,8.53E-06,0.000107953,0.000981657,0.000458465,1.42E-05,0.857191804,1.01E-05,0.122616587,0.000155828
3190,text-classification9,181,"Hence , MCFA is successful in effectively using translations as additional context to improve the performance of the classifier .",Results and Discussion,Results and Discussion,text-classification,9,14,1,0,,0.90753562,1,results,0.089226149,8.17E-06,0.000124024,2.10E-06,3.50E-06,9.41E-06,0.001326881,3.64E-05,3.11E-06,0.079435762,8.95E-06,0.829707969,0.000107576
3191,text-classification9,182,"We compare our model ( CNN + MCFA ) and the baselines discussed above ( CNN + B1 , CNN + B2 ) .",Results and Discussion,Results and Discussion,text-classification,9,15,1,0,,0.685907838,1,negative,0.003765273,5.73E-05,0.00187901,1.54E-06,5.01E-06,5.85E-05,0.000933421,0.000283947,3.69E-05,0.939830708,1.18E-05,0.053081308,5.53E-05
3192,text-classification9,183,"On all settings , our model outperforms the baselines .",Results and Discussion,Results and Discussion,text-classification,9,16,1,0,,0.918571169,1,results,0.021452205,6.44E-06,4.10E-05,2.01E-05,4.45E-06,8.85E-05,0.010003085,0.000469837,4.89E-06,0.054836131,1.83E-05,0.911641124,0.001413911
3193,text-classification9,184,"When N = 10 , the performance of our model increases over the performance when N = 1 , however the performance of CNN + B1 decreases when compared to the performance when N = 1 .",Results and Discussion,Results and Discussion,text-classification,9,17,1,0,,0.851798988,1,results,0.090055175,3.11E-06,4.50E-05,2.34E-06,1.62E-06,1.97E-05,0.003223113,0.000107198,2.21E-06,0.04945326,9.11E-06,0.85685343,0.000224731
3194,text-classification9,185,We also show the accuracies of the worst classifiers when N = 1 in .,Results and Discussion,Results and Discussion,text-classification,9,18,1,0,,0.025212615,0,negative,0.003316282,6.38E-06,3.90E-05,8.89E-07,3.38E-06,1.50E-05,0.000172304,9.63E-05,6.45E-06,0.96171982,2.41E-06,0.03460704,1.47E-05
3195,text-classification9,186,"On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .",Results and Discussion,Results and Discussion,text-classification,9,19,1,0,,0.960041614,1,results,0.192334752,3.55E-06,3.61E-05,4.99E-06,2.32E-06,1.51E-05,0.003997428,7.16E-05,1.57E-06,0.037037421,5.80E-06,0.766209931,0.000279412
3196,text-classification9,187,"This is resolved by CNN + B2 by applying L2 regularization , however the increase in performance is marginal .",Results and Discussion,Results and Discussion,text-classification,9,20,1,0,,0.406984376,0,negative,0.373302914,9.52E-06,0.000561521,4.75E-06,5.83E-06,2.10E-05,0.000263772,6.01E-05,2.37E-05,0.536236958,4.75E-06,0.089450356,5.48E-05
3197,text-classification9,188,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .",Results and Discussion,Results and Discussion,text-classification,9,21,1,0,,0.959459994,1,negative,0.029542793,0.000463256,0.018346476,1.15E-05,5.26E-05,9.51E-05,0.002703274,0.000367957,0.000262093,0.699384203,3.71E-05,0.248394152,0.000339452
3198,text-classification9,189,"Overall , we conclude that translations are better additional contexts than topics .",Results and Discussion,Results and Discussion,text-classification,9,22,1,0,,0.92818117,1,results,0.099167271,4.29E-05,0.000115417,0.000121143,3.66E-05,0.000428125,0.011272219,0.002565591,6.23E-05,0.270068739,6.85E-05,0.61225152,0.003799633
3199,text-classification9,190,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .",Results and Discussion,Results and Discussion,text-classification,9,23,1,0,,0.938259186,1,results,0.010159925,8.72E-07,2.68E-05,7.31E-07,6.42E-07,5.64E-06,0.002369491,2.42E-05,3.32E-07,0.026497702,2.26E-06,0.960767243,0.000144228
3200,text-classification9,191,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",Results and Discussion,Results and Discussion,text-classification,9,24,1,0,,0.975612483,1,results,0.262372605,2.71E-06,4.12E-05,1.53E-06,2.21E-06,8.19E-06,0.001179784,2.99E-05,1.25E-06,0.10241451,1.93E-06,0.633861496,8.26E-05
3201,text-classification9,192,Model Interpretation,,,text-classification,9,0,1,0,,0.011938328,0,research-problem,0.000162101,9.30E-05,6.42E-05,1.37E-05,4.73E-06,6.58E-05,0.001834476,0.000479969,2.61E-05,0.482077289,0.508196605,0.006874877,0.000107118
3202,text-classification9,193,We first provide examples shown in on how the self usability module determines the score of sentences .,Model Interpretation,Model Interpretation,text-classification,9,1,1,0,,2.66E-05,0,negative,1.79E-05,1.05E-07,1.95E-07,1.55E-08,1.08E-07,1.61E-06,4.19E-06,7.93E-07,9.03E-07,0.999968944,2.21E-07,4.95E-06,1.79E-08
3203,text-classification9,194,"In the first example , it is hard to classify whether the translated sentence is positive or negative , thus it is given a low self usability score .",Model Interpretation,Model Interpretation,text-classification,9,2,1,0,,3.74E-05,0,negative,5.79E-05,2.86E-08,3.98E-07,7.99E-09,1.62E-07,7.72E-07,6.87E-06,2.58E-07,1.07E-07,0.999914022,2.42E-07,1.92E-05,1.79E-08
3204,text-classification9,195,"In the second example , although the sentence contains mistranslations , these are minimal and may actually help the classifier by telling it that thirst for violence is not a attention ( negative sentence ) the mothman prophecies , which is mostly a bore , seems to exist only for its climactic setpiece . negative phrase .",Model Interpretation,Model Interpretation,text-classification,9,3,1,0,,3.40E-05,0,negative,1.66E-05,2.61E-08,4.31E-07,8.86E-08,3.43E-07,5.49E-06,1.24E-05,6.33E-07,2.55E-07,0.999958354,4.36E-07,4.80E-06,8.28E-08
3205,text-classification9,196,"Thus , it is given a high self usability score .",Model Interpretation,Model Interpretation,text-classification,9,4,1,0,,5.30E-05,0,negative,5.50E-05,1.80E-07,1.38E-06,9.98E-10,2.25E-08,8.30E-07,4.66E-06,8.49E-07,2.76E-06,0.999916377,1.20E-07,1.78E-05,1.09E-08
3206,text-classification9,197,shows two data instance examples where we show the attention weights given to the other contexts when fixing a Korean sentence .,Model Interpretation,Model Interpretation,text-classification,9,5,1,0,,0.000115587,0,negative,4.37E-05,7.72E-08,2.66E-06,6.29E-08,1.54E-06,2.80E-06,2.49E-05,6.31E-07,2.84E-07,0.999901856,2.03E-07,2.12E-05,1.13E-07
3207,text-classification9,198,"The larger the attention weight is , the more the context is used to fix the Korean sentence .",Model Interpretation,Model Interpretation,text-classification,9,6,1,0,,0.013893096,0,negative,0.000775419,6.90E-07,1.02E-06,4.94E-08,2.27E-07,5.67E-06,3.59E-05,1.11E-05,3.33E-06,0.999061255,7.60E-07,0.000104497,1.52E-07
3208,text-classification9,199,In the Original sentence : skip this turd and pick your nose instead because you 're sure to get more out of the latter experience .,Model Interpretation,Model Interpretation,text-classification,9,7,1,0,,1.07E-05,0,negative,1.04E-05,3.49E-08,2.25E-07,1.91E-07,1.97E-07,7.94E-06,6.24E-06,1.09E-06,5.86E-07,0.999970633,4.06E-07,1.98E-06,8.83E-08
3209,text-classification9,200,Korean translation :,Model Interpretation,Model Interpretation,text-classification,9,8,1,0,,0.006177486,0,negative,0.000135474,6.50E-07,5.90E-06,2.49E-07,1.92E-06,1.16E-05,0.000192394,4.49E-06,3.41E-06,0.999561474,1.31E-05,6.78E-05,1.63E-06
3210,text-classification9,201,Human re-translation :,Model Interpretation,Model Interpretation,text-classification,9,9,1,0,,0.059229119,0,negative,0.009892531,2.12E-05,0.000801325,2.01E-06,1.14E-05,4.47E-05,0.10636593,2.15E-05,5.25E-06,0.749855215,0.001607607,0.130988636,0.000382668
3211,text-classification9,202,"In order to get more from the latter experience , you need to skip this puddle and choose your nose .",Model Interpretation,Model Interpretation,text-classification,9,10,1,0,,1.77E-05,0,negative,3.12E-05,9.62E-08,2.24E-07,5.76E-08,1.50E-07,3.64E-06,8.01E-06,1.68E-06,1.17E-06,0.999944635,5.90E-07,8.47E-06,9.71E-08
3212,text-classification9,203,Self,Model Interpretation,,text-classification,9,11,1,0,,0.000854801,0,negative,9.96E-05,1.22E-06,1.97E-05,1.24E-07,3.77E-07,1.96E-05,0.00027799,1.41E-05,6.19E-05,0.999349832,5.80E-06,0.000144932,4.78E-06
3213,text-classification9,204,Usability : 0.3958 ( a ) Low self usability example Original sentence : michael moore 's latest documentary about america 's thirst for violence is his best film yet . . .,Model Interpretation,Self,text-classification,9,12,1,0,,4.61E-06,0,negative,0.00017874,2.37E-07,6.95E-06,6.76E-07,1.07E-07,7.49E-06,5.63E-05,2.13E-05,1.92E-06,0.997583355,8.12E-06,0.002128015,6.88E-06
3214,text-classification9,205,Korean translation :,Model Interpretation,Self,text-classification,9,13,1,0,,0.000203509,0,negative,7.99E-05,3.80E-07,1.56E-05,2.26E-07,2.26E-07,2.47E-06,2.15E-05,7.77E-06,2.88E-06,0.999466636,7.17E-06,0.000392373,2.94E-06
3215,text-classification9,206,Human re-translation :,Model Interpretation,Self,text-classification,9,14,1,0,,0.002875529,0,results,0.001793362,5.18E-06,0.001149004,9.95E-07,1.06E-06,7.32E-06,0.004825876,1.68E-05,2.01E-06,0.480247305,0.000617901,0.511122853,0.000210332
3216,text-classification9,207,"Michael Moore 's latest American documentary "" Violent Scene "" is his best film yet . . .",Model Interpretation,Self,text-classification,9,15,1,0,,4.67E-06,0,negative,1.42E-05,8.04E-08,2.34E-06,1.69E-07,5.14E-08,1.26E-06,4.63E-06,3.09E-06,1.49E-06,0.999897644,1.77E-06,7.19E-05,1.37E-06
3217,text-classification9,208,Self,Model Interpretation,,text-classification,9,16,1,0,,0.000812775,0,negative,9.24E-05,1.05E-06,1.71E-05,1.03E-07,3.94E-07,1.64E-05,0.000278137,1.22E-05,5.54E-05,0.999381117,3.07E-06,0.000137421,5.15E-06
3218,text-classification9,209,Usability : 1.0000 ( b ) High self usability example you know that ten bucks you 'd spend on a ticket ?,Model Interpretation,Self,text-classification,9,17,1,0,,1.47E-05,0,negative,6.66E-05,1.57E-07,3.11E-06,3.10E-07,8.12E-08,5.78E-06,3.76E-05,2.05E-05,1.06E-06,0.998335192,2.28E-06,0.001521909,5.42E-06
3219,text-classification9,210,just send it to cranky .,Model Interpretation,Self,text-classification,9,18,1,0,,3.88E-07,0,negative,8.83E-06,3.72E-08,1.02E-06,1.49E-08,1.00E-08,8.06E-07,1.11E-06,2.99E-06,7.69E-07,0.999963651,7.75E-08,2.06E-05,1.29E-07
3220,text-classification9,211,we do n't get paid enough to sit through crap like this .,Model Interpretation,Self,text-classification,9,19,1,0,,1.02E-06,0,negative,1.04E-05,2.32E-08,5.40E-07,2.91E-08,1.74E-08,6.33E-07,1.03E-06,1.53E-06,4.24E-07,0.999956537,7.10E-08,2.87E-05,1.29E-07
3221,text-classification9,212,NN ( altered ),Model Interpretation,Self,text-classification,9,20,1,0,,1.34E-05,0,negative,3.75E-06,6.39E-08,4.80E-06,3.39E-09,4.14E-09,4.27E-07,2.19E-06,3.50E-06,2.33E-06,0.999955676,1.97E-07,2.69E-05,1.39E-07
3222,text-classification9,213,"after scenes of nonsense , you 'll be wistful for the testosteronecharged wizardry of jerry bruckheimer productions , especially because half past dead is like the rock on walmart budget . :",Model Interpretation,Self,text-classification,9,21,1,0,,4.80E-07,0,negative,8.26E-06,2.37E-08,5.99E-07,1.46E-08,1.17E-08,4.24E-07,8.62E-07,1.61E-06,6.25E-07,0.999962894,5.33E-08,2.45E-05,1.15E-07
3223,text-classification9,214,"Two example sentences , from English ( first ) and Korean ( second ) vector spaces , and their nearest neighbors ( NN ) on both the unaltered and altered vector spaces .",Model Interpretation,Self,text-classification,9,22,1,0,,3.18E-06,0,negative,4.62E-06,5.87E-08,8.17E-06,3.02E-09,3.93E-08,2.57E-07,4.50E-06,1.02E-06,3.19E-07,0.999777381,1.13E-07,0.000203333,1.89E-07
3224,text-classification9,215,We only show the original English sentences for the Korean example for conciseness .,Model Interpretation,Self,text-classification,9,23,1,0,,1.48E-05,0,negative,2.46E-06,3.84E-08,2.69E-07,3.09E-09,1.41E-08,2.18E-07,7.17E-07,2.09E-06,2.17E-07,0.999960324,2.01E-08,3.36E-05,3.91E-08
3225,text-classification9,216,"first example , the Korean sentence contains translation errors ; especially , the words bore and climactic setpiece were not translated and were only spelled using the Korean alphabet .",Model Interpretation,Self,text-classification,9,24,1,0,,2.87E-06,0,negative,1.16E-05,1.60E-08,3.73E-07,8.40E-09,2.31E-08,2.03E-07,1.00E-06,7.04E-07,9.27E-08,0.999917478,8.04E-08,6.83E-05,8.28E-08
3226,text-classification9,217,"In this example , the English attention weight is larger than the Korean attention weight .",Model Interpretation,Self,text-classification,9,25,1,0,,1.74E-06,0,negative,1.58E-05,5.75E-08,3.82E-06,1.42E-09,9.27E-09,1.79E-07,1.31E-06,1.75E-06,7.65E-07,0.999897688,1.42E-08,7.86E-05,4.20E-08
3227,text-classification9,218,"In the second example , the Korean sentence correctly translates all parts of the English sentence , except for the phrase as it does in trouble .",Model Interpretation,Self,text-classification,9,26,1,0,,8.24E-06,0,negative,0.00014822,6.54E-08,4.14E-06,7.32E-09,6.96E-08,3.06E-07,3.09E-05,1.39E-06,1.70E-07,0.987516494,2.06E-07,0.012297444,5.57E-07
3228,text-classification9,219,"However , this phrase is not necessary to classify the sentence correctly , and may induce possible vagueness because of the word trouble .",Model Interpretation,Self,text-classification,9,27,1,0,,7.24E-07,0,negative,2.34E-05,2.40E-08,3.60E-07,3.12E-08,2.98E-08,3.25E-07,5.20E-07,1.25E-06,3.94E-07,0.999936758,3.12E-08,3.68E-05,1.01E-07
3229,text-classification9,220,"Thus , the Korean attention weight is larger .",Model Interpretation,Self,text-classification,9,28,1,0,,3.21E-05,0,negative,0.000173536,4.28E-07,2.35E-06,5.27E-08,7.95E-08,1.14E-06,3.92E-06,3.09E-05,5.82E-06,0.999624776,5.92E-08,0.000156281,6.84E-07
3230,text-classification9,221,shows the PCA visualization of the unaltered and the altered vectors of four different languages .,Model Interpretation,Self,text-classification,9,29,1,0,,4.20E-05,0,negative,1.61E-05,8.51E-08,4.85E-06,9.92E-09,1.18E-08,7.39E-07,7.30E-06,5.08E-06,2.04E-06,0.999730006,1.48E-07,0.000232807,8.52E-07
3231,text-classification9,222,"In the first example , the unaltered sentence vectors are mostly in the middle of the vector space , making it hard to draw a boundary between the two examples .",Model Interpretation,Self,text-classification,9,30,1,0,,3.59E-05,0,negative,0.000582628,5.02E-08,2.90E-06,6.86E-09,6.91E-08,2.67E-07,6.77E-06,1.15E-06,1.65E-07,0.996046128,1.50E-08,0.003359688,1.64E-07
3232,text-classification9,223,"After the fixing , the boundary is much clearer .",Model Interpretation,Self,text-classification,9,31,1,0,,9.35E-06,0,negative,0.000153467,1.40E-07,2.03E-06,1.54E-08,2.96E-08,4.04E-07,4.48E-06,4.00E-06,2.08E-06,0.999422544,7.63E-08,0.000410227,5.01E-07
3233,text-classification9,224,We also show the English sentence vectors in the second example .,Model Interpretation,Self,text-classification,9,32,1,0,,6.73E-07,0,negative,1.54E-06,8.75E-09,4.30E-07,6.77E-10,4.06E-09,1.24E-07,6.89E-07,6.65E-07,1.38E-07,0.999969574,4.22E-09,2.68E-05,2.69E-08
3234,text-classification9,225,"Even without fixing the unaltered English sentence vectors , it is easy to distinguish both classes .",Model Interpretation,Self,text-classification,9,33,1,0,,9.05E-07,0,negative,0.000159007,3.40E-08,1.45E-06,8.22E-10,4.74E-09,3.84E-08,8.77E-07,3.06E-07,2.26E-07,0.998878685,2.85E-08,0.000959305,3.36E-08
3235,text-classification9,226,"After the fix , the sentence vectors in the middle of the space are moved , making the distinction more obvious and clearer .",Model Interpretation,Self,text-classification,9,34,1,0,,5.58E-06,0,negative,3.23E-05,2.31E-07,7.27E-06,2.25E-09,1.12E-08,1.43E-07,8.25E-07,2.05E-06,9.09E-06,0.999920072,9.12E-09,2.79E-05,8.84E-08
3236,text-classification9,227,We also provide quantitative evidence by showing that the Mahalanobis distance between the two classes in the altered vectors are significantly farther than that of the unaltered vectors .,Model Interpretation,Self,text-classification,9,35,1,0,,0.000207083,0,negative,0.000984286,1.09E-06,3.20E-06,3.16E-08,1.20E-07,4.06E-07,1.02E-05,6.23E-06,3.71E-06,0.995048772,8.29E-08,0.003941319,5.86E-07
3237,text-classification9,228,We also show two examples sentences from English and Korean vector spaces and their corresponding nearest neighbors on both the unaltered and altered vector spaces in Table 5 .,Model Interpretation,Self,text-classification,9,36,1,0,,2.18E-05,0,negative,9.81E-06,1.92E-08,1.35E-06,2.31E-09,8.50E-08,1.16E-07,2.52E-06,4.41E-07,5.20E-08,0.999652739,3.08E-09,0.000332792,6.26E-08
3238,text-classification9,229,"In the first example , the unaltered vector focuses on the meaning of "" wasted yours "" in the sentence , which puts it near sentences regarding wasted time or money .",Model Interpretation,Self,text-classification,9,37,1,0,,4.90E-06,0,negative,2.25E-05,2.48E-08,4.28E-06,1.27E-09,3.69E-08,1.09E-07,1.59E-06,4.09E-07,2.02E-07,0.999680629,3.87E-09,0.000290139,6.14E-08
3239,text-classification9,230,"After fixing , the sentence vector focuses its meaning on the slow yet worth - the - wait pace of the movie , thus moving it closer to the correct vectors .",Model Interpretation,Self,text-classification,9,38,1,0,,5.14E-06,0,negative,4.15E-05,2.65E-07,1.75E-05,1.22E-09,1.95E-08,9.89E-08,1.15E-06,1.12E-06,8.76E-06,0.999888439,2.56E-09,4.10E-05,5.28E-08
3240,text-classification9,231,"In the second example , all three sentences have highly descriptive tones , however , the nearest neighbor on the altered space is hyperbolically negative , comparing the movie to a description unrelated to the movie itself .",Model Interpretation,Self,text-classification,9,39,1,0,,2.46E-06,0,negative,6.47E-05,3.35E-08,2.93E-06,2.72E-09,5.49E-08,1.03E-07,1.80E-06,4.21E-07,1.77E-07,0.999375894,4.67E-09,0.000553775,6.73E-08
3241,text-classification9,232,NN ( Unaltered ),Model Interpretation,Self,text-classification,9,40,1,0,,3.13E-06,0,negative,4.08E-06,5.62E-08,1.12E-05,2.18E-09,6.10E-09,2.28E-07,2.56E-06,2.00E-06,2.90E-06,0.999942035,4.29E-08,3.46E-05,2.63E-07
3242,text-classification9,233,"in the new release of cinema paradiso , the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .",Model Interpretation,Self,text-classification,9,41,1,0,,1.82E-06,0,negative,1.82E-05,3.69E-08,6.61E-07,4.98E-07,1.57E-07,9.98E-07,1.64E-06,1.68E-06,7.34E-07,0.999940484,4.45E-08,3.37E-05,1.17E-06
3243,text-classification9,234,Related Work,,,text-classification,9,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
3244,text-classification9,248,Conclusion,,,text-classification,9,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
3245,natural_language_inference18,1,title,,,natural_language_inference,18,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
3246,natural_language_inference18,2,Neural Variational Inference for Text Processing Phil Blunsom 12,title,,natural_language_inference,18,1,1,1,research-problem,0.97630615,1,research-problem,2.11E-08,8.33E-06,5.21E-08,8.14E-08,3.47E-08,2.63E-07,7.53E-07,2.83E-06,3.14E-06,0.004306117,0.995678265,6.25E-08,4.31E-08
3247,natural_language_inference18,3,abstract,,,natural_language_inference,18,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
3248,natural_language_inference18,4,Recent advances in neural variational inference have spawned a renaissance in deep latent variable models .,abstract,abstract,natural_language_inference,18,1,1,0,,0.428185135,0,research-problem,4.37E-08,2.00E-05,2.74E-08,2.39E-06,4.62E-07,1.24E-06,5.29E-07,5.78E-06,2.40E-06,0.042453207,0.957513826,2.36E-08,7.02E-08
3249,natural_language_inference18,5,In this paper we introduce a generic variational inference framework for generative and conditional models of text .,abstract,abstract,natural_language_inference,18,2,1,0,,0.762088837,1,research-problem,1.14E-05,0.125238718,3.05E-05,4.04E-05,0.000213832,2.34E-05,1.62E-05,0.000354084,0.005466679,0.119795505,0.74879871,6.53E-06,4.02E-06
3250,natural_language_inference18,6,"While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables , here we construct an inference network conditioned on the discrete text input to provide the variational distribution .",abstract,abstract,natural_language_inference,18,3,1,0,,0.232738992,0,approach,3.16E-05,0.442237506,6.77E-05,2.62E-05,0.000389285,3.50E-05,1.17E-05,0.000690743,0.032850035,0.311235228,0.212413927,8.53E-06,2.67E-06
3251,natural_language_inference18,7,"We validate this framework on two very different text modelling applications , generative document modelling and supervised question answering .",abstract,abstract,natural_language_inference,18,4,1,0,,0.012679256,0,negative,9.02E-06,0.107174801,2.49E-05,2.50E-05,0.000386034,7.06E-05,2.77E-05,0.000997089,0.002250937,0.609729485,0.279286444,1.51E-05,2.77E-06
3252,natural_language_inference18,8,Our neural variational document model combines a continuous stochastic document representation with a bagof - words generative model and achieves the lowest reported perplexities on two standard test corpora .,abstract,abstract,natural_language_inference,18,5,1,0,,0.047103398,0,research-problem,3.92E-05,0.039879274,0.000138235,9.85E-06,7.95E-05,3.10E-05,5.30E-05,0.000562244,0.003554286,0.163024343,0.792517299,0.000106251,5.57E-06
3253,natural_language_inference18,9,The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair .,abstract,abstract,natural_language_inference,18,6,1,0,,0.388852415,0,negative,2.00E-05,0.073245353,0.001023925,1.14E-05,0.000101622,8.94E-05,2.07E-05,0.000707357,0.144218101,0.480198583,0.300356695,3.27E-06,3.64E-06
3254,natural_language_inference18,10,On two question answering benchmarks this model exceeds all previous published benchmarks .,abstract,abstract,natural_language_inference,18,7,1,0,,0.076575152,0,research-problem,2.17E-05,0.001956982,9.36E-06,6.27E-06,2.52E-05,1.78E-05,0.000130644,0.000302743,0.000100535,0.17357474,0.823492601,0.00035527,6.21E-06
3255,natural_language_inference18,11,Introduction,,,natural_language_inference,18,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
3256,natural_language_inference18,12,Probabilistic generative models underpin many successful applications within the field of natural language processing ( NLP ) .,Introduction,Introduction,natural_language_inference,18,1,1,0,,0.83715523,1,research-problem,4.84E-07,0.000157918,3.33E-07,1.28E-06,1.94E-06,3.33E-06,6.97E-06,5.73E-06,0.000197846,0.03256457,0.967058069,7.33E-07,8.03E-07
3257,natural_language_inference18,13,"Their popularity stems from their ability to use unlabelled data effectively , to incorporate abundant linguistic features , and to learn interpretable dependencies among data .",Introduction,Introduction,natural_language_inference,18,2,1,0,,0.004041023,0,negative,1.10E-05,0.002360046,4.72E-06,0.000262331,0.000508949,0.000286957,4.79E-05,0.000149577,0.001137897,0.699252423,0.295965511,4.43E-06,8.19E-06
3258,natural_language_inference18,14,"However these successes are tempered by the fact that as the structure of such generative models becomes deeper and more complex , true Bayesian inference becomes intractable due to the high dimensional integrals required .",Introduction,Introduction,natural_language_inference,18,3,1,0,,0.005080647,0,research-problem,5.25E-06,0.001201336,1.24E-06,3.63E-05,5.67E-05,9.45E-05,2.31E-05,8.29E-05,0.001092367,0.497219216,0.500180752,2.83E-06,3.46E-06
3259,natural_language_inference18,15,Markov chain Monte Carlo ( MCMC ),Introduction,Introduction,natural_language_inference,18,4,1,0,,0.874358855,1,negative,1.50E-05,0.025896054,6.51E-05,1.78E-05,9.06E-05,0.000579503,0.000268103,0.000955599,0.141749827,0.420769126,0.409551981,1.93E-05,2.20E-05
3260,natural_language_inference18,16,"Proceedings of the 33 rd International Conference on Machine Learning , New York , NY , USA , 2016 .",Introduction,Introduction,natural_language_inference,18,5,1,0,,0.043899123,0,research-problem,3.82E-05,0.004503816,1.53E-05,0.000312602,0.000970708,0.000165196,0.000194175,0.000118438,0.002489761,0.42331116,0.567824289,3.24E-05,2.40E-05
3261,natural_language_inference18,17,JMLR : W&CP volume,Introduction,Introduction,natural_language_inference,18,6,1,0,,0.043542302,0,research-problem,0.000102132,0.021720442,9.15E-05,1.25E-05,0.000212163,4.86E-05,0.000445246,9.22E-05,0.015974749,0.294653485,0.666436581,0.000190034,2.03E-05
3262,natural_language_inference18,18,48 . Copyright 2016 by the author ( s ) .,Introduction,Introduction,natural_language_inference,18,7,1,0,,0.010480431,0,negative,1.38E-05,0.00413866,3.97E-06,4.06E-05,0.000277494,0.000391189,4.55E-05,0.000405697,0.012117783,0.968117239,0.01444018,4.55E-06,3.40E-06
3263,natural_language_inference18,19,and variational inference are the standard approaches for approximating these integrals .,Introduction,Introduction,natural_language_inference,18,8,1,0,,0.022931248,0,negative,7.86E-06,0.015347651,8.84E-06,4.10E-05,0.000148541,0.000263002,9.20E-05,0.000519614,0.015174401,0.486161316,0.482215615,8.53E-06,1.16E-05
3264,natural_language_inference18,20,"However the computational cost of the former results in impractical training for the large and deep neural networks which are now fashionable , and the latter is conventionally confined due to the underestimation of posterior variance .",Introduction,Introduction,natural_language_inference,18,9,1,0,,0.01079354,0,negative,1.75E-05,0.001585647,2.96E-06,0.000151409,0.000486847,0.000113307,3.07E-05,6.39E-05,0.000644329,0.903574489,0.093318854,5.39E-06,4.61E-06
3265,natural_language_inference18,21,"The lack of effective and efficient inference methods hinders our ability to create highly expressive models of text , especially in the situation where the model is non-conjugate .",Introduction,Introduction,natural_language_inference,18,10,1,0,,0.501514547,1,research-problem,3.07E-06,0.000729014,6.43E-07,2.34E-05,6.60E-05,2.45E-05,2.37E-05,2.67E-05,0.000146825,0.316749266,0.682200667,3.41E-06,2.81E-06
3266,natural_language_inference18,22,"This paper introduces a neural variational framework for generative models of text , inspired by the variational autoencoder .",Introduction,Introduction,natural_language_inference,18,11,1,1,model,0.972990072,1,approach,8.81E-05,0.4654774,0.000364633,1.90E-05,0.001192363,6.99E-05,0.000154882,0.000149884,0.455426643,0.037906624,0.039092933,4.05E-05,1.72E-05
3267,natural_language_inference18,23,"The principle idea is to build an inference network , implemented by a deep neural network conditioned on text , to approximate the intractable distributions over the latent variables .",Introduction,Introduction,natural_language_inference,18,12,1,1,model,0.869387296,1,model,3.86E-05,0.210822839,9.24E-05,8.29E-06,0.000883859,4.53E-05,1.49E-05,7.86E-05,0.724029685,0.062554476,0.001423445,5.36E-06,2.28E-06
3268,natural_language_inference18,24,"Instead of providing an analytic approximation , as in traditional variational Bayes , neural variational inference learns to model the posterior probability , thus endowing the model with strong generalis ation abilities .",Introduction,Introduction,natural_language_inference,18,13,1,1,model,0.846207501,1,model,0.000108382,0.402443835,0.000214971,1.31E-05,0.001471505,5.49E-05,4.52E-05,0.000129035,0.461670424,0.123377149,0.010443956,2.28E-05,4.81E-06
3269,natural_language_inference18,25,"Due to the flexibility of deep neural networks , the inference network is capable of learning complicated non-linear distributions and processing structured inputs such as word sequences .",Introduction,Introduction,natural_language_inference,18,14,1,0,,0.857668298,1,model,0.000115261,0.286457453,9.37E-05,1.67E-05,0.001523375,6.81E-05,3.43E-05,0.000151632,0.501984212,0.204104961,0.005422534,2.36E-05,4.13E-06
3270,natural_language_inference18,26,"Inference networks can be designed as , but not restricted to , multilayer perceptrons ( MLP ) , convolutional neural networks ( CNN ) , and recurrent neural networks ( RNN ) , approaches which are rarely used in conventional generative models .",Introduction,Introduction,natural_language_inference,18,15,1,0,,0.673150529,1,negative,1.85E-05,0.031637015,3.73E-05,4.45E-05,0.00027686,0.000344481,8.24E-05,0.000506282,0.159096649,0.699724758,0.108208285,9.71E-06,1.34E-05
3271,natural_language_inference18,27,"By using the reparameteris ation method , the inference network is trained through back - propagating unbiased and low variance gradients w.r.t. the latent variables .",Introduction,Introduction,natural_language_inference,18,16,1,1,model,0.953371818,1,model,2.62E-05,0.209166672,4.74E-05,1.67E-06,0.000275307,2.48E-05,1.44E-05,8.91E-05,0.779495138,0.010656647,0.000198346,3.01E-06,1.31E-06
3272,natural_language_inference18,28,"Within this framework , we propose a Neural Variational Document Model ( NVDM ) for document modelling and a Neural Answer Selection Model ( NASM ) for question answering , a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences .",Introduction,Introduction,natural_language_inference,18,17,1,1,model,0.968972986,1,model,1.16E-05,0.141427481,0.000113638,9.86E-07,0.000124879,1.69E-05,2.64E-05,3.75E-05,0.848032177,0.007919706,0.002280519,5.60E-06,2.63E-06
3273,natural_language_inference18,29,The NVDM is an unsupervised generative model of text which aims to extract a continuous semantic latent variable for each document .,Introduction,Introduction,natural_language_inference,18,18,1,0,,0.95196481,1,model,1.74E-05,0.172041502,0.000539541,5.62E-06,0.000307838,7.42E-05,8.15E-05,0.000122532,0.779890934,0.036435268,0.010462514,1.01E-05,1.11E-05
3274,natural_language_inference18,30,"This model can be interpreted as a variational auto - encoder : an MLP encoder ( inference network ) compresses the bag - of - words document representation into a continuous latent distribution , and a softmax decoder ( generative model ) reconstructs the document by generating the words independently .",Introduction,Introduction,natural_language_inference,18,19,1,0,,0.945956099,1,model,3.34E-06,0.017871432,3.64E-05,2.61E-07,2.41E-05,8.51E-06,5.38E-06,1.66E-05,0.974118176,0.007581702,0.000332456,9.42E-07,6.73E-07
3275,natural_language_inference18,31,A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector .,Introduction,Introduction,natural_language_inference,18,20,1,1,model,0.5909493,1,negative,0.000151644,0.365198053,0.000219413,0.000103874,0.002611303,0.000273755,0.000174418,0.00043748,0.141218913,0.438512748,0.051017115,5.86E-05,2.27E-05
3276,natural_language_inference18,32,Our experiments demonstrate that our neural document model achieves the stateof - the - art perplexities on the 20 New s Groups and RCV1 - v 2 .,Introduction,Introduction,natural_language_inference,18,21,1,0,,0.044718611,0,negative,0.000934714,0.037029087,4.16E-05,1.53E-05,0.001307206,0.000283872,0.010417721,0.000832113,0.005059691,0.922641663,0.008756949,0.012579359,0.00010072
3277,natural_language_inference18,33,The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,Introduction,Introduction,natural_language_inference,18,22,1,1,model,0.971660923,1,model,0.000226966,0.256614377,0.01476413,6.53E-05,0.008093971,0.00052689,0.001289515,0.000257797,0.587887647,0.120821094,0.009223055,0.000129768,9.95E-05
3278,natural_language_inference18,34,The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution .,Introduction,Introduction,natural_language_inference,18,23,1,1,model,0.905761315,1,model,3.53E-06,0.027552101,1.56E-05,3.18E-07,3.48E-05,1.32E-05,6.27E-06,4.39E-05,0.965370545,0.006861116,9.76E-05,5.01E-07,6.01E-07
3279,natural_language_inference18,35,"This mechanism allows the model to deal with the ambiguity inherent in the task and learns pair- specific representations thatare more effective at predicting answer matches , rather than independent embeddings of question and answer sentences .",Introduction,Introduction,natural_language_inference,18,24,1,0,,0.558614663,1,model,7.43E-06,0.046982355,3.17E-05,5.03E-07,8.97E-05,1.32E-05,7.11E-06,2.94E-05,0.932753612,0.01996878,0.000114335,1.27E-06,6.05E-07
3280,natural_language_inference18,36,"Bayesian inference provides a natural safeguard against overfitting , especially as the training sets available for this task are small .",Introduction,Introduction,natural_language_inference,18,25,1,1,model,0.055559847,0,negative,0.000435107,0.046504308,3.40E-05,0.000186537,0.004335748,0.000386784,0.000584758,0.000497735,0.009804855,0.925631291,0.011313807,0.000248859,3.62E-05
3281,natural_language_inference18,37,"The experiments show that the LSTM with a latent stochastic attention mechanism learns an effective attention model and outperforms both previously published results , and our own strong nonstochastic attention baselines .",Introduction,Introduction,natural_language_inference,18,26,1,0,,0.042356876,0,negative,0.003614719,0.19247656,0.000113279,4.01E-05,0.003921045,0.000500436,0.007394182,0.001498826,0.029789031,0.753097635,0.00226327,0.005191511,9.94E-05
3282,natural_language_inference18,38,"In summary , we demonstrate the effectiveness of neural variational inference for text processing on two diverse tasks .",Introduction,Introduction,natural_language_inference,18,27,1,0,,0.130814843,0,negative,0.000192841,0.228976823,6.21E-05,0.000223681,0.005502461,0.001173769,0.003466696,0.002058995,0.037927578,0.687048739,0.03267168,0.000546874,0.000147731
3283,natural_language_inference18,39,"These models are simple , expressive and can be trained efficiently with the highly scalable stochastic gradient back - propagation .",Introduction,Introduction,natural_language_inference,18,28,1,0,,0.885037664,1,approach,9.03E-05,0.417859411,0.000296927,2.91E-05,0.003379177,0.000278323,0.000324568,0.000447811,0.412504875,0.162879111,0.001824099,6.55E-05,2.08E-05
3284,natural_language_inference18,40,"Our neural variational framework is suitable for both unsupervised and supervised learning tasks , and can be generalised to incorporate any type of neural networks .",Introduction,Introduction,natural_language_inference,18,29,1,0,,0.799327549,1,model,2.18E-05,0.264321693,3.46E-05,1.65E-05,0.000672625,0.000164983,7.80E-05,0.000588047,0.66727853,0.065741543,0.001061428,1.12E-05,9.13E-06
3285,natural_language_inference18,41,Neural Variational Inference Framework,Introduction,,natural_language_inference,18,30,1,0,,0.053677448,0,model,1.24E-05,0.038272623,5.36E-05,8.69E-06,9.68E-05,0.000267409,0.000545744,0.000654937,0.513133581,0.34537868,0.101496318,4.22E-05,3.70E-05
3286,natural_language_inference18,42,"Latent variable modelling is popular in many NLP problems , but it is non-trivial to carry out effective and efficient inference for models with complex and deep structure .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,31,1,0,,0.010227931,0,negative,6.01E-06,2.74E-05,2.70E-06,4.11E-06,9.86E-07,5.08E-05,4.05E-05,0.000178402,2.44E-05,0.994850943,0.004696692,0.000111754,5.39E-06
3287,natural_language_inference18,43,In this section we introduce a generic neural variational inference framework that we apply to both the unsupervised NVDM and supervised NASM in the follow sections .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,32,1,0,,0.000271484,0,negative,5.01E-05,0.003114883,5.52E-05,1.28E-06,1.94E-05,2.38E-05,1.41E-05,0.000229236,0.003540287,0.992694556,2.11E-05,0.000234366,1.61E-06
3288,natural_language_inference18,44,"We define a generative model with a latent variable h , which can be considered as the stochastic units in deep neural networks .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,33,1,0,,4.45E-06,0,negative,2.75E-05,0.000796296,3.94E-05,2.45E-07,4.02E-06,9.43E-06,4.27E-06,0.000150963,0.00752975,0.991390494,4.10E-06,4.29E-05,5.32E-07
3289,natural_language_inference18,45,We designate the observed parent and child nodes of h as x and y respectively .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,34,1,0,,7.01E-06,0,negative,1.65E-07,4.93E-06,2.01E-07,1.29E-08,7.51E-08,4.51E-06,5.01E-07,5.53E-05,5.23E-05,0.999879862,2.29E-07,1.93E-06,2.00E-08
3290,natural_language_inference18,46,"Hence , the joint distribution of the generative model is p ? ( x , y ) = hp ? ( y|h ) p ? ( h|x ) p ( x ) , and the variational lower bound L is derived as :",Introduction,Neural Variational Inference Framework,natural_language_inference,18,35,1,0,,2.65E-06,0,negative,4.65E-06,3.14E-06,3.88E-06,7.18E-09,1.20E-07,8.92E-07,5.07E-07,5.07E-06,1.56E-05,0.999931224,2.12E-07,3.47E-05,1.85E-08
3291,natural_language_inference18,47,where ?,Introduction,Neural Variational Inference Framework,natural_language_inference,18,36,1,0,,3.45E-07,0,negative,4.13E-07,9.36E-07,1.70E-07,2.85E-08,4.69E-08,3.07E-06,3.92E-07,1.71E-05,1.16E-05,0.999962991,3.15E-07,2.91E-06,2.10E-08
3292,natural_language_inference18,48,parameterises the generative distributions p ? ( y|h ) and p ? ( h|x ) .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,37,1,0,,7.05E-06,0,negative,3.62E-06,1.83E-05,2.01E-06,1.54E-07,5.23E-07,2.40E-05,2.95E-06,0.00036926,8.91E-05,0.999481499,1.80E-07,8.37E-06,1.48E-07
3293,natural_language_inference18,49,"In order to have a tight lower bound , the variational distribution q ( h ) should approach the true posterior p ( h|x , y ) .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,38,1,0,,3.57E-05,0,negative,4.74E-06,6.18E-06,4.54E-07,5.39E-08,2.76E-07,5.09E-06,1.62E-06,7.69E-05,2.49E-05,0.999842546,4.72E-07,3.67E-05,9.96E-08
3294,natural_language_inference18,50,"Here , we employ a parameterised diagonal Gaussian N ( h| ( x , y ) , diag ( ?",Introduction,Neural Variational Inference Framework,natural_language_inference,18,39,1,0,,1.52E-05,0,negative,2.59E-05,2.11E-05,2.14E-05,1.70E-08,4.85E-07,5.27E-06,3.39E-06,3.49E-05,9.35E-05,0.999742135,8.53E-08,5.17E-05,6.15E-08
3295,natural_language_inference18,51,"2 ( x , y ) ) ) as q ? ( h|x , y ) .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,40,1,0,,9.08E-06,0,negative,4.14E-05,6.70E-06,7.15E-06,3.66E-08,6.29E-07,2.58E-06,2.63E-06,1.37E-05,1.70E-05,0.999637106,2.11E-07,0.00027078,7.13E-08
3296,natural_language_inference18,52,The three steps to construct the inference network are :,Introduction,Neural Variational Inference Framework,natural_language_inference,18,41,1,0,,2.70E-05,0,negative,1.45E-05,8.93E-06,3.92E-05,6.91E-09,3.19E-07,8.59E-07,7.87E-07,4.21E-06,0.000163741,0.999729459,1.04E-07,3.79E-05,4.76E-08
3297,natural_language_inference18,53,1 .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,42,1,0,,8.76E-07,0,negative,1.01E-06,8.45E-07,1.42E-07,2.13E-08,8.33E-08,3.13E-06,3.52E-07,1.87E-05,1.15E-05,0.99995983,5.96E-08,4.29E-06,2.09E-08
3298,natural_language_inference18,54,"Construct vector representations of the observed variables : u = f x ( x ) , v = f y ( y ) .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,43,1,0,,2.02E-06,0,negative,8.42E-07,1.05E-06,7.75E-07,1.23E-08,1.01E-07,3.53E-06,6.28E-07,1.58E-05,1.08E-05,0.999959795,5.78E-08,6.61E-06,3.07E-08
3299,natural_language_inference18,55,2 .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,44,1,0,,1.96E-06,0,negative,1.15E-06,1.16E-06,2.60E-07,1.14E-08,6.83E-08,3.03E-06,5.24E-07,2.09E-05,1.70E-05,0.999951547,6.28E-08,4.28E-06,2.68E-08
3300,natural_language_inference18,56,"Assemble a joint representation : ? = g ( u , v ) .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,45,1,0,,3.65E-06,0,negative,1.75E-06,1.25E-06,2.56E-06,8.13E-09,1.19E-07,1.78E-06,7.25E-07,7.49E-06,1.84E-05,0.999949065,5.97E-08,1.67E-05,4.28E-08
3301,natural_language_inference18,57,3 .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,46,1,0,,5.74E-06,0,negative,1.94E-06,8.90E-07,2.65E-07,1.13E-08,7.38E-08,2.60E-06,6.61E-07,1.74E-05,1.23E-05,0.999955385,3.82E-08,8.47E-06,2.66E-08
3302,natural_language_inference18,58,"Parameterise the variational distribution over the latent variable : = l 1 ( ? ) , log ? = l 2 ( ? ) . f x ( ) and f y ( ) can be any type of deep neural networks thatare suitable for the observed data ; g ( ) is an MLP that concatenates the vector representations of the conditioning variables ; l ( ) is a linear transformation which outputs the parameters of the Gaussian distribution .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,47,1,0,,1.27E-05,0,negative,1.16E-06,1.21E-05,1.45E-06,1.63E-07,4.27E-07,3.55E-05,2.99E-06,0.000397143,0.000123598,0.999420515,1.30E-07,4.63E-06,2.25E-07
3303,natural_language_inference18,59,"By sampling from the variational distribution , h ? q ? ( h|x , y ) , we are able to carry out stochastic back - propagation to optimise the lower bound ( Eq. 1 ) .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,48,1,0,,8.69E-06,0,negative,7.94E-06,2.38E-05,5.73E-06,8.66E-09,4.70E-07,6.58E-07,6.08E-07,8.16E-06,0.000140486,0.99978679,4.63E-08,2.52E-05,3.36E-08
3304,natural_language_inference18,60,"During training , the model parameters ?",Introduction,Neural Variational Inference Framework,natural_language_inference,18,49,1,0,,4.25E-06,0,negative,2.31E-06,9.12E-06,3.51E-07,2.86E-08,1.81E-07,1.15E-05,3.65E-06,0.00035668,2.91E-05,0.999564555,1.23E-07,2.23E-05,8.98E-08
3305,natural_language_inference18,61,together with the inference network parameters ?,Introduction,Neural Variational Inference Framework,natural_language_inference,18,50,1,0,,2.76E-06,0,negative,5.04E-07,1.05E-06,7.71E-08,2.18E-08,5.97E-08,2.46E-06,3.76E-07,3.01E-05,7.32E-06,0.999953703,2.89E-08,4.28E-06,1.96E-08
3306,natural_language_inference18,62,"are updated by stochastic back - propagation based on the samples h drawn from q ? ( h|x , y ) .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,51,1,0,,2.60E-06,0,negative,1.85E-05,6.86E-05,1.09E-05,7.30E-08,1.78E-06,8.35E-06,3.47E-06,0.000137604,0.000563764,0.999167574,6.86E-08,1.91E-05,2.39E-07
3307,natural_language_inference18,63,"For the gradients w.r.t. ? , we have the form :",Introduction,Neural Variational Inference Framework,natural_language_inference,18,52,1,0,,5.61E-06,0,negative,8.20E-06,5.76E-06,5.64E-06,2.20E-08,3.87E-07,2.35E-06,8.62E-07,1.61E-05,5.59E-05,0.999885894,2.87E-08,1.87E-05,5.35E-08
3308,natural_language_inference18,64,For the gradients w.r.t. ?,Introduction,Neural Variational Inference Framework,natural_language_inference,18,53,1,0,,6.74E-05,0,negative,2.37E-06,2.34E-06,9.00E-07,1.34E-08,2.57E-07,3.08E-06,7.46E-07,2.67E-05,1.19E-05,0.999942215,1.03E-08,9.42E-06,2.54E-08
3309,natural_language_inference18,65,"we reparameterise h = + ? and sample ( l ) ? N ( 0 , I ) to reduce the variance in stochastic estimation .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,54,1,0,,9.17E-06,0,negative,7.50E-06,1.68E-05,1.84E-06,1.57E-07,1.82E-06,6.23E-05,1.42E-05,0.000726099,3.85E-05,0.99910375,2.71E-08,2.65E-05,4.15E-07
3310,natural_language_inference18,66,The update of ?,Introduction,Neural Variational Inference Framework,natural_language_inference,18,55,1,0,,2.85E-05,0,negative,4.25E-06,3.80E-06,2.98E-06,1.95E-09,1.06E-07,5.31E-07,5.33E-07,5.02E-06,5.65E-05,0.999904405,3.10E-08,2.18E-05,2.27E-08
3311,natural_language_inference18,67,can be carried out by backpropagating the gradients w.r.t. and ?:,Introduction,Neural Variational Inference Framework,natural_language_inference,18,56,1,0,,1.40E-05,0,negative,2.84E-06,1.53E-06,1.73E-06,7.27E-09,1.29E-07,1.26E-06,6.96E-07,6.84E-06,1.64E-05,0.999949432,3.08E-08,1.91E-05,3.96E-08
3312,natural_language_inference18,68,It is worth mentioning that unsupervised learning is a special case of the neural variational framework where h has no parent node x .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,57,1,0,,2.94E-06,0,negative,1.05E-06,7.16E-06,3.05E-06,5.47E-08,2.40E-07,3.44E-06,1.81E-06,2.93E-05,4.02E-05,0.999891912,1.11E-06,2.04E-05,2.58E-07
3313,natural_language_inference18,69,"In that case h is directly drawn from the prior p ( h ) instead of the conditional distribution p ? ( h|x ) , and s ( h ) = log p ? ( y |h ) p ? ( h ) ? log q ? ( h|y ) .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,58,1,0,,1.26E-06,0,negative,1.01E-06,1.21E-06,1.18E-06,8.13E-10,4.12E-08,2.88E-07,2.27E-07,3.12E-06,1.06E-05,0.999975796,4.49E-09,6.49E-06,5.71E-09
3314,natural_language_inference18,70,Here we only discuss the scenario where the latent variables are continuous and the parameterised diagonal Gaussian is employed as the variational distribution .,Introduction,Neural Variational Inference Framework,natural_language_inference,18,59,1,0,,2.57E-05,0,negative,1.12E-05,3.26E-05,8.45E-06,8.76E-09,4.47E-07,1.27E-06,1.71E-06,1.43E-05,0.000110157,0.999746187,4.53E-08,7.35E-05,7.35E-08
3315,natural_language_inference18,71,"However the framework is also suitable for discrete units , and the only modification needed is to replace the Gaussian with a multinomial parameterised by the outputs of a softmax function .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,60,1,0,,3.53E-05,0,negative,1.25E-05,1.28E-05,5.06E-06,1.34E-08,4.21E-07,1.05E-06,6.59E-07,8.12E-06,0.000100718,0.999839285,2.65E-08,1.94E-05,5.51E-08
3316,natural_language_inference18,72,"Though the reparameteris ation trick for continuous variables is not applicable for this case , a policy gradient approach can help to alleviate the high variance problem during stochastic estimation .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,61,1,0,,4.51E-05,0,negative,7.96E-05,6.62E-06,1.61E-06,1.31E-07,1.08E-06,5.11E-06,4.03E-06,3.30E-05,1.71E-05,0.99952078,5.40E-08,0.000330481,3.46E-07
3317,natural_language_inference18,73,"proposed a variational inference framework for semi-supervised learning , but the prior distribution over the hidden variable p ( h ) remains as the standard Gaussian prior , while we apply a conditional parameterised Gaussian distribution , which is jointly learned with the variational distribution .",Introduction,Neural Variational Inference Framework,natural_language_inference,18,62,1,0,,1.15E-05,0,negative,5.89E-06,9.88E-06,1.63E-05,1.35E-06,4.13E-06,3.79E-05,1.20E-05,0.000111944,2.36E-05,0.999735073,4.92E-07,3.82E-05,3.29E-06
3318,natural_language_inference18,74,Neural Variational Document Model,,,natural_language_inference,18,0,1,0,,0.116135691,0,research-problem,2.71E-05,0.000564748,0.000343546,4.14E-06,1.20E-06,0.000232228,0.000798468,0.003831926,0.001104088,0.389440272,0.602456124,0.00110667,8.95E-05
3319,natural_language_inference18,75,The Neural Variational Document Model ( ) is a simple instance of unsupervised learning where a continuous hidden variable h ?,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,1,1,0,,0.000113382,0,negative,8.67E-07,1.07E-05,0.000130134,2.57E-08,1.10E-08,6.77E-06,8.95E-06,0.000129927,0.000165229,0.999186151,0.000309226,5.17E-05,2.90E-07
3320,natural_language_inference18,76,"R K , which generates all the words in a document independently , is introduced to represent its semantic content .",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,2,1,0,,2.50E-05,0,negative,8.09E-06,7.64E-06,5.87E-05,2.30E-08,3.82E-08,2.79E-06,1.57E-06,6.13E-05,0.000106703,0.999719915,1.15E-06,3.20E-05,5.33E-08
3321,natural_language_inference18,77,Let X ?,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,3,1,0,,2.56E-06,0,negative,1.23E-06,2.50E-07,9.31E-07,2.58E-09,3.38E-09,1.16E-06,5.88E-07,1.41E-05,2.03E-06,0.999956472,3.69E-07,2.28E-05,7.26E-09
3322,natural_language_inference18,78,R | V | be the bag - of - words representation of a document and x i ?,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,4,1,0,,1.93E-05,0,negative,8.78E-07,4.71E-07,1.33E-06,6.82E-09,4.90E-09,1.72E-06,1.07E-06,3.39E-05,3.02E-06,0.999932318,1.02E-06,2.42E-05,2.04E-08
3323,natural_language_inference18,79,R | V | be the one - hot representation of the word at position i .,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,5,1,0,,8.68E-06,0,negative,8.86E-07,9.32E-07,2.68E-06,2.91E-09,4.71E-09,1.02E-06,8.13E-07,2.29E-05,6.12E-06,0.999939664,7.94E-07,2.41E-05,1.24E-08
3324,natural_language_inference18,80,"As an unsupervised generative model , we could interpret NVDM as a variational autoencoder : an MLP encoder q ( h|X ) compresses document representations into continuous hidden vectors ( X ? h ) ; a softmax decoder p ( X |h ) = N i =1 p ( x i | h ) reconstructs the documents by independently generating the words ( h ? {x i }) .",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,6,1,0,,5.41E-05,0,negative,1.29E-06,7.39E-06,3.69E-05,3.78E-08,2.23E-08,5.97E-06,2.35E-06,8.34E-05,0.000370888,0.999473542,6.82E-06,1.12E-05,1.69E-07
3325,natural_language_inference18,81,"To maximise the log - likelihood log h p ( X|h ) p ( h ) of documents , we derive the lower bound :",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,7,1,0,,2.15E-05,0,negative,2.84E-06,2.27E-06,4.59E-06,1.43E-08,1.20E-08,2.21E-06,1.66E-06,5.48E-05,1.81E-05,0.999832952,2.89E-06,7.76E-05,6.30E-08
3326,natural_language_inference18,82,where N is the number of words in the document and p ( h ) is a Gaussian prior for h .,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,8,1,0,,5.20E-06,0,negative,8.68E-07,1.29E-06,1.24E-06,8.35E-09,7.38E-09,2.50E-06,7.69E-07,8.44E-05,1.53E-05,0.999885805,3.60E-07,7.36E-06,1.91E-08
3327,natural_language_inference18,83,"Here , we consider N is observed for all the documents .",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,9,1,0,,2.48E-05,0,negative,8.64E-07,2.54E-06,2.14E-06,5.37E-09,1.16E-08,2.57E-06,1.01E-06,0.000111027,3.11E-05,0.999838379,2.89E-07,1.00E-05,1.85E-08
3328,natural_language_inference18,84,The conditional probability over words p ? ( x i |h ) ( decoder ) is modelled by multinomial logistic regression and shared across documents :,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,10,1,0,,2.60E-06,0,negative,2.67E-06,3.65E-06,3.02E-05,8.74E-09,1.40E-08,3.66E-06,1.45E-06,8.21E-05,0.000164161,0.999701966,3.87E-07,9.63E-06,4.32E-08
3329,natural_language_inference18,85,where R ?,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,11,1,0,,2.78E-06,0,negative,1.36E-06,2.74E-07,9.15E-07,5.19E-09,4.64E-09,1.72E-06,1.15E-06,2.85E-05,2.66E-06,0.999933468,4.17E-07,2.95E-05,1.70E-08
3330,natural_language_inference18,86,R K|V | learns the semantic word embeddings and b xi represents the bias term .,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,12,1,0,,2.36E-05,0,negative,6.65E-06,5.04E-06,1.00E-05,2.04E-08,3.70E-08,3.48E-06,2.08E-06,0.000113869,3.55E-05,0.999788375,3.09E-07,3.45E-05,5.69E-08
3331,natural_language_inference18,87,"As there is no supervision information for the latent semantics , h , the posterior approximation q ? ( h|X ) is only conditioned on the current document X .",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,13,1,0,,2.17E-05,0,negative,5.52E-06,4.38E-06,8.31E-06,2.40E-09,1.82E-08,3.21E-07,2.86E-07,9.51E-06,2.56E-05,0.999902219,1.82E-07,4.36E-05,8.47E-09
3332,natural_language_inference18,88,"The inference network q ? ( h|X ) = N ( h| ( X ) , diag ( ?",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,14,1,0,,1.68E-05,0,negative,1.63E-06,3.55E-06,3.58E-05,2.58E-09,1.00E-08,1.13E-06,1.15E-06,2.63E-05,6.29E-05,0.999845294,4.75E-07,2.18E-05,2.14E-08
3333,natural_language_inference18,89,2 ( X ) ) ) is modelled as :,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,15,1,0,,6.84E-05,0,negative,2.66E-06,1.19E-06,2.38E-05,5.73E-09,1.34E-08,1.39E-06,1.29E-06,1.76E-05,2.70E-05,0.999882673,3.81E-07,4.20E-05,3.32E-08
3334,natural_language_inference18,90,"For each document X , the neural network generates its own parameters and ?",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,16,1,0,,2.18E-05,0,negative,1.49E-06,1.85E-06,2.18E-06,3.07E-09,1.00E-08,2.05E-06,1.12E-06,8.11E-05,1.28E-05,0.999873629,1.13E-07,2.37E-05,1.35E-08
3335,natural_language_inference18,91,that parameterise the latent distribution over document semantics h.,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,17,1,0,,6.15E-06,0,negative,1.88E-06,1.83E-06,1.70E-06,1.25E-08,1.82E-08,2.65E-06,9.25E-07,8.57E-05,2.53E-05,0.999868909,5.91E-08,1.10E-05,2.52E-08
3336,natural_language_inference18,92,"Based on the samples h ? q ? ( h|X ) , the lower bound ( Eq. 5 ) can be optimised by back - propagating the stochastic gradients w.r.t. ? and ?.",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,18,1,0,,2.68E-05,0,negative,4.61E-06,2.10E-06,3.67E-06,4.70E-09,2.08E-08,6.96E-07,5.94E-07,1.97E-05,1.48E-05,0.999906587,9.44E-08,4.71E-05,1.54E-08
3337,natural_language_inference18,93,"Since p ( h ) is a standard Gaussian prior , the Gaussian KL -",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,19,1,0,,1.80E-05,0,negative,1.20E-05,4.83E-06,5.09E-05,2.88E-09,2.14E-08,7.90E-07,2.01E-06,1.76E-05,2.26E-05,0.999554351,3.93E-07,0.00033444,2.70E-08
3338,natural_language_inference18,94,can be computed analytically to further lower the variance of the gradients .,Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,20,1,0,,0.000101867,0,negative,9.27E-06,2.38E-06,2.17E-06,4.94E-08,6.46E-08,4.60E-06,2.15E-06,0.000113758,2.40E-05,0.999783887,1.80E-07,5.74E-05,1.00E-07
3339,natural_language_inference18,95,"Moreover , it also acts as a regulariser for updating the parameters of the inference network q ? ( h|X ) .",Neural Variational Document Model,Neural Variational Document Model,natural_language_inference,18,21,1,0,,4.90E-05,0,negative,2.45E-05,7.84E-06,4.35E-05,2.07E-08,8.17E-08,1.61E-06,1.48E-06,3.18E-05,0.000123346,0.999691616,1.40E-07,7.39E-05,6.97E-08
3340,natural_language_inference18,96,Neural Answer Selection Model,,,natural_language_inference,18,0,1,0,,0.256955954,0,research-problem,4.67E-06,9.61E-05,3.40E-05,1.09E-06,3.04E-07,3.12E-05,0.000299872,0.000474471,6.21E-05,0.07931929,0.919315578,0.000337727,2.36E-05
3341,natural_language_inference18,97,Answer sentence selection is a question answering paradigm where a model must identify the correct sentences answering a factual question from a set of candidate sentences .,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,1,1,0,,0.181855641,0,negative,8.17E-05,3.04E-05,8.65E-05,5.71E-05,2.59E-06,0.000135694,0.001947387,0.00017256,5.21E-05,0.75992567,0.235017674,0.002314356,0.000176223
3342,natural_language_inference18,98,"Assume a question q is associated with a set of answer sentences {a 1 , a 2 , ... , an } , together with their judgements {y 1 , y 2 , ... , y n } , where y m = 1 if the answer am is correct and y m = 0 otherwise .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,2,1,0,,8.99E-07,0,negative,2.60E-07,1.31E-06,9.30E-07,3.80E-09,6.06E-09,1.14E-06,8.82E-07,1.23E-05,1.56E-05,0.999955195,5.07E-06,7.31E-06,1.72E-08
3343,natural_language_inference18,99,"This is a classification task where we treat each training data point as a triple ( q , a , y ) while predicting y for the unlabelled question - answer pair ( q , a ) .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,3,1,0,,8.86E-06,0,negative,6.55E-06,2.24E-05,1.66E-05,5.04E-06,4.52E-07,4.55E-05,9.18E-05,0.000208358,8.67E-05,0.99439533,0.004897623,0.000214193,9.52E-06
3344,natural_language_inference18,100,The Neural Answer Selection Model ( ) is a supervised model that learns the question and answer representations and predicts their relatedness .,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,4,1,0,,0.006448008,0,negative,2.48E-05,5.86E-05,0.015425911,1.61E-06,6.28E-07,0.000126875,0.000585981,0.000313881,0.000580525,0.979986165,0.002309086,0.000563597,2.23E-05
3345,natural_language_inference18,101,"It employs two different LSTMs to embed raw question inputs q and answer inputs a. Let sq ( j ) and s a ( i ) be the state outputs of the two LSTMs , and i , j be the positions of the states .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,5,1,0,,3.64E-06,0,negative,1.34E-05,2.07E-05,0.001137924,7.49E-08,2.02E-07,1.08E-05,2.16E-05,4.42E-05,0.000493001,0.998159245,1.13E-05,8.70E-05,5.22E-07
3346,natural_language_inference18,102,"Conventionally , the last state outputs sq ( | q | ) and s a ( | a | ) , as the independent question and answer representations , can be used for relatedness prediction .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,6,1,0,,2.44E-05,0,negative,3.67E-06,1.05E-06,8.64E-06,3.46E-09,8.90E-09,8.47E-07,6.78E-07,5.94E-06,2.85E-05,0.999924302,6.21E-07,2.57E-05,1.22E-08
3347,natural_language_inference18,103,"In NASM , however , we aim to learn pair- specific representations through a latent attention mechanism , which is more effective for pair relatedness prediction .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,7,1,0,,0.001199387,0,negative,2.18E-05,6.00E-05,3.41E-05,1.31E-06,3.43E-07,3.64E-05,6.02E-05,0.000292115,9.41E-05,0.997277363,0.001568719,0.000550003,3.43E-06
3348,natural_language_inference18,104,NASM applies an attention model to focus on the words in the answer sentence thatare prominent for predicting the answer matched to the current question .,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,8,1,0,,0.001378875,0,negative,8.68E-05,0.000202231,0.028741942,1.95E-06,2.34E-06,0.00011787,0.000412109,0.000291666,0.001614538,0.967156972,0.000505943,0.000846209,1.94E-05
3349,natural_language_inference18,105,"Instead of using a deterministic question vector , such ass q ( | q| ) , NASM employs a latent distribution p ? ( h|q ) to model the question semantics , which is a parameterised diagonal Gaussian N ( h| ( q ) , diag ( ? 2 ( q ) ) ) .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,9,1,0,,3.86E-05,0,negative,1.31E-05,6.17E-05,0.00038717,2.58E-08,1.36E-07,5.92E-06,8.96E-06,5.71E-05,0.000724487,0.998668584,6.26E-06,6.64E-05,1.80E-07
3350,natural_language_inference18,106,"Therefore , the attention model extracts a context vector c ( a , h) by iteratively attending to the answer tokens based on the stochastic vector h ? p ? ( h|q ) .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,10,1,0,,6.90E-05,0,negative,5.37E-06,9.44E-06,3.63E-05,4.06E-09,2.51E-08,1.27E-06,1.31E-06,1.46E-05,0.000353712,0.99955388,8.12E-07,2.33E-05,2.71E-08
3351,natural_language_inference18,107,In doing so the model is able to adapt to the ambiguity inherent in questions and obtain salient information through attention .,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,11,1,0,,3.88E-05,0,negative,2.94E-05,2.58E-05,1.23E-05,6.75E-08,1.82E-07,3.64E-06,3.95E-06,3.78E-05,0.000881468,0.998890829,1.97E-06,0.000112343,1.84E-07
3352,natural_language_inference18,108,"Compared to its deterministic counterpart ( applying sq ( | q | ) as the question semantics ) , the stochastic units incorporated into NASM allow multi-modal attention distributions .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,12,1,0,,0.000230947,0,negative,0.000205391,5.65E-05,0.00019083,6.19E-08,3.54E-07,4.22E-06,2.21E-05,2.94E-05,0.000228373,0.997470318,5.11E-06,0.001786993,2.99E-07
3353,natural_language_inference18,109,"Further , by marginalising over the latent variables , NASM is more robust against overfitting , which is important for small question answering training sets .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,13,1,0,,0.077709,0,negative,0.005369983,3.17E-05,4.77E-05,8.38E-07,1.33E-06,2.37E-05,0.000566961,0.000231226,2.66E-05,0.742430054,1.42E-05,0.251247805,7.82E-06
3354,natural_language_inference18,110,"In this model , the conditional distribution p ? ( h|q ) is :",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,14,1,0,,9.91E-07,0,negative,1.11E-06,4.74E-07,4.29E-06,2.13E-09,5.36E-09,1.02E-06,9.91E-07,7.24E-06,1.58E-05,0.999955224,1.83E-07,1.36E-05,1.14E-08
3355,natural_language_inference18,111,"For each question q , the neural network generates the corresponding parameters and ?",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,15,1,0,,3.58E-06,0,negative,1.91E-06,4.03E-07,9.59E-07,2.92E-09,1.08E-08,1.46E-06,1.23E-06,1.39E-05,4.72E-06,0.999952737,9.58E-08,2.26E-05,1.02E-08
3356,natural_language_inference18,112,that parameterise the latent distribution over question semantics h.,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,16,1,0,,1.05E-06,0,negative,2.55E-06,1.26E-06,9.71E-07,2.76E-08,3.05E-08,5.25E-06,2.13E-06,5.61E-05,2.17E-05,0.999899174,8.58E-08,1.07E-05,3.32E-08
3357,natural_language_inference18,113,"Following Bahdanau et al. , the attention model is defined as :",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,17,1,0,,5.47E-06,0,negative,1.88E-06,3.27E-06,2.87E-05,1.09E-08,1.97E-08,4.52E-06,3.90E-06,3.86E-05,0.000313257,0.999588326,1.80E-06,1.56E-05,1.30E-07
3358,natural_language_inference18,114,"where ?( i ) is the normalised attention score at answer token i , and the context vector c ( a , h) is the weighted sum of all the state outputs s a ( i ) .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,18,1,0,,5.29E-07,0,negative,2.32E-06,9.43E-07,1.90E-06,1.89E-08,2.93E-08,3.28E-06,2.27E-06,2.37E-05,1.68E-05,0.999927721,2.05E-07,2.07E-05,4.57E-08
3359,natural_language_inference18,115,"We adopt z q ( q ) , z a ( a , h) as the question and answer representations for predicting their relatedness y. z q ( q ) is a deterministic vector that is equal to sq ( | q | ) , while z a ( a , h) is a combination of the sequence output s a ( | a | ) and the context vector c ( a , h) ( Eq. 14 ) .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,19,1,0,,1.82E-06,0,negative,4.05E-07,1.97E-06,1.42E-06,4.40E-09,8.62E-09,6.50E-06,2.73E-06,9.37E-05,8.31E-05,0.999806028,2.18E-07,3.97E-06,3.23E-08
3360,natural_language_inference18,116,"For the prediction of pair relatedness y , we model the conditional probability distribution p ? ( y|z q , z a ) by sigmoid function :",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,20,1,0,,2.26E-06,0,negative,1.09E-05,3.04E-06,2.59E-05,1.25E-08,4.24E-08,3.63E-06,3.52E-06,3.86E-05,9.70E-05,0.999781337,2.02E-07,3.58E-05,7.22E-08
3361,natural_language_inference18,117,"To maximise the log - likelihood log p ( y | q , a ) we use the variational lower bound :",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,21,1,0,,3.51E-06,0,negative,9.94E-06,2.17E-06,1.24E-05,1.32E-08,3.77E-08,2.71E-06,4.62E-06,2.37E-05,1.84E-05,0.999812012,2.95E-07,0.000113558,8.45E-08
3362,natural_language_inference18,118,"Following the neural variational inference framework , we construct a deep neural network as the inference network",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,22,1,0,,8.37E-05,0,negative,1.75E-05,5.89E-05,7.82E-05,4.10E-07,6.73E-07,2.51E-05,2.23E-05,0.000364156,0.002709631,0.996683963,2.27E-06,3.48E-05,2.16E-06
3363,natural_language_inference18,119,"where q and a are also modelled by LSTMs 1 , and the relatedness label y is modelled by a simple linear transformation into the vector s y .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,23,1,0,,9.83E-07,0,negative,2.78E-06,7.17E-07,2.16E-06,2.61E-08,4.00E-08,2.83E-06,2.33E-06,1.85E-05,1.61E-05,0.999929221,1.34E-07,2.51E-05,5.69E-08
3364,natural_language_inference18,120,"According to the joint representation ? ? , we then generate the parameters ? and ? ? , which parameterise the variational distribution over the question semantics h.",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,24,1,0,,3.92E-06,0,negative,1.11E-06,8.50E-07,1.13E-06,5.09E-09,1.49E-08,1.69E-06,1.28E-06,2.69E-05,2.61E-05,0.999934206,3.21E-08,6.67E-06,2.17E-08
3365,natural_language_inference18,121,"To emphasise , though both p ? ( h|q ) and q ? ( h|q , a , y ) are modelled as parameterised Gaussian distributions , q ? ( h|q , a , y ) as an approximation only functions during inference by producing samples to compute the stochastic gradients , while p ? ( h|q ) is the generative distribution that generates the samples for predicting the question - answer relatedness y.",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,25,1,0,,4.54E-07,0,negative,5.92E-07,6.18E-07,5.17E-07,3.08E-09,8.67E-09,6.90E-07,4.01E-07,7.37E-06,1.55E-05,0.999969772,4.16E-08,4.53E-06,7.91E-09
3366,natural_language_inference18,122,"Based on the samples h ? q ? ( h|q , a , y ) , we use SGVB to optimise the lower bound ( Eq.16 ) .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,26,1,0,,1.10E-05,0,negative,4.95E-06,1.76E-05,2.02E-05,9.32E-09,6.48E-08,7.90E-06,1.06E-05,0.000196638,0.000134061,0.999582245,1.33E-07,2.55E-05,1.07E-07
3367,natural_language_inference18,123,The model parameters ?,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,27,1,0,,1.86E-06,0,negative,1.04E-06,9.46E-07,2.70E-07,1.58E-08,1.46E-08,7.91E-06,4.81E-06,0.000171545,1.38E-05,0.999776624,2.05E-07,2.27E-05,6.63E-08
3368,natural_language_inference18,124,and the inference network parameters ?,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,28,1,0,,2.44E-06,0,negative,1.12E-06,3.69E-07,1.57E-07,1.49E-08,1.47E-08,4.41E-06,2.14E-06,7.19E-05,3.65E-06,0.999899266,5.08E-08,1.69E-05,2.86E-08
3369,natural_language_inference18,125,are updated jointly using their stochastic gradients .,Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,29,1,0,,0.000163032,0,negative,2.37E-05,1.01E-05,9.29E-06,1.28E-07,2.57E-07,9.65E-06,7.52E-06,0.000154706,0.000349498,0.999398013,9.81E-08,3.67E-05,3.14E-07
3370,natural_language_inference18,126,"In this case , similar to the NVDM , the Gaussian KL divergence D KL [ q ? ( h|q , a , y ) ) p ? ( h|q ) ] can be analytically computed during training process .",Neural Answer Selection Model,Neural Answer Selection Model,natural_language_inference,18,30,1,0,,3.06E-06,0,negative,1.67E-06,1.68E-06,2.68E-06,5.31E-09,1.73E-08,1.38E-06,1.68E-06,2.09E-05,2.06E-05,0.999919274,1.18E-07,3.00E-05,3.96E-08
3371,natural_language_inference18,127,Experiments,,,natural_language_inference,18,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
3372,natural_language_inference18,128,Dataset & Setup for Document Modelling,Experiments,,natural_language_inference,18,1,1,0,,0.012394841,0,negative,0.000248131,6.97E-05,0.001745137,0.000743581,0.000341429,0.017328443,0.066721259,0.020042368,3.16E-05,0.876367867,0.004557574,0.007736696,0.004066163
3373,natural_language_inference18,129,We experiment with NVDM on two standard news corpora : the 20 News Groups 2 and the Reuters RCV1 - v2 3 .,Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,2,1,0,,0.553468819,1,negative,0.00010309,9.22E-05,0.003725036,3.15E-06,2.57E-05,0.025209521,0.247909155,0.001710262,1.84E-05,0.719783546,1.66E-05,0.001161321,0.000242018
3374,natural_language_inference18,130,"The former is a collection of newsgroup documents , consisting of 11,314 training and 7,531 test articles .",Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,3,1,0,,0.002237917,0,negative,3.91E-05,5.46E-06,0.000499886,9.82E-05,0.001669138,0.005918501,0.007293923,4.60E-05,1.54E-06,0.984233853,2.14E-06,9.95E-05,9.27E-05
3375,natural_language_inference18,131,"The latter is a large collection from Reuters newswire stories with 794,414 training and 10,000 test cases .",Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,4,1,0,,0.006384738,0,negative,2.79E-05,2.13E-06,0.000143931,0.000148117,0.001658104,0.007636101,0.004045337,2.92E-05,7.22E-07,0.986189672,9.30E-07,4.33E-05,7.46E-05
3376,natural_language_inference18,132,"The vocabulary size of these two datasets are set as 2,000 and 10,000 .",Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,5,1,0,,0.695837737,1,negative,1.60E-05,1.34E-05,3.21E-05,9.51E-07,1.09E-06,0.392774575,0.079745929,0.106442296,1.68E-05,0.4208013,3.52E-06,3.17E-05,0.000120498
3377,natural_language_inference18,133,To make a direct comparison with the prior work we follow the same preprocessing procedure and setup as Hinton & Salakhutdinov arabs collection ( b ) The five nearest words in the semantic space .,Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,6,1,0,,0.000869101,0,negative,6.83E-05,4.26E-06,0.001039991,3.78E-07,1.09E-05,0.001829492,0.009635215,6.69E-05,2.08E-06,0.987103257,6.51E-07,0.000230407,8.23E-06
3378,natural_language_inference18,134,Adam and tuned by hold - out validation perplexity .,Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,7,1,0,,0.326364862,0,experimental-setup,3.61E-05,1.83E-05,6.69E-05,1.79E-06,1.23E-06,0.53736782,0.077563454,0.108826662,4.46E-05,0.275865336,4.44E-06,2.60E-05,0.000177339
3379,natural_language_inference18,135,We alternately optimise the generative model and the inference network by fixing the parameters of one while updating the parameters of the other .,Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,8,1,0,,0.077493118,0,negative,0.000332363,0.000632054,0.002514234,6.01E-06,1.84E-05,0.029649924,0.014531512,0.006976465,0.002278409,0.942822322,1.30E-05,0.000115382,0.000109924
3380,natural_language_inference18,136,presents the test document perplexity .,Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,9,1,0,,0.014087663,0,negative,3.35E-05,3.44E-07,5.86E-05,1.69E-07,2.08E-06,0.000446291,0.002906715,3.30E-05,4.11E-07,0.996170592,9.61E-07,0.000338024,9.36E-06
3381,natural_language_inference18,137,"The first column lists the models , and the second column shows the dimension of latent variables used in the experiments .",Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,10,1,0,,0.006927594,0,negative,4.72E-06,3.92E-07,4.01E-06,1.84E-07,8.32E-07,0.003038526,0.001534611,0.000267388,6.80E-07,0.995114847,2.04E-07,3.05E-05,3.09E-06
3382,natural_language_inference18,138,The final two columns present the perplexity achieved by each topic model on the 20 New s Groups and RCV1 - v2 datasets .,Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,11,1,0,,0.044189389,0,negative,0.000161802,1.02E-06,6.18E-05,8.93E-08,1.68E-06,0.000445635,0.027467324,3.95E-05,3.44E-07,0.962467804,6.03E-07,0.009338291,1.41E-05
3383,natural_language_inference18,139,"In document modelling , perplexity is computed by exp ( ?",Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,12,1,0,,0.001795484,0,negative,2.19E-05,3.36E-06,0.000610275,6.79E-08,3.61E-07,0.001566624,0.005366293,0.000137681,5.44E-06,0.992175024,7.15E-06,9.44E-05,1.14E-05
3384,natural_language_inference18,140,1,Experiments,Dataset & Setup for Document Modelling,natural_language_inference,18,13,1,0,,0.000281688,0,negative,3.85E-06,2.49E-07,5.79E-06,1.43E-07,2.14E-07,0.000881562,0.000241551,5.86E-05,3.91E-06,0.998796526,3.12E-07,5.61E-06,1.74E-06
3385,natural_language_inference18,141,Experiments on Document Modelling,,,natural_language_inference,18,0,1,1,experiments,0.967114026,1,research-problem,0.003225851,0.000403718,0.000828545,0.000651457,0.000331889,0.000704747,0.15107112,0.00140979,2.91E-05,0.268650956,0.422636598,0.147628412,0.002427771
3386,natural_language_inference18,142,", where Dis the number of documents , Nd represents the length of the dth document and log p ( X ) = log p ( X |h ) p ( h ) dh is the log probability of the words in the document .",Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,1,1,0,,0.000293713,0,negative,0.00058582,1.74E-06,6.09E-05,1.82E-07,1.16E-06,5.44E-05,0.024249243,3.68E-06,1.22E-05,0.974869963,2.30E-06,0.000126787,3.16E-05
3387,natural_language_inference18,143,"Since log p ( X ) is intractable in the NVDM , we use the variational lower bound ( which is an upper bound on perplexity ) to compute the perplexity following .",Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,2,1,0,,0.002031835,0,negative,0.001396839,1.11E-05,0.000135098,2.92E-07,1.80E-06,9.45E-05,0.067854843,1.21E-05,6.84E-05,0.930054554,5.70E-06,0.000290542,7.42E-05
3388,natural_language_inference18,144,"apply discrete latent variables , here NVDM employs a continuous stochastic document representation .",Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,3,1,0,,0.044752493,0,negative,0.002839757,8.07E-06,0.002640262,1.68E-06,3.53E-06,0.000318018,0.175228236,1.28E-05,5.22E-05,0.817586184,2.14E-05,0.000811115,0.000476788
3389,natural_language_inference18,145,The experimental results indicate that NVDM achieves the best performance on both datasets .,Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,4,1,1,experiments,0.973783733,1,experiments,0.002208078,1.50E-07,1.60E-06,1.31E-07,1.57E-07,2.59E-05,0.938031059,4.15E-06,9.15E-08,0.042572255,1.15E-06,0.016628313,0.000526974
3390,natural_language_inference18,146,"For the experiments on RCV1 - v2 dataset , the NVDM with latent variable of 50 dimension performs even better than the fDARN with 200 dimension .",Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,5,1,1,experiments,0.983972283,1,experiments,0.004494334,2.23E-07,1.30E-06,5.37E-07,3.26E-07,4.53E-05,0.971558313,8.46E-06,1.33E-07,0.013001652,8.45E-07,0.009286063,0.001602466
3391,natural_language_inference18,147,It demonstrates that our document model with continuous latent variables has higher expressiveness and better generalis ation ability .,Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,6,1,0,,0.905927907,1,experiments,0.015774737,7.34E-07,6.83E-06,2.24E-07,8.67E-07,3.24E-05,0.800886511,4.09E-06,6.62E-07,0.1668119,1.18E-06,0.016116477,0.000363356
3392,natural_language_inference18,148,compares the 5 nearest words selected according to the semantic vector learned from NVDM and docNADE .,Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,7,1,0,,0.037261023,0,negative,0.000764775,2.17E-06,0.000773799,4.54E-07,2.22E-06,0.000205781,0.245760132,8.31E-06,1.10E-05,0.751632342,2.91E-06,0.000384858,0.000451239
3393,natural_language_inference18,149,While all the baseline models listed in,Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,8,1,0,,0.002652747,0,negative,0.001082647,4.14E-07,9.73E-05,8.55E-08,1.07E-06,3.51E-05,0.168726,1.91E-06,1.01E-06,0.82872347,6.39E-07,0.001269163,6.12E-05
3394,natural_language_inference18,150,"In addition to the perplexities , we also qualitatively evaluate the semantic information learned by NVDM on the 20 News Groups dataset with latent variables of 50 dimension .",Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,9,1,0,,0.284730843,0,experiments,0.003032857,3.98E-06,1.19E-05,1.41E-07,1.51E-06,4.57E-05,0.574126409,7.39E-06,2.03E-06,0.417910771,8.28E-07,0.004730788,0.000125653
3395,natural_language_inference18,151,We assume each dimension in the latent space represents a topic that corresponds to a specific semantic meaning .,Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,10,1,0,,0.003034476,0,negative,0.000262229,2.98E-05,7.29E-05,8.60E-07,2.39E-06,0.00057143,0.045959896,8.98E-05,0.001731271,0.951046238,4.72E-06,2.20E-05,0.000206409
3396,natural_language_inference18,152,presents 5 randomly selected topics with 10 words that have the strongest positive connection with the topic .,Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,11,1,0,,0.036652996,0,experiments,0.00119445,3.98E-06,0.000291891,3.09E-06,2.13E-05,0.000343673,0.587971975,2.32E-05,8.26E-06,0.407147741,1.17E-06,0.00082291,0.002166336
3397,natural_language_inference18,153,"Based on the words in each column , we can deduce their corresponding topics as : Space , Religion , Encryption , Sport and Policy .",Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,12,1,0,,0.004813846,0,negative,8.94E-05,9.19E-07,9.27E-06,1.53E-07,1.75E-06,6.94E-05,0.010998245,5.06E-06,2.04E-05,0.988748844,2.02E-07,2.56E-05,3.07E-05
3398,natural_language_inference18,154,"Although the model does not impose independent interpretability on the latent representation dimensions , we still see that the NVDM learns locally interpretable structure .",Experiments on Document Modelling,Experiments on Document Modelling,natural_language_inference,18,13,1,0,,0.003026016,0,experiments,0.083043488,1.07E-05,4.71E-05,1.71E-06,5.92E-06,9.01E-05,0.554158753,1.18E-05,2.29E-05,0.355761863,3.21E-06,0.006220875,0.0006216
3399,natural_language_inference18,155,Dataset & Setup for Answer Sentence Selection,Experiments on Document Modelling,,natural_language_inference,18,14,1,1,experiments,0.010129233,0,negative,0.000141402,7.26E-07,3.51E-05,1.04E-05,7.95E-05,0.000272138,0.375437983,4.28E-06,5.39E-07,0.608155602,1.00E-05,0.000946534,0.014905766
3400,natural_language_inference18,156,"We experiment on two answer selection datasets , the QASent and the WikiQA datasets .",Experiments on Document Modelling,Dataset & Setup for Answer Sentence Selection,natural_language_inference,18,15,1,0,,0.111846109,0,negative,0.000152088,1.31E-05,0.000259031,8.22E-07,2.35E-05,0.000410731,0.34277582,2.61E-05,3.47E-06,0.652213172,3.12E-06,0.003135606,0.000983331
3401,natural_language_inference18,157,"QASent is created from the TREC QA track , and the WikiQA is constructed from Wikipedia , which is less noisy and less biased towards lexical overlap 4 . summarises the statistics of the two datasets ..",Experiments on Document Modelling,Dataset & Setup for Answer Sentence Selection,natural_language_inference,18,16,1,0,,0.012465918,0,negative,2.08E-05,5.81E-08,1.50E-05,1.26E-06,1.33E-05,0.000250574,0.025021958,2.20E-06,6.45E-08,0.973918505,3.59E-07,0.000345234,0.000410597
3402,natural_language_inference18,158,Bigram - CNN is the simple convolutional model reported in .,Experiments on Document Modelling,Dataset & Setup for Answer Sentence Selection,natural_language_inference,18,17,1,0,,0.888368728,1,experiments,3.79E-05,2.23E-06,0.003785865,4.92E-06,4.74E-06,0.010451516,0.836816635,0.000169991,1.90E-05,0.137667705,2.53E-06,0.000128053,0.010908878
3403,natural_language_inference18,159,Deep CNN is the deep convolutional model from .,Experiments on Document Modelling,Dataset & Setup for Answer Sentence Selection,natural_language_inference,18,18,1,0,,0.517972687,1,negative,8.39E-05,7.45E-06,0.002295031,5.23E-06,4.20E-06,0.012272299,0.308961706,0.000686024,0.000282889,0.668822868,7.01E-06,6.69E-05,0.006504421
3404,natural_language_inference18,160,WA is a model based on word alignment .,Experiments on Document Modelling,Dataset & Setup for Answer Sentence Selection,natural_language_inference,18,19,1,0,,0.460390702,0,negative,0.00017395,1.78E-05,0.038720498,2.38E-06,4.94E-06,0.002623559,0.347900106,0.000116357,0.000530017,0.599772088,3.21E-05,0.000302383,0.009803788
3405,natural_language_inference18,161,LCLR is the SVM - based classifier trained using a set of features .,Experiments on Document Modelling,Dataset & Setup for Answer Sentence Selection,natural_language_inference,18,20,1,0,,0.580034095,1,negative,7.31E-05,9.81E-06,0.013524792,1.94E-06,2.54E-06,0.003047484,0.380562118,0.000140499,9.27E-05,0.594318086,3.08E-05,0.000249668,0.007946505
3406,natural_language_inference18,162,Model +,,,natural_language_inference,18,0,1,0,,0.001900324,0,negative,0.000408091,0.000331849,0.000470184,2.39E-05,2.06E-05,0.001160557,0.002708183,0.007617083,0.00033229,0.966308758,0.01295177,0.00750734,0.000159434
3407,natural_language_inference18,163,Cnt means that the result is obtained from a combination of a lexical overlap feature and the output from the distributional model .,Model +,Model +,natural_language_inference,18,1,1,0,,0.000212441,0,negative,6.88E-05,1.80E-05,0.00063443,1.97E-06,2.13E-06,0.00016754,0.00013554,0.000235906,0.000323402,0.998342097,5.39E-06,5.86E-05,6.21E-06
3408,natural_language_inference18,164,In order to investigate the effectiveness of our NASM model we also implemented two strong baseline modelsa vanilla LSTM model ( LSTM ) and an LSTM model with a deterministic attention mechanism ( LSTM + Att ) .,Model +,Model +,natural_language_inference,18,2,1,0,,0.431134097,0,negative,0.000906794,0.000158952,0.04444451,1.65E-06,7.34E-06,0.001017699,0.008670283,0.000995392,0.000594091,0.940759426,1.35E-05,0.002400383,2.99E-05
3409,natural_language_inference18,165,The former directly applies the QA matching function ( Eq. 15 ) on the independent question and answer representations which are the last state outputs sq ( | q | ) and s a ( | a | ) from the question and answer LSTM models .,Model +,Model +,natural_language_inference,18,3,1,0,,0.005681578,0,negative,0.000488085,0.000147523,0.019583257,1.68E-06,2.43E-06,0.000126484,0.00029035,0.000240398,0.002076389,0.976485144,6.17E-05,0.000478348,1.82E-05
3410,natural_language_inference18,166,The latter adds an attention model to learn pair- specific representation for prediction on the basis of the vanilla LSTM .,Model +,Model +,natural_language_inference,18,4,1,0,,0.102681406,0,negative,0.002986079,0.00038039,0.210699038,3.44E-06,9.49E-06,0.000309667,0.001360881,0.000358158,0.030274262,0.75268945,4.19E-05,0.000771394,0.000115831
3411,natural_language_inference18,167,"Moreover , LSTM +",Model +,Model +,natural_language_inference,18,5,1,0,,0.000335356,0,negative,0.001309031,8.84E-06,0.01486569,9.87E-08,3.50E-07,3.74E-05,0.000824167,3.59E-05,7.77E-05,0.976610946,1.18E-05,0.006212636,5.42E-06
3412,natural_language_inference18,168,"Att is the deterministic counterpart of NASM , which has the same neural network architecture as NASM .",Model +,Model +,natural_language_inference,18,6,1,0,,0.022588388,0,negative,0.000123726,0.000182381,0.460859149,9.52E-07,3.46E-06,0.000276503,0.004048895,0.000390796,0.001326887,0.531927117,9.00E-05,0.000706942,6.32E-05
3413,natural_language_inference18,169,"The only difference is that it replaces the stochastic units h with deterministic ones , and no inference network is required to carry out stochastic estimation .",Model +,Model +,natural_language_inference,18,7,1,0,,0.001905508,0,negative,0.000752693,0.000184556,0.005613104,1.16E-06,4.48E-06,7.77E-05,0.000213534,0.0002118,0.001689281,0.990750955,7.51E-06,0.000483946,9.23E-06
3414,natural_language_inference18,170,"Following previous work , for each of our models we also add a lexical overlap feature by combining a co-occurrence word count feature with the probability generated from the neural model .",Model +,Model +,natural_language_inference,18,8,1,0,,0.009983186,0,negative,0.00128699,0.000213877,0.003492732,1.89E-06,5.18E-06,0.000718617,0.000632836,0.001498497,0.005491657,0.986305608,2.94E-06,0.000330904,1.83E-05
3415,natural_language_inference18,171,MAP and MRR are adopted as the evaluation metrics for this task .,Model +,Model +,natural_language_inference,18,9,1,0,,0.000462157,0,negative,8.14E-06,1.87E-05,6.19E-05,5.40E-08,2.35E-07,0.000162492,0.000183365,0.001167471,7.35E-05,0.998260048,2.29E-06,6.00E-05,1.78E-06
3416,natural_language_inference18,172,To facilitate direct comparison with previous work we follow the same experimental setup as and .,Model +,Model +,natural_language_inference,18,10,1,0,,4.22E-05,0,negative,2.48E-05,1.98E-05,0.000190648,3.60E-07,1.85E-06,7.32E-05,0.000187646,0.000146542,7.56E-05,0.999152311,1.12E-06,0.000124259,1.85E-06
3417,natural_language_inference18,173,The word embeddings ( K = 50 ) are obtained by running the word2vec tool on the English Wikipedia dump and the AQUAINT 5 corpus .,Model +,Model +,natural_language_inference,18,11,1,1,experiments,0.604584673,1,negative,8.60E-05,0.000148651,7.27E-05,1.09E-05,4.02E-06,0.062407295,0.017113995,0.269021829,0.00054701,0.650197708,8.49E-06,0.000173387,0.000208086
3418,natural_language_inference18,174,"We use LSTMs with 3 layers and 50 hidden units , and apply 40 % dropout after the embedding layer .",Model +,Model +,natural_language_inference,18,12,1,1,experiments,0.840321418,1,hyperparameters,0.000645831,0.000582776,0.000737475,0.000335427,4.99E-05,0.19492511,0.155775338,0.407951176,0.001851733,0.232419024,2.26E-05,0.000695665,0.004007883
3419,natural_language_inference18,175,"For the construction of the inference network , we use an MLP ( Eq. 10 ) with 2 layers and tanh units of 50 dimension , and an MLP ( Eq. 17 ) with 2 layers and tanh units of 150 dimension for modelling the joint representation .",Model +,Model +,natural_language_inference,18,13,1,1,experiments,0.523048243,1,negative,0.000342397,0.000612716,0.00061152,5.75E-05,2.52E-05,0.080988266,0.039051747,0.311467531,0.004694105,0.560912036,1.81E-05,0.000264549,0.000954285
3420,natural_language_inference18,176,"During training we carry out stochastic estimation by taking one sample for computing the gradients , while in prediction we use 20 samples to calculate the expectation of the lower bound .",Model +,Model +,natural_language_inference,18,14,1,1,experiments,0.157633547,0,negative,0.000529714,0.000414161,0.000323158,1.34E-05,2.15E-05,0.017595549,0.022847683,0.081143722,0.000996487,0.875284778,5.27E-06,0.000541955,0.00028263
3421,natural_language_inference18,177,presents the standard deviation of NASM 's MAP scores while using different numbers of samples .,Model +,Model +,natural_language_inference,18,15,1,0,,0.001120524,0,negative,7.30E-05,1.52E-06,2.73E-05,1.77E-07,3.85E-07,4.65E-05,0.000276258,0.000113862,1.07E-05,0.998379057,1.63E-06,0.001065095,4.51E-06
3422,natural_language_inference18,178,"Considering the trade - off between computational cost and variance , we chose 20 samples for prediction in all the experiments .",Model +,Model +,natural_language_inference,18,16,1,0,,0.161900302,0,negative,0.00016258,8.58E-05,3.65E-05,6.43E-06,7.34E-06,0.01375891,0.008472661,0.068999602,0.000188717,0.907687903,2.26E-06,0.000448722,0.000142581
3423,natural_language_inference18,179,"The models are trained using Adam , with hyperparameters selected by optimising the MAP score on the development set .",Model +,Model +,natural_language_inference,18,17,1,0,,0.709806066,1,negative,0.000155409,0.000409383,0.000178628,1.27E-05,6.04E-06,0.036660051,0.012664903,0.303628132,0.002961873,0.642962236,4.84E-06,9.44E-05,0.000261447
3424,natural_language_inference18,180,that the evaluation scripts used by previous work are noisy - 4 out of 72 questions in the test set are treated answered incorrectly .,Model +,Model +,natural_language_inference,18,18,1,0,,6.69E-05,0,negative,2.12E-05,3.34E-07,5.34E-06,4.85E-07,1.15E-06,2.78E-05,3.13E-05,2.19E-05,2.01E-06,0.999793041,2.73E-07,9.37E-05,1.36E-06
3425,natural_language_inference18,181,This makes the MAP and MRR scores ?,Model +,Model +,natural_language_inference,18,19,1,0,,0.000135085,0,negative,9.51E-06,7.03E-07,6.75E-06,1.92E-07,1.67E-07,2.53E-05,2.82E-05,4.67E-05,2.24E-05,0.999824508,3.15E-07,3.43E-05,1.02E-06
3426,natural_language_inference18,182,4 % lower than the true scores .,Model +,Model +,natural_language_inference,18,20,1,0,,0.058086151,0,negative,0.014896201,1.68E-05,0.000354196,1.51E-06,1.73E-05,9.42E-05,0.004526044,0.000240002,3.70E-05,0.90078808,1.10E-06,0.078944909,8.27E-05
3427,natural_language_inference18,183,"Since and use a cleaned - up evaluation scripts , we apply the original noisy scripts to re-evaluate their outputs in A NASM the peso is subdivided into 100 centavos , represented by "" _ UNK _ "" Q2 how much is centavos in mexico A LSTM the actress who played lolita , sue lyon , was fourteen at the time of filming .",Model +,Model +,natural_language_inference,18,21,1,0,,0.000155397,0,negative,4.52E-05,3.66E-06,0.000287633,9.76E-08,3.66E-06,3.29E-05,0.000285,4.10E-05,1.27E-05,0.998747663,2.05E-07,0.000536942,3.30E-06
3428,natural_language_inference18,184,Experiments on Answer Sentence Selection,Model +,,natural_language_inference,18,22,1,0,,0.598658214,1,results,0.003514233,7.13E-05,0.001599078,7.24E-05,0.00044075,0.000472168,0.173884506,0.000272171,2.28E-05,0.219113189,0.000168208,0.593903264,0.006465905
3429,natural_language_inference18,185,"A NASM the actress who played lolita , sue lyon , was fourteen at the time of filming .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,23,1,0,,0.000532041,0,negative,0.000204036,1.01E-07,9.94E-06,4.61E-07,1.75E-06,4.59E-05,0.016357596,8.23E-07,8.80E-07,0.982892884,3.87E-07,7.42E-05,0.00041101
3430,natural_language_inference18,186,MAP and 6 % on MRR .,Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,24,1,0,,0.036814428,0,negative,0.02592896,1.12E-06,0.000205308,8.39E-07,6.05E-06,7.59E-05,0.36252531,2.72E-06,6.54E-06,0.604679522,4.27E-07,0.004396135,0.002171132
3431,natural_language_inference18,187,"The LSTM + Att performs slightly better than the vanilla LSTM model , and our NASM improves the results further .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,25,1,1,experiments,0.980459482,1,experiments,0.003815341,4.10E-08,1.25E-06,1.71E-07,1.84E-07,1.83E-05,0.979573162,1.41E-06,3.01E-08,0.008756518,3.66E-08,0.004980065,0.002853471
3432,natural_language_inference18,188,"Since the QASent dataset is biased towards lexical overlapping features , after combining with a co-occurrence word count feature , our best model NASM outperforms all the previous models , including both neural network based models and classifiers with a set of hand - crafted features ( e.g. LCLR ) .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,26,1,1,experiments,0.987718592,1,experiments,0.005015766,6.96E-08,3.72E-06,8.85E-08,2.79E-07,1.04E-05,0.952204002,6.18E-07,4.41E-08,0.021164326,6.20E-08,0.019124341,0.002476276
3433,natural_language_inference18,189,"Similarly , on the Wik - iQA dataset , all of our models outperform the previous distributional models by a large margin .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,27,1,1,experiments,0.986791633,1,experiments,0.003556734,5.09E-08,9.09E-07,5.13E-08,1.34E-07,8.48E-06,0.971589545,8.89E-07,2.21E-08,0.01449976,3.45E-08,0.008891975,0.001451417
3434,natural_language_inference18,190,"By including a word count feature , our models improve further and achieve the state - of - the - art .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,28,1,1,experiments,0.964912587,1,experiments,0.044364315,6.82E-07,7.18E-06,7.37E-07,1.43E-06,4.28E-05,0.913151855,3.47E-06,1.30E-06,0.030530218,1.04E-07,0.006621653,0.005274273
3435,natural_language_inference18,191,"Notably , on both datasets , our two LSTMbased models have set strong baselines and NASM works even better , which demonstrates the effectiveness of introducing stochastic units to model question semantics in this answer sentence selection task .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,29,1,0,,0.969187306,1,experiments,0.00287333,3.64E-08,1.03E-06,5.89E-08,1.35E-07,1.15E-05,0.956953485,1.05E-06,2.53E-08,0.027621086,4.78E-08,0.011311786,0.001226472
3436,natural_language_inference18,192,Q1 how old was sue lyon when she made lolita,Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,30,1,0,,0.000314173,0,negative,6.35E-05,2.18E-08,2.41E-06,4.37E-08,1.80E-07,1.69E-05,0.012160175,4.30E-07,2.80E-07,0.987596633,1.01E-07,4.40E-05,0.000115331
3437,natural_language_inference18,193,"In , we compare the effectiveness of the latent attention mechanism ( NASM ) and its deterministic counterpart ( LSTM + Att ) by visualising the attention scores on the answer sentences .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,31,1,0,,0.396156779,0,experiments,0.002325485,1.24E-05,0.000551066,2.25E-07,3.77E-06,7.51E-05,0.773626885,3.34E-06,1.99E-05,0.220850973,6.44E-07,0.001056187,0.001474065
3438,natural_language_inference18,194,"For most of the negative answer sentences , neither of the two attention models can attend to reasonable words thatare beneficial for predicting relatedness .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,32,1,0,,0.063483787,0,experiments,0.023065276,2.47E-07,1.13E-05,1.63E-07,2.92E-06,3.27E-05,0.637571475,1.44E-06,2.07E-07,0.327563912,4.30E-08,0.010974574,0.00077579
3439,natural_language_inference18,195,"But for the correct answer sentences , such as the ones in , both attention models are able to capture crucial information by attending to different parts of the sentence based on the question semantics .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,33,1,0,,0.015062732,0,negative,0.003011348,1.52E-07,1.86E-05,1.66E-08,2.21E-07,5.85E-06,0.0311637,3.05E-07,8.81E-07,0.965009712,5.20E-08,0.000756448,3.27E-05
3440,natural_language_inference18,196,"Interestingly , compared to the deterministic counterpart LSTM + Att , our NASM assigns higher attention scores on the prominent words thatare relevant to the question , which forms a more peaked distribution and in turn helps the model achieve better performance .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,34,1,0,,0.925719076,1,experiments,0.059205164,3.75E-07,7.60E-06,1.19E-07,8.55E-07,1.54E-05,0.806619358,1.44E-06,3.07E-07,0.114696433,4.93E-08,0.018036741,0.001416112
3441,natural_language_inference18,197,"In order to have an intuitive observation on the latent distributions , we present Hinton diagrams of their log standard deviation parameters ( .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,35,1,0,,0.000277114,0,negative,0.000118035,1.92E-07,3.51E-06,5.42E-08,7.52E-07,2.26E-05,0.01041496,1.39E-06,1.52E-06,0.989385801,2.04E-08,2.67E-05,2.45E-05
3442,natural_language_inference18,198,"In a Hinton diagram , the size of a square is proportional to a value 's magnitude , and the colour ( black / white ) indicates its sign ( positive / negative ) .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,36,1,0,,0.000256735,0,negative,3.49E-05,2.53E-07,2.00E-05,6.80E-08,5.39E-07,4.98E-05,0.011336361,1.46E-06,8.79E-06,0.988340916,9.44E-08,1.36E-05,0.000193115
3443,natural_language_inference18,199,"In this case , we visualise the parameters order to make the results directly comparable with previous work. of 50 conditional distributions p ? ( h|q ) with the questions selected from 5 different groups , which start with ' howwhere ' .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,37,1,0,,0.000459417,0,negative,3.24E-05,1.11E-07,3.23E-06,2.67E-08,7.06E-07,5.16E-05,0.049640905,2.20E-06,3.73E-07,0.950101932,1.15E-08,3.50E-05,0.000131599
3444,natural_language_inference18,200,All the log standard deviations are initialised as zero before training .,Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,38,1,0,,0.204760259,0,experiments,0.000173509,1.71E-06,4.00E-06,1.39E-07,5.41E-07,0.000885758,0.838289392,0.000181704,1.31E-05,0.159084223,2.86E-08,1.76E-05,0.001348324
3445,natural_language_inference18,201,"According to , we can see that the questions starting with ' how ' have more white are as , which indicates higher variances or more uncertainties are in these dimensions .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,39,1,0,,0.003738146,0,negative,0.002206262,6.08E-08,2.98E-06,5.68E-08,8.71E-07,2.36E-05,0.183206417,1.08E-06,1.72E-07,0.813088717,3.22E-08,0.001205107,0.000264592
3446,natural_language_inference18,202,"By contrast , the questions starting with ' what ' have black squares in almost every dimension .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,40,1,0,,0.000320631,0,negative,0.000217937,3.84E-08,6.84E-06,6.79E-09,3.25E-07,7.07E-06,0.015361995,2.16E-07,2.01E-07,0.984285003,4.85E-09,0.000105201,1.52E-05
3447,natural_language_inference18,203,"Intuitively , it is more difficult to understand and answer the questions starting with ' how ' than the others , while the ' what ' questions commonly have explicit words indicating the possible answers .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,41,1,0,,0.000257352,0,negative,2.52E-05,4.11E-08,7.87E-07,5.84E-08,3.01E-07,1.21E-05,0.002481729,5.04E-07,2.34E-07,0.997398038,6.02E-08,2.08E-05,6.01E-05
3448,natural_language_inference18,204,"To validate this , we compute the stratified MAP scores based on different question type .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,42,1,0,,0.005558146,0,negative,0.000584247,1.01E-06,1.82E-05,9.10E-09,1.03E-06,1.23E-05,0.068221125,9.06E-07,1.95E-06,0.930899119,4.73E-09,0.000222775,3.74E-05
3449,natural_language_inference18,205,The MAP of ' how ' questions is 0.524 which is the lowest among the five groups .,Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,43,1,0,,0.596141203,1,experiments,0.007500682,3.14E-07,2.41E-05,1.03E-06,1.10E-05,0.000126441,0.616364404,3.00E-06,7.21E-07,0.367833949,4.76E-08,0.001272536,0.006861788
3450,natural_language_inference18,206,"Hence empirically , ' how ' questions are harder to ' understand and answer ' .",Model +,Experiments on Answer Sentence Selection,natural_language_inference,18,44,1,0,,0.000790821,0,negative,5.03E-05,6.68E-08,1.42E-06,4.22E-07,6.08E-07,3.88E-05,0.007014992,1.25E-06,3.75E-07,0.992467869,2.10E-07,2.50E-05,0.00039865
3451,natural_language_inference18,207,Discussion,Model +,,natural_language_inference,18,45,1,0,,5.37E-05,0,negative,1.74E-05,1.09E-06,7.33E-06,2.41E-07,5.12E-07,3.48E-05,0.000118146,8.91E-05,2.53E-05,0.999571522,1.20E-07,0.000123267,1.12E-05
3452,natural_language_inference18,208,"As shown in the experiments , neural variational inference brings consistent improvements on the performance of both NLP tasks .",Model +,Discussion,natural_language_inference,18,46,1,0,,0.033406723,0,negative,0.096766684,4.33E-05,3.45E-05,8.75E-07,9.73E-06,1.75E-05,0.002654054,4.68E-05,2.52E-05,0.86942347,1.29E-06,0.030912787,6.38E-05
3453,natural_language_inference18,209,The basic intuition is that the latent distributions grant the ability to sum over all the possibilities in terms of semantics .,Model +,Discussion,natural_language_inference,18,47,1,0,,9.16E-07,0,negative,6.00E-06,3.45E-06,4.17E-06,7.09E-09,2.11E-07,5.22E-07,3.77E-07,1.38E-06,7.08E-05,0.999912333,5.02E-08,5.95E-07,7.61E-08
3454,natural_language_inference18,210,"From the perspective of optimis ation , one of the most important reasons is that Bayesian learning guards against overfitting .",Model +,Discussion,natural_language_inference,18,48,1,0,,2.46E-06,0,negative,1.53E-05,2.17E-06,1.39E-06,3.87E-08,2.99E-07,9.03E-07,1.31E-06,2.59E-06,7.66E-06,0.999961791,1.51E-06,4.67E-06,3.72E-07
3455,natural_language_inference18,211,"According to Eq. 5 in NVDM , since we adopt p ( h ) as a standard Gaussian prior , the KL divergence term D KL [ q ? ( h|X ) p ( h ) ] can be analytically computed as 1 2 ( K ?  2 ? ? 2 + log | diag ( ?",Model +,Discussion,natural_language_inference,18,49,1,0,,1.61E-06,0,negative,0.000621964,1.05E-05,4.88E-05,6.89E-09,8.75E-07,8.43E-07,7.52E-06,1.29E-06,2.90E-05,0.999237354,1.11E-07,4.14E-05,2.68E-07
3456,natural_language_inference18,212,2 ) | ) .,Model +,Discussion,natural_language_inference,18,50,1,0,,2.75E-06,0,negative,1.63E-05,9.35E-07,1.18E-06,1.99E-08,4.34E-07,6.15E-07,4.28E-07,1.78E-06,7.47E-06,0.999969314,1.56E-08,1.50E-06,6.71E-08
3457,natural_language_inference18,213,It is not difficult to find that it actually acts as L2 regulariser when we update the .,Model +,Discussion,natural_language_inference,18,51,1,0,,5.17E-06,0,negative,0.000102617,7.75E-07,3.67E-06,3.30E-09,1.98E-07,2.24E-07,4.38E-07,3.65E-07,6.10E-06,0.999881687,5.45E-09,3.90E-06,1.72E-08
3458,natural_language_inference18,214,"Similarly , in NASM ( Eq. 16 ) , we also have the KL divergence term D KL [ q ? ( h|q , a , y ) ) p ? ( h|q ) ] .",Model +,Discussion,natural_language_inference,18,52,1,0,,5.17E-07,0,negative,1.55E-05,2.56E-06,1.02E-05,2.96E-09,1.34E-07,4.01E-07,6.21E-07,1.35E-06,3.69E-05,0.999931095,1.59E-08,1.16E-06,6.51E-08
3459,natural_language_inference18,215,"Different from NVDM , it attempts to minimise the distance between q ? ( h|q , a , y ) ) and p ? ( h|q ) thatare both conditional distributions .",Model +,Discussion,natural_language_inference,18,53,1,0,,3.40E-06,0,negative,1.96E-05,7.29E-05,0.00014835,4.92E-08,1.30E-06,1.67E-06,4.50E-06,4.64E-06,0.000196308,0.999538022,2.91E-06,8.21E-06,1.48E-06
3460,natural_language_inference18,216,"Because p ? ( h|q ) as well as q ? ( h|q , a , y ) ) are learned during training , the two distributions are mutually restrained while being updated .",Model +,Discussion,natural_language_inference,18,54,1,0,,6.06E-07,0,negative,5.79E-05,6.53E-06,1.06E-05,1.16E-09,2.35E-07,8.75E-08,3.01E-07,4.07E-07,9.26E-05,0.999828406,4.64E-09,2.86E-06,1.57E-08
3461,natural_language_inference18,217,"Therefore , NVDM simply penalises the large and encourages q ? ( h|X ) to approach the prior p ( h ) for every document X , but in NASM , p ? ( h|q ) acts like a moving baseline distribution which regularises the update of q ? ( h|q , a , y ) ) for every different conditions .",Model +,Discussion,natural_language_inference,18,55,1,0,,2.41E-06,0,negative,2.32E-05,5.45E-06,2.87E-05,8.32E-10,1.23E-07,1.31E-07,4.65E-07,3.79E-07,6.17E-05,0.999876944,1.89E-08,2.87E-06,3.08E-08
3462,natural_language_inference18,218,"In practice , we carry out early stopping by observing the prediction performance on development dataset for the question answer selection task .",Model +,Discussion,natural_language_inference,18,56,1,0,,9.90E-05,0,negative,3.14E-05,4.94E-05,5.98E-06,6.52E-08,2.65E-06,7.03E-06,1.02E-05,6.54E-05,7.21E-05,0.999751189,7.13E-08,2.75E-06,1.76E-06
3463,natural_language_inference18,219,"Using the same learning rate and neural network structure , LSTM + Att reaches optimal performance and starts to overfit on training dataset generally at the 20th iteration , while NASM starts to overfit around the 35th iteration .",Model +,Discussion,natural_language_inference,18,57,1,0,,0.01821637,0,negative,0.064257636,3.85E-05,4.89E-05,8.87E-07,1.41E-05,3.77E-05,0.006327696,0.000128959,1.94E-05,0.910314981,2.73E-07,0.01865158,0.000159385
3464,natural_language_inference18,220,"More interestingly , in the question answer selection experiments , NASM learns more peaked attention scores than its deterministic counterpart LSTM + Att . For the update process of LSTM + Att , we find there exists a relatively big variance in the gradients w.r.t. question semantics ( LSTM + Att applies deterministic sq ( | q | ) while NASM applies stochastic h) .",Model +,Discussion,natural_language_inference,18,58,1,0,,0.001045994,0,negative,0.009370565,1.30E-05,5.95E-06,9.47E-08,5.13E-06,3.35E-06,0.000122182,1.09E-05,6.10E-06,0.989705097,7.10E-08,0.000753578,3.99E-06
3465,natural_language_inference18,221,This is because the training dataset is small and contains many negative answer sentences that brings no benefit but noise to the learning of the attention model .,Model +,Discussion,natural_language_inference,18,59,1,0,,1.74E-06,0,negative,2.49E-05,3.22E-07,6.76E-07,6.80E-09,1.07E-06,2.98E-07,4.14E-07,4.19E-07,4.51E-07,0.999969074,1.89E-09,2.34E-06,2.78E-08
3466,natural_language_inference18,222,"In contrast , for the update process of NASM , we observe more stable gradients w.r.t. the parameters of latent distributions .",Model +,Discussion,natural_language_inference,18,60,1,0,,0.002295439,0,negative,0.017846903,1.89E-05,9.91E-06,4.27E-08,3.76E-06,1.39E-06,8.17E-05,6.19E-06,1.17E-05,0.981039058,2.89E-08,0.000978942,1.50E-06
3467,natural_language_inference18,223,The optimis ation of the lower bound on one hand maximises the conditional log -likelihood ( that the deterministic counterpart cares about ) and on the other hand minimises the KL - divergence ( that regularises the gradients ) .,Model +,Discussion,natural_language_inference,18,61,1,0,,1.20E-05,0,negative,5.31E-05,3.21E-05,2.49E-05,5.56E-09,7.54E-07,3.69E-07,1.28E-06,2.01E-06,0.000350587,0.999529816,1.52E-08,4.85E-06,1.44E-07
3468,natural_language_inference18,224,"Hence , each update of the lower bound actually keeps the gradients w.r.t. from swinging heavily .",Model +,Discussion,natural_language_inference,18,62,1,0,,4.09E-06,0,negative,0.000210438,3.51E-06,8.71E-06,2.39E-09,2.91E-07,2.24E-07,1.13E-06,7.56E-07,5.05E-05,0.999719219,2.95E-09,5.12E-06,7.36E-08
3469,natural_language_inference18,225,"Besides , since the values of ?",Model +,Discussion,natural_language_inference,18,63,1,0,,9.74E-08,0,negative,5.91E-05,2.78E-07,2.08E-06,1.00E-09,1.79E-07,9.99E-08,3.44E-07,1.38E-07,1.50E-06,0.99993139,9.83E-10,4.93E-06,7.04E-09
3470,natural_language_inference18,226,"are not very significant in this case , the distribution of attention scores mainly depends on .",Model +,Discussion,natural_language_inference,18,64,1,0,,9.85E-07,0,negative,0.003107857,9.06E-07,7.13E-06,3.15E-08,3.08E-06,8.49E-07,5.14E-06,8.45E-07,2.43E-06,0.9968327,1.86E-09,3.89E-05,1.40E-07
3471,natural_language_inference18,227,"Therefore , the learning of the attention model benefits from the regularis ation as well , and it explains the fact that NASM learns more peaked attention scores which in turn helps achieve a better prediction performance .",Model +,Discussion,natural_language_inference,18,65,1,0,,0.001328578,0,negative,0.005227443,3.35E-06,1.54E-05,1.52E-08,1.38E-06,5.70E-07,1.28E-05,9.98E-07,1.10E-05,0.994549667,6.24E-09,0.000176898,4.53E-07
3472,natural_language_inference18,228,"Since the computations of NVDM and NASM can be parallelised on GPU and only one sample is required during training process , it is very efficient to carry out the neural variational inference .",Model +,Discussion,natural_language_inference,18,66,1,0,,8.13E-05,0,negative,0.000299218,6.03E-05,2.16E-05,2.97E-07,1.04E-05,1.12E-05,5.71E-05,2.85E-05,0.000140981,0.999293824,9.23E-08,6.14E-05,1.51E-05
3473,natural_language_inference18,229,"Moreover , for both NVDM and NASM , all the parameters are updated by backpropagation .",Model +,Discussion,natural_language_inference,18,67,1,0,,8.32E-05,0,negative,8.88E-05,4.63E-05,6.73E-05,5.16E-09,1.49E-06,4.55E-07,2.34E-06,2.41E-06,0.000348419,0.999438503,4.15E-09,3.81E-06,1.52E-07
3474,natural_language_inference18,230,"Thus , the increased computation time for the stochastic units only comes from the added parameters of the inference network .",Model +,Discussion,natural_language_inference,18,68,1,0,,1.10E-06,0,negative,0.000536663,1.40E-06,3.19E-06,5.55E-09,8.18E-07,3.07E-07,1.76E-06,6.20E-07,8.11E-06,0.999436251,2.04E-09,1.07E-05,1.26E-07
3475,natural_language_inference18,231,Related Work,,,natural_language_inference,18,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
3476,natural_language_inference18,246,Conclusion,,,natural_language_inference,18,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
3477,part-of-speech_tagging5,1,title,,,part-of-speech_tagging,5,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
3478,part-of-speech_tagging5,2,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,title,title,part-of-speech_tagging,5,1,1,1,research-problem,0.994796082,1,research-problem,3.00E-08,1.25E-05,1.11E-07,2.42E-08,2.04E-08,1.16E-07,4.49E-07,2.13E-06,7.66E-06,0.002747138,0.997229664,1.09E-07,2.96E-08
3479,part-of-speech_tagging5,3,abstract,,,part-of-speech_tagging,5,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
3480,part-of-speech_tagging5,4,"The rise of neural networks , and particularly recurrent neural networks , has produced significant advances in part - ofspeech tagging accuracy ( Zeman et al. , 2017 ) .",abstract,abstract,part-of-speech_tagging,5,1,1,0,,0.183688306,0,research-problem,5.90E-08,1.18E-05,2.07E-08,1.20E-06,3.97E-07,7.11E-07,5.41E-07,3.99E-06,1.38E-06,0.041204999,0.958774791,4.57E-08,6.88E-08
3481,part-of-speech_tagging5,5,One characteristic common among these models is the presence of rich initial word encodings .,abstract,abstract,part-of-speech_tagging,5,2,1,0,,0.002665163,0,negative,1.05E-06,0.000417426,3.63E-07,2.18E-05,1.48E-05,1.10E-05,1.30E-06,4.66E-05,4.48E-05,0.629404099,0.370036342,1.98E-07,2.13E-07
3482,part-of-speech_tagging5,6,These encodings typically are composed of a recurrent character - based representation with learned and pre-trained word embeddings .,abstract,abstract,part-of-speech_tagging,5,3,1,0,,0.097226219,0,negative,1.73E-06,0.003131946,4.52E-06,1.06E-05,2.09E-05,3.32E-05,3.25E-06,0.000217254,0.00149653,0.631570037,0.363509139,3.50E-07,5.26E-07
3483,part-of-speech_tagging5,7,"However , these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts .",abstract,abstract,part-of-speech_tagging,5,4,1,0,,0.02769785,0,research-problem,5.53E-07,0.000258927,4.14E-07,5.44E-06,4.91E-06,9.31E-06,1.39E-06,4.09E-05,7.28E-05,0.433283559,0.566321377,1.76E-07,1.82E-07
3484,part-of-speech_tagging5,8,"In this paper , we investigate models that use recurrent neural networks with sentence - level context for initial character and word - based representations .",abstract,abstract,part-of-speech_tagging,5,5,1,0,,0.811145175,1,research-problem,9.83E-06,0.080965183,8.06E-05,7.95E-06,0.000125307,2.08E-05,1.81E-05,0.000279312,0.004049125,0.160290087,0.754145392,6.37E-06,1.92E-06
3485,part-of-speech_tagging5,9,In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states .,abstract,abstract,part-of-speech_tagging,5,6,1,0,,0.056691141,0,negative,8.74E-05,0.117163824,2.48E-05,8.20E-06,0.000142752,2.15E-05,1.07E-05,0.000518388,0.012985176,0.684201589,0.184792575,4.16E-05,1.51E-06
3486,part-of-speech_tagging5,10,We present results on part - of - speech and morphological tagging with state - of - the - art performance on a number of languages .,abstract,abstract,part-of-speech_tagging,5,7,1,0,,0.087196686,0,negative,1.34E-05,0.016971371,2.73E-05,0.000109551,0.002379115,4.05E-05,7.40E-05,0.000211372,0.000191669,0.571406128,0.408541612,2.88E-05,5.23E-06
3487,part-of-speech_tagging5,11,Introduction,,,part-of-speech_tagging,5,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
3488,part-of-speech_tagging5,12,Morphosyntactic tagging accuracy has seen dramatic improvements through the adoption of recurrent neural networks - specifically BiLSTMs to create sentence - level context sensitive encodings of words .,Introduction,Introduction,part-of-speech_tagging,5,1,1,0,,0.165991083,0,research-problem,4.11E-06,0.000252679,9.63E-07,2.08E-05,7.40E-05,1.20E-05,2.69E-05,9.69E-06,5.43E-05,0.125287759,0.874249259,3.93E-06,3.63E-06
3489,part-of-speech_tagging5,13,"A successful recipe is to first create an initial context insensitive word representation , which usually has three main parts :",Introduction,Introduction,part-of-speech_tagging,5,2,1,0,,0.345078807,0,research-problem,2.00E-05,0.006091924,1.57E-05,5.26E-06,6.61E-05,2.74E-05,2.35E-05,3.87E-05,0.0066633,0.415315469,0.571720335,1.01E-05,2.20E-06
3490,part-of-speech_tagging5,14,"1 ) A dynamically trained word embedding ; 2 ) a fixed pre-trained word - embedding , induced from a large corpus ; and 3 ) a sub-word character model , which itself is usually the final state of a recurrent model that ingests one character at a time .",Introduction,Introduction,part-of-speech_tagging,5,3,1,0,,0.951742427,1,model,1.13E-05,0.089702714,8.95E-05,4.78E-07,0.000102947,1.41E-05,8.52E-06,3.81E-05,0.89521625,0.013605212,0.001207616,2.54E-06,7.32E-07
3491,part-of-speech_tagging5,15,Such word / sub - word models originated with .,Introduction,Introduction,part-of-speech_tagging,5,4,1,0,,0.007381156,0,negative,3.66E-06,0.001089833,2.75E-06,6.41E-06,2.04E-05,0.000105704,3.19E-05,0.000104551,0.005662361,0.604830251,0.388136731,2.82E-06,2.61E-06
3492,part-of-speech_tagging5,16,"Recently , used precisely such a context insensitive word representation as input to a BiLSTM in order to obtain context sensitive word encodings used to predict partof - speech tags .",Introduction,Introduction,part-of-speech_tagging,5,5,1,0,,0.009175835,0,research-problem,3.42E-06,0.000579668,1.23E-06,1.65E-05,4.38E-05,4.71E-05,2.53E-05,4.53E-05,0.000547574,0.336275454,0.66240882,2.68E-06,3.12E-06
3493,part-of-speech_tagging5,17,The had the highest accuracy of all participating systems in the CoNLL 2017 shared task .,Introduction,Introduction,part-of-speech_tagging,5,6,1,0,,0.0242847,0,negative,0.000962679,0.010477494,0.000196592,0.00043813,0.026293257,0.000557241,0.00504755,0.000270474,0.00099875,0.908603194,0.043291285,0.002727371,0.000135982
3494,part-of-speech_tagging5,18,"In such a model , sub - word character - based representations only interact indirectly via subsequent recurrent layers .",Introduction,Introduction,part-of-speech_tagging,5,7,1,0,,0.126006677,0,model,2.50E-05,0.035402588,0.000105247,1.26E-06,6.45E-05,2.54E-05,1.18E-05,3.62E-05,0.890343375,0.06154361,0.012433916,5.59E-06,1.61E-06
3495,part-of-speech_tagging5,19,"For example , consider the sentence I had shingles , which is a painful disease .",Introduction,Introduction,part-of-speech_tagging,5,8,1,0,,0.001511587,0,negative,3.47E-06,0.001535868,1.99E-06,2.87E-05,0.000475869,0.000147041,2.72E-05,7.92E-05,0.002098049,0.949560216,0.046036007,3.30E-06,3.03E-06
3496,part-of-speech_tagging5,20,"Context insensitive character and word representations may have learned that for unknown or infrequent words like ' shingles ' , ' s ' and more so ' es ' is a common way to end a plural noun .",Introduction,Introduction,part-of-speech_tagging,5,9,1,0,,0.01261405,0,negative,2.72E-05,0.006570333,1.09E-05,3.20E-05,0.000578483,0.000213659,5.25E-05,0.000129946,0.02646572,0.921307197,0.04459534,1.22E-05,4.53E-06
3497,part-of-speech_tagging5,21,It is up to the subsequent BiLSTM layer to override this once it sees the singular verb is to the right .,Introduction,Introduction,part-of-speech_tagging,5,10,1,0,,0.01074021,0,negative,4.29E-05,0.010531085,1.70E-05,3.58E-05,0.000754169,0.00031372,3.16E-05,0.000212378,0.051348638,0.933118908,0.003584002,6.66E-06,3.07E-06
3498,part-of-speech_tagging5,22,Note that this differs from traditional linear models where word and sub-word representations are directly concatenated with similar features in the surrounding context .,Introduction,Introduction,part-of-speech_tagging,5,11,1,0,,0.561516434,1,model,6.19E-05,0.20835669,0.000164122,1.18E-05,0.000715553,9.45E-05,3.38E-05,0.000152685,0.572338682,0.206684631,0.011366513,1.52E-05,3.98E-06
3499,part-of-speech_tagging5,23,In this paper we aim to investigate to what extent having initial sub - word and word context insensitive representations affects performance .,Introduction,Introduction,part-of-speech_tagging,5,12,1,0,,0.427716048,0,negative,0.000115307,0.348538434,6.43E-05,3.68E-05,0.002556791,0.000139679,0.000150618,0.00034656,0.115902564,0.458181176,0.073885803,6.93E-05,1.27E-05
3500,part-of-speech_tagging5,24,We propose a novel model where we learn context sensitive initial character and word representations through two separate sentence - level recurrent models .,Introduction,Introduction,part-of-speech_tagging,5,13,1,1,model,0.930124362,1,model,1.71E-05,0.079657224,0.000127936,1.39E-06,0.000112887,1.94E-05,2.20E-05,2.92E-05,0.911437194,0.006307301,0.00225998,5.42E-06,2.88E-06
3501,part-of-speech_tagging5,25,These are then combined via a meta-BiLSTM model that builds a unified representation of each word that is then used for syntactic tagging .,Introduction,Introduction,part-of-speech_tagging,5,14,1,1,model,0.90160819,1,model,5.62E-06,0.01768736,3.66E-05,1.99E-07,3.50E-05,8.29E-06,4.97E-06,1.11E-05,0.973391634,0.008586635,0.000231057,1.08E-06,4.63E-07
3502,part-of-speech_tagging5,26,"Critically , while each of these three models - character , word and meta - are trained synchronously , they are ultimately separate models using different network configurations , training hyperparameters and loss functions .",Introduction,Introduction,part-of-speech_tagging,5,15,1,0,,0.463420997,0,model,9.47E-05,0.27756296,9.31E-05,3.17E-05,0.004153484,0.000109384,3.86E-05,0.000150297,0.385069255,0.329806605,0.002868308,1.81E-05,3.48E-06
3503,part-of-speech_tagging5,27,"Empirically , we found this optimal as it allowed control over the fact that each representation has a different learning capacity .",Introduction,Introduction,part-of-speech_tagging,5,16,1,0,,0.09542756,0,negative,4.60E-05,0.220879158,3.17E-05,6.97E-05,0.002134725,0.003769311,0.000366646,0.009020435,0.246389884,0.513373718,0.003870452,2.48E-05,2.35E-05
3504,part-of-speech_tagging5,28,We tested the system on the 2017 CoNLL shared task data sets and gain improvements compared to the top performing systems for the majority of languages for part - of - speech and morphological tagging .,Introduction,Introduction,part-of-speech_tagging,5,17,1,0,,0.017373416,0,negative,0.000395361,0.162685024,8.52E-05,2.25E-05,0.003557186,0.000300542,0.003016982,0.000619385,0.024754312,0.789274729,0.01212034,0.003109619,5.88E-05
3505,part-of-speech_tagging5,29,"As we will see , a pattern emerged where gains were largest for morphologically rich languages , especially those in the Slavic family group .",Introduction,Introduction,part-of-speech_tagging,5,18,1,0,,0.760609566,1,negative,0.001219417,0.051653628,6.96E-05,6.99E-05,0.004068151,0.000699805,0.00131433,0.000774246,0.028706206,0.901805974,0.009092809,0.00049569,3.03E-05
3506,part-of-speech_tagging5,30,"We also applied the approach to the benchmark English PTB data , where our model achieved 97.9 using the standard train / dev / test split , which constitutes a relative reduction in error of 12 % over the previous best system .",Introduction,Introduction,part-of-speech_tagging,5,19,1,0,,0.056413641,0,negative,0.000511642,0.156044657,7.97E-05,2.44E-05,0.003036923,0.000403248,0.002332061,0.000793292,0.028966169,0.796161931,0.0094365,0.002165789,4.37E-05
3507,part-of-speech_tagging5,31,Related Work,,,part-of-speech_tagging,5,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
3508,part-of-speech_tagging5,51,Models,,,part-of-speech_tagging,5,0,1,0,,0.013378693,0,negative,0.001769058,0.000281584,0.001312991,0.00016118,0.000150821,0.001858418,0.032853537,0.00468419,0.00015464,0.882857056,0.025147519,0.048083361,0.000685644
3509,part-of-speech_tagging5,52,"In this section , we introduce models that we investigate and experiment within 4 .",Models,Models,part-of-speech_tagging,5,1,1,0,,0.008545831,0,negative,0.000226893,2.27E-05,0.004769147,3.81E-06,9.68E-06,0.000403472,0.01406383,0.000238328,0.000145839,0.979850957,6.81E-06,0.000204013,5.45E-05
3510,part-of-speech_tagging5,53,Sentence - based Character Model,Models,Models,part-of-speech_tagging,5,2,1,0,,0.706463737,1,experiments,0.000119742,6.98E-06,0.017441849,2.94E-06,1.25E-06,0.000358723,0.90991366,0.000157821,3.61E-05,0.055263478,0.000595294,0.008232588,0.007869562
3511,part-of-speech_tagging5,54,The feature that distinguishes our model most from previous work is that we apply a bidirectional recurrent layer ( LSTM ) on all characters of a sentence to induce fully context sensitive initial word encodings .,Models,Models,part-of-speech_tagging,5,3,1,0,,0.206111506,0,negative,0.003425987,0.000130435,0.410130892,1.23E-05,2.13E-05,0.000566579,0.040074274,0.000226047,0.002350337,0.541958196,2.59E-05,0.00078253,0.000295189
3512,part-of-speech_tagging5,55,"That is , we do not restrict the context of this layer to the words themselves ( as in ) .",Models,Models,part-of-speech_tagging,5,4,1,0,,0.003982244,0,negative,0.000677513,1.27E-05,0.01914772,1.41E-06,3.01E-06,0.000123769,0.002633492,8.62E-05,0.000406168,0.976763786,2.88E-06,0.000122014,1.92E-05
3513,part-of-speech_tagging5,56,shows the sentence - based character model applied to an example token in context .,Models,Models,part-of-speech_tagging,5,5,1,0,,0.07018151,0,negative,8.50E-05,4.85E-06,0.039216516,7.42E-07,6.15E-07,0.00030273,0.020056671,0.000132883,0.00054636,0.93933239,1.70E-05,0.000110504,0.000193748
3514,part-of-speech_tagging5,57,"The character model uses , as input , sentences split into UTF8 characters .",Models,Models,part-of-speech_tagging,5,6,1,0,,0.738387231,1,negative,6.10E-05,2.92E-05,0.07657005,9.60E-07,3.05E-06,0.004113201,0.308727431,0.003499144,0.00072045,0.605688068,6.32E-06,0.000112483,0.000468577
3515,part-of-speech_tagging5,58,We include the spaces between the tokens 1 in the input and map each ( a ) Sentence - based Character Model .,Models,Models,part-of-speech_tagging,5,7,1,0,,0.04817285,0,negative,0.000525657,2.30E-05,0.059805276,1.34E-06,1.83E-06,0.001051326,0.028195502,0.001065907,0.002932271,0.90619911,3.33E-06,7.91E-05,0.000116342
3516,part-of-speech_tagging5,59,The representation for the token shingles is the concatenation of the four shaded boxes .,Models,Models,part-of-speech_tagging,5,8,1,0,,0.0778272,0,negative,9.35E-05,1.18E-05,0.140853081,3.05E-07,8.35E-07,0.000642842,0.032119457,0.000296339,0.001919752,0.823903041,2.62E-06,6.31E-05,9.33E-05
3517,part-of-speech_tagging5,60,Note the surrounding sentence context affects the representation .,Models,Models,part-of-speech_tagging,5,9,1,0,,0.032448463,0,negative,0.000199841,5.42E-07,0.000870961,4.40E-07,3.65E-07,0.000152583,0.01131167,6.00E-05,1.63E-05,0.987154428,9.25E-07,0.00020002,3.19E-05
3518,part-of-speech_tagging5,61,character to a dynamically learned embedding .,Models,Models,part-of-speech_tagging,5,10,1,0,,0.418234952,0,negative,0.00026495,1.05E-05,0.066750727,1.39E-06,2.07E-06,0.000274212,0.01996864,0.000202042,0.001061698,0.911112198,4.55E-06,0.000173582,0.000173485
3519,part-of-speech_tagging5,62,"Next , a forward LSTM reads the characters from left to right and a backward LSTM reads sentences from right to left , in standard BiLSTM fashion .",Models,Models,part-of-speech_tagging,5,11,1,0,,0.641818132,1,baselines,0.000127731,1.03E-05,0.753997395,9.23E-08,3.58E-07,5.98E-05,0.012058599,2.55E-05,0.002177269,0.231429919,2.42E-06,6.04E-05,5.02E-05
3520,part-of-speech_tagging5,63,"More formally , for an n-character sentence , we apply for each character embedding ( e char 1 , ... , e char n ) a BiLSTM :",Models,Models,part-of-speech_tagging,5,12,1,0,,0.010607468,0,negative,0.000142637,1.25E-05,0.419527036,1.06E-07,3.69E-07,6.45E-05,0.008976287,3.51E-05,0.001137077,0.569984595,3.14E-06,7.73E-05,3.93E-05
3521,part-of-speech_tagging5,64,"As is also typical , we can have multiple such layers ( l ) that feed into each other through the concatenation of previous layer encodings .",Models,Models,part-of-speech_tagging,5,13,1,0,,0.016117804,0,negative,7.40E-05,7.69E-06,0.007809818,7.78E-07,7.81E-07,0.000149511,0.005366072,0.00011896,0.001028934,0.98530894,5.56E-06,4.67E-05,8.23E-05
3522,part-of-speech_tagging5,65,"The last layer l has both forward ( f l c , 1 , ... , fl c , n ) and backward ( b l c , 1 , ... , bl c , n ) output vectors for each character .",Models,Models,part-of-speech_tagging,5,14,1,0,,0.111763894,0,negative,0.000455812,2.52E-05,0.085682517,1.70E-06,3.16E-06,0.000625241,0.033481838,0.000574721,0.005187571,0.873413811,4.52E-06,0.000113479,0.000430441
3523,part-of-speech_tagging5,66,"To create word encodings , we need to combine a relevant subset of these context sensitive character encodings .",Models,Models,part-of-speech_tagging,5,15,1,0,,0.070669696,0,negative,0.000713418,2.02E-05,0.0199508,7.34E-07,3.58E-06,6.67E-05,0.007750978,5.25E-05,0.000497882,0.970391072,4.23E-06,0.000476015,7.20E-05
3524,part-of-speech_tagging5,67,These word encodings can then be used in a model that assigns morphosyntactic tags to each word directly or via subsequent layers .,Models,Models,part-of-speech_tagging,5,16,1,0,,0.159636326,0,negative,0.000157009,1.60E-05,0.06089503,3.47E-07,1.25E-06,9.59E-05,0.006389877,6.18E-05,0.004121793,0.928122697,2.31E-06,7.36E-05,6.24E-05
3525,part-of-speech_tagging5,68,"To accomplish this , the model concatenates up to four character output vectors : the { forward , backward } output of the { first , last } character in the token ( F 1st ( w ) , F last ( w ) , B 1st ( w ) , B last ( w ) ) .",Models,Models,part-of-speech_tagging,5,17,1,0,,0.397562328,0,negative,0.000258892,6.25E-05,0.21026469,3.31E-07,1.50E-06,0.000478355,0.035537532,0.000434574,0.012704975,0.73996692,4.57E-06,7.10E-05,0.000214095
3526,part-of-speech_tagging5,69,"In , the four shaded boxes indicate these four outputs for the example token .",Models,Models,part-of-speech_tagging,5,18,1,0,,0.008080289,0,negative,1.01E-05,4.36E-07,0.000404384,8.32E-08,2.64E-07,4.73E-05,0.002040319,3.27E-05,3.10E-05,0.997403478,1.38E-07,2.22E-05,7.57E-06
3527,part-of-speech_tagging5,70,"Thus , the proposed model concatenates all four of these and passes it as input to an multilayer perceptron ( MLP ) :",Models,Models,part-of-speech_tagging,5,19,1,0,,0.396290347,0,negative,0.000356598,3.33E-05,0.38998467,3.20E-07,1.24E-06,0.000182713,0.016036295,0.000104007,0.010197917,0.582871756,3.56E-06,8.06E-05,0.000147049
3528,part-of-speech_tagging5,71,A tag can then be predicted with a linear classifier that takes as input the output of the MLP enized / segmented .,Models,Models,part-of-speech_tagging,5,20,1,0,,0.046874694,0,negative,0.000151068,5.62E-06,0.033112021,4.63E-07,1.83E-06,8.20E-05,0.005811332,5.44E-05,0.000572406,0.960029917,1.02E-06,9.35E-05,8.43E-05
3529,part-of-speech_tagging5,72,"m chars i , applies a softmax function and chooses for each word the tag with highest probability .",Models,Models,part-of-speech_tagging,5,21,1,0,,0.037672705,0,negative,0.000457623,7.72E-06,0.143993726,1.00E-07,7.24E-07,5.60E-05,0.014156023,3.37E-05,0.000418679,0.840552242,7.37E-07,0.000273549,4.92E-05
3530,part-of-speech_tagging5,73,Table 8 investigates the empirical impact of alternative definitions of g i that concatenate only subsets,Models,Models,part-of-speech_tagging,5,22,1,0,,0.053080413,0,negative,0.002929974,1.48E-06,0.00132109,1.94E-07,2.47E-06,3.39E-05,0.063246655,2.27E-05,3.32E-06,0.922319004,3.50E-07,0.010077677,4.11E-05
3531,part-of-speech_tagging5,74,Word - based Character Model,Models,Models,part-of-speech_tagging,5,23,1,0,,0.443919683,0,experiments,0.000134053,8.34E-06,0.058016325,2.22E-06,1.76E-06,0.000535312,0.686014903,0.000262621,0.00038536,0.240734833,5.53E-05,0.00315464,0.010694331
3532,part-of-speech_tagging5,75,"To investigate whether a sentence sensitive character model is better than a character model where the context is restricted to the characters of a word , we reimplemented the word - based character model of as shown in .",Models,Models,part-of-speech_tagging,5,24,1,0,,0.340338709,0,negative,0.000539765,2.17E-05,0.061610183,2.79E-07,2.97E-06,0.000495246,0.284817403,0.000305421,0.000401259,0.650948467,8.86E-07,0.000587931,0.000268519
3533,part-of-speech_tagging5,76,"This model uses the final state of a unidirectional LSTM over the characters of the word , combined with the attention mechanism of Cao and Rei ( 2016 ) over all characters .",Models,Models,part-of-speech_tagging,5,25,1,0,,0.223695106,0,baselines,0.00018478,2.05E-05,0.71095269,7.51E-07,1.87E-06,0.000181495,0.025605276,0.000106092,0.004979529,0.257400264,2.27E-06,5.34E-05,0.000511128
3534,part-of-speech_tagging5,77,We refer the reader to those works for more details .,Models,Models,part-of-speech_tagging,5,26,1,0,,0.002033767,0,negative,2.41E-05,5.48E-07,0.000145917,8.76E-07,1.04E-06,8.38E-05,0.004247081,5.69E-05,6.54E-06,0.995285865,2.39E-07,0.000113845,3.32E-05
3535,part-of-speech_tagging5,78,"Critically , however , all the information fed to this representation comes from the word itself , and not a wider sentence - level context .",Models,Models,part-of-speech_tagging,5,27,1,0,,0.006233846,0,negative,0.000108838,1.82E-06,0.001404629,3.56E-07,1.63E-06,3.18E-05,0.001887835,1.98E-05,5.58E-05,0.996389535,3.45E-07,7.17E-05,2.60E-05
3536,part-of-speech_tagging5,79,Sentence - based Word Model,Models,Models,part-of-speech_tagging,5,28,1,0,,0.610239732,1,experiments,8.99E-05,4.22E-06,0.027987014,4.73E-07,6.34E-07,0.00014604,0.865585388,7.34E-05,5.19E-05,0.093062211,2.54E-05,0.005740754,0.007232739
3537,part-of-speech_tagging5,80,We used a similar setup for our context sensitive word encodings as the character encodings .,Models,Models,part-of-speech_tagging,5,29,1,0,,0.007525216,0,negative,5.18E-05,2.56E-06,0.003928475,3.73E-07,2.13E-06,0.000355879,0.040043807,0.000130115,4.09E-05,0.955258125,1.36E-07,8.08E-05,0.000105006
3538,part-of-speech_tagging5,81,There are a few differences .,Models,Models,part-of-speech_tagging,5,30,1,0,,0.000369171,0,negative,2.72E-05,1.87E-07,0.000243936,9.79E-08,4.17E-07,2.49E-05,0.003664119,1.29E-05,1.91E-06,0.995878818,8.73E-08,0.000130532,1.49E-05
3539,part-of-speech_tagging5,82,"Obviously , the inputs are the words of the sentence .",Models,Models,part-of-speech_tagging,5,31,1,0,,0.003649166,0,negative,5.31E-06,3.09E-07,0.000147057,2.66E-08,1.64E-07,1.61E-05,0.000663232,1.97E-05,2.63E-05,0.999105083,3.36E-08,1.14E-05,5.24E-06
3540,part-of-speech_tagging5,83,"For each of the words , we use pretrained word embeddings ( p word The summed embeddings in i are passed as input to one or more BiLSTM layers whose output fl w , i , bl w, i is concatenated and used as the final encoding , which is then passed to an MLP",Models,Models,part-of-speech_tagging,5,32,1,0,,0.152835941,0,negative,0.000395072,1.08E-05,0.067691871,2.77E-07,2.46E-06,0.000158905,0.030230611,0.000134474,0.000640857,0.90035769,1.89E-07,0.000193226,0.000183556
3541,part-of-speech_tagging5,84,"It should be noted , that the output of this BiL - STM is essentially the Dozat et al.",Models,Models,part-of-speech_tagging,5,33,1,0,,0.016605776,0,negative,0.000299333,3.47E-06,0.060989626,5.81E-08,8.97E-07,2.90E-05,0.010206577,1.30E-05,0.000204105,0.928030968,1.20E-07,0.000188413,3.44E-05
3542,part-of-speech_tagging5,85,"model before tag prediction , with the exception that the wordbased character encodings are excluded .",Models,Models,part-of-speech_tagging,5,34,1,0,,0.04278216,0,negative,0.000156261,3.33E-06,0.172872436,3.08E-07,8.31E-07,9.54E-05,0.102427314,3.79E-05,0.000275646,0.721881459,3.48E-06,0.000772306,0.001473297
3543,part-of-speech_tagging5,86,Meta- BiLSTM : Model Combination,Models,Models,part-of-speech_tagging,5,35,1,0,,0.508703368,1,experiments,0.000137385,1.01E-05,0.038360091,2.42E-07,4.28E-07,0.000117734,0.616603946,6.58E-05,0.000252917,0.337061594,2.94E-05,0.003779186,0.003581217
3544,part-of-speech_tagging5,87,"Given initial word encodings , both character and word - based , a common strategy is to pass these through a sentence - level BiLSTM to create context sensitive encodings , e.g. , this is precisely what and do .",Models,Models,part-of-speech_tagging,5,36,1,0,,0.005148899,0,negative,7.74E-05,2.94E-06,0.004748903,3.48E-07,1.43E-06,4.81E-05,0.009312104,2.48E-05,4.85E-05,0.9852526,1.22E-06,0.000312518,0.000169117
3545,part-of-speech_tagging5,88,"However , we found that if we trained each of the character - based and word - based encodings with their own loss , and combined them using an additional meta-BiLSTM model , we obtained optimal performance .",Models,Models,part-of-speech_tagging,5,37,1,0,,0.475927232,0,negative,0.001095748,1.64E-06,0.000440228,9.88E-07,3.11E-06,0.000226678,0.253835338,0.00016915,7.53E-06,0.732360778,3.77E-07,0.011159199,0.000699238
3546,part-of-speech_tagging5,89,"In the meta -BiLSTM model , we concatenate the output , for each word , of its context sensitive character and word - based encodings , and put this through another BiLSTM to create an additional combined context sensitive encoding .",Models,Models,part-of-speech_tagging,5,38,1,0,,0.199216761,0,negative,0.000273637,3.60E-05,0.44767558,1.47E-07,1.81E-06,9.74E-05,0.024444546,7.41E-05,0.009541742,0.517470712,3.62E-07,7.08E-05,0.000313171
3547,part-of-speech_tagging5,90,This is followed by a final MLP whose output is passed to a linear layer for tag prediction .,Models,Models,part-of-speech_tagging,5,39,1,0,,0.176119956,0,negative,0.000512615,2.52E-05,0.264187435,5.36E-07,3.24E-06,0.000136995,0.038430453,9.60E-05,0.006781977,0.68856578,5.61E-07,0.000166957,0.001092239
3548,part-of-speech_tagging5,91,"With this setup , each of the models can be optimized independently which we describe in more detail in 3.5 .",Models,Models,part-of-speech_tagging,5,40,1,0,,0.051291279,0,negative,0.000128408,1.08E-05,0.005595052,1.69E-07,1.68E-06,3.27E-05,0.005524821,4.20E-05,0.000300898,0.988199737,5.06E-08,0.000118518,4.52E-05
3549,part-of-speech_tagging5,92,depicts the architecture of the combined system and contrasts it with that of the Dozat et al. model ) .,Models,Models,part-of-speech_tagging,5,41,1,0,,0.008265312,0,negative,6.41E-05,7.87E-07,0.004981699,1.48E-07,9.99E-07,3.13E-05,0.003929322,1.27E-05,6.94E-05,0.990774425,7.64E-08,6.38E-05,7.12E-05
3550,part-of-speech_tagging5,93,Training Schema,,,part-of-speech_tagging,5,0,1,0,,0.001297679,0,negative,5.02E-05,0.00073763,4.48E-05,7.31E-06,1.14E-05,0.000260691,0.000122195,0.005316556,0.000723627,0.990936633,0.001560544,0.000214254,1.41E-05
3551,part-of-speech_tagging5,94,"As mentioned in the previous section , the character and word - based encoding models have their own tagging loss functions , which are trained independently and joined via the meta-BiLSTM .",Training Schema,Training Schema,part-of-speech_tagging,5,1,1,0,,0.001885341,0,negative,0.000218744,0.000813862,0.001338055,1.70E-06,3.69E-06,0.001078218,0.000105079,0.008384536,0.003544108,0.984426181,1.76E-05,6.18E-05,6.46E-06
3552,part-of-speech_tagging5,95,"I.e. , the loss of each model is minimized independently by separate optimizers with their own hyperparameters .",Training Schema,Training Schema,part-of-speech_tagging,5,2,1,0,,0.00328949,0,negative,0.000279576,0.002642024,0.002536591,3.57E-06,1.09E-05,0.000578022,9.35E-05,0.005123256,0.005428989,0.983164072,3.93E-05,9.01E-05,1.00E-05
3553,part-of-speech_tagging5,96,"Thus , it is in some sense a multitask learning model and we must define a schedule in which individual models are updated .",Training Schema,Training Schema,part-of-speech_tagging,5,3,1,0,,0.000113043,0,negative,1.96E-05,8.91E-05,9.89E-05,1.87E-06,1.65E-06,0.000546649,4.49E-05,0.002308064,0.000863528,0.995926592,8.04E-05,1.32E-05,5.49E-06
3554,part-of-speech_tagging5,97,We opted for a simple synchronous schedule outline in Algorithm,Training Schema,Training Schema,part-of-speech_tagging,5,4,1,0,,0.002378362,0,negative,0.000125949,0.000348553,0.004286442,8.86E-07,3.61E-06,0.000802404,0.000116943,0.002279267,0.001673357,0.990169067,4.39E-05,0.000142079,7.49E-06
3555,part-of-speech_tagging5,98,"1 . Here , during each epoch , we update each of the models in sequence - character , word and meta-using the entire training data .",Training Schema,Training Schema,part-of-speech_tagging,5,5,1,0,,0.003559047,0,negative,0.000199054,0.000974729,0.002020858,5.68E-06,8.16E-06,0.002331769,0.000254776,0.014487641,0.007098393,0.972498086,3.42E-05,5.65E-05,3.02E-05
3556,part-of-speech_tagging5,99,Algorithm,Training Schema,,part-of-speech_tagging,5,6,1,0,,0.000166504,0,negative,7.09E-06,6.67E-05,4.68E-05,1.22E-06,8.31E-07,0.00143103,0.000138774,0.011171568,0.000971425,0.985978003,0.000153153,2.01E-05,1.33E-05
3557,part-of-speech_tagging5,100,1 : Training procedure for learning initial character and word - based context sensitive encodings synchronously with meta-BiLSTM .,Training Schema,Algorithm,part-of-speech_tagging,5,7,1,0,,6.30E-05,0,negative,1.81E-05,2.29E-05,6.01E-05,6.31E-07,3.67E-07,8.80E-05,7.62E-06,0.002267619,0.000101015,0.997362124,2.41E-05,4.27E-05,4.64E-06
3558,part-of-speech_tagging5,101,"In terms of model selection , after each epoch , the algorithm evaluates the tagging accuracy of the development set and keeps the parameters of the best model .",Training Schema,Algorithm,part-of-speech_tagging,5,8,1,0,,0.006978486,0,negative,9.61E-05,0.000651217,0.000629611,2.13E-07,1.39E-06,3.49E-05,3.84E-06,0.002527895,0.000500526,0.995442969,1.22E-05,9.73E-05,1.91E-06
3559,part-of-speech_tagging5,102,"Accuracy is measured using the meta -BiLSTM tagging layer , which requires a forward pass through all three models .",Training Schema,Algorithm,part-of-speech_tagging,5,9,1,0,,2.31E-05,0,negative,1.02E-05,2.17E-05,1.97E-05,1.98E-07,4.28E-07,0.000293354,1.15E-05,0.011236313,1.10E-05,0.988351356,2.15E-06,4.10E-05,1.10E-06
3560,part-of-speech_tagging5,103,"Though we use all three losses to update the models , only the meta-BiLSTM layer is used for model selection and test - time prediction .",Training Schema,Algorithm,part-of-speech_tagging,5,10,1,0,,0.002421567,0,negative,0.001845516,0.000922313,0.000534397,2.47E-06,9.28E-06,0.000176554,2.57E-05,0.010031316,0.000383052,0.985509007,4.14E-06,0.000548694,7.54E-06
3561,part-of-speech_tagging5,104,"While each of the three models - character , word and meta - are trained with their own loss functions , it should be emphasized that training is synchronous in the sense that the meta - BiLSTM model is trained in tandem with the two encoding models , and not after those models have converged .",Training Schema,Algorithm,part-of-speech_tagging,5,11,1,0,,1.73E-06,0,negative,2.36E-05,0.000111004,8.26E-05,2.78E-07,1.11E-06,3.59E-05,2.55E-06,0.001507095,0.000108426,0.998103171,1.38E-06,2.21E-05,8.06E-07
3562,part-of-speech_tagging5,105,"Since accuracy from the meta-BiLSTM model on the development set determines the best parameters , training is not completely independent .",Training Schema,Algorithm,part-of-speech_tagging,5,12,1,0,,4.28E-06,0,negative,9.35E-05,1.05E-05,1.30E-05,3.19E-07,9.72E-07,1.31E-05,1.49E-06,0.000337251,5.09E-06,0.999404841,8.63E-07,0.000118663,3.91E-07
3563,part-of-speech_tagging5,106,We found this to improve accuracy over all .,Training Schema,Algorithm,part-of-speech_tagging,5,13,1,0,,0.000132262,0,negative,0.000156886,7.25E-06,5.51E-06,1.26E-06,1.40E-06,0.000180424,1.09E-05,0.002211217,8.72E-06,0.997082226,1.59E-06,0.000329715,2.90E-06
3564,part-of-speech_tagging5,107,"Crucially , when we allowed the meta -BiLSTM to back - propagate through the whole network , per - formance degraded regardless of whether one or multiple loss functions were used .",Training Schema,Algorithm,part-of-speech_tagging,5,14,1,0,,0.185668798,0,negative,0.05809605,0.00021682,0.000253882,1.20E-05,3.00E-05,0.000384029,0.000281529,0.006079484,5.73E-05,0.813091148,4.02E-05,0.121393964,6.37E-05
3565,part-of-speech_tagging5,108,"Each language could in theory use separate hyperparameters , optimized for highest accuracy .",Training Schema,Algorithm,part-of-speech_tagging,5,15,1,0,,9.99E-06,0,negative,1.18E-05,3.16E-05,2.37E-05,3.07E-07,3.17E-07,0.000114959,3.23E-06,0.00456848,0.000158634,0.995071088,1.38E-06,1.29E-05,1.65E-06
3566,part-of-speech_tagging5,109,"However , for our main experiments we use identical settings for each language which worked well for large corpora and simplified things .",Training Schema,Algorithm,part-of-speech_tagging,5,16,1,0,,1.68E-05,0,negative,2.05E-05,4.69E-05,1.56E-05,4.62E-07,2.73E-06,8.88E-05,8.95E-06,0.002317534,9.40E-06,0.997360421,1.18E-06,0.000126461,1.01E-06
3567,part-of-speech_tagging5,110,We provide an overview of the selected hyperparameters in 4.1 .,Training Schema,Algorithm,part-of-speech_tagging,5,17,1,0,,2.14E-06,0,negative,1.00E-05,1.47E-05,1.40E-06,3.80E-06,1.26E-06,0.000100329,1.62E-06,0.002809007,1.42E-05,0.997034116,3.39E-07,8.44E-06,7.89E-07
3568,part-of-speech_tagging5,111,We explored more settings for selected individual languages with a grid search and ablation experiments and present the results in 5 .,Training Schema,Algorithm,part-of-speech_tagging,5,18,1,0,,0.003526302,0,negative,0.001797389,0.000200774,0.00010762,6.71E-06,3.77E-05,0.000415804,6.89E-05,0.00501814,4.00E-05,0.989806725,1.68E-06,0.002486479,1.21E-05
3569,part-of-speech_tagging5,112,Experiments and Results,,,part-of-speech_tagging,5,0,1,0,,0.022779798,0,negative,2.87E-05,1.11E-05,2.71E-06,1.71E-07,6.24E-07,1.96E-05,9.54E-05,0.000143494,1.93E-06,0.997728407,0.000542957,0.001423986,9.31E-07
3570,part-of-speech_tagging5,113,"In this section , we present the experimental setup and the selected hyperparameter for the main experiments where we use the CoNLL Shared Task 2017 treebanks and compare with the best systems of the shared task .",Experiments and Results,Experiments and Results,part-of-speech_tagging,5,1,1,0,,0.000456192,0,negative,0.000348238,3.51E-06,0.000170709,1.67E-06,1.68E-06,0.000123868,0.000633944,0.000122895,1.16E-06,0.996028574,1.86E-06,0.002545716,1.62E-05
3571,part-of-speech_tagging5,114,Experimental Setup,,,part-of-speech_tagging,5,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
3572,part-of-speech_tagging5,115,"For our main results , we selected one network configuration and set of the hyperparameters .",Experimental Setup,Experimental Setup,part-of-speech_tagging,5,1,1,0,,0.190034722,0,hyperparameters,0.000147789,5.78E-05,0.000337479,1.51E-05,3.62E-06,0.142784509,0.001851275,0.477281178,2.56E-05,0.376942215,1.42E-05,0.000520404,1.89E-05
3573,part-of-speech_tagging5,116,These settings are not optimal for all languages .,Experimental Setup,Experimental Setup,part-of-speech_tagging,5,2,1,0,,0.07620904,0,negative,7.43E-05,1.38E-05,0.001069278,1.25E-05,1.28E-06,0.013433178,0.001666546,0.033168713,1.22E-05,0.948364411,0.000587228,0.001567452,2.91E-05
3574,part-of-speech_tagging5,117,"However , since hyperparameter exploration is computationally demanding due to the number of languages we optimized these hyperparameters on initial development data experiments over a few languages .",Experimental Setup,Experimental Setup,part-of-speech_tagging,5,3,1,0,,0.045775674,0,negative,0.00029109,4.92E-05,0.000741388,1.16E-05,5.28E-06,0.092154625,0.00115012,0.203490793,2.55E-05,0.701285413,2.00E-05,0.000757862,1.72E-05
3575,part-of-speech_tagging5,118,"shows an overview of the architecture , hyperparameters and the initialization settings of the network .",Experimental Setup,Experimental Setup,part-of-speech_tagging,5,4,1,0,,0.022337005,0,negative,5.93E-05,2.57E-05,0.000966352,2.24E-05,3.96E-06,0.023160491,0.000656983,0.070730462,4.96E-05,0.903993423,2.45E-05,0.000283965,2.29E-05
3576,part-of-speech_tagging5,119,The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training .,Experimental Setup,Experimental Setup,part-of-speech_tagging,5,5,1,1,hyperparameters,0.980541005,1,hyperparameters,1.10E-05,2.33E-05,0.00011929,7.17E-07,1.70E-07,0.058034354,0.000376198,0.905705013,2.53E-05,0.035681799,2.54E-06,1.44E-05,6.03E-06
3577,part-of-speech_tagging5,120,The dropout used on the embeddings is achieved by a RRIE is the relative reduction in error .,Experimental Setup,Experimental Setup,part-of-speech_tagging,5,6,1,1,hyperparameters,0.069561764,0,negative,0.00031393,6.90E-05,0.012718097,5.41E-06,3.39E-06,0.027046648,0.001174769,0.119943601,7.15E-05,0.83796401,2.77E-05,0.000631848,3.02E-05
3578,part-of-speech_tagging5,121,We excluded ties in the calculation of macro-avg since these treebanks do not contain meaningful xpos tags .,Experimental Setup,Experimental Setup,part-of-speech_tagging,5,7,1,0,,0.070687987,0,negative,9.09E-05,2.13E-05,0.002429,1.21E-06,1.64E-06,0.060701599,0.000800601,0.171505986,3.27E-05,0.7640503,7.57E-06,0.000348379,8.76E-06
3579,part-of-speech_tagging5,122,Data Sets,Experimental Setup,,part-of-speech_tagging,5,8,1,0,,0.013610506,0,negative,1.38E-05,3.16E-06,0.006920673,1.64E-07,4.59E-07,0.00363024,0.001941787,0.009536241,1.98E-06,0.975773125,1.18E-05,0.002162029,4.54E-06
3580,part-of-speech_tagging5,123,"For the experiments , we use the data sets as provided by the CoNLL Shared Task 2017 .",Experimental Setup,Data Sets,part-of-speech_tagging,5,9,1,0,,0.000588171,0,negative,8.67E-06,1.05E-05,3.38E-05,2.23E-06,9.67E-06,0.006194162,5.21E-05,0.003094314,9.84E-07,0.990564408,7.39E-07,2.80E-05,5.14E-07
3581,part-of-speech_tagging5,124,"For training , we use the training sets which were denoted as big treebanks",Experimental Setup,Data Sets,part-of-speech_tagging,5,10,1,0,,0.001302658,0,negative,6.70E-06,3.99E-06,4.90E-05,2.40E-07,3.88E-06,0.005283301,5.06E-05,0.004815741,7.93E-07,0.989752205,3.37E-07,3.29E-05,3.19E-07
3582,part-of-speech_tagging5,125,2 .,Experimental Setup,Data Sets,part-of-speech_tagging,5,11,1,0,,4.42E-05,0,negative,1.02E-05,8.17E-07,7.79E-06,7.42E-08,7.45E-08,0.001884177,4.44E-06,0.001666507,1.97E-06,0.996411286,7.61E-07,1.18E-05,1.59E-07
3583,part-of-speech_tagging5,126,We followed the same methodology used in the CoNLL Shared Task .,Experimental Setup,Data Sets,part-of-speech_tagging,5,12,1,0,,2.13E-05,0,negative,1.21E-05,3.67E-06,0.000136643,6.58E-07,2.60E-06,0.004000594,8.03E-05,0.001160402,1.81E-06,0.994467604,3.70E-06,0.000128835,1.09E-06
3584,part-of-speech_tagging5,127,We use the training treebank for training only and the development sets for hyperparameter tuning and early stopping .,Experimental Setup,Data Sets,part-of-speech_tagging,5,13,1,0,,0.062800981,0,negative,1.91E-05,4.24E-05,6.05E-05,1.19E-06,1.36E-06,0.102960392,0.000155815,0.279540059,1.61E-05,0.617178213,8.02E-07,2.19E-05,2.09E-06
3585,part-of-speech_tagging5,128,"To keep our results comparable with the Shared Task , we use the provided precomputed word embeddings .",Experimental Setup,Data Sets,part-of-speech_tagging,5,14,1,0,,0.022608049,0,negative,9.23E-05,5.44E-05,0.000159168,1.22E-06,2.93E-06,0.087253527,0.000326986,0.12938322,1.36E-05,0.782536049,9.02E-07,0.00017261,3.01E-06
3586,part-of-speech_tagging5,129,We excluded Gothic from our experiments as the available downloadable content did not include embeddings for this language .,Experimental Setup,Data Sets,part-of-speech_tagging,5,15,1,0,,0.000280899,0,negative,8.15E-06,8.71E-07,9.83E-06,4.36E-07,2.33E-06,0.005500513,1.34E-05,0.001605216,4.92E-07,0.992839227,1.91E-07,1.91E-05,2.34E-07
3587,part-of-speech_tagging5,130,As input to our system - for both part - ofspeech tagging and morphological tagging - we use the output of the UDPipe - base baseline system which provides segmentation .,Experimental Setup,Data Sets,part-of-speech_tagging,5,16,1,0,,0.001768877,0,negative,6.48E-06,7.79E-06,0.000201489,4.82E-07,4.24E-06,0.016473657,0.000134414,0.010567848,3.89E-06,0.972530663,7.48E-07,6.67E-05,1.58E-06
3588,part-of-speech_tagging5,131,The segmentation differs from the gold segmentation and impacts accuracy negatively for a number of languages .,Experimental Setup,Data Sets,part-of-speech_tagging,5,17,1,0,,0.148839245,0,negative,0.000307357,2.44E-06,0.000228265,1.81E-07,1.21E-06,0.000159104,3.82E-05,0.000140001,3.87E-07,0.992063224,1.01E-05,0.007048175,1.30E-06
3589,part-of-speech_tagging5,132,Most of the top performing systems for part - of - speech tagging used as input UDPipe to obtain the segmentation for the input data .,Experimental Setup,Data Sets,part-of-speech_tagging,5,18,1,0,,0.003481499,0,negative,9.45E-06,1.25E-06,4.50E-05,1.01E-06,8.82E-07,0.001232236,8.87E-05,0.000691952,4.00E-07,0.99756001,0.000186165,0.000178872,4.00E-06
3590,part-of-speech_tagging5,133,"For morphology , the top system for most languages ( IMS ) used its own segmentation .",Experimental Setup,Data Sets,part-of-speech_tagging,5,19,1,0,,0.01570463,0,negative,9.20E-05,2.85E-06,0.000419,9.40E-06,1.24E-05,0.004350986,0.000327866,0.000720403,1.96E-06,0.993336814,0.000122627,0.000587575,1.62E-05
3591,part-of-speech_tagging5,134,"For the evaluation , we used the official evaluation script .",Experimental Setup,Data Sets,part-of-speech_tagging,5,20,1,0,,0.000395924,0,negative,2.06E-06,4.19E-07,8.97E-06,5.55E-08,6.16E-07,0.001166468,1.60E-05,0.000804396,1.71E-07,0.997957691,1.01E-07,4.30E-05,1.17E-07
3592,part-of-speech_tagging5,135,Part - of - Speech Tagging Results,Experimental Setup,,part-of-speech_tagging,5,21,1,1,results,0.867856401,1,results,0.00021786,3.31E-06,0.004252106,3.29E-07,7.90E-07,0.000527924,0.016600817,0.001737504,6.55E-07,0.105929224,2.62E-05,0.870643387,5.99E-05
3593,part-of-speech_tagging5,136,"In this section , we present the results of the application of our model to part - of - speech tagging .",Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,22,1,0,,0.206063535,0,negative,0.000307015,7.47E-07,6.71E-05,1.26E-07,1.80E-06,0.000155088,0.018644362,9.43E-06,4.06E-07,0.962567213,3.69E-07,0.018205047,4.13E-05
3594,part-of-speech_tagging5,137,"In our first experiment , we used our model in the setting of the CoNLL 2017 Shared Task to annotate words with XPOS 3 tags .",Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,23,1,0,,0.550052229,1,negative,0.000322625,2.83E-06,0.000222163,5.22E-07,7.86E-06,0.00060461,0.05450271,3.27E-05,1.16E-06,0.925663068,3.82E-07,0.018492778,0.000146543
3595,part-of-speech_tagging5,138,We compare our results against the top systems of the CoNLL 2017 Shared Task .,Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,24,1,0,,0.167770171,0,negative,1.52E-05,1.32E-07,1.06E-05,5.79E-08,3.95E-07,0.000170121,0.006094223,1.00E-05,9.17E-08,0.991451778,9.28E-08,0.002226548,2.08E-05
3596,part-of-speech_tagging5,139,contains the results of this task for the large treebanks .,Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,25,1,0,,0.01831273,0,negative,3.46E-05,2.79E-08,2.08E-05,7.90E-07,4.81E-06,0.000136755,0.001202247,1.38E-06,6.85E-08,0.997901334,1.09E-07,0.000654702,4.24E-05
3597,part-of-speech_tagging5,140,"Because won the challenge for the majority of the languages , we first compare our results with the performance of their system .",Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,26,1,0,,0.008264202,0,negative,9.84E-06,4.27E-08,2.68E-06,1.34E-08,9.04E-08,6.98E-05,0.001039782,4.40E-06,5.80E-08,0.998441606,2.86E-08,0.00043005,1.58E-06
3598,part-of-speech_tagging5,141,Our model outperforms in 32 of the 54 treebanks with 13 ties .,Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,27,1,1,results,0.875731653,1,results,0.000681868,1.33E-07,2.34E-05,3.26E-07,5.58E-07,0.0003127,0.39596101,2.80E-05,6.41E-08,0.098681277,3.73E-07,0.502598932,0.001711385
3599,part-of-speech_tagging5,142,These ties correspond mostly to languages where XPOS tagging anyhow obtains accuracies above 99 % .,Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,28,1,0,,0.045646322,0,negative,1.64E-05,1.73E-08,9.99E-06,3.25E-08,3.47E-07,5.06E-05,0.000775447,1.36E-06,3.89E-08,0.998588442,5.87E-08,0.000547621,9.59E-06
3600,part-of-speech_tagging5,143,"Our model tends to produce better results , especially for morphologically rich languages ( e.g. Slavic",Experimental Setup,Part - of - Speech Tagging Results,part-of-speech_tagging,5,29,1,1,results,0.932583469,1,results,0.000897068,4.10E-08,1.47E-05,6.34E-08,1.53E-07,0.00010332,0.162490109,1.35E-05,2.53E-08,0.205896958,2.55E-07,0.630305494,0.00027834
3601,part-of-speech_tagging5,144,System,,,part-of-speech_tagging,5,0,1,0,,0.000753896,0,negative,0.000130436,0.000731751,0.000115647,6.11E-06,6.78E-06,0.000218556,0.000211513,0.002769048,0.001114849,0.978150168,0.015527904,0.000976566,4.07E-05
3602,part-of-speech_tagging5,145,Accuracy 97.50 97.64 . 97.44 97.41 ours 97.96,System,System,part-of-speech_tagging,5,1,1,0,,0.002413914,0,negative,0.000652097,1.10E-05,1.88E-05,6.36E-06,5.11E-06,0.000209538,0.000495652,0.000601025,2.81E-05,0.966190122,8.09E-05,0.031661555,3.98E-05
3603,part-of-speech_tagging5,146,Part - of - Speech Tagging on WSJ,System,System,part-of-speech_tagging,5,2,1,0,,0.85640608,1,negative,0.000988412,0.001173278,0.00144867,1.42E-05,2.08E-05,0.000188236,0.006860232,0.001047625,0.000358789,0.650228073,0.098030896,0.238945392,0.000695395
3604,part-of-speech_tagging5,147,"We also performed experiments on the Penn Treebank with the usual split in train , development and test set .",System,System,part-of-speech_tagging,5,3,1,0,,0.020967084,0,negative,0.000797284,0.000199042,8.65E-05,6.55E-07,1.10E-05,7.78E-05,0.000132945,0.000321133,6.47E-05,0.994275072,6.12E-06,0.004025382,2.41E-06
3605,part-of-speech_tagging5,148,shows the results of our model in comparison to the results reported in state - of the - art literature .,System,System,part-of-speech_tagging,5,4,1,0,,0.024193079,0,negative,8.03E-05,2.71E-06,2.43E-06,7.87E-08,2.37E-07,1.56E-05,2.56E-05,8.47E-05,4.03E-06,0.997233665,5.46E-06,0.002544328,8.05E-07
3606,part-of-speech_tagging5,149,"Our model significantly outperforms these systems , with an absolute difference of 0.32 % in accuracy , which corresponds to a RRIE of 12 % .",System,System,part-of-speech_tagging,5,5,1,0,,0.768515392,1,results,0.013180549,5.17E-05,0.000103141,7.88E-06,1.57E-05,6.96E-05,0.002798149,0.000335009,1.91E-05,0.070330016,4.73E-05,0.912797508,0.000244416
3607,part-of-speech_tagging5,150,Morphological Tagging Results,,,part-of-speech_tagging,5,0,1,1,results,0.581380419,1,negative,0.000945686,9.26E-05,0.000142072,2.59E-07,7.45E-07,2.60E-05,0.000818465,0.000311199,1.57E-05,0.755731047,0.020306299,0.221597876,1.20E-05
3608,part-of-speech_tagging5,151,"In addition to the XPOS tagging experiments , we performed experiments with morphological tagging .",Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,1,1,0,,0.004543537,0,negative,0.003288876,6.68E-07,0.000555759,4.92E-07,2.61E-07,3.71E-06,2.85E-05,1.06E-05,2.11E-06,0.96691724,1.71E-06,0.029176708,1.34E-05
3609,part-of-speech_tagging5,152,This annotation was part of the CONLL 2017 Shared Task and the objective was to predict a bundle of morphological features for each token in the text .,Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,2,1,0,,0.000577562,0,negative,0.003193085,5.03E-07,0.000206063,0.003154626,1.31E-05,8.82E-05,0.000108284,1.23E-05,5.19E-06,0.978984246,2.33E-05,0.009352716,0.004858443
3610,part-of-speech_tagging5,153,Our model treats the morphological bundle as one tag making the problem equivalent to a sequential tagging problem .,Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,3,1,0,,0.000375208,0,negative,0.00259499,1.10E-05,0.010622105,4.82E-07,1.66E-07,4.41E-06,8.73E-06,3.41E-05,0.000215219,0.978620757,1.57E-05,0.007839523,3.28E-05
3611,part-of-speech_tagging5,154,shows the results .,Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,4,1,0,,2.77E-05,0,negative,7.79E-05,2.70E-08,1.26E-05,1.26E-08,4.30E-09,2.73E-07,2.02E-06,2.46E-06,1.50E-07,0.992515582,2.99E-07,0.007386346,2.31E-06
3612,part-of-speech_tagging5,155,"Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task ( i.e. , 1.8 % absolute improvement on average , corresponding to a RRIE of 21.20 % ) .",Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,5,1,1,results,0.847979154,1,results,0.000920668,1.47E-08,3.71E-06,5.95E-08,4.77E-09,3.01E-07,8.10E-05,1.65E-06,1.39E-08,0.021362882,4.71E-07,0.977592226,3.70E-05
3613,part-of-speech_tagging5,156,"The only cases for which this is not true are again languages that require significant segmentation efforts ( i.e. , Hebrew , Chinese , Vietnamese and Japanese ) or when the task was trivial .",Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,6,1,0,,0.000101574,0,negative,0.000144173,5.22E-08,6.68E-06,1.42E-07,1.67E-08,1.19E-06,4.89E-06,5.21E-06,2.02E-07,0.990712331,1.81E-06,0.009110468,1.28E-05
3614,part-of-speech_tagging5,157,"Given the fact that obtained the best results in part - of - speech tagging by a significant margin in the CoNLL 2017 Shared Task , it would be expected that their model would also perform significantly well in morphological tagging since the tasks are very similar .",Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,7,1,0,,0.00017906,0,results,0.000996076,6.39E-08,0.000121402,6.83E-08,1.46E-08,8.40E-07,2.28E-05,2.94E-06,1.73E-07,0.498732988,2.36E-06,0.500095143,2.52E-05
3615,part-of-speech_tagging5,158,"Since they did not participate in this particular challenge , we decided to reimplement their system to serve",Morphological Tagging Results,Morphological Tagging Results,part-of-speech_tagging,5,8,1,0,,4.55E-06,0,negative,0.000320119,8.63E-08,7.82E-05,2.45E-08,2.04E-08,2.88E-07,8.20E-07,1.59E-06,5.25E-07,0.995851275,1.25E-07,0.003745917,1.02E-06
3616,part-of-speech_tagging5,159,Ablation Study,,,part-of-speech_tagging,5,0,1,0,,0.006818876,0,negative,0.046739945,0.000279772,0.002808551,0.000142874,8.76E-05,0.000276199,0.001210224,0.00046093,0.000147782,0.934438164,0.002965146,0.010376843,6.60E-05
3617,part-of-speech_tagging5,160,The model proposed in this paper of a Meta - BiLSTM with a sentence - based character model differs from prior work in multiple aspects .,Ablation Study,Ablation Study,part-of-speech_tagging,5,1,1,0,,0.0194265,0,negative,0.482910358,0.000152194,0.014607858,2.66E-06,4.03E-05,5.87E-05,6.82E-05,4.59E-06,0.000660215,0.499697592,0.001353504,2.13E-05,0.000422544
3618,part-of-speech_tagging5,161,"In this section , we perform ablations to determine the relative impact of each modeling decision .",Ablation Study,Ablation Study,part-of-speech_tagging,5,2,1,0,,0.048797176,0,ablation-analysis,0.702738717,2.16E-05,0.002172909,1.67E-06,5.66E-05,2.18E-05,2.71E-05,1.30E-06,7.95E-05,0.294826958,8.47E-06,8.30E-06,3.53E-05
3619,part-of-speech_tagging5,162,"For the experimental setup of the ablation experiments , we report accuracy scores for the development sets .",Ablation Study,Ablation Study,part-of-speech_tagging,5,3,1,0,,0.00289173,0,negative,0.034981794,2.78E-06,0.000120346,1.27E-06,4.61E-05,3.77E-05,2.11E-05,3.55E-06,9.79E-06,0.964730357,5.56E-06,3.10E-06,3.65E-05
3620,part-of-speech_tagging5,163,We split off 5 % of the sentences from each training corpus and we use this part for early stopping .,Ablation Study,Ablation Study,part-of-speech_tagging,5,4,1,0,,0.622054876,1,negative,0.414911821,0.000351911,0.003494837,4.26E-05,0.000212035,0.008716454,0.00298107,0.002197583,0.001926379,0.54506034,4.99E-05,6.42E-06,0.020048648
3621,part-of-speech_tagging5,164,Ablation experiments are either performed on a few selected treebanks to show individual language results or averaged across all treebanks for which tagging is non-trivial .,Ablation Study,Ablation Study,part-of-speech_tagging,5,5,1,0,,0.005288591,0,ablation-analysis,0.527192778,8.00E-06,0.002327884,2.04E-06,9.78E-05,2.53E-05,2.56E-05,1.11E-06,2.35E-05,0.470197596,1.33E-05,7.36E-06,7.78E-05
3622,part-of-speech_tagging5,165,Impact of the Training Schema,Ablation Study,,part-of-speech_tagging,5,6,1,0,,0.731855308,1,ablation-analysis,0.996009282,7.71E-08,1.82E-05,1.92E-06,3.17E-06,1.76E-06,2.97E-06,2.90E-08,4.86E-07,0.00394039,3.31E-07,2.02E-06,1.93E-05
3623,part-of-speech_tagging5,166,"We first compare jointly training the three model components ( Meta - BiLSTM , character model , word model ) to training each separately .",Ablation Study,Impact of the Training Schema,part-of-speech_tagging,5,7,1,0,,0.113552614,0,negative,0.04566438,0.000612594,0.001946111,3.80E-07,1.25E-05,2.27E-05,4.16E-05,2.54E-05,0.000548042,0.950618731,3.64E-05,0.000469084,2.09E-06
3624,part-of-speech_tagging5,167,shows that separately optimized models are significantly more accurate on average than jointly optimized models .,Ablation Study,Impact of the Training Schema,part-of-speech_tagging,5,8,1,1,ablation-analysis,0.851323106,1,ablation-analysis,0.939645829,4.20E-06,4.05E-05,6.79E-07,4.93E-06,4.84E-06,0.000104274,3.95E-06,2.37E-06,0.05628307,8.36E-06,0.003887975,9.00E-06
3625,part-of-speech_tagging5,168,Separate optimization leads to better accuracy for 34 out of 40 treebanks for the morphological features task and for 30 out of 39 treebanks for xpos tagging .,Ablation Study,Impact of the Training Schema,part-of-speech_tagging,5,9,1,1,ablation-analysis,0.863436182,1,ablation-analysis,0.954912566,1.33E-05,4.83E-05,2.03E-06,6.27E-06,1.61E-05,0.000283413,1.51E-05,7.51E-06,0.035918489,2.76E-05,0.008698563,5.08E-05
3626,part-of-speech_tagging5,169,"Separate optimization outperformed joint optimization by up to 2.1 percent absolute , while joint never out - performed separate by more than 0.5 % absolute .",Ablation Study,Impact of the Training Schema,part-of-speech_tagging,5,10,1,1,ablation-analysis,0.840805602,1,ablation-analysis,0.961887335,1.48E-05,0.000171552,5.11E-06,2.09E-05,2.58E-05,0.000254338,9.95E-06,1.34E-05,0.032974706,2.40E-05,0.00450451,9.37E-05
3627,part-of-speech_tagging5,170,We hypothesize that separately training the models forces each submodel ( word and character ) to be strong enough to make high accuracy predictions and in some sense serves as a regularizer in the same way that dropout does for individual neurons .,Ablation Study,Impact of the Training Schema,part-of-speech_tagging,5,11,1,0,,0.000486451,0,negative,0.001263202,0.00020034,0.000157038,1.45E-06,1.06E-05,2.88E-05,2.89E-06,5.37E-05,0.002944984,0.995310678,1.48E-05,8.07E-06,3.51E-06
3628,part-of-speech_tagging5,171,optimization .,Ablation Study,Impact of the Training Schema,part-of-speech_tagging,5,12,1,0,,0.012876188,0,negative,0.000789352,3.46E-05,1.97E-05,8.18E-07,3.33E-06,5.27E-05,5.54E-06,9.25E-05,0.00014203,0.998827458,2.07E-05,8.92E-06,2.26E-06
3629,part-of-speech_tagging5,172,Impact of the Sentence - based Character Model,Ablation Study,,part-of-speech_tagging,5,13,1,0,,0.502374361,1,ablation-analysis,0.994397604,3.13E-07,0.000115525,2.88E-07,1.49E-06,2.05E-06,2.84E-05,6.58E-08,1.60E-06,0.005303017,5.22E-06,1.30E-05,0.000131507
3630,part-of-speech_tagging5,173,We compared the setup with sentence - based character context ) to word - based character context ) .,Ablation Study,Impact of the Sentence - based Character Model,part-of-speech_tagging,5,14,1,0,,0.048004733,0,negative,0.006060854,6.96E-06,0.000479375,9.88E-08,2.13E-06,4.59E-06,7.30E-05,2.15E-06,1.74E-05,0.992496137,2.66E-06,0.000851722,2.91E-06
3631,part-of-speech_tagging5,174,We selected for these experiments a number of morphological rich languages .,Ablation Study,Impact of the Sentence - based Character Model,part-of-speech_tagging,5,15,1,0,,0.000129246,0,negative,0.000371641,1.36E-06,1.31E-05,9.83E-07,2.62E-05,1.05E-05,1.93E-05,2.69E-06,2.89E-06,0.999471924,1.60E-06,6.84E-05,9.37E-06
3632,part-of-speech_tagging5,175,The results are shown in .,Ablation Study,Impact of the Sentence - based Character Model,part-of-speech_tagging,5,16,1,0,,0.000734761,0,negative,0.007055226,1.20E-07,6.71E-06,4.93E-09,6.34E-08,3.37E-07,2.35E-05,2.48E-07,6.10E-07,0.989972814,4.73E-07,0.002939246,6.48E-07
3633,part-of-speech_tagging5,176,The accuracy of the word - based character model joint with a word - based model were significantly lower than a sentence - based character model .,Ablation Study,Impact of the Sentence - based Character Model,part-of-speech_tagging,5,17,1,0,,0.809413181,1,ablation-analysis,0.536423846,9.68E-07,3.23E-05,4.00E-07,1.40E-06,4.16E-06,0.000602317,2.43E-06,2.08E-06,0.379995866,7.48E-06,0.08283103,9.57E-05
3634,part-of-speech_tagging5,177,We conclude Here we investigate the part - ofspeech tagging performance of the joint model compared with the word and character models on their own ( using hyperparameters from in 4.1 ) .,Ablation Study,Impact of the Sentence - based Character Model,part-of-speech_tagging,5,18,1,0,,0.000824252,0,negative,0.021182287,8.27E-06,7.24E-05,2.08E-07,2.36E-06,3.27E-06,0.000103008,1.98E-06,2.11E-05,0.974155736,7.06E-06,0.004433721,8.60E-06
3635,part-of-speech_tagging5,178,"shows , for selected languages , the results averaged over 10 runs in order to measure standard deviation .",Ablation Study,Impact of the Sentence - based Character Model,part-of-speech_tagging,5,19,1,0,,6.61E-05,0,negative,0.000744078,1.08E-07,1.84E-06,1.77E-08,2.20E-07,9.91E-07,1.27E-05,8.56E-07,3.66E-07,0.998938397,2.40E-07,0.000299343,8.95E-07
3636,part-of-speech_tagging5,179,The examples show that the combined model has significantly higher accuracy compared with either the character and word models individually .,Ablation Study,Impact of the Sentence - based Character Model,part-of-speech_tagging,5,20,1,1,ablation-analysis,0.261389179,0,negative,0.207928256,1.06E-06,1.61E-05,1.18E-07,4.76E-07,2.11E-06,0.000569376,2.34E-06,2.16E-06,0.718340349,3.11E-06,0.073112366,2.22E-05
3637,part-of-speech_tagging5,180,Concatenation Strategies for the Context - Sensitive Character Encodings,Ablation Study,,part-of-speech_tagging,5,21,1,0,,0.592791887,1,negative,0.24195926,0.000405812,0.007859592,3.93E-06,1.22E-05,0.00019799,0.000990313,4.04E-05,0.020471612,0.700522043,0.009803882,6.16E-05,0.017671315
3638,part-of-speech_tagging5,181,The proposed model bases a token encoding on both the forward and the backward character representations of both the first and last character in the token ( see Equation 1 ) .,Ablation Study,Concatenation Strategies for the Context - Sensitive Character Encodings,part-of-speech_tagging,5,22,1,0,,0.000108369,0,negative,0.001696021,0.000211541,0.002774361,3.22E-07,2.39E-06,9.30E-06,1.85E-05,1.34E-05,0.040971987,0.954185698,3.76E-05,6.10E-05,1.78E-05
3639,part-of-speech_tagging5,182,"reports , for a few morphological rich languages , the part - of - speech tagging performance of different strategies to gather the characters when creating initial word encodings .",Ablation Study,Concatenation Strategies for the Context - Sensitive Character Encodings,part-of-speech_tagging,5,23,1,0,,3.79E-05,0,negative,0.000673176,2.78E-07,1.01E-05,2.14E-08,3.18E-07,4.57E-07,4.01E-06,5.81E-07,6.31E-07,0.999051335,1.96E-06,0.000256466,6.94E-07
3640,part-of-speech_tagging5,183,The strategies were defined in 3.1 .,Ablation Study,Concatenation Strategies for the Context - Sensitive Character Encodings,part-of-speech_tagging,5,24,1,0,,1.00E-05,0,negative,0.000183862,8.46E-07,6.13E-06,8.66E-08,4.41E-07,4.06E-06,3.07E-06,8.92E-06,1.45E-05,0.999768823,7.36E-08,8.63E-06,5.47E-07
3641,part-of-speech_tagging5,184,The reimplementation of .,Ablation Study,Concatenation Strategies for the Context - Sensitive Character Encodings,part-of-speech_tagging,5,25,1,0,,1.20E-05,0,negative,0.002708931,3.05E-06,0.00015928,4.88E-08,6.13E-07,9.91E-07,8.67E-06,9.04E-07,2.58E-05,0.996493971,4.47E-06,0.000590907,2.41E-06
3642,part-of-speech_tagging5,185,"We removed , for all systems , the word model in order to assess each strategy in isolation .",Ablation Study,Concatenation Strategies for the Context - Sensitive Character Encodings,part-of-speech_tagging,5,26,1,0,,0.000137506,0,negative,0.014281063,1.76E-05,0.000102249,3.71E-06,1.51E-05,7.72E-05,0.000124373,9.14E-05,0.000162319,0.985007705,7.74E-07,8.50E-05,3.14E-05
3643,part-of-speech_tagging5,186,The performance is quite different per language .,Ablation Study,Concatenation Strategies for the Context - Sensitive Character Encodings,part-of-speech_tagging,5,27,1,0,,2.62E-05,0,negative,0.001619814,4.26E-07,2.87E-06,1.51E-07,4.23E-07,3.03E-06,5.30E-05,5.94E-06,1.47E-06,0.996570733,1.21E-06,0.001732719,8.17E-06
3644,part-of-speech_tagging5,187,"E.g. , for Latin , the outputs of the forward and backward LSTMs of the last character scored highest .",Ablation Study,Concatenation Strategies for the Context - Sensitive Character Encodings,part-of-speech_tagging,5,28,1,0,,5.60E-06,0,negative,0.005776834,8.17E-08,2.51E-06,5.60E-08,6.92E-07,1.28E-06,2.35E-05,1.19E-06,2.45E-07,0.992099618,1.99E-07,0.002092051,1.69E-06
3645,part-of-speech_tagging5,188,Sensitivity to Hyperparameter,Ablation Study,,part-of-speech_tagging,5,29,1,0,,0.165322263,0,negative,0.394406086,1.84E-05,0.000583251,8.23E-06,3.09E-05,0.000322256,0.000400664,8.44E-05,0.000309874,0.598881186,5.59E-06,1.09E-05,0.004938217
3646,part-of-speech_tagging5,189,Search,Ablation Study,,part-of-speech_tagging,5,30,1,0,,0.016682339,0,negative,0.049448521,2.64E-05,0.001289674,2.67E-05,0.000442244,0.000988123,0.000347824,5.06E-05,0.000227973,0.933685165,1.14E-05,7.29E-06,0.013447999
3647,part-of-speech_tagging5,190,We picked Vietnamese for a more in - depth analysis since it did not perform well and investigated the influence of the sizes of LSTMs for the word and character model on the accuracy of development set .,Ablation Study,Search,part-of-speech_tagging,5,31,1,0,,0.000180198,0,negative,0.000113802,1.55E-05,2.85E-05,1.04E-07,0.000148436,2.09E-05,5.97E-06,5.57E-06,4.22E-06,0.999654362,2.46E-07,2.17E-06,1.10E-07
3648,part-of-speech_tagging5,191,"With larger network sizes , the capacity of the network increases , however , on the other hand it is prune to overfitting .",Ablation Study,Search,part-of-speech_tagging,5,32,1,0,,0.001145528,0,negative,0.006822842,1.42E-05,2.74E-05,1.01E-06,2.91E-05,2.28E-05,4.16E-06,9.70E-06,2.44E-05,0.993030247,5.11E-06,8.29E-06,7.51E-07
3649,part-of-speech_tagging5,192,"We fixed all the hyperparameters except those for the network size of the character model and the word model , and ran a grid search over dimension sizes from 200 to 500 in steps of 50 .",Ablation Study,Search,part-of-speech_tagging,5,33,1,0,,0.115845723,0,negative,0.000703424,0.001144489,6.27E-05,3.99E-05,0.00056374,0.033475879,0.001406464,0.029087454,0.000623741,0.932765278,6.16E-06,5.02E-06,0.000115676
3650,part-of-speech_tagging5,193,The surface plot in 3 shows that the grid peaks with more moderate settings around 350 LSTM cells which might lead to a higher accuracy .,Ablation Study,Search,part-of-speech_tagging,5,34,1,0,,0.122863598,0,negative,0.108200094,5.42E-05,5.69E-05,2.22E-06,0.000102197,0.000135422,0.000145325,6.28E-05,3.02E-05,0.890951999,4.40E-06,0.000246786,7.53E-06
3651,part-of-speech_tagging5,194,"For all of the network sizes in the grid search , we still observed during training that the accuracy reach a high value and degrades with more iterations for the character and word model .",Ablation Study,Search,part-of-speech_tagging,5,35,1,1,ablation-analysis,0.254035235,0,negative,0.280748174,5.45E-05,2.72E-05,6.40E-06,0.000126696,0.00026317,0.000842628,0.000177243,1.90E-05,0.716799015,1.56E-05,0.000877474,4.30E-05
3652,part-of-speech_tagging5,195,This suggests that future variants of this model might benefit from higher regularization .,Ablation Study,Search,part-of-speech_tagging,5,36,1,0,,0.001020176,0,negative,0.00126857,5.68E-06,1.47E-05,1.44E-07,9.34E-06,1.05E-05,1.73E-06,4.02E-06,1.41E-05,0.998667747,2.32E-07,3.11E-06,1.48E-07
3653,part-of-speech_tagging5,196,Discussion,Ablation Study,,part-of-speech_tagging,5,37,1,0,,0.000786683,0,negative,0.033600194,2.43E-06,0.000127672,1.83E-06,1.19E-05,5.94E-05,4.20E-05,5.53E-06,5.11E-05,0.965218279,1.79E-06,1.22E-06,0.000876679
3654,part-of-speech_tagging5,197,"Generally , the fact that different techniques for creating word encodings from character encodings and different network sizes can lead to different accuracies per language suggests that it should be possible to increase the accuracy of our model on a per language basis via a grid search over all possibilities .",Ablation Study,Discussion,part-of-speech_tagging,5,38,1,0,,0.000128472,0,negative,0.000987155,4.12E-05,2.75E-06,1.39E-07,4.11E-06,1.70E-06,1.59E-06,6.50E-06,6.18E-05,0.998849032,9.80E-06,3.40E-05,1.70E-07
3655,part-of-speech_tagging5,198,"In fact , there are many variations on the models we presented in this work ( e.g. , how the character and word models are combined with the meta - BiLSTM ) .",Ablation Study,Discussion,part-of-speech_tagging,5,39,1,0,,4.66E-07,0,negative,7.86E-06,1.17E-05,1.51E-06,3.40E-07,3.09E-06,2.89E-06,2.68E-07,6.02E-06,2.18E-05,0.999938234,5.69E-06,4.38E-07,9.41E-08
3656,part-of-speech_tagging5,199,"Since we are using separate losses , we could also change our training schema .",Ablation Study,Discussion,part-of-speech_tagging,5,40,1,0,,2.77E-07,0,negative,1.78E-05,1.29E-05,9.12E-07,5.62E-08,1.18E-06,1.05E-06,1.29E-07,4.53E-06,7.04E-05,0.999890151,3.89E-07,4.85E-07,2.85E-08
3657,part-of-speech_tagging5,200,"For example , one could use methods like stack - propagation where we burn - in the character and word models and then train the meta-BiLSTM backpropagating throughout the entire network .",Ablation Study,Discussion,part-of-speech_tagging,5,41,1,0,,2.61E-06,0,negative,2.36E-05,4.00E-05,6.52E-06,4.00E-07,4.02E-06,1.28E-05,1.60E-06,3.45E-05,0.000169708,0.999701477,4.05E-06,8.81E-07,4.54E-07
3658,part-of-speech_tagging5,201,Conclusions,,,part-of-speech_tagging,5,0,1,0,,0.000388901,0,negative,8.30E-05,6.93E-05,4.02E-06,8.07E-07,8.64E-07,3.36E-05,1.76E-05,0.00031599,6.31E-05,0.998796926,0.000458235,0.000155019,1.50E-06
3659,text_summarization9,1,title,,,text_summarization,9,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
3660,text_summarization9,2,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,title,,text_summarization,9,1,1,1,research-problem,0.997201684,1,research-problem,4.35E-07,1.79E-05,6.28E-07,4.03E-06,2.84E-06,7.97E-07,2.27E-05,2.78E-06,4.77E-07,0.003209726,0.996735026,1.74E-06,8.85E-07
3661,text_summarization9,3,abstract,,,text_summarization,9,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
3662,text_summarization9,4,Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning .,abstract,abstract,text_summarization,9,1,1,0,,0.102973567,0,research-problem,3.89E-08,3.20E-06,2.20E-08,1.54E-06,3.66E-07,1.69E-07,7.46E-07,5.16E-07,1.38E-07,0.004405952,0.995587201,3.11E-08,7.66E-08
3663,text_summarization9,5,We introduce a conditional recurrent neural network ( RNN ) which generates a summary of an input sentence .,abstract,abstract,text_summarization,9,2,1,0,,0.5922027,1,research-problem,1.09E-05,0.025535574,0.000205795,5.92E-06,6.76E-05,1.08E-05,1.05E-05,9.04E-05,0.009754014,0.104781785,0.859519961,4.60E-06,2.23E-06
3664,text_summarization9,6,The conditioning is provided by a novel convolutional attention - based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation .,abstract,abstract,text_summarization,9,3,1,0,,0.169756218,0,negative,5.01E-05,0.26369061,0.000166094,3.08E-05,0.000308845,0.000114225,1.41E-05,0.001509577,0.298424556,0.389910763,0.045772243,4.54E-06,3.52E-06
3665,text_summarization9,7,Our model relies only on learned features and is easy to train in an end - to - end fashion on large data sets .,abstract,abstract,text_summarization,9,4,1,0,,0.076987676,0,negative,1.89E-05,0.105243072,1.12E-05,7.79E-05,0.000360105,7.44E-05,1.52E-05,0.001370526,0.003895653,0.636251131,0.252670104,8.11E-06,3.82E-06
3666,text_summarization9,8,Our experiments show that the model significantly outperforms the recently proposed state - of - the - art method on the Gigaword corpus while performing competitively on the DUC - 2004 shared task .,abstract,abstract,text_summarization,9,5,1,0,,0.008399674,0,negative,0.000215441,0.005785296,7.17E-06,2.99E-05,0.000141372,5.19E-05,0.000161576,0.000917806,0.000127041,0.709618465,0.281958534,0.000976695,8.81E-06
3667,text_summarization9,9,Introduction,,,text_summarization,9,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
3668,text_summarization9,10,Generating a condensed version of a passage while preserving its meaning is known as text summarization .,Introduction,Introduction,text_summarization,9,1,1,1,research-problem,0.9191227,1,research-problem,1.06E-06,0.000160031,3.64E-07,1.36E-05,2.64E-05,4.96E-06,1.05E-05,5.21E-06,3.86E-05,0.037145534,0.96259053,1.14E-06,2.18E-06
3669,text_summarization9,11,Tackling this task is an important step towards natural language understanding .,Introduction,Introduction,text_summarization,9,2,1,0,,0.854079852,1,research-problem,6.95E-07,9.82E-05,1.63E-07,1.63E-05,1.85E-05,7.32E-06,8.00E-06,6.67E-06,3.47E-05,0.067836767,0.931970045,8.11E-07,1.81E-06
3670,text_summarization9,12,Summarization systems can be broadly classified into two categories .,Introduction,Introduction,text_summarization,9,3,1,0,,0.024039453,0,research-problem,8.79E-07,0.000175093,5.31E-07,3.25E-06,1.16E-05,5.33E-06,1.29E-05,6.66E-06,6.07E-05,0.073763073,0.925957129,1.53E-06,1.37E-06
3671,text_summarization9,13,Extractive models generate summaries by cropping important segments from the original text and putting them together to form a coherent summary .,Introduction,Introduction,text_summarization,9,4,1,0,,0.019039561,0,research-problem,3.51E-06,0.000487649,1.65E-06,0.000157556,0.000172928,5.76E-05,2.86E-05,2.72E-05,0.00018112,0.160308949,0.838563368,2.05E-06,7.87E-06
3672,text_summarization9,14,Abstractive models generate summaries from scratch without being constrained to reuse phrases from the original text .,Introduction,Introduction,text_summarization,9,5,1,0,,0.091940539,0,research-problem,2.20E-06,0.000365735,1.19E-06,4.69E-05,8.75E-05,2.52E-05,2.09E-05,1.51E-05,9.91E-05,0.133236893,0.866093211,1.74E-06,4.30E-06
3673,text_summarization9,15,In this paper we propose a novel recurrent neural network for the problem of abstractive sentence summarization .,Introduction,Introduction,text_summarization,9,6,1,0,,0.970216791,1,research-problem,6.87E-05,0.135105329,0.000261281,7.84E-06,0.000229645,5.38E-05,0.000201886,0.000124669,0.293522722,0.064501525,0.505849361,5.34E-05,2.00E-05
3674,text_summarization9,16,"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .",Introduction,Introduction,text_summarization,9,7,1,1,model,0.942201155,1,model,9.17E-06,0.056548296,9.78E-05,1.15E-06,9.63E-05,1.50E-05,1.06E-05,2.19E-05,0.933146408,0.007506758,0.002541709,3.02E-06,1.87E-06
3675,text_summarization9,17,"In addition , at every time step the decoder also takes a conditioning input which is the output of an encoder module .",Introduction,Introduction,text_summarization,9,8,1,1,model,0.808133478,1,model,8.92E-06,0.029922736,2.45E-05,3.55E-07,4.26E-05,8.82E-06,3.14E-06,2.13E-05,0.949016294,0.020290677,0.00065895,1.40E-06,3.54E-07
3676,text_summarization9,18,"Depending on the current state of the RNN , the encoder computes scores over the words in the input sentence .",Introduction,Introduction,text_summarization,9,9,1,1,model,0.870313043,1,model,3.49E-06,0.032071537,1.76E-05,1.05E-07,1.88E-05,4.31E-06,1.89E-06,1.32E-05,0.957564177,0.009869874,0.000434178,7.00E-07,2.04E-07
3677,text_summarization9,19,"These scores can be interpreted as a soft alignment over the input text , informing the decoder which part of the input sentence it should focus onto generate the next word .",Introduction,Introduction,text_summarization,9,10,1,0,,0.39269923,0,model,1.16E-05,0.04928531,2.38E-05,1.14E-06,0.000119099,3.21E-05,6.53E-06,7.25E-05,0.844430059,0.10448502,0.001529624,2.58E-06,6.38E-07
3678,text_summarization9,20,Both the decoder and encoder are jointly trained on a data set consisting of sentence - summary pairs .,Introduction,Introduction,text_summarization,9,11,1,1,model,0.937181729,1,model,6.21E-06,0.19462148,2.11E-05,3.85E-07,0.000107332,1.06E-05,6.59E-06,5.23E-05,0.798533799,0.006303795,0.000334402,1.43E-06,6.33E-07
3679,text_summarization9,21,Our model can be seen as an extension of the recently proposed model for the same problem by .,Introduction,Introduction,text_summarization,9,12,1,0,,0.520219316,1,model,8.31E-06,0.030641389,4.70E-05,8.19E-07,8.72E-05,1.82E-05,7.15E-06,2.50E-05,0.944068149,0.023693473,0.001399742,2.72E-06,9.29E-07
3680,text_summarization9,22,"While they use a feed - forward neural language model for generation , we use a recurrent neural network .",Introduction,Introduction,text_summarization,9,13,1,0,,0.09199816,0,negative,0.000184278,0.175941054,0.001223365,9.40E-05,0.004045118,0.000707871,0.000376294,0.000545366,0.286846617,0.506938971,0.023008385,5.83E-05,3.04E-05
3681,text_summarization9,23,"Furthermore , our encoder is more sophisticated , in that it explicitly encodes the position information of the input words .",Introduction,Introduction,text_summarization,9,14,1,0,,0.753445096,1,model,0.000189962,0.263835734,0.000282685,2.23E-05,0.002266219,0.000129961,6.27E-05,0.000139849,0.652889962,0.078374723,0.00177195,2.66E-05,7.33E-06
3682,text_summarization9,24,"Lastly , our encoder uses a convolutional network to encode input words .",Introduction,Introduction,text_summarization,9,15,1,1,model,0.948232405,1,model,4.80E-05,0.059675512,0.000230021,2.09E-06,0.00028047,3.95E-05,2.05E-05,3.93E-05,0.924374695,0.01510857,0.000176078,3.36E-06,1.88E-06
3683,text_summarization9,25,These extensions result in improved performance .,Introduction,Introduction,text_summarization,9,16,1,0,,0.005527597,0,negative,0.000345064,0.008490499,3.39E-05,1.19E-05,0.00048841,7.51E-05,7.13E-05,6.67E-05,0.015862683,0.969450665,0.005018955,8.23E-05,2.59E-06
3684,text_summarization9,26,The main contribution of this paper is a novel convolutional attention - based conditional recurrent neural network model for the problem of abstractive sentence summarization .,Introduction,Introduction,text_summarization,9,17,1,0,,0.944697962,1,research-problem,0.000184648,0.248774022,0.000276435,0.000124761,0.00208088,0.000239537,0.000839117,0.000439864,0.108745718,0.187989962,0.450036044,0.000166422,0.000102591
3685,text_summarization9,27,Empirically we show that our model beats the state - of - the - art systems of on multiple data sets .,Introduction,Introduction,text_summarization,9,18,1,0,,0.029368204,0,negative,0.000926447,0.080511827,1.74E-05,4.22E-05,0.002056526,0.000399239,0.001905521,0.001500289,0.010355668,0.895998436,0.004417641,0.001832179,3.67E-05
3686,text_summarization9,28,"Particularly notable is the fact that even with a simple generation module , which does not use any extractive feature tuning , our model manages to significantly outperform their ABS + system on the Gigaword data set and is comparable on the DUC - 2004 task .",Introduction,Introduction,text_summarization,9,19,1,0,,0.056338283,0,negative,0.007950995,0.043790237,0.000102133,0.000217254,0.004075829,0.000997719,0.032964319,0.002650031,0.004037171,0.860717405,0.011533363,0.030518627,0.000444916
3687,text_summarization9,29,93,Introduction,Introduction,text_summarization,9,20,1,0,,0.000674239,0,negative,7.82E-06,0.001270602,9.88E-07,4.41E-06,7.92E-05,9.36E-05,2.23E-05,0.000137468,0.005500873,0.991412529,0.001465818,3.59E-06,9.13E-07
3688,text_summarization9,30,"While there is a large body of work for generating extractive summaries of sentences , there has been much less research on abstractive summarization .",Introduction,Introduction,text_summarization,9,21,1,0,,0.55714737,1,research-problem,5.39E-06,0.001188231,2.56E-06,6.84E-05,0.000323639,7.27E-05,0.000167809,5.56E-05,0.000122685,0.399121732,0.598846208,1.12E-05,1.39E-05
3689,text_summarization9,31,A count - based noisy - channel machine translation model was proposed for the problem in .,Introduction,Introduction,text_summarization,9,22,1,0,,0.593735439,1,model,7.14E-05,0.194739056,0.000217525,7.44E-06,0.000407901,8.36E-05,0.000266981,0.000180385,0.37182749,0.24624931,0.185864461,6.69E-05,1.75E-05
3690,text_summarization9,32,"The task of abstractive sentence summarization was later formalized around the , where the TOP - IARY system was the state - of the - art .",Introduction,Introduction,text_summarization,9,23,1,0,,0.278122972,0,research-problem,4.60E-06,0.001004344,3.78E-06,8.44E-05,0.000315524,8.41E-05,0.000203418,4.54E-05,0.000146025,0.311194739,0.686884378,1.02E-05,1.91E-05
3691,text_summarization9,33,More recently and later proposed systems which made heavy use of the syntactic features of the sentence - summary pairs .,Introduction,Introduction,text_summarization,9,24,1,0,,0.001672372,0,negative,8.22E-06,0.001446295,4.36E-06,4.13E-05,0.000227755,0.000341516,0.000173145,0.000242309,0.001373869,0.928401265,0.067715557,1.16E-05,1.28E-05
3692,text_summarization9,34,"Later , along the lines of , MOSES was used directly as a method for text simplification by .",Introduction,Introduction,text_summarization,9,25,1,0,,0.007224693,0,negative,1.17E-05,0.00211153,9.87E-06,3.03E-05,0.000405363,0.000290849,0.000192144,0.000155733,0.002449628,0.964188578,0.030130842,1.36E-05,9.83E-06
3693,text_summarization9,35,Other works which have recently been proposed for the problem of sentence summarization include .,Introduction,Introduction,text_summarization,9,26,1,0,,0.060626686,0,negative,6.90E-06,0.001345367,4.81E-06,1.56E-05,0.00013559,7.98E-05,0.000188298,5.68E-05,0.000357736,0.712928045,0.284852243,2.03E-05,8.58E-06
3694,text_summarization9,36,Very recently proposed a neural attention model for this problem using a new data set for training and showing state - of - the - art performance on the DUC tasks .,Introduction,Introduction,text_summarization,9,27,1,0,,0.105828811,0,negative,7.55E-06,0.007995581,1.26E-05,2.45E-05,0.000131027,0.000429938,0.0003688,0.000664243,0.011549742,0.764225806,0.21454439,1.87E-05,2.71E-05
3695,text_summarization9,37,Our model can be seen as an extension of their model .,Introduction,Introduction,text_summarization,9,28,1,0,,0.020158991,0,model,7.91E-06,0.032861019,5.81E-05,3.76E-06,0.000277501,0.000126444,4.40E-05,0.000154053,0.770597701,0.194399033,0.0014613,4.82E-06,4.37E-06
3696,text_summarization9,38,Attentive Recurrent Architecture,,,text_summarization,9,0,1,0,,0.053251343,0,research-problem,0.000119634,0.000447613,0.001261843,1.81E-05,3.89E-06,0.000305144,0.001508626,0.001738218,0.001545309,0.267865701,0.723369508,0.001609164,0.000207223
3697,text_summarization9,39,"Let x denote the input sentence consisting of a sequence of M words x = [x 1 , . . . , x M ] , where each word xi is part of vocabulary V , of size | V | = V .",Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,1,1,0,,1.01E-05,0,negative,7.46E-07,3.75E-05,1.32E-06,3.84E-08,1.03E-07,2.94E-06,1.53E-06,5.20E-05,0.000276732,0.999589641,3.36E-05,3.71E-06,7.87E-08
3698,text_summarization9,40,"Our task is to generate a target sequence y = [ y 1 , . . . , y N ] , of N words , where N < M , such that the meaning of x is preserved : y = argmax y P ( y|x ) , where y is a random variable denoting a sequence of N words .",Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,2,1,0,,7.47E-05,0,negative,2.74E-06,0.000117513,4.12E-06,4.12E-07,4.13E-07,5.12E-06,5.43E-06,0.000108414,0.000470679,0.998242733,0.001022086,1.95E-05,8.13E-07
3699,text_summarization9,41,Typically the conditional probability is modeled by a parametric function with parameters ?: P ( y|x ) = P ( y |x ; ?) .,Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,3,1,0,,3.22E-06,0,negative,7.69E-07,7.44E-06,1.26E-06,1.46E-08,1.35E-08,2.37E-06,7.41E-07,3.63E-05,0.000218308,0.999717714,1.34E-05,1.63E-06,3.14E-08
3700,text_summarization9,42,Training involves finding the ?,Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,4,1,0,,1.63E-05,0,negative,1.64E-06,1.63E-06,5.08E-07,5.39E-08,8.44E-08,1.40E-06,8.75E-07,8.09E-06,1.58E-05,0.999962052,2.72E-06,5.10E-06,3.69E-08
3701,text_summarization9,43,which maximizes the conditional probability of sentence - summary pairs in the training corpus .,Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,5,1,0,,1.33E-05,0,negative,5.33E-06,2.76E-05,5.67E-05,3.59E-09,3.86E-08,7.83E-07,1.16E-06,1.22E-05,0.000296299,0.999570017,7.47E-06,2.23E-05,2.16E-08
3702,text_summarization9,44,"If the model is trained to generate the next word of the summary , given the previous words , then the above conditional can be factorized into a product of indi-vidual conditional probabilities :",Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,6,1,0,,8.70E-07,0,negative,3.91E-06,6.14E-06,2.10E-06,6.38E-09,2.50E-08,5.44E-07,3.05E-07,6.73E-06,0.000258606,0.999717773,8.12E-07,3.05E-06,9.37E-09
3703,text_summarization9,45,"In this work we model this conditional probability using an RNN Encoder - Decoder architecture , inspired by and subsequently extended in .",Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,7,1,0,,0.000400985,0,negative,9.83E-05,0.003428355,0.001334352,4.83E-07,3.70E-06,1.23E-05,2.11E-05,9.14E-05,0.082406915,0.912259293,0.00025269,8.94E-05,1.77E-06
3704,text_summarization9,46,We call our model RAS ( Recurrent Attentive Summarizer ) .,Attentive Recurrent Architecture,Attentive Recurrent Architecture,text_summarization,9,8,1,0,,0.012807084,0,negative,0.000180681,0.000748025,0.010397537,1.30E-06,1.04E-05,2.76E-05,0.000116189,7.21E-05,0.015351508,0.972407624,0.000289161,0.000389412,8.44E-06
3705,text_summarization9,47,Recurrent Decoder,Attentive Recurrent Architecture,,text_summarization,9,9,1,0,,0.00036064,0,negative,9.25E-05,0.000274107,0.000749393,2.87E-06,3.46E-06,4.44E-05,7.17E-05,0.000162232,0.038938827,0.958941867,0.000551891,0.000151196,1.56E-05
3706,text_summarization9,48,The above conditional is modeled using an RNN :,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,10,1,0,,1.41E-07,0,negative,8.06E-06,1.08E-05,0.00011605,2.90E-09,1.85E-08,3.19E-07,2.93E-07,4.21E-06,0.001123983,0.99872562,1.76E-06,8.87E-06,2.29E-08
3707,text_summarization9,49,where ht is the hidden state of the RNN :,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,11,1,0,,8.00E-08,0,negative,2.28E-06,2.92E-06,1.62E-06,8.55E-09,1.85E-08,2.80E-07,1.71E-07,5.28E-06,6.55E-05,0.999916006,1.24E-06,4.67E-06,1.57E-08
3708,text_summarization9,50,Here ct is the output of the encoder module ( detailed in 3.2 ) .,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,12,1,0,,2.40E-07,0,negative,1.46E-06,5.11E-06,8.33E-07,8.06E-09,1.45E-08,3.85E-07,1.71E-07,1.24E-05,0.000145347,0.999831127,8.37E-07,2.26E-06,1.57E-08
3709,text_summarization9,51,It can be seen as a context vector which is computed as a function of the current state h t?1 and the input sequence x .,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,13,1,0,,3.35E-07,0,negative,3.63E-06,1.08E-05,2.01E-06,2.49E-08,5.12E-08,8.82E-07,2.58E-07,2.33E-05,0.000337388,0.999617653,7.41E-07,3.23E-06,2.77E-08
3710,text_summarization9,52,"Our Elman RNN takes the following form ( Elman , 1990 ) :",Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,14,1,0,,9.99E-07,0,negative,1.19E-06,9.68E-06,4.35E-05,7.55E-09,2.64E-08,8.89E-07,5.87E-07,1.21E-05,0.001174385,0.998748392,4.65E-06,4.55E-06,8.23E-08
3711,text_summarization9,53,where ?,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,15,1,0,,3.96E-08,0,negative,5.50E-07,1.13E-06,2.28E-07,1.38E-08,9.40E-09,4.46E-07,1.35E-07,7.70E-06,2.69E-05,0.999960288,9.12E-07,1.66E-06,1.28E-08
3712,text_summarization9,54,is the sigmoid function and ?,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,16,1,0,,1.81E-06,0,negative,1.62E-06,1.55E-05,1.44E-06,5.14E-08,4.48E-08,5.31E-06,1.32E-06,0.000285711,0.00036866,0.999314889,2.67E-06,2.68E-06,1.08E-07
3713,text_summarization9,55,"is the softmax , defined as : ? ( o t ) = e ot / j e o j and W i ( i = 1 , . . . , 5 ) are matrices of learnable parameters of sizes W { 1,2,3 } ?",Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,17,1,0,,3.57E-07,0,negative,2.46E-06,2.98E-06,1.04E-06,2.13E-08,6.53E-08,5.53E-07,2.32E-07,1.05E-05,2.03E-05,0.999955379,2.16E-07,6.22E-06,1.42E-08
3714,text_summarization9,56,"R dd and W { 4,5 } ?",Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,18,1,0,,3.49E-06,0,negative,1.58E-05,2.47E-06,6.10E-06,1.16E-08,6.62E-08,7.68E-07,1.93E-06,1.21E-05,1.31E-05,0.999806914,6.77E-07,0.0001399,5.11E-08
3715,text_summarization9,57,R dV .,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,19,1,0,,1.27E-06,0,negative,5.76E-06,1.79E-06,5.85E-07,6.32E-08,4.29E-08,9.58E-07,4.98E-07,1.27E-05,3.31E-05,0.999929902,9.12E-07,1.37E-05,6.21E-08
3716,text_summarization9,58,The LSTM decoder is defined as :,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,20,1,0,,1.20E-06,0,negative,2.99E-06,5.44E-06,4.63E-06,8.42E-09,2.33E-08,4.56E-07,2.36E-07,1.25E-05,0.000322629,0.999646891,4.88E-07,3.71E-06,2.90E-08
3717,text_summarization9,59,"Operator refers to component - wise multiplication , and W i ( i = 1 , . . . , 14 ) are matrices of learnable parameters of sizes W { 1 ,... ,12 } ?",Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,21,1,0,,7.38E-07,0,negative,8.13E-07,3.17E-06,9.23E-07,2.04E-08,3.01E-08,1.54E-06,4.64E-07,3.02E-05,5.44E-05,0.999905513,3.50E-07,2.59E-06,3.11E-08
3718,text_summarization9,60,"R dd , and W { 13,14 } ?",Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,22,1,0,,7.90E-07,0,negative,8.93E-06,1.69E-06,3.23E-06,7.66E-09,4.29E-08,3.91E-07,9.26E-07,6.99E-06,1.08E-05,0.999896277,4.01E-07,7.03E-05,3.28E-08
3719,text_summarization9,61,R dV .,Attentive Recurrent Architecture,Recurrent Decoder,text_summarization,9,23,1,0,,1.12E-06,0,negative,5.29E-06,1.62E-06,5.60E-07,5.50E-08,4.33E-08,8.62E-07,4.77E-07,1.22E-05,3.10E-05,0.99993339,5.87E-07,1.39E-05,6.40E-08
3720,text_summarization9,62,Attentive Encoder,Attentive Recurrent Architecture,,text_summarization,9,24,1,0,,8.81E-05,0,negative,8.25E-05,0.00014963,0.000371915,6.96E-07,2.60E-06,1.48E-05,3.77E-05,7.81E-05,0.027885992,0.971155258,3.77E-05,0.000174909,8.12E-06
3721,text_summarization9,63,We now give the details of the encoder which computes the context vector ct for every time step t of the decoder above .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,25,1,0,,2.95E-07,0,negative,1.69E-06,2.47E-06,7.39E-07,1.09E-08,1.80E-08,2.23E-07,1.51E-07,8.29E-06,0.00011192,0.999868964,1.11E-07,5.36E-06,5.45E-08
3722,text_summarization9,64,"With a slight overload of notation , for an input sentence x we denote by xi the d dimensional learnable embedding of the i - th word ( x i ?",Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,26,1,0,,6.22E-08,0,negative,6.53E-07,2.27E-06,7.57E-07,2.84E-09,1.03E-08,1.76E-07,1.30E-07,5.42E-06,2.50E-05,0.999958932,2.35E-07,6.38E-06,1.83E-08
3723,text_summarization9,65,Rd ) . In addition the position i of the word xi is also associated with a learnable embedding l i of size d ( l i ?,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,27,1,0,,6.91E-07,0,negative,8.31E-06,8.24E-06,6.93E-06,4.40E-08,1.08E-07,2.26E-06,2.31E-06,6.78E-05,6.86E-05,0.999761329,7.04E-07,7.30E-05,4.31E-07
3724,text_summarization9,66,Rd ) . Then the full embedding for i - th word in x is given by a i = x i + l i .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,28,1,0,,3.56E-07,0,negative,5.15E-06,1.57E-06,1.29E-05,5.84E-09,2.72E-08,3.05E-07,6.03E-07,4.97E-06,2.03E-05,0.999876866,2.16E-07,7.71E-05,8.50E-08
3725,text_summarization9,67,Let us denote by Bk ?,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,29,1,0,,2.32E-07,0,negative,2.06E-06,5.99E-07,1.50E-06,2.49E-09,1.02E-08,1.82E-07,2.72E-07,3.52E-06,7.26E-06,0.999958361,6.11E-08,2.62E-05,2.01E-08
3726,text_summarization9,68,R qd a learnable weight matrix which is used to convolve over the full embeddings of consecutive words .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,30,1,0,,5.58E-07,0,negative,7.84E-06,7.95E-06,3.66E-06,1.03E-07,1.06E-07,1.91E-06,9.01E-07,0.000121626,0.0001749,0.999668997,1.14E-07,1.16E-05,3.05E-07
3727,text_summarization9,69,"Let there bed such matrices ( k ? { 1 , . . . , d} ) .",Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,31,1,0,,6.42E-08,0,negative,8.77E-07,3.38E-07,3.40E-07,2.51E-09,8.46E-09,1.45E-07,1.07E-07,4.89E-06,5.73E-06,0.999980201,1.71E-08,7.33E-06,1.36E-08
3728,text_summarization9,70,The output of convolution is given by :,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,32,1,0,,5.06E-07,0,negative,1.31E-06,8.13E-07,3.29E-06,6.54E-09,1.38E-08,2.85E-07,1.93E-07,5.38E-06,6.59E-05,0.999916785,8.69E-08,5.87E-06,7.68E-08
3729,text_summarization9,71,where bk j is the j - th column of the matrix Bk .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,33,1,0,,5.47E-08,0,negative,2.56E-06,3.48E-07,8.57E-07,4.43E-09,1.25E-08,1.16E-07,1.22E-07,2.33E-06,7.46E-06,0.999972039,2.81E-08,1.41E-05,2.51E-08
3730,text_summarization9,72,"Thus the d dimensional aggregate embedding vector z i is defined as z i = [ z i 1 , . . . , z id ] .",Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,34,1,0,,2.18E-07,0,negative,4.30E-07,1.58E-06,5.34E-07,6.71E-09,1.38E-08,3.57E-07,1.74E-07,1.73E-05,6.05E-05,0.999916146,5.96E-08,2.86E-06,5.65E-08
3731,text_summarization9,73,Note that each word x i in the input sequence is associated with one aggregate embedding vector z i .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,35,1,0,,6.11E-08,0,negative,2.48E-06,8.35E-07,1.13E-06,1.60E-09,8.06E-09,6.84E-08,5.24E-08,2.43E-06,3.85E-05,0.999948601,1.17E-08,5.90E-06,1.06E-08
3732,text_summarization9,74,The vectors z i can be seen as a representation of the word which captures the position in which it occurs in the sentence and also the context in which it appears in the sentence .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,36,1,0,,2.68E-07,0,negative,8.02E-07,8.30E-07,5.78E-07,3.17E-09,8.21E-09,2.13E-07,1.15E-07,8.38E-06,6.62E-05,0.999919545,2.42E-08,3.29E-06,3.08E-08
3733,text_summarization9,75,In our experiments the width q of the convolution matrix Bk was set to 5 .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,37,1,0,,0.00041861,0,negative,2.44E-05,4.08E-05,4.00E-06,9.31E-07,6.22E-07,0.000270221,0.000125398,0.018018865,0.000199111,0.981148589,5.45E-07,0.000154086,1.25E-05
3734,text_summarization9,76,To account for words at the boundaries of x we first pad the sequence on both sides with dummy words before computing the aggregate vectors z i 's .,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,38,1,0,,5.10E-07,0,negative,5.13E-05,4.45E-06,1.06E-05,8.62E-09,1.15E-07,5.89E-07,9.61E-07,1.26E-05,7.14E-05,0.999796393,1.32E-08,5.14E-05,8.15E-08
3735,text_summarization9,77,"Given these aggregate vectors of words , we compute the context vector ct ( the encoder output ) as :",Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,39,1,0,,2.22E-07,0,negative,1.95E-06,9.01E-07,2.17E-06,3.10E-09,1.51E-08,1.05E-07,1.34E-07,2.97E-06,3.51E-05,0.999945493,2.48E-08,1.11E-05,4.80E-08
3736,text_summarization9,78,where the weights ?,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,40,1,0,,1.32E-07,0,negative,6.81E-07,1.77E-07,2.68E-07,1.98E-09,3.97E-09,1.35E-07,9.05E-08,5.14E-06,5.26E-06,0.999982853,7.16E-09,5.36E-06,1.25E-08
3737,text_summarization9,79,"j,t?",Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,41,1,0,,3.58E-07,0,negative,8.46E-07,1.05E-07,2.94E-07,9.22E-10,3.14E-09,6.27E-08,1.00E-07,2.01E-06,2.80E-06,0.999981673,5.39E-09,1.21E-05,1.30E-08
3738,text_summarization9,80,1 are computed as,Attentive Recurrent Architecture,Attentive Encoder,text_summarization,9,42,1,0,,6.79E-07,0,negative,1.54E-06,2.22E-07,5.17E-07,2.88E-09,9.34E-09,1.57E-07,1.14E-07,4.49E-06,9.34E-06,0.999974585,7.06E-09,8.99E-06,2.52E-08
3739,text_summarization9,81,Training and Generation,,,text_summarization,9,0,1,0,,0.043909946,0,negative,5.25E-05,0.001294419,2.11E-05,1.92E-05,1.79E-05,0.000952559,0.000303327,0.018702695,0.000361936,0.974792759,0.003284935,0.000176173,2.04E-05
3740,text_summarization9,82,"Given a training corpus S = { ( x i , y i ) } S i=1 of S sentence - summary pairs , the above model can be trained end - to - end using stochastic gradient descent by minimizing the negative conditional log likelihood of the training data with respect to ?:",Training and Generation,Training and Generation,text_summarization,9,1,1,0,,6.93E-05,0,negative,4.17E-05,5.96E-05,0.000107657,9.23E-07,1.05E-06,0.005281532,0.000145843,0.015029021,6.86E-05,0.979140195,8.54E-05,3.33E-05,5.13E-06
3741,text_summarization9,83,where the parameters ?,Training and Generation,Training and Generation,text_summarization,9,2,1,0,,1.02E-05,0,negative,7.50E-06,4.18E-06,3.47E-06,3.41E-07,2.15E-07,0.010334858,6.24E-05,0.020775253,1.05E-05,0.968788176,6.29E-06,5.91E-06,8.38E-07
3742,text_summarization9,84,constitute the parameters of the decoder and the encoder .,Training and Generation,Training and Generation,text_summarization,9,3,1,0,,0.00020217,0,negative,2.37E-05,1.88E-05,1.01E-05,1.00E-06,9.28E-07,0.015881845,0.000110869,0.037710649,5.23E-05,0.946172796,3.23E-06,1.15E-05,2.20E-06
3743,text_summarization9,85,"Once the parametric model is trained we generate a summary for a new sentence x through a wordbased beam search such that P ( y|x ) is maximized , argmax P ( y t | {y 1 , . . . , y t?1 } , x ) .",Training and Generation,Training and Generation,text_summarization,9,4,1,0,,6.11E-05,0,negative,4.36E-05,4.25E-05,0.000197782,3.73E-07,1.34E-06,0.003734858,0.00013381,0.007506749,7.22E-05,0.988228554,7.14E-06,2.81E-05,2.97E-06
3744,text_summarization9,86,The search is parameterized by the number of paths k thatare pursued at each time step .,Training and Generation,Training and Generation,text_summarization,9,5,1,0,,0.000427362,0,negative,6.39E-05,0.000381297,9.94E-05,2.08E-06,2.32E-06,0.079564298,0.000568395,0.459168259,0.000574346,0.459505345,2.75E-05,2.01E-05,2.27E-05
3745,text_summarization9,87,Experimental Setup,,,text_summarization,9,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
3746,text_summarization9,88,Datasets and Evaluation,,,text_summarization,9,0,1,0,,0.000885072,0,negative,2.25E-06,3.08E-05,3.80E-06,1.83E-07,2.20E-07,4.68E-05,2.69E-05,0.000684509,1.38E-05,0.99643648,0.0027055,4.78E-05,9.37E-07
3747,text_summarization9,89,Our models are trained on the annotated version of the Gigaword corpus and we use only the annotations for tokenization and sentence separation while discarding other annotations such as tags and parses .,Datasets and Evaluation,Datasets and Evaluation,text_summarization,9,1,1,0,,0.007705237,0,negative,0.000207172,0.001296041,0.346387987,1.24E-05,6.06E-05,0.000700416,0.003965245,0.012061874,0.000175813,0.63400672,6.72E-05,0.00101811,4.05E-05
3748,text_summarization9,90,We pair the first sentence of each article with its headline to form sentence - summary pairs .,Datasets and Evaluation,Datasets and Evaluation,text_summarization,9,2,1,0,,0.02772957,0,negative,9.66E-05,0.000457155,0.334711861,7.33E-06,3.67E-05,0.000334015,0.001724332,0.005732154,0.000380276,0.656034357,5.40E-05,0.000382913,4.83E-05
3749,text_summarization9,91,"The data is pre-processed in the same way as and we use the same splits for training , validation , and testing .",Datasets and Evaluation,Datasets and Evaluation,text_summarization,9,3,1,0,,0.000737651,0,negative,7.30E-05,5.51E-05,0.016791977,6.90E-06,1.64E-05,0.000476772,0.002660783,0.005434857,2.11E-05,0.974029087,9.29E-06,0.000412689,1.20E-05
3750,text_summarization9,92,For Gigaword we report results on the same randomly held - out test set of 2000 sentence - summary pairs as .,Datasets and Evaluation,Datasets and Evaluation,text_summarization,9,4,1,0,,0.017308518,0,negative,0.000153233,8.95E-05,0.03085163,3.28E-06,2.34E-05,0.000132133,0.010231963,0.001406398,5.52E-06,0.93691003,7.24E-05,0.020090779,2.98E-05
3751,text_summarization9,93,We also evaluate our models on the DUC - 2004 evaluation data set comprising 500 pairs .,Datasets and Evaluation,Datasets and Evaluation,text_summarization,9,5,1,0,,0.0134943,0,negative,0.000105868,0.000131875,0.029533774,6.29E-07,5.36E-06,9.73E-05,0.002722583,0.001823012,1.02E-05,0.957609355,2.74E-05,0.007921869,1.08E-05
3752,text_summarization9,94,"Our evaluation is based on three variants of ROUGE , namely , ROUGE - 1 ( unigrams ) , ROUGE - 2 ( bigrams ) , and ROUGE - L ( longest - common substring ) .",Datasets and Evaluation,Datasets and Evaluation,text_summarization,9,6,1,0,,0.033509854,0,negative,0.000111221,0.000779871,0.305756161,5.17E-06,5.19E-05,0.000217881,0.007742224,0.003234535,5.25E-05,0.67627769,0.000123008,0.005591867,5.60E-05
3753,text_summarization9,95,Architectural Choices,Datasets and Evaluation,,text_summarization,9,7,1,0,,0.009813639,0,negative,5.02E-05,3.21E-05,0.008554889,2.32E-06,9.12E-07,0.000562551,0.002056192,0.008501325,6.35E-05,0.97848986,0.000242373,0.001416189,2.76E-05
3754,text_summarization9,96,We implemented our models in the Torch library ( http://torch.ch/),Datasets and Evaluation,Architectural Choices,text_summarization,9,8,1,1,experimental-setup,0.667859051,1,negative,0.000242866,9.83E-06,0.001475946,0.029365024,0.000111383,0.316889678,0.035193253,0.003967879,3.11E-05,0.610689059,2.99E-05,6.07E-05,0.001933451
3755,text_summarization9,97,2 . To optimize our loss ( Equation 5 ) we used stochastic gradient descent with mini-batches of size 32 .,Datasets and Evaluation,Architectural Choices,text_summarization,9,9,1,1,experimental-setup,0.564082402,1,negative,9.55E-05,2.05E-05,0.000420978,1.37E-05,3.46E-06,0.242520505,0.07902073,0.176584167,4.38E-05,0.50058715,1.22E-05,0.000135395,0.000541925
3756,text_summarization9,98,"During training we measure the perplexity of the summaries in the validation set and adjust our hyper - parameters , such as the learning rate , based on this number .",Datasets and Evaluation,Architectural Choices,text_summarization,9,10,1,1,experimental-setup,0.000308442,0,negative,0.000148957,1.72E-05,0.000954364,2.13E-07,2.36E-06,0.000219774,0.000740034,0.000457889,1.82E-05,0.997309341,3.67E-07,0.000126359,4.94E-06
3757,text_summarization9,99,For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,Datasets and Evaluation,Architectural Choices,text_summarization,9,11,1,1,experimental-setup,0.025304383,0,negative,0.00013829,1.55E-05,0.010489581,6.93E-06,1.00E-05,0.019981468,0.024739133,0.005130735,8.56E-05,0.939105804,4.03E-06,0.000103236,0.0001896
3758,text_summarization9,100,We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .,Datasets and Evaluation,Architectural Choices,text_summarization,9,12,1,1,experimental-setup,0.020418043,0,negative,4.94E-05,6.24E-06,9.06E-05,1.34E-06,1.16E-06,0.016104537,0.005372968,0.021360328,1.48E-05,0.956885027,1.59E-06,7.86E-05,3.34E-05
3759,text_summarization9,101,"In particular we searched over the number of hidden units H of the recurrent layer , the learning rate ? , the learning rate annealing schedule ?",Datasets and Evaluation,Architectural Choices,text_summarization,9,13,1,0,,0.000898717,0,negative,3.32E-05,2.11E-06,5.17E-05,4.73E-07,7.43E-07,0.002132293,0.001344161,0.002290927,5.94E-06,0.994044468,1.24E-06,8.34E-05,9.29E-06
3760,text_summarization9,102,( the factor by which to decrease ?,Datasets and Evaluation,Architectural Choices,text_summarization,9,14,1,0,,7.83E-06,0,negative,1.90E-05,1.14E-07,2.90E-05,9.20E-08,1.33E-07,4.04E-05,5.58E-05,2.25E-05,9.01E-07,0.999807337,4.40E-07,2.33E-05,1.05E-06
3761,text_summarization9,103,"if the validation perplexity increases ) , and the gradient clipping threshold ?.",Datasets and Evaluation,Architectural Choices,text_summarization,9,15,1,0,,0.000517207,0,negative,0.000115323,1.64E-06,0.000238082,1.42E-07,4.38E-07,0.000222479,0.000649539,0.000458477,4.28E-06,0.998112966,1.95E-06,0.000186303,8.39E-06
3762,text_summarization9,104,"Our final Elman architecture ( RAS - Elman ) uses a single layer with H = 512 , ? = 0.5 , ? = 2 , and ? = 10 . ",Datasets and Evaluation,Architectural Choices,text_summarization,9,16,1,1,,0.368612981,0,negative,0.000219327,3.57E-05,0.026090058,1.17E-05,1.57E-05,0.062588231,0.424540802,0.022694439,8.88E-05,0.460374109,9.49E-06,0.000794439,0.002537274
3763,text_summarization9,105,"The LSTM model ( RAS - LSTM ) also has a single layer with H = 512 , ? = 0.1 , ? = 2 , and ? = 10 .",Datasets and Evaluation,Architectural Choices,text_summarization,9,17,1,1,experimental-setup,0.414586084,0,negative,9.87E-05,1.76E-05,0.019155901,6.54E-06,5.63E-06,0.073698178,0.31362398,0.038500255,6.22E-05,0.553079498,5.88E-06,0.000344325,0.001401331
3764,text_summarization9,106,Results,,,text_summarization,9,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
3765,text_summarization9,107,On the Gigaword corpus we evaluate our models in terms of perplexity on a held - out set .,Results,Results,text_summarization,9,1,1,0,,0.477596884,0,negative,0.004875578,2.53E-05,0.000507233,2.81E-06,5.78E-06,3.92E-05,0.002167118,0.00018623,5.54E-06,0.595312462,3.56E-05,0.396711683,0.000125431
3766,text_summarization9,108,"We then pick the model with best perplexity on the held - out set and use it to compute the F1-score of ROUGE - 1 , ROUGE - 2 , and ROUGE - L on the test sets , all of which we report .",Results,Results,text_summarization,9,2,1,0,,0.040837394,0,negative,0.008273001,0.000130372,0.002929077,7.96E-06,1.28E-05,0.000379865,0.001511808,0.002427338,8.80E-05,0.953850889,1.97E-05,0.030217725,0.000151448
3767,text_summarization9,109,"For the DUC corpus however , inline with the standard , we report the recall - only ROUGE .",Results,Results,text_summarization,9,3,1,0,,0.010923744,0,negative,0.005016681,5.19E-06,0.000413536,3.00E-06,7.20E-06,3.86E-05,0.00193587,0.000142866,2.28E-06,0.630439627,1.54E-05,0.361881725,9.80E-05
3768,text_summarization9,110,As baseline we use the state - of - the - art attention - based system ( ABS ) of which relies on a feed - forward network decoder .,Results,Results,text_summarization,9,4,1,0,,0.893909639,1,baselines,0.004972976,0.000187214,0.795629134,1.00E-05,1.06E-05,0.000185092,0.002795139,0.000445903,0.000362654,0.175441173,0.000226222,0.018995417,0.000738457
3769,text_summarization9,111,"Additionally , we compare to an enhanced version of their system ( ABS + ) , which relies on a range of separate extractive summarization features thatare added as log -linear features in a secondary learning step with minimum error rate training .",Results,Results,text_summarization,9,5,1,0,,0.025214326,0,negative,0.01423932,0.00038211,0.17835226,2.20E-05,3.82E-05,0.000217488,0.002570047,0.00053164,0.000314598,0.715549596,0.000210545,0.086949527,0.000622654
3770,text_summarization9,112,shows that both our RAS - Elman and RAS - LSTM models achieve lower perplexity than ABS as well as other models reported in .,Results,Results,text_summarization,9,6,1,1,results,0.948755751,1,results,0.003381745,3.59E-07,4.99E-05,3.21E-07,3.89E-07,3.65E-06,0.00214694,1.53E-05,8.38E-08,0.033323243,3.92E-06,0.960992074,8.21E-05
3771,text_summarization9,113,"The RAS - LSTM performs slightly worse than RAS - Elman , most likely due to over-fitting .",Results,Results,text_summarization,9,7,1,1,results,0.941017948,1,results,0.025818294,9.78E-07,8.43E-05,8.66E-06,1.59E-06,3.21E-05,0.006609907,0.000118095,7.10E-07,0.020103171,6.28E-06,0.946460971,0.000754884
3772,text_summarization9,114,We attribute this to the relatively simple nature of this task which can be framed as English - to - English translation with few long - term dependencies .,Results,Results,text_summarization,9,8,1,0,,0.001390984,0,negative,0.005126263,4.02E-05,0.000889495,5.12E-05,2.66E-05,0.000107347,0.000231986,0.000297522,8.28E-05,0.975616338,9.49E-05,0.017012577,0.000422744
3773,text_summarization9,115,The ROUGE results show that our models comfortably outperform both ABS and ABS + by a wide margin on all metrics .,Results,Results,text_summarization,9,9,1,1,results,0.955589927,1,results,0.004701017,8.66E-07,2.94E-05,1.96E-06,7.99E-07,1.34E-05,0.004645977,7.79E-05,3.72E-07,0.023276702,6.15E-06,0.966819082,0.000426371
3774,text_summarization9,116,"This is even the case when we rely only on very fast greedy search ( k = 1 ) , while as ABS uses a much wider beam of size k = 50 ; the stronger ABS + system also uses additional extractive features which our model does not .",Results,Results,text_summarization,9,10,1,0,,0.073882403,0,negative,0.114528016,1.53E-05,0.00128093,5.11E-06,8.35E-06,7.35E-05,0.001986945,0.000284999,1.46E-05,0.467357619,1.60E-05,0.414207957,0.000220687
3775,text_summarization9,117,"These features cause ABS + to copy 92 % of words from the input into the summary , whereas our model copies only 74 % of the words leading to more abstractive summaries .",Results,Results,text_summarization,9,11,1,0,,0.090046127,0,ablation-analysis,0.576804487,1.87E-05,0.001781003,3.37E-05,2.60E-05,8.36E-05,0.001141692,0.000222774,6.01E-05,0.252398731,6.40E-06,0.167007043,0.000415708
3776,text_summarization9,118,On DUC - 2004 we report recall ROUGE as is customary on this dataset .,Results,Results,text_summarization,9,12,1,1,results,0.004192684,0,negative,0.003998485,3.83E-06,0.000439337,3.40E-06,1.03E-05,3.02E-05,0.002724885,9.76E-05,1.86E-06,0.511622134,8.36E-06,0.480812709,0.000246845
3777,text_summarization9,119,The results ( Table 3 ) show that our models are better than ABS + .,Results,Results,text_summarization,9,13,1,1,results,0.932408613,1,results,0.004307378,1.11E-06,3.07E-05,1.75E-06,1.14E-06,3.22E-05,0.005915685,0.000186182,5.64E-07,0.068493104,7.98E-06,0.92058817,0.000434052
3778,text_summarization9,120,However the improvements are smaller than for Gi-gaword which is likely due to two reasons :,Results,Results,text_summarization,9,14,1,0,,0.028843169,0,results,0.175104434,6.92E-06,0.000503559,3.78E-06,5.32E-06,2.27E-05,0.00164608,9.38E-05,4.83E-06,0.359963529,9.22E-06,0.462485174,0.000150593
3779,text_summarization9,121,"First , tokenization of DUC - 2004 differs slightly from our training corpus .",Results,Results,text_summarization,9,15,1,0,,0.002638824,0,negative,0.036268874,8.24E-06,0.000731522,5.15E-05,6.04E-05,6.69E-05,0.002865678,0.000105427,5.84E-06,0.61214026,2.57E-05,0.346963824,0.000705861
3780,text_summarization9,122,"Second , headlines in Gigaword are much shorter than in For the sake of completeness we also compare our models to the recently proposed standard Neural Machine Translation ( NMT ) systems .",Results,Results,text_summarization,9,16,1,0,,0.004607809,0,negative,0.005833629,1.54E-05,0.001031388,4.58E-05,0.000102371,0.000136301,0.001027139,0.000264032,1.65E-05,0.956675485,8.08E-06,0.034497883,0.000346015
3781,text_summarization9,123,"In particular , we compare to a smaller re-implementation of the attentive stacked LSTM encoder - decoder of .",Results,Results,text_summarization,9,17,1,0,,0.265752384,0,negative,0.045571374,0.000109928,0.014721906,1.04E-05,2.35E-05,8.62E-05,0.001336736,0.000377807,0.000161884,0.844192475,1.25E-05,0.093097305,0.000297997
3782,text_summarization9,124,Our implementation uses two - layer LSTMs for the encoder - decoder with 500 hidden units in each layer .,Results,Results,text_summarization,9,18,1,0,,0.900380949,1,hyperparameters,0.039662965,0.00151353,0.010368375,0.007507101,0.000431366,0.077386914,0.065723327,0.348962752,0.004114762,0.255420151,0.000218681,0.014220369,0.174469707
3783,text_summarization9,125,Tables 2 and 3 report ROUGE scores on the two data sets .,Results,Results,text_summarization,9,19,1,0,,0.022637066,0,negative,0.001593767,1.31E-06,3.41E-05,6.03E-07,1.72E-06,1.63E-05,0.000753692,0.000102359,7.58E-07,0.773378866,2.16E-06,0.224042438,7.19E-05
3784,text_summarization9,126,From the tables we observe that the proposed RAS - Elman model is able to match the performance of the NMT model of .,Results,Results,text_summarization,9,20,1,0,,0.808552886,1,results,0.009924165,3.96E-07,2.19E-05,6.56E-07,5.04E-07,6.34E-06,0.002223859,4.04E-05,2.27E-07,0.038003597,1.36E-06,0.949616904,0.000159739
3785,text_summarization9,127,This is noteworthy because RAS - Elman is significantly simpler than the NMT model at multiple levels .,Results,Results,text_summarization,9,21,1,0,,0.001507465,0,negative,0.036203706,5.36E-06,0.00089959,1.27E-06,2.50E-06,1.62E-05,0.000427138,6.23E-05,1.03E-05,0.797988303,4.21E-06,0.16430461,7.46E-05
3786,text_summarization9,128,"First , the encoder used by RAS - Elman is extremely light - weight ( attention over the convolutional representation of the input words ) , compared to Luong 's ( a 2 hidden layer LSTM ) .",Results,Results,text_summarization,9,22,1,0,,0.079476789,0,negative,0.394535619,5.44E-05,0.006291822,0.000121391,6.71E-05,0.000209811,0.0025628,0.000416478,8.77E-05,0.402200515,1.70E-05,0.192249812,0.001185572
3787,text_summarization9,129,"Second , the decoder used by RAS - Elman is a single layer standard RNN as opposed to a multi - layer LSTM .",Results,Results,text_summarization,9,23,1,0,,0.003848862,0,negative,0.075984469,0.000192452,0.111545833,0.000197254,0.000102466,0.000610077,0.003276775,0.001039754,0.000841946,0.746532687,9.13E-05,0.055023075,0.004561898
3788,text_summarization9,130,"In an independent work , also trained a collection of standard NMT models and report numbers in the same ballpark as RAS - Elman on both datasets .",Results,Results,text_summarization,9,24,1,0,,0.001413559,0,negative,0.00225109,7.89E-06,0.001204311,1.02E-05,1.87E-05,0.00010265,0.000870699,0.000242076,1.05E-05,0.963628427,7.82E-06,0.031124079,0.000521561
3789,text_summarization9,131,"In order to better understand which component of the proposed architecture is responsible for the improvements , we trained the recurrent model with 's ABS encoder on a subset of the Gigaword dataset .",Results,Results,text_summarization,9,25,1,0,,0.012927207,0,negative,0.050559641,7.72E-05,0.001156678,1.11E-05,3.31E-05,0.000118524,0.002491946,0.00083998,4.76E-05,0.789164654,4.20E-06,0.155029305,0.000466064
3790,text_summarization9,132,"The ABS encoder , which does not have the position features , achieves a final validation perplexity of 38 compared to 29 for the proposed encoder , which uses position features as well as context information .",Results,Results,text_summarization,9,26,1,0,,0.911959582,1,results,0.020850381,1.94E-06,9.96E-05,1.43E-06,1.63E-06,9.50E-06,0.004736576,6.83E-05,9.13E-07,0.042878234,1.09E-06,0.930658207,0.00069222
3791,text_summarization9,133,This clearly shows the benefits of using the position feature in the proposed encoder .,Results,Results,text_summarization,9,27,1,0,,0.156964084,0,negative,0.074663317,4.12E-06,0.000250657,1.18E-06,2.13E-06,9.66E-06,0.000395307,5.76E-05,8.38E-06,0.679286654,1.22E-06,0.245235299,8.45E-05
3792,text_summarization9,134,Finally in we highlight anecdotal examples of summaries produced by the RAS - Elman system on the Gigaword dataset .,Results,Results,text_summarization,9,28,1,0,,0.016960068,0,negative,0.003387969,3.55E-06,0.000119806,2.73E-05,3.02E-05,4.82E-05,0.000419322,0.000126351,4.87E-06,0.950278253,2.58E-06,0.045091401,0.000460202
3793,text_summarization9,135,The first two examples highlight typical improvements in the RAS model over ABS + .,Results,Results,text_summarization,9,29,1,0,,0.015269597,0,negative,0.007135761,1.75E-06,0.000127819,1.57E-06,3.70E-06,1.41E-05,0.001018504,5.98E-05,1.73E-06,0.588482977,1.39E-06,0.402943841,0.000207082
3794,text_summarization9,136,Generally the model produces more fluent summaries and is better able to capture the main actors of the input .,Results,Results,text_summarization,9,30,1,0,,0.858588848,1,results,0.016091657,7.11E-07,2.39E-05,1.12E-06,1.08E-06,5.90E-06,0.002028106,4.83E-05,5.07E-07,0.059553503,6.95E-07,0.921955316,0.000289231
3795,text_summarization9,137,"For instance in Sentence 1 , RAS - Elman correctly distinguishes the actions of "" pepe "" from "" ferreira "" , and in Sentence 2 it identifies the correct role of the "" think tank "" .",Results,Results,text_summarization,9,31,1,0,,0.000890612,0,negative,0.007954928,3.00E-06,0.00033666,7.67E-06,1.99E-05,2.90E-05,0.000444447,9.78E-05,6.85E-06,0.921058379,2.30E-06,0.069629606,0.000409467
3796,text_summarization9,138,"The final two ex -I (1 ) : brazilian defender pepe is out for the rest of the season with a knee injury , his porto coach jesualdo ferreira said saturday .",Results,Results,text_summarization,9,32,1,0,,0.001557527,0,negative,0.005039985,4.43E-06,0.000240547,4.51E-05,2.65E-05,0.000118712,0.000695583,0.000413455,1.56E-05,0.963819274,1.76E-06,0.028227591,0.001351501
3797,text_summarization9,139,"G : football : pepe out for season A + : ferreira out for rest of season with knee injury R : brazilian defender pepe out for rest of season with knee injury I ( 2 ) : economic growth in toronto will suffer this year because of sars , a think tank said friday as health authorities insisted the illness was under control in canada 's largest city .",Results,Results,text_summarization,9,33,1,0,,0.000685701,0,negative,0.004989127,4.27E-06,0.000927701,8.99E-06,9.65E-06,4.21E-05,0.000463869,0.000115975,1.96E-05,0.95209965,6.95E-06,0.040268968,0.001043224
3798,text_summarization9,140,G : sars toll on toronto economy estimated at c$ # billion A + : think tank under control in canada 's largest city R : think tank says economic growth in toronto will suffer this year I ( 3 ) : colin l. powell said nothing - a silence that spoke volumes to many in the white house on thursday morning .,Results,Results,text_summarization,9,34,1,0,,0.000612114,0,negative,0.007436886,3.98E-06,0.000622076,2.80E-05,1.65E-05,8.50E-05,0.000713134,0.000205425,2.23E-05,0.952702362,4.78E-06,0.035962574,0.002197014
3799,text_summarization9,141,"G : in meeting with former officials bush defends iraq policy A + : colin powell speaks volumes about silence in white house R : powell speaks volumes on the white house I ( 4 ) : an international terror suspect who had been under a controversial loose form of house arrest is on the run , british home secretary john reid said tuesday .",Results,Results,text_summarization,9,35,1,0,,0.001213527,0,negative,0.012352347,6.01E-06,0.000808394,5.43E-05,2.67E-05,0.000110328,0.001213602,0.000222398,2.58E-05,0.929858472,8.57E-06,0.050932921,0.004380259
3800,text_summarization9,142,G : international terror suspect slips net in britain A + : reid under house arrest terror suspect on the run R : international terror suspect under house arrest :,Results,Results,text_summarization,9,36,1,0,,0.000968814,0,negative,0.022584853,1.86E-05,0.00405708,0.000108255,4.61E-05,0.00015453,0.0066993,0.000388088,5.83E-05,0.534258899,5.36E-05,0.401792593,0.029779806
3801,text_summarization9,143,Example sentence summaries produced on Gigaword .,Results,Results,text_summarization,9,37,1,0,,0.001619809,0,negative,0.001241898,3.32E-06,0.000231402,4.48E-06,1.44E-05,4.26E-05,0.001483022,0.000196806,3.54E-06,0.866794716,3.40E-06,0.128033051,0.001947395
3802,text_summarization9,144,"I is the input , G is the true headline , A is ABS + , and R is RAS - ELMAN .",Results,Results,text_summarization,9,38,1,0,,0.000242347,0,negative,0.000745515,7.89E-06,0.000149594,3.36E-06,4.75E-06,8.29E-05,0.00015809,0.00149387,2.85E-05,0.994493535,3.74E-07,0.002549971,0.000281696
3803,text_summarization9,145,amples highlight typical mistakes of the models .,Results,Results,text_summarization,9,39,1,0,,0.000743883,0,negative,0.0008641,1.58E-06,4.56E-05,3.67E-06,3.44E-06,3.84E-05,0.000108564,0.00024777,6.05E-06,0.993218485,1.98E-07,0.005263293,0.000198748
3804,text_summarization9,146,"In Sentence 3 both models take literally the figurative use of the idiom "" a silence that spoke volumes , "" and produce fluent but nonsensical summaries .",Results,Results,text_summarization,9,40,1,0,,0.004269275,0,negative,0.012854262,7.92E-06,0.001529347,8.40E-06,5.26E-05,2.69E-05,0.000659352,9.00E-05,1.34E-05,0.887201509,1.20E-06,0.096699464,0.000855592
3805,text_summarization9,147,"In Sentence 4 the RAS model mistakes the content of a relative clause for the main verb , leading to a summary with the opposite meaning of the input .",Results,Results,text_summarization,9,41,1,0,,0.004877402,0,negative,0.020076627,3.42E-06,0.000855204,2.21E-06,1.22E-05,1.23E-05,0.000412018,4.51E-05,9.40E-06,0.858037827,5.31E-07,0.120160557,0.00037262
3806,text_summarization9,148,"These difficult cases are somewhat rare in the Gigaword , but they highlight future challenges for obtaining human - level sentence summary .",Results,Results,text_summarization,9,42,1,0,,0.003463442,0,negative,0.001556831,2.06E-06,7.52E-05,2.59E-05,1.30E-05,5.18E-05,0.000362524,0.00013895,3.82E-06,0.972048052,3.29E-06,0.024714056,0.001004527
3807,text_summarization9,149,Conclusion,,,text_summarization,9,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
3808,sentiment_analysis51,1,title,,,sentiment_analysis,51,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
3809,sentiment_analysis51,2,Fine - grained Sentiment Classification using BERT,title,title,sentiment_analysis,51,1,1,1,research-problem,0.998950752,1,research-problem,2.07E-07,0.000218111,8.83E-07,6.45E-07,1.02E-06,9.97E-07,6.62E-06,1.62E-05,9.99E-06,0.006435883,0.993307409,1.60E-06,4.03E-07
3810,sentiment_analysis51,3,abstract,,,sentiment_analysis,51,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
3811,sentiment_analysis51,4,"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic .",abstract,abstract,sentiment_analysis,51,1,1,1,research-problem,0.789941713,1,research-problem,2.41E-08,4.62E-06,1.80E-08,7.83E-07,2.68E-07,1.95E-07,5.15E-07,9.83E-07,2.42E-07,0.00542043,0.994571835,2.58E-08,6.35E-08
3812,sentiment_analysis51,5,Many natural language processing models have been proposed to solve the sentiment classification problem .,abstract,abstract,sentiment_analysis,51,2,1,0,,0.869846446,1,research-problem,1.49E-08,6.00E-06,7.66E-09,1.23E-07,7.36E-08,2.05E-07,2.04E-07,2.02E-06,3.65E-07,0.016488722,0.983502233,1.59E-08,1.69E-08
3813,sentiment_analysis51,6,"However , most of them have focused on binary sentiment classification .",abstract,abstract,sentiment_analysis,51,3,1,0,,0.011824612,0,research-problem,8.40E-08,7.08E-05,4.69E-08,1.91E-06,9.96E-07,3.46E-06,9.76E-07,3.27E-05,5.21E-06,0.116311859,0.883571829,6.62E-08,9.94E-08
3814,sentiment_analysis51,7,"In this paper , we use a promising deep learning model called BERT to solve the fine - grained sentiment classification task .",abstract,abstract,sentiment_analysis,51,4,1,0,,0.865644448,1,research-problem,1.64E-05,0.148906553,0.000160678,4.19E-05,0.000428288,4.31E-05,5.89E-05,0.000564803,0.004103033,0.087447636,0.758203851,1.60E-05,8.83E-06
3815,sentiment_analysis51,8,Experiments show that our model outperforms other popular models for this task without sophisticated architecture .,abstract,abstract,sentiment_analysis,51,5,1,0,,0.007076035,0,negative,0.000140743,0.001843165,2.33E-06,1.62E-05,0.000106794,2.54E-05,3.56E-05,0.00030981,4.58E-05,0.945111259,0.052088667,0.000272406,1.86E-06
3816,sentiment_analysis51,9,We also demonstrate the effectiveness of transfer learning in natural language processing in the process .,abstract,abstract,sentiment_analysis,51,6,1,0,,0.008327974,0,negative,1.32E-05,0.004768323,3.23E-06,0.000125531,0.001120182,3.84E-05,1.48E-05,0.000142876,0.000186882,0.914108642,0.079470425,6.19E-06,1.32E-06
3817,sentiment_analysis51,10,I. INTRODUCTION,,,sentiment_analysis,51,0,1,0,,0.000196916,0,negative,0.000783432,0.00042639,3.86E-05,0.010720081,7.66E-05,0.002184256,0.00040315,0.002121679,0.000468556,0.974938341,0.007667188,4.70E-05,0.000124609
3818,sentiment_analysis51,11,Sentiment classification is a form of text classification in which apiece of text has to be classified into one of the predefined sentiment classes .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,1,1,0,,0.484706251,0,research-problem,3.34E-07,0.000108299,2.83E-07,1.43E-05,2.37E-05,5.30E-06,3.85E-06,3.46E-06,3.87E-05,0.031247231,0.968553249,2.75E-07,1.03E-06
3819,sentiment_analysis51,12,It is a supervised machine learning problem .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,2,1,0,,0.068579803,0,research-problem,8.04E-07,0.002168768,1.54E-06,8.32E-06,7.23E-05,1.19E-05,3.38E-06,1.50E-05,0.001147387,0.163662728,0.832906671,4.45E-07,7.62E-07
3820,sentiment_analysis51,13,"In binary sentiment classification , the possible classes are positive and negative .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,3,1,0,,0.334319703,0,research-problem,2.11E-07,0.000386434,1.78E-07,1.94E-06,8.56E-06,3.04E-06,1.98E-06,6.47E-06,0.000150392,0.049814038,0.949626157,2.19E-07,3.80E-07
3821,sentiment_analysis51,14,"In fine - grained sentiment classification , there are five classes ( very negative , negative , neutral , positive , and very positive ) .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,4,1,0,,0.142537814,0,research-problem,9.45E-07,0.009282626,2.26E-06,1.37E-05,0.000223899,4.33E-05,1.53E-05,0.000107835,0.003513372,0.18209978,0.804692284,1.51E-06,3.17E-06
3822,sentiment_analysis51,15,"Sentiment classification model , like any other machine learning model , requires its input to be a fixed - sized vector of numbers .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,5,1,0,,0.461907078,0,research-problem,5.16E-07,0.001167888,7.41E-07,1.18E-05,4.46E-05,1.40E-05,4.08E-06,2.11E-05,0.00073342,0.12204463,0.875955652,4.30E-07,1.06E-06
3823,sentiment_analysis51,16,"Therefore , we need to convert a text - sequence of words represented as ASCII or Unicode - into a fixedsized vector that encodes the meaningful information of the text .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,6,1,0,,0.708764666,1,research-problem,1.20E-05,0.023278008,8.78E-06,9.57E-06,0.000228044,2.91E-05,1.14E-05,4.74E-05,0.024753896,0.375366587,0.576248613,4.80E-06,1.84E-06
3824,sentiment_analysis51,17,Many statistical and deep learning NLP models have been proposed just for that .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,7,1,0,,0.026148541,0,research-problem,5.67E-07,0.000303318,2.57E-07,1.46E-06,7.12E-06,8.21E-06,3.42E-06,1.18E-05,0.000310202,0.20414667,0.79520614,4.59E-07,3.49E-07
3825,sentiment_analysis51,18,"Recently , there has been an explosion of developments in NLP as well as other deep learning architectures .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,8,1,0,,0.311857221,0,research-problem,8.66E-07,0.000634132,5.16E-07,7.03E-06,2.11E-05,1.64E-05,7.27E-06,1.81E-05,0.000531155,0.176414368,0.822347239,7.96E-07,9.75E-07
3826,sentiment_analysis51,19,"While transfer learning ( pretraining and finetuning ) has become the de-facto standard in computer vision , NLP is yet to utilize this concept fully .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,9,1,0,,0.628508756,1,research-problem,6.01E-07,0.000345456,3.56E-07,5.98E-06,2.18E-05,4.82E-06,5.04E-06,5.27E-06,6.70E-05,0.078626429,0.920916,6.16E-07,6.41E-07
3827,sentiment_analysis51,20,"However , neural language models such as word vectors , paragraph vectors , and Glo Ve have started the transfer learning revolution in NLP .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,10,1,0,,0.064347121,0,research-problem,1.60E-06,0.000490073,7.28E-07,4.23E-06,2.50E-05,9.36E-06,9.95E-06,9.70E-06,0.000166876,0.143948597,0.855331608,1.32E-06,9.57E-07
3828,sentiment_analysis51,21,"Recently , Google researchers published BERT ( Bidirectional Encoder Representations from Transformers ) , a deep bidirectional language model based on the Transformer architecture , and advanced the state - of - the - art in many popular NLP tasks .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,11,1,0,,0.791661778,1,research-problem,8.88E-06,0.011805563,6.34E-05,1.51E-05,0.000183858,7.86E-05,8.98E-05,7.74E-05,0.021381863,0.207808724,0.758469051,8.28E-06,9.54E-06
3829,sentiment_analysis51,22,"In this paper , we use the pretrained BERT model and finetune it for the fine - grained sentiment classification task on the Stanford Sentiment Treebank ( SST ) dataset .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,12,1,1,model,0.951287374,1,approach,3.44E-05,0.860974419,8.40E-05,1.81E-05,0.001833893,7.51E-05,8.04E-05,0.000301021,0.113072807,0.017062523,0.006437801,1.73E-05,8.26E-06
3830,sentiment_analysis51,23,The rest of the paper is organized into six sections .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,13,1,0,,0.014804069,0,negative,9.90E-06,0.026844404,6.06E-06,0.000228325,0.003269576,0.000362653,2.73E-05,0.000374691,0.021791135,0.925256869,0.021822119,2.32E-06,4.65E-06
3831,sentiment_analysis51,24,"In Section II , we mention our motivation for this work .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,14,1,0,,0.006338399,0,negative,4.20E-06,0.003915232,2.48E-06,0.000266647,0.003928289,0.000145831,6.03E-06,5.34E-05,0.002244737,0.983344408,0.006086827,6.17E-07,1.26E-06
3832,sentiment_analysis51,25,"In Section III , we discuss related works .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,15,1,0,,0.013540853,0,negative,9.74E-06,0.032490006,7.25E-06,0.000181075,0.011505011,0.000261948,1.87E-05,0.000181392,0.010541854,0.936866054,0.007931678,2.31E-06,2.97E-06
3833,sentiment_analysis51,26,"In Section IV , we describe the dataset we performed our experiments on .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,16,1,0,,0.024990443,0,negative,7.34E-06,0.032383914,1.63E-05,0.000376534,0.306144414,0.00025808,3.17E-05,6.93E-05,0.002659745,0.6568847,0.001161642,3.75E-06,2.61E-06
3834,sentiment_analysis51,27,We explain our model architecture and methodology in detail in Section V .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,17,1,0,,0.0162552,0,negative,1.31E-05,0.085483615,2.08E-05,0.000360091,0.018832303,0.000525877,2.88E-05,0.000330903,0.119129599,0.772606653,0.002659861,2.51E-06,5.86E-06
3835,sentiment_analysis51,28,Then we present and analyze our results in Section VI .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,18,1,0,,0.007735388,0,negative,1.27E-05,0.013143499,4.85E-06,0.000115037,0.007599399,0.000176967,1.59E-05,0.000111428,0.005976026,0.971216568,0.001622661,3.21E-06,1.69E-06
3836,sentiment_analysis51,29,"Finally , we provide our concluding remarks in Section VII .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,19,1,0,,0.004865769,0,negative,2.99E-05,0.01237904,6.33E-06,0.00044272,0.00607208,0.000575566,4.27E-05,0.000310011,0.006782948,0.970801272,0.002547285,5.64E-06,4.46E-06
3837,sentiment_analysis51,30,II .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,51,20,1,0,,0.003289234,0,negative,3.97E-05,0.010701219,2.11E-05,3.59E-05,0.001345778,0.000214913,2.04E-05,0.000116325,0.026818222,0.958750489,0.001930398,4.03E-06,1.57E-06
3838,sentiment_analysis51,31,MOTIVATION,I. INTRODUCTION,,sentiment_analysis,51,21,1,0,,0.014183716,0,negative,3.13E-06,0.005793973,3.73E-06,4.51E-05,0.000296583,0.000290134,2.28E-05,0.000255024,0.021183085,0.95326026,0.018841834,1.03E-06,3.30E-06
3839,sentiment_analysis51,32,"We have been working on replicating the different research paper results for sentiment analysis , especially on the finegrained Stanford Sentiment Treebank ( SST ) dataset .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,22,1,0,,0.439319605,0,negative,4.22E-06,0.016936958,9.67E-06,9.48E-05,0.005807783,0.000224103,7.28E-05,0.000104255,0.000981856,0.77557454,0.200175723,7.92E-06,5.45E-06
3840,sentiment_analysis51,33,"After the popularity of BERT , researchers have tried to use it on different NLP tasks , including binary sentiment classification on SST - 2 ( binary ) dataset , and they were able to obtain state - of - the - art results as well .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,23,1,0,,0.02391892,0,negative,1.24E-06,0.001902763,1.38E-06,2.84E-05,0.000206271,0.000127783,1.52E-05,9.24E-05,0.000377875,0.768876081,0.228367527,1.19E-06,1.85E-06
3841,sentiment_analysis51,34,But we have n't yet found any experimentation done using BERT on the SST - 5 ( fine - grained ) dataset .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,24,1,0,,0.011990787,0,negative,9.53E-07,0.001132713,1.33E-06,1.85E-05,0.000507665,0.000127244,1.43E-05,5.46E-05,0.00024074,0.965624109,0.032275211,1.68E-06,9.62E-07
3842,sentiment_analysis51,35,"Because BERT is so powerful , fast , and easy to use for downstream tasks , it is likely to give promising results in SST - 5 dataset as well .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,25,1,0,,0.022602512,0,negative,0.000464031,0.017005094,3.04E-05,4.71E-05,0.006737763,0.000181724,0.000338735,0.000178832,0.001613099,0.963320134,0.0094975,0.000576848,8.75E-06
3843,sentiment_analysis51,36,This became the main motivation for pursuing this work .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,26,1,0,,0.000387109,0,negative,7.14E-07,0.001802767,8.20E-07,4.33E-05,0.000401783,0.000104468,3.22E-06,5.47E-05,0.001534198,0.983407301,0.012646041,2.60E-07,4.98E-07
3844,sentiment_analysis51,37,III .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,27,1,0,,0.000803253,0,negative,2.60E-05,0.009275876,1.17E-05,1.26E-05,0.001000084,0.000117918,5.75E-06,7.35E-05,0.020177012,0.968499355,0.000797568,2.23E-06,4.10E-07
3845,sentiment_analysis51,38,RELATED WORK,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,28,1,0,,0.000611181,0,negative,9.08E-07,0.001665688,1.41E-06,5.55E-06,0.00023959,8.43E-05,8.54E-06,4.57E-05,0.000865095,0.986494239,0.010587677,8.57E-07,4.75E-07
3846,sentiment_analysis51,39,"Sentiment classification is one of the most popular tasks in NLP , and so there has been a lot of research and progress in solving this task accurately .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,29,1,0,,0.658539532,1,research-problem,1.41E-06,0.000845667,2.15E-06,0.000190956,0.000720859,0.000102089,4.74E-05,3.35E-05,5.99E-05,0.304628966,0.693359088,1.63E-06,6.46E-06
3847,sentiment_analysis51,40,"Most of the approaches have focused on binary sentiment classification , most probably because there are large public datasets for it such as the IMDb movie review dataset .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,30,1,0,,0.024975335,0,negative,9.38E-07,0.00218384,1.29E-06,3.33E-05,0.000239298,0.000132667,1.90E-05,0.000104488,0.000322464,0.786183759,0.210775554,1.17E-06,2.30E-06
3848,sentiment_analysis51,41,"In this section , we only discuss some significant deep learning NLP approaches applied to sentiment classification .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,31,1,0,,0.007386433,0,negative,3.44E-06,0.037737205,6.81E-06,3.10E-05,0.003014371,0.000132313,1.84E-05,0.000125976,0.004156343,0.932182828,0.022586734,2.97E-06,1.63E-06
3849,sentiment_analysis51,42,"The first step in sentiment classification of a text is the embedding , where a text is converted into a fixed - size vector .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,32,1,0,,0.631542981,1,negative,8.17E-06,0.039163869,1.62E-05,0.000248247,0.002479418,0.000399596,5.60E-05,0.00031516,0.008727348,0.777114348,0.171453275,5.43E-06,1.29E-05
3850,sentiment_analysis51,43,"Since the number of words in the vocabulary after tokenization and stemming is limited , researchers first tackled the problem of learning word embeddings .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,33,1,0,,0.070626085,0,negative,1.22E-06,0.003283711,1.34E-06,8.20E-05,0.000343878,0.000170598,1.70E-05,0.000109214,0.000477241,0.778898757,0.216611609,9.16E-07,2.56E-06
3851,sentiment_analysis51,44,The first promising language model was proposed by Mikolov et al ..,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,34,1,0,,0.001241269,0,negative,3.66E-06,0.004040387,3.34E-05,5.88E-05,0.000902156,0.000593128,7.70E-05,0.000186731,0.007154368,0.935484815,0.051454106,2.88E-06,8.54E-06
3852,sentiment_analysis51,45,They trained continuous semantic representation of words from large unlabeled text that could be fine - tuned for downstream tasks .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,35,1,0,,0.001959391,0,negative,9.47E-06,0.025033712,4.47E-05,5.37E-05,0.004371949,0.000410277,2.18E-05,0.000171289,0.026224386,0.941534862,0.00211937,1.97E-06,2.55E-06
3853,sentiment_analysis51,46,Pennington et al. used a co-occurrence matrix and only trained on nonzero elements to efficiently learn semantic word embeddings .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,36,1,0,,0.000499162,0,negative,5.32E-06,0.014243883,1.99E-05,6.09E-05,0.001723746,0.000784338,4.05E-05,0.000448109,0.007243746,0.970223415,0.005198405,2.62E-06,5.14E-06
3854,sentiment_analysis51,47,Bojanowski et al. broke words into character n-grams for smaller vocabulary size and fast training .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,37,1,0,,0.001298181,0,negative,8.37E-06,0.006139024,2.28E-05,0.000115033,0.004579668,0.00062964,3.47E-05,0.000146746,0.002800997,0.982594268,0.002922001,2.09E-06,4.62E-06
3855,sentiment_analysis51,48,The next step is to combine a variable number of word vectors into a single fixed - size document vector .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,38,1,0,,0.766478132,1,model,2.67E-05,0.234957793,5.59E-05,1.76E-06,0.000978083,4.40E-05,1.06E-05,5.95E-05,0.622896,0.140705707,0.000259229,3.97E-06,7.08E-07
3856,sentiment_analysis51,49,"The trivial way is to take the sum or the average , but they do n't lose the ordering information of words and thus do n't give good results .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,39,1,0,,0.000349109,0,negative,1.97E-06,0.000847253,1.27E-06,2.39E-05,0.000363652,0.000179348,1.29E-05,7.62E-05,0.000252599,0.995344076,0.002894609,1.31E-06,8.65E-07
3857,sentiment_analysis51,50,Tai et al .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,40,1,0,,0.006825012,0,negative,3.83E-05,0.006067887,4.85E-05,4.22E-05,0.008416947,0.00028365,3.18E-05,8.55E-05,0.004932228,0.979642213,0.000402172,6.37E-06,2.24E-06
3858,sentiment_analysis51,51,used recursive neural networks to compute vector representation of sentences by utilizing the intrinsic tree structure of natural language sentences .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,41,1,0,,0.005205987,0,negative,5.88E-06,0.001927031,3.36E-05,6.22E-05,0.004718367,0.000339883,4.70E-05,4.63E-05,0.000664902,0.986623857,0.005524814,2.66E-06,3.63E-06
3859,sentiment_analysis51,52,Socher et al . introduced a tensor - based compositionaity function for better interaction between child nodes in recursive networks .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,42,1,0,,0.001713485,0,negative,1.10E-05,0.018749627,4.31E-05,7.31E-05,0.003301552,0.000512836,4.41E-05,0.000281754,0.012055986,0.962253818,0.002665278,3.01E-06,4.87E-06
3860,sentiment_analysis51,53,They also introduced the Stanford Sentiment Treebank ( SST ) dataset for fine - grained sentiment classification .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,43,1,0,,0.060777716,0,negative,7.55E-06,0.013817054,7.76E-05,4.97E-05,0.047675906,0.0005324,0.000140902,0.000119837,0.001565937,0.933605857,0.00238827,1.00E-05,8.91E-06
3861,sentiment_analysis51,54,Tai et al. applied various forms of long short - term memory ( LSTM ) networks and Kim applied convolutional neural networks ( CNN ) towards sentiment classification .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,44,1,0,,0.002034611,0,negative,3.60E-06,0.00561797,1.91E-05,5.40E-05,0.002794936,0.000604361,8.97E-05,0.00024666,0.001693762,0.981081729,0.007781552,3.79E-06,8.81E-06
3862,sentiment_analysis51,55,"All of the approaches mentioned above are context - free , i.e. , they generate single word embedding for each word in the vocabulary .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,45,1,0,,0.067740573,0,negative,3.46E-06,0.028187067,1.63E-05,8.77E-05,0.002003965,0.000577619,4.31E-05,0.000439345,0.00449267,0.957961862,0.006178297,3.09E-06,5.50E-06
3863,sentiment_analysis51,56,"For instance , "" bank "" would have the same representation in "" bank deposit "" and "" river bank "" .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,46,1,0,,0.000210189,0,negative,5.00E-07,0.001480223,1.81E-06,1.61E-05,0.002794798,0.000319205,1.05E-05,0.000142593,0.001123353,0.994053714,5.60E-05,5.58E-07,6.30E-07
3864,sentiment_analysis51,57,Recent language model research has been trying to train contextual embeddings .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,47,1,0,,0.044209508,0,negative,2.24E-06,0.002937853,3.73E-06,6.01E-05,0.000340412,0.000433461,7.36E-05,0.000266876,0.000752285,0.954258607,0.040858717,3.41E-06,8.72E-06
3865,sentiment_analysis51,58,Peters et al. extracted context - sensitive features from leftto - right and right - to - left LSTM - based language model .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,48,1,0,,0.000835103,0,negative,5.47E-06,0.007335454,2.65E-05,6.77E-05,0.00311939,0.000774795,7.15E-05,0.000291134,0.003728759,0.982850216,0.00171782,3.02E-06,8.19E-06
3866,sentiment_analysis51,59,"Devlin et al. proposed BERT ( Bidirectional Encoder Representations from Transformers ) , an attention - based Transformer architecture , to train deep bidirectional representations from unlabeled texts .",I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,49,1,0,,0.019861362,0,negative,3.57E-05,0.207785813,0.001200923,1.45E-05,0.002484746,0.000492875,0.000257261,0.000348791,0.38164575,0.402568859,0.003131648,1.72E-05,1.59E-05
3867,sentiment_analysis51,60,Their architecture not only obtains stateof - the - art results on many NLP tasks but also allows a high degree of parallelism since it is not based on sequential or recurrent connections .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,50,1,0,,0.002767016,0,negative,1.19E-05,0.026387334,0.000263431,3.12E-05,0.006493651,0.00051685,6.69E-05,9.77E-05,0.043628175,0.921592969,0.00089899,5.78E-06,5.08E-06
3868,sentiment_analysis51,61,IV .,I. INTRODUCTION,MOTIVATION,sentiment_analysis,51,51,1,0,,0.000513518,0,negative,1.28E-05,0.006326334,6.15E-06,4.68E-06,0.00077074,0.000206383,1.47E-05,0.000160193,0.010715816,0.981760856,1.91E-05,1.76E-06,4.24E-07
3869,sentiment_analysis51,62,DATASET,I. INTRODUCTION,,sentiment_analysis,51,52,1,0,,0.001680162,0,negative,2.76E-06,0.001007975,4.82E-06,8.19E-05,0.001071736,0.001272573,0.000223511,0.000537365,0.001395741,0.994205989,0.000177546,5.17E-06,1.29E-05
3870,sentiment_analysis51,63,Stanford Sentiment Treebank ( SST ) is one of the most popular publicly available datasets for fine - grained sentiment classification task .,I. INTRODUCTION,DATASET,sentiment_analysis,51,53,1,0,,0.481476912,0,negative,4.12E-06,0.009581294,0.000128391,0.001699642,0.4768043,0.006006174,0.001633606,0.000302129,0.000189804,0.50118256,0.002367419,9.14E-06,9.14E-05
3871,sentiment_analysis51,64,"It contains 11,855 one - sentence movie reviews extracted from Rotten Tomatoes .",I. INTRODUCTION,DATASET,sentiment_analysis,51,54,1,0,,0.425607472,0,dataset,1.46E-07,0.000532088,2.43E-06,9.63E-05,0.996743867,6.79E-05,6.54E-06,1.64E-06,4.23E-06,0.002544334,1.88E-07,8.33E-08,2.79E-07
3872,sentiment_analysis51,65,"Not only that , each sentence is also parsed by the Stanford constituency parser into a tree structure with the whole sentence as the root node and the individual words as leaf nodes .",I. INTRODUCTION,DATASET,sentiment_analysis,51,55,1,0,,0.274047493,0,model,1.78E-05,0.241237514,0.000540626,1.70E-06,0.022976451,0.000172033,4.96E-05,3.61E-05,0.420894143,0.314039404,3.11E-05,2.50E-06,9.42E-07
3873,sentiment_analysis51,66,"Moreover , each node is labeled by at least three humans .",I. INTRODUCTION,DATASET,sentiment_analysis,51,56,1,0,,0.035061415,0,negative,1.88E-05,0.313315423,0.000163759,4.71E-06,0.039317104,0.000499427,7.35E-05,0.000250993,0.252391563,0.393955021,6.28E-06,2.24E-06,1.18E-06
3874,sentiment_analysis51,67,"In total , SST contains 215,154 unique manually labeled texts of varying lengths .",I. INTRODUCTION,DATASET,sentiment_analysis,51,57,1,0,,0.718501774,1,dataset,8.48E-08,0.000307308,1.08E-06,0.000140568,0.995547285,0.00011869,5.48E-06,3.01E-06,2.86E-06,0.003873286,6.57E-08,4.32E-08,2.28E-07
3875,sentiment_analysis51,68,a sample review from the SST dataset in a parsetree structure with all its nodes labeled .,I. INTRODUCTION,DATASET,sentiment_analysis,51,58,1,0,,0.008104356,0,negative,5.78E-06,0.031455077,0.000188448,1.15E-05,0.227023444,0.000660837,9.74E-05,6.57E-05,0.00565853,0.734814181,1.41E-05,2.98E-06,1.94E-06
3876,sentiment_analysis51,69,"Therefore , this dataset can be used to train models to learn the sentiment of words , phrases , and sentences together .",I. INTRODUCTION,DATASET,sentiment_analysis,51,59,1,0,,0.113880256,0,dataset,9.71E-07,0.001496373,5.56E-06,5.00E-05,0.806299192,0.000239718,2.12E-05,1.28E-05,5.82E-05,0.191814256,6.99E-07,7.00E-07,3.62E-07
3877,sentiment_analysis51,70,"There are five sentiment labels in SST : 0 ( very negative ) , 1 ( negative ) , 2 ( neutral ) , 3 ( positive ) , and 4 ( very positive ) .",I. INTRODUCTION,DATASET,sentiment_analysis,51,60,1,0,,0.234022031,0,negative,2.72E-06,0.087045805,3.26E-05,2.72E-05,0.04441505,0.015615794,0.000855073,0.01083868,0.015668826,0.82546051,1.78E-05,1.94E-06,1.81E-05
3878,sentiment_analysis51,71,"If we only consider positivity and negativity , we get the binary SST - 2 dataset .",I. INTRODUCTION,DATASET,sentiment_analysis,51,61,1,0,,0.371941989,0,negative,4.97E-05,0.076791961,0.000653626,5.06E-06,0.234309547,0.000831471,0.000639926,8.44E-05,0.008392113,0.678197952,8.87E-06,3.26E-05,2.80E-06
3879,sentiment_analysis51,72,"If we consider all five labels , we get SST - 5 .",I. INTRODUCTION,DATASET,sentiment_analysis,51,62,1,0,,0.061614968,0,negative,0.000546985,0.076654585,0.000362543,2.05E-05,0.100474146,0.002512422,0.00271602,0.000594931,0.011302593,0.804708578,1.43E-05,7.90E-05,1.35E-05
3880,sentiment_analysis51,73,"For this research , we evaluate the performance of various models on all nodes as well as on just the root nodes , and on both SST - 2 and SST - 5 .",I. INTRODUCTION,DATASET,sentiment_analysis,51,63,1,0,,0.177965166,0,negative,8.09E-06,0.252092799,9.05E-05,4.92E-06,0.091962081,0.001042921,0.000455321,0.000382636,0.010893183,0.643048159,8.58E-06,8.62E-06,2.23E-06
3881,sentiment_analysis51,74,V .,I. INTRODUCTION,,sentiment_analysis,51,64,1,0,,0.001227567,0,negative,6.66E-06,0.001454263,3.12E-06,6.10E-06,0.000443095,0.000293381,6.10E-05,0.000199371,0.002470583,0.995055911,3.26E-06,1.61E-06,1.67E-06
3882,sentiment_analysis51,75,METHODOLOGY,,,sentiment_analysis,51,0,1,0,,0.043255215,0,negative,1.91E-05,0.000159627,1.45E-05,1.37E-06,4.99E-07,0.000139902,0.000211372,0.00286638,0.000149264,0.913404109,0.082542361,0.000474296,1.72E-05
3883,sentiment_analysis51,76,"Sentiment classification takes a natural language text as input and outputs a sentiment score ? { 0 , 1 , 2 , 3 , 4 }.",METHODOLOGY,METHODOLOGY,sentiment_analysis,51,1,1,0,,0.000278935,0,negative,6.79E-06,7.70E-05,5.75E-05,2.60E-06,6.45E-07,2.24E-05,7.42E-05,0.000315322,1.71E-05,0.897448733,0.101794543,0.000174715,8.44E-06
3884,sentiment_analysis51,77,"Our method has three stages from input sentence to output score , which are described below .",METHODOLOGY,METHODOLOGY,sentiment_analysis,51,2,1,0,,0.000106292,0,negative,4.12E-05,0.00096119,0.000545054,2.73E-06,2.80E-06,6.96E-05,2.21E-05,0.001002028,0.002128215,0.994645591,0.000514101,5.95E-05,5.94E-06
3885,sentiment_analysis51,78,We use pretrained BERT model to build a sentiment classifier .,METHODOLOGY,METHODOLOGY,sentiment_analysis,51,3,1,0,,0.000412525,0,negative,2.67E-05,0.00163922,0.000367749,1.07E-06,1.07E-06,0.000586839,6.90E-05,0.035440257,0.003011923,0.958742643,7.56E-05,3.35E-05,4.45E-06
3886,sentiment_analysis51,79,"Therefore , in this section , we briefly explain BERT and then describe our model architecture .",METHODOLOGY,METHODOLOGY,sentiment_analysis,51,4,1,0,,6.89E-07,0,negative,1.52E-05,7.01E-05,3.99E-05,3.20E-08,3.26E-07,2.33E-06,8.87E-07,3.31E-05,5.11E-05,0.999749601,5.17E-06,3.22E-05,5.16E-08
3887,sentiment_analysis51,80,A. BERT,METHODOLOGY,,sentiment_analysis,51,5,1,0,,0.00018283,0,negative,0.000130151,3.31E-05,0.001366067,1.47E-06,1.17E-06,2.05E-05,8.40E-05,9.57E-05,3.10E-05,0.996158947,0.000593653,0.0014796,4.56E-06
3888,sentiment_analysis51,81,BERT ( Bidirectional Encoder Representations from Transformers is an embedding layer designed to train deep bidirectional representations from unlabeled texts by jointly conditioning on both left and right context in all layers .,METHODOLOGY,A. BERT,sentiment_analysis,51,6,1,0,,0.007082839,0,negative,0.000348927,0.009343074,0.115193901,8.17E-06,6.43E-05,0.000651339,0.000535872,0.000728119,0.045706088,0.823763214,0.00319741,0.000417079,4.25E-05
3889,sentiment_analysis51,82,It is pretrained from a large unsupervised text corpus ( such as Wikipedia dump or BookCorpus ) using the following objectives :,METHODOLOGY,A. BERT,sentiment_analysis,51,7,1,0,,0.000104677,0,negative,0.000116507,0.007872311,0.000832475,1.41E-06,3.69E-05,0.000160996,4.25E-05,0.00081328,0.003311525,0.986591614,0.000112821,0.00010638,1.33E-06
3890,sentiment_analysis51,83,Masked word prediction :,METHODOLOGY,A. BERT,sentiment_analysis,51,8,1,0,,0.01010901,0,negative,0.00018379,0.00060293,0.008071757,5.21E-07,2.27E-06,5.79E-05,0.000555497,9.78E-05,0.000998579,0.944988224,0.039144935,0.005284903,1.09E-05
3891,sentiment_analysis51,84,"In this task , 15 % of the words in the input sequence are masked out , the entire sequence is fed to a deep bidirectional Transfomer encoder , and then the model learns to predict the masked words .",METHODOLOGY,A. BERT,sentiment_analysis,51,9,1,0,,0.000626512,0,negative,0.000191405,0.000992972,0.000542468,1.36E-06,8.17E-05,0.000108867,7.01E-05,0.000210658,0.000412132,0.997074374,2.26E-05,0.000290404,1.06E-06
3892,sentiment_analysis51,85,Next sentence prediction :,METHODOLOGY,A. BERT,sentiment_analysis,51,10,1,0,,0.136033444,0,negative,0.000152908,0.000529275,0.0020882,5.39E-06,1.67E-05,0.000105612,0.003915681,0.000193835,0.000196146,0.77331832,0.198425522,0.020947218,0.000105143
3893,sentiment_analysis51,86,"To learn the relationship between sentences , BERT takes two sentences A and B as inputs and learns to classify whether B actually follows A or is it just a random sentence .",METHODOLOGY,A. BERT,sentiment_analysis,51,11,1,0,,0.000138409,0,negative,1.75E-05,0.001094986,0.00189471,1.06E-07,2.29E-06,1.97E-05,9.29E-06,5.87E-05,0.013287551,0.98343271,0.000152243,2.96E-05,6.55E-07
3894,sentiment_analysis51,87,"Unlike traditional sequential or recurrent models , the attention architecture processes the whole input sequence at once , enabling all input tokens to be processed in parallel .",METHODOLOGY,A. BERT,sentiment_analysis,51,12,1,0,,0.000529,0,negative,8.13E-05,0.002717091,0.001759982,3.13E-07,7.05E-06,4.32E-05,1.84E-05,8.17E-05,0.042275393,0.952850185,0.000107389,5.68E-05,1.23E-06
3895,sentiment_analysis51,88,The layers of BERT architecture are visualized in Pretrained BERT model can be fine - tuned with just one additional layer to obtain state - of - the - art results in a wide range of NLP tasks .,METHODOLOGY,A. BERT,sentiment_analysis,51,13,1,0,,2.55E-05,0,negative,8.75E-05,0.000231133,3.32E-05,1.29E-06,5.49E-06,0.000177805,2.15E-05,0.000481365,0.0017012,0.997172481,2.00E-05,6.60E-05,1.04E-06
3896,sentiment_analysis51,89,There are two variants for BERT models :,METHODOLOGY,A. BERT,sentiment_analysis,51,14,1,0,,0.0008368,0,negative,5.43E-05,0.000231025,0.009924036,2.37E-07,2.09E-06,4.89E-05,6.82E-05,7.22E-05,0.000834835,0.988288374,0.000225657,0.000248607,1.47E-06
3897,sentiment_analysis51,90,BERT BASE and BERT LARGE .,METHODOLOGY,A. BERT,sentiment_analysis,51,15,1,0,,0.00033936,0,negative,0.000106589,2.42E-05,0.000188489,2.74E-06,2.42E-05,0.000315146,0.000196883,0.000159739,7.90E-05,0.998254652,8.23E-06,0.000637812,2.34E-06
3898,sentiment_analysis51,91,The difference between them is listed in,METHODOLOGY,A. BERT,sentiment_analysis,51,16,1,0,,2.04E-07,0,negative,7.09E-07,8.32E-07,2.36E-07,1.82E-08,1.28E-07,3.08E-06,4.97E-07,4.63E-06,4.42E-06,0.999980988,2.64E-06,1.81E-06,1.08E-08
3899,sentiment_analysis51,92,OpenAI GPT,METHODOLOGY,A. BERT,sentiment_analysis,51,17,1,0,,3.60E-05,0,negative,3.51E-05,3.07E-05,9.67E-05,3.39E-06,1.31E-05,0.000712163,0.000553673,0.000506097,0.000187161,0.997304603,7.02E-05,0.00047673,1.04E-05
3900,sentiment_analysis51,93,Lstm Lstm,METHODOLOGY,,sentiment_analysis,51,18,1,0,,0.000511479,0,negative,0.000210682,7.06E-05,0.003174505,5.71E-06,4.77E-06,0.00029547,0.000389434,0.00149144,0.000622157,0.992820613,6.25E-05,0.000813269,3.89E-05
3901,sentiment_analysis51,94,Lstm Lstm sequence embedding that can be used for classifying the whole sequence .,METHODOLOGY,Lstm Lstm,sentiment_analysis,51,19,1,0,,0.000556642,0,negative,5.27E-06,2.92E-06,4.77E-05,4.26E-08,5.19E-08,6.15E-06,2.66E-06,3.99E-05,0.000118965,0.999745283,2.13E-06,2.86E-05,3.50E-07
3902,sentiment_analysis51,95,B. Preprocessing,METHODOLOGY,,sentiment_analysis,51,20,1,0,,0.000165865,0,negative,1.03E-05,8.44E-05,8.75E-06,4.59E-07,6.91E-07,3.29E-05,8.42E-06,0.000836188,9.64E-05,0.998854158,1.34E-05,5.29E-05,1.11E-06
3903,sentiment_analysis51,96,We perform the following preprocessing stepson the review text before we feed them into out model .,METHODOLOGY,B. Preprocessing,sentiment_analysis,51,21,1,0,,1.43E-05,0,negative,6.95E-05,0.000921997,1.47E-05,4.52E-07,3.52E-05,0.000111838,1.88E-06,0.000125109,0.000856845,0.997851808,3.21E-06,7.16E-06,2.32E-07
3904,sentiment_analysis51,97,1 ) Canonicalization :,METHODOLOGY,B. Preprocessing,sentiment_analysis,51,22,1,0,,0.026000632,0,negative,8.87E-05,0.001893084,0.00093472,6.70E-08,7.15E-06,5.80E-05,6.60E-06,3.90E-05,0.002328988,0.994534634,5.42E-05,5.46E-05,3.16E-07
3905,sentiment_analysis51,98,"First , we remove all the digits , punctuation symbols and accent marks , and convert everything to lowercase .",METHODOLOGY,B. Preprocessing,sentiment_analysis,51,23,1,0,,0.000178823,0,negative,0.000356025,0.001679203,8.27E-05,1.11E-06,0.000190648,0.000202995,8.71E-06,0.00012057,0.000896632,0.996434756,1.78E-06,2.44E-05,4.20E-07
3906,sentiment_analysis51,99,2 ) Tokenization :,METHODOLOGY,B. Preprocessing,sentiment_analysis,51,24,1,0,,0.00779142,0,negative,0.000289553,0.003581684,0.005567816,2.19E-07,2.34E-05,0.000150466,3.06E-05,6.40E-05,0.006302164,0.983680606,0.000160409,0.000147355,1.68E-06
3907,sentiment_analysis51,100,We then tokenize the text using the Word - Piece tokenizer .,METHODOLOGY,B. Preprocessing,sentiment_analysis,51,25,1,0,,0.006016132,0,negative,5.73E-05,0.003331839,0.000259913,5.65E-07,7.38E-05,0.000606452,1.75E-05,0.000467292,0.00673,0.988441796,5.42E-06,6.74E-06,1.40E-06
3908,sentiment_analysis51,101,"It breaks the words down to their prefix , root , and suffix to handle unseen words better .",METHODOLOGY,B. Preprocessing,sentiment_analysis,51,26,1,0,,0.000364395,0,negative,9.30E-05,0.002031179,0.000686002,1.08E-06,0.000106481,0.000428416,1.29E-05,0.0001808,0.006690101,0.989749273,9.73E-06,9.47E-06,1.47E-06
3909,sentiment_analysis51,102,"For example , playing ?",METHODOLOGY,B. Preprocessing,sentiment_analysis,51,27,1,0,,9.22E-07,0,negative,1.77E-06,6.94E-06,6.63E-07,2.43E-08,2.68E-06,1.28E-05,6.24E-07,7.01E-06,7.39E-06,0.999953863,3.70E-06,2.50E-06,2.20E-08
3910,sentiment_analysis51,103,play + ##ing .,METHODOLOGY,B. Preprocessing,sentiment_analysis,51,28,1,0,,1.44E-06,0,negative,2.19E-05,1.08E-05,3.74E-06,1.03E-05,0.000267949,0.000280433,3.26E-06,2.06E-05,1.01E-05,0.99936448,1.73E-06,4.26E-06,5.30E-07
3911,sentiment_analysis51,104,3 ) Special token addition :,METHODOLOGY,B. Preprocessing,sentiment_analysis,51,29,1,0,,0.010360253,0,negative,0.000216138,0.001790117,0.005591095,1.01E-07,1.44E-05,0.000130085,2.58E-05,4.78E-05,0.007932604,0.984102194,4.25E-05,0.00010597,1.13E-06
3912,sentiment_analysis51,105,"Finally , we add the [ CLS ] and [ SEP ] tokens at the appropriate positions .",METHODOLOGY,B. Preprocessing,sentiment_analysis,51,30,1,0,,8.62E-05,0,negative,4.38E-05,0.000515807,3.63E-05,6.31E-08,5.62E-06,0.000200039,4.78E-06,0.000276537,0.004867779,0.994043438,1.15E-06,4.52E-06,2.08E-07
3913,sentiment_analysis51,106,C. Proposed Architecture,,,sentiment_analysis,51,0,1,0,,0.014257282,0,negative,0.000163175,0.004288861,0.000178469,4.42E-05,1.76E-05,0.00048846,0.000205877,0.005649878,0.006981061,0.936174278,0.045326853,0.000367434,0.000113797
3914,sentiment_analysis51,107,We build a simple architecture with just a dropout regularization and a softmax classifier layers on top of pretrained BERT layer to demonstrate that BERT can produce great results even without any sophisticated task - specific architecture .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,1,1,0,,0.042126131,0,negative,0.001385252,0.187674416,0.000613185,4.26E-05,0.000588376,0.00046918,7.46E-05,0.00489048,0.17663917,0.624065481,0.001901624,0.001621716,3.39E-05
3915,sentiment_analysis51,108,the over all architecture of our model .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,2,1,0,,0.000428207,0,negative,4.05E-05,0.003379407,6.92E-05,8.77E-06,2.34E-05,0.000266883,1.65E-05,0.00112054,0.095318265,0.898614174,0.00108846,4.01E-05,1.37E-05
3916,sentiment_analysis51,109,There are four main stages .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,3,1,0,,0.000841197,0,negative,0.000195861,0.002703208,0.00013016,3.15E-05,7.08E-05,0.000244425,1.60E-05,0.000536829,0.031971566,0.963833085,0.00019258,6.50E-05,8.91E-06
3917,sentiment_analysis51,110,The first is the proprocessing step as described earlier .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,4,1,0,,0.001532151,0,negative,0.000666885,0.00333464,9.87E-05,8.50E-06,0.000125775,0.000105213,1.09E-05,0.00033329,0.008833193,0.986203921,5.31E-05,0.000222917,3.03E-06
3918,sentiment_analysis51,111,Then we compute the sequence embedding from BERT .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,5,1,0,,0.000134259,0,negative,0.000143157,0.00153436,6.47E-05,5.03E-07,1.30E-05,3.11E-05,3.11E-06,0.000194448,0.004003635,0.993806617,3.19E-05,0.000172838,6.00E-07
3919,sentiment_analysis51,112,We then apply dropout with a probability factor of 0.1 to regularize and prevent overfitting .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,6,1,0,,0.26407981,0,negative,0.00040641,0.038712345,7.17E-05,5.03E-05,0.00010652,0.020432345,0.000341765,0.289729,0.06966272,0.580163012,0.000161103,9.53E-05,6.75E-05
3920,sentiment_analysis51,113,Dropout is only applied in training phase and not in inference phase .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,7,1,0,,0.082678291,0,negative,0.001328608,0.072621398,0.000253282,9.75E-05,0.000365926,0.00138677,6.74E-05,0.027179511,0.114493601,0.781596182,0.000245346,0.000326637,3.78E-05
3921,sentiment_analysis51,114,"Finally , the softmax classification layer will output the probabilities of the input text belonging to each of the class labels such that the sum of the probabilities is 1 .",C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,8,1,0,,0.001155681,0,negative,5.33E-05,0.005406085,8.66E-05,1.04E-06,1.02E-05,0.000226906,7.67E-06,0.001744395,0.167407227,0.824946071,8.25E-05,2.51E-05,2.87E-06
3922,sentiment_analysis51,115,The softmax layer is just a fully connected neural network layer with the softmax activation function .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,9,1,0,,0.0633487,0,negative,0.00055797,0.033266494,0.000195533,0.00028042,0.000319814,0.007760667,0.000243397,0.079506859,0.196585891,0.680696942,0.000303948,0.000123856,0.00015821
3923,sentiment_analysis51,116,The softmax function ? :,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,10,1,0,,0.004813036,0,negative,1.71E-05,0.00063855,1.61E-05,1.90E-07,1.54E-06,9.81E-05,3.10E-06,0.000939863,0.006554479,0.991654991,5.43E-05,2.12E-05,4.65E-07
3924,sentiment_analysis51,117,R K ?,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,11,1,0,,4.38E-05,0,negative,1.84E-05,9.56E-05,1.91E-06,1.13E-06,3.92E-06,7.12E-05,4.53E-06,0.000285422,0.000284738,0.999121995,4.85E-05,6.19E-05,7.33E-07
3925,sentiment_analysis51,118,R K is given in .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,12,1,0,,3.33E-05,0,negative,1.79E-05,9.79E-05,1.64E-06,5.69E-07,3.06E-06,3.85E-05,2.26E-06,0.000271415,0.000418438,0.999099444,1.43E-05,3.42E-05,4.06E-07
3926,sentiment_analysis51,119,"where z = ( z 1 , . . . , z K )",C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,13,1,0,,7.79E-06,0,negative,4.54E-06,0.000143326,8.84E-07,3.98E-07,2.21E-06,3.40E-05,1.13E-06,0.000191914,0.000603453,0.998995481,1.16E-05,1.08E-05,1.99E-07
3927,sentiment_analysis51,120,?,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,14,1,0,,4.57E-06,0,negative,5.21E-06,3.58E-05,6.31E-07,2.33E-07,1.08E-06,3.13E-05,1.29E-06,0.000124644,0.000252674,0.999523462,1.07E-05,1.28E-05,1.73E-07
3928,sentiment_analysis51,121,R K is the intermediate output of the softmax layer ( also called logits ) .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,15,1,0,,1.20E-05,0,negative,2.22E-05,0.000432849,7.59E-06,6.02E-07,6.41E-06,4.59E-05,2.35E-06,0.000335781,0.002476696,0.996633015,9.34E-06,2.69E-05,4.28E-07
3929,sentiment_analysis51,122,The output node with the highest probability is then chosen as the predicted label for the input .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,16,1,0,,7.61E-05,0,negative,1.09E-05,0.00109457,9.93E-06,1.07E-07,2.57E-06,3.71E-05,1.43E-06,0.000548935,0.015811569,0.982460984,9.16E-06,1.24E-05,2.99E-07
3930,sentiment_analysis51,123,VI .,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,17,1,0,,1.08E-05,0,negative,4.20E-05,0.000112123,1.37E-06,1.01E-06,4.25E-06,8.35E-05,2.99E-06,0.000380807,0.000587362,0.998744736,4.35E-06,3.50E-05,5.32E-07
3931,sentiment_analysis51,124,EXPERIMENTS AND RESULTS,C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,18,1,0,,0.000475077,0,negative,1.78E-05,8.59E-05,2.18E-06,2.18E-07,8.43E-06,3.49E-05,1.73E-05,0.000126032,8.76E-05,0.998782585,2.54E-05,0.000810997,7.13E-07
3932,sentiment_analysis51,125,"In this section , we discuss the results of our model and compare with it some of the popular models that solve the same problem , i.e. , sentiment classification on the SST dataset .",C. Proposed Architecture,C. Proposed Architecture,sentiment_analysis,51,19,1,0,,0.002340405,0,negative,0.000265596,0.005106989,4.29E-05,2.52E-06,0.000258742,5.10E-05,4.41E-05,0.000341659,0.000640288,0.989408061,2.33E-05,0.003812636,2.21E-06
3933,sentiment_analysis51,126,A. Comparison Models,,,sentiment_analysis,51,0,1,0,,0.331292388,0,negative,0.000205643,0.000606548,0.000545269,3.31E-07,9.79E-07,8.22E-05,0.000218941,0.001508722,0.000123535,0.981774673,0.007096857,0.007831035,5.30E-06
3934,sentiment_analysis51,127,1 ) Word embeddings :,A. Comparison Models,A. Comparison Models,sentiment_analysis,51,1,1,1,baselines,0.846262668,1,baselines,0.000171403,9.66E-06,0.88673347,4.57E-07,1.34E-07,1.83E-05,3.71E-05,0.000113639,3.20E-05,0.108862811,8.12E-05,0.003888297,5.16E-05
3935,sentiment_analysis51,128,"In this method , the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector , which is then fed to the sentiment classifier to compute the sentiment score .",A. Comparison Models,A. Comparison Models,sentiment_analysis,51,2,1,1,baselines,0.047047788,0,negative,0.000998478,0.0001798,0.209077027,4.23E-06,2.33E-06,3.26E-05,3.84E-05,0.000606559,0.000537467,0.782217509,0.000115054,0.006062943,0.000127595
3936,sentiment_analysis51,129,2 ) Recursive networks :,A. Comparison Models,A. Comparison Models,sentiment_analysis,51,3,1,1,baselines,0.166279626,0,baselines,0.000128902,2.66E-06,0.865213942,4.45E-08,3.19E-08,3.03E-06,9.30E-06,1.63E-05,4.19E-05,0.132301683,2.14E-05,0.00225223,8.51E-06
3937,sentiment_analysis51,130,Various types of recursive neural networks ( RNN ) have been applied on SST .,A. Comparison Models,A. Comparison Models,sentiment_analysis,51,4,1,1,baselines,0.092471513,0,negative,0.000455555,1.23E-05,0.033706918,3.66E-06,7.08E-07,6.67E-05,0.000153608,0.00041986,2.22E-05,0.942202225,0.00185747,0.020612608,0.000486213
3938,sentiment_analysis51,131,We compare our results with the standard RNN and the more sophisticated RNTN .,A. Comparison Models,A. Comparison Models,sentiment_analysis,51,5,1,0,,0.039309064,0,negative,0.000177321,1.61E-06,0.002069686,1.14E-07,1.00E-07,6.24E-06,1.43E-05,6.54E-05,1.75E-06,0.983141223,2.18E-06,0.014515415,4.62E-06
3939,sentiment_analysis51,132,"Both of them were trained on SST from scratch , without pretraining .",A. Comparison Models,A. Comparison Models,sentiment_analysis,51,6,1,0,,0.013099721,0,negative,0.000836407,2.10E-05,0.099859431,3.62E-06,1.56E-06,4.17E-05,4.91E-05,0.000492547,0.000109666,0.895111346,6.77E-06,0.003394373,7.24E-05
3940,sentiment_analysis51,133,3 ) Recurrent networks :,A. Comparison Models,A. Comparison Models,sentiment_analysis,51,7,1,1,baselines,0.266042929,0,baselines,9.31E-05,2.15E-06,0.914858278,4.60E-08,3.00E-08,2.86E-06,1.14E-05,1.37E-05,3.97E-05,0.082861198,2.24E-05,0.002081312,1.38E-05
3941,sentiment_analysis51,134,Sophisticated recurrent networks such as left - to - right and bidrectional LSTM networks have also been applied on SST .,A. Comparison Models,A. Comparison Models,sentiment_analysis,51,8,1,1,baselines,0.083612094,0,negative,0.000426373,4.27E-06,0.017698894,2.69E-06,5.30E-07,6.69E-05,0.000131299,0.00031758,1.57E-05,0.959302818,0.000370349,0.02115914,0.000503442
3942,sentiment_analysis51,135,4 ) Convolutional networks :,A. Comparison Models,A. Comparison Models,sentiment_analysis,51,9,1,1,baselines,0.590168774,1,baselines,9.89E-05,1.86E-06,0.939336399,5.88E-08,3.69E-08,3.22E-06,1.17E-05,1.38E-05,3.74E-05,0.059040888,1.03E-05,0.001428928,1.65E-05
3943,sentiment_analysis51,136,"In this approach , the input sequences were passed through a 1 - dimensional convolutional neural network as feature extractors .",A. Comparison Models,A. Comparison Models,sentiment_analysis,51,10,1,1,baselines,0.028048368,0,negative,0.000781479,6.85E-05,0.078388744,2.86E-06,1.57E-06,3.81E-05,1.82E-05,0.000534233,0.00105333,0.916904013,2.23E-05,0.002091766,9.49E-05
3944,sentiment_analysis51,137,B. Evaluation Metric,,,sentiment_analysis,51,0,1,0,,0.00244895,0,negative,2.35E-05,0.000188206,1.24E-05,1.02E-07,2.66E-07,4.45E-05,2.37E-05,0.00129175,4.94E-05,0.996420317,0.00152501,0.000419942,9.08E-07
3945,sentiment_analysis51,138,"Since the dataset has roughly balanced number of samples of all classes , we directly use the accuracy measure to evaluate the performance of our model and compare it with other models .",B. Evaluation Metric,B. Evaluation Metric,sentiment_analysis,51,1,1,0,,0.000420753,0,negative,0.000234136,2.32E-06,0.000328743,8.74E-08,2.33E-07,1.64E-05,6.43E-06,0.000132502,6.89E-07,0.996439488,8.77E-07,0.002837282,8.10E-07
3946,sentiment_analysis51,139,The accuracy is defined simply as follows : accuracy = number of correct predictions total number of predictions ?,B. Evaluation Metric,B. Evaluation Metric,sentiment_analysis,51,2,1,0,,0.000537243,0,negative,2.31E-05,1.39E-06,0.000205078,2.91E-07,1.10E-07,5.98E-05,6.79E-06,0.000488303,2.77E-06,0.998821567,7.52E-06,0.000378785,4.51E-06
3947,sentiment_analysis51,140,"[ 0 , 1 ] ( 2 )",B. Evaluation Metric,B. Evaluation Metric,sentiment_analysis,51,3,1,0,,0.00032545,0,negative,9.11E-06,1.96E-06,0.00029593,6.66E-08,2.22E-08,7.06E-05,3.97E-06,0.001460276,9.18E-06,0.99799801,7.53E-06,0.000140844,2.47E-06
3948,sentiment_analysis51,141,C. Results,,,sentiment_analysis,51,0,1,0,,0.45544127,0,negative,0.002046931,0.000230689,4.04E-05,7.70E-07,3.14E-06,3.29E-05,0.000536346,0.000553535,1.33E-05,0.911137789,0.003199485,0.082195437,9.25E-06
3949,sentiment_analysis51,142,The result and comparisons are shown in .,C. Results,C. Results,sentiment_analysis,51,1,1,0,,0.032200041,0,negative,0.00131713,6.93E-07,7.13E-05,4.77E-07,2.85E-07,1.51E-05,0.000291211,3.71E-05,1.51E-06,0.653099608,1.43E-05,0.344821793,0.000329502
3950,sentiment_analysis51,143,It shows the accuracy of various models on SST - 2 and SST - 5 .,C. Results,C. Results,sentiment_analysis,51,2,1,0,,0.026187493,0,negative,0.003381068,1.97E-06,0.000190209,4.80E-06,1.72E-06,3.75E-05,0.000518057,9.60E-05,4.16E-06,0.59996132,1.90E-05,0.39479294,0.000991316
3951,sentiment_analysis51,144,It includes results for all phrases as well as for just the root ( whole review ) .,C. Results,C. Results,sentiment_analysis,51,3,1,0,,0.001422499,0,negative,0.005398626,7.98E-06,0.001429005,2.55E-05,1.82E-05,8.77E-05,0.000329539,0.000132734,3.24E-05,0.868837567,1.28E-05,0.122482388,0.001205635
3952,sentiment_analysis51,145,"We can see that our model , despite being a simple architecture , performs better in terms of accuracy than many popular and sophisticated NLP models .",C. Results,C. Results,sentiment_analysis,51,4,1,1,results,0.945262304,1,results,0.001792432,2.12E-07,8.45E-06,8.96E-07,1.71E-07,4.72E-06,0.000700538,1.29E-05,1.58E-07,0.017652323,4.28E-06,0.979282088,0.000540823
3953,sentiment_analysis51,146,VII .,C. Results,C. Results,sentiment_analysis,51,5,1,0,,0.000114903,0,negative,0.006901719,7.74E-06,0.000200511,1.96E-05,2.55E-06,8.18E-05,9.16E-05,0.000228063,7.41E-05,0.96610723,1.21E-05,0.025600928,0.000672208
3954,sentiment_analysis51,147,CONCLUSION,,,sentiment_analysis,51,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
3955,text_summarization14,1,title,,,text_summarization,14,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
3956,text_summarization14,2,Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence Summarization,title,title,text_summarization,14,1,1,1,research-problem,0.99276795,1,research-problem,3.72E-08,5.90E-06,4.16E-08,1.43E-07,8.27E-08,7.91E-08,1.08E-06,8.38E-07,3.56E-07,0.002209575,0.997781649,1.53E-07,6.36E-08
3957,text_summarization14,3,abstract,,,text_summarization,14,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
3958,text_summarization14,4,"In this paper , we investigate the sentence summarization task that produces a summary from a source sentence .",abstract,abstract,text_summarization,14,1,1,1,research-problem,0.956829564,1,research-problem,1.12E-07,0.000112455,1.65E-07,1.28E-06,2.29E-06,2.74E-07,1.47E-06,2.49E-06,1.54E-06,0.007655085,0.992222528,1.45E-07,1.61E-07
3959,text_summarization14,5,"Neural sequence - to - sequence models have gained considerable success for this task , while most existing approaches only focus on improving word overlap between the generated summary and the reference , which ignore the correctness , i.e. , the summary should not contain error messages with respect to the source sentence .",abstract,abstract,text_summarization,14,2,1,0,,0.230550334,0,research-problem,1.46E-08,4.32E-06,8.05E-09,4.57E-07,1.14E-07,2.25E-07,2.32E-07,1.32E-06,3.01E-07,0.013616365,0.986376609,1.36E-08,2.60E-08
3960,text_summarization14,6,We argue that correctness is an essential requirement for summarization systems .,abstract,abstract,text_summarization,14,3,1,0,,0.041768346,0,research-problem,1.09E-07,8.90E-05,3.52E-08,8.82E-07,9.54E-07,6.61E-07,3.65E-07,7.27E-06,7.93E-06,0.089276281,0.91061631,8.78E-08,6.09E-08
3961,text_summarization14,7,"Considering a correct summary is semantically entailed by the source sentence , we incorporate entailment knowledge into abstractive summarization models .",abstract,abstract,text_summarization,14,4,1,0,,0.107388108,0,research-problem,2.31E-05,0.126684015,5.75E-05,7.27E-06,0.000156541,1.43E-05,1.07E-05,0.000230023,0.01006817,0.239203203,0.623532034,1.11E-05,2.06E-06
3962,text_summarization14,8,"We propose an entailment - aware encoder under multi-task framework ( i.e. , summarization generation and entailment recognition ) and an entailment - aware decoder by entailment Reward Augmented Maximum Likelihood ( RAML ) training .",abstract,abstract,text_summarization,14,5,1,0,,0.397267134,0,research-problem,2.85E-05,0.353704629,0.00019174,1.15E-05,0.000190172,3.24E-05,1.93E-05,0.000613527,0.069972115,0.170452625,0.404763486,1.56E-05,4.41E-06
3963,text_summarization14,9,Experimental results demonstrate that our models significantly outperform baselines from the aspects of informativeness and correctness .,abstract,abstract,text_summarization,14,6,1,0,,0.006059815,0,negative,0.000155668,0.002456045,2.19E-06,1.55E-05,9.61E-05,2.23E-05,7.66E-05,0.000380706,3.83E-05,0.888451169,0.107762881,0.000539555,3.04E-06
3964,text_summarization14,10,This work is licensed under a Creative Commons Attribution 4.0 International License .,abstract,abstract,text_summarization,14,7,1,0,,0.004710259,0,negative,2.48E-06,0.005258232,2.07E-06,0.000194138,8.79E-05,0.000267557,1.24E-05,0.002335473,0.002074473,0.918531503,0.071230001,8.01E-07,2.95E-06
3965,text_summarization14,11,License details : http:// creativecommons.org/licenses/by/4.0/.,abstract,,text_summarization,14,8,1,0,,0.00505352,0,negative,5.88E-06,0.008324804,3.40E-06,0.000730624,0.000320711,0.000282283,1.07E-05,0.001564299,0.001952054,0.954966999,0.031834486,8.34E-07,2.96E-06
3966,text_summarization14,12,Introduction,,,text_summarization,14,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
3967,text_summarization14,13,Sentence summarization is a well - studied task that creates a condensed version of along source sentence .,Introduction,Introduction,text_summarization,14,1,1,0,,0.774611711,1,research-problem,1.54E-06,9.99E-05,5.16E-07,4.97E-05,5.50E-05,8.68E-06,2.34E-05,4.65E-06,1.87E-05,0.032642338,0.967089098,1.61E-06,4.87E-06
3968,text_summarization14,14,Sequence - to - sequence ( seq2seq ) model that encodes a source sequence into a latent representation and outputs another sequence is the dominating framework for sentence summarization .,Introduction,Introduction,text_summarization,14,2,1,0,,0.716734953,1,research-problem,1.48E-06,0.001029825,3.70E-06,2.02E-06,8.16E-06,6.31E-06,1.93E-05,1.24E-05,0.001056678,0.029071631,0.968783681,2.27E-06,2.63E-06
3969,text_summarization14,15,"Despite substantial improvements on this task , most of the existing researches typically aim to improve word overlap between the generated summary and the references , which is measured by n-gram matching metrics ( e.g. , ROUGE ) .",Introduction,Introduction,text_summarization,14,3,1,0,,0.242267664,0,research-problem,1.95E-06,0.00040882,4.62E-07,7.26E-06,2.77E-05,7.09E-06,1.21E-05,1.14E-05,6.22E-05,0.122240467,0.877216918,2.14E-06,1.56E-06
3970,text_summarization14,16,"Hence , it can not guarantee the semantic correctness of the summary as a whole .",Introduction,Introduction,text_summarization,14,4,1,0,,0.014858071,0,negative,5.21E-05,0.008248484,4.41E-06,7.48E-06,0.000164253,2.38E-05,1.84E-05,5.09E-05,0.005656145,0.843736374,0.142019042,1.71E-05,1.43E-06
3971,text_summarization14,17,"Therefore , in some cases , the summary giving high matching scores may contain critical error messages , which makes the summary fail to capture the correct information with respect to the source sentence .",Introduction,Introduction,text_summarization,14,5,1,0,,0.012923648,0,negative,0.000110893,0.004110171,3.07E-06,2.09E-05,0.00068883,3.13E-05,1.81E-05,3.56E-05,0.001581007,0.974636621,0.018740797,2.14E-05,1.25E-06
3972,text_summarization14,18,Previous study shows that about 30 % of the summaries generated by state - of - the - art seq2seq system are subject to this problem .,Introduction,Introduction,text_summarization,14,6,1,0,,0.032255283,0,research-problem,4.48E-06,0.000520233,9.44E-07,7.68E-05,0.000166087,4.39E-05,2.90E-05,3.48E-05,9.15E-05,0.348178739,0.650845285,2.74E-06,5.48E-06
3973,text_summarization14,19,"Here is an example ( the digits are replaced by "" # "" ) :",Introduction,Introduction,text_summarization,14,7,1,0,,0.00045076,0,negative,4.59E-06,0.000941041,3.01E-06,5.43E-05,0.002274325,0.000195255,3.86E-05,6.00E-05,0.000845475,0.989684648,0.005892973,3.16E-06,2.60E-06
3974,text_summarization14,20,Source sentence : franch won the gold medal at women 's epee team event of the fie #### world championships by beating china ## -# # .,Introduction,Introduction,text_summarization,14,8,1,0,,0.001350582,0,negative,3.70E-05,0.001143683,1.39E-05,0.087572156,0.092430336,0.00293325,0.000315446,0.000137174,0.000378739,0.798938429,0.01602572,6.46E-06,6.77E-05
3975,text_summarization14,21,Reference : france beats china for women 's epee team gold State - of - the - art seq2seq model : canada wins women 's epee team event,Introduction,Introduction,text_summarization,14,9,1,0,,0.013931743,0,negative,6.89E-05,0.002484918,2.34E-05,0.001554335,0.00346543,0.000475685,0.000526175,0.000151593,0.001809591,0.776292869,0.213036568,5.59E-05,5.47E-05
3976,text_summarization14,22,"For the example shown above , the seq2seq system produces a fluent summary which contains an obvious mistake .",Introduction,Introduction,text_summarization,14,10,1,0,,0.02390804,0,negative,2.04E-05,0.003139154,6.47E-06,3.41E-05,0.00636258,0.000120876,8.04E-05,5.24E-05,0.000917207,0.977819444,0.011422288,2.09E-05,3.82E-06
3977,text_summarization14,23,"The true winner of the "" women 's epee team event "" is "" france "" , while the summarization model wrongly generates "" canada "" , which is probably due to similar word representations for country names .",Introduction,Introduction,text_summarization,14,11,1,0,,0.006660644,0,negative,4.06E-05,0.001739775,6.55E-06,0.00020786,0.036053353,0.000191969,0.000217674,5.97E-05,0.000106128,0.955958133,0.005360716,4.70E-05,1.05E-05
3978,text_summarization14,24,"Though the word overlap between the generated summary and the reference is considerable , leading to high ROUGE scores , the summary is invalid .",Introduction,Introduction,text_summarization,14,12,1,0,,0.11855775,0,negative,0.000336371,0.003207235,6.08E-06,2.51E-05,0.002413943,3.92E-05,6.79E-05,3.38E-05,0.000378481,0.986570851,0.006842775,7.60E-05,2.26E-06
3979,text_summarization14,25,"We argue that correctness is an essential requirement for summarization systems , while most existing systems ignore it .",Introduction,Introduction,text_summarization,14,13,1,0,,0.124121483,0,research-problem,9.71E-06,0.007897583,2.37E-06,1.53E-05,0.00011613,3.71E-05,3.86E-05,6.88E-05,0.003196497,0.439915311,0.548688263,9.95E-06,4.36E-06
3980,text_summarization14,26,"Generally , a correct summary is semantically entailed by the source sentence , thus we believe entailment 1 knowledge is beneficial to avoid producing contradictory or unrelated information in the summary .",Introduction,Introduction,text_summarization,14,14,1,0,,0.02216037,0,negative,0.000710082,0.024372914,2.84E-05,2.27E-05,0.002022074,8.49E-05,0.000133591,0.000122195,0.0117999,0.950763337,0.009735064,0.000199774,5.14E-06
3981,text_summarization14,27,"To incorporate entailment knowledge into abstractive summarization models , we propose in this work an entailment - aware encoder and an entailment - aware decoder .",Introduction,Introduction,text_summarization,14,15,1,1,model,0.962869417,1,model,2.74E-05,0.205157364,0.000176717,2.21E-06,0.000262127,2.07E-05,4.21E-05,3.76E-05,0.777679504,0.011111996,0.005464956,1.33E-05,3.91E-06
3982,text_summarization14,28,"We share the encoder of the summarization generation system with the entailment recognition system , so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships .",Introduction,Introduction,text_summarization,14,16,1,1,model,0.619254736,1,model,4.57E-05,0.255738567,0.000234335,5.74E-05,0.005689913,0.000192159,7.76E-05,0.000201139,0.691864048,0.045024283,0.000849127,1.35E-05,1.23E-05
3983,text_summarization14,29,"Furthermore , we propose an entailment Reward Augmented Maximum Likelihood ( RAML ) training that encourages the decoder of the summarization system to produce summary entailed by the source .",Introduction,Introduction,text_summarization,14,17,1,1,model,0.957976785,1,model,3.24E-05,0.263975109,0.000139785,3.59E-06,0.000525941,2.99E-05,2.94E-05,6.54E-05,0.725630503,0.008888153,0.000668286,7.88E-06,3.62E-06
3984,text_summarization14,30,Experimental results demonstrate that our models significantly outperform some solid baselines on objective evaluation for informativeness and manual evaluation for correctness .,Introduction,Introduction,text_summarization,14,18,1,0,,0.014061845,0,negative,0.002079885,0.051749754,2.74E-05,7.05E-05,0.003030951,0.000353771,0.005599971,0.001086694,0.004069288,0.918686301,0.006809278,0.006340308,9.59E-05
3985,text_summarization14,31,Further analysis suggests that our summarization model is aware of entailment knowledge .,Introduction,Introduction,text_summarization,14,19,1,0,,0.014594602,0,negative,0.001268289,0.031206296,2.92E-05,4.25E-05,0.004440347,0.000212732,0.000347388,0.000252073,0.009365351,0.949784333,0.002820172,0.000222088,9.18E-06
3986,text_summarization14,32,Our main contributions are as follows :,Introduction,Introduction,text_summarization,14,20,1,0,,0.001839898,0,negative,2.86E-05,0.010865224,1.66E-05,0.000468133,0.002850457,0.000633038,0.00010787,0.000290466,0.006314701,0.96680043,0.011598816,1.03E-05,1.54E-05
3987,text_summarization14,33,We incorporate entailment knowledge into summarization models to avoid producing unrelated information with respect to the source sentence .,Introduction,Introduction,text_summarization,14,21,1,0,,0.829413492,1,model,4.88E-05,0.443431565,0.00015193,3.95E-06,0.000910001,4.67E-05,6.10E-05,9.85E-05,0.542029042,0.012781549,0.000420283,1.19E-05,4.75E-06
3988,text_summarization14,34,We propose an entailment - aware encoder by jointly modeling summarization generation and entailment recognition .,Introduction,Introduction,text_summarization,14,22,1,0,,0.886193844,1,model,1.77E-05,0.169018477,9.62E-05,8.40E-07,0.000162012,1.24E-05,2.64E-05,2.49E-05,0.820478578,0.009052766,0.001100452,7.22E-06,2.00E-06
3989,text_summarization14,35,We introduce an entailment - aware decoder via entailment RAML training .,Introduction,Introduction,text_summarization,14,23,1,0,,0.890990351,1,model,4.80E-05,0.314259545,0.000175769,2.88E-06,0.000533724,2.47E-05,6.85E-05,5.15E-05,0.66244037,0.020166106,0.002201495,2.29E-05,4.55E-06
3990,text_summarization14,36,Background : Seq2seq Learning,,,text_summarization,14,0,1,0,,0.111913987,0,research-problem,2.75E-05,6.78E-05,7.69E-05,5.43E-06,1.00E-06,3.04E-05,0.000684244,0.000141306,1.11E-05,0.073280793,0.925209609,0.000428829,3.50E-05
3991,text_summarization14,116,Related work,,,text_summarization,14,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
3992,text_summarization14,149,Comparative Methods,,,text_summarization,14,0,1,0,,0.000519954,0,negative,1.56E-05,8.25E-05,5.27E-05,3.11E-07,5.29E-07,0.000151325,0.000118504,0.001797944,5.44E-05,0.996271118,0.001054328,0.00039842,2.30E-06
3993,text_summarization14,150,We compare a set of sentence summarization baselines .,Comparative Methods,,text_summarization,14,1,1,0,,0.113521132,0,baselines,1.43E-05,1.55E-05,0.956288895,2.72E-07,4.76E-07,1.63E-05,0.00066758,0.000178974,1.11E-06,0.041310673,0.000123214,0.001372713,9.95E-06
3994,text_summarization14,151,ABS . first apply the seq2seq model to abstractive sentence summarization .,Comparative Methods,We compare a set of sentence summarization baselines .,text_summarization,14,2,1,1,baselines,0.805494652,1,baselines,0.000796657,2.09E-05,0.695143006,8.01E-05,1.11E-05,0.000614023,0.003571734,0.000271925,2.76E-05,0.199103029,0.002340599,0.019696027,0.078323307
3995,text_summarization14,152,They use an attentive CNN encoder and neural network language model decoder to summarize sentence .,Comparative Methods,We compare a set of sentence summarization baselines .,text_summarization,14,3,1,0,,0.588915458,1,baselines,0.000922075,6.90E-06,0.908056295,2.65E-05,2.58E-06,0.000289583,0.000492931,0.000139804,8.81E-05,0.083149931,5.42E-05,0.001501998,0.005269134
3996,text_summarization14,153,ABS +. propose a neural machine translation model with two - layer LSTMs for the encoder - decoder .,Comparative Methods,We compare a set of sentence summarization baselines .,text_summarization,14,4,1,1,baselines,0.907387172,1,baselines,8.16E-05,1.24E-06,0.985303574,2.12E-06,4.19E-07,6.00E-05,0.00023975,2.71E-05,7.36E-06,0.011235745,3.64E-05,0.000483948,0.002520812
3997,text_summarization14,154,Seq2seq .,Comparative Methods,,text_summarization,14,5,1,1,baselines,0.83497413,1,baselines,3.37E-05,2.43E-06,0.951978668,1.34E-05,6.75E-07,0.000548184,0.003391434,0.001340762,4.12E-06,0.041925367,4.12E-05,0.000624586,9.54E-05
3998,text_summarization14,155,This is a standard seq2seq model with attention mechanism .,Comparative Methods,Seq2seq .,text_summarization,14,6,1,1,baselines,0.36996241,0,baselines,4.54E-06,1.06E-06,0.986666737,9.28E-08,8.19E-08,3.96E-05,0.000183153,3.98E-05,4.05E-05,0.013005773,3.00E-06,3.50E-06,1.22E-05
3999,text_summarization14,156,Seq2seq + MTL .,Comparative Methods,,text_summarization,14,7,1,1,baselines,0.77032309,1,baselines,5.27E-06,4.06E-07,0.990518058,3.08E-07,5.25E-08,2.49E-05,0.001265672,0.00010689,4.59E-07,0.007067654,3.66E-05,0.000925718,4.80E-05
4000,text_summarization14,157,"This is our proposed model with entailment - aware encoder , which applies a multi-task learning ( MTL ) framework to seq2seq model .",Comparative Methods,Seq2seq + MTL .,text_summarization,14,8,1,1,baselines,0.900275033,1,baselines,1.15E-05,6.55E-06,0.990497813,1.42E-07,2.23E-07,1.55E-05,0.000147915,2.55E-05,4.74E-05,0.009202992,5.34E-06,2.84E-05,1.07E-05
4001,text_summarization14,158,Seq2seq + MTL ( Share decoder ) .,Comparative Methods,,text_summarization,14,9,1,1,baselines,0.89326881,1,baselines,1.02E-06,5.07E-08,0.999143451,1.52E-08,4.13E-09,1.61E-06,7.42E-05,7.23E-06,1.05E-07,0.000708826,8.83E-07,6.01E-05,2.45E-06
4002,text_summarization14,159,propose a multi - task learning ( MTL ) framework in which the decoder is shared for summarization generation and entailment generation task .,Comparative Methods,Seq2seq + MTL ( Share decoder ) .,text_summarization,14,10,1,1,baselines,0.448757687,0,baselines,0.000132559,8.48E-06,0.516837376,8.69E-06,1.19E-06,0.000375597,0.003288061,0.000966037,1.64E-05,0.468459546,0.001260706,0.006367488,0.002277869
4003,text_summarization14,160,Seq2seq + ERAML .,Comparative Methods,,text_summarization,14,11,1,1,baselines,0.660948869,1,baselines,2.76E-05,9.24E-07,0.979379436,4.05E-06,3.75E-07,0.000198665,0.002347701,0.000567677,2.39E-06,0.01666512,9.58E-06,0.000688755,0.000107704
4004,text_summarization14,161,"This is our proposed model with entailment - aware decoder , which conducts an Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework .",Comparative Methods,Seq2seq + ERAML .,text_summarization,14,12,1,1,baselines,0.82528924,1,baselines,3.44E-05,0.000103805,0.962331733,5.57E-07,3.18E-06,8.01E-05,0.00057595,9.76E-05,0.000956822,0.035717683,1.51E-05,6.05E-05,2.26E-05
4005,text_summarization14,162,Seq2seq + ROUGE -2 RAML .,Comparative Methods,Seq2seq + ERAML .,text_summarization,14,13,1,1,baselines,0.163897785,0,baselines,0.000128015,1.52E-05,0.324919644,0.000287925,0.000107384,0.127074453,0.220664591,0.009160707,6.70E-05,0.315040521,3.82E-05,0.001059185,0.001437208
4006,text_summarization14,163,We apply ROUGE - 2 RAML training for seq2seq model .,Comparative Methods,Seq2seq + ERAML .,text_summarization,14,14,1,1,baselines,0.628218523,1,negative,0.000128955,0.000289809,0.252692987,5.98E-06,3.08E-05,0.034686045,0.101783292,0.090359082,0.000256076,0.519045459,9.04E-06,0.000486472,0.000225998
4007,text_summarization14,164,Seq2seq + RL .,Comparative Methods,,text_summarization,14,15,1,1,baselines,0.62328172,1,baselines,5.13E-06,3.18E-07,0.986977196,2.34E-07,6.76E-08,3.51E-05,0.000815047,0.000147499,6.15E-07,0.011563073,3.80E-06,0.000427036,2.49E-05
4008,text_summarization14,165,We implement Reinforcement Learning ( RL ) models ( policy gradient ) with reward metrics of Entailment and ROUGE - 2 .,Comparative Methods,Seq2seq + RL .,text_summarization,14,16,1,1,baselines,0.93968053,1,baselines,0.000163237,0.000161674,0.823774607,8.17E-06,8.56E-06,0.004205288,0.016264498,0.019207673,0.000463467,0.135079628,9.42E-06,0.000314806,0.00033897
4009,text_summarization14,166,Seq2seq + selective .,Comparative Methods,,text_summarization,14,17,1,1,baselines,0.743794529,1,baselines,1.85E-05,8.75E-07,0.980538379,1.60E-06,3.15E-07,0.000130601,0.001439457,0.000587021,2.13E-06,0.016805537,2.12E-06,0.00041432,5.91E-05
4010,text_summarization14,167,employ a selective encoding model to control the information flow from encoder to decoder .,Comparative Methods,Seq2seq + selective .,text_summarization,14,18,1,1,baselines,0.4078566,0,baselines,0.000175576,6.72E-06,0.831461384,1.38E-06,2.37E-06,0.000110544,0.000379262,0.000176607,6.85E-05,0.167371545,5.28E-06,0.000191182,4.97E-05
4011,text_summarization14,168,"To verify the generalization of our entailment - based strategies , we adopt selective encoding mechanism to our seq2seq model and apply MTL and RAML to Seq2seq + selective model , which is denoted as the Seq2seq + selective + MTL + RAML model .",Comparative Methods,Seq2seq + selective .,text_summarization,14,19,1,0,,0.751253175,1,baselines,0.000173961,5.58E-05,0.914824876,2.20E-07,1.94E-06,8.27E-05,0.000900251,0.000236563,0.000154377,0.083323876,1.49E-06,0.000229468,1.45E-05
4012,text_summarization14,169,Model,,,text_summarization,14,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
4013,text_summarization14,170,ROUGE - 1 ROUGE - 2 ROUGE-L ABS 37.41 15.87 34.70 Seq2seq 43 .,Model,Model,text_summarization,14,1,1,0,,0.003037447,0,negative,0.006101409,0.000217332,0.003507099,0.000378567,0.000482697,0.005023008,0.026773289,0.006422812,0.001677857,0.79637639,0.000204885,0.151281684,0.00155297
4014,text_summarization14,171,Our models perform significantly better than baselines by the 95 % confidence interval measured by the official ROUGE script .,Model,Model,text_summarization,14,2,1,0,,0.120747887,0,results,0.004278686,0.000142472,0.000272871,0.000107857,4.28E-05,0.00075811,0.013960908,0.004536684,0.000381783,0.115650979,0.000285028,0.857930963,0.001650898
4015,text_summarization14,172,Experimental Results : Gigaword Corpus,Model,Model,text_summarization,14,3,1,1,results,0.60317769,1,results,0.001033083,0.000163495,0.000921305,9.85E-07,3.20E-06,4.19E-05,0.003980866,0.00030763,0.000329531,0.285237741,0.001201602,0.706674944,0.000103763
4016,text_summarization14,173,In .,Model,,text_summarization,14,4,1,0,,0.00098683,0,negative,0.00018381,3.98E-05,0.000117126,2.29E-06,3.29E-06,5.44E-05,6.49E-05,0.00025813,0.001909255,0.996689124,3.39E-05,0.000633,1.09E-05
4017,text_summarization14,174,"The comparison to the model of Seq2seq + selective shows that our entailment - aware strategies are also useful for seq2seq model with selective encoding framework , which demonstrates the good generalization of our method .",Model,In .,text_summarization,14,5,1,0,,0.222588228,0,negative,0.035256855,5.69E-06,0.000132248,6.97E-07,5.44E-07,8.23E-06,0.000206649,6.13E-05,1.33E-05,0.511301639,1.22E-05,0.452884909,0.000115747
4018,text_summarization14,175,The results on English Gigaword test set provided by are shown in .,Model,In .,text_summarization,14,6,1,0,,0.064018104,0,negative,0.002813997,4.82E-06,0.000216419,2.47E-07,2.57E-07,5.40E-06,0.000227645,3.83E-05,1.19E-05,0.573960799,4.27E-05,0.422519124,0.00015842
4019,text_summarization14,176,Our model performs better than the previous works .,Model,,text_summarization,14,7,1,1,results,0.074577452,0,results,0.002938063,4.71E-05,0.000156596,1.49E-05,1.46E-05,0.000262971,0.006410456,0.001849615,0.000129519,0.126661223,5.53E-05,0.860962449,0.000497279
4020,text_summarization14,177,Experimental Results : DUC 2004,Model,,text_summarization,14,8,1,1,results,0.481231891,0,results,0.00065366,0.000139336,0.000911528,1.09E-06,3.37E-06,5.35E-05,0.0062331,0.00039762,0.000494299,0.306512428,0.00063425,0.683789628,0.000176169
4021,text_summarization14,178,Test Corpus,Model,,text_summarization,14,9,1,0,,0.002922368,0,negative,0.00019365,0.000136483,0.000453176,3.30E-06,2.89E-05,0.000160535,0.002933002,0.001290808,0.000687939,0.959505278,9.29E-05,0.034406077,0.000107979
4022,text_summarization14,179,We evaluate our model with the ROUGE recall score .,Model,Test Corpus,text_summarization,14,10,1,0,,0.000600832,0,negative,1.02E-05,2.46E-07,7.32E-06,4.41E-09,1.22E-08,1.58E-05,1.45E-05,0.000188544,1.46E-07,0.998955093,5.84E-08,0.000807778,1.93E-07
4023,text_summarization14,180,The reference summaries of the DUC 2004 test set are fixed to 75 bytes and we set the maximum length of the summary to 18 following .,Model,Test Corpus,text_summarization,14,11,1,0,,0.430353764,0,negative,1.20E-05,5.16E-06,3.23E-05,3.50E-07,1.08E-07,0.012174081,0.00128143,0.178904574,7.49E-06,0.807309866,1.24E-06,0.00025623,1.52E-05
4024,text_summarization14,181,"In , experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models , surpassing Feats2s by 0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L without fine - tuning on DUC data .",Model,Test Corpus,text_summarization,14,12,1,1,results,0.70544743,1,results,0.000723392,2.25E-07,3.94E-05,8.12E-08,9.50E-08,9.14E-06,0.000593222,7.08E-05,7.28E-08,0.038643767,3.91E-07,0.959900197,1.92E-05
4025,text_summarization14,182,Manual Evaluation,,,text_summarization,14,0,1,0,,0.008589318,0,negative,7.78E-05,1.02E-05,1.54E-06,1.33E-06,1.31E-06,4.07E-05,8.66E-05,0.000248166,3.18E-06,0.997894073,0.000998153,0.000634341,2.64E-06
4026,text_summarization14,183,"Next , we conduct a manual evaluation to inspect the correctness of the generated summaries .",Manual Evaluation,Manual Evaluation,text_summarization,14,1,1,0,,0.008624478,0,negative,0.002120043,2.54E-05,6.52E-05,2.22E-07,1.40E-05,1.85E-05,0.000161363,2.63E-05,1.70E-06,0.996489778,3.72E-06,0.001072962,7.82E-07
4027,text_summarization14,184,"We randomly select 500 samples in the test set and employ five postgraduates to classify the generated summaries as correct ( i.e. , not contain wrong information ) or not .",Manual Evaluation,Manual Evaluation,text_summarization,14,2,1,0,,0.001036335,0,negative,0.000267655,0.000141171,7.44E-05,7.18E-07,1.20E-05,0.001387386,0.000675753,0.004361593,2.81E-05,0.992960154,1.02E-05,7.68E-05,4.08E-06
4028,text_summarization14,185,"As shown in , 60.6 % of the summaries generated by seq2seq model are correct , and it rises to 69.4 % and 74.2 % for our model with selective encoding and entailment - aware strategies , respectively , which indicates the effectiveness of our model to generate a correct summary .",Manual Evaluation,Manual Evaluation,text_summarization,14,3,1,0,,0.481399146,0,negative,0.067372655,4.83E-06,7.70E-05,6.40E-07,7.42E-06,4.48E-05,0.008662127,6.87E-05,3.24E-07,0.606996129,7.86E-05,0.316658149,2.87E-05
4029,text_summarization14,186,Test set of DUC 2004 test set Model RG - 1 RG - 2 RG- L RG- 1 RG- 2 RG- L ABS 29.55 11.32 26.42 26.55 7.06 22.05 ABS + 29.76 11.88 26.96 28.18 8.49 23.81 Feats2s 32.67 15.59 30.64 28.35 9.46 24.59 CAs2s 33.78 15.97 31.15 28.97 8.26 24.06 Luong - NMT 33.10 14.45 30.71 28.55 8.79 24.43 Seq2seq 34 : Manual evaluation for correctness .,Manual Evaluation,Manual Evaluation,text_summarization,14,4,1,0,,2.90E-05,0,negative,0.000298287,3.14E-06,0.000234659,1.90E-07,1.25E-06,0.000238213,0.003257012,0.000180187,1.29E-06,0.993701258,0.000103842,0.001970819,9.86E-06
4030,text_summarization14,187,Further Analysis,Manual Evaluation,,text_summarization,14,5,1,0,,5.12E-06,0,negative,8.14E-05,4.83E-07,4.31E-06,7.66E-08,2.47E-07,3.70E-05,6.13E-05,2.80E-05,9.14E-07,0.999710094,7.98E-06,6.76E-05,4.69E-07
4031,text_summarization14,188,"To further investigate the effectiveness of our model , we perform analysis on the entailment score improvement , the abstraction degree of our model and the impact for entailment recognition task .",Manual Evaluation,Further Analysis,text_summarization,14,6,1,0,,0.000346568,0,negative,0.004145474,3.42E-05,9.30E-06,4.58E-07,3.34E-05,2.40E-05,1.48E-05,1.15E-05,2.20E-06,0.99559604,6.56E-06,0.000122078,1.51E-07
4032,text_summarization14,189,Does our summarization model learn entailment knowledge ?,Manual Evaluation,Further Analysis,text_summarization,14,7,1,1,ablation-analysis,6.53E-06,0,negative,1.47E-05,8.97E-06,1.99E-06,2.63E-07,6.28E-07,7.39E-05,9.25E-06,4.80E-05,1.17E-05,0.997934743,0.001893117,2.00E-06,7.54E-07
4033,text_summarization14,190,The motivation of our work is to encourage summarization model to generate summaries thatare entailed by the source sentences .,Manual Evaluation,Further Analysis,text_summarization,14,8,1,0,,0.0001901,0,negative,0.000203729,0.000355516,7.63E-05,2.38E-05,0.000100657,0.000336587,8.70E-05,0.000113111,7.07E-05,0.938199832,0.060389132,1.89E-05,2.48E-05
4034,text_summarization14,191,"To verify this goal , we investigate the entailment score for source - summary pairs for different models .",Manual Evaluation,Further Analysis,text_summarization,14,9,1,0,,3.24E-05,0,negative,0.000492431,0.000819132,0.000104955,9.29E-08,1.09E-05,2.95E-05,1.73E-05,3.46E-05,0.000105092,0.998189728,0.000130434,6.56E-05,2.30E-07
4035,text_summarization14,192,"For the test set of , the average entailment score for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .",Manual Evaluation,Further Analysis,text_summarization,14,10,1,1,ablation-analysis,0.064393903,0,negative,0.050598559,6.49E-05,0.000116328,2.02E-06,0.000122672,0.000210962,0.002740936,0.00012076,3.50E-06,0.918489644,0.00010047,0.027413559,1.57E-05
4036,text_summarization14,193,"When we adopt entailmentbased strategies , the entailment score rises to 0.63 for seq2seq model .",Manual Evaluation,Further Analysis,text_summarization,14,11,1,1,ablation-analysis,0.091930562,0,ablation-analysis,0.587925992,6.60E-05,0.000203705,2.75E-06,6.37E-05,0.000146903,0.001644302,7.85E-05,9.93E-06,0.38552587,0.000111103,0.024194334,2.69E-05
4037,text_summarization14,194,"Note that the entailment score is 0.57 for seq2seq model with selective encoding , and we believe that the selective mechanism can filter out secondary information in the input , which will reduce the possibility to generate irrelevant information .",Manual Evaluation,Further Analysis,text_summarization,14,12,1,1,ablation-analysis,0.018202703,0,negative,0.124572527,4.08E-05,7.52E-05,1.60E-06,5.80E-05,0.000263433,0.00218543,0.000144026,4.03E-06,0.848607197,7.27E-05,0.02396235,1.27E-05
4038,text_summarization14,195,Entailment - aware selective model achieves a high entailment reward of 0.71 .,Manual Evaluation,Further Analysis,text_summarization,14,13,1,1,ablation-analysis,0.167375837,0,negative,0.119217296,0.001380288,0.014178623,4.49E-06,7.20E-05,0.000575139,0.011479293,0.000433508,0.000346737,0.759232311,0.00533014,0.087535716,0.000214495
4039,text_summarization14,196,"In part at least , we can conclude that our model has successfully learned entailment knowledge .",Manual Evaluation,Further Analysis,text_summarization,14,14,1,1,ablation-analysis,3.43E-06,0,negative,0.000154201,2.56E-06,1.32E-06,5.96E-07,1.05E-05,5.77E-05,5.76E-06,9.87E-06,2.43E-06,0.999742663,6.27E-06,5.82E-06,2.78E-07
4040,text_summarization14,197,6.6.2,Manual Evaluation,Further Analysis,text_summarization,14,15,1,0,,6.92E-06,0,negative,0.000347865,6.78E-07,5.39E-06,3.37E-06,2.49E-05,0.000104775,6.86E-06,4.03E-06,8.94E-07,0.999494869,1.95E-06,4.09E-06,3.16E-07
4041,text_summarization14,198,Is it less abstractive for our model ?,Manual Evaluation,Further Analysis,text_summarization,14,16,1,1,ablation-analysis,7.93E-07,0,negative,2.48E-05,6.87E-06,2.22E-06,5.80E-07,2.09E-06,0.000101974,4.98E-06,3.99E-05,1.09E-05,0.999769715,3.37E-05,1.81E-06,3.61E-07
4042,text_summarization14,199,We have shown that our entailment - aware model can generate correct summaries more frequently ( Section 6.5 ) .,Manual Evaluation,Further Analysis,text_summarization,14,17,1,0,,0.000985464,0,negative,0.002762766,1.03E-05,1.31E-05,6.18E-08,3.00E-06,3.53E-05,0.000193974,1.98E-05,1.57E-06,0.994191847,4.96E-05,0.002717668,1.10E-06
4043,text_summarization14,200,"Intuitively , it is more likely to be correct if summary segments are directly extracted from the source .",Manual Evaluation,Further Analysis,text_summarization,14,18,1,0,,7.35E-06,0,negative,2.92E-05,3.18E-06,1.34E-06,4.01E-08,2.17E-06,1.37E-05,1.40E-06,8.29E-06,3.18E-06,0.999932752,2.65E-06,2.07E-06,5.03E-08
4044,text_summarization14,201,"Thus , readers may wonder whether our model is less abstractive .",Manual Evaluation,Further Analysis,text_summarization,14,19,1,0,,4.89E-07,0,negative,1.16E-05,2.86E-06,9.16E-07,1.32E-07,1.71E-06,4.28E-05,1.27E-06,1.40E-05,9.35E-06,0.999911698,2.84E-06,7.57E-07,7.16E-08
4045,text_summarization14,202,"shows that the seq2seq model produces more novel words ( i.e. , words that do not appear in the article ) than our model , indicating a lower degree of abstraction for our model .",Manual Evaluation,Further Analysis,text_summarization,14,20,1,1,ablation-analysis,0.001071514,0,negative,0.080238678,1.06E-05,4.80E-05,7.39E-07,0.000111744,6.61E-05,0.000332629,1.70E-05,1.08E-06,0.914850647,5.08E-06,0.004315221,2.56E-06
4046,text_summarization14,203,"However , when we exclude all the words not in the reference ( these words may lead to wrong information ) , our model generates more novel words , suggesting that our model provides a compromise solution for informativeness and correctness .",Manual Evaluation,Further Analysis,text_summarization,14,21,1,1,ablation-analysis,0.024295351,0,negative,0.144703607,1.97E-05,2.10E-05,8.94E-07,8.13E-05,5.36E-05,9.44E-05,1.92E-05,3.58E-06,0.853550656,4.57E-06,0.001445949,1.52E-06
4047,text_summarization14,204,"Thus , our model can generate summary with fewer mistakes .",Manual Evaluation,Further Analysis,text_summarization,14,22,1,0,,0.000190674,0,negative,0.002441357,0.000140462,3.27E-05,2.88E-07,1.82E-05,4.50E-05,4.07E-05,4.59E-05,9.78E-05,0.996722334,2.64E-05,0.000387023,1.81E-06
4048,text_summarization14,205,6.6.3 Could the entailment recognition also be improved ?,Manual Evaluation,Further Analysis,text_summarization,14,23,1,1,ablation-analysis,5.77E-05,0,negative,6.62E-05,3.20E-06,2.81E-06,4.43E-07,2.01E-06,0.000127917,2.18E-05,3.02E-05,4.93E-06,0.999680746,4.40E-05,1.47E-05,9.81E-07
4049,text_summarization14,206,"Multi - task learning ( MTL ) involves sharing parameters between related tasks , whereby each task can benefit from extra information of other tasks in the training process .",Manual Evaluation,Further Analysis,text_summarization,14,24,1,0,,0.009505359,0,negative,0.000130509,0.000143553,8.19E-05,0.000111253,0.000150762,0.00069252,0.000509255,0.000111447,3.29E-05,0.896514044,0.101268511,4.23E-05,0.000211016
4050,text_summarization14,207,"In this section , we explore whether the entailment recognition can benefit from summarization generation task .",Manual Evaluation,Further Analysis,text_summarization,14,25,1,0,,0.000469566,0,negative,0.001954459,0.002550747,0.000674863,6.69E-07,0.000100975,0.000122581,0.000106373,7.69E-05,0.000358901,0.993641158,0.000143101,0.000265081,4.17E-06
4051,text_summarization14,208,shows that our summarization model with MTL outperforms basic seq2seq model .,Manual Evaluation,Further Analysis,text_summarization,14,26,1,1,ablation-analysis,0.174134161,0,negative,0.150733464,1.55E-05,0.00018325,1.79E-06,7.93E-05,0.000203125,0.008869987,5.82E-05,1.50E-06,0.719525363,2.77E-05,0.120241027,5.98E-05
4052,text_summarization14,209,"As ? increases , the accuracy of entailment recognition improves and finally exceeds that of the model without MTL , which reveals the advantage of MTL framework .",Manual Evaluation,Further Analysis,text_summarization,14,27,1,1,ablation-analysis,0.068691549,0,negative,0.454301694,5.37E-05,9.35E-05,2.22E-06,7.33E-05,0.000143201,0.00445233,7.31E-05,6.19E-06,0.493392908,3.83E-05,0.04732148,4.80E-05
4053,text_summarization14,210,Case Study,Manual Evaluation,,text_summarization,14,28,1,0,,0.000146036,0,negative,0.000150271,2.11E-05,0.000115139,1.91E-06,2.22E-05,0.000212255,0.000753926,0.000283487,2.25E-05,0.997911341,1.47E-05,0.000448365,4.29E-05
4054,text_summarization14,211,We illustrate the examples of outputs in .,Manual Evaluation,,text_summarization,14,29,1,0,,7.61E-06,0,negative,3.16E-05,5.38E-07,2.64E-06,1.28E-07,3.37E-06,1.47E-05,8.45E-06,8.53E-06,6.29E-07,0.999915943,3.78E-08,1.33E-05,1.70E-07
4055,text_summarization14,212,"As shown in the table , seq2seq model generates summaries thatare not relevant to the source sentence , while the output of our model obtains higher entailment scores than those of seq2seq model .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,30,1,0,,0.880911354,1,results,0.105574071,9.38E-07,5.25E-05,4.66E-07,5.18E-06,2.59E-05,0.023866544,2.16E-05,7.95E-08,0.226575179,1.27E-06,0.642662525,0.001213773
4056,text_summarization14,213,"For the first example , seq2seq model regards the reason for "" brazil stocks rise "" as "" consumer credit concerns "" , while in fact , "" consumer "" is not worried because "" government said it would n't impose restraints on consumer credit "" .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,31,1,0,,0.000198666,0,negative,0.000496627,2.43E-07,0.00015716,5.61E-07,3.96E-05,3.55E-05,8.19E-05,3.69E-06,2.20E-07,0.9988718,1.10E-07,0.000252192,6.04E-05
4057,text_summarization14,214,"By contrast , since our model incorporates entailment knowledge , the true reason is captured and the output of our model is related to the source sentence .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,32,1,0,,0.000528519,0,negative,0.003741434,5.61E-06,0.000448824,3.81E-07,9.72E-06,1.62E-05,4.80E-05,9.95E-06,9.92E-06,0.994965641,2.27E-07,0.000716439,2.76E-05
4058,text_summarization14,215,"A similar problem happens in example 2 , and seq2seq model generates a summary that is contradictory to the source .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,33,1,0,,0.000395928,0,negative,0.000310044,2.39E-07,3.42E-05,1.34E-07,2.25E-06,2.51E-05,8.67E-05,6.80E-06,3.46E-07,0.999166136,4.60E-07,0.00030474,6.28E-05
4059,text_summarization14,216,"The "" demonstration "" is "" denied "" by the "" authorities "" , while seq2seq model confirms the "" demonstration "" .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,34,1,0,,0.007119192,0,negative,0.001372879,5.03E-07,3.38E-05,1.19E-06,3.17E-05,6.33E-05,0.000146802,2.24E-05,6.60E-07,0.997719481,8.17E-08,0.000426904,0.000180287
4060,text_summarization14,217,"In Example 3 , neither seq2seq nor our model performs satisfactorily .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,35,1,0,,0.063968668,0,negative,0.019428726,8.13E-07,6.04E-05,5.88E-07,6.19E-06,0.000125626,0.018235129,5.52E-05,2.46E-07,0.789029294,1.53E-06,0.171252859,0.001803433
4061,text_summarization14,218,Seq2seq model again misunderstands the meaning of the source and outputs summary containing wrong information .,Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,36,1,0,,0.015470567,0,negative,0.003962288,3.84E-07,0.000407363,9.60E-07,2.88E-05,6.41E-05,0.000777314,6.17E-06,4.64E-07,0.990167506,6.77E-07,0.003755777,0.00082827
4062,text_summarization14,219,"Though the summary generated by our model is entailed by the source , the summary fails to produce an integrated sentence and misses the key points of the source , such as the object of the event , "" queens taxi driver "" .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,37,1,0,,0.016689609,0,negative,0.006029935,4.27E-07,3.99E-05,1.22E-06,5.67E-05,3.13E-05,0.000552818,6.55E-06,1.51E-07,0.985885504,3.45E-07,0.007063254,0.000331833
4063,text_summarization14,220,"A mixed reward , i.e. , combining entailment and ROUGE - 2 , may address this issue .",Manual Evaluation,We illustrate the examples of outputs in .,text_summarization,14,38,1,0,,0.013527894,0,negative,0.006424375,1.73E-06,0.000116706,1.77E-06,1.01E-05,0.000156562,0.000408962,7.18E-05,3.50E-06,0.990881203,5.10E-07,0.001113423,0.000809327
4064,text_summarization14,221,We leave it for our future work .,Manual Evaluation,,text_summarization,14,39,1,0,,5.55E-06,0,negative,0.000113734,2.09E-06,8.28E-06,1.11E-06,6.28E-06,0.000153961,7.00E-05,7.34E-05,2.88E-06,0.999525372,2.20E-07,3.94E-05,3.27E-06
4065,text_summarization14,222,Conclusion,,,text_summarization,14,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
4066,named-entity-recognition3,1,title,,,named-entity-recognition,3,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
4067,named-entity-recognition3,2,Semi-supervised sequence tagging with bidirectional language models,title,,named-entity-recognition,3,1,1,1,research-problem,0.998235522,1,research-problem,2.51E-08,6.13E-06,7.80E-08,6.39E-08,4.23E-08,8.85E-08,9.19E-07,1.09E-06,8.09E-07,0.001742912,0.998247642,1.56E-07,4.78E-08
4068,named-entity-recognition3,3,abstract,,,named-entity-recognition,3,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
4069,named-entity-recognition3,4,Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks .,abstract,abstract,named-entity-recognition,3,1,1,0,,0.232817946,0,research-problem,3.97E-08,1.91E-05,1.63E-08,7.28E-07,1.95E-07,8.93E-07,4.25E-07,8.69E-06,2.04E-06,0.029593295,0.970374466,3.02E-08,5.70E-08
4070,named-entity-recognition3,5,"However , in most cases , the recurrent network that operates on word - level representations to produce context sensitive representations is trained on relatively little labeled data .",abstract,abstract,named-entity-recognition,3,2,1,0,,0.011804407,0,research-problem,2.90E-07,0.000277563,1.56E-07,7.10E-06,1.01E-05,6.98E-06,1.31E-06,4.85E-05,1.45E-05,0.47098738,0.528645924,1.07E-07,1.26E-07
4071,named-entity-recognition3,6,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",abstract,abstract,named-entity-recognition,3,3,1,1,research-problem,0.759457442,1,research-problem,3.44E-05,0.173626681,6.72E-05,0.000126967,0.000748635,4.01E-05,3.83E-05,0.000448456,0.002260538,0.154672281,0.667912692,1.53E-05,8.51E-06
4072,named-entity-recognition3,7,"We evaluate our model on two standard datasets for named entity recognition ( NER ) and chunking , and in both cases achieve state of the art results , surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers .",abstract,abstract,named-entity-recognition,3,4,1,0,,0.01593216,0,negative,2.89E-05,0.092386947,2.60E-05,4.61E-05,0.000926993,9.71E-05,0.000105816,0.001578209,0.000805787,0.669977635,0.233925351,8.84E-05,6.66E-06
4073,named-entity-recognition3,8,Introduction,,,named-entity-recognition,3,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
4074,named-entity-recognition3,9,"Due to their simplicity and efficacy , pre-trained word embedding have become ubiquitous in NLP systems .",Introduction,Introduction,named-entity-recognition,3,1,1,0,,0.057569454,0,research-problem,4.55E-06,0.0006692,7.40E-07,2.59E-05,2.81E-05,4.68E-05,1.79E-05,6.19E-05,0.000330103,0.220861682,0.77794747,2.38E-06,3.36E-06
4075,named-entity-recognition3,10,Many prior studies have shown that they capture useful semantic and syntactic information and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks .,Introduction,Introduction,named-entity-recognition,3,2,1,0,,0.165761396,0,research-problem,8.27E-06,0.0008256,1.18E-06,2.48E-05,0.000129719,2.94E-05,2.05E-05,2.99E-05,0.000272617,0.436686864,0.561964633,3.94E-06,2.61E-06
4076,named-entity-recognition3,11,"However , in many NLP tasks it is essential to represent not just the meaning of a word , but also the word in context .",Introduction,Introduction,named-entity-recognition,3,3,1,0,,0.409499747,0,research-problem,6.17E-07,0.00015256,1.37E-07,8.27E-06,9.68E-06,7.98E-06,5.31E-06,9.19E-06,6.74E-05,0.097084997,0.902652351,5.66E-07,9.69E-07
4077,named-entity-recognition3,12,"For example , in the two phrases "" A Central Bank spokesman "" and "" The Central African Republic "" , the word ' Central ' is used as part of both an Organization and Location .",Introduction,Introduction,named-entity-recognition,3,4,1,0,,0.004068304,0,negative,7.38E-06,0.001818538,3.26E-06,0.000353066,0.002305876,0.000526705,4.24E-05,0.000189061,0.001153205,0.973997955,0.019592686,3.84E-06,6.00E-06
4078,named-entity-recognition3,13,"Accordingly , current state of the art sequence tagging models typically include a bidirectional re-current neural network ( RNN ) that encodes token sequences into a context sensitive representation before making token specific predictions .",Introduction,Introduction,named-entity-recognition,3,5,1,0,,0.609787181,1,research-problem,1.13E-05,0.003067236,8.19E-06,1.98E-05,6.28E-05,3.71E-05,3.38E-05,4.24E-05,0.001979904,0.161451267,0.833274696,6.29E-06,5.15E-06
4079,named-entity-recognition3,14,"Although the token representation is initialized with pre-trained embeddings , the parameters of the bidirectional RNN are typically learned only on labeled data .",Introduction,Introduction,named-entity-recognition,3,6,1,0,,0.660396995,1,negative,5.58E-05,0.288243179,3.05E-05,4.54E-05,0.001197119,0.000208962,4.08E-05,0.000982636,0.201649138,0.474111254,0.033413308,1.54E-05,6.60E-06
4080,named-entity-recognition3,15,"Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks ( e.g. , .",Introduction,Introduction,named-entity-recognition,3,7,1,0,,0.144045598,0,research-problem,5.85E-06,0.004451404,4.04E-06,2.64E-05,9.57E-05,7.84E-05,4.41E-05,0.000129871,0.001862208,0.358382996,0.634907449,5.89E-06,5.73E-06
4081,named-entity-recognition3,16,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .",Introduction,Introduction,named-entity-recognition,3,8,1,1,approach,0.954678449,1,approach,9.51E-05,0.816529667,0.00017277,5.00E-05,0.002961385,8.79E-05,9.35E-05,0.000211701,0.121199136,0.037485532,0.021062116,3.80E-05,1.32E-05
4082,named-entity-recognition3,17,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .",Introduction,Introduction,named-entity-recognition,3,9,1,1,approach,0.950508656,1,model,6.89E-06,0.167533855,8.40E-05,7.24E-07,0.000140989,1.71E-05,1.21E-05,4.86E-05,0.825213093,0.006290157,0.000649249,2.00E-06,1.33E-06
4083,named-entity-recognition3,18,"Since the LM embeddings are used to compute the probability of future words in a neural LM , they are likely to encode both the semantic and syntactic roles of words in context .",Introduction,Introduction,named-entity-recognition,3,10,1,0,,0.76930358,1,model,7.11E-05,0.12443171,4.74E-05,7.93E-06,0.000690793,8.46E-05,1.88E-05,0.000185873,0.61900691,0.253566604,0.0018761,1.05E-05,1.73E-06
4084,named-entity-recognition3,19,Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting .,Introduction,Introduction,named-entity-recognition,3,11,1,0,,0.767553336,1,approach,0.000198402,0.584780408,7.08E-05,2.33E-05,0.001256041,8.14E-05,0.000108823,0.000270591,0.219249683,0.158840113,0.035025091,8.42E-05,1.11E-05
4085,named-entity-recognition3,20,"When we include the LM embeddings in our system over all performance increases from 90. 87 % to 91.93 % F 1 for the CoNLL 2003 NER task , a more then 1 % absolute F1 increase , and a substantial improvement over the previous state of the art .",Introduction,Introduction,named-entity-recognition,3,12,1,0,,0.092070392,0,negative,0.013609472,0.053278651,0.00012705,0.000207848,0.005368678,0.000392317,0.010641621,0.001125789,0.007934104,0.867239552,0.024082335,0.015751955,0.000240628
4086,named-entity-recognition3,21,We also establish a new state of the art result ( 96.37 % F 1 ) for the CoNLL 2000 Chunking task .,Introduction,Introduction,named-entity-recognition,3,13,1,0,,0.05017558,0,negative,0.00049843,0.127005027,0.000118134,7.03E-05,0.005150417,0.000248233,0.002889677,0.000543957,0.029005174,0.748840171,0.082788065,0.002748014,9.44E-05
4087,named-entity-recognition3,22,"As a secondary contribution , we show that using both forward and backward LM embeddings boosts performance over a forward only LM .",Introduction,Introduction,named-entity-recognition,3,14,1,0,,0.756679825,1,approach,0.002103446,0.588212329,0.000223071,2.57E-05,0.003020496,0.00010948,0.000523989,0.000317693,0.216096783,0.18250207,0.006352637,0.000493879,1.84E-05
4088,named-entity-recognition3,23,We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers .,Introduction,Introduction,named-entity-recognition,3,15,1,0,,0.683834252,1,negative,0.003221933,0.213044927,4.93E-05,4.63E-05,0.003340359,0.000185497,0.000609925,0.000634441,0.040118817,0.730106819,0.007871168,0.000752151,1.84E-05
4089,named-entity-recognition3,24,The main components in our language - modelaugmented sequence tagger ( TagLM ) are illustrated in .,Introduction,Introduction,named-entity-recognition,3,16,1,0,,0.914745197,1,negative,5.04E-05,0.082107329,0.000170274,0.000760052,0.015100369,0.001055048,0.000230888,0.000408189,0.151795377,0.711103502,0.03712708,2.31E-05,6.83E-05
4090,named-entity-recognition3,25,"After pre-training word embeddings and a neural LM on large , unlabeled corpora ( Step 1 ) , we extract the word and LM embeddings for every token in a given input sequence Step 2 ) and use them in the supervised sequence tagging model (",Introduction,Introduction,named-entity-recognition,3,17,1,0,,0.879382177,1,model,3.68E-05,0.186352496,0.000198446,1.42E-06,0.000564923,2.37E-05,3.71E-05,4.17E-05,0.779716698,0.031884946,0.001127093,1.23E-05,2.27E-06
4091,named-entity-recognition3,26,Step 3 ) .,Introduction,,named-entity-recognition,3,18,1,0,,0.018718179,0,negative,2.74E-05,0.025225629,1.96E-05,1.50E-06,9.67E-05,0.000148161,4.82E-05,0.00039513,0.223893187,0.746182529,0.003949911,9.86E-06,2.23E-06
4092,named-entity-recognition3,27,Baseline sequence tagging model,,,named-entity-recognition,3,0,1,0,,0.105668862,0,negative,0.000323934,0.000885718,0.005588002,3.36E-05,9.10E-06,0.000738347,0.00238508,0.004044067,0.002204324,0.796793439,0.1824495,0.00425253,0.000292356
4093,named-entity-recognition3,28,"Our baseline sequence tagging model is a hierarchical neural tagging model , closely following a number of recent studies ) ( left side of ) .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,1,1,0,,0.000820441,0,negative,2.21E-05,0.000210905,0.051822886,4.86E-07,1.17E-06,5.87E-05,8.23E-05,0.000228587,0.010141694,0.937284674,0.000114645,2.72E-05,4.58E-06
4094,named-entity-recognition3,29,"Given a sentence of tokens ( t 1 , t 2 , . . . , t N ) it first forms a representation , x k , for each token by concatenating a character based representation ck with a token embedding wk :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,2,1,0,,0.000215786,0,negative,1.29E-05,3.05E-05,0.001510928,2.69E-08,1.65E-07,2.20E-06,3.44E-06,1.33E-05,0.000615567,0.997769754,1.24E-05,2.87E-05,1.49E-07
4095,named-entity-recognition3,30,The character representation ck captures morphological information and is either a convolutional neural network ( CNN ) or RNN .,Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,3,1,0,,0.02865606,0,negative,2.33E-05,0.000255221,0.021005963,2.13E-07,7.43E-07,2.62E-05,2.93E-05,0.000186022,0.020350443,0.958054828,4.59E-05,1.99E-05,1.88E-06
4096,named-entity-recognition3,31,"It is parameterized by C ( , ? c ) with parameters ? c .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,4,1,0,,0.00028411,0,negative,1.32E-05,2.49E-05,7.54E-05,2.43E-07,4.51E-07,1.73E-05,7.10E-06,0.000169911,0.000414284,0.999263565,2.17E-06,1.13E-05,2.03E-07
4097,named-entity-recognition3,32,"The token embeddings , wk , are obtained as a lookup E ( , ? w ) , initialized using pre-trained word embeddings , and fine tuned during training .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,5,1,0,,0.001755621,0,negative,9.35E-06,0.000124854,9.08E-05,1.99E-07,4.05E-07,4.92E-05,1.79E-05,0.001335282,0.002311882,0.996046092,5.29E-06,8.19E-06,5.00E-07
4098,named-entity-recognition3,33,"To learn a context sensitive representation , we employ multiple layers of bidirectional RNNs .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,6,1,0,,0.118282738,0,negative,0.000261773,0.003969928,0.0361685,1.58E-06,1.17E-05,6.83E-05,0.000110696,0.00052112,0.161444945,0.797205759,8.94E-05,0.000134959,1.13E-05
4099,named-entity-recognition3,34,"For each token position , k , the hidden state h k , i of RNN layer i is formed by concatenating the hidden states from the forward ( ? ? h k , i ) and backward ( ? ? h k , i ) RNNs .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,7,1,0,,0.000154994,0,negative,3.52E-06,2.51E-05,0.000153619,2.90E-08,1.74E-07,3.12E-06,2.67E-06,4.00E-05,0.000955853,0.998806378,3.16E-06,6.22E-06,1.14E-07
4100,named-entity-recognition3,35,"As a result , the bidirectional RNN is able to use both past and future information to make a prediction at token k.",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,8,1,0,,0.001060611,0,negative,1.95E-05,0.000139159,0.002537792,2.37E-08,2.99E-07,1.96E-06,3.08E-06,1.78E-05,0.014129048,0.983113451,1.00E-05,2.76E-05,2.37E-07
4101,named-entity-recognition3,36,"More formally , for the first RNN layer that operates on x k to output h k,1 :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,9,1,0,,1.04E-05,0,negative,1.30E-05,6.42E-06,0.000342473,6.17E-09,6.99E-08,9.22E-07,1.33E-06,6.61E-06,0.000159994,0.999445666,5.33E-07,2.29E-05,3.09E-08
4102,named-entity-recognition3,37,Step 2 : Prepare word embedding and LM embedding for each token in the input sequence .,Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,10,1,0,,0.022097057,0,negative,1.05E-05,3.46E-05,0.000840028,3.75E-08,1.58E-07,7.46E-06,8.50E-06,9.10E-05,0.001823204,0.997155985,4.38E-06,2.37E-05,4.91E-07
4103,named-entity-recognition3,38,"Two representations of the word "" York """,Baseline sequence tagging model,,named-entity-recognition,3,11,1,0,,0.000584595,0,negative,8.25E-06,2.99E-05,0.001309183,4.91E-08,1.20E-07,7.02E-06,4.13E-05,4.66E-05,0.0013099,0.996295229,0.000673314,0.000276526,2.55E-06
4104,named-entity-recognition3,39,Step 3 : Use both word embeddings and LM embeddings in the sequence tagging model .,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,12,1,0,,0.010535772,0,negative,3.22E-05,3.21E-06,0.001299201,1.56E-07,3.98E-08,6.42E-06,7.84E-06,0.000367936,0.000100238,0.997790546,1.23E-06,0.000384026,6.99E-06
4105,named-entity-recognition3,40,New York is located :,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,13,1,0,,2.43E-06,0,negative,4.75E-05,5.71E-07,0.000389362,1.57E-06,2.16E-07,7.54E-06,8.41E-06,6.92E-05,1.97E-05,0.99863725,6.52E-07,0.000806263,1.18E-05
4106,named-entity-recognition3,41,"The main components in TagLM , our language - model - augmented sequence tagging system .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,14,1,0,,0.000278357,0,negative,0.000687345,4.01E-06,0.071671226,1.45E-05,1.58E-06,2.84E-05,0.000158127,5.57E-05,7.86E-05,0.903717018,4.88E-05,0.023059918,0.00047473
4107,named-entity-recognition3,42,The language model component ( in orange ) is used to augment the input token representation in a traditional sequence tagging models ( in grey ) .,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,15,1,0,,0.000155364,0,negative,9.29E-05,3.17E-06,0.003510096,3.11E-07,1.01E-07,3.73E-06,6.17E-06,0.00010293,0.000244461,0.995635201,1.39E-06,0.000389252,1.03E-05
4108,named-entity-recognition3,43,"The second RNN layer is similar and uses h k , 1 to output h k ,2 .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,16,1,0,,7.05E-06,0,negative,0.000103394,2.38E-06,0.010548106,1.42E-07,7.51E-08,3.83E-06,9.79E-06,7.73E-05,0.000229555,0.988619641,9.36E-07,0.000397532,7.36E-06
4109,named-entity-recognition3,44,"In this paper , we use L = 2 layers of RNNs in all experiments and parameterize R i as either Gated Recurrent Units ( GRU ) or Long Short - Term Memory units ( LSTM ) depending on the task .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,17,1,0,,0.000595851,0,negative,7.04E-05,2.75E-05,0.000460457,4.61E-06,7.08E-07,0.000375241,0.000330116,0.025506715,0.000142482,0.972199511,3.77E-06,0.000801942,7.65E-05
4110,named-entity-recognition3,45,"Finally , the output of the final RNN layer h k,L is used to predict a score for each possible tag using a single dense layer .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,18,1,0,,9.55E-06,0,negative,2.51E-05,1.64E-06,0.000424288,6.58E-08,4.13E-08,9.67E-07,1.21E-06,4.85E-05,8.26E-05,0.999307073,2.14E-07,0.000106746,1.62E-06
4111,named-entity-recognition3,46,"Due to the dependencies between successive tags in our sequence labeling tasks ( e.g. using the BIOES labeling scheme , it is not possible for I - PER to follow B - LOC ) , it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,19,1,0,,2.13E-05,0,negative,0.000256612,2.60E-06,0.000380927,1.45E-07,1.16E-07,8.99E-07,1.90E-06,1.99E-05,2.69E-05,0.996512888,7.82E-07,0.002794484,1.91E-06
4112,named-entity-recognition3,47,"Accordingly , we add another layer with parameters for each label bigram , computing the sentence conditional random field ( CRF ) loss using the forward - backward algorithm at training time , and using the Viterbi algorithm to find the most likely tag sequence at test time , similar to Collobert et al .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,20,1,0,,0.000139246,0,negative,0.000208125,1.02E-05,0.009363329,1.21E-07,1.77E-07,1.99E-06,6.43E-06,5.73E-05,0.0004614,0.989401036,8.97E-07,0.000483987,5.07E-06
4113,named-entity-recognition3,48,Bidirectional LM,Baseline sequence tagging model,,named-entity-recognition,3,21,1,0,,0.006727093,0,negative,8.19E-05,0.000216016,0.03215834,1.41E-06,4.53E-06,4.06E-05,0.000320418,0.00019202,0.018641369,0.946579093,0.000172135,0.001525135,6.70E-05
4114,named-entity-recognition3,49,"A language model computes the probability of a token sequence ( t 1 , t 2 , . . . , t N )",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,22,1,0,,7.55E-06,0,negative,6.68E-06,4.86E-06,0.000685636,5.72E-08,1.47E-07,1.12E-06,1.51E-06,1.92E-05,0.000396724,0.9988473,3.23E-06,3.10E-05,2.47E-06
4115,named-entity-recognition3,50,"Recent state of the art neural language models ) use a similar architecture to our baseline sequence tagger where they pass a token representation ( either from a CNN over characters or as token embeddings ) through multiple layers of LSTMs to embed the history ( t 1 , t 2 , . . . , t k ) into a fixed dimensional vector ? ? h LM k .",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,23,1,0,,6.42E-05,0,negative,2.30E-05,1.64E-05,0.003544265,2.16E-06,6.76E-07,2.33E-05,3.79E-05,0.000149446,0.000617638,0.995156213,0.000118586,0.000193485,0.000116986
4116,named-entity-recognition3,51,This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,24,1,0,,1.30E-05,0,negative,1.14E-05,4.17E-06,0.000220201,1.39E-07,2.29E-07,2.69E-06,2.77E-06,7.40E-05,0.000285015,0.999377643,2.18E-07,1.92E-05,2.34E-06
4117,named-entity-recognition3,52,"Finally , the language model predicts the probability of token t k + 1 using a softmax layer over words in the vocabulary .",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,25,1,0,,7.61E-05,0,negative,3.41E-05,1.05E-05,0.003984467,8.00E-08,2.50E-07,2.70E-06,4.35E-06,4.72E-05,0.00297336,0.992913127,4.47E-07,2.53E-05,4.22E-06
4118,named-entity-recognition3,53,The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,26,1,0,,0.00066822,0,negative,0.000232632,4.63E-06,0.000652853,7.43E-08,2.27E-07,8.64E-07,1.63E-06,1.21E-05,0.000167987,0.998643052,4.23E-07,0.000282595,8.93E-07
4119,named-entity-recognition3,54,A backward LM predicts the previous token given the future context .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,27,1,0,,0.000686752,0,negative,6.42E-05,2.07E-05,0.132276716,3.89E-08,3.10E-07,1.85E-06,9.68E-06,1.68E-05,0.00773731,0.859784025,1.33E-06,7.97E-05,7.34E-06
4120,named-entity-recognition3,55,"Given a sentence with N tokens , it computes",Baseline sequence tagging model,,named-entity-recognition,3,28,1,0,,0.000120548,0,negative,8.61E-06,2.24E-05,0.00105875,1.08E-08,2.90E-07,7.37E-07,3.20E-06,5.94E-06,0.000475853,0.998364196,1.15E-06,5.86E-05,2.09E-07
4121,named-entity-recognition3,56,A backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ? ?,Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,29,1,0,,6.07E-05,0,negative,1.32E-05,8.21E-07,0.001482864,2.56E-08,6.16E-08,9.78E-07,1.40E-06,9.71E-06,9.27E-05,0.998338919,2.12E-07,5.23E-05,6.71E-06
4122,named-entity-recognition3,57,"h LM k , for the sequence ( t k , t k+1 , . . . , t N ) , the output embeddings of the top layer LSTM .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,30,1,0,,1.67E-05,0,negative,2.14E-05,6.94E-07,0.000558676,4.88E-08,1.58E-07,1.32E-06,1.78E-06,2.05E-05,2.35E-05,0.99928333,5.14E-08,8.23E-05,6.19E-06
4123,named-entity-recognition3,58,"In our final system , after pre-training the forward and backward LMs separately , we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings , i.e. , h LM",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,31,1,0,,0.000930504,0,negative,0.000680527,3.00E-05,0.008259793,2.19E-06,5.03E-06,2.15E-05,4.37E-05,0.000538588,0.000860459,0.988654845,3.76E-07,0.000679641,0.000223386
4124,named-entity-recognition3,59,"Note that in our formulation , the forward and backward LMs are independent , without any shared parameters .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,32,1,0,,7.91E-05,0,negative,8.26E-05,6.77E-06,0.001052366,1.38E-07,3.51E-07,1.56E-06,1.60E-06,4.10E-05,0.000608466,0.9981325,1.00E-07,6.43E-05,8.34E-06
4125,named-entity-recognition3,60,Combining LM with sequence model,Baseline sequence tagging model,,named-entity-recognition,3,33,1,0,,0.124961502,0,negative,3.87E-05,0.000188377,0.0047947,3.00E-07,9.86E-07,1.35E-05,0.000475604,0.000111802,0.005094134,0.983716325,0.001010538,0.004498884,5.61E-05
4126,named-entity-recognition3,61,"Our combined system , TagLM , uses the LM embeddings as additional inputs to the sequence tagging model .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,34,1,0,,0.00066231,0,negative,0.000112203,2.77E-05,0.011920219,8.60E-08,8.23E-07,1.48E-06,1.16E-05,1.39E-05,0.001729544,0.985765852,3.15E-07,0.000410884,5.41E-06
4127,named-entity-recognition3,62,"In particular , we concatenate the LM embeddings h LM with the output from one of the bidirectional RNN layers in the sequence model .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,35,1,0,,4.31E-06,0,negative,1.61E-05,4.86E-06,0.000106719,6.30E-08,1.71E-07,1.76E-06,3.34E-06,8.48E-05,0.000150162,0.999601692,1.92E-08,2.93E-05,1.03E-06
4128,named-entity-recognition3,63,"In our experiments , we found that introducing the LM embeddings at the output of the first layer performed the best .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,36,1,0,,0.053251403,0,negative,0.003274488,6.27E-06,3.54E-05,5.98E-06,1.77E-06,5.90E-05,0.00044515,0.001069011,3.06E-05,0.943151095,5.87E-07,0.051851993,6.87E-05
4129,named-entity-recognition3,64,"More formally , we simply replace ( 2 ) with",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,37,1,0,,4.47E-07,0,negative,1.24E-05,4.54E-07,3.73E-05,4.53E-09,2.04E-08,9.09E-08,2.78E-07,1.95E-06,1.29E-05,0.999865882,1.22E-08,6.86E-05,5.14E-08
4130,named-entity-recognition3,65,There are alternate possibilities for adding the LM embeddings to the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,38,1,0,,1.83E-06,0,negative,7.18E-06,5.61E-07,2.25E-05,3.78E-08,3.86E-08,5.78E-07,8.65E-07,1.36E-05,1.74E-05,0.999888282,2.84E-08,4.85E-05,3.77E-07
4131,named-entity-recognition3,66,One pos-sibility adds a non-linear mapping after the concatenation and before the second RNN ( e.g. re -,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,39,1,0,,1.13E-06,0,negative,5.97E-05,1.24E-06,0.000749901,2.83E-08,9.66E-08,4.37E-07,2.54E-06,4.70E-06,4.53E-05,0.998840677,1.59E-07,0.000294216,9.56E-07
4132,named-entity-recognition3,67,where f is a non-linear function ) .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,40,1,0,,1.04E-06,0,negative,2.92E-06,1.82E-07,1.54E-05,1.52E-09,8.00E-09,4.71E-08,1.30E-07,1.36E-06,6.16E-06,0.999939489,1.27E-08,3.42E-05,4.05E-08
4133,named-entity-recognition3,68,Another possibility introduces an attention - like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,41,1,0,,4.91E-07,0,negative,7.48E-06,3.55E-07,2.61E-05,1.92E-08,3.57E-08,3.77E-07,6.35E-07,8.37E-06,2.17E-05,0.99991446,1.48E-08,2.02E-05,2.91E-07
4134,named-entity-recognition3,69,"Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study , preferring to leave them for future work .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,42,1,0,,5.68E-06,0,negative,2.17E-05,1.86E-07,1.08E-05,2.16E-08,7.19E-08,1.09E-06,4.72E-06,9.89E-06,2.15E-06,0.999236354,1.97E-08,0.000712394,6.23E-07
4135,named-entity-recognition3,70,Experiments,,,named-entity-recognition,3,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
4136,named-entity-recognition3,71,"We evaluate our approach on two well benchmarked sequence tagging tasks , the CoNLL 2003 NER task and the CoNLL 2000 Chunking task .",Experiments,Experiments,named-entity-recognition,3,1,1,0,,0.048287865,0,negative,0.000652681,0.003051615,0.002080511,4.66E-05,0.000142941,0.005578571,0.012552744,0.043566502,0.000120426,0.917547163,0.001579491,0.01237916,0.000701614
4137,named-entity-recognition3,72,We report the official evaluation metric ( micro - averaged F 1 ) .,Experiments,Experiments,named-entity-recognition,3,2,1,0,,0.000711382,0,negative,9.05E-05,2.40E-05,9.71E-05,5.56E-06,1.37E-05,0.000964383,0.000677194,0.005209988,4.49E-06,0.991688512,2.62E-05,0.001176224,2.20E-05
4138,named-entity-recognition3,73,"In both cases , we use the BIOES labeling scheme for the output tags , following previous work which showed it outperforms other options ( e.g. , ) .",Experiments,Experiments,named-entity-recognition,3,3,1,0,,0.052611942,0,negative,0.001572053,0.002765905,0.009122433,2.52E-05,3.83E-05,0.033133425,0.005166952,0.271540154,0.000581608,0.673582161,0.000108903,0.002068209,0.000294676
4139,named-entity-recognition3,74,"Following , we use the Senna word embeddings and pre-processed the text by lowercasing all tokens and replacing all digits with 0 .",Experiments,Experiments,named-entity-recognition,3,4,1,0,,0.653972158,1,hyperparameters,0.000885004,0.000443272,0.002151388,3.60E-05,2.48E-05,0.118507951,0.007818474,0.647144552,0.000279509,0.221770279,2.18E-05,0.000372391,0.000544501
4140,named-entity-recognition3,75,CoNLL 2003 NER .,Experiments,,named-entity-recognition,3,5,1,1,tasks,0.006538981,0,experiments,0.002380676,6.81E-05,0.00741195,0.000381641,0.000205192,0.013603874,0.441464368,0.02217532,1.93E-05,0.226852836,0.001546901,0.271228959,0.01266082
4141,named-entity-recognition3,76,"The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,6,1,0,,0.074196897,0,tasks,0.000626794,1.26E-06,0.000716813,3.48E-05,3.54E-05,0.000180223,0.091374832,7.46E-06,9.98E-07,0.278902698,2.60E-05,0.01645326,0.611639586
4142,named-entity-recognition3,77,"It includes standard train , development and test sets .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,7,1,0,,0.003525185,0,negative,0.000454353,6.20E-07,0.000567258,5.71E-06,6.84E-06,8.42E-05,0.00529251,1.31E-05,2.56E-06,0.97429128,1.28E-06,0.002233153,0.017047171
4143,named-entity-recognition3,78,Following previous work we trained on both the train and development sets after tuning hyperparameters on the development set .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,8,1,0,,0.001109167,0,negative,0.000377697,2.17E-06,0.000558353,1.16E-06,7.55E-07,0.000373111,0.012217407,0.000165543,1.49E-05,0.981976134,6.82E-07,0.000303177,0.004008933
4144,named-entity-recognition3,79,The hyperparameters for our baseline model are similar to .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,9,1,0,,0.009266004,0,negative,0.000865901,3.93E-06,0.000188865,4.24E-06,1.13E-06,0.005114538,0.171802898,0.003295067,2.65E-05,0.785920538,2.47E-06,0.000851857,0.031922043
4145,named-entity-recognition3,80,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,10,1,1,tasks,0.975023823,1,tasks,0.000389171,5.80E-06,0.000752953,1.92E-05,1.98E-06,0.006964385,0.36221346,0.001766129,5.40E-05,0.058559038,3.48E-06,0.000416752,0.568853685
4146,named-entity-recognition3,81,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,11,1,1,tasks,0.853513413,1,tasks,0.000774589,1.23E-05,0.007956575,2.50E-05,3.02E-06,0.003992855,0.17934183,0.001201069,0.000407723,0.113995027,7.68E-06,0.000435822,0.691846512
4147,named-entity-recognition3,82,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,12,1,1,tasks,0.961766348,1,tasks,0.005225981,2.54E-05,0.000575818,6.29E-05,8.59E-06,0.009735909,0.39939802,0.008187429,0.00012947,0.172928831,3.42E-06,0.00132487,0.402393322
4148,named-entity-recognition3,83,CoNLL 2000 chunking .,Experiments,,named-entity-recognition,3,13,1,1,tasks,0.007338303,0,experiments,0.001007123,6.70E-05,0.009395245,0.000322618,9.30E-05,0.021490907,0.669068005,0.028089671,3.14E-05,0.124477493,0.001189106,0.120610306,0.024158178
4149,named-entity-recognition3,84,The CoNLL 2000 chunking task uses sections 15 - 18 from the Wall Street Journal corpus for training and section 20 for testing .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,14,1,0,,0.011464908,0,negative,0.000818608,1.41E-06,0.000578952,1.04E-05,5.43E-05,0.0003291,0.344045358,6.37E-06,1.22E-06,0.394918941,5.19E-06,0.003315187,0.255914976
4150,named-entity-recognition3,85,"It defines 11 syntactic chunk types ( e.g. , NP , VP , ADJP ) in addition to other .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,15,1,0,,0.033392441,0,negative,0.001299742,2.31E-06,0.001031905,7.26E-06,7.55E-05,0.000681829,0.085216487,1.84E-05,1.23E-05,0.870237052,8.75E-07,0.000468445,0.040947879
4151,named-entity-recognition3,86,We randomly sampled 1000 sentences from the training set as a held - out development set .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,16,1,0,,0.04099293,0,negative,0.000464701,2.53E-06,5.19E-05,1.13E-06,5.87E-06,0.0020087,0.33579992,0.000289128,8.94E-06,0.63310933,4.34E-07,0.000143982,0.028113453
4152,named-entity-recognition3,87,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,17,1,1,tasks,0.768253082,1,experiments,0.000406452,6.09E-06,0.010414082,2.03E-06,3.73E-06,0.002203319,0.675303113,8.57E-05,9.34E-05,0.177311796,2.25E-06,0.00018949,0.133978548
4153,named-entity-recognition3,88,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,18,1,1,tasks,0.801596025,1,experiments,0.000768993,8.20E-06,0.001492606,1.18E-05,4.32E-06,0.004644978,0.514198083,0.000388049,0.000253746,0.069956313,2.23E-06,0.000107688,0.40816298
4154,named-entity-recognition3,89,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,19,1,1,tasks,0.252393566,0,experiments,0.005141956,3.80E-06,0.000138377,9.19E-06,1.11E-05,0.00426108,0.681641504,0.000302254,2.43E-05,0.222714858,4.05E-07,0.00031684,0.085434338
4155,named-entity-recognition3,90,Pre-trained language models .,,,named-entity-recognition,3,0,1,0,,0.056351904,0,negative,6.26E-05,0.00101597,0.00018587,6.89E-07,1.46E-06,0.000347197,5.60E-05,0.011331741,0.000939027,0.98480731,0.000940746,0.000305787,5.56E-06
4156,named-entity-recognition3,91,"The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark , a publicly available benchmark for largescale language modeling .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,1,1,0,,0.00517929,0,negative,8.05E-05,0.000227457,0.005537442,5.61E-06,9.58E-06,0.001767378,0.00078205,0.054160328,4.53E-05,0.936019674,3.35E-05,0.001229963,0.000101166
4157,named-entity-recognition3,92,"The training split has approximately 800 million tokens , about a 4000X increase over the number training tokens in the CoNLL datasets .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,2,1,0,,0.001485252,0,negative,0.00028884,6.24E-05,0.00132104,1.40E-05,1.22E-05,0.0083891,0.001284387,0.205348161,7.29E-05,0.782180571,6.50E-06,0.000649524,0.000370344
4158,named-entity-recognition3,93,explored several model architectures and released their best single model and training recipes .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,3,1,0,,1.51E-05,0,negative,0.000221294,3.35E-06,0.003301068,1.40E-05,2.47E-06,0.000528107,0.00013468,0.003049772,1.74E-05,0.992219244,7.60E-06,0.000420789,8.02E-05
4159,named-entity-recognition3,94,"Following , they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,4,1,0,,0.000567382,0,negative,0.000221312,2.09E-05,0.001323743,2.38E-05,1.39E-06,0.001233054,0.000149759,0.025883528,7.30E-05,0.97044564,9.07E-05,0.00028541,0.0002478
4160,named-entity-recognition3,95,Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,5,1,0,,0.00173595,0,negative,0.000556182,5.04E-05,0.002607706,0.00011972,1.46E-05,0.014747703,0.004984959,0.12410463,0.000105727,0.842361592,0.000133152,0.005308497,0.004905208
4161,named-entity-recognition3,96,"It uses a character CNN with 4096 filters for input , followed by two stacked LSTMs , each with 8192 hidden units and a 1024 dimensional projection layer .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,6,1,0,,0.041076091,0,negative,0.000579592,0.000337334,0.227019502,4.46E-05,1.18E-05,0.01842225,0.006279286,0.239587002,0.001405782,0.50235386,6.97E-05,0.000935327,0.002953886
4162,named-entity-recognition3,97,We use CNN - BIG - LSTM to refer to this language model in our results .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,7,1,0,,0.001506809,0,negative,2.63E-05,8.76E-06,0.000897181,7.00E-06,1.53E-06,0.003129905,0.000230795,0.054817467,3.89E-05,0.94066544,2.18E-06,0.00010411,7.04E-05
4163,named-entity-recognition3,98,"In addition to CNN - BIG - LSTM from , 1 we used the same corpus to train two additional language models with fewer parameters : forward LSTM - 2048-512 and backward LSTM - 2048-512 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,8,1,0,,0.023408866,0,negative,0.000543381,5.51E-05,0.040684732,8.84E-06,1.93E-05,0.001768682,0.002839204,0.021159291,3.66E-05,0.927114807,4.11E-06,0.005496151,0.000269854
4164,named-entity-recognition3,99,Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,9,1,0,,0.031445612,0,hyperparameters,5.06E-05,6.00E-05,0.001234648,1.03E-05,1.10E-06,0.018402974,0.001495388,0.853120443,0.000218127,0.124818806,5.48E-06,8.06E-05,0.000501633
4165,named-entity-recognition3,100,"We closely followed the procedure outlined in , except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,10,1,0,,0.000305195,0,negative,0.00012208,5.65E-05,0.000742569,2.09E-05,3.59E-06,0.01073489,0.000886472,0.246906492,0.000109531,0.739905629,5.46E-06,0.000216042,0.000289814
4166,named-entity-recognition3,101,"The test set perplexities for our forward and backward LSTM - 2048 - 512 language models are 47.7 and 47.3 , respectively .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,11,1,0,,0.270633204,0,negative,0.000261737,2.44E-05,0.000314147,5.87E-06,4.06E-06,0.004966062,0.003016994,0.258527484,1.64E-05,0.71782636,6.03E-06,0.01435329,0.000677154
4167,named-entity-recognition3,102,2,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,12,1,0,,4.97E-06,0,negative,1.06E-05,7.42E-07,3.33E-05,8.55E-08,4.20E-08,2.26E-05,3.49E-06,0.001178392,7.71E-06,0.998698788,4.49E-07,4.01E-05,3.68E-06
4168,named-entity-recognition3,103,Model F 1 std 90.91 0.20 90.94 91.37 Our baseline without LM 90.87 0.13 TagLM 91.93 0.19 Training .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,13,1,0,,0.000102659,0,negative,0.000196945,5.04E-06,0.001281637,1.01E-06,9.66E-07,0.001443111,0.000937068,0.04929231,7.60E-06,0.938155772,2.56E-06,0.008514588,0.000161384
4169,named-entity-recognition3,104,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,14,1,1,hyperparameters,0.74903451,1,hyperparameters,1.85E-05,2.61E-05,6.12E-05,1.65E-05,6.14E-07,0.014960607,0.000756799,0.954987201,3.21E-05,0.028911384,9.81E-07,3.15E-05,0.000196518
4170,named-entity-recognition3,105,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,15,1,1,hyperparameters,0.048284573,0,negative,0.000477453,0.000231495,0.000409047,8.95E-06,5.23E-06,0.003542876,0.000570231,0.418795759,0.000109637,0.575131549,1.52E-06,0.000580983,0.000135269
4171,named-entity-recognition3,106,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,16,1,1,hyperparameters,0.055626948,0,negative,0.002480109,0.000912598,0.00806023,8.10E-06,8.62E-06,0.001491362,0.000403223,0.213240629,0.001149481,0.770850189,3.70E-06,0.001124334,0.000267422
4172,named-entity-recognition3,107,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,17,1,1,hyperparameters,0.096998272,0,hyperparameters,0.000145695,8.68E-05,0.000329793,3.02E-06,1.47E-06,0.004841371,0.000627032,0.765481911,9.86E-05,0.22799723,1.91E-06,0.000176556,0.000208627
4173,named-entity-recognition3,108,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,18,1,0,,0.000104905,0,negative,0.000107639,8.89E-06,0.00040885,3.25E-07,3.15E-07,0.000178976,2.70E-05,0.019684597,4.60E-05,0.979323248,6.97E-07,0.000192354,2.11E-05
4174,named-entity-recognition3,109,"an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,19,1,0,,0.000106026,0,negative,4.27E-05,7.67E-06,9.79E-05,7.21E-07,3.06E-07,0.001326344,0.0001106,0.129681657,3.02E-05,0.868489087,1.18E-06,0.000136818,7.48E-05
4175,named-entity-recognition3,110,"an order of magnitude again , train for five more epochs and stop .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,20,1,0,,6.68E-05,0,negative,0.000536643,1.60E-05,0.000865831,1.08E-06,1.16E-06,0.000710406,0.000222898,0.050395981,7.67E-05,0.945982133,1.45E-06,0.000971542,0.000218167
4176,named-entity-recognition3,111,"Following , we train each final model configuration ten times with different random seeds and report the mean and standard deviation F 1 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,21,1,0,,2.44E-05,0,negative,4.88E-05,4.94E-06,1.88E-05,2.73E-07,4.31E-07,0.000314791,4.75E-05,0.032560262,3.92E-06,0.966755399,1.19E-07,0.000233462,1.13E-05
4177,named-entity-recognition3,112,It is important to estimate the variance of model performance since the test data sets are relatively small .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,22,1,0,,5.09E-05,0,negative,0.000126951,2.41E-06,3.58E-05,3.00E-07,3.35E-07,3.50E-05,1.19E-05,0.003419325,3.88E-06,0.995354157,6.87E-07,0.000996717,1.25E-05
4178,named-entity-recognition3,113,Overall system results,,,named-entity-recognition,3,0,1,0,,0.534064979,1,negative,0.000351354,7.50E-05,1.69E-05,5.34E-07,1.00E-06,4.95E-05,0.000966855,0.000855211,1.33E-05,0.871467188,0.012699863,0.113488987,1.44E-05
4179,named-entity-recognition3,114,Tables 1 and 2 compare results from Tag LM with previously published state of the art results without additional labeled data or task specific gazetteers .,Overall system results,Overall system results,named-entity-recognition,3,1,1,0,,0.049060076,0,negative,0.000485993,3.13E-07,4.37E-05,1.33E-07,1.43E-07,3.75E-06,0.000145224,1.01E-05,1.77E-07,0.613034064,2.86E-06,0.38626811,5.46E-06
4180,named-entity-recognition3,115,compare results of,Overall system results,Overall system results,named-entity-recognition,3,2,1,0,,0.001359992,0,negative,8.86E-05,5.55E-08,3.54E-05,1.98E-08,8.55E-09,2.92E-06,4.00E-05,7.15E-06,1.40E-07,0.94440958,1.49E-06,0.05541281,1.95E-06
4181,named-entity-recognition3,116,Tag LM to other systems that include additional labeled data or gazetteers .,Overall system results,Overall system results,named-entity-recognition,3,3,1,0,,0.00033399,0,negative,0.002370847,1.68E-06,0.001289089,5.15E-06,7.14E-07,3.34E-05,0.000166348,3.88E-05,7.35E-06,0.851316761,2.43E-05,0.144598846,0.000146666
4182,named-entity-recognition3,117,"In both tasks , Tag LM establishes a new state of the art using bidirectional LMs ( the forward CNN - BIG - LSTM and the backward LSTM - 2048 - 512 ) .",Overall system results,Overall system results,named-entity-recognition,3,4,1,0,,0.944744167,1,results,0.003543334,7.93E-06,0.043199693,1.82E-06,6.42E-07,3.65E-05,0.000958147,5.31E-05,7.07E-06,0.357901343,0.000119258,0.594026275,0.000144858
4183,named-entity-recognition3,118,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .",Overall system results,Overall system results,named-entity-recognition,3,5,1,1,results,0.926435016,1,results,0.000302462,2.57E-08,4.91E-06,4.48E-08,1.22E-08,7.16E-07,0.000180951,2.21E-06,1.31E-08,0.013302217,5.75E-07,0.9861932,1.27E-05
4184,named-entity-recognition3,119,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .",Overall system results,Overall system results,named-entity-recognition,3,6,1,1,results,0.934203018,1,results,0.000249597,9.36E-08,2.62E-05,7.70E-08,2.45E-08,1.05E-06,0.000345598,3.21E-06,2.26E-08,0.017605828,3.28E-06,0.981738298,2.68E-05
4185,named-entity-recognition3,120,The improvement over the previous best result of 95.77 in that jointly trains with Penn Treebank ( PTB ) POS tags is statistically significant at 95 % ( p < 0.001 assuming standard deviation of 0.1 ) .,Overall system results,Overall system results,named-entity-recognition,3,7,1,0,,0.938457205,1,results,0.000769997,3.72E-08,1.08E-05,5.38E-08,1.84E-08,6.69E-07,0.000208776,1.99E-06,2.34E-08,0.019993209,4.73E-07,0.979000032,1.40E-05
4186,named-entity-recognition3,121,"Importantly , the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F 1 in the NER and Chunking tasks , respectively .",Overall system results,Overall system results,named-entity-recognition,3,8,1,0,,0.924432474,1,results,0.03881175,3.33E-07,2.00E-05,6.34E-07,1.15E-07,4.53E-06,0.000448631,1.72E-05,3.60E-07,0.032953259,1.02E-06,0.927695892,4.63E-05
4187,named-entity-recognition3,122,Adding external resources .,Overall system results,,named-entity-recognition,3,9,1,0,,0.010298886,0,negative,0.000478411,1.38E-06,5.57E-05,6.91E-07,1.22E-07,2.83E-05,6.03E-05,9.85E-05,5.99E-06,0.955713265,1.34E-05,0.043509361,3.46E-05
4188,named-entity-recognition3,123,"Although we do not use external labeled data or gazetteers , we found that TagLM outperforms previous state of the art results in both tasks when external resources ( labeled data or task specific gazetteers ) are available .",Overall system results,Adding external resources .,named-entity-recognition,3,10,1,0,,0.302488388,0,results,0.001766546,1.23E-06,1.98E-05,5.49E-07,2.41E-07,8.96E-05,0.000591922,0.000188508,4.47E-07,0.273847546,1.79E-05,0.723451366,2.44E-05
4189,named-entity-recognition3,124,"Furthermore , show that , in most cases , the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning .",Overall system results,Adding external resources .,named-entity-recognition,3,11,1,0,,0.018059104,0,negative,0.006977323,1.27E-06,1.63E-05,2.68E-07,3.57E-07,2.31E-05,0.000171312,6.17E-05,4.37E-07,0.562866299,2.07E-06,0.429874466,5.08E-06
4190,named-entity-recognition3,125,"For example , noted an improvement of only 0.06 F 1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and reported an increase of 0.71 F 1 when adding gazetteers to their baseline .",Overall system results,Adding external resources .,named-entity-recognition,3,12,1,0,,3.78E-06,0,negative,8.85E-05,3.99E-07,5.94E-05,7.64E-08,8.51E-08,2.65E-05,3.38E-05,3.98E-05,4.70E-07,0.996604655,6.03E-06,0.003138079,2.20E-06
4191,named-entity-recognition3,126,"In the Chunking task , previous work has reported from 0.28 to 0.75 improvement in F 1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities .",Overall system results,Adding external resources .,named-entity-recognition,3,13,1,0,,0.000313544,0,negative,0.000153671,1.11E-06,2.55E-05,1.57E-07,7.14E-08,1.70E-05,7.14E-05,5.61E-05,3.46E-07,0.980176657,0.000178727,0.019312721,6.55E-06
4192,named-entity-recognition3,127,Analysis,Overall system results,,named-entity-recognition,3,14,1,0,,0.000153613,0,negative,0.000197319,6.23E-07,1.19E-05,1.35E-06,1.21E-07,1.86E-05,1.81E-05,6.51E-05,6.36E-06,0.997097309,2.21E-06,0.00256803,1.29E-05
4193,named-entity-recognition3,128,"To elucidate the characteristics of our LM augmented sequence tagger , we ran a number of additional experiments on the CoNLL 2003 NER task .",Overall system results,Analysis,named-entity-recognition,3,15,1,0,,7.87E-06,0,negative,0.000834993,8.68E-05,1.76E-05,2.19E-07,4.70E-05,2.19E-05,2.27E-05,1.80E-05,7.09E-06,0.998637934,1.01E-05,0.000295517,2.12E-07
4194,named-entity-recognition3,129,How to use LM embeddings ?,Overall system results,Analysis,named-entity-recognition,3,16,1,0,,5.06E-06,0,negative,2.71E-05,3.16E-05,1.07E-06,5.13E-07,7.79E-07,4.95E-05,3.19E-06,0.000105909,2.36E-05,0.999247896,0.000504076,4.35E-06,5.25E-07
4195,named-entity-recognition3,130,"In this experiment , we concatenate the LM embeddings at dif - shows that the second alternative performs best .",Overall system results,Analysis,named-entity-recognition,3,17,1,0,,0.000114647,0,negative,0.001950339,0.001150745,0.000120441,4.33E-07,2.77E-05,0.000282239,4.00E-05,0.000552795,0.000381763,0.995379769,5.53E-06,0.000107595,6.26E-07
4196,named-entity-recognition3,131,We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in a way that improves over all system performance .,Overall system results,Analysis,named-entity-recognition,3,18,1,0,,3.00E-06,0,negative,0.002171823,0.000122412,1.46E-05,7.70E-07,1.10E-05,3.02E-05,2.45E-06,4.13E-05,0.000414055,0.997151933,5.34E-06,3.38E-05,3.29E-07
4197,named-entity-recognition3,132,These results are consistent with who found that chunking performance was sensitive to the level at which additional POS supervision was added .,Overall system results,Analysis,named-entity-recognition,3,19,1,0,,6.37E-05,0,negative,0.000851156,3.03E-05,1.25E-06,2.95E-07,9.13E-06,1.99E-05,7.64E-06,3.17E-05,1.28E-05,0.998863775,1.55E-05,0.000156242,2.52E-07
4198,named-entity-recognition3,133,Does it matter which language model to use ?,Overall system results,Analysis,named-entity-recognition,3,20,1,0,,2.17E-07,0,negative,7.66E-06,6.46E-06,2.00E-07,8.40E-08,2.64E-07,1.10E-05,6.31E-07,2.82E-05,1.07E-05,0.999890764,4.27E-05,1.32E-06,8.57E-08
4199,named-entity-recognition3,134,"In this experiment , we compare six different configurations of the forward and backward language models ( including the baseline model which does not use any language models ) .",Overall system results,Analysis,named-entity-recognition,3,21,1,0,,0.000476709,0,negative,0.001166427,0.002515088,0.000284885,4.58E-07,7.96E-05,9.56E-05,6.54E-05,0.000193874,0.000399003,0.994917212,2.02E-05,0.000260781,1.52E-06
4200,named-entity-recognition3,135,The results are reported in .,Overall system results,,named-entity-recognition,3,22,1,0,,0.000404652,0,negative,0.000262285,6.40E-08,7.61E-06,3.58E-08,5.15E-08,1.30E-06,1.46E-05,4.33E-06,1.57E-07,0.954553294,7.16E-08,0.045154171,2.06E-06
4201,named-entity-recognition3,136,"We find that adding backward LM embeddings consistently outperforms forward - only LM embeddings , with F 1 improvements between 0.22 and 0.27 % , even with the relatively small backward LSTM - 2048-512 LM .",Overall system results,The results are reported in .,named-entity-recognition,3,23,1,0,,0.930402883,1,results,0.005217684,3.06E-07,1.00E-05,2.18E-06,1.46E-07,1.22E-05,0.001147643,3.80E-05,4.35E-07,0.062534157,7.84E-07,0.930111131,0.000925315
4202,named-entity-recognition3,137,"LM size is important , and replacing the forward LSTM - 2048 - 512 with CNN - BIG - LSTM ( test perplexities of 47.7 to 30.0 on 1B Word Benchmark ) improves F 1 by 0.26 - 0.31 % , about as much as adding backward LM .",Overall system results,The results are reported in .,named-entity-recognition,3,24,1,0,,0.867128148,1,results,0.12297271,6.15E-07,4.96E-05,6.97E-06,4.44E-07,1.45E-05,0.000904259,3.21E-05,1.94E-06,0.106145901,8.09E-07,0.768950275,0.000919914
4203,named-entity-recognition3,138,"Accordingly , we hypothesize ( but have not tested ) that replacing the backward LSTM - 2048 - 512 with a backward LM analogous to the CNN - BIG - LSTM would further improve performance .",Overall system results,The results are reported in .,named-entity-recognition,3,25,1,0,,0.000279726,0,negative,0.00058262,1.38E-07,2.02E-05,5.50E-08,5.47E-08,1.61E-06,1.30E-05,4.66E-06,4.74E-07,0.972711238,7.11E-08,0.026660523,5.35E-06
4204,named-entity-recognition3,139,"To highlight the importance of including language models trained on a large scale data , we also experimented with training a language model on just the CoNLL 2003 training and development data .",Overall system results,The results are reported in .,named-entity-recognition,3,26,1,0,,0.001874392,0,negative,0.000462788,4.88E-07,5.46E-05,2.41E-07,5.77E-07,4.92E-06,5.02E-05,8.01E-06,5.73E-07,0.961918307,1.44E-07,0.037473732,2.54E-05
4205,named-entity-recognition3,140,Due to the much smaller size of this data Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM .,Overall system results,The results are reported in .,named-entity-recognition,3,27,1,0,,0.003296095,0,results,0.022268938,6.68E-08,1.01E-05,2.06E-07,5.65E-08,1.69E-06,0.00023626,3.21E-06,9.96E-08,0.12289368,1.42E-07,0.854498908,8.67E-05
4206,named-entity-recognition3,141,"This result supports the hypothesis that adding language models help because they learn composition functions ( i.e. , the RNN parameters in the language model ) from much larger data compared to the composition functions in the baseline tagger , which are only learned from labeled data .",Overall system results,The results are reported in .,named-entity-recognition,3,28,1,0,,0.052235053,0,results,0.003026269,1.50E-07,5.35E-06,2.12E-07,1.14E-07,2.72E-06,0.000223661,8.68E-06,1.74E-07,0.373893098,2.18E-07,0.622743168,9.62E-05
4207,named-entity-recognition3,142,Importance of task specific RNN .,Overall system results,,named-entity-recognition,3,29,1,0,,0.005421869,0,negative,0.001505754,3.79E-07,5.37E-05,2.19E-07,8.61E-08,3.70E-06,3.05E-05,1.33E-05,1.63E-06,0.913673612,7.65E-07,0.084701972,1.43E-05
4208,named-entity-recognition3,143,To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,30,1,0,,0.000124548,0,negative,0.012644358,7.00E-07,3.76E-05,6.56E-08,3.13E-07,2.10E-06,6.35E-06,5.24E-06,2.09E-06,0.972597409,1.13E-07,0.014699177,4.52E-06
4209,named-entity-recognition3,144,"In this setup , performance was very low , 88.17 F 1 , well below our baseline .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,31,1,0,,0.003532993,0,negative,0.001647859,1.40E-07,1.19E-05,3.13E-07,5.76E-07,4.40E-06,4.89E-05,4.61E-06,2.29E-07,0.834580388,1.80E-07,0.163604051,9.65E-05
4210,named-entity-recognition3,145,This result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,32,1,0,,0.000553107,0,negative,0.002749059,3.89E-07,8.53E-06,6.98E-08,1.32E-07,2.00E-06,2.31E-05,6.81E-06,9.46E-07,0.954362135,2.25E-07,0.042822253,2.43E-05
4211,named-entity-recognition3,146,Does the LM transfer across domains ?,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,33,1,0,,8.20E-05,0,negative,7.26E-05,3.35E-07,1.14E-05,3.98E-08,2.33E-08,1.64E-06,2.32E-06,7.82E-06,4.17E-06,0.999206326,8.26E-07,0.000670529,2.19E-05
4212,named-entity-recognition3,147,One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,34,1,0,,8.57E-06,0,negative,0.000592335,1.73E-06,4.25E-05,4.31E-07,1.49E-06,2.72E-06,2.40E-06,6.61E-06,2.59E-06,0.997446938,2.36E-07,0.001886583,1.34E-05
4213,named-entity-recognition3,148,"To test the sensitivity to the LM training domain , we also applied Tag LM with a LM trained on news articles to the SemEval 2017 Shared Task 10 , Science IE .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,35,1,0,,0.00080959,0,negative,0.001709606,5.71E-06,0.000797297,1.89E-07,2.24E-06,5.23E-06,5.15E-05,1.15E-05,3.84E-06,0.974440314,2.84E-07,0.022928838,4.34E-05
4214,named-entity-recognition3,149,"5 Scien - ce IE requires end - to - end joint entity and relationship extraction from scientific publications across three diverse fields ( computer science , material sciences , and physics ) and defines three broad entity types ( Task , Material and Process ) .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,36,1,0,,0.000492472,0,negative,0.001316798,2.16E-06,0.000395969,7.61E-06,3.33E-06,1.85E-05,0.000112529,9.87E-06,1.96E-06,0.889379457,9.18E-05,0.102364699,0.006295341
4215,named-entity-recognition3,150,"For this task , Tag LM increased F 1 on the development set by 4.12 % ( from 49.93 to to 54.05 % ) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to Science IE , Scenario 1 .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,37,1,0,,0.044322186,0,results,0.007540066,4.29E-07,9.57E-05,1.88E-07,4.48E-07,2.67E-06,0.000165882,3.36E-06,2.90E-07,0.269262069,4.95E-07,0.722657211,0.0002712
4216,named-entity-recognition3,151,We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,38,1,0,,0.012527401,0,negative,0.01054534,9.61E-07,9.45E-05,2.10E-07,2.45E-07,3.79E-06,0.000154726,6.11E-06,1.06E-06,0.510242398,2.18E-06,0.478725351,0.000223098
4217,named-entity-recognition3,152,Related work,,,named-entity-recognition,3,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
4218,named-entity-recognition3,164,Neural language models .,,,named-entity-recognition,3,0,1,0,,0.067756354,0,research-problem,4.71E-05,0.000132093,0.000193023,1.16E-06,7.04E-07,6.84E-05,0.001326147,0.000611743,3.52E-05,0.293680097,0.697213204,0.006648404,4.28E-05
4219,named-entity-recognition3,165,LMs have always been a critical component in statistical machine translation systems .,Neural language models .,Neural language models .,named-entity-recognition,3,1,1,0,,0.608190327,1,negative,0.000200783,2.39E-06,0.000252082,2.83E-06,1.32E-07,3.42E-05,0.002344035,4.29E-05,2.75E-06,0.948852377,0.020997779,0.027011695,0.000255999
4220,named-entity-recognition3,166,"Recently , neural LMs have also been integrated in neural machine translation systems ( e.g. , to score candidate translations .",Neural language models .,Neural language models .,named-entity-recognition,3,2,1,0,,0.012273249,0,negative,1.62E-05,4.40E-07,3.92E-05,3.56E-07,1.50E-08,2.25E-05,0.000117661,3.54E-05,3.39E-06,0.998651316,0.000606106,0.000494024,1.34E-05
4221,named-entity-recognition3,167,"In contrast , Tag LM uses neural LMs to encode words in the input sequence .",Neural language models .,Neural language models .,named-entity-recognition,3,3,1,0,,0.0305403,0,negative,0.000224083,4.05E-06,0.020716656,5.28E-07,1.22E-07,2.22E-05,0.000215612,3.14E-05,3.02E-05,0.976959584,7.20E-05,0.001709196,1.43E-05
4222,named-entity-recognition3,168,"Unlike forward LMs , bidirectional LMs have received little prior attention .",Neural language models .,Neural language models .,named-entity-recognition,3,4,1,0,,0.023903951,0,negative,1.72E-05,4.75E-07,6.39E-05,5.58E-07,2.32E-08,1.63E-05,7.43E-05,2.63E-05,2.23E-06,0.999002308,0.000350906,0.000435152,1.03E-05
4223,named-entity-recognition3,169,"Most similar to our formulation , used a bidirectional neural LM in a statistical machine translation system for instance selection .",Neural language models .,Neural language models .,named-entity-recognition,3,5,1,0,,7.64E-05,0,negative,1.42E-05,2.22E-07,3.63E-05,4.70E-07,3.31E-08,2.10E-05,4.35E-05,2.43E-05,2.34E-06,0.999645166,2.16E-05,0.000186303,4.65E-06
4224,named-entity-recognition3,170,"They tied the input token embeddings and softmax weights in the forward and backward directions , unlike our approach which uses two distinct models without any shared parameters .",Neural language models .,Neural language models .,named-entity-recognition,3,6,1,0,,0.006912833,0,negative,0.000199202,3.17E-06,0.000399601,2.25E-07,9.39E-08,1.06E-05,2.86E-05,3.47E-05,3.02E-05,0.99898679,1.97E-06,0.000303499,1.31E-06
4225,named-entity-recognition3,171,Frinken et al. ( 2012 ) also used a bidirectional n-gram LM for handwriting recognition .,Neural language models .,Neural language models .,named-entity-recognition,3,7,1,0,,0.007610208,0,negative,1.34E-05,4.45E-07,0.000112171,1.47E-07,2.16E-08,1.86E-05,0.000159667,3.89E-05,3.97E-06,0.998851231,4.13E-05,0.000748393,1.17E-05
4226,named-entity-recognition3,172,Interpreting RNN states .,Neural language models .,,named-entity-recognition,3,8,1,0,,0.000243509,0,negative,3.36E-06,2.84E-07,8.96E-06,1.38E-08,1.78E-09,3.84E-06,5.62E-05,1.94E-05,4.46E-06,0.998767372,0.000114977,0.001018532,2.60E-06
4227,named-entity-recognition3,173,"Recently , there has been some interest in interpreting the activations of RNNs .",Neural language models .,Interpreting RNN states .,named-entity-recognition,3,9,1,0,,0.004181048,0,negative,9.85E-06,7.40E-07,2.23E-06,1.70E-07,2.61E-08,6.14E-06,2.07E-05,1.21E-05,3.80E-06,0.998922712,0.000954413,6.57E-05,1.41E-06
4228,named-entity-recognition3,174,showed that single LSTM units can learn to predict singular - plural distinctions .,Neural language models .,Interpreting RNN states .,named-entity-recognition,3,10,1,0,,0.000411176,0,negative,2.10E-05,1.45E-07,6.76E-06,2.90E-08,1.97E-08,3.13E-06,1.31E-05,2.95E-06,2.52E-06,0.999836902,1.32E-05,9.99E-05,3.26E-07
4229,named-entity-recognition3,175,"visualized character level LSTM states and showed that individual cells capture long - range dependencies such as line lengths , quotes and brackets .",Neural language models .,Interpreting RNN states .,named-entity-recognition,3,11,1,0,,0.000542905,0,negative,3.17E-05,2.08E-07,1.01E-05,2.40E-08,3.68E-08,2.88E-06,1.17E-05,2.76E-06,4.83E-06,0.999728282,2.89E-06,0.000204426,2.60E-07
4230,named-entity-recognition3,176,Our work complements these studies by showing that LM states are useful for downstream tasks as away of interpreting what they learn .,Neural language models .,Interpreting RNN states .,named-entity-recognition,3,12,1,0,,0.002808009,0,negative,0.001361332,2.23E-06,2.00E-05,4.35E-08,2.44E-07,1.90E-06,2.34E-05,3.40E-06,9.90E-06,0.995526588,9.90E-07,0.003049693,2.48E-07
4231,named-entity-recognition3,177,Other sequence tagging models .,Neural language models .,,named-entity-recognition,3,13,1,0,,0.001377783,0,negative,1.15E-05,8.94E-08,0.000132749,5.28E-08,1.17E-08,8.12E-06,4.10E-05,1.30E-05,2.83E-06,0.999593853,2.42E-06,0.000191316,2.92E-06
4232,named-entity-recognition3,178,Current state of the art results in sequence tagging problems are based on bidirectional RNN models .,Neural language models .,Other sequence tagging models .,named-entity-recognition,3,14,1,0,,0.395260264,0,negative,1.20E-05,1.13E-06,7.71E-05,2.01E-07,2.92E-08,1.20E-05,5.84E-05,4.70E-05,1.79E-06,0.998546518,0.000930295,0.000308445,5.00E-06
4233,named-entity-recognition3,179,"However , many other sequence tagging models have been proposed in the literature for this class of problems ( e.g. , .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,15,1,0,,0.000109135,0,negative,2.33E-06,2.80E-07,9.34E-06,3.11E-08,5.34E-09,3.09E-06,5.42E-06,1.85E-05,9.92E-07,0.999846859,6.88E-05,4.39E-05,3.84E-07
4234,named-entity-recognition3,180,"LM embeddings could also be used as additional features in other models , although it is not clear whether the model complexity would be sufficient to effectively make use of them .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,16,1,0,,0.000366319,0,negative,1.68E-05,2.97E-07,1.16E-05,9.59E-08,2.10E-08,1.75E-05,7.76E-06,5.71E-05,5.56E-06,0.999837819,5.89E-07,4.45E-05,3.84E-07
4235,named-entity-recognition3,181,Conclusion,,,named-entity-recognition,3,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
4236,relation-classification5,1,title,,,relation-classification,5,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
4237,relation-classification5,2,End - to - end neural relation extraction using deep biaffine attention,title,title,relation-classification,5,1,1,1,research-problem,0.996969054,1,research-problem,4.34E-08,2.72E-05,2.04E-07,4.30E-08,4.97E-08,7.03E-08,5.64E-07,1.45E-06,4.71E-06,0.00201107,0.997954365,2.34E-07,4.37E-08
4238,relation-classification5,3,abstract,,,relation-classification,5,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
4239,relation-classification5,4,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .",abstract,abstract,relation-classification,5,1,1,1,research-problem,0.83981331,1,research-problem,1.05E-06,0.00352869,1.20E-05,1.02E-06,4.84E-06,1.78E-06,2.63E-06,2.49E-05,0.000622544,0.016365343,0.979433788,9.07E-07,4.96E-07
4240,relation-classification5,5,"The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship .",abstract,abstract,relation-classification,5,2,1,0,,0.234416717,0,research-problem,2.86E-05,0.111271122,7.53E-05,8.72E-05,0.000423002,3.65E-05,2.29E-05,0.000347685,0.005551697,0.177428093,0.704712023,9.25E-06,6.58E-06
4241,relation-classification5,6,"On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",abstract,abstract,relation-classification,5,3,1,0,,0.015207159,0,research-problem,5.91E-05,0.001121345,5.39E-06,4.99E-06,3.20E-05,1.05E-05,0.000120243,0.00017148,1.86E-05,0.390081973,0.607529427,0.000841279,3.66E-06
4242,relation-classification5,7,Introduction,,,relation-classification,5,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
4243,relation-classification5,8,Extracting entities and their semantic relations from raw text is a key information extraction task .,Introduction,Introduction,relation-classification,5,1,1,1,research-problem,0.929447494,1,research-problem,1.30E-06,0.000142444,4.12E-07,2.76E-05,4.10E-05,6.16E-06,1.31E-05,4.55E-06,3.40E-05,0.035407854,0.964317777,1.11E-06,2.70E-06
4244,relation-classification5,9,"For example , given the sentence "" David Foster is the AP 's Northwest regional reporter , based in Seattle "" in the CoNLL04 dataset , our goal is to recognize "" David Foster "" as person , "" AP "" as organization , and "" Northwest "" and "" Seattle "" as location entities , then classifiy entity pairs to extract structured information : Work For ( David Foster , AP ) , OrgBased In ( AP , Northwest ) and OrgBased In ( AP , Seattle ) .",Introduction,Introduction,relation-classification,5,2,1,0,,0.189707346,0,negative,2.61E-05,0.110953178,0.000117436,5.71E-05,0.006041493,0.000121985,7.34E-05,0.000126321,0.062704182,0.459963584,0.359770566,2.97E-05,1.50E-05
4245,relation-classification5,10,Such information is useful in many other NLP tasks .,Introduction,Introduction,relation-classification,5,3,1,0,,0.026512667,0,research-problem,2.69E-06,0.000864082,9.39E-07,2.75E-05,0.0001028,4.59E-05,1.90E-05,4.32E-05,0.000663841,0.399814513,0.598409474,2.92E-06,3.17E-06
4246,relation-classification5,11,"Especially in IR applications such as entity search , structured search and question answering , it helps provide end users with significantly better search experience .",Introduction,Introduction,relation-classification,5,4,1,0,,0.415385994,0,research-problem,3.25E-06,0.000597591,5.24E-07,1.14E-05,3.35E-05,1.16E-05,1.77E-05,2.01E-05,0.000172231,0.153807903,0.845316901,5.03E-06,2.32E-06
4247,relation-classification5,12,A common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification .,Introduction,Introduction,relation-classification,5,5,1,0,,0.330362416,0,research-problem,2.04E-06,0.000632335,1.27E-06,8.25E-06,2.04E-05,1.20E-05,1.30E-05,1.27E-05,0.000318419,0.099983429,0.898992282,1.93E-06,1.95E-06
4248,relation-classification5,13,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",Introduction,Introduction,relation-classification,5,6,1,1,research-problem,0.580384343,1,research-problem,1.63E-06,0.000460574,5.36E-07,3.55E-06,9.20E-06,9.82E-06,1.22E-05,1.76E-05,0.000270821,0.124031594,0.875178688,2.25E-06,1.53E-06
4249,relation-classification5,14,Traditional joint approaches are feature - based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources .,Introduction,Introduction,relation-classification,5,7,1,0,,0.109506264,0,research-problem,1.76E-05,0.007521579,1.68E-05,6.58E-05,0.000436069,0.00013636,9.76E-05,0.000127994,0.001625523,0.420270357,0.569660838,1.14E-05,1.21E-05
4250,relation-classification5,15,State - of - the - art relation extraction performance has been obtained by end - to - end models based on neural networks .,Introduction,Introduction,relation-classification,5,8,1,0,,0.040975537,0,research-problem,3.74E-06,0.000651222,1.04E-06,8.52E-06,2.15E-05,2.89E-05,4.13E-05,4.71E-05,0.000226608,0.225214821,0.773744959,6.54E-06,3.78E-06
4251,relation-classification5,16,"Specifically , proposed a RNNbased model which achieved top results on the CoNLL04 dataset .",Introduction,Introduction,relation-classification,5,9,1,0,,0.094668874,0,negative,5.55E-05,0.021714735,7.22E-05,7.35E-05,0.000981516,0.000416857,0.000199683,0.000376508,0.025392159,0.789343013,0.161309924,3.81E-05,2.64E-05
4252,relation-classification5,17,Their approach relies on various manually extracted features .,Introduction,,relation-classification,5,10,1,0,,0.003610907,0,negative,2.29E-05,0.008355858,1.17E-05,0.000191864,0.000767558,0.000737074,0.000101915,0.000524261,0.004910091,0.880927745,0.103421395,1.12E-05,1.64E-05
4253,relation-classification5,18,Other neural models employ dependency parsing - based information .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,11,1,0,,0.009157481,0,negative,7.47E-05,0.000644722,0.000105446,3.29E-06,8.69E-05,0.000300758,2.73E-05,3.51E-05,0.030430638,0.958373456,0.00989158,7.92E-06,1.82E-05
4254,relation-classification5,19,"In particular , applied bottom - up and top - down tree - structured LSTMs to model dependency paths between entities .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,12,1,0,,0.015124224,0,negative,0.000152745,0.000526674,0.000101496,1.51E-05,0.001837172,0.000180652,1.53E-05,1.05E-05,0.001675352,0.99291769,0.002549623,8.84E-06,8.82E-06
4255,relation-classification5,20,integrated implicit syntactic information by using latent feature representations extracted from a pre-trained BiLSTM - based dependency parser .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,13,1,0,,0.03305381,0,negative,0.002464546,0.009868874,0.003104077,7.52E-06,0.000649378,0.000168495,0.000111388,2.26E-05,0.154214959,0.797133484,0.031944812,0.000266034,4.38E-05
4256,relation-classification5,21,"entity recognition , and a CNN on top of the BiLSTM for classifying relations .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,14,1,0,,0.405949637,0,negative,0.000337575,0.002601611,0.00039961,1.26E-05,0.000566578,0.000166442,0.000421931,2.18E-05,0.009797521,0.625750368,0.359269673,0.000375731,0.000278565
4257,relation-classification5,22,"Adel and Schtze ( 2017 ) assumed that entity boundaries are given , and trained a CNN to extract context features around the entities , and using these features for entity and relation classification .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,15,1,0,,0.014632139,0,negative,0.000138015,0.002421335,0.000166595,2.30E-05,0.000558148,0.000556524,4.85E-05,7.11E-05,0.031880651,0.947182366,0.01685824,1.80E-05,7.74E-05
4258,relation-classification5,23,"Recently , formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM - and transition - based approach to generate the graph incrementally . [ 4 ] extended the multi-head selection - based joint model with adversarial training .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,16,1,0,,0.01304319,0,negative,0.000184992,0.002662817,0.000222013,4.74E-05,0.000931078,0.000800027,8.12E-05,8.91E-05,0.018642395,0.940264751,0.035903952,2.45E-05,0.00014581
4259,relation-classification5,24,"In , the joint task is formulated as a sequence tagging problem , and a BiLSTM with a softmax output layer can then be used for joint prediction .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,17,1,0,,0.221092437,0,model,0.000474855,0.032219722,0.001367354,1.62E-05,0.002069143,0.000152404,2.87E-05,2.90E-05,0.490839894,0.468169333,0.004559336,5.11E-05,2.30E-05
4260,relation-classification5,25,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,18,1,1,model,0.928857709,1,model,0.000777852,0.083082766,0.003506008,1.08E-05,0.001126367,0.000205165,0.000198868,4.27E-05,0.824076016,0.052983935,0.033785161,0.000108143,9.63E-05
4261,relation-classification5,26,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,19,1,1,model,0.745084779,1,model,0.000229795,0.011156826,0.00055633,1.36E-05,0.001811532,0.000144398,1.61E-05,1.64E-05,0.861823586,0.123664747,0.000535285,9.64E-06,2.17E-05
4262,relation-classification5,27,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,20,1,1,model,0.846918382,1,model,5.82E-05,0.0067245,0.000709427,9.69E-07,0.000261948,4.38E-05,1.05E-05,5.80E-06,0.97222403,0.019724728,0.000222255,3.56E-06,1.03E-05
4263,relation-classification5,28,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,21,1,1,model,0.771337437,1,model,2.61E-05,0.004164788,0.000396486,9.73E-08,3.26E-05,8.72E-06,3.84E-06,2.18E-06,0.983749594,0.011553143,5.99E-05,1.11E-06,1.40E-06
4264,relation-classification5,29,"In most previous neural joint models , the relation classification part relies on a common "" linear "" concatenation - based mechanism over the latent features associated with entity pairs , i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,22,1,0,,0.401416103,0,negative,0.000575058,0.025618712,0.001300484,4.61E-05,0.002646748,0.000522334,8.18E-05,7.11E-05,0.165225454,0.792848501,0.010921414,5.82E-05,8.40E-05
4265,relation-classification5,30,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,23,1,1,model,0.807626297,1,model,0.000413311,0.022025389,0.001213165,1.49E-06,0.000395028,5.04E-05,1.95E-05,1.15E-05,0.906532523,0.069098999,0.000220011,1.34E-05,5.27E-06
4266,relation-classification5,31,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,24,1,1,model,0.876144869,1,model,0.000818637,0.097833559,0.001404915,1.19E-05,0.001828047,0.000147575,8.22E-05,3.74E-05,0.861504116,0.035139548,0.001113191,4.71E-05,3.19E-05
4267,relation-classification5,32,"Experimental results on the benchmark "" relation and entity recognition "" dataset CoNLL04 show that our model outperforms previous models , obtaining new stateof - the - art scores .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,25,1,0,,0.040199032,0,negative,0.013175782,0.002700938,0.000180469,9.82E-06,0.000891774,0.000263423,0.005612637,5.92E-05,0.001915828,0.95153046,0.001729519,0.02170926,0.000220842
4268,relation-classification5,33,"In addition , using the biaffine attention improves the performance compared to using the linear mechanism significantly .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,26,1,0,,0.6475372,1,ablation-analysis,0.603265829,0.002788451,0.00021588,0.000161271,0.005576811,0.001228423,0.010875177,0.000221388,0.001269836,0.3608699,0.000181873,0.012694495,0.000650666
4269,relation-classification5,34,We also provide an ablation study to investigate effects of different contributing factors in our model .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,27,1,0,,0.024443859,0,negative,0.000843423,0.000683917,7.66E-05,1.28E-05,0.003439925,0.000154477,2.16E-05,1.15E-05,0.004259693,0.990446167,2.32E-05,2.31E-05,3.58E-06
4270,relation-classification5,35,Our proposed model,,,relation-classification,5,0,1,0,,0.006336549,0,negative,0.000193807,0.002594737,0.000136741,8.07E-05,2.30E-05,0.000644828,0.000286901,0.006976608,0.0064963,0.950624039,0.031428596,0.000344249,0.000169428
4271,relation-classification5,36,This section details our end - to - end relation extraction model .,Our proposed model,Our proposed model,relation-classification,5,1,1,0,,0.008434731,0,negative,3.77E-05,0.056974085,0.000134452,2.44E-06,0.000107232,6.04E-05,1.03E-05,0.000642599,0.106234571,0.834982866,0.00073056,8.04E-05,2.42E-06
4272,relation-classification5,37,"Given an input sequence of n word tokens w 1 , w 2 , ... , w n , we use a vector vi to represent each i th word w i by concatenating word embedding e",Our proposed model,Our proposed model,relation-classification,5,2,1,0,,0.000105797,0,negative,1.05E-05,0.005038884,1.99E-05,5.52E-07,7.18E-06,7.12E-05,2.91E-06,0.000847068,0.028610814,0.965280603,0.000100012,9.90E-06,4.74E-07
4273,relation-classification5,38,"Here , for each word type w , we use a one - layer BiLSTM ( BiLSTM char ) to learn its character - level word embedding e ( C ) w.",Our proposed model,Our proposed model,relation-classification,5,3,1,0,,0.000631458,0,negative,2.80E-05,0.021639285,0.000283903,1.11E-06,1.92E-05,0.000134726,9.83E-06,0.001442779,0.265341383,0.710952616,0.000135063,1.05E-05,1.66E-06
4274,relation-classification5,39,Named entity recognition ( NER ) :,Our proposed model,Our proposed model,relation-classification,5,4,1,0,,0.734484928,1,research-problem,0.00013737,0.006493514,0.000358624,3.23E-05,0.000146384,0.000108675,0.001326784,0.000573456,0.000897253,0.389123307,0.592573124,0.008117768,0.000111488
4275,relation-classification5,40,"The NER component feeds the sequence of vectors v 1:n with an additional context position index i into another BiLSTM ( BiLSTM NER ) to learn a "" latent "" feature vector representing the i th word token .",Our proposed model,Our proposed model,relation-classification,5,5,1,0,,0.002224584,0,negative,4.91E-05,0.014781512,0.00034477,4.46E-06,4.33E-05,0.000210966,2.10E-05,0.001367264,0.288694272,0.694255916,0.000202649,1.83E-05,6.54E-06
4276,relation-classification5,41,Then the NER component performs linear transformation of each latent feature vector by using a single - layer feed - forward network ( FFNN NER ) :,Our proposed model,Our proposed model,relation-classification,5,6,1,0,,0.005870401,0,negative,0.000124266,0.009975302,0.003390349,4.98E-07,2.02E-05,7.71E-05,1.55E-05,0.000294193,0.172036282,0.813914064,9.73E-05,5.35E-05,1.53E-06
4277,relation-classification5,42,The output layer size of FFNN NER is the number of BIOLU - based NER labels .,Our proposed model,Our proposed model,relation-classification,5,7,1,0,,0.001756355,0,negative,6.94E-06,0.011730244,1.62E-05,2.86E-06,1.23E-05,0.004140723,6.16E-05,0.108799796,0.031152495,0.843807715,0.000252092,9.86E-06,7.21E-06
4278,relation-classification5,43,The NER component feeds the output vectors h 1:n into a linear - chain CRF layer for NER label prediction .,Our proposed model,Our proposed model,relation-classification,5,8,1,0,,0.002003408,0,negative,6.77E-05,0.018295271,0.000298078,4.96E-06,4.83E-05,0.000285577,2.73E-05,0.002245353,0.373865516,0.604729744,0.000106946,1.83E-05,7.04E-06
4279,relation-classification5,44,"A cross-entropy loss L NER is computed during training , while the Viterbi algorithm is used for decoding .",Our proposed model,Our proposed model,relation-classification,5,9,1,0,,0.0016835,0,negative,3.25E-05,0.048403954,2.75E-05,4.34E-06,4.04E-05,0.000528712,1.81E-05,0.025206758,0.100542234,0.825107964,6.90E-05,1.54E-05,3.00E-06
4280,relation-classification5,45,Our NER component thus is the BiLSTM - CRF model with additional LSTM - based character - level word embeddings .,Our proposed model,Our proposed model,relation-classification,5,10,1,0,,0.040542616,0,model,0.000133453,0.063520429,0.001204044,1.23E-05,0.000360836,0.000266469,5.41E-05,0.001145549,0.537276423,0.395624966,0.00029021,8.86E-05,2.26E-05
4281,relation-classification5,46,"Relation classification ( RC ) : Assume that t 1 , t 2 , ... , tn are NER labels predicted by the NER component for the input words .",Our proposed model,Our proposed model,relation-classification,5,11,1,0,,0.075076295,0,negative,1.28E-05,0.011066946,0.000112166,6.77E-06,1.89E-05,0.000139955,0.000104578,0.001340152,0.010510874,0.751343673,0.225108204,0.000209767,2.52E-05
4282,relation-classification5,47,We represent each i th predicted label by a vector embedding e ti .,Our proposed model,Our proposed model,relation-classification,5,12,1,0,,0.000245596,0,negative,1.00E-05,0.003610941,1.35E-05,6.96E-07,1.16E-05,8.16E-05,3.21E-06,0.000983724,0.032529149,0.962733687,1.43E-05,6.99E-06,5.61E-07
4283,relation-classification5,48,We create a sequence of vectors x 1:n in which each x i is computed as :,Our proposed model,Our proposed model,relation-classification,5,13,1,0,,3.05E-05,0,negative,1.85E-06,0.000768886,2.52E-06,9.26E-08,2.73E-06,3.03E-05,1.09E-06,0.000333406,0.00386343,0.994984467,7.43E-06,3.71E-06,8.67E-08
4284,relation-classification5,49,"As for NER , the RC component also uses a BiLSTM ( BiLSTM RC ) to learn another set of latent feature vectors , but from the sequence x 1:n :",Our proposed model,Our proposed model,relation-classification,5,14,1,0,,0.001791332,0,negative,8.46E-05,0.016892294,0.001962111,5.47E-07,3.05E-05,6.35E-05,2.13E-05,0.000362513,0.21354969,0.766850161,0.00010571,7.50E-05,2.01E-06
4285,relation-classification5,50,The RC component further uses these latent vectors r i for relation classification .,Our proposed model,Our proposed model,relation-classification,5,15,1,0,,0.000883299,0,negative,2.88E-05,0.011907378,0.000397941,4.35E-07,1.53E-05,5.85E-05,7.81E-06,0.000488059,0.331838651,0.655202174,3.97E-05,1.40E-05,1.32E-06
4286,relation-classification5,51,We propose a novel use of the deep biaffine attention mechanism for relation classification .,Our proposed model,Our proposed model,relation-classification,5,16,1,0,,0.020389084,0,approach,0.000381707,0.446575616,0.001269937,3.11E-05,0.000925884,0.000174821,0.000125577,0.001597308,0.224292494,0.314700524,0.008756671,0.00112617,4.22E-05
4287,relation-classification5,52,"The biaffine attention mechanism was proposed for dependency parsing , helping to produce the best reported parsing performance to date .",Our proposed model,Our proposed model,relation-classification,5,17,1,0,,0.011938153,0,negative,1.96E-05,0.004269413,6.16E-05,5.24E-06,2.72E-05,0.000178525,5.09E-05,0.000992818,0.021888525,0.965163057,0.007264669,6.86E-05,1.00E-05
4288,relation-classification5,53,"First , to encode the directionality of a relation , we use two single - layer feed - forward networks to project each r i into head and tail vector representations which correspond to whether the i th word serves as the head or tail argument of the relation :",Our proposed model,Our proposed model,relation-classification,5,18,1,0,,0.000236235,0,negative,6.34E-05,0.030447702,0.000860019,1.00E-06,5.31E-05,7.95E-05,1.49E-05,0.000647249,0.357445853,0.610306522,4.49E-05,3.37E-05,2.06E-06
4289,relation-classification5,54,"Following , our RC component incrementally constructs relation candidates using all possible combinations of the last word tokens of predicted entities , i.e. words with L or U labels .",Our proposed model,Our proposed model,relation-classification,5,19,1,0,,0.028178958,0,model,6.34E-05,0.053643023,0.000847517,6.35E-07,7.72E-05,2.70E-05,1.16E-05,0.00019863,0.614067793,0.330927813,7.34E-05,5.96E-05,2.41E-06
4290,relation-classification5,55,We assign an entity pair to a negative relation class ( NEG ) when the pair has no relation or when the predicted entities are not correct .,Our proposed model,Our proposed model,relation-classification,5,20,1,0,,0.000421385,0,negative,1.89E-05,0.022389052,0.000338833,3.06E-07,5.30E-05,6.60E-05,1.00E-05,0.000842334,0.053994562,0.922228065,1.67E-05,4.17E-05,6.05E-07
4291,relation-classification5,56,"For example , for , we would have two relation candidates : NEG ( Paris , International ) and OrgBased In ( International , Paris ) .",Our proposed model,Our proposed model,relation-classification,5,21,1,0,,3.47E-05,0,negative,1.76E-06,0.000213739,3.72E-06,3.78E-07,1.52E-05,5.90E-05,3.43E-06,0.00024358,0.00140539,0.998035548,5.26E-06,1.26E-05,3.37E-07
4292,relation-classification5,57,"Then for each head - tail candidate pair ( w j , wk ) , we apply the biaffine attention operator :",Our proposed model,Our proposed model,relation-classification,5,22,1,0,,6.69E-05,0,negative,9.95E-05,0.004154155,0.000399327,3.47E-07,2.87E-05,3.48E-05,8.31E-06,0.000234244,0.019594136,0.975354892,7.32E-06,8.38E-05,5.25E-07
4293,relation-classification5,58,3 Experiments,Our proposed model,Our proposed model,relation-classification,5,23,1,0,,0.001842704,0,negative,1.57E-05,0.000230926,2.46E-06,1.83E-07,1.52E-05,4.12E-05,1.91E-05,0.000245542,0.000107615,0.998831546,6.15E-06,0.000483961,3.59E-07
4294,relation-classification5,59,Experimental setup,,,relation-classification,5,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
4295,relation-classification5,60,Evaluation scenarios :,Experimental setup,Experimental setup,relation-classification,5,1,1,0,,0.48434383,0,negative,9.92E-05,6.87E-06,0.020635793,2.58E-07,3.86E-07,0.002062418,0.001486447,0.005933192,2.39E-06,0.95888589,4.90E-05,0.010832246,5.96E-06
4296,relation-classification5,61,We evaluate our joint model on two evaluation setup scenarios :,Experimental setup,Experimental setup,relation-classification,5,2,1,0,,0.097398431,0,negative,0.000234574,5.61E-05,0.030801372,1.03E-06,1.82E-06,0.004241915,0.001645049,0.011986889,9.08E-06,0.945416471,2.34E-05,0.00557457,7.69E-06
4297,relation-classification5,62,( 1 ) NER&RC :,Experimental setup,Experimental setup,relation-classification,5,3,1,0,,0.926968105,1,baselines,3.20E-05,9.29E-06,0.97663999,1.13E-07,1.12E-07,0.000274837,0.000646151,0.000662494,3.90E-06,0.020375414,5.11E-05,0.001298119,6.48E-06
4298,relation-classification5,63,A realistic scenario where entity boundaries are not given .,Experimental setup,Experimental setup,relation-classification,5,4,1,0,,0.009371918,0,negative,4.11E-05,9.99E-06,0.003077563,1.88E-06,1.10E-06,0.002735704,0.000521027,0.009262805,1.20E-05,0.982990047,0.000186309,0.001143939,1.65E-05
4299,relation-classification5,64,( 2 ) EC&RC : A less realistic scenario where the entity boundaries are given ] .,Experimental setup,Experimental setup,relation-classification,5,5,1,0,,0.879343043,1,baselines,0.000144783,2.09E-05,0.734184314,8.44E-07,1.02E-06,0.001134297,0.001027997,0.002115783,1.10E-05,0.258262378,9.87E-05,0.002980612,1.75E-05
4300,relation-classification5,65,Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .,Experimental setup,Experimental setup,relation-classification,5,6,1,0,,0.732196752,1,negative,0.000158127,6.42E-05,0.004202724,0.000108989,9.01E-06,0.010866141,0.024782329,0.022605022,2.36E-05,0.767155112,0.162385361,0.00670376,0.000935613
4301,relation-classification5,66,"Following , we encode the gold entity boundaries in the BILOU scheme .",Experimental setup,Experimental setup,relation-classification,5,7,1,0,,0.909908188,1,baselines,0.000715908,0.000338102,0.616065788,1.54E-06,3.14E-06,0.002585267,0.000664701,0.014453019,0.000764963,0.362688739,2.29E-05,0.001665713,3.02E-05
4302,relation-classification5,67,"Then we represent each B , I , O , L or U boundary tag as a vector embedding .",Experimental setup,Experimental setup,relation-classification,5,8,1,0,,0.187800808,0,negative,0.000154524,9.33E-05,0.036698564,3.39E-06,2.03E-06,0.015171693,0.000582416,0.070597766,0.000872721,0.875606553,1.58E-05,0.000170244,3.10E-05
4303,relation-classification5,68,"As a result , the vector vi in Equation 1 now also includes the boundary tag embedding in addition to the word embedding and character - level word embedding .",Experimental setup,Experimental setup,relation-classification,5,9,1,0,,0.390469893,0,negative,0.000539893,0.000130735,0.016657978,3.24E-06,2.86E-06,0.01449325,0.000752461,0.087008595,0.000616157,0.879002312,1.69E-05,0.000743153,3.25E-05
4304,relation-classification5,69,"Dataset : We use the benchmark "" entity and relation recognition "" dataset CoNLL04 from .",Experimental setup,Experimental setup,relation-classification,5,10,1,0,,0.207104574,0,negative,0.000462785,0.000105206,0.203025219,5.71E-05,0.000150002,0.008319489,0.132982441,0.008610543,1.28E-05,0.502182767,0.000966896,0.141929137,0.00119558
4305,relation-classification5,70,"Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .",Experimental setup,Experimental setup,relation-classification,5,11,1,0,,0.438384997,0,negative,0.000139355,7.22E-05,0.012314735,8.63E-06,1.92E-05,0.04099686,0.005118134,0.102898914,3.01E-05,0.837516996,3.80E-06,0.000848487,3.26E-05
4306,relation-classification5,71,Implementation :,Experimental setup,Experimental setup,relation-classification,5,12,1,0,,0.141013341,0,negative,0.000123849,0.00028124,0.00331453,0.000128303,2.67E-05,0.063570677,0.0051723,0.188493317,0.000185826,0.737082629,0.000160653,0.001279132,0.000180886
4307,relation-classification5,72,Our model is implemented using DYNET v 2.0 .,Experimental setup,,relation-classification,5,13,1,1,experimental-setup,0.987537844,1,experimental-setup,5.14E-05,2.49E-05,0.000794957,0.000565562,5.39E-06,0.670348243,0.01296977,0.295144919,3.98E-05,0.019783285,1.24E-05,7.04E-05,0.000189069
4308,relation-classification5,73,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .",Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,14,1,1,experimental-setup,0.982398326,1,experimental-setup,5.26E-07,1.09E-06,3.39E-07,2.96E-07,6.45E-07,0.993869089,0.000830026,0.004417216,5.90E-07,0.000878189,7.69E-08,1.16E-07,1.80E-06
4309,relation-classification5,74,We compute the average of NER / EC score and RC score after each training epoch .,Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,15,1,0,,0.111888871,0,experimental-setup,4.47E-05,8.05E-05,0.00011628,5.46E-07,5.10E-05,0.888765668,0.008805442,0.003668004,4.61E-05,0.09840094,2.43E-06,1.19E-05,6.45E-06
4310,relation-classification5,75,"We choose the model with the highest average score on the development set , which is then applied to the test set for the final evaluation phase .",Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,16,1,0,,0.278229664,0,experimental-setup,1.51E-05,4.66E-05,3.57E-05,5.56E-07,2.35E-05,0.942598103,0.0032018,0.004672545,3.46E-05,0.049364802,7.15E-07,2.52E-06,3.38E-06
4311,relation-classification5,76,More details of the implementation as well as optimal hyper - parameters are in the Appendix .,Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,17,1,0,,0.009720126,0,experimental-setup,2.98E-06,2.97E-06,1.92E-06,1.75E-05,7.14E-05,0.822852606,0.000595237,0.000534392,1.81E-06,0.175914591,1.39E-06,1.05E-06,2.17E-06
4312,relation-classification5,77,Our code is available at : https : //github.com/datquocnguyen/jointRE,Experimental setup,,relation-classification,5,18,1,1,code,0.988750033,1,code,3.47E-05,9.89E-07,2.96E-05,0.979513343,2.31E-05,0.018085109,0.000180544,0.000395222,2.73E-06,0.001639837,1.01E-06,2.17E-06,9.16E-05
4313,relation-classification5,78,"Metric : Similar to previous works in , we use the macro -averaged F1 - score over the entity classes to score NER / EC and over the relation classes to score RC .",Experimental setup,Our code is available at : https : //github.com/datquocnguyen/jointRE,relation-classification,5,19,1,0,,0.057826345,0,experimental-setup,3.00E-05,0.000343854,0.000360223,1.49E-05,0.001972793,0.796105082,0.010409087,0.000879337,0.000144036,0.18958927,0.000114449,8.75E-06,2.83E-05
4314,relation-classification5,79,More details of the metric are also in the Appendix .,Experimental setup,Our code is available at : https : //github.com/datquocnguyen/jointRE,relation-classification,5,20,1,0,,0.000527078,0,experimental-setup,3.59E-06,9.12E-06,4.52E-06,2.46E-05,0.000398418,0.720295156,0.000730172,0.000321829,6.30E-06,0.278201365,2.61E-06,4.26E-07,1.93E-06
4315,relation-classification5,80,"Unlike previous neural models , we report results as mean and standard deviation of the scores over 10 runs with 10 random seeds .",Experimental setup,Our code is available at : https : //github.com/datquocnguyen/jointRE,relation-classification,5,21,1,0,,0.016348747,0,experimental-setup,6.15E-06,4.43E-05,9.53E-06,2.23E-06,0.000466098,0.763445803,0.005580715,0.000872929,9.47E-06,0.229550784,5.71E-06,2.75E-06,3.56E-06
4316,relation-classification5,81,Main results,,,relation-classification,5,0,1,0,,0.204992025,0,negative,3.39E-05,5.69E-05,4.65E-06,1.50E-07,2.86E-07,2.47E-05,2.53E-05,0.000366261,2.49E-05,0.998372142,0.000758001,0.000332078,7.58E-07
4317,relation-classification5,82,End - to - end results :,Main results,Main results,relation-classification,5,1,1,0,,0.879858816,1,negative,0.005011739,1.82E-05,0.001952776,5.38E-07,1.00E-06,2.33E-05,0.000392584,0.000129839,5.52E-06,0.514564513,9.41E-05,0.477787277,1.86E-05
4318,relation-classification5,83,The first six rows in compare our results with previous state - of - the - art published results on the same test set .,Main results,Main results,relation-classification,5,2,1,0,,0.010215648,0,negative,0.000600687,5.46E-06,0.000148494,2.39E-06,3.64E-06,6.49E-05,0.000113869,0.000177289,4.39E-06,0.981845387,1.23E-05,0.017006916,1.43E-05
4319,relation-classification5,84,"In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .",Main results,Main results,relation-classification,5,3,1,0,,0.985575882,1,results,0.002238054,9.08E-07,8.78E-05,5.00E-07,4.06E-07,5.61E-06,0.000707257,3.29E-05,1.80E-07,0.016261283,1.07E-05,0.980609566,4.49E-05
4320,relation-classification5,85,We also obtain 7 + % higher EC and RC scores ( Setup 2 ) than Adel and Schtze ( 2017 ) .,Main results,Main results,relation-classification,5,4,1,0,,0.902122643,1,results,0.01183089,2.49E-06,0.000235712,1.60E-06,1.86E-06,1.58E-05,0.000908846,5.94E-05,8.85E-07,0.051558022,1.02E-05,0.93532184,5.24E-05
4321,relation-classification5,86,"Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .",Main results,Main results,relation-classification,5,5,1,0,,0.000929766,0,negative,0.003076546,6.52E-06,0.001228756,1.43E-06,3.24E-06,5.25E-05,0.000386273,0.000152146,2.71E-06,0.882795225,2.59E-05,0.112244345,2.45E-05
4322,relation-classification5,87,"These results show that our model performs better than previous state - of - the - art models , using the same setup .",Main results,Main results,relation-classification,5,6,1,0,,0.916777573,1,results,0.001568283,1.85E-06,4.09E-05,9.69E-07,7.37E-07,4.40E-05,0.000749711,0.000203174,5.21E-07,0.169009444,1.96E-05,0.828320128,4.07E-05
4323,relation-classification5,88,"In , the last two rows present results reported in and on the dataset CoNLL04 .",Main results,Main results,relation-classification,5,7,1,0,,0.150785377,0,negative,0.001233303,4.69E-06,0.000606749,1.52E-06,4.59E-06,3.26E-05,0.000389363,8.80E-05,1.83E-06,0.846234627,2.85E-05,0.151344185,3.01E-05
4324,relation-classification5,89,"However , these results are not comparable due to their random sampling of the test set , i.e. using different train - test splits .",Main results,Main results,relation-classification,5,8,1,0,,0.00067848,0,negative,0.000209438,3.36E-06,0.000114873,1.01E-06,8.60E-07,3.78E-05,9.92E-05,0.000172173,2.42E-06,0.980370849,3.70E-05,0.018937012,1.41E-05
4325,relation-classification5,90,Both and employ additional extra features based on external NLP tools and use larger training sets than ours .,Main results,Main results,relation-classification,5,9,1,0,,0.001270937,0,negative,0.001655814,1.91E-05,0.000581835,3.86E-05,2.55E-05,0.000285047,0.000199545,0.000452284,2.11E-05,0.991463152,3.02E-05,0.005145553,8.23E-05
4326,relation-classification5,91,"Specifically , integrate syntactic features by using a pre-trained BiLSTM - based dependency parser to extract BiLSTM - based latent feature representations for words in the input sentence , and then using these latent representations directly as part of the input embeddings in their model .",Main results,Main results,relation-classification,5,10,1,0,,0.000392087,0,negative,0.002614712,3.04E-05,0.006061195,5.38E-06,4.71E-06,0.000103882,7.43E-05,0.000270148,5.81E-05,0.987134886,4.68E-05,0.003552213,4.33E-05
4327,relation-classification5,92,We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .,Main results,Main results,relation-classification,5,11,1,0,,0.018344447,0,negative,0.002275343,4.32E-05,0.000540965,2.04E-05,9.57E-06,0.000484892,0.000349853,0.001760729,8.90E-05,0.976129454,7.64E-05,0.017994543,0.000225646
4328,relation-classification5,93,Ablation analysis :,Main results,Main results,relation-classification,5,12,1,0,,0.015991728,0,negative,0.173451107,6.04E-05,0.061525483,6.57E-05,3.85E-05,0.000235085,0.000942144,0.000259292,0.00013023,0.61529409,0.000119685,0.147494573,0.000383697
4329,relation-classification5,94,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .",Main results,Main results,relation-classification,5,13,1,1,ablation-analysis,0.468335873,0,negative,0.019364671,0.000133627,0.013776965,5.73E-06,1.64E-05,6.92E-05,0.000162528,0.000227544,0.00016855,0.885332737,3.12E-05,0.080626433,8.44E-05
4330,relation-classification5,95,"Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .",Main results,Main results,relation-classification,5,14,1,0,,0.001597088,0,negative,0.004105091,0.000173209,0.092910586,2.14E-06,6.58E-06,5.48E-05,6.95E-05,0.000316068,0.000778054,0.896514556,1.32E-05,0.00501196,4.43E-05
4331,relation-classification5,96,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .",Main results,Main results,relation-classification,5,15,1,1,ablation-analysis,0.916263491,1,results,0.006106854,3.09E-06,0.000101003,2.94E-06,1.82E-06,6.68E-05,0.001579574,0.000293883,1.06E-06,0.067060234,1.46E-05,0.924638157,0.000130036
4332,relation-classification5,97,"Ablation results on the development set . * and ** denote the statistically significant differences against the full results at p < 0.05 and p < 0.01 , respectively ( using the two - tailed paired t- test ) .",Main results,Main results,relation-classification,5,16,1,0,,0.003408212,0,negative,0.001403342,8.52E-06,0.000611317,3.79E-06,4.46E-06,0.000144995,0.000286844,0.000579604,1.77E-05,0.969798656,1.33E-05,0.027030266,9.72E-05
4333,relation-classification5,98,differences are not significant .,Main results,Main results,relation-classification,5,17,1,0,,0.003576349,0,negative,0.00105859,3.71E-06,0.000102099,3.12E-06,2.48E-06,0.000100282,0.000248224,0.000425637,6.78E-06,0.964111895,7.02E-06,0.033873337,5.68E-05
4334,relation-classification5,99,A similar observation is also found in .,Main results,,relation-classification,5,18,1,0,,0.000295339,0,negative,0.00082153,1.71E-06,9.56E-05,1.10E-06,1.38E-06,2.66E-05,2.63E-05,8.33E-05,3.51E-06,0.994714532,1.86E-06,0.004215831,6.64E-06
4335,relation-classification5,100,"Also , in preliminary experiments , we do not find any significant difference in performance of our joint model when feeding gold NER labels instead of predicted NER labels into the RC component during training .",Main results,A similar observation is also found in .,relation-classification,5,19,1,0,,0.000497681,0,results,0.01605848,2.89E-06,6.80E-05,7.85E-07,1.76E-06,1.27E-05,0.000509949,3.57E-05,2.34E-06,0.45222428,4.27E-06,0.530934027,0.000144847
4336,relation-classification5,101,This is not surprising as the training NER score is at 99 +% .,Main results,A similar observation is also found in .,relation-classification,5,20,1,0,,0.000406133,0,negative,0.002518812,1.43E-06,4.72E-05,1.27E-06,3.80E-06,2.45E-05,0.000104066,3.77E-05,3.51E-06,0.960642169,2.74E-06,0.036374818,0.000238032
4337,relation-classification5,102,also presents ablation tests over 5 factors of our joint model on the development set .,Main results,A similar observation is also found in .,relation-classification,5,21,1,0,,0.000336162,0,negative,0.002343522,5.01E-06,0.001290551,1.63E-06,7.65E-06,1.75E-05,4.23E-05,2.67E-05,2.04E-05,0.987306868,2.26E-06,0.008826764,0.000108798
4338,relation-classification5,103,"In particular , Setup 1 performances significantly degrade by 4 + % absolutely , when not using the character - level word embeddings .",Main results,A similar observation is also found in .,relation-classification,5,22,1,0,,0.427242429,0,ablation-analysis,0.424956881,9.69E-06,0.000147363,1.76E-05,1.08E-05,4.51E-05,0.001332105,9.85E-05,1.11E-05,0.200076524,6.59E-06,0.371576302,0.001711423
4339,relation-classification5,104,"The performances also decrease when using a softmax classifier for NER label prediction rather than a CRF layer ( here , the decrease is significant ) .",Main results,A similar observation is also found in .,relation-classification,5,23,1,0,,0.584540758,1,negative,0.117681849,5.79E-06,0.000148182,5.59E-06,6.62E-06,3.07E-05,0.000653816,7.19E-05,7.94E-06,0.514236163,6.50E-06,0.366556808,0.00058813
4340,relation-classification5,105,"In contrast , we do not find any significant difference in Setup 2 scores when not using either the character - level embeddings or the CRF layer , clearly showing the usefulness of the given gold entity boundaries .",Main results,A similar observation is also found in .,relation-classification,5,24,1,0,,0.343847559,0,results,0.042453382,4.61E-06,6.94E-05,2.34E-06,3.85E-06,2.00E-05,0.001261681,6.10E-05,2.92E-06,0.201018679,4.32E-06,0.754682893,0.000414956
4341,relation-classification5,106,"The 3 remaining factors , including removing NER label embeddings and not taking either the Bilinear or Linear part ( in Equation 8 ) into the Biaffine attention layer , do not affect the NER / EC score .",Main results,A similar observation is also found in .,relation-classification,5,25,1,0,,0.006317892,0,ablation-analysis,0.693321884,1.82E-05,0.000226378,0.000145593,4.68E-05,0.000167339,0.001067619,0.000215866,4.04E-05,0.21275972,4.50E-06,0.089775111,0.002210647
4342,relation-classification5,107,"However , they significantly decrease the RC score .",Main results,,relation-classification,5,26,1,0,,0.003165058,0,negative,0.091426659,7.13E-06,0.000374203,1.34E-06,4.67E-06,1.24E-05,9.88E-05,7.98E-05,7.59E-06,0.738522297,1.80E-06,0.169439465,2.39E-05
4343,relation-classification5,108,"This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .",Main results,"However , they significantly decrease the RC score .",relation-classification,5,27,1,0,,0.001322744,0,negative,0.014215977,1.12E-05,0.000167476,4.24E-06,1.85E-05,5.58E-05,0.000235498,8.81E-05,5.58E-05,0.95982774,2.55E-06,0.024897502,0.000419646
4344,relation-classification5,109,"More specifically , using the Biaffine attention produces about 1.5 % significant improvements to a common Linear transformation mechanism in relation classification , i.e. , "" w / o Bilinear "" results against the full results in : 65.4 % vs. 66.9 % and 72.0 % vs. 73.3 % ( although using Biaffine increases training time over using Linear by 35 % , relatively ) .",Main results,"However , they significantly decrease the RC score .",relation-classification,5,28,1,0,,0.766576083,1,results,0.118799876,1.58E-05,0.00019815,1.56E-05,2.76E-05,6.90E-05,0.006208284,9.27E-05,1.55E-05,0.117180677,4.57E-06,0.748200914,0.00917134
4345,relation-classification5,110,Conclusion,,,relation-classification,5,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
4346,sentiment_analysis25,1,title,,,sentiment_analysis,25,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
4347,sentiment_analysis25,2,Aspect Based Sentiment Analysis with Gated Convolutional Networks,title,,sentiment_analysis,25,1,1,1,research-problem,0.998270466,1,research-problem,6.03E-08,8.15E-06,9.10E-08,3.69E-07,1.57E-07,2.40E-07,2.98E-06,2.13E-06,6.60E-07,0.002089481,0.997895182,3.26E-07,1.73E-07
4348,sentiment_analysis25,3,abstract,,,sentiment_analysis,25,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
4349,sentiment_analysis25,4,"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .",abstract,abstract,sentiment_analysis,25,1,1,1,research-problem,0.878298497,1,research-problem,3.49E-08,4.72E-06,2.40E-08,1.15E-06,3.70E-07,1.88E-07,6.63E-07,7.01E-07,2.07E-07,0.004409179,0.995582661,3.19E-08,7.65E-08
4350,sentiment_analysis25,5,We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .,abstract,abstract,sentiment_analysis,25,2,1,1,research-problem,0.149660415,0,research-problem,4.62E-07,0.000410926,7.64E-07,2.88E-05,3.79E-05,2.16E-06,4.52E-06,7.79E-06,5.22E-06,0.032336445,0.967164185,2.97E-07,6.15E-07
4351,sentiment_analysis25,6,"Most previous approaches employ long short - term memory and attention mechanisms to predict the sentiment polarity of the concerned targets , which are often complicated and need more training time .",abstract,abstract,sentiment_analysis,25,3,1,0,,0.022195337,0,research-problem,2.13E-07,0.000124176,1.39E-07,6.47E-06,3.35E-06,5.01E-06,1.45E-06,2.72E-05,8.81E-06,0.107061732,0.89276118,9.07E-08,1.89E-07
4352,sentiment_analysis25,7,"We propose a model based on convolutional neural networks and gating mechanisms , which is more accurate and efficient .",abstract,abstract,sentiment_analysis,25,4,1,0,,0.121115874,0,negative,6.89E-05,0.139837746,0.000417688,3.84E-05,0.000344325,8.04E-05,2.13E-05,0.000606048,0.092440571,0.462578907,0.303549193,1.05E-05,6.02E-06
4353,sentiment_analysis25,8,"First , the novel Gated Tanh - ReLU Units can selectively output the sentiment features according to the given aspect or entity .",abstract,abstract,sentiment_analysis,25,5,1,0,,0.20424913,0,negative,0.000769879,0.073060628,0.000143648,0.000191288,0.001601377,6.20E-05,3.26E-05,0.000446734,0.006068621,0.868156307,0.049389666,7.17E-05,5.60E-06
4354,sentiment_analysis25,9,The architecture is much simpler than attention layer used in the existing models .,abstract,abstract,sentiment_analysis,25,6,1,0,,0.071913921,0,negative,2.98E-05,0.061921411,5.38E-05,0.000255416,0.000798194,0.00022057,2.24E-05,0.001018706,0.02537583,0.773158018,0.137136067,3.43E-06,6.32E-06
4355,sentiment_analysis25,10,"Second , the computations of our model could be easily parallelized during training , because convolutional layers do not have time dependency as in LSTM layers , and gating units also work independently .",abstract,abstract,sentiment_analysis,25,7,1,0,,0.074342697,0,negative,1.45E-05,0.015891025,8.08E-06,0.000119507,0.000327497,9.60E-05,1.32E-05,0.000377042,0.001854805,0.86542012,0.115871349,4.60E-06,2.21E-06
4356,sentiment_analysis25,11,The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models .,abstract,abstract,sentiment_analysis,25,8,1,0,,0.002871104,0,negative,9.75E-05,0.001405171,1.76E-06,4.00E-06,6.80E-05,1.31E-05,3.32E-05,0.000176323,3.05E-05,0.967535091,0.030406366,0.0002282,8.21E-07
4357,sentiment_analysis25,12,1,abstract,abstract,sentiment_analysis,25,9,1,0,,3.75E-05,0,negative,5.93E-07,0.000263911,1.26E-07,5.14E-06,7.60E-06,1.43E-05,4.93E-07,8.10E-05,0.000236848,0.993266386,0.006123423,9.99E-08,6.90E-08
4358,sentiment_analysis25,13,Introduction,,,sentiment_analysis,25,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
4359,sentiment_analysis25,14,Opinion mining and sentiment analysis on user - generated reviews can provide valuable information for providers and consumers .,Introduction,Introduction,sentiment_analysis,25,1,1,0,,0.627962914,1,research-problem,1.03E-06,0.000199058,3.30E-07,9.87E-06,3.01E-05,5.04E-06,1.15E-05,6.14E-06,4.44E-05,0.041016494,0.958672809,1.22E-06,2.10E-06
4360,sentiment_analysis25,15,"Instead of predicting the over all sen-timent polarity , fine - grained aspect based sentiment analysis ( ABSA ) ) is proposed to better understand reviews than traditional sentiment analysis .",Introduction,Introduction,sentiment_analysis,25,2,1,0,,0.227766144,0,research-problem,1.62E-06,0.000462912,9.88E-07,4.04E-06,1.68E-05,4.14E-06,1.52E-05,7.25E-06,0.000107144,0.025132196,0.974243444,2.04E-06,2.14E-06
4361,sentiment_analysis25,16,"Specifically , we are interested in the sentiment polarity of aspect categories or target entities in the text .",Introduction,Introduction,sentiment_analysis,25,3,1,0,,0.109819661,0,negative,1.68E-05,0.144018035,1.52E-05,1.52E-05,0.00039714,7.78E-05,3.40E-05,0.000277039,0.111952462,0.38296513,0.360213213,1.27E-05,5.34E-06
4362,sentiment_analysis25,17,"Sometimes , it is coupled with aspect term extractions .",Introduction,Introduction,sentiment_analysis,25,4,1,0,,0.004017433,0,negative,7.32E-06,0.001001089,1.83E-06,5.24E-05,0.000116491,9.22E-05,2.01E-05,6.61E-05,0.000603369,0.733896138,0.264136976,2.91E-06,2.99E-06
4363,sentiment_analysis25,18,"A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .",Introduction,Introduction,sentiment_analysis,25,5,1,1,research-problem,0.815283076,1,research-problem,1.41E-06,0.000383919,8.01E-07,6.59E-06,3.23E-05,5.92E-06,1.78E-05,7.76E-06,5.82E-05,0.063084787,0.9363965,1.99E-06,1.99E-06
4364,sentiment_analysis25,19,"The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories .",Introduction,Introduction,sentiment_analysis,25,6,1,1,research-problem,0.899666423,1,research-problem,9.83E-07,0.000529194,5.49E-07,2.02E-05,4.14E-05,1.30E-05,1.52E-05,1.78E-05,0.000137636,0.066780013,0.9324393,1.23E-06,3.43E-06
4365,sentiment_analysis25,20,"On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word .",Introduction,Introduction,sentiment_analysis,25,7,1,1,research-problem,0.838309794,1,research-problem,2.08E-06,0.001457631,1.06E-06,2.56E-05,0.000107879,1.81E-05,1.86E-05,2.67E-05,0.000254325,0.125452307,0.872629332,2.31E-06,4.14E-06
4366,sentiment_analysis25,21,The number of distinct words contributing to aspect terms could be more than a thousand .,Introduction,Introduction,sentiment_analysis,25,8,1,0,,0.359256247,0,negative,2.02E-05,0.009376297,4.12E-06,4.50E-05,0.000357225,0.000695286,9.78E-05,0.001067505,0.014768515,0.923331713,0.050216175,9.47E-06,1.07E-05
4367,sentiment_analysis25,22,"For example , in the sentence "" Average to good Thai food , but terrible delivery . "" , ATSA would ask the sentiment polarity towards the entity Thai food ; while ACSA would ask the sentiment polarity toward the aspect service , even though the word service does not appear in the sentence .",Introduction,Introduction,sentiment_analysis,25,9,1,0,,0.002726357,0,negative,5.56E-06,0.00141853,2.16E-06,5.23E-05,0.002356194,0.000171234,2.38E-05,8.09E-05,0.001026685,0.987413241,0.007443705,3.10E-06,2.55E-06
4368,sentiment_analysis25,23,"Many existing models use LSTM layers to distill sentiment information from embedding vectors , and apply attention mechanisms to enforce models to focus on the text spans related to the given aspect / entity .",Introduction,Introduction,sentiment_analysis,25,10,1,0,,0.29395903,0,negative,3.84E-05,0.026208239,3.17E-05,0.000103525,0.000512578,0.000446931,0.000129225,0.000608065,0.017874514,0.745755665,0.208252907,1.88E-05,1.95E-05
4369,sentiment_analysis25,24,"Such models include Attention - based LSTM with Aspect Embedding ( ATAE - LSTM ) for ACSA ; Target - Dependent Sentiment Classification ( TD - LSTM ) , Gated Neural Networks and Recurrent Attention Memory Network ( RAM ) for ATSA .",Introduction,Introduction,sentiment_analysis,25,11,1,0,,0.860165514,1,research-problem,9.13E-06,0.005959717,3.37E-05,2.16E-05,0.000121476,0.000104525,0.000167916,0.000131347,0.005264375,0.293990575,0.694161938,1.70E-05,1.67E-05
4370,sentiment_analysis25,25,Attention mechanisms has been successfully used in many NLP tasks .,Introduction,Introduction,sentiment_analysis,25,12,1,0,,0.598306354,1,research-problem,3.80E-06,0.002648656,2.11E-06,3.08E-06,1.46E-05,3.68E-05,6.12E-05,9.54E-05,0.00595079,0.237724955,0.753447437,6.95E-06,4.30E-06
4371,sentiment_analysis25,26,It first computes the alignment scores between context vectors and target vector ; then carry out a weighted sum with the scores and the context vectors .,Introduction,Introduction,sentiment_analysis,25,13,1,0,,0.925330811,1,model,9.08E-06,0.025457772,0.000129376,2.10E-07,4.87E-05,7.21E-06,6.38E-06,8.86E-06,0.965766244,0.008250912,0.000313116,1.60E-06,5.60E-07
4372,sentiment_analysis25,27,"However , the context vectors have to encode both the aspect and sentiment information , and the alignment scores are applied across all feature dimensions regardless of the differences between these two types of information .",Introduction,Introduction,sentiment_analysis,25,14,1,0,,0.785147417,1,model,5.22E-05,0.130806465,2.45E-05,1.78E-06,0.000147624,3.81E-05,1.32E-05,0.000141822,0.69707633,0.169158458,0.002530087,8.37E-06,1.07E-06
4373,sentiment_analysis25,28,Both LSTM and attention layer are very timeconsuming during training .,Introduction,Introduction,sentiment_analysis,25,15,1,0,,0.063096572,0,negative,0.0002513,0.018747171,2.52E-05,0.000858069,0.004479972,0.003025462,0.000850836,0.002985915,0.008427673,0.930242058,0.02993365,6.56E-05,0.000107039
4374,sentiment_analysis25,29,LSTM processes one token in a step .,Introduction,Introduction,sentiment_analysis,25,16,1,0,,0.907025074,1,model,4.21E-05,0.037372507,0.000741931,4.25E-06,0.000301178,9.33E-05,8.14E-05,7.46E-05,0.924938334,0.035047772,0.001285154,9.80E-06,7.63E-06
4375,sentiment_analysis25,30,Attention layer involves exponential operation and normalization of all alignment scores of all the words in the sentence .,Introduction,Introduction,sentiment_analysis,25,17,1,0,,0.933630923,1,model,2.30E-05,0.03613223,6.43E-05,9.35E-06,0.000188442,0.000255533,7.56E-05,0.000550505,0.926053287,0.035733199,0.000902,3.06E-06,9.61E-06
4376,sentiment_analysis25,31,"Moreover , some models needs the positional information between words and targets to produce weighted LSTM , which can be unreliable in noisy review text .",Introduction,Introduction,sentiment_analysis,25,18,1,0,,0.005271357,0,negative,2.95E-05,0.002567272,5.36E-06,3.79E-05,0.000427259,0.000127804,5.92E-05,0.000107292,0.000754815,0.9522954,0.043568896,1.43E-05,5.02E-06
4377,sentiment_analysis25,32,"Certainly , it is possible to achieve higher accuracy by building more and more complicated LSTM cells and sophisticated attention mechanisms ; but one has to hold more parameters in memory , get more hyper - parameters to tune and spend more time in training .",Introduction,Introduction,sentiment_analysis,25,19,1,0,,0.006298221,0,negative,2.61E-05,0.003939186,3.30E-06,8.29E-05,0.0001831,0.000555346,0.000171085,0.000666156,0.004613839,0.930834056,0.058888224,2.21E-05,1.46E-05
4378,sentiment_analysis25,33,"In this paper , we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .",Introduction,Introduction,sentiment_analysis,25,20,1,1,model,0.966468758,1,model,7.91E-05,0.436481171,0.000256654,6.85E-06,0.000691292,5.52E-05,0.000212448,0.000136646,0.504268869,0.036911208,0.020830926,5.61E-05,1.36E-05
4379,sentiment_analysis25,34,"For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .",Introduction,Introduction,sentiment_analysis,25,21,1,1,model,0.949521135,1,model,5.37E-05,0.304366734,0.00027001,1.03E-05,0.001327559,9.27E-05,8.00E-05,0.000154488,0.682416869,0.010858616,0.000347525,1.04E-05,1.10E-05
4380,sentiment_analysis25,35,Convolutional layers with multiple filters can efficiently extract n-gram features at many granularities on each receptive field .,Introduction,Introduction,sentiment_analysis,25,22,1,0,,0.799360484,1,model,8.80E-05,0.126872075,0.000161305,1.99E-05,0.000397961,0.000227688,0.000265047,0.000389882,0.55022806,0.278019259,0.043224922,7.78E-05,2.81E-05
4381,sentiment_analysis25,36,"The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .",Introduction,Introduction,sentiment_analysis,25,23,1,1,model,0.942981261,1,model,2.69E-05,0.102264092,6.79E-05,1.66E-05,0.000778598,0.000180964,4.64E-05,0.000319134,0.862512944,0.03343015,0.000345777,3.24E-06,7.28E-06
4382,sentiment_analysis25,37,"With the given aspect information , they can selectively extract aspect - specific sentiment information for sentiment prediction .",Introduction,Introduction,sentiment_analysis,25,24,1,0,,0.331372684,0,model,0.000100326,0.240527919,0.000135257,3.24E-06,0.000811404,5.02E-05,6.85E-05,9.79E-05,0.536194232,0.219634455,0.002335778,3.72E-05,3.64E-06
4383,sentiment_analysis25,38,"For example , in the sentence "" Average to good Thai food , but terrible delivery . "" , when the aspect food is provided , the gating units automatically ignore the negative sentiment of aspect delivery from the second clause , and only output the positive sentiment from the first clause .",Introduction,Introduction,sentiment_analysis,25,25,1,0,,0.061836285,0,negative,2.59E-05,0.005905399,1.18E-05,2.42E-05,0.002659551,0.000194296,6.55E-05,0.000117492,0.006088745,0.983381567,0.001506261,1.52E-05,4.11E-06
4384,sentiment_analysis25,39,"Since each component of the proposed model could be easily parallelized , it has much less training time than the models based on LSTM and attention mechanisms .",Introduction,Introduction,sentiment_analysis,25,26,1,0,,0.592918946,1,negative,0.000670885,0.279526944,0.000159818,7.00E-05,0.004908338,0.000483231,0.00064513,0.000893215,0.179373143,0.528479556,0.004516348,0.000233672,3.98E-05
4385,sentiment_analysis25,40,"For ATSA task , where the aspect terms consist of multiple words , we extend our model to include another convolutional layer for the target expressions .",Introduction,Introduction,sentiment_analysis,25,27,1,1,model,0.841957201,1,approach,0.000150362,0.490604093,0.000326427,5.69E-06,0.001601209,8.33E-05,0.000194003,0.000137313,0.478279986,0.028241426,0.000334336,3.33E-05,8.55E-06
4386,sentiment_analysis25,41,"We evaluate our models on the SemEval datasets , which contains restaurants and laptops reviews with labels on aspect level .",Introduction,Introduction,sentiment_analysis,25,28,1,0,,0.089605589,0,approach,6.09E-05,0.630145875,9.59E-05,0.00010874,0.085064947,0.000787973,0.001219339,0.001424275,0.010578232,0.269987182,0.000338946,0.000149284,3.85E-05
4387,sentiment_analysis25,42,"To the best of our knowledge , no CNNbased model has been proposed for aspect based sentiment analysis so far .",Introduction,Introduction,sentiment_analysis,25,29,1,0,,0.046190159,0,negative,1.02E-05,0.018970082,1.60E-05,0.000105611,0.000600528,0.000654069,0.000522914,0.001034864,0.005995816,0.806326201,0.165682336,3.44E-05,4.68E-05
4388,sentiment_analysis25,43,Related Work,,,sentiment_analysis,25,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
4389,sentiment_analysis25,125,Datasets and Experiment Preparation,,,sentiment_analysis,25,0,1,0,,0.000862539,0,negative,2.27E-06,4.30E-05,1.76E-06,4.04E-07,5.04E-07,4.71E-05,1.08E-05,0.000720137,2.29E-05,0.998737894,0.000398487,1.43E-05,4.52E-07
4390,sentiment_analysis25,126,"We conduct experiments on public datasets from SemEval workshops , which consist of customer reviews about restaurants and laptops .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,1,1,0,,0.008102733,0,negative,0.000202303,0.002582066,0.019465209,7.02E-05,0.000316446,0.005636899,0.007510998,0.036268271,7.90E-05,0.926613319,0.000272695,0.000894948,8.76E-05
4391,sentiment_analysis25,127,"Some existing work ) removed "" conflict "" labels from four sentiment labels , which makes their results incomparable to those from the workshop report .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,2,1,0,,0.000491029,0,negative,3.96E-05,2.52E-05,0.003100303,1.48E-05,4.53E-06,0.002448391,0.002403848,0.011596158,9.55E-06,0.975435634,0.004630266,0.00022605,6.57E-05
4392,sentiment_analysis25,128,"We reimplemented the compared methods , and used hyper - parameter settings described in these references .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,3,1,0,,0.001352193,0,negative,4.84E-05,0.000165848,0.006007689,7.78E-05,2.19E-05,0.076402328,0.005801949,0.240542753,8.64E-05,0.670672591,5.77E-05,6.43E-05,5.04E-05
4393,sentiment_analysis25,129,The sentences which have different sentiment labels for different aspects or targets in the sentence are more common in review data than in standard sentiment classification benchmark .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,4,1,0,,0.00348139,0,negative,1.65E-05,2.91E-05,0.001676213,8.11E-06,5.10E-06,0.000922624,0.001732894,0.005977561,5.09E-06,0.982493995,0.006879212,0.000204221,4.94E-05
4394,sentiment_analysis25,130,The sentence in shows the reviewer 's different attitude towards two aspects : food and delivery .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,5,1,0,,0.000618388,0,negative,6.25E-05,1.22E-05,0.000690252,0.000424939,5.73E-05,0.003771068,0.00058183,0.003554937,2.14E-05,0.990693261,6.29E-05,2.84E-05,3.90E-05
4395,sentiment_analysis25,131,"Therefore , to access how the models perform on review sentences more accurately , we create small but difficult datasets , which are made up of the sentences having opposite or different sentiments on different aspects / targets .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,6,1,0,,0.003240729,0,negative,0.00015878,0.000468175,0.018853376,9.24E-06,0.000159485,0.000884523,0.001346673,0.00725477,3.78E-05,0.970249714,5.99E-05,0.000498526,1.90E-05
4396,sentiment_analysis25,132,"In , the two identical sentences but with different sentiment labels are both included in the dataset .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,7,1,0,,0.001660872,0,negative,4.71E-05,5.54E-05,0.00472974,7.86E-06,8.54E-05,0.001255108,0.000799607,0.00854238,1.16E-05,0.98436148,3.87E-06,9.24E-05,8.17E-06
4397,sentiment_analysis25,133,"If a sentence has 4 aspect targets , this sentence would have 4 copies in the data set , each of which is associated with different target and sentiment label .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,8,1,0,,0.000856701,0,negative,6.16E-05,0.000115122,0.012637562,2.48E-06,2.28E-05,0.002191462,0.00114819,0.021448781,5.84E-05,0.962084426,2.04E-05,0.000189764,1.91E-05
4398,sentiment_analysis25,134,"For ACSA task , we conduct experiments on restaurant review data of SemEval 2014 Task 4 .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,9,1,0,,0.088798173,0,negative,0.000672229,0.003871975,0.094270151,8.65E-05,0.00089542,0.004957394,0.037617419,0.041732173,0.000107792,0.802442777,0.000417323,0.01238924,0.000539635
4399,sentiment_analysis25,135,"There are 5 aspects : food , price , service , ambience , and misc ; 4 sentiment polarities : positive , negative , neutral , and conflict .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,10,1,0,,0.038663336,0,negative,0.00015606,0.000210477,0.030187457,8.15E-05,0.000218171,0.013297706,0.006076864,0.067146154,0.000120344,0.8818287,2.64E-05,0.000495741,0.00015441
4400,sentiment_analysis25,136,"By merging restaurant reviews of three years 2014 - 2016 , we obtain a larger dataset called "" Restaurant - Large "" .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,11,1,0,,0.083460567,0,negative,0.000931989,0.00168595,0.078825211,0.003008044,0.025061856,0.010974318,0.023086353,0.014134687,0.000179097,0.839005658,0.000194216,0.001979301,0.000933322
4401,sentiment_analysis25,137,Incompatibilities of data are fixed during merging .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,12,1,0,,0.033421377,0,negative,0.000614977,0.001009629,0.274011768,1.49E-05,4.31E-05,0.002626558,0.001489971,0.033565932,0.001275898,0.68454715,4.10E-05,0.000646464,0.000112669
4402,sentiment_analysis25,138,We replace conflict labels with neutral labels in the 2014 dataset .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,13,1,0,,0.010780885,0,negative,0.001194166,0.000197036,0.159651235,5.33E-06,5.16E-05,0.002421573,0.003643973,0.021335501,5.93E-05,0.809191362,5.45E-06,0.00219359,4.99E-05
4403,sentiment_analysis25,139,"In the 2015 and 2016 datasets , there could be multiple pairs of "" aspect terms "" and "" aspect category "" in one sentence .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,14,1,0,,0.002625451,0,negative,3.44E-05,2.31E-05,0.002815775,1.11E-05,9.86E-05,0.001821354,0.000959262,0.005949771,7.50E-06,0.988030653,8.43E-06,0.000221479,1.86E-05
4404,sentiment_analysis25,140,"For each sentence , let p denote the number of positive labels minus the number of negative labels .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,15,1,0,,7.25E-05,0,negative,2.67E-05,6.03E-05,0.00469633,2.43E-07,1.72E-06,0.00059619,0.000180005,0.012897207,5.24E-05,0.98138308,5.16E-06,9.63E-05,4.43E-06
4405,sentiment_analysis25,141,"We assign a sentence a positive label if p > 0 , a negative label if p < 0 , or a neutral label if p = 0 .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,16,1,0,,0.000968425,0,negative,1.20E-05,7.71E-05,0.003524954,5.37E-07,1.59E-06,0.004213343,0.000325387,0.114515919,0.000206144,0.877077712,4.48E-06,3.08E-05,1.00E-05
4406,sentiment_analysis25,142,"After removing duplicates , the statistics are show in .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,17,1,0,,0.000702364,0,negative,0.000106768,6.51E-06,0.001972555,8.53E-07,7.52E-06,0.000549066,0.000691718,0.003119909,4.69E-06,0.991569532,2.46E-06,0.001958733,9.68E-06
4407,sentiment_analysis25,143,"The resulting dataset has 8 aspects : restaurant , food , drinks , ambience , service , price , misc and location .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,18,1,0,,0.007633113,0,negative,6.08E-05,7.78E-05,0.003836869,0.000117469,0.001196005,0.005762056,0.001968669,0.012383234,2.60E-05,0.974317581,3.97E-06,0.000167401,8.21E-05
4408,sentiment_analysis25,144,"For ATSA task , we use restaurant reviews and laptop reviews from SemEval 2014 Task 4 .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,19,1,0,,0.073160235,0,negative,0.000192105,0.001659693,0.052249498,0.00024842,0.002901406,0.011958099,0.053703919,0.071246231,9.05E-05,0.800399152,0.00010257,0.004008492,0.001239868
4409,sentiment_analysis25,145,"On each dataset , we duplicate each sentence n a times , which is equal to the number of associated aspect categories ( ACSA ) or aspect terms ( ATSA ) .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,20,1,0,,0.003787893,0,negative,8.58E-05,0.00034963,0.010609699,7.99E-06,0.000123646,0.004342154,0.00307959,0.070821292,7.61E-05,0.910207626,4.35E-06,0.00022536,6.67E-05
4410,sentiment_analysis25,146,The statistics of the datasets are shown in .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,21,1,0,,0.00224221,0,negative,2.37E-05,3.28E-06,0.000500152,3.93E-07,2.91E-06,0.000700978,0.000726272,0.004902418,1.65E-06,0.991751737,2.06E-06,0.001377603,6.88E-06
4411,sentiment_analysis25,147,The sizes of hard data sets are also shown in for the sentences associated with only one sentiment label .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,22,1,0,,0.001940591,0,negative,2.29E-05,1.75E-05,0.001219056,8.63E-07,2.92E-06,0.006551959,0.001635207,0.075389982,1.47E-05,0.914766178,4.03E-06,0.000356696,1.80E-05
4412,sentiment_analysis25,148,"In our experiments , word embedding vectors are initialized with 300 - dimension GloVe vectors which are pre-trained on unlabeled data of 840 billion tokens .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,23,1,1,hyperparameters,0.960110975,1,hyperparameters,2.65E-06,4.00E-05,5.23E-05,1.20E-06,6.08E-07,0.020570432,0.001125321,0.966795683,2.01E-05,0.011357147,1.20E-06,7.15E-06,2.63E-05
4413,sentiment_analysis25,149,"Words out of the vocabulary of Glo Ve are randomly initialized with a uniform distribution U ( ? 0.25 , 0.25 ) .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,24,1,1,hyperparameters,0.875047121,1,hyperparameters,3.74E-06,3.17E-05,6.87E-05,9.19E-07,6.06E-07,0.019103832,0.001199534,0.955308034,2.00E-05,0.02422263,1.42E-06,1.11E-05,2.77E-05
4414,sentiment_analysis25,150,"We use Adagrad with a batch size of 32 instances , default learning rate of 1 e ? 2 , and maximal epochs of 30 .",Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,25,1,1,hyperparameters,0.968565398,1,hyperparameters,6.71E-06,3.40E-05,0.000131614,2.06E-05,3.07E-06,0.059897452,0.006707764,0.927398546,1.39E-05,0.005575974,1.75E-06,1.86E-05,0.000190059
4415,sentiment_analysis25,151,We only fine tune early stopping with 5 - fold cross validation on training datasets .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,26,1,1,hyperparameters,0.878209056,1,hyperparameters,9.13E-05,0.00027874,0.00041638,1.53E-05,1.07E-05,0.024645778,0.003152565,0.923776923,6.92E-05,0.047294408,2.10E-06,9.41E-05,0.000152542
4416,sentiment_analysis25,152,All neural models are implemented in PyTorch .,Datasets and Experiment Preparation,Datasets and Experiment Preparation,sentiment_analysis,25,27,1,1,hyperparameters,0.942460892,1,hyperparameters,5.94E-05,8.16E-05,0.001709562,0.002523655,3.61E-05,0.308653213,0.023739585,0.633576546,8.50E-05,0.028302794,8.78E-06,5.28E-05,0.001171036
4417,sentiment_analysis25,153,Compared Methods,,,sentiment_analysis,25,0,1,0,,0.003363972,0,negative,1.91E-05,0.000148937,7.51E-05,4.41E-07,6.71E-07,0.00022496,0.000164342,0.00299226,8.80E-05,0.993920181,0.001856336,0.00050613,3.53E-06
4418,sentiment_analysis25,154,"To comprehensively evaluate the performance of GCAE , we compare our model against the following models .",Compared Methods,Compared Methods,sentiment_analysis,25,1,1,0,,0.053499191,0,baselines,4.79E-05,1.46E-05,0.5624102,1.16E-07,2.23E-07,3.88E-05,0.000269617,0.000599267,1.74E-06,0.434312896,1.24E-05,0.002290173,2.03E-06
4419,sentiment_analysis25,155,NRC - Canada is the top method in SemEval 2014 Task 4 for ACSA and ATSA task .,Compared Methods,Compared Methods,sentiment_analysis,25,2,1,1,baselines,0.784906497,1,baselines,6.61E-05,1.59E-06,0.973398132,7.29E-07,3.93E-07,2.84E-05,0.002312518,0.000126742,3.24E-07,0.007913895,4.32E-05,0.016056325,5.17E-05
4420,sentiment_analysis25,156,"SVM is trained with extensive feature engineering : various types of n-grams , POS tags , and lexicon features .",Compared Methods,Compared Methods,sentiment_analysis,25,3,1,0,,0.898548159,1,baselines,3.84E-05,8.63E-05,0.701738864,1.41E-06,5.18E-07,0.00163042,0.000529388,0.060824638,0.000127328,0.23486043,2.63E-05,0.000101478,3.45E-05
4421,sentiment_analysis25,157,"The sentiment lexicons improve the performance significantly , but it requires large scale labeled data : 183 thousand Yelp reviews , 124 thousand Amazon laptop reviews , 56 million tweets , and 3 sentiment lexicons labeled manually .",Compared Methods,Compared Methods,sentiment_analysis,25,4,1,0,,0.506240569,1,negative,0.005037428,8.57E-05,0.323252501,4.38E-05,4.39E-05,0.000681617,0.00851033,0.004415252,1.13E-05,0.486213046,0.000272542,0.171016586,0.000416056
4422,sentiment_analysis25,158,CNN is widely used on text classification task .,Compared Methods,Compared Methods,sentiment_analysis,25,5,1,1,baselines,0.568251988,1,negative,5.28E-05,3.01E-05,0.342176205,1.01E-05,7.85E-07,0.001033651,0.004355214,0.01450628,1.90E-05,0.620464226,0.013928101,0.002860281,0.000563209
4423,sentiment_analysis25,159,"It can not directly capture aspectspecific sentiment information on ACSA task , but it provides a very strong baseline for sentiment classification .",Compared Methods,Compared Methods,sentiment_analysis,25,6,1,0,,0.803848863,1,baselines,0.00057008,1.00E-05,0.797064369,8.13E-07,1.00E-06,3.65E-05,0.00091592,0.000299574,1.28E-06,0.124741155,0.000216796,0.076099834,4.27E-05
4424,sentiment_analysis25,160,"We set the widths of filters to 3 , 4 , 5 with 100 features each .",Compared Methods,Compared Methods,sentiment_analysis,25,7,1,0,,0.992321029,1,hyperparameters,8.58E-05,7.43E-05,0.01040165,1.40E-05,1.62E-06,0.021787675,0.003800983,0.883253264,4.68E-05,0.080138747,1.71E-05,0.00026095,0.00011707
4425,sentiment_analysis25,161,TD - LSTM uses two LSTM networks to model the preceding and following contexts of the target to generate target - dependent representation for sentiment prediction .,Compared Methods,Compared Methods,sentiment_analysis,25,8,1,1,baselines,0.997834454,1,baselines,5.55E-07,2.12E-07,0.999465091,1.76E-08,7.03E-09,1.64E-06,2.53E-05,9.58E-06,2.01E-07,0.000480838,1.99E-06,1.28E-05,1.84E-06
4426,sentiment_analysis25,162,ATAE - LSTM is an attention - based LSTM for ACSA task .,Compared Methods,Compared Methods,sentiment_analysis,25,9,1,1,baselines,0.993346288,1,baselines,3.34E-06,6.14E-07,0.99893145,1.21E-07,5.29E-08,5.67E-06,0.000167576,2.42E-05,4.08E-07,0.000741465,3.93E-06,0.000110644,1.05E-05
4427,sentiment_analysis25,163,"It appends the given aspect embedding with each word embedding as the input of LSTM , and has an attention layer above the LSTM layer .",Compared Methods,Compared Methods,sentiment_analysis,25,10,1,0,,0.938968743,1,baselines,1.09E-05,1.57E-06,0.994745754,4.87E-08,4.36E-08,3.22E-06,1.42E-05,3.53E-05,8.74E-06,0.005151861,7.98E-07,2.49E-05,2.69E-06
4428,sentiment_analysis25,164,"IAN stands for interactive attention network for ATSA task , which is also based on LSTM and attention mechanisms .",Compared Methods,Compared Methods,sentiment_analysis,25,11,1,1,baselines,0.167441469,0,baselines,7.82E-06,9.25E-07,0.995407813,1.49E-07,1.01E-07,5.88E-06,8.50E-05,3.03E-05,7.75E-07,0.004244224,4.69E-06,0.000205763,6.62E-06
4429,sentiment_analysis25,165,"RAM is a recurrent attention network for ATSA task , which uses LSTM and multiple attention mechanisms .",Compared Methods,Compared Methods,sentiment_analysis,25,12,1,1,baselines,0.987553485,1,baselines,1.55E-06,3.95E-07,0.999287142,2.44E-08,1.58E-08,1.45E-06,3.93E-05,8.12E-06,3.76E-07,0.000617881,2.91E-06,3.81E-05,2.75E-06
4430,sentiment_analysis25,166,"GCN stands for gated convolutional neural network , in which GTRU does not have the aspect embedding as an additional input .",Compared Methods,Compared Methods,sentiment_analysis,25,13,1,1,baselines,0.338121929,0,baselines,5.41E-06,3.58E-06,0.943967853,6.45E-07,2.05E-07,8.83E-05,0.000319601,0.000830109,7.53E-06,0.054614356,1.07E-05,0.000127667,2.40E-05
4431,sentiment_analysis25,167,Results and Analysis,,,sentiment_analysis,25,0,1,0,,0.004153659,0,negative,4.92E-05,4.74E-05,7.12E-06,1.41E-07,3.63E-07,1.90E-05,8.00E-05,0.000295611,1.21E-05,0.995144313,0.001609051,0.002734443,1.25E-06
4432,sentiment_analysis25,168,ACSA,Results and Analysis,,sentiment_analysis,25,1,1,0,,0.2570792,0,negative,0.002080269,1.31E-05,0.001062417,1.53E-05,1.90E-06,0.000161955,0.000546613,0.000733527,3.89E-05,0.949795865,0.000153557,0.045018692,0.000377943
4433,sentiment_analysis25,169,"Following the SemEval workshop , we report the over all accuracy of all competing models over the test datasets of restaurant reviews as well as the hard test datasets .",Results and Analysis,ACSA,sentiment_analysis,25,2,1,0,,0.006090436,0,negative,0.000786155,4.39E-06,0.00013017,1.43E-06,1.17E-06,5.36E-06,9.01E-05,1.67E-05,2.03E-06,0.866156063,2.47E-05,0.132748729,3.31E-05
4434,sentiment_analysis25,170,Every experiment is repeated five times .,Results and Analysis,ACSA,sentiment_analysis,25,3,1,0,,0.000589584,0,negative,0.000484952,3.49E-06,3.50E-05,9.81E-07,3.40E-07,5.99E-05,0.000127372,0.000574376,7.69E-06,0.989994749,8.52E-06,0.008655798,4.67E-05
4435,sentiment_analysis25,171,The mean and the standard deviation are reported in .,Results and Analysis,ACSA,sentiment_analysis,25,4,1,0,,2.75E-05,0,negative,7.58E-05,3.73E-07,7.13E-06,2.72E-07,5.12E-08,8.77E-06,1.48E-05,4.04E-05,1.71E-06,0.995105037,4.13E-06,0.004735223,6.29E-06
4436,sentiment_analysis25,172,LSTM based model ATAE - LSTM has the worst performance of all neural networks .,Results and Analysis,ACSA,sentiment_analysis,25,5,1,1,results,0.916190722,1,results,0.001090687,9.56E-08,0.000161715,1.63E-07,3.68E-08,2.70E-06,0.000339324,5.63E-06,1.10E-07,0.038363499,4.84E-06,0.959988895,4.23E-05
4437,sentiment_analysis25,173,Aspect - based sentiment analysis is to extract the sentiment information closely related to the given aspect .,Results and Analysis,ACSA,sentiment_analysis,25,6,1,0,,0.204078202,0,negative,0.001926152,1.43E-05,0.000829663,8.71E-05,2.61E-06,7.52E-05,0.001372213,0.000106645,2.23E-05,0.663955214,0.053237715,0.264090103,0.014280735
4438,sentiment_analysis25,174,It is important to separate aspect information and sentiment information from the extracted information of sentences .,Results and Analysis,ACSA,sentiment_analysis,25,7,1,0,,0.025414325,0,negative,0.003887662,1.89E-06,9.75E-05,2.14E-06,2.97E-07,5.97E-06,3.02E-05,1.79E-05,4.75E-06,0.861307877,0.000150286,0.134391336,0.000102232
4439,sentiment_analysis25,175,The context vectors generated by LSTM have to convey the two kinds of information at the same time .,Results and Analysis,ACSA,sentiment_analysis,25,8,1,0,,0.000693413,0,negative,0.000322368,1.80E-06,5.50E-05,9.08E-08,5.17E-08,1.60E-06,2.36E-06,1.40E-05,2.80E-05,0.994520263,4.78E-06,0.005045868,3.96E-06
4440,sentiment_analysis25,176,"Moreover , the attention scores generated by the similarity scoring function are for the entire context vector .",Results and Analysis,ACSA,sentiment_analysis,25,9,1,0,,2.96E-05,0,negative,0.000217439,2.03E-06,8.22E-05,6.73E-08,4.10E-08,1.36E-06,1.44E-06,1.86E-05,3.07E-05,0.99851239,1.08E-06,0.001131525,1.08E-06
4441,sentiment_analysis25,177,GCAE improves the performance by 1.1 % to 2.5 % compared with ATAE - LSTM .,Results and Analysis,ACSA,sentiment_analysis,25,10,1,1,results,0.915615229,1,results,0.003706801,6.89E-08,2.82E-05,8.56E-08,2.36E-08,5.55E-07,0.000159438,2.09E-06,5.66E-08,0.016203942,7.31E-07,0.979869394,2.87E-05
4442,sentiment_analysis25,178,"First , our model incorporates GTRU to control the sentiment information flow according to the given aspect information at each dimension of the context vectors .",Results and Analysis,ACSA,sentiment_analysis,25,11,1,0,,0.081544457,0,negative,0.042654743,0.000184579,0.015000765,5.42E-06,5.29E-06,2.24E-05,9.03E-05,0.000102636,0.000771165,0.862585165,2.71E-05,0.078455408,9.50E-05
4443,sentiment_analysis25,179,The element - wise gating mechanism works at fine granularity instead of exerting an alignment score to all the dimensions of the context vectors in the attention layer of other models .,Results and Analysis,ACSA,sentiment_analysis,25,12,1,0,,0.00685226,0,negative,0.01009594,5.28E-05,0.004836733,9.07E-07,9.76E-07,6.75E-06,2.08E-05,5.45E-05,0.00068381,0.960374608,9.61E-06,0.023837493,2.51E-05
4444,sentiment_analysis25,180,"Second , GCAE does not generate a single context vector , but two vectors for aspect and sentiment features respectively , so that aspect and sentiment information is unraveled .",Results and Analysis,ACSA,sentiment_analysis,25,13,1,0,,0.001568965,0,negative,0.002171955,5.57E-06,0.001786394,1.12E-06,5.70E-07,7.71E-06,3.65E-05,1.70E-05,9.02E-06,0.933792292,5.42E-05,0.062067816,4.98E-05
4445,sentiment_analysis25,181,"By comparing the performance on the hard test datasets against CNN , it is easy to see the convolutional layer of GCAE is able to differentiate the sentiments of multiple entities .",Results and Analysis,ACSA,sentiment_analysis,25,14,1,0,,0.207148476,0,results,0.004048965,4.26E-08,1.12E-05,2.07E-08,1.35E-08,3.86E-07,5.68E-05,1.70E-06,3.33E-08,0.086906691,3.86E-07,0.908969469,4.35E-06
4446,sentiment_analysis25,182,"Convolutional neural networks CNN and GCN are not designed for aspect based sentiment analysis , but their performance exceeds that of ATAE - LSTM .",Results and Analysis,ACSA,sentiment_analysis,25,15,1,0,,0.315817588,0,results,0.002405654,9.48E-07,0.003765482,8.07E-07,2.49E-07,8.56E-06,0.000245178,1.25E-05,1.91E-06,0.389687063,8.83E-05,0.603463301,0.000320107
4447,sentiment_analysis25,183,The performance of SVM depends on the availability of the features it can use .,Results and Analysis,ACSA,sentiment_analysis,25,16,1,0,,0.000749379,0,negative,0.000562236,6.39E-07,2.92E-05,6.84E-07,1.08E-07,5.87E-06,1.71E-05,2.56E-05,2.52E-06,0.969744613,1.19E-05,0.02955411,4.53E-05
4448,sentiment_analysis25,184,"Without the large amount of sentiment lexicons , SVM perform worse than neural methods .",Results and Analysis,ACSA,sentiment_analysis,25,17,1,1,results,0.238180332,0,results,0.005163968,1.93E-08,4.86E-06,4.86E-08,1.19E-08,5.26E-07,9.86E-05,1.82E-06,1.90E-08,0.02275754,2.56E-07,0.971959294,1.30E-05
4449,sentiment_analysis25,185,"With multiple sentiment lexicons , the performance is increased by 7.6 % .",Results and Analysis,ACSA,sentiment_analysis,25,18,1,1,results,0.849817674,1,results,0.011157716,2.00E-07,1.74E-05,1.39E-07,6.56E-08,8.30E-07,0.000167099,4.67E-06,2.18E-07,0.045279679,4.89E-07,0.943329855,4.17E-05
4450,sentiment_analysis25,186,This inspires us to work on leveraging sentiment lexicons in neural networks in the future .,Results and Analysis,ACSA,sentiment_analysis,25,19,1,0,,0.001953746,0,negative,0.001014058,6.79E-07,2.18E-05,5.45E-07,1.71E-07,5.86E-06,1.15E-05,2.29E-05,4.20E-06,0.983919478,1.58E-06,0.014975019,2.22E-05
4451,sentiment_analysis25,187,The hard test datasets consist of replicated sentences with different sentiments towards different aspects .,Results and Analysis,ACSA,sentiment_analysis,25,20,1,0,,0.001150375,0,negative,0.000396991,1.49E-06,0.000143113,3.09E-07,2.39E-06,3.88E-06,3.53E-05,2.05E-05,1.10E-06,0.983361949,2.06E-07,0.016021917,1.08E-05
4452,sentiment_analysis25,188,"The models which can not utilize the given aspect information such as CNN and GCN perform poorly as expected , but GCAE has higher accuracy than other neural network models .",Results and Analysis,ACSA,sentiment_analysis,25,21,1,0,,0.838223639,1,results,0.002461668,1.43E-08,4.11E-06,5.58E-08,1.37E-08,4.35E-07,0.000107523,1.40E-06,1.31E-08,0.0237932,1.68E-07,0.973613985,1.74E-05
4453,sentiment_analysis25,189,GCAE achieves 4 % higher accuracy than ATAE - LSTM on Restaurant - Large and 5 % higher on SemEval - 2014 on ACSA task .,Results and Analysis,ACSA,sentiment_analysis,25,22,1,1,results,0.580573435,1,results,0.00049466,4.81E-08,1.46E-05,5.12E-08,1.96E-08,3.84E-07,0.000214408,1.70E-06,2.41E-08,0.015306967,3.70E-07,0.983908845,5.79E-05
4454,sentiment_analysis25,190,"However , GCN , which does not have aspect modeling part , has higher score than GCAE on the original restaurant dataset .",Results and Analysis,ACSA,sentiment_analysis,25,23,1,1,results,0.07725881,0,results,0.001873323,2.35E-08,3.94E-06,2.76E-08,1.66E-08,3.44E-07,0.000103711,1.75E-06,1.36E-08,0.040296486,1.27E-07,0.957708939,1.13E-05
4455,sentiment_analysis25,191,"It suggests that GCN performs better than GCAE when there is only one sentiment label in the given sentence , but not on the hard test dataset .",Results and Analysis,ACSA,sentiment_analysis,25,24,1,0,,0.031318936,0,results,0.005359559,1.95E-07,1.44E-05,2.32E-07,1.30E-07,2.78E-06,0.000127901,1.52E-05,2.56E-07,0.374819794,5.52E-07,0.619633422,2.56E-05
4456,sentiment_analysis25,192,ATSA,Results and Analysis,,sentiment_analysis,25,25,1,1,results,0.012239618,0,negative,0.008644614,1.12E-05,0.000716305,0.000103749,8.62E-06,0.000184064,0.000811497,0.000753258,5.07E-05,0.91710503,2.48E-05,0.068927767,0.002658403
4457,sentiment_analysis25,193,We apply the extended version of GCAE on ATSA task .,Results and Analysis,ATSA,sentiment_analysis,25,26,1,0,,0.002133852,0,negative,0.003535704,7.97E-05,0.004841489,1.33E-06,3.31E-06,1.54E-05,0.000453244,7.76E-05,2.96E-05,0.76234905,1.83E-05,0.228461976,0.000133238
4458,sentiment_analysis25,194,"On this task , the aspect terms are marked in the sentences and usually consist of multiple words .",Results and Analysis,ATSA,sentiment_analysis,25,27,1,0,,0.000117637,0,negative,0.00039618,3.38E-06,4.25E-05,1.64E-06,7.88E-07,6.94E-06,2.91E-05,3.46E-05,4.70E-06,0.969460273,1.51E-05,0.029894188,0.000110584
4459,sentiment_analysis25,195,"We compare IAN , RAM , TD - LSTM , ATAE - LSTM , and our GCAE model in .",Results and Analysis,ATSA,sentiment_analysis,25,28,1,0,,0.175308044,0,negative,0.000646192,3.34E-07,7.14E-05,2.58E-08,9.11E-08,1.30E-06,3.26E-05,5.16E-06,3.23E-07,0.952236104,9.26E-08,0.04700452,1.92E-06
4460,sentiment_analysis25,196,The models other than GCAE is based on LSTM and attention mechanisms .,Results and Analysis,ATSA,sentiment_analysis,25,29,1,0,,0.000988142,0,negative,0.000756019,2.35E-06,0.001404301,1.45E-06,6.62E-07,2.06E-05,0.000128541,4.17E-05,1.11E-05,0.983524453,1.31E-05,0.013896599,0.00019902
4461,sentiment_analysis25,197,"IAN has better performance than TD - LSTM and ATAE - LSTM , because two attention layers guides the representation learning of the context and the entity interactively .",Results and Analysis,ATSA,sentiment_analysis,25,30,1,1,results,0.442558981,0,results,0.001712204,2.95E-08,5.80E-06,2.61E-08,1.71E-08,4.35E-07,0.000219873,1.62E-06,1.43E-08,0.028669933,8.73E-08,0.969374438,1.55E-05
4462,sentiment_analysis25,198,"RAM also achieves good accuracy by combining multiple attentions with a recurrent neural network , but it needs more training time as shown in the following section .",Results and Analysis,ATSA,sentiment_analysis,25,31,1,1,results,0.132295566,0,results,0.001153386,2.87E-07,0.000240888,6.27E-08,8.43E-08,1.26E-06,0.000253234,3.29E-06,2.07E-07,0.221726383,1.63E-06,0.776580809,3.85E-05
4463,sentiment_analysis25,199,"On the hard test dataset , GCAE has 1 % higher accuracy than RAM on restaurant data and 1.7 % higher on laptop data .",Results and Analysis,ATSA,sentiment_analysis,25,32,1,1,results,0.394532564,0,results,0.001014603,4.38E-08,4.94E-06,2.07E-08,2.12E-08,3.45E-07,0.000211309,1.68E-06,1.74E-08,0.034077709,8.67E-08,0.964668078,2.11E-05
4464,sentiment_analysis25,200,GCAE uses the outputs of the small CNN over aspect terms to guide the composition of the sentiment features through the ReLU gate .,Results and Analysis,ATSA,sentiment_analysis,25,33,1,0,,0.008868102,0,negative,0.004128685,3.81E-05,0.069182637,8.20E-07,2.21E-06,1.26E-05,0.000146899,3.93E-05,0.000146007,0.902649965,3.49E-06,0.023520979,0.000128247
4465,sentiment_analysis25,201,"Because of the gating mechanisms and the convolutional layer over aspect terms , GCAE outperforms other neural models and basic SVM .",Results and Analysis,ATSA,sentiment_analysis,25,34,1,1,results,0.664534801,1,results,0.005489797,9.10E-08,1.00E-05,8.96E-08,4.11E-08,9.01E-07,0.000261631,3.07E-06,6.62E-08,0.05180538,1.44E-07,0.942399848,2.89E-05
4466,sentiment_analysis25,202,"Again , large scale sentiment lexicons bring significant improvement to SVM .",Results and Analysis,ATSA,sentiment_analysis,25,35,1,0,,0.158485031,0,results,0.007836369,6.10E-08,9.59E-06,5.22E-08,3.53E-08,6.20E-07,0.000177628,2.10E-06,5.05E-08,0.071148531,1.30E-07,0.920801562,2.33E-05
4467,sentiment_analysis25,203,Training Time,,,sentiment_analysis,25,0,1,0,,0.009231054,0,negative,0.000296956,0.00078508,0.000115462,7.20E-05,3.21E-05,0.001994874,0.001041637,0.022583277,0.000640504,0.966652565,0.004956896,0.000725357,0.000103212
4468,sentiment_analysis25,204,"We record the training time of all models until convergence on a validation set on a desktop machine with a 1080 Ti GPU , as shown in Table 6 .",Training Time,Training Time,sentiment_analysis,25,1,1,0,,0.104369719,0,negative,1.46E-05,3.76E-05,2.17E-05,1.72E-05,1.58E-05,0.412821781,0.003279827,0.088086175,2.01E-05,0.495619808,2.23E-05,1.98E-05,2.34E-05
4469,sentiment_analysis25,205,LSTM based models take more training time than convolutional models .,Training Time,Training Time,sentiment_analysis,25,2,1,0,,0.092820732,0,negative,0.000537771,4.21E-05,9.21E-05,3.87E-05,2.04E-05,0.053876709,0.002239441,0.021805145,5.78E-05,0.920652946,0.000377734,0.000156718,0.000102381
4470,sentiment_analysis25,206,"On ATSA task , because of multiple attention layers in IAN and RAM , they need even more time to finish the training .",Training Time,Training Time,sentiment_analysis,25,3,1,0,,0.023694531,0,negative,0.000426957,2.61E-05,6.25E-05,1.59E-05,3.17E-05,0.007392479,0.001633431,0.003349166,8.18E-06,0.985955368,0.000587018,0.000464003,4.72E-05
4471,sentiment_analysis25,207,"GCAE is much faster than other neural models , because neither convolutional operation nor GTRU has time dependency compared with LSTM and attention layer .",Training Time,Training Time,sentiment_analysis,25,4,1,0,,0.555936944,1,negative,0.001871067,0.000805616,0.005752518,2.48E-05,3.05E-05,0.053451464,0.091383121,0.07734089,0.000349945,0.719597112,0.008225012,0.039832878,0.001335029
4472,sentiment_analysis25,208,"Therefore , it is easier for hardware and libraries to parallel the comput - ing process .",Training Time,Training Time,sentiment_analysis,25,5,1,0,,0.000871284,0,negative,0.000158834,1.44E-05,7.02E-05,9.74E-07,3.81E-06,0.000475768,3.50E-05,0.000296934,1.45E-05,0.998863316,1.67E-05,4.86E-05,9.01E-07
4473,sentiment_analysis25,209,"Since the performance of SVM is retrieved from the original paper , we are notable to compare the training time of SVM .",Training Time,Training Time,sentiment_analysis,25,6,1,0,,0.000241179,0,negative,1.34E-05,3.13E-06,8.25E-06,1.43E-07,6.24E-07,0.000655976,3.13E-05,0.000801476,4.24E-06,0.998463247,6.76E-06,1.11E-05,3.58E-07
4474,sentiment_analysis25,210,Visualization,Training Time,,sentiment_analysis,25,7,1,0,,0.000159256,0,negative,1.15E-05,2.93E-05,1.49E-05,2.39E-06,1.61E-06,0.009826715,0.000483358,0.010976984,9.34E-05,0.978282815,0.000235131,3.03E-05,1.16E-05
4475,sentiment_analysis25,211,"In this section , we take a concrete review sentence as an example to illustrate how the proposed gate GTRU works .",Training Time,Visualization,sentiment_analysis,25,8,1,0,,1.32E-05,0,negative,2.83E-05,2.45E-05,1.37E-05,2.84E-08,1.78E-06,3.33E-05,7.62E-06,1.99E-05,2.03E-05,0.999822734,1.01E-05,1.76E-05,7.38E-08
4476,sentiment_analysis25,212,It is more difficult to visualize the weights generated by the gates than the attention weights in other neural networks .,Training Time,Visualization,sentiment_analysis,25,9,1,0,,5.71E-07,0,negative,1.44E-06,3.03E-07,1.91E-07,2.87E-08,7.21E-08,4.94E-05,1.90E-06,1.06E-05,8.45E-07,0.999900199,3.45E-05,4.95E-07,2.97E-08
4477,sentiment_analysis25,213,"The attention weight score is a global score over the words and the vector dimensions ; whereas in our model , there are N word N filter N dimension gate outputs .",Training Time,Visualization,sentiment_analysis,25,10,1,0,,2.97E-05,0,negative,0.000178586,7.22E-05,0.000451536,2.26E-07,3.36E-06,0.000562496,2.43E-05,0.000165305,0.001318686,0.997210993,4.36E-06,7.37E-06,6.15E-07
4478,sentiment_analysis25,214,"Therefore , we train a small model with only one filter which is only three word wide .",Training Time,Visualization,sentiment_analysis,25,11,1,0,,0.000101171,0,negative,0.001713108,5.39E-05,0.000124812,2.48E-06,5.13E-05,0.005899172,0.000420261,0.000696798,8.42E-05,0.990879256,7.12E-06,6.16E-05,6.06E-06
4479,sentiment_analysis25,215,"Then , for each word , we sum the N dimension outputs of the ReLU gates .",Training Time,Visualization,sentiment_analysis,25,12,1,0,,2.19E-05,0,negative,7.16E-05,1.96E-05,0.000113191,1.80E-08,4.60E-07,0.000201758,1.12E-05,6.05E-05,0.000495083,0.999022516,9.96E-07,3.05E-06,1.32E-07
4480,sentiment_analysis25,216,"After normalization , we plot the values on each word in .",Training Time,Visualization,sentiment_analysis,25,13,1,0,,7.92E-06,0,negative,1.48E-05,2.92E-06,2.51E-06,6.02E-08,7.49E-07,0.000774624,1.14E-05,0.000167281,1.85E-05,0.999004821,5.60E-07,1.71E-06,8.43E-08
4481,sentiment_analysis25,217,"Given different aspect targets , the ReLU gates would control the magnitude of the outputs of the tanh gates .",Training Time,Visualization,sentiment_analysis,25,14,1,0,,5.29E-06,0,negative,1.47E-05,2.75E-06,6.01E-06,1.40E-08,2.11E-07,0.000126712,3.24E-06,3.92E-05,7.30E-05,0.999732419,5.74E-07,1.20E-06,3.74E-08
4482,sentiment_analysis25,218,Conclusions and Future Work,,,sentiment_analysis,25,0,1,0,,0.001063554,0,negative,5.43E-05,3.39E-05,5.59E-06,3.77E-07,3.51E-07,5.34E-05,6.19E-05,0.000376148,2.43E-05,0.995250656,0.003578738,0.000557487,2.86E-06
4483,natural_language_inference53,1,title,,,natural_language_inference,53,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
4484,natural_language_inference53,2,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,title,title,natural_language_inference,53,1,1,1,research-problem,0.99476838,1,research-problem,4.10E-08,1.48E-05,8.72E-08,1.14E-07,1.28E-07,8.58E-08,1.27E-06,1.22E-06,4.94E-07,0.002295595,0.997685815,2.50E-07,6.23E-08
4485,natural_language_inference53,3,abstract,,,natural_language_inference,53,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
4486,natural_language_inference53,4,"Many modern NLP systems rely on word embeddings , previously trained in an unsupervised manner on large corpora , as base features .",abstract,abstract,natural_language_inference,53,1,1,0,,0.430670102,0,research-problem,4.09E-08,1.59E-05,1.73E-08,2.05E-06,5.73E-07,1.28E-06,4.35E-07,5.85E-06,1.13E-06,0.049541934,0.950430756,2.24E-08,5.46E-08
4487,natural_language_inference53,5,"Efforts to obtain embeddings for larger chunks of text , such as sentences , have however not been so successful .",abstract,abstract,natural_language_inference,53,2,1,0,,0.409046232,0,research-problem,8.15E-08,1.54E-05,2.02E-08,3.09E-06,8.56E-07,1.84E-06,6.76E-07,8.23E-06,1.28E-06,0.087706134,0.912262221,4.82E-08,8.18E-08
4488,natural_language_inference53,6,Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted .,abstract,abstract,natural_language_inference,53,3,1,0,,0.214579632,0,research-problem,3.28E-08,5.65E-06,8.31E-09,6.65E-07,2.10E-07,4.03E-07,2.75E-07,2.23E-06,3.40E-07,0.036179737,0.963810391,2.43E-08,3.00E-08
4489,natural_language_inference53,7,"In this paper , we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors ( Kiros et al. , 2015 ) on a wide range of transfer tasks .",abstract,abstract,natural_language_inference,53,4,1,0,,0.631280002,1,research-problem,3.43E-05,0.07294527,5.00E-05,1.06E-05,0.00017506,1.99E-05,4.08E-05,0.00035614,0.001712628,0.27783075,0.646750533,7.08E-05,3.24E-06
4490,natural_language_inference53,8,"Much like how computer vision uses ImageNet to obtain features , which can then be transferred to other tasks , our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks .",abstract,abstract,natural_language_inference,53,5,1,0,,0.030530207,0,negative,2.58E-06,0.001028706,1.16E-06,9.28E-06,6.14E-05,5.74E-06,2.09E-06,3.30E-05,4.44E-05,0.7576958,0.241114401,1.26E-06,2.21E-07
4491,natural_language_inference53,9,Our encoder is publicly available 1 .,abstract,,natural_language_inference,53,6,1,0,,0.188897003,0,negative,3.61E-06,0.007619392,7.10E-06,0.014259406,0.003908062,0.001216691,1.77E-05,0.001029856,0.000486538,0.958457749,0.012990637,4.48E-07,2.82E-06
4492,natural_language_inference53,10,Introduction,,,natural_language_inference,53,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
4493,natural_language_inference53,11,Distributed representations of words ( or word embeddings ) have shown to provide useful features for various tasks in natural language processing and computer vision .,Introduction,Introduction,natural_language_inference,53,1,1,0,,0.215527594,0,research-problem,7.87E-07,0.000202441,2.33E-07,1.16E-06,2.31E-06,5.48E-06,7.77E-06,1.22E-05,0.00017794,0.066178433,0.933409273,1.17E-06,7.77E-07
4494,natural_language_inference53,12,"While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them , this is not yet clear with regard to representations that carry the meaning of a full sentence .",Introduction,Introduction,natural_language_inference,53,2,1,0,,0.230816856,0,research-problem,1.16E-06,0.000447609,3.01E-07,5.92E-06,1.11E-05,1.96E-05,8.13E-06,3.58E-05,0.000328769,0.22732162,0.771817705,1.17E-06,1.10E-06
4495,natural_language_inference53,13,"That is , how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved .",Introduction,Introduction,natural_language_inference,53,3,1,0,,0.088355302,0,research-problem,1.78E-06,0.000831456,3.66E-07,1.13E-05,2.01E-05,2.57E-05,7.80E-06,4.11E-05,0.000633979,0.247187298,0.751236498,1.27E-06,1.37E-06
4496,natural_language_inference53,14,1 https://www.github.com/facebookresearch/InferSent,Introduction,Introduction,natural_language_inference,53,4,1,0,,0.982273328,1,code,3.78E-06,1.96E-05,1.08E-06,0.990706125,0.004893838,0.000350831,9.64E-06,2.97E-06,9.78E-06,0.003906775,8.93E-05,9.37E-08,6.21E-06
4497,natural_language_inference53,15,"In this paper , we study the task of learning universal representations of sentences , i.e. , a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks .",Introduction,Introduction,natural_language_inference,53,5,1,1,approach,0.950120476,1,research-problem,3.43E-05,0.22775415,6.01E-05,6.45E-05,0.002253659,6.08E-05,0.000216042,0.000183876,0.017855307,0.095523581,0.655916304,5.11E-05,2.63E-05
4498,natural_language_inference53,16,"Two questions need to be solved in order to build such an encoder , namely : what is the preferable neural network architecture ; and how and on what task should such a network be trained .",Introduction,Introduction,natural_language_inference,53,6,1,0,,0.003920415,0,negative,7.19E-06,0.002055797,1.04E-06,3.03E-05,9.41E-05,7.72E-05,1.22E-05,8.66E-05,0.002706263,0.86571709,0.129207538,2.99E-06,1.59E-06
4499,natural_language_inference53,17,"Following existing work on learning word embeddings , most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought or FastSent .",Introduction,Introduction,natural_language_inference,53,7,1,0,,0.349217569,0,research-problem,8.51E-06,0.003898587,4.44E-06,7.40E-05,0.000236047,8.42E-05,4.06E-05,9.22E-05,0.000725975,0.388892576,0.605929194,6.17E-06,7.46E-06
4500,natural_language_inference53,18,"Here , we investigate whether supervised learning can be leveraged instead , taking inspiration from previous results in computer vision , where many models are pretrained on the ImageNet ) before being transferred .",Introduction,Introduction,natural_language_inference,53,8,1,1,approach,0.79095135,1,approach,7.75E-05,0.708103264,9.85E-05,9.72E-06,0.000783088,5.23E-05,6.06E-05,0.000196579,0.227840464,0.04998314,0.012761844,2.77E-05,5.21E-06
4501,natural_language_inference53,19,"We compare sentence embeddings trained on various supervised tasks , and show that sentence embeddings generated from models trained on a natural language inference ( NLI ) task reach the best results in terms of transfer accuracy .",Introduction,Introduction,natural_language_inference,53,9,1,0,,0.278389738,0,approach,0.000497303,0.558331578,9.27E-05,1.57E-05,0.001966755,0.000137378,0.000853811,0.000679992,0.087377185,0.309761185,0.039539327,0.000722081,2.50E-05
4502,natural_language_inference53,20,We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high - level understanding task that involves reasoning about the semantic relationships within sentences .,Introduction,Introduction,natural_language_inference,53,10,1,0,,0.383849683,0,negative,7.36E-06,0.007053398,2.69E-06,1.03E-05,0.000156642,2.68E-05,1.93E-05,5.63E-05,0.003115965,0.547062454,0.442479563,7.07E-06,2.22E-06
4503,natural_language_inference53,21,"Unlike in computer vision , where convolutional neural networks are predominant , there are multiple ways to encode a sentence using neural networks .",Introduction,Introduction,natural_language_inference,53,11,1,0,,0.1827003,0,research-problem,3.46E-06,0.004427683,3.82E-06,9.93E-06,7.01E-05,2.72E-05,2.02E-05,4.05E-05,0.003026213,0.394529667,0.597833496,5.22E-06,2.59E-06
4504,natural_language_inference53,22,"Hence , we investigate the impact of the sentence encoding architecture on representational transferability , and compare convolutional , recurrent and even simpler word composition schemes .",Introduction,Introduction,natural_language_inference,53,12,1,1,approach,0.495752205,0,approach,0.000111588,0.571715026,6.01E-05,4.82E-06,0.000921013,7.59E-05,8.87E-05,0.000239042,0.255310682,0.165101463,0.006327207,4.07E-05,3.72E-06
4505,natural_language_inference53,23,"Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling , trained on the Stanford Natural Language Inference ( SNLI ) dataset , yields state - of - the - art sentence embeddings compared to all existing alternative unsupervised approaches like SkipThought or FastSent , while be-ing much faster to train .",Introduction,Introduction,natural_language_inference,53,13,1,0,,0.068695401,0,approach,0.000679574,0.43473556,9.89E-05,1.80E-05,0.00121278,0.000199264,0.001446169,0.000854249,0.127069006,0.386730221,0.045668535,0.001233783,5.39E-05
4506,natural_language_inference53,24,We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information .,Introduction,Introduction,natural_language_inference,53,14,1,0,,0.200898276,0,negative,0.000106258,0.444541785,4.43E-05,0.00010528,0.019080202,0.000320682,0.000237259,0.000637266,0.050113121,0.478406374,0.006285765,0.000105963,1.58E-05
4507,natural_language_inference53,25,Related work,,,natural_language_inference,53,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
4508,natural_language_inference53,44,Approach,,,natural_language_inference,53,0,1,0,,6.30E-05,0,negative,0.000256653,0.000405673,1.67E-05,0.000511976,2.16E-05,0.00097816,0.00011784,0.00317846,0.000653531,0.990596627,0.003176926,4.36E-05,4.23E-05
4509,natural_language_inference53,45,"This work combines two research directions , which we describe in what follows .",Approach,Approach,natural_language_inference,53,1,1,0,,0.004797953,0,negative,5.15E-06,0.063260624,8.28E-06,2.86E-05,0.0009736,7.63E-05,3.55E-06,0.000245473,0.007063953,0.921396393,0.006936077,1.33E-06,6.82E-07
4510,natural_language_inference53,46,"First , we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task .",Approach,Approach,natural_language_inference,53,2,1,0,,0.002224206,0,negative,6.52E-06,0.126974574,5.26E-06,4.71E-05,0.001580513,4.51E-05,3.43E-06,0.00025927,0.005152828,0.854088183,0.01183385,2.46E-06,8.41E-07
4511,natural_language_inference53,47,"We subsequently describe the architectures that we investigated for the sentence encoder , which , in our opinion , covers a suitable range of sentence encoders currently in use .",Approach,Approach,natural_language_inference,53,3,1,0,,0.015760633,0,negative,6.26E-06,0.163133082,7.78E-06,6.43E-06,0.000309511,6.36E-05,3.76E-06,0.000412747,0.01450418,0.814796615,0.006752507,3.14E-06,4.22E-07
4512,natural_language_inference53,48,"Specifically , we examine standard recurrent models such as LSTMs and GRUs , for which we investigate mean and maxpooling over the hidden representations ; a selfattentive network that incorporates different views of the sentence ; and a hierarchical convolutional network that can be seen as a tree - based method that blends different levels of abstraction .",Approach,Approach,natural_language_inference,53,4,1,0,,0.429369591,0,approach,2.39E-05,0.824645912,0.000137434,7.14E-06,0.0007476,0.000104221,1.61E-05,0.000795033,0.088559191,0.083929378,0.00102539,7.21E-06,1.49E-06
4513,natural_language_inference53,49,The Natural Language Inference task,Approach,Approach,natural_language_inference,53,5,1,0,,0.605091095,1,research-problem,2.86E-05,0.014874617,3.18E-05,0.000206442,0.005133243,0.000118827,0.000785731,0.000267018,0.000315224,0.280467165,0.697506774,0.000222408,4.22E-05
4514,natural_language_inference53,50,"The SNLI dataset consists of 570 k humangenerated English sentence pairs , manually labeled with one of three categories : entailment , contradiction and neutral .",Approach,Approach,natural_language_inference,53,6,1,0,,0.125120916,0,dataset,7.81E-06,0.028709825,4.36E-05,0.001482687,0.8427433,0.00018615,3.65E-05,7.20E-05,0.000182427,0.125733626,0.000792422,5.21E-06,4.37E-06
4515,natural_language_inference53,51,"It captures natural language inference , also known in previous incarnations as Recognizing Textual Entailment ( RTE ) , and constitutes one of the largest high - quality labeled resources explicitly constructed in order to require understanding sentence semantics .",Approach,Approach,natural_language_inference,53,7,1,0,,0.467497062,0,dataset,2.55E-05,0.064854735,0.000146752,0.003078622,0.496208633,0.000332757,0.000153192,0.000131728,0.001131298,0.410427503,0.023463736,2.30E-05,2.26E-05
4516,natural_language_inference53,52,We hypothesize that the semantic nature of NLI makes it a good candidate for learning universal sentence embeddings in a supervised way .,Approach,Approach,natural_language_inference,53,8,1,0,,0.071805783,0,negative,7.37E-06,0.135695829,1.21E-05,3.68E-05,0.001213654,8.93E-05,4.78E-06,0.000473269,0.016856115,0.835563974,0.010042152,3.21E-06,1.47E-06
4517,natural_language_inference53,53,"That is , we aim to demonstrate that sentence encoders trained on natural language inference are able to learn sentence representations that capture universally useful features .",Approach,Approach,natural_language_inference,53,9,1,0,,0.250027361,0,approach,1.31E-05,0.604115211,1.72E-05,1.78E-05,0.000780373,4.54E-05,8.22E-06,0.000563987,0.037770076,0.337958272,0.018698687,9.65E-06,2.03E-06
4518,natural_language_inference53,54,Models can be trained on SNLI in two different ways : ( i ) sentence encoding - based models that explicitly separate the encoding of the individual sentences and ( ii ) joint methods that allow to use encoding of both sentences ( to use cross - features or attention from one sentence to the other ) .,Approach,Approach,natural_language_inference,53,10,1,0,,0.409720979,0,negative,6.49E-06,0.079586173,2.98E-05,3.80E-06,5.44E-05,7.35E-05,9.28E-06,0.000672941,0.039971936,0.785960564,0.093624591,4.94E-06,1.57E-06
4519,natural_language_inference53,55,"Since our goal is to train a generic sentence encoder , we adopt the first setting .",Approach,Approach,natural_language_inference,53,11,1,0,,0.009322023,0,approach,2.40E-05,0.78232283,7.07E-05,2.01E-05,0.002294454,5.57E-05,8.93E-06,0.000441042,0.025855226,0.187838693,0.001057362,9.64E-06,1.42E-06
4520,natural_language_inference53,56,"As illustrated in , a typical architecture of this kind uses a shared sentence encoder that outputs a representation for the premise u and the hypothesis v.",Approach,Approach,natural_language_inference,53,12,1,0,,0.003156308,0,negative,5.48E-06,0.030177792,2.22E-05,3.65E-05,0.00021394,0.000186638,1.41E-05,0.000509493,0.041828547,0.900938406,0.026060138,3.03E-06,3.81E-06
4521,natural_language_inference53,57,"Once the sentence vectors are generated , 3 matching methods are applied to extract relations between u and v : ( i ) concatenation of the two representations ( u , v ) ; ( ii ) element - wise product u * v ; and ( iii ) absolute element - wise difference |u ? v|.",Approach,Approach,natural_language_inference,53,13,1,0,,0.446087968,0,approach,9.31E-06,0.373604132,0.000119566,3.98E-07,0.000198674,2.97E-05,3.53E-06,0.000311708,0.257336604,0.367618887,0.000764573,2.63E-06,2.57E-07
4522,natural_language_inference53,58,"The resulting vector , which captures information from both the premise and the hypothesis , is fed into a 3 - class classifier consisting of multiple fullyconnected layers culminating in a softmax layer .",Approach,Approach,natural_language_inference,53,14,1,0,,0.372602616,0,model,4.67E-06,0.208373206,3.76E-05,3.07E-07,5.50E-05,2.24E-05,2.62E-06,0.000296259,0.72633187,0.064698336,0.000176654,8.35E-07,3.00E-07
4523,natural_language_inference53,59,Sentence encoder architectures,,,natural_language_inference,53,0,1,0,,0.09381714,0,research-problem,5.75E-05,0.000842661,0.000366166,6.89E-06,2.48E-06,0.000184503,0.000692225,0.001666346,0.002312315,0.262548524,0.729583555,0.001638981,9.78E-05
4524,natural_language_inference53,60,"A wide variety of neural networks for encoding sentences into fixed - size representations exists , and it is not yet clear which one best captures generically useful information .",Sentence encoder architectures,Sentence encoder architectures,natural_language_inference,53,1,1,0,,0.000272469,0,negative,3.49E-06,5.10E-06,2.05E-06,2.87E-07,3.45E-08,7.69E-06,5.18E-06,8.25E-05,1.60E-05,0.997280687,0.002538714,5.74E-05,7.96E-07
4525,natural_language_inference53,61,"We compare 7 different architectures : standard recurrent encoders with either Long Short - Term Memory ( LSTM ) or Gated Recurrent Units ( GRU ) , concatenation of last hidden states of forward and backward GRU , Bi-directional LSTMs ( BiLSTM ) with either mean or max pooling , self - attentive network and hierarchical convolutional networks .",Sentence encoder architectures,Sentence encoder architectures,natural_language_inference,53,2,1,0,,0.237006182,0,negative,0.000150246,0.000659216,0.000983826,1.48E-05,6.70E-06,0.000991033,0.000804213,0.004234456,0.002344411,0.988196593,0.000423118,0.001151854,3.96E-05
4526,natural_language_inference53,62,LSTM and GRU,Sentence encoder architectures,Sentence encoder architectures,natural_language_inference,53,3,1,0,,0.001481163,0,negative,3.89E-05,1.75E-05,0.000941797,5.02E-07,2.08E-07,6.76E-05,0.000592696,0.000332467,0.000234803,0.983686945,0.001255,0.012804253,2.73E-05
4527,natural_language_inference53,63,"Our first , and simplest , encoders apply recurrent neural networks using either LSTM or GRU modules , as in sequence to sequence encoders .",Sentence encoder architectures,Sentence encoder architectures,natural_language_inference,53,4,1,0,,0.003060286,0,negative,0.000708942,0.00033239,0.008398312,5.78E-06,8.28E-06,0.000101682,0.000149677,0.000281214,0.003204824,0.985268902,6.93E-05,0.001460927,9.76E-06
4528,natural_language_inference53,64,"For a sequence of T words ( w 1 , . . . , w T ) , the network computes a set of T hidden representations",Sentence encoder architectures,Sentence encoder architectures,natural_language_inference,53,5,1,0,,9.28E-06,0,negative,1.47E-05,2.30E-05,3.40E-05,5.16E-08,1.32E-07,2.79E-06,1.58E-06,3.41E-05,0.000524701,0.999289417,9.98E-06,6.54E-05,2.06E-07
4529,natural_language_inference53,65,"We also consider a model BiGRU - last that concatenates the last hidden state of a forward GRU , and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors .",Sentence encoder architectures,Sentence encoder architectures,natural_language_inference,53,6,1,0,,0.000255677,0,negative,0.000251001,0.000378348,0.009519048,3.81E-07,1.49E-06,3.36E-05,6.55E-05,0.000255953,0.007765361,0.981038768,4.34E-05,0.000643489,3.68E-06
4530,natural_language_inference53,66,BiLSTM with mean / max pooling,Sentence encoder architectures,,natural_language_inference,53,7,1,0,,0.049842815,0,negative,5.84E-05,0.000188997,0.003992,6.18E-07,3.33E-07,3.99E-05,0.000223477,0.000353094,0.005438122,0.978172092,0.005905403,0.005598326,2.93E-05
4531,natural_language_inference53,67,For a sequence of T words {w,Sentence encoder architectures,,natural_language_inference,53,8,1,0,,5.35E-07,0,negative,1.16E-06,1.43E-06,1.94E-06,3.37E-09,1.00E-08,6.03E-07,3.85E-07,8.70E-06,2.82E-05,0.99993919,2.21E-06,1.62E-05,2.68E-08
4532,natural_language_inference53,68,is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions :,Sentence encoder architectures,For a sequence of T words {w,natural_language_inference,53,9,1,0,,0.000647055,0,negative,9.40E-05,3.89E-05,0.017679314,6.18E-07,7.94E-07,9.27E-06,8.90E-06,0.000125498,0.001928207,0.979826811,1.83E-05,0.00024448,2.49E-05
4533,natural_language_inference53,69,"We experiment with two ways of combining the varying number of {h t } t to form a fixed - size vector , either by selecting the maximum value over each dimension of the hidden units ( max pooling ) or by considering the average of the representations ( mean pooling ) .",Sentence encoder architectures,For a sequence of T words {w,natural_language_inference,53,10,1,0,,0.000747876,0,negative,0.000223509,0.000268592,0.000512143,1.15E-06,2.07E-06,2.47E-05,2.21E-05,0.00101909,0.001047936,0.99642098,5.07E-06,0.000441279,1.14E-05
4534,natural_language_inference53,70,The movie was great,Sentence encoder architectures,,natural_language_inference,53,11,1,0,,5.38E-05,0,negative,1.44E-05,4.15E-06,1.68E-06,2.93E-07,1.67E-07,1.23E-05,1.00E-05,0.00011023,5.78E-05,0.999466318,9.32E-06,0.000312454,9.12E-07
4535,natural_language_inference53,71,Figure 2 : Bi - LSTM max - pooling network .,Sentence encoder architectures,The movie was great,natural_language_inference,53,12,1,0,,0.002655116,0,negative,3.48E-05,4.80E-06,0.000274631,1.13E-06,1.79E-07,6.28E-05,2.49E-05,0.000245585,0.000305626,0.998884313,1.72E-05,0.000138974,5.13E-06
4536,natural_language_inference53,72,Self - attentive network,Sentence encoder architectures,,natural_language_inference,53,13,1,0,,0.000247491,0,negative,3.79E-05,0.000144192,0.000740837,6.85E-07,5.33E-07,3.07E-05,9.40E-05,0.000317098,0.007806388,0.986601678,0.001954016,0.002244609,2.73E-05
4537,natural_language_inference53,73,The self - attentive sentence encoder uses an attention mechanism over the hidden states of a BiLSTM to generate a representation u of an input sentence .,Sentence encoder architectures,Self - attentive network,natural_language_inference,53,14,1,0,,1.11E-05,0,negative,6.86E-05,4.22E-05,0.003028432,1.50E-07,2.52E-07,2.48E-06,4.14E-06,4.40E-05,0.002826983,0.993831351,7.22E-06,0.000140988,3.28E-06
4538,natural_language_inference53,74,The attention mechanism is defined as :,Sentence encoder architectures,Self - attentive network,natural_language_inference,53,15,1,0,,1.05E-05,0,negative,4.47E-06,1.71E-06,6.55E-05,4.87E-09,6.55E-09,4.15E-07,3.27E-07,1.26E-05,0.000278825,0.99961275,7.21E-07,2.25E-05,1.52E-07
4539,natural_language_inference53,75,". . , h T } are the output hidden vectors of a BiLSTM .",Sentence encoder architectures,Self - attentive network,natural_language_inference,53,16,1,0,,1.89E-07,0,negative,4.99E-06,1.19E-06,6.62E-06,3.65E-09,1.20E-08,1.33E-07,1.51E-07,4.67E-06,1.33E-05,0.999910297,1.75E-07,5.84E-05,3.38E-08
4540,natural_language_inference53,76,"These are fed to an affine transformation ( W , b w ) which outputs a set of keys ( h 1 , . . . ,h T ) .",Sentence encoder architectures,Self - attentive network,natural_language_inference,53,17,1,0,,1.08E-06,0,negative,6.86E-06,1.69E-06,1.08E-05,5.36E-09,1.30E-08,2.29E-07,2.63E-07,6.47E-06,6.29E-05,0.999872161,1.02E-07,3.85E-05,6.14E-08
4541,natural_language_inference53,77,The {?,Sentence encoder architectures,Self - attentive network,natural_language_inference,53,18,1,0,,1.75E-07,0,negative,3.23E-06,7.95E-08,1.31E-06,4.47E-09,3.86E-09,2.31E-07,2.42E-07,3.11E-06,3.69E-06,0.999948842,8.37E-08,3.91E-05,5.04E-08
4542,natural_language_inference53,78,i } represent the score of similarity between the keys and a learned context query vector u w .,Sentence encoder architectures,Self - attentive network,natural_language_inference,53,19,1,0,,2.97E-07,0,negative,9.21E-06,6.70E-07,9.32E-06,5.09E-09,1.08E-08,1.30E-07,1.48E-07,3.81E-06,1.23E-05,0.999886404,8.51E-08,7.78E-05,4.37E-08
4543,natural_language_inference53,79,"These weights are used to produce the final representation u , which is a weighted linear combination of the hidden vectors .",Sentence encoder architectures,Self - attentive network,natural_language_inference,53,20,1,0,,6.84E-07,0,negative,9.14E-06,4.12E-06,7.02E-06,1.71E-08,2.24E-08,6.63E-07,4.77E-07,3.73E-05,0.000315283,0.999602163,2.03E-07,2.34E-05,2.30E-07
4544,natural_language_inference53,80,"Following we use a selfattentive network with multiple views of the input sentence , so that the model can learn which part of the sentence is important for the given task .",Sentence encoder architectures,Self - attentive network,natural_language_inference,53,21,1,0,,1.12E-05,0,negative,6.57E-05,2.49E-05,0.000559323,7.99E-08,2.09E-07,1.28E-06,1.86E-06,2.43E-05,0.001051648,0.998088399,8.19E-07,0.000180383,1.12E-06
4545,natural_language_inference53,81,"Concretely , we have 4 context vectors u 1 w , u 2 w , u 3 w , u 4 w which generate 4 representations thatare then concatenated to obtain the sentence representation u. illustrates this architecture .",Sentence encoder architectures,Self - attentive network,natural_language_inference,53,22,1,0,,7.30E-07,0,negative,5.44E-06,2.57E-06,1.98E-05,4.99E-08,5.30E-08,9.60E-07,8.67E-07,1.98E-05,0.00031085,0.999596571,5.57E-07,4.16E-05,8.52E-07
4546,natural_language_inference53,82,The movie was great,Sentence encoder architectures,,natural_language_inference,53,23,1,0,,4.35E-05,0,negative,1.18E-05,3.08E-06,1.36E-06,2.12E-07,1.96E-07,8.63E-06,9.37E-06,9.09E-05,4.48E-05,0.999479903,2.32E-06,0.000346415,1.01E-06
4547,natural_language_inference53,83,Hierarchical ConvNet,Sentence encoder architectures,,natural_language_inference,53,24,1,0,,0.000942308,0,negative,9.17E-05,6.38E-05,0.001872808,1.62E-06,1.40E-06,6.79E-05,0.000246047,0.000381694,0.004182985,0.988630428,0.000117317,0.0042639,7.84E-05
4548,natural_language_inference53,84,"One of the currently best performing models on classification tasks is a convolutional architecture termed AdaSent , which concatenates different representations of the sentences at different level of abstractions .",Sentence encoder architectures,Hierarchical ConvNet,natural_language_inference,53,25,1,0,,0.001172886,0,negative,1.90E-05,2.58E-05,0.001034532,3.98E-06,1.14E-06,6.04E-05,8.29E-05,0.000263918,0.001079314,0.997057248,0.000132787,0.000184632,5.44E-05
4549,natural_language_inference53,85,"Inspired by this architecture , we introduce a faster version consisting of 4 convolutional layers .",Sentence encoder architectures,Hierarchical ConvNet,natural_language_inference,53,26,1,0,,0.000967936,0,negative,0.000133134,0.000430167,0.003297795,1.37E-06,5.28E-06,1.88E-05,3.92E-05,0.000189687,0.023559919,0.972098652,1.06E-05,0.000201815,1.35E-05
4550,natural_language_inference53,86,"At every layer , a representation u i is computed by a max - pooling operation over the feature maps ( see ) .",Sentence encoder architectures,Hierarchical ConvNet,natural_language_inference,53,27,1,0,,1.00E-05,0,negative,4.39E-06,7.14E-06,1.48E-05,3.43E-08,1.05E-07,8.76E-07,6.95E-07,2.39E-05,0.000382447,0.999554349,2.35E-07,1.08E-05,1.91E-07
4551,natural_language_inference53,87,Training details,,,natural_language_inference,53,0,1,0,,0.002340091,0,negative,6.05E-05,0.001338931,1.43E-05,0.000123468,5.30E-05,0.00076714,0.000215992,0.00645336,0.000307534,0.988848599,0.00167076,0.000130888,1.56E-05
4552,natural_language_inference53,88,"For all our models trained on SNLI , we use SGD with a learning rate of 0.1 and a weight decay of 0.99 .",Training details,Training details,natural_language_inference,53,1,1,1,hyperparameters,0.995024828,1,experimental-setup,3.03E-06,5.50E-06,3.36E-06,4.22E-06,4.96E-07,0.821132199,0.000690435,0.176533281,2.87E-06,0.001615628,7.27E-07,1.58E-06,6.67E-06
4553,natural_language_inference53,89,"At each epoch , we divide the learning rate by 5 if the dev accuracy decreases .",Training details,Training details,natural_language_inference,53,2,1,1,hyperparameters,0.989651742,1,experimental-setup,7.25E-06,1.30E-05,3.48E-06,1.74E-06,6.03E-07,0.59416377,0.000627853,0.401919186,8.96E-06,0.003239768,1.96E-06,3.08E-06,9.32E-06
4554,natural_language_inference53,90,We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10 ?5 .,Training details,Training details,natural_language_inference,53,3,1,1,hyperparameters,0.989871988,1,experimental-setup,2.93E-06,4.72E-06,1.96E-06,1.47E-06,4.08E-07,0.734174474,0.00068387,0.262079223,3.24E-06,0.003036185,1.69E-06,2.85E-06,6.99E-06
4555,natural_language_inference53,91,"For the classifier , we use a multi - layer perceptron with 1 hidden - layer of 512 hidden units .",Training details,Training details,natural_language_inference,53,4,1,1,hyperparameters,0.986759167,1,experimental-setup,6.20E-06,1.06E-05,9.65E-06,2.70E-06,6.14E-07,0.752687495,0.000968962,0.243478129,1.23E-05,0.002806551,1.63E-06,3.06E-06,1.21E-05
4556,natural_language_inference53,92,We use opensource GloVe vectors trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .,Training details,Training details,natural_language_inference,53,5,1,1,hyperparameters,0.996389588,1,experimental-setup,1.30E-06,4.30E-06,1.97E-06,1.28E-06,2.88E-07,0.787893866,0.000608421,0.20929538,2.51E-06,0.002184048,5.79E-07,1.56E-06,4.50E-06
4557,natural_language_inference53,93,Evaluation of sentence representations,,,natural_language_inference,53,0,1,0,,0.056174235,0,negative,0.00027218,0.00022017,0.000205718,1.77E-06,3.54E-06,4.54E-05,0.004170578,0.000608633,1.04E-05,0.576988488,0.234631746,0.182787466,5.39E-05
4558,natural_language_inference53,94,Our aim is to obtain general - purpose sentence embeddings that capture generic information that is useful for a broad set of tasks .,Evaluation of sentence representations,Evaluation of sentence representations,natural_language_inference,53,1,1,0,,0.01479264,0,negative,0.001408141,5.61E-05,0.002124669,3.75E-05,4.00E-06,9.95E-05,0.00094404,0.000308397,8.62E-05,0.920470818,0.000643068,0.069719202,0.004098485
4559,natural_language_inference53,95,"To evaluate the quality of these representations , we use them as features in 12 transfer tasks .",Evaluation of sentence representations,Evaluation of sentence representations,natural_language_inference,53,2,1,0,,0.008119613,0,negative,0.000256454,1.17E-06,0.00019109,3.25E-07,3.72E-07,1.04E-05,0.000436619,3.51E-05,1.77E-06,0.982847769,1.92E-06,0.0161677,4.93E-05
4560,natural_language_inference53,96,We present our sentence - embedding evaluation procedure in name N task C examples MR 11 k sentiment ( movies ),Evaluation of sentence representations,Evaluation of sentence representations,natural_language_inference,53,3,1,0,,0.001462191,0,negative,8.66E-05,1.60E-07,0.00010143,1.31E-07,9.78E-08,4.86E-06,0.000343835,9.91E-06,2.73E-07,0.964029262,2.35E-06,0.035332067,8.91E-05
4561,natural_language_inference53,97,"2 "" Too slow for a younger crowd , too shallow for an older one . """,Evaluation of sentence representations,Evaluation of sentence representations,natural_language_inference,53,4,1,0,,7.69E-05,0,negative,5.81E-05,3.34E-08,1.01E-05,1.94E-07,2.52E-08,5.36E-06,3.23E-05,1.05E-05,4.02E-07,0.997219178,4.36E-07,0.002635863,2.75E-05
4562,natural_language_inference53,98,( neg ) CR 4 k product reviews,Evaluation of sentence representations,Evaluation of sentence representations,natural_language_inference,53,5,1,0,,0.054269573,0,negative,0.000886122,1.90E-07,0.000270762,1.75E-06,3.15E-07,1.56E-05,0.004133901,1.91E-05,7.99E-07,0.51227757,1.33E-05,0.476879475,0.005501136
4563,natural_language_inference53,99,"2 "" We tried it out christmas night and it worked great ."" ( pos ) SUBJ 10 k subjectivity / objectivity",Evaluation of sentence representations,Evaluation of sentence representations,natural_language_inference,53,6,1,0,,0.000669789,0,negative,0.000516286,8.88E-08,8.61E-05,9.96E-06,3.48E-07,2.97E-05,0.000332594,1.36E-05,1.69E-06,0.985975338,2.57E-06,0.012180349,0.000851383
4564,natural_language_inference53,100,"2 "" A movie that does n't aim too high , but does n't need to . "" ( subj ) MPQA 11 k opinion polarity 2 "" do n't want "" ; "" would like to tell "" ; ( neg , pos )",Evaluation of sentence representations,Evaluation of sentence representations,natural_language_inference,53,7,1,0,,0.000189555,0,negative,5.15E-05,5.24E-08,4.02E-05,1.02E-06,1.17E-07,1.62E-05,0.000140702,1.28E-05,7.20E-07,0.994528137,1.16E-06,0.004920653,0.000286839
4565,natural_language_inference53,101,TREC 6 k question - type,Evaluation of sentence representations,,natural_language_inference,53,8,1,0,,0.374805537,0,results,0.000171641,2.28E-07,0.000147915,2.83E-06,3.98E-07,2.56E-05,0.018013167,2.74E-05,3.51E-07,0.197115264,2.59E-05,0.764309074,0.020160207
4566,natural_language_inference53,102,"6 "" What are the twin cities ? "" ( LOC : city ) SST",Evaluation of sentence representations,TREC 6 k question - type,natural_language_inference,53,9,1,0,,0.001113751,0,negative,6.32E-05,2.88E-08,1.71E-05,1.58E-06,1.10E-07,1.55E-05,0.000119607,8.55E-06,4.18E-07,0.99641301,3.82E-07,0.003103614,0.000256864
4567,natural_language_inference53,103,70 k sentiment ( movies ),Evaluation of sentence representations,TREC 6 k question - type,natural_language_inference,53,10,1,0,,0.289175321,0,negative,0.00054752,1.21E-07,0.000340191,1.28E-06,4.25E-07,4.44E-05,0.008141827,4.58E-05,4.58E-07,0.756645757,1.13E-06,0.227180405,0.007050712
4568,natural_language_inference53,104,"2 "" Audrey Tautou has a knack for picking roles that magnify her [.. ] "" ( pos ) this section .",Evaluation of sentence representations,TREC 6 k question - type,natural_language_inference,53,11,1,0,,0.001362064,0,negative,0.000136768,5.13E-08,1.58E-05,9.36E-06,2.13E-07,3.49E-05,0.000170481,1.56E-05,9.59E-07,0.996356685,5.01E-07,0.002680585,0.000578151
4569,natural_language_inference53,105,We constructed a sentence evaluation tool 2 called SentEval to automate evaluation on all the tasks mentioned in this paper .,Evaluation of sentence representations,TREC 6 k question - type,natural_language_inference,53,12,1,0,,0.053800438,0,negative,0.001224605,3.96E-06,0.002395528,0.000107085,1.43E-05,0.000374782,0.018081966,0.000130903,1.81E-05,0.768034714,3.75E-05,0.113795041,0.095781656
4570,natural_language_inference53,106,"The tool uses Adam ( Kingma and to fit a logistic regression classifier , with batch size 64 .",Evaluation of sentence representations,TREC 6 k question - type,natural_language_inference,53,13,1,0,,0.626121033,1,negative,0.001549324,1.42E-05,0.001837746,0.000494453,3.51E-06,0.022817128,0.114726989,0.029717974,0.00011262,0.664773613,1.61E-05,0.00737952,0.156556861
4571,natural_language_inference53,107,Binary and multi-class classification,Evaluation of sentence representations,,natural_language_inference,53,14,1,0,,0.520389675,1,negative,0.000117821,1.17E-06,0.000787704,4.17E-07,5.96E-08,1.59E-05,0.003865861,4.35E-05,7.07E-06,0.821579575,0.00030507,0.170577285,0.002698546
4572,natural_language_inference53,108,"We use a set of binary classification tasks ( see ) that covers various types of sentence classification , including sentiment analysis ( MR , SST ) , question - type ( TREC ) , product reviews ( CR ) , subjectivity / objectivity ( SUBJ ) and opinion polarity ( MPQA ) .",Evaluation of sentence representations,Binary and multi-class classification,natural_language_inference,53,15,1,0,,0.031301354,0,negative,0.000586647,1.26E-05,0.001802261,2.36E-05,9.26E-06,9.02E-05,0.002216407,0.000154549,1.42E-05,0.875201153,7.09E-05,0.11524787,0.004570285
4573,natural_language_inference53,109,We generate sentence vectors and train a logistic regression on top .,Evaluation of sentence representations,Binary and multi-class classification,natural_language_inference,53,16,1,0,,0.005650892,0,negative,7.04E-05,2.70E-06,0.000135054,7.72E-07,1.66E-07,0.000158502,0.000467347,0.002592958,4.11E-05,0.995178162,8.96E-07,0.001126048,0.000225881
4574,natural_language_inference53,110,"A linear classifier requires fewer parameters than an MLP and is thus suitable for small datasets , where transfer learning is especially well - suited .",Evaluation of sentence representations,Binary and multi-class classification,natural_language_inference,53,17,1,0,,0.003737701,0,negative,0.006628806,2.88E-06,0.000380864,1.95E-06,6.85E-07,1.88E-05,0.000499236,8.95E-05,8.80E-06,0.777737645,4.56E-06,0.21436764,0.000258643
4575,natural_language_inference53,111,We tune the L2 penalty of the logistic regression with grid - search on the validation set .,Evaluation of sentence representations,Binary and multi-class classification,natural_language_inference,53,18,1,0,,0.381142045,0,negative,0.000963155,1.33E-05,0.000151002,1.09E-05,9.85E-07,0.001284568,0.003615149,0.024690351,5.35E-05,0.962177446,2.55E-06,0.005684829,0.001352261
4576,natural_language_inference53,112,Entailment and semantic relatedness,,,natural_language_inference,53,0,1,0,,0.223799899,0,research-problem,5.06E-05,0.000100142,2.55E-05,1.49E-05,5.37E-06,3.58E-05,0.001141426,0.000186733,1.39E-05,0.116478874,0.880787797,0.001083338,7.56E-05
4577,natural_language_inference53,139,Empirical results,,,natural_language_inference,53,0,1,0,,0.167318371,0,negative,0.000135557,0.000161954,7.18E-06,7.43E-07,1.27E-06,0.000131529,0.000773802,0.002922979,2.39E-05,0.972983721,0.008782315,0.014063916,1.11E-05
4578,natural_language_inference53,140,"In this section , we refer to "" micro "" and "" macro "" averages of development set ( dev ) results on transfer tasks whose metrics is accuracy : we compute a "" macro "" aggregated score that corresponds to the classical average of dev accuracies , and the "" micro "" score that is a sum of the dev accuracies , weighted by the number of dev samples .",Empirical results,Empirical results,natural_language_inference,53,1,1,0,,0.002017449,0,negative,0.001419519,6.25E-05,0.006886088,7.55E-06,5.80E-06,9.70E-05,0.00133373,0.000310732,1.48E-05,0.830856018,0.000160828,0.158744629,0.000100727
4579,natural_language_inference53,141,Architecture impact,,,natural_language_inference,53,0,1,1,results,0.247031613,0,negative,0.003098682,0.000171359,2.75E-05,0.000906699,4.96E-05,0.000601874,0.000267921,0.001010209,0.000330552,0.991673098,0.001399425,0.000415168,4.79E-05
4580,natural_language_inference53,142,Model,,,natural_language_inference,53,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
4581,natural_language_inference53,143,We observe in that different models trained on the same NLI corpus lead to different transfer tasks results .,Model,Model,natural_language_inference,53,1,1,0,,0.112059642,0,negative,0.004392037,0.00015713,6.18E-05,8.28E-06,1.36E-05,0.000111161,0.000314815,0.001024399,0.000540143,0.959812129,0.000125171,0.033408815,3.05E-05
4582,natural_language_inference53,144,The BiLSTM - 4096 with the max - pooling operation performs best on both SNLI and transfer tasks .,Model,Model,natural_language_inference,53,2,1,1,results,0.777096131,1,results,0.009927905,4.37E-05,0.000328104,1.78E-05,1.21E-05,0.000362607,0.010314024,0.002270613,0.00010717,0.057785579,7.22E-05,0.918361614,0.000396524
4583,natural_language_inference53,145,"Looking at the micro and macro averages , we see that it performs significantly better than the other models LSTM , GRU , BiGRU - last , BiLSTM - Mean , inner-attention and the hierarchical - ConvNet. also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM - Mean for instance .",Model,Model,natural_language_inference,53,3,1,1,results,0.718996446,1,results,0.006159421,0.000143425,0.000228597,8.17E-05,3.33E-05,0.001137909,0.01660933,0.006690585,0.000498722,0.249446758,0.000391351,0.717600883,0.000978007
4584,natural_language_inference53,146,We hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general - purpose information of the input sentence .,Model,Model,natural_language_inference,53,4,1,0,,0.002476222,0,negative,8.63E-05,0.000212709,4.33E-05,7.08E-06,7.74E-06,0.00010714,2.96E-05,0.001439266,0.00683315,0.991077614,1.67E-05,0.000132705,6.71E-06
4585,natural_language_inference53,147,"For example , the inner-attention model has the ability to focus only on certain parts of a sentence thatare useful for the SNLI task , but not necessarily for the transfer tasks .",Model,Model,natural_language_inference,53,5,1,0,,0.000888701,0,negative,0.000228988,6.64E-05,0.000436596,4.13E-07,9.35E-07,3.11E-05,2.44E-05,0.000177186,0.008487127,0.990096913,2.84E-05,0.000418476,3.08E-06
4586,natural_language_inference53,148,"On the other hand , BiLSTM - Mean does not make sharp choices on which part of the sentence is more important than others .",Model,Model,natural_language_inference,53,6,1,0,,0.003450133,0,negative,0.00775593,9.83E-05,0.001135629,8.35E-06,4.70E-05,8.05E-05,0.001375122,0.000316279,0.000137482,0.669390449,8.61E-05,0.319512052,5.68E-05
4587,natural_language_inference53,149,The difference between the results seems to come from the different abilities of the models to incorporate general information while not focusing too much on specific features useful for the task at hand .,Model,Model,natural_language_inference,53,7,1,0,,0.004063925,0,negative,0.000592078,6.96E-05,3.48E-05,5.41E-06,7.40E-06,0.000146094,0.000197622,0.001275919,0.000593341,0.991515912,2.85E-05,0.005517597,1.58E-05
4588,natural_language_inference53,150,"For a given model , the transfer quality is also sensitive to the optimization algorithm : when training with Adam instead of SGD , we observed that the BiLSTM - max converged faster on SNLI ( 5 epochs instead of 10 ) , but obtained worse results on the transfer tasks , most likely because of the model and classifier 's increased capability to over-specialize on the training task .",Model,Model,natural_language_inference,53,8,1,1,results,0.271143013,0,results,0.020825501,0.000169719,0.000161182,2.42E-05,2.77E-05,0.000276667,0.003088199,0.002763287,0.000441689,0.395669697,0.000118976,0.576191996,0.000241123
4589,natural_language_inference53,151,"compares the over all performance of different architectures , showing the evolution of micro averaged performance with regard to the embedding size .",Model,Model,natural_language_inference,53,9,1,0,,0.013931292,0,negative,0.000418169,4.61E-05,0.000778106,4.95E-07,2.07E-06,2.35E-05,0.000108928,0.000148637,0.000606061,0.993101236,1.18E-05,0.004749875,5.01E-06
4590,natural_language_inference53,152,Embedding size,Model,,natural_language_inference,53,10,1,1,results,0.085683332,0,negative,0.000529508,0.001997644,0.000809546,9.12E-05,7.59E-05,0.005471632,0.00383411,0.078777707,0.078565474,0.826856429,0.000312893,0.001727515,0.000950436
4591,natural_language_inference53,153,"Since it is easier to linearly separate in high dimension , especially with logistic regression , it is not surprising that increased embedding sizes lead to increased performance for almost all models .",Model,Embedding size,natural_language_inference,53,11,1,1,results,0.02027477,0,negative,0.002109678,3.35E-05,0.000157894,1.24E-06,2.29E-06,0.000110275,8.79E-05,0.005623772,7.59E-06,0.933445998,3.87E-05,0.058375256,5.94E-06
4592,natural_language_inference53,154,"However , this is particularly true for some mod - :",Model,Embedding size,natural_language_inference,53,12,1,0,,1.37E-06,0,negative,2.27E-05,3.25E-06,3.24E-05,6.12E-08,8.99E-08,8.00E-06,1.40E-06,0.000442144,2.56E-06,0.999288919,4.67E-06,0.000193604,1.40E-07
4593,natural_language_inference53,155,Transfer test results for various architectures trained in different ways .,Model,Embedding size,natural_language_inference,53,13,1,0,,1.08E-05,0,negative,6.07E-06,2.05E-06,2.69E-05,1.14E-08,2.31E-08,6.56E-05,2.15E-05,0.006327041,2.25E-06,0.992379815,9.07E-06,0.00115919,4.72E-07
4594,natural_language_inference53,156,"Underlined are best results for transfer learning approaches , in bold are best results among the models trained in the same way .",Model,Embedding size,natural_language_inference,53,14,1,0,,3.38E-05,0,negative,6.41E-06,3.87E-06,1.97E-05,1.76E-07,2.16E-07,0.000216129,2.24E-05,0.010628866,5.74E-06,0.988706971,2.14E-06,0.000386433,9.49E-07
4595,natural_language_inference53,157,"indicates methods that we trained , other transfer models have been extracted from .",Model,Embedding size,natural_language_inference,53,15,1,0,,2.86E-07,0,negative,3.19E-06,1.61E-06,5.08E-06,9.18E-07,1.04E-06,7.36E-05,2.80E-06,0.001230418,2.91E-06,0.998664708,5.04E-07,1.30E-05,2.14E-07
4596,natural_language_inference53,158,"For best published supervised methods ( no transfer ) , we consider AdaSent , TF - KLD , Tree - LSTM and Illinois - LH system .",Model,Embedding size,natural_language_inference,53,16,1,0,,0.115759932,0,negative,0.000138058,0.000550979,0.069560035,3.16E-06,1.72E-05,0.000935694,0.000562727,0.027123602,0.000135283,0.898435961,2.15E-05,0.002490214,2.55E-05
4597,natural_language_inference53,159,"( * ) Our model trained on SST obtained 83.4 for MR and 86.0 for SST ( MR and SST come from the same source ) , which we do not put in the tables for fair comparison with transfer methods .",Model,Embedding size,natural_language_inference,53,17,1,0,,0.10996312,0,negative,0.00302087,4.60E-05,0.000549891,1.49E-05,3.24E-05,0.000485574,0.001314393,0.014173626,1.14E-05,0.691754072,2.28E-05,0.288508557,6.56E-05
4598,natural_language_inference53,160,"els ( BiLSTM - Max , HConvNet , inner-att ) , which demonstrate unequal abilities to incorporate more information as the size grows .",Model,Embedding size,natural_language_inference,53,18,1,0,,0.005107762,0,negative,5.41E-05,5.48E-05,0.025463812,4.94E-07,9.56E-07,0.000101093,0.000101901,0.003578577,5.67E-05,0.967740387,0.000165597,0.002668822,1.27E-05
4599,natural_language_inference53,161,We hypothesize that such networks are able to incorporate information that is not directly relevant to the objective task ( results on SNLI are relatively stable with regard to embedding size ) but that can nevertheless be useful as features for transfer tasks .,Model,Embedding size,natural_language_inference,53,19,1,0,,4.39E-05,0,negative,9.15E-06,4.00E-05,1.44E-05,3.58E-07,5.19E-07,6.92E-05,2.86E-06,0.007812843,0.000132302,0.991853496,2.80E-06,6.11E-05,9.51E-07
4600,natural_language_inference53,162,Task transfer,Model,,natural_language_inference,53,20,1,0,,0.57930886,1,results,0.002152261,0.000766574,0.004040585,9.44E-05,0.000229488,0.000547637,0.027600115,0.003596279,0.002185338,0.421092976,0.000527296,0.534315182,0.002851875
4601,natural_language_inference53,163,We report in transfer tasks results for different architectures trained in different ways .,Model,Task transfer,natural_language_inference,53,21,1,0,,0.000377981,0,negative,4.79E-05,3.42E-07,1.50E-05,3.19E-08,1.30E-07,1.59E-06,9.89E-05,4.24E-06,1.38E-06,0.996974655,2.60E-07,0.002853636,1.95E-06
4602,natural_language_inference53,164,We group models by the nature of the data on which they were trained .,Model,Task transfer,natural_language_inference,53,22,1,0,,2.15E-06,0,negative,8.20E-06,3.19E-07,9.54E-06,4.97E-08,1.11E-07,7.53E-06,3.44E-05,2.40E-05,8.00E-06,0.99986484,3.41E-08,4.21E-05,9.00E-07
4603,natural_language_inference53,165,The first group corresponds to models trained with unsupervised unordered sentences .,Model,Task transfer,natural_language_inference,53,23,1,0,,0.000481001,0,negative,0.000173436,8.26E-07,0.000710105,9.85E-08,2.62E-07,3.84E-06,0.000120718,6.02E-06,1.18E-05,0.998136393,4.93E-07,0.000830713,5.27E-06
4604,natural_language_inference53,166,"This includes bagof - words models such as word2 vec - SkipGram , the Unigram - TFIDF model , the Paragraph Vector model , the Sequential Denoising Auto - Encoder ( SDAE ) and the SIF model , all trained on the Toronto book corpus .",Model,Task transfer,natural_language_inference,53,24,1,0,,0.034722529,0,negative,0.00033846,4.90E-06,0.005500761,5.36E-07,2.56E-06,2.48E-05,0.002337638,2.62E-05,3.14E-05,0.987299643,7.90E-07,0.004378654,5.37E-05
4605,natural_language_inference53,167,The second group consists of models trained with unsupervised ordered sentences such as FastSent and SkipThought ( also trained on the Toronto book corpus ) .,Model,Task transfer,natural_language_inference,53,25,1,0,,0.000279032,0,negative,0.000161186,2.06E-06,0.001080581,9.90E-07,9.76E-07,2.89E-05,0.001012091,3.59E-05,1.90E-05,0.995794168,2.88E-06,0.001773226,8.81E-05
4606,natural_language_inference53,168,"We also include the FastSent variant "" FastSent + AE "" and the SkipThought - LN version that uses layer normalization .",Model,Task transfer,natural_language_inference,53,26,1,0,,0.001505829,0,negative,0.000850178,2.16E-05,0.021024253,7.84E-06,1.10E-05,0.000159057,0.011619558,0.000114853,0.000233926,0.954952444,3.03E-06,0.010395598,0.000606713
4607,natural_language_inference53,169,"We report results from models trained on supervised data in the third group , and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches .",Model,Task transfer,natural_language_inference,53,27,1,0,,0.000111912,0,negative,9.55E-05,5.71E-07,2.63E-05,5.93E-08,3.60E-07,1.75E-06,0.00015108,4.06E-06,1.83E-06,0.994558061,1.18E-07,0.00515773,2.65E-06
4608,natural_language_inference53,170,Comparison with SkipThought,Model,,natural_language_inference,53,28,1,1,results,0.687980032,1,negative,0.000650261,0.000246541,0.002895759,3.36E-07,3.72E-06,5.74E-05,0.00608929,0.000548623,0.001813037,0.787633674,4.08E-05,0.199921933,9.87E-05
4609,natural_language_inference53,171,"The best performing sentence encoder to date is the SkipThought - LN model , which was trained on a very large corpora of ordered sentences .",Model,Comparison with SkipThought,natural_language_inference,53,29,1,0,,0.012448445,0,negative,6.21E-05,3.23E-07,0.029260035,8.81E-07,6.79E-08,8.64E-06,0.000200617,4.26E-05,4.28E-06,0.941448788,4.23E-06,0.024906567,0.00406082
4610,natural_language_inference53,172,"With much less data ( 570 k compared to 64M sentences ) but with high - quality supervision from the SNLI dataset , we are able to consistently outperform the results obtained by SkipThought vectors .",Model,Comparison with SkipThought,natural_language_inference,53,30,1,1,results,0.004132399,0,results,0.000290793,1.27E-08,4.96E-05,1.42E-08,2.95E-09,1.42E-07,7.00E-05,2.21E-06,2.60E-08,0.096736124,3.32E-08,0.902807038,4.39E-05
4611,natural_language_inference53,173,We train our model in less than a day on a single GPU compared to the best SkipThought - LN network trained for a month .,Model,Comparison with SkipThought,natural_language_inference,53,31,1,0,,0.000750958,0,negative,0.000698681,9.26E-07,0.000633288,1.24E-05,4.79E-07,4.22E-05,0.0003736,0.000299974,7.36E-06,0.934363507,6.33E-07,0.050522448,0.013044518
4612,natural_language_inference53,174,"Our BiLSTM - max trained on SNLI performs much better than released SkipThought vectors on MR , CR , MPQA , SST , MRPC - accuracy , SICK - R , SICK - E and STS14 ( see ) .",Model,Comparison with SkipThought,natural_language_inference,53,32,1,1,results,0.042281642,0,results,0.000110516,6.30E-09,5.04E-05,2.24E-08,2.81E-09,2.62E-07,0.000149338,2.78E-06,1.41E-08,0.066499685,3.55E-08,0.933045548,0.000141406
4613,natural_language_inference53,175,"Except for the SUBJ dataset , it also performs better than SkipThought - LN on MR , CR and MPQA .",Model,Comparison with SkipThought,natural_language_inference,53,33,1,1,results,0.127799205,0,results,0.000171113,3.82E-09,1.83E-05,2.21E-08,2.12E-09,1.65E-07,0.000114067,1.65E-06,6.74E-09,0.035042575,2.45E-08,0.964546677,0.000105407
4614,natural_language_inference53,176,We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space ( pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST - LN ) .,Model,Comparison with SkipThought,natural_language_inference,53,34,1,1,results,0.056204049,0,results,0.00211613,1.01E-08,3.74E-05,1.87E-08,5.72E-09,1.28E-07,6.09E-05,1.57E-06,2.14E-08,0.123593789,1.34E-08,0.874155775,3.42E-05
4615,natural_language_inference53,177,We hypothesize that this is namely linked to the matching method of SNLI models which incorporates a notion of distance ( element - wise product and absolute difference ) during training .,Model,Comparison with SkipThought,natural_language_inference,53,35,1,0,,7.62E-07,0,negative,2.06E-05,7.21E-08,0.000115477,8.69E-08,1.11E-08,3.66E-07,8.24E-07,8.76E-06,3.27E-06,0.999504628,3.12E-08,0.000317991,2.79E-05
4616,natural_language_inference53,178,NLI as a supervised training set,Model,,natural_language_inference,53,36,1,1,results,0.202610206,0,negative,0.000124144,0.001091238,0.003050137,9.86E-07,5.89E-06,4.24E-05,0.001397931,0.000864866,0.019412487,0.929389383,0.001084139,0.043190164,0.000346189
4617,natural_language_inference53,179,"Our findings indicate that our model trained on SNLI obtains much better over all results than models trained on other supervised tasks such as COCO , dictionary definitions , NMT , PPDB and SST .",Model,NLI as a supervised training set,natural_language_inference,53,37,1,1,results,0.025724799,0,results,0.000244643,1.05E-07,3.93E-06,1.05E-07,2.59E-08,1.46E-06,0.000217734,4.61E-05,2.97E-07,0.278846863,1.26E-07,0.720577636,6.09E-05
4618,natural_language_inference53,180,"For SST , we tried exactly the same models as for SNLI ; it is worth noting that SST is smaller than NLI .",Model,NLI as a supervised training set,natural_language_inference,53,38,1,0,,4.90E-06,0,negative,5.67E-05,1.68E-07,4.20E-05,1.84E-08,3.43E-08,1.51E-06,1.22E-05,3.24E-05,1.53E-06,0.998269085,1.00E-08,0.001582396,1.98E-06
4619,natural_language_inference53,181,Our representations constitute higher - quality features for both classification and similarity tasks .,Model,NLI as a supervised training set,natural_language_inference,53,39,1,0,,0.00170352,0,negative,7.73E-05,4.83E-07,1.76E-05,3.91E-08,3.43E-08,4.69E-07,4.14E-06,1.48E-05,9.31E-06,0.99358566,2.20E-08,0.006285013,5.09E-06
4620,natural_language_inference53,182,"One explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence , and that the information required to perform NLI is generally discriminative and informative .",Model,NLI as a supervised training set,natural_language_inference,53,40,1,0,,3.70E-07,0,negative,4.17E-06,9.48E-08,2.53E-06,4.09E-08,7.57E-09,6.50E-07,8.58E-07,2.01E-05,1.82E-06,0.999837082,7.64E-08,0.00012927,3.31E-06
4621,natural_language_inference53,183,Domain adaptation on SICK tasks,Model,,natural_language_inference,53,41,1,1,results,0.881006984,1,results,0.000648015,0.000346796,0.001844366,1.25E-05,3.32E-05,0.000181816,0.039018349,0.001545333,0.001179899,0.214015079,0.000317837,0.736005843,0.004850914
4622,natural_language_inference53,184,Our transfer learning approach obtains better results than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,Model,Domain adaptation on SICK tasks,natural_language_inference,53,42,1,1,results,0.024680274,0,results,0.000221534,8.58E-07,1.62E-05,9.25E-08,1.66E-07,2.58E-06,0.002079619,2.15E-05,8.38E-07,0.239445573,4.20E-07,0.758170149,4.04E-05
4623,natural_language_inference53,185,"We obtain a pearson score of 0.885 on SICK - R while obtained 0.868 , and we obtain 86.3 % test accuracy on SICK - E while previous best handengineered models obtained 84.5 % .",Model,Domain adaptation on SICK tasks,natural_language_inference,53,43,1,1,results,0.010445009,0,results,0.00064022,4.46E-07,8.24E-06,6.38E-08,2.43E-07,2.12E-06,0.001710336,2.26E-05,6.76E-07,0.374312872,4.84E-08,0.623277657,2.44E-05
4624,natural_language_inference53,186,"We also significantly outperformed previous transfer learning approaches on SICK - E ( Bowman et al. , 2015 ) that used the parameters of an LSTM model trained on SNLI to fine - tune on SICK ( 80.8 % accuracy ) .",Model,Domain adaptation on SICK tasks,natural_language_inference,53,44,1,1,results,0.048872057,0,results,0.000350252,8.12E-07,2.41E-05,3.01E-07,4.71E-07,6.77E-06,0.003998502,3.66E-05,1.20E-06,0.237371592,2.04E-07,0.758115266,9.40E-05
4625,natural_language_inference53,187,"We hypothesize that our embeddings already contain the information learned from the in - domain task , and that learning only the classifier limits the number of parameters learned on the small out - domain task .",Model,Domain adaptation on SICK tasks,natural_language_inference,53,45,1,0,,1.42E-06,0,negative,2.09E-06,3.44E-07,1.10E-06,2.33E-08,2.76E-08,1.10E-06,2.02E-06,1.97E-05,1.49E-05,0.999937619,7.36E-09,2.08E-05,2.25E-07
4626,natural_language_inference53,188,Image - caption retrieval results,Model,,natural_language_inference,53,46,1,1,results,0.684769997,1,results,0.000402625,2.35E-05,0.000303198,3.50E-07,4.17E-06,1.35E-05,0.010600517,0.00013044,5.03E-05,0.112393022,4.06E-06,0.875750695,0.000323493
4627,natural_language_inference53,189,"In , we report results for the COCO image - caption retrieval task .",Model,Image - caption retrieval results,natural_language_inference,53,47,1,0,,0.073785286,0,negative,5.61E-05,3.23E-07,3.43E-05,7.63E-08,4.15E-07,3.92E-06,0.000582804,4.00E-06,3.17E-07,0.96515588,1.05E-07,0.034119304,4.25E-05
4628,natural_language_inference53,190,We report the mean recalls of 5 random splits of 1 K test images .,Model,Image - caption retrieval results,natural_language_inference,53,48,1,0,,8.08E-06,0,negative,1.57E-06,6.36E-09,3.09E-07,7.16E-10,6.91E-09,5.83E-07,1.74E-05,1.43E-06,2.56E-08,0.999762664,5.01E-10,0.000215857,1.92E-07
4629,natural_language_inference53,191,"When trained with ResNet features and 30 k more training data , the SkipThought vectors perform significantly better than the original setting , going from 33.8 to 37.9 for caption retrieval R@1 , and from 25.9 to 30.6 on image retrieval R@1 .",Model,Image - caption retrieval results,natural_language_inference,53,49,1,1,results,0.251868943,0,results,0.001579088,1.31E-07,5.20E-06,1.69E-07,9.91E-08,2.23E-05,0.018836742,6.33E-05,2.15E-07,0.313734177,4.43E-08,0.665416946,0.00034161
4630,natural_language_inference53,192,"Our approach pushes the results even further , from 37.9 to 42.4 on cap-tion retrieval , and 30.6 to 33.2 on image retrieval .",Model,Image - caption retrieval results,natural_language_inference,53,50,1,1,results,0.022344119,0,negative,0.001228208,7.51E-08,9.51E-06,4.26E-08,9.03E-08,4.41E-06,0.004406373,7.77E-06,1.92E-07,0.532420348,2.09E-08,0.461784296,0.000138664
4631,natural_language_inference53,193,These results are comparable to previous approach of that did not do transfer but directly learned the sentence encoding on the imagecaption retrieval task .,Model,Image - caption retrieval results,natural_language_inference,53,51,1,0,,7.22E-05,0,negative,8.13E-05,1.12E-08,2.28E-06,2.32E-09,1.53E-08,6.27E-07,0.000443638,1.07E-06,4.23E-08,0.925842899,4.38E-09,0.07362407,4.05E-06
4632,natural_language_inference53,194,This supports the claim that pre-trained representations such as ResNet image features and our sentence embeddings can achieve competitive results compared to features learned directly on the objective task .,Model,Image - caption retrieval results,natural_language_inference,53,52,1,0,,0.000146679,0,negative,1.70E-05,5.50E-09,5.43E-07,9.65E-10,3.47E-09,4.54E-07,3.64E-05,9.38E-07,4.51E-08,0.997941924,1.74E-09,0.00200207,5.60E-07
4633,natural_language_inference53,195,MultiGenre NLI,Model,Image - caption retrieval results,natural_language_inference,53,53,1,1,results,0.00072301,0,negative,9.51E-05,2.85E-07,0.000132578,9.63E-08,1.34E-07,1.54E-05,0.017369717,1.06E-05,7.54E-07,0.797393756,1.14E-06,0.18417219,0.000808201
4634,natural_language_inference53,196,The MultiNLI corpus was recently released as a multi-genre version of SNLI .,Model,Image - caption retrieval results,natural_language_inference,53,54,1,0,,0.000298666,0,negative,3.79E-06,1.78E-08,9.25E-06,1.07E-07,6.72E-07,5.29E-06,7.90E-05,1.89E-06,7.29E-08,0.999367049,6.23E-09,0.00049871,3.41E-05
4635,natural_language_inference53,197,"With 433K sentence pairs , MultiNLI improves upon SNLI in its coverage : it contains ten distinct genres of written and spoken English , covering most of the complexity of the language .",Model,Image - caption retrieval results,natural_language_inference,53,55,1,0,,0.189765525,0,negative,0.000271919,4.36E-07,9.77E-05,1.37E-06,1.38E-05,2.44E-05,0.002781325,8.44E-06,6.02E-07,0.958157006,1.96E-08,0.037937806,0.000705112
4636,natural_language_inference53,198,We augment with our model trained on both SNLI and MultiNLI ( All NLI ) .,Model,Image - caption retrieval results,natural_language_inference,53,56,1,0,,0.000712222,0,negative,0.000137362,7.77E-07,0.000296568,1.90E-08,1.75E-07,8.97E-06,0.000532724,1.87E-05,1.28E-05,0.998455089,2.12E-09,0.000520156,1.67E-05
4637,natural_language_inference53,199,We observe a significant boost in performance over all compared to the model trained only on SLNI .,Model,Image - caption retrieval results,natural_language_inference,53,57,1,1,results,0.081756363,0,results,0.0031889,8.71E-08,9.57E-06,1.02E-07,1.10E-07,4.87E-06,0.007821244,1.14E-05,1.86E-07,0.334358902,9.82E-09,0.654314643,0.000289942
4638,natural_language_inference53,200,"Our model even reaches AdaSent performance on CR , suggesting that having a larger coverage for the training task helps learn even better general representations .",Model,Image - caption retrieval results,natural_language_inference,53,58,1,1,results,0.194692717,0,results,0.001112013,4.44E-08,7.00E-06,3.06E-08,4.20E-08,4.06E-06,0.007388473,9.07E-06,9.29E-08,0.268445163,8.58E-09,0.722842917,0.000191082
4639,natural_language_inference53,201,"On semantic textual similarity STS14 , we are also competitive with PPDB based paragramphrase embeddings with a pearson score of 0.70 .",Model,Image - caption retrieval results,natural_language_inference,53,59,1,0,,0.15792253,0,results,0.000343048,3.38E-08,1.49E-05,1.13E-08,3.90E-08,3.64E-06,0.008789334,4.89E-06,4.68E-08,0.300675691,6.79E-09,0.69008031,8.80E-05
4640,natural_language_inference53,202,"Interestingly , on caption - related transfer tasks such as the COCO image caption retrieval task , training our sentence encoder on other genres from MultiNLI does not degrade the performance compared to the model trained only SNLI ( which contains mostly captions ) , which confirms the generalization power of our embeddings .",Model,Image - caption retrieval results,natural_language_inference,53,60,1,0,,0.153323946,0,negative,0.002286478,3.28E-08,1.82E-06,9.87E-09,2.93E-08,2.30E-06,0.003972094,6.03E-06,4.42E-08,0.539031443,4.10E-09,0.454669028,3.07E-05
4641,natural_language_inference53,203,Conclusion,,,natural_language_inference,53,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
4642,sentiment_analysis8,1,title,,,sentiment_analysis,8,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
4643,sentiment_analysis8,2,Multimodal Speech Emotion Recognition and Ambiguity Resolution,title,,sentiment_analysis,8,1,1,1,research-problem,0.989197422,1,research-problem,6.74E-08,5.14E-06,8.25E-08,3.14E-07,2.09E-07,1.81E-07,4.68E-06,1.38E-06,2.47E-07,0.002257703,0.997729267,5.67E-07,1.64E-07
4644,sentiment_analysis8,3,abstract,,,sentiment_analysis,8,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
4645,sentiment_analysis8,4,Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .,abstract,abstract,sentiment_analysis,8,1,1,1,research-problem,0.916870734,1,research-problem,5.14E-08,5.38E-06,1.60E-08,5.86E-06,1.57E-06,3.61E-07,5.82E-07,9.72E-07,2.40E-07,0.016074514,0.983910353,2.29E-08,8.47E-08
4646,sentiment_analysis8,5,"In this work , we adopt a featureengineering based approach to tackle the task of speech emotion recognition .",abstract,abstract,sentiment_analysis,8,2,1,0,,0.838262784,1,research-problem,1.14E-05,0.044074663,3.75E-05,2.13E-05,0.000144997,1.46E-05,1.72E-05,0.000138015,0.000936212,0.063444411,0.891150946,5.66E-06,3.02E-06
4647,sentiment_analysis8,6,"Formalizing our problem as a multi-class classification problem , we compare the performance of two categories of models .",abstract,abstract,sentiment_analysis,8,3,1,0,,0.0471474,0,negative,6.17E-06,0.114914102,2.00E-05,1.99E-05,0.000395731,2.71E-05,1.19E-05,0.000433824,0.002803584,0.52442258,0.356938046,5.60E-06,1.48E-06
4648,sentiment_analysis8,7,"For both , we extract eight hand - crafted features from the audio signal .",abstract,abstract,sentiment_analysis,8,4,1,0,,0.220926499,0,negative,1.08E-05,0.180696404,1.57E-05,0.000211625,0.001767648,0.003288593,7.44E-05,0.041637938,0.01066528,0.74144769,0.020173867,2.45E-06,7.61E-06
4649,sentiment_analysis8,8,"In the first approach , the extracted features are used to train six traditional machine learning classifiers , whereas the second approach is based on deep learning wherein a baseline feed - forward neural network and an LSTM - based classifier are trained over the same features .",abstract,abstract,sentiment_analysis,8,5,1,0,,0.201140333,0,negative,3.26E-05,0.28872615,0.000260227,2.01E-05,0.000733705,5.24E-05,1.81E-05,0.000530606,0.015317497,0.561355908,0.1329429,7.70E-06,2.08E-06
4650,sentiment_analysis8,9,"In order to resolve ambiguity in communication , we also include features from the text domain .",abstract,abstract,sentiment_analysis,8,6,1,0,,0.137586866,0,negative,4.39E-05,0.23379816,0.000140029,7.45E-05,0.003015202,0.000507333,3.89E-05,0.002943823,0.035911908,0.710458659,0.013053253,1.01E-05,4.15E-06
4651,sentiment_analysis8,10,"We report accuracy , f- score , precision and recall for the different experiment settings we evaluated our models in .",abstract,abstract,sentiment_analysis,8,7,1,0,,0.001467935,0,negative,5.98E-07,0.000336854,1.18E-07,1.06E-05,8.31E-05,1.92E-05,2.00E-06,0.000101218,1.24E-05,0.990030685,0.00940238,7.88E-07,1.06E-07
4652,sentiment_analysis8,11,"Overall , we show that lighter machine learning based models trained over a few hand - crafted features are able to achieve performance comparable to the current deep learning based stateof - the - art method for emotion recognition .",abstract,abstract,sentiment_analysis,8,8,1,0,,0.041146243,0,research-problem,8.88E-05,0.0064431,5.50E-06,7.58E-05,0.000168407,8.45E-05,0.000219942,0.001461304,0.000187925,0.400024745,0.590771317,0.000450336,1.83E-05
4653,sentiment_analysis8,12,I. INTRODUCTION,,,sentiment_analysis,8,0,1,0,,0.000196916,0,negative,0.000783432,0.00042639,3.86E-05,0.010720081,7.66E-05,0.002184256,0.00040315,0.002121679,0.000468556,0.974938341,0.007667188,4.70E-05,0.000124609
4654,sentiment_analysis8,13,"Communication is the key to human existence and more often than not , we have to deal with ambiguous situations .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,1,1,0,,0.662821484,1,research-problem,5.68E-07,0.000215319,2.60E-07,1.62E-05,5.02E-05,6.49E-06,3.36E-06,4.46E-06,6.95E-05,0.090585976,0.909046672,3.22E-07,7.02E-07
4655,sentiment_analysis8,14,"For instance , the phrase "" This is awesome "" could be said under either happy or sad settings .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,2,1,0,,0.003277155,0,negative,2.07E-06,0.000696697,6.74E-07,4.31E-05,0.000448883,9.18E-05,5.61E-06,4.78E-05,0.000827196,0.957090256,0.040744499,6.53E-07,8.02E-07
4656,sentiment_analysis8,15,"Humans are able to resolve ambiguity in most cases because we can efficiently comprehend information from multiple domains ( henceforth , referred to as modalities ) , namely , speech , text and visual .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,3,1,0,,0.418781499,0,research-problem,3.33E-07,0.000176173,1.15E-07,4.11E-06,1.09E-05,4.22E-06,1.99E-06,4.47E-06,9.27E-05,0.079376987,0.920327401,2.43E-07,3.37E-07
4657,sentiment_analysis8,16,"With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,4,1,1,research-problem,0.953549008,1,research-problem,2.15E-07,6.62E-05,7.69E-08,1.14E-06,3.20E-06,9.06E-07,1.58E-06,1.31E-06,1.89E-05,0.023383986,0.976522144,1.87E-07,2.20E-07
4658,sentiment_analysis8,17,"However , this rise has made practitioners rely more on the power of the deep learning models as opposed to using domain knowledge to construct meaningful features and building models that perform well as well as are interpretable .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,5,1,0,,0.043618622,0,negative,8.43E-06,0.001468254,1.68E-06,8.17E-05,0.000476927,7.09E-05,9.31E-06,3.66E-05,0.000713432,0.813293284,0.183836274,1.65E-06,1.53E-06
4659,sentiment_analysis8,18,"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,6,1,1,model,0.931790817,1,approach,6.73E-05,0.775286352,6.49E-05,1.85E-05,0.002781734,4.60E-05,3.75E-05,0.000130454,0.108800242,0.079066193,0.033679271,1.78E-05,3.67E-06
4660,sentiment_analysis8,19,"Furthermore , we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,7,1,1,model,0.722448036,1,model,1.21E-05,0.116219464,3.27E-05,1.79E-06,0.000294076,1.83E-05,4.40E-06,3.03E-05,0.868816421,0.013818609,0.000749242,1.93E-06,6.09E-07
4661,sentiment_analysis8,20,"More formally , we pose our task as a multi-class classification problem and employ the two classes of models to solve that .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,8,1,0,,0.216904377,0,model,6.80E-06,0.275260662,1.56E-05,7.01E-06,0.000539262,3.25E-05,5.42E-06,8.78E-05,0.573957624,0.126898846,0.023185417,2.06E-06,1.04E-06
4662,sentiment_analysis8,21,"For both the approaches , we first extract handcrafted features from the time domain of the audio signal and train the respective models .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,9,1,1,model,0.926430978,1,approach,2.79E-05,0.609812662,5.70E-05,1.50E-05,0.003230116,8.33E-05,1.55E-05,0.000188473,0.34332056,0.042019958,0.001223827,3.82E-06,1.92E-06
4663,sentiment_analysis8,22,"In the first approach , we train traditional machine learning classifiers , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes and Logistic Regression .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,10,1,1,model,0.850594322,1,approach,9.32E-05,0.770703939,0.00110609,9.49E-05,0.022643419,0.000246205,0.000131702,0.000211181,0.137272335,0.063161736,0.004304263,2.08E-05,1.02E-05
4664,sentiment_analysis8,23,"In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a speech signal .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,11,1,1,model,0.915847009,1,model,2.22E-05,0.21751078,0.000285207,3.60E-06,0.000620153,2.83E-05,1.93E-05,3.32E-05,0.761281065,0.016458918,0.003729612,5.67E-06,1.99E-06
4665,sentiment_analysis8,24,"The models are evaluated on the IEMOCAP dataset under different settings , namely , Audio- only , Text - only and Audio + Text 1 .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,12,1,0,,0.080623974,0,approach,4.55E-05,0.741040173,0.000119907,6.23E-06,0.003297668,0.000157376,0.000221419,0.000442921,0.095520389,0.153393006,0.005682486,6.67E-05,6.25E-06
4666,sentiment_analysis8,25,The rest of the paper is organized as follows :,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,13,1,0,,0.00179669,0,negative,6.07E-06,0.006739065,4.07E-06,0.000693152,0.00293415,0.000563993,2.74E-05,0.000280917,0.005721435,0.964848595,0.018174811,1.20E-06,5.10E-06
4667,sentiment_analysis8,26,"Section II describes existing methods in the literature for the task of speech emotion recognition ; Section III gives an overview of the dataset used in this work and the pre-processing steps applied before feature extraction ; Section IV describes the proposed models and implementation details ; Results are reported in Section V , followed by the conclusion and future scope of this work in Section VI .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,14,1,0,,0.022089171,0,negative,5.56E-06,0.044160757,8.25E-06,6.30E-05,0.008996127,0.000104316,2.16E-05,9.52E-05,0.008232727,0.880683281,0.057622679,3.62E-06,2.83E-06
4668,sentiment_analysis8,27,II .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,15,1,0,,0.003902939,0,negative,4.31E-05,0.01031532,1.99E-05,3.47E-05,0.001139143,0.000188606,1.66E-05,0.00010276,0.027292484,0.957266337,0.003575848,3.76E-06,1.40E-06
4669,sentiment_analysis8,28,LITERATURE REVIEW,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,16,1,0,,0.004771409,0,negative,4.14E-06,0.006357147,2.80E-06,1.98E-05,0.000192189,0.000297293,2.67E-05,0.000396625,0.020228366,0.923842286,0.048627941,1.97E-06,2.84E-06
4670,sentiment_analysis8,29,"In this section , we review some of the work that has been done in the field of speech emotion recognition ( SER ) .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,17,1,0,,0.593632437,1,research-problem,5.65E-06,0.009169341,4.86E-06,5.25E-05,0.000977569,5.77E-05,4.05E-05,5.13E-05,0.001632415,0.447007043,0.540991533,4.50E-06,4.94E-06
4671,sentiment_analysis8,30,The task of SER is not new and has been studied for quite sometime in literature .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,18,1,0,,0.567442336,1,research-problem,8.22E-07,0.000809657,7.85E-07,2.17E-05,6.22E-05,2.46E-05,1.22E-05,2.37E-05,0.000341414,0.230247299,0.76845264,9.02E-07,2.11E-06
4672,sentiment_analysis8,31,A majority of the early approaches ( [ 6 ] ) used Hidden Markov Models ( HMMs ) for identifying emotion from speech .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,19,1,0,,0.036027102,0,negative,5.28E-06,0.001761084,3.95E-06,7.32E-05,0.000326501,0.000167703,4.87E-05,8.22E-05,0.000711535,0.536586767,0.460222723,2.96E-06,7.34E-06
4673,sentiment_analysis8,32,Recent introduction of deep neural networks to the domain has also significantly improved the state - of - the - art performance .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,20,1,0,,0.029177049,0,negative,1.35E-05,0.001994984,6.19E-06,0.000224466,0.00072299,0.000327393,8.54E-05,0.000124707,0.001559721,0.890793014,0.104129007,7.51E-06,1.11E-05
4674,sentiment_analysis8,33,"For instance , and use recurrent autoencoders to solve the task .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,21,1,0,,0.017073502,0,negative,3.10E-06,0.003699838,8.53E-06,1.96E-05,0.000408886,0.000165423,2.87E-05,9.37E-05,0.009630015,0.957015815,0.028921272,2.71E-06,2.51E-06
4675,sentiment_analysis8,34,"Recently , methods have also been proposed to efficiently combine features from multiple domains , such as , Tensor Fusion Networks and Low - Rank Matrix Multiplication , instead of trivial concatenation .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,22,1,0,,0.020617402,0,negative,4.64E-06,0.004505538,3.81E-06,3.77E-05,0.000130425,0.000174039,4.49E-05,0.000172447,0.003639108,0.69760557,0.293672377,3.97E-06,5.42E-06
4676,sentiment_analysis8,35,"This work aims to provide a comparative study between 1 ) deep learning based models thatare trained end - to - end , and 2 ) lighter machine learning and deep learning based models trained over handcrafted features .",I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,23,1,0,,0.812750719,1,approach,3.17E-05,0.664040159,0.000168546,3.66E-05,0.009578285,0.000126712,8.24E-05,0.000167332,0.130319239,0.18116728,0.014251039,2.36E-05,7.14E-06
4677,sentiment_analysis8,36,We also investigate the information residing in multiple modalities and how their combination affects the performance .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,24,1,0,,0.281402076,0,model,3.91E-05,0.211395876,2.57E-05,2.89E-06,0.000508326,4.79E-05,1.76E-05,0.000108391,0.62024037,0.166555689,0.00104958,7.46E-06,9.73E-07
4678,sentiment_analysis8,37,III .,I. INTRODUCTION,I. INTRODUCTION,sentiment_analysis,8,25,1,0,,0.003973465,0,negative,7.27E-05,0.01530691,2.49E-05,1.90E-05,0.00111267,0.000199729,2.84E-05,0.000138334,0.055165331,0.926808109,0.001115299,7.03E-06,1.60E-06
4679,sentiment_analysis8,38,DATASET,I. INTRODUCTION,,sentiment_analysis,8,26,1,0,,0.002255634,0,negative,3.80E-06,0.001513528,3.80E-06,0.000111011,0.000699855,0.000653454,8.09E-05,0.00031296,0.003224078,0.986011547,0.007373265,4.50E-06,7.34E-06
4680,sentiment_analysis8,39,"In this work , we use the IEMOCAP released in 2008 by researchers at the University of Southern California ( USC ) .",I. INTRODUCTION,DATASET,sentiment_analysis,8,27,1,0,,0.45131997,0,dataset,2.44E-06,0.120412169,0.000114706,0.000129555,0.799756799,0.000862038,7.95E-05,9.16E-05,0.004519963,0.073801061,0.00022435,2.99E-06,2.88E-06
4681,sentiment_analysis8,40,It contains five recorded sessions of conversations from ten speakers and amounts to nearly 12 hours of audio- visual information along with transcriptions .,I. INTRODUCTION,DATASET,sentiment_analysis,8,28,1,0,,0.49894688,0,dataset,2.78E-07,0.000507854,2.02E-06,0.00051949,0.993820372,8.16E-05,4.45E-06,1.17E-06,9.64E-06,0.005048298,4.52E-06,7.63E-08,2.42E-07
4682,sentiment_analysis8,41,"It is annotated with eight categorical emotion labels , namely , anger , happiness , sadness , neutral , surprise , fear , frustration and excited .",I. INTRODUCTION,DATASET,sentiment_analysis,8,29,1,0,,0.389769398,0,dataset,1.52E-06,0.054404784,4.88E-05,6.32E-05,0.784170782,0.000895431,3.73E-05,0.000111123,0.005142242,0.155054216,6.82E-05,8.39E-07,1.67E-06
4683,sentiment_analysis8,42,"It also contains dimensional labels such as values of the activation and valence from 1 to 5 ; however , they are not used in this work .",I. INTRODUCTION,DATASET,sentiment_analysis,8,30,1,0,,0.149499629,0,negative,1.08E-05,0.064735038,8.22E-05,1.70E-05,0.025816947,0.001466934,3.63E-05,0.000264527,0.042854547,0.864437886,0.000274582,2.24E-06,9.10E-07
4684,sentiment_analysis8,43,The dataset is already split into multiple utterances for each session and we further split each utterance file to obtain wav files for each sentence .,I. INTRODUCTION,DATASET,sentiment_analysis,8,31,1,0,,0.405116606,0,dataset,9.09E-07,0.005651062,7.68E-06,0.000107793,0.956494073,0.000306151,1.56E-05,1.61E-05,0.000192758,0.037198339,8.71E-06,3.41E-07,4.47E-07
4685,sentiment_analysis8,44,This was done using the start timestamp and end timestamp provided for the transcribed sentences .,I. INTRODUCTION,DATASET,sentiment_analysis,8,32,1,0,,0.327269584,0,negative,8.25E-06,0.129440795,3.39E-05,3.28E-05,0.030992942,0.003566896,8.47E-05,0.000833259,0.074901029,0.759647131,0.000453956,1.49E-06,2.81E-06
4686,sentiment_analysis8,45,This results in a total of ? 10 K audio files which are then used to extract features .,I. INTRODUCTION,DATASET,sentiment_analysis,8,33,1,0,,0.456886884,0,dataset,4.32E-06,0.015948325,1.36E-05,0.00035976,0.806266612,0.002194183,8.87E-05,0.000172232,0.000978473,0.173922006,4.72E-05,9.10E-07,3.66E-06
4687,sentiment_analysis8,46,IV .,I. INTRODUCTION,DATASET,sentiment_analysis,8,34,1,0,,0.004002243,0,negative,2.42E-05,0.014497827,1.07E-05,9.40E-06,0.001871547,0.00046345,1.87E-05,0.000157118,0.017281525,0.965498421,0.000165194,1.49E-06,3.86E-07
4688,sentiment_analysis8,47,METHODOLOGY,,,sentiment_analysis,8,0,1,0,,0.043255215,0,negative,1.91E-05,0.000159627,1.45E-05,1.37E-06,4.99E-07,0.000139902,0.000211372,0.00286638,0.000149264,0.913404109,0.082542361,0.000474296,1.72E-05
4689,sentiment_analysis8,48,This section describes the data pre-processing steps followed by a detailed description of the features extracted and the two models applied to the classification problem .,METHODOLOGY,METHODOLOGY,sentiment_analysis,8,1,1,0,,1.27E-06,0,negative,6.08E-06,2.92E-05,1.40E-06,3.03E-07,3.94E-07,6.21E-06,1.18E-06,8.97E-05,1.47E-05,0.999834252,7.24E-06,9.18E-06,7.69E-08
4690,sentiment_analysis8,49,A.,METHODOLOGY,,sentiment_analysis,8,2,1,0,,6.07E-07,0,negative,1.18E-05,2.20E-06,2.16E-06,7.65E-08,3.72E-08,6.49E-06,9.96E-07,5.46E-05,6.60E-06,0.999902897,3.89E-06,8.22E-06,4.75E-08
4691,sentiment_analysis8,50,Data Pre-processing a),METHODOLOGY,A.,sentiment_analysis,8,3,1,0,,0.000201555,0,negative,7.39E-05,1.10E-05,1.17E-05,4.53E-07,1.11E-06,1.13E-05,3.95E-06,2.55E-05,2.09E-05,0.999354022,0.000408597,7.55E-05,2.09E-06
4692,sentiment_analysis8,51,Audio :,METHODOLOGY,A.,sentiment_analysis,8,4,1,0,,9.56E-05,0,negative,0.000916749,9.29E-06,0.000401256,3.76E-07,1.34E-06,9.48E-06,1.45E-05,1.55E-05,2.10E-05,0.995147041,0.000253699,0.003206064,3.75E-06
4693,sentiment_analysis8,52,A preliminary frequency analysis revealed that the dataset is not balanced .,METHODOLOGY,A.,sentiment_analysis,8,5,1,0,,9.76E-06,0,negative,5.72E-05,8.25E-07,4.70E-07,1.78E-07,1.28E-06,3.11E-06,3.30E-07,4.56E-06,1.60E-06,0.999905883,1.97E-06,2.25E-05,8.86E-08
4694,sentiment_analysis8,53,"The emotions "" fear "" and "" surprise "" were under-represented and use upsampling techniques to alleviate the issue .",METHODOLOGY,A.,sentiment_analysis,8,6,1,0,,4.29E-06,0,negative,0.000897754,2.60E-06,6.27E-06,1.83E-06,7.29E-06,7.37E-06,6.42E-07,4.42E-06,5.85E-06,0.999022844,3.46E-06,3.94E-05,2.52E-07
4695,sentiment_analysis8,54,"We then merged examples from "" happy "" and "" excited "" classes as "" happy "" was under-represented and the two emotions closely resemble each other .",METHODOLOGY,A.,sentiment_analysis,8,7,1,0,,1.88E-05,0,negative,4.28E-05,2.06E-05,3.08E-05,4.55E-07,2.07E-05,2.08E-05,1.52E-06,3.61E-05,2.91E-05,0.999779352,1.51E-06,1.58E-05,3.19E-07
4696,sentiment_analysis8,55,"In addition to that , we discard examples classified as "" others "" ; they corresponded to examples that were labeled ambiguous even for a human .",METHODOLOGY,A.,sentiment_analysis,8,8,1,0,,3.29E-06,0,negative,2.89E-05,6.75E-06,5.17E-06,2.75E-07,8.81E-06,8.22E-06,4.40E-07,1.91E-05,8.17E-06,0.999906637,6.75E-07,6.78E-06,8.15E-08
4697,sentiment_analysis8,56,Applying the aforementioned operations resulted in 7837 examples in total .,METHODOLOGY,A.,sentiment_analysis,8,9,1,0,,0.00027252,0,negative,0.00019205,4.70E-05,6.67E-06,3.67E-05,0.00021462,9.04E-05,5.80E-06,7.46E-05,3.05E-05,0.999256031,1.11E-05,3.01E-05,4.51E-06
4698,sentiment_analysis8,57,Final sample distribution for each of the emotions is shown in .,METHODOLOGY,A.,sentiment_analysis,8,10,1,0,,2.02E-05,0,negative,3.82E-05,1.47E-05,2.12E-06,1.06E-07,1.09E-06,1.31E-05,1.88E-06,0.000102895,3.53E-05,0.999721162,6.72E-06,6.24E-05,3.93E-07
4699,sentiment_analysis8,58,b),METHODOLOGY,A.,sentiment_analysis,8,11,1,0,,8.21E-07,0,negative,0.000103453,3.68E-06,3.84E-05,3.78E-08,2.56E-07,3.42E-06,8.83E-07,1.04E-05,3.48E-05,0.999749286,6.59E-06,4.85E-05,3.65E-07
4700,sentiment_analysis8,59,Text :,METHODOLOGY,A.,sentiment_analysis,8,12,1,0,,7.97E-07,0,negative,8.34E-06,6.05E-06,1.22E-06,7.26E-06,1.73E-06,1.97E-05,6.32E-07,3.88E-05,3.87E-05,0.999839617,3.55E-05,1.00E-06,1.40E-06
4701,sentiment_analysis8,60,The available transcriptions were first normalized to lowercase and any special symbols were removed .,METHODOLOGY,A.,sentiment_analysis,8,13,1,0,,4.70E-05,0,negative,8.70E-05,3.68E-05,3.01E-05,3.84E-07,1.29E-05,2.65E-05,2.26E-06,6.76E-05,8.62E-05,0.999626683,1.39E-06,2.15E-05,5.75E-07
4702,sentiment_analysis8,61,B. Feature Extraction,METHODOLOGY,,sentiment_analysis,8,14,1,0,,0.000107801,0,negative,1.35E-05,4.88E-05,3.96E-05,1.06E-07,1.93E-07,2.06E-05,1.05E-05,0.000354574,7.68E-05,0.999235572,5.20E-05,0.00014692,8.29E-07
4703,sentiment_analysis8,62,"We now describe the handcrafted features used to train both , the ML - and the DL - based models .",METHODOLOGY,B. Feature Extraction,sentiment_analysis,8,15,1,0,,2.74E-06,0,negative,1.46E-06,0.000187742,2.07E-07,4.66E-07,9.71E-06,4.94E-05,3.87E-07,5.71E-05,9.05E-05,0.999599717,2.85E-06,4.54E-07,2.04E-08
4704,sentiment_analysis8,63,1 ) Audio Features : a) Pitch : Pitch is important because waveforms produced by our vocal cords change depending on our emotion .,METHODOLOGY,B. Feature Extraction,sentiment_analysis,8,16,1,0,,0.013225647,0,negative,0.000279149,0.000919245,0.000129143,4.34E-07,1.74E-05,9.95E-05,1.59E-05,6.81E-05,0.000625302,0.996682925,0.000988946,0.000173133,7.82E-07
4705,sentiment_analysis8,64,Many algorithms for estimating the pitch signal exist .,METHODOLOGY,B. Feature Extraction,sentiment_analysis,8,17,1,0,,3.07E-05,0,negative,1.51E-06,4.45E-05,3.08E-07,1.23E-06,3.09E-06,4.88E-05,1.78E-06,3.40E-05,1.85E-05,0.99479817,0.005047025,7.60E-07,2.47E-07
4706,sentiment_analysis8,65,We use the most common method based on autocorrelation of center - clipped frames .,METHODOLOGY,B. Feature Extraction,sentiment_analysis,8,18,1,0,,0.000126646,0,negative,1.66E-05,0.000542552,1.92E-05,1.15E-05,0.000141314,0.000428483,9.28E-06,0.000112344,0.000141753,0.998435626,0.000136411,4.13E-06,8.18E-07
4707,sentiment_analysis8,66,"Formally , the input signal y [n ] is center - clipped to give a resultant signal , y clipped [ n] :",METHODOLOGY,B. Feature Extraction,sentiment_analysis,8,19,1,0,,1.26E-06,0,negative,3.74E-05,0.000148605,1.75E-05,2.39E-08,2.97E-06,8.61E-06,4.08E-07,7.60E-06,0.000317324,0.999452397,2.11E-06,5.05E-06,1.18E-08
4708,sentiment_analysis8,67,"Typically ,",METHODOLOGY,,sentiment_analysis,8,20,1,0,,4.61E-07,0,negative,5.83E-06,2.70E-06,9.22E-07,1.86E-08,1.05E-07,3.54E-06,8.25E-07,0.000104742,2.53E-06,0.999863569,1.48E-07,1.50E-05,2.44E-08
4709,sentiment_analysis8,68,Cl is nearly half the mean of the input signal and [ ] denotes the discrete nature of the input signal .,METHODOLOGY,"Typically ,",sentiment_analysis,8,21,1,0,,8.35E-06,0,negative,0.000171113,3.40E-06,3.35E-05,1.04E-06,2.77E-06,0.000157411,6.72E-06,0.000564509,1.18E-05,0.998970253,7.64E-07,4.46E-05,3.21E-05
4710,sentiment_analysis8,69,"Now , autocorrelation is calculated for the obtained signal y clipped , which is further normalized and the peak values associated with the pitch of the given input y [n ] .",METHODOLOGY,"Typically ,",sentiment_analysis,8,22,1,0,,1.80E-05,0,negative,0.000209506,2.95E-06,0.000193013,9.30E-08,5.84E-07,1.27E-05,1.82E-06,2.83E-05,3.15E-05,0.999464651,4.32E-07,4.87E-05,5.88E-06
4711,sentiment_analysis8,70,It was found that centerclipping the input signal resulted in more distinct autocorrelation peaks .,METHODOLOGY,"Typically ,",sentiment_analysis,8,23,1,0,,0.019285856,0,negative,0.07495342,1.17E-05,0.000152849,2.29E-06,1.08E-05,5.99E-05,4.98E-05,0.000172811,1.98E-05,0.908299532,3.26E-06,0.016156788,0.000107072
4712,sentiment_analysis8,71,b),METHODOLOGY,"Typically ,",sentiment_analysis,8,24,1,0,,5.00E-06,0,negative,0.000404219,6.53E-07,0.000481475,4.92E-08,3.22E-07,1.15E-05,4.44E-06,1.79E-05,6.89E-06,0.99890106,3.06E-07,0.000159623,1.16E-05
4713,sentiment_analysis8,72,Harmonics :,METHODOLOGY,"Typically ,",sentiment_analysis,8,25,1,0,,0.002068599,0,negative,0.010790636,8.41E-06,0.013613548,3.09E-06,7.14E-06,7.16E-05,4.51E-05,5.05E-05,9.37E-05,0.971137065,6.86E-06,0.003867939,0.000304463
4714,sentiment_analysis8,73,"In the emotional state of anger or for stressed speech , there are additional excitation signals other than pitch ( , ) .",METHODOLOGY,"Typically ,",sentiment_analysis,8,26,1,0,,2.01E-05,0,negative,0.000480009,1.45E-06,0.000210279,1.52E-07,1.17E-06,9.17E-06,2.19E-06,1.81E-05,1.19E-05,0.999086838,8.08E-07,0.000164354,1.36E-05
4715,sentiment_analysis8,74,This additional excitation is apparent in the spectrum as harmonics ( see ) and cross-harmonics .,METHODOLOGY,"Typically ,",sentiment_analysis,8,27,1,0,,2.61E-05,0,negative,0.002084332,2.62E-06,0.000390029,2.50E-07,2.15E-06,1.69E-05,3.88E-06,2.95E-05,3.12E-05,0.997221291,2.01E-07,0.000203534,1.41E-05
4716,sentiment_analysis8,75,We calculate harmonics using a median - based filter as described in .,METHODOLOGY,"Typically ,",sentiment_analysis,8,28,1,0,,0.000388262,0,negative,0.000740419,1.15E-05,0.007862335,4.25E-07,4.02E-06,0.000101901,1.61E-05,0.000135614,0.000134382,0.990864536,2.44E-07,7.27E-05,5.58E-05
4717,sentiment_analysis8,76,"First , the median filter is created for a given window size l , given by :",METHODOLOGY,"Typically ,",sentiment_analysis,8,29,1,0,,6.33E-06,0,negative,0.000376574,3.56E-06,0.000553474,3.64E-08,5.59E-07,9.83E-06,2.11E-06,3.98E-05,3.05E-05,0.99892359,6.87E-08,5.56E-05,4.33E-06
4718,sentiment_analysis8,77,where l is odd .,METHODOLOGY,"Typically ,",sentiment_analysis,8,30,1,0,,6.21E-06,0,negative,5.26E-05,7.21E-07,2.16E-05,1.34E-07,5.49E-07,1.23E-05,1.73E-06,4.55E-05,4.46E-06,0.999798123,3.12E-07,5.15E-05,1.05E-05
4719,sentiment_analysis8,78,"For cases when l is even , the median is obtained as the mean of two values in the middle of the sorted list .",METHODOLOGY,"Typically ,",sentiment_analysis,8,31,1,0,,4.47E-06,0,negative,9.29E-05,2.74E-06,0.000342174,3.24E-08,5.57E-07,1.58E-05,3.12E-06,4.49E-05,1.00E-05,0.999422129,9.95E-08,6.17E-05,3.79E-06
4720,sentiment_analysis8,79,"This filter is then applied to S h , the h?th frequency slice of a given spectrogram S , to get harmonic - enhanced spectrogram frequency slice H h as :",METHODOLOGY,"Typically ,",sentiment_analysis,8,32,1,0,,2.31E-06,0,negative,0.000784833,2.91E-06,0.0007355,9.94E-08,9.37E-07,7.22E-06,1.99E-06,1.55E-05,3.36E-05,0.998295078,1.49E-07,0.000114398,7.79E-06
4721,sentiment_analysis8,80,"Here M is the median filter , i is the i?th time step and l harm is the length of the harmonic filter .",METHODOLOGY,"Typically ,",sentiment_analysis,8,33,1,0,,1.02E-06,0,negative,8.19E-05,1.87E-06,3.35E-05,2.24E-07,9.69E-07,2.54E-05,2.65E-06,0.00013476,1.10E-05,0.999671676,1.52E-07,2.03E-05,1.55E-05
4722,sentiment_analysis8,81,c),METHODOLOGY,"Typically ,",sentiment_analysis,8,34,1,0,,7.06E-06,0,negative,0.000310956,3.32E-07,0.000280771,2.21E-08,2.35E-07,5.58E-06,3.84E-06,8.98E-06,3.02E-06,0.999158606,8.33E-08,0.000218797,8.77E-06
4723,sentiment_analysis8,82,"Speech Energy : Since the energy of a speech signal can be related to its loudness , we can use it to detect certain emotions .",METHODOLOGY,"Typically ,",sentiment_analysis,8,35,1,0,,0.005875223,0,negative,0.000403542,6.20E-06,0.002968909,1.42E-06,3.90E-06,6.81E-05,6.02E-05,9.40E-05,5.17E-05,0.99299655,2.73E-05,0.001405708,0.001912357
4724,sentiment_analysis8,83,"shows the difference in energy levels of an "" angry "" signal v/s that of a "" sad "" signal .",METHODOLOGY,"Typically ,",sentiment_analysis,8,36,1,0,,8.38E-06,0,negative,6.87E-05,7.56E-07,5.83E-05,1.92E-07,1.08E-06,3.26E-05,5.36E-06,8.07E-05,7.63E-06,0.999664132,1.25E-07,3.31E-05,4.74E-05
4725,sentiment_analysis8,84,We use standard Root Mean Square Energy ( RMSE ) to represent speech energy using the equation :,METHODOLOGY,"Typically ,",sentiment_analysis,8,37,1,0,,0.000164726,0,negative,5.40E-05,2.65E-06,0.000348374,1.92E-07,1.34E-06,5.90E-05,9.40E-06,0.000186509,1.23E-05,0.999243478,1.85E-07,3.14E-05,5.13E-05
4726,sentiment_analysis8,85,"RMSE is calculated frame by frame and we take both , the average and standard deviation as features .",METHODOLOGY,"Typically ,",sentiment_analysis,8,38,1,0,,9.28E-06,0,negative,4.41E-05,1.38E-06,0.000121617,5.70E-08,9.62E-07,3.37E-05,9.75E-06,9.75E-05,3.07E-06,0.999620556,5.58E-08,4.88E-05,1.84E-05
4727,sentiment_analysis8,86,d ),METHODOLOGY,"Typically ,",sentiment_analysis,8,39,1,0,,3.93E-06,0,negative,0.00016624,3.45E-07,0.000126884,4.37E-08,3.43E-07,6.31E-06,2.68E-06,1.21E-05,4.19E-06,0.999583973,5.36E-08,8.38E-05,1.30E-05
4728,sentiment_analysis8,87,Pause :,METHODOLOGY,"Typically ,",sentiment_analysis,8,40,1,0,,1.01E-05,0,negative,2.05E-05,1.66E-07,6.25E-06,6.08E-08,2.43E-07,1.01E-05,1.37E-06,2.59E-05,1.49E-06,0.999907811,3.69E-08,1.65E-05,9.59E-06
4729,sentiment_analysis8,88,"We use this feature to represent the "" silent "" portion in the audio signal .",METHODOLOGY,"Typically ,",sentiment_analysis,8,41,1,0,,0.00013405,0,negative,8.27E-05,1.89E-05,0.000577953,3.49E-07,2.99E-06,5.26E-05,5.32E-06,0.000389371,0.000431661,0.998326463,1.14E-07,1.78E-05,9.39E-05
4730,sentiment_analysis8,89,"This quantity is directly related to our emotions ; for instance , we tend to speak very fast when excited ( say , angry or happy , resulting in a low Pause value ) .",METHODOLOGY,"Typically ,",sentiment_analysis,8,42,1,0,,2.32E-06,0,negative,3.05E-05,3.38E-07,1.43E-05,4.83E-08,3.79E-07,5.77E-06,1.05E-06,1.85E-05,2.20E-06,0.99988999,6.98E-08,2.70E-05,9.79E-06
4731,sentiment_analysis8,90,The feature value is given by :,METHODOLOGY,"Typically ,",sentiment_analysis,8,43,1,0,,6.23E-06,0,negative,1.62E-05,8.32E-07,2.60E-05,4.34E-08,2.29E-07,1.26E-05,1.15E-06,9.44E-05,1.38E-05,0.999818174,2.42E-08,7.63E-06,8.92E-06
4732,sentiment_analysis8,91,where t represents a carefully - chosen threshold of ?,METHODOLOGY,"Typically ,",sentiment_analysis,8,44,1,0,,1.61E-06,0,negative,0.000179194,3.64E-06,1.23E-05,1.13E-06,4.13E-06,0.000162526,1.66E-05,0.0016472,6.99E-06,0.997752882,1.05E-07,6.82E-05,0.000145127
4733,sentiment_analysis8,92,"0.4 * E , E being the RMSE .",METHODOLOGY,"Typically ,",sentiment_analysis,8,45,1,0,,2.43E-05,0,negative,0.00510351,4.22E-06,0.000239854,2.49E-07,6.95E-06,2.39E-05,1.78E-05,8.64E-05,7.54E-06,0.993521626,3.11E-08,0.000941842,4.61E-05
4734,sentiment_analysis8,93,e),METHODOLOGY,"Typically ,",sentiment_analysis,8,46,1,0,,3.90E-06,0,negative,8.55E-05,2.20E-07,3.49E-05,2.55E-08,2.46E-07,3.74E-06,1.48E-06,1.13E-05,2.28E-06,0.999804162,1.88E-08,4.76E-05,8.57E-06
4735,sentiment_analysis8,94,Central moments :,METHODOLOGY,"Typically ,",sentiment_analysis,8,47,1,0,,0.002449161,0,negative,0.000410155,1.79E-06,0.00332389,6.05E-08,9.35E-07,1.60E-05,3.74E-05,1.71E-05,1.29E-05,0.994879027,1.56E-07,0.001235884,6.47E-05
4736,sentiment_analysis8,95,"Finally , we use the mean and standard deviation of the amplitude of the signal to incorporate a "" summarized "" information of the input .",METHODOLOGY,"Typically ,",sentiment_analysis,8,48,1,0,,4.50E-06,0,negative,0.000751531,1.25E-05,0.001177782,9.40E-08,3.24E-06,9.48E-06,3.56E-06,4.08E-05,0.000201406,0.997697481,2.18E-08,8.11E-05,2.11E-05
4737,sentiment_analysis8,96,a),METHODOLOGY,"Typically ,",sentiment_analysis,8,49,1,0,,2.05E-06,0,negative,7.49E-05,1.79E-07,3.21E-05,2.46E-08,2.53E-07,4.50E-06,1.85E-06,1.10E-05,2.30E-06,0.999813978,1.79E-08,4.37E-05,1.52E-05
4738,sentiment_analysis8,97,Term Frequency - Inverse Document Frequency ( TFIDF ) :,METHODOLOGY,"Typically ,",sentiment_analysis,8,50,1,0,,0.013664404,0,negative,0.000341593,2.14E-05,0.192026578,1.09E-07,2.16E-06,3.23E-05,0.000122191,4.22E-05,0.000171761,0.805348213,1.84E-06,0.001289047,0.000600627
4739,sentiment_analysis8,98,TFIDF is a numerical statistic that shows the correlation between a word and a document in a collection or corpus .,METHODOLOGY,"Typically ,",sentiment_analysis,8,51,1,0,,0.000176808,0,negative,0.000181663,1.43E-05,0.009938861,2.74E-06,1.68E-05,0.000188481,0.000154655,0.000200261,6.09E-05,0.981469794,5.83E-06,0.000455288,0.007310473
4740,sentiment_analysis8,99,It consists of two parts :,METHODOLOGY,"Typically ,",sentiment_analysis,8,52,1,0,,9.19E-06,0,negative,0.00264023,2.16E-05,0.017184042,3.32E-07,1.09E-05,1.28E-05,2.07E-05,1.62E-05,0.000259095,0.978838559,2.95E-07,0.000829323,0.000165964
4741,sentiment_analysis8,100,Term Frequency :,METHODOLOGY,"Typically ,",sentiment_analysis,8,53,1,0,,0.00265683,0,negative,0.001227757,7.35E-06,0.022499214,1.10E-07,3.13E-06,2.83E-05,7.45E-05,3.81E-05,5.24E-05,0.973908802,1.27E-07,0.001908315,0.00025182
4742,sentiment_analysis8,101,It denotes how many times a word / token occurs in a document .,METHODOLOGY,"Typically ,",sentiment_analysis,8,54,1,0,,2.65E-06,0,negative,2.46E-05,7.27E-07,7.34E-05,7.03E-08,1.18E-06,9.49E-06,1.52E-06,3.27E-05,6.72E-06,0.999819073,1.13E-08,1.08E-05,1.98E-05
4743,sentiment_analysis8,102,"The simplest choice is to use raw count of a token in a document ( sentences , in our case ) .",METHODOLOGY,"Typically ,",sentiment_analysis,8,55,1,0,,1.83E-05,0,negative,3.50E-05,1.19E-06,8.66E-05,7.06E-07,3.09E-06,4.01E-05,6.71E-06,7.71E-05,4.20E-06,0.999499872,2.03E-07,4.59E-05,0.000199305
4744,sentiment_analysis8,103,Inverse Document,METHODOLOGY,,sentiment_analysis,8,56,1,0,,6.34E-05,0,negative,1.90E-05,1.31E-05,0.000248755,4.63E-08,7.64E-07,4.82E-06,0.000108794,8.52E-05,3.47E-05,0.994827899,1.55E-06,0.00464683,8.51E-06
4745,sentiment_analysis8,104,Frequency :,METHODOLOGY,Inverse Document,sentiment_analysis,8,57,1,0,,2.15E-05,0,negative,5.58E-05,7.95E-07,0.000104483,1.94E-08,1.93E-07,1.58E-06,1.05E-05,6.43E-06,9.26E-06,0.999245808,1.59E-08,0.000562795,2.35E-06
4746,sentiment_analysis8,105,"This term is introduced to lessen the bias due to frequently occurring words in language such "" the "" , "" a "" and "" an "" .",METHODOLOGY,Inverse Document,sentiment_analysis,8,58,1,0,,1.78E-07,0,negative,2.05E-06,3.08E-07,2.34E-06,6.32E-08,1.58E-07,3.18E-06,1.59E-06,2.40E-05,3.29E-06,0.999951988,4.18E-09,1.01E-05,9.22E-07
4747,sentiment_analysis8,106,"Usually , idf for a term t and a document Dis defined as :",METHODOLOGY,Inverse Document,sentiment_analysis,8,59,1,0,,8.41E-08,0,negative,6.21E-07,1.48E-07,3.37E-06,6.11E-10,6.37E-09,2.16E-07,5.85E-07,2.03E-06,1.21E-06,0.999972212,1.32E-08,1.95E-05,1.22E-07
4748,sentiment_analysis8,107,The denominator shows the frequency of documents containing the term t and N is the total number of documents .,METHODOLOGY,Inverse Document,sentiment_analysis,8,60,1,0,,8.77E-08,0,negative,7.91E-07,4.45E-08,4.46E-07,5.77E-10,4.96E-09,1.59E-07,2.46E-07,2.01E-06,9.21E-07,0.999989156,8.00E-10,6.18E-06,4.33E-08
4749,sentiment_analysis8,108,"Finally , TFIDF value for a term is calculated by taking the product of TF and IDF values .",METHODOLOGY,Inverse Document,sentiment_analysis,8,61,1,0,,2.01E-06,0,negative,5.47E-06,7.66E-07,3.74E-05,9.42E-10,3.90E-08,3.94E-07,1.59E-06,3.33E-06,1.22E-05,0.9999105,1.92E-09,2.80E-05,2.60E-07
4750,sentiment_analysis8,109,C. Machine Learning Models :,METHODOLOGY,Inverse Document,sentiment_analysis,8,62,1,0,,3.99E-05,0,negative,3.75E-05,6.99E-07,0.000734054,3.72E-09,7.56E-08,6.86E-07,3.03E-05,1.53E-06,2.91E-06,0.996249092,7.82E-08,0.002940347,2.79E-06
4751,sentiment_analysis8,110,"This section describes the various ML - based classifiers considered in this work , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes , and Logistic Regression a ) Random Forest ( RF ) : Random forests are ensemble learners that operate by constructing multiple decision trees at training time and outputting the class that is the mode of the classes ( classification ) of the individual trees .",METHODOLOGY,Inverse Document,sentiment_analysis,8,63,1,0,,0.000371231,0,negative,1.08E-05,4.53E-06,0.001097027,1.28E-08,2.88E-07,1.94E-06,2.32E-05,5.48E-06,1.60E-05,0.998577106,6.05E-08,0.000258858,4.61E-06
4752,sentiment_analysis8,111,It has two base working principles :,METHODOLOGY,Inverse Document,sentiment_analysis,8,64,1,0,,3.90E-07,0,negative,4.24E-05,5.15E-07,0.000180542,7.08E-09,1.73E-07,4.42E-07,4.31E-06,9.69E-07,5.36E-06,0.999450166,6.87E-09,0.000314211,8.62E-07
4753,sentiment_analysis8,112,Each decision tree predicts using a random subset of features Each decision tree is trained with only a subset of training samples .,METHODOLOGY,Inverse Document,sentiment_analysis,8,65,1,0,,1.93E-06,0,negative,8.89E-06,8.19E-07,4.49E-05,1.20E-09,5.72E-08,2.24E-07,1.24E-06,2.36E-06,1.62E-05,0.999901537,7.52E-10,2.35E-05,2.54E-07
4754,sentiment_analysis8,113,"This is known as bootstrap aggregating Finally , a majority vote of all the decision trees is taken to predict the class of a given input .",METHODOLOGY,Inverse Document,sentiment_analysis,8,66,1,0,,7.75E-06,0,negative,1.98E-06,5.36E-07,2.76E-05,5.79E-09,4.32E-08,7.94E-07,2.66E-06,6.88E-06,9.58E-06,0.99992949,7.24E-09,1.85E-05,1.85E-06
4755,sentiment_analysis8,114,b) Gradient Boosting ( XGB ) : XGB refers to eXtreme Gradient Boosting .,METHODOLOGY,Inverse Document,sentiment_analysis,8,67,1,0,,0.012154468,0,negative,3.04E-05,4.29E-06,0.011970051,1.88E-08,3.26E-07,3.98E-06,0.000127621,8.04E-06,2.40E-05,0.986802786,1.19E-07,0.001000189,2.82E-05
4756,sentiment_analysis8,115,It is an implementation of boosting that supports training the model in a fast and parallelized way .,METHODOLOGY,Inverse Document,sentiment_analysis,8,68,1,0,,9.66E-05,0,negative,3.91E-05,8.04E-06,0.002781542,1.37E-07,1.37E-06,1.15E-05,8.58E-05,2.88E-05,6.18E-05,0.996729318,3.89E-08,0.000207747,4.48E-05
4757,sentiment_analysis8,116,"Boosting is another ensemble classifier combining a number of weak learners , typically decision trees .",METHODOLOGY,Inverse Document,sentiment_analysis,8,69,1,0,,1.05E-05,0,negative,1.34E-05,1.63E-06,0.001190523,7.84E-08,4.33E-07,1.03E-05,0.000109746,2.31E-05,1.31E-05,0.998287458,1.33E-07,0.000279039,7.10E-05
4758,sentiment_analysis8,117,"They are trained in a sequential manner , unlike RFs , using forward stagewise additive modeling .",METHODOLOGY,Inverse Document,sentiment_analysis,8,70,1,0,,9.70E-07,0,negative,6.66E-05,7.29E-06,0.000317303,1.16E-08,3.62E-07,9.47E-07,5.95E-06,5.68E-06,6.54E-05,0.999446393,3.33E-09,8.25E-05,1.57E-06
4759,sentiment_analysis8,118,"During the early iterations , the decision trees learned are simple .",METHODOLOGY,Inverse Document,sentiment_analysis,8,71,1,0,,4.21E-05,0,negative,2.12E-05,7.29E-07,3.49E-06,5.36E-09,6.34E-08,1.27E-06,3.60E-06,3.42E-05,4.48E-06,0.999865191,7.95E-10,6.49E-05,8.94E-07
4760,sentiment_analysis8,119,"As training progresses , the classifier becomes more powerful because it is made to focus on the instances where the previous learners made errors .",METHODOLOGY,Inverse Document,sentiment_analysis,8,72,1,0,,8.34E-05,0,negative,0.000551446,5.63E-07,6.41E-06,1.06E-08,1.33E-07,5.16E-07,7.51E-06,4.72E-06,4.44E-06,0.998378445,1.61E-09,0.001044404,1.41E-06
4761,sentiment_analysis8,120,"At the end of training , the final prediction is a weighted linear combination of the output from the individual learners .",METHODOLOGY,Inverse Document,sentiment_analysis,8,73,1,0,,1.02E-06,0,negative,5.36E-06,1.35E-06,1.69E-05,1.18E-09,4.83E-08,3.04E-07,2.00E-06,7.06E-06,2.87E-05,0.999918175,5.29E-10,1.97E-05,4.57E-07
4762,sentiment_analysis8,121,c),METHODOLOGY,Inverse Document,sentiment_analysis,8,74,1,0,,2.77E-07,0,negative,4.42E-06,1.29E-08,3.28E-06,2.10E-10,6.24E-09,1.01E-07,1.15E-06,4.70E-07,1.93E-07,0.999907396,1.53E-10,8.28E-05,1.26E-07
4763,sentiment_analysis8,122,Support Vector Machines ( SVMs ) :,METHODOLOGY,Inverse Document,sentiment_analysis,8,75,1,0,,0.001822153,0,negative,6.49E-06,5.95E-07,0.001074149,1.07E-09,2.69E-08,8.21E-07,4.04E-05,2.82E-06,6.44E-06,0.998135685,1.71E-08,0.000728122,4.45E-06
4764,sentiment_analysis8,123,SVMs are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis .,METHODOLOGY,Inverse Document,sentiment_analysis,8,76,1,0,,3.29E-05,0,negative,4.31E-06,8.83E-07,9.03E-05,4.66E-08,1.47E-07,6.42E-06,2.08E-05,3.46E-05,7.78E-06,0.999749512,3.91E-08,6.17E-05,2.35E-05
4765,sentiment_analysis8,124,An SVM training algorithm essentially builds a non-probabilistic binary linear classifier ( although methods such as Platt scaling exist to use SVM in a probabilistic classification setting ) .,METHODOLOGY,Inverse Document,sentiment_analysis,8,77,1,0,,2.48E-07,0,negative,1.34E-06,2.11E-07,6.89E-06,4.60E-09,3.26E-08,5.32E-07,1.04E-06,4.19E-06,2.02E-06,0.999968892,5.56E-09,1.37E-05,1.18E-06
4766,sentiment_analysis8,125,"It represents each training example as a point in space , mapped such that the examples of the separate categories are divided by a clear gap that is as wide as possible ( this is usually achieved by minimizing the hinge loss ) .",METHODOLOGY,Inverse Document,sentiment_analysis,8,78,1,0,,1.38E-06,0,negative,2.73E-05,3.44E-06,0.000447647,5.64E-09,2.53E-07,5.42E-07,5.14E-06,4.03E-06,5.71E-05,0.999405398,1.28E-09,4.74E-05,1.75E-06
4767,sentiment_analysis8,126,New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall .,METHODOLOGY,Inverse Document,sentiment_analysis,8,79,1,0,,2.59E-07,0,negative,1.26E-06,2.13E-07,1.06E-05,4.16E-10,2.56E-08,1.04E-07,5.81E-07,1.44E-06,6.57E-06,0.99997078,1.12E-10,8.29E-06,1.79E-07
4768,sentiment_analysis8,127,"SVMs were originally introduced to perform linear classification ; however , they can efficiently perform a non-linear classification using the kernel trick , implicitly mapping their inputs into high - dimensional feature spaces .",METHODOLOGY,Inverse Document,sentiment_analysis,8,80,1,0,,2.44E-06,0,negative,2.86E-06,3.03E-07,6.34E-06,6.69E-08,6.68E-08,5.43E-06,9.87E-06,2.47E-05,1.63E-06,0.999885481,4.78E-08,5.15E-05,1.17E-05
4769,sentiment_analysis8,128,d ) Multinomial Naive Bayes ( MNB ) :,METHODOLOGY,Inverse Document,sentiment_analysis,8,81,1,0,,0.003571295,0,negative,1.49E-05,2.85E-06,0.006595397,1.66E-09,1.25E-07,1.16E-06,5.94E-05,3.12E-06,2.90E-05,0.992982539,5.39E-09,0.00030472,6.78E-06
4770,sentiment_analysis8,129,"Naive Bayes classifiers are a family of simple "" probabilistic classifiers "" based on applying Bayes ' theorem with strong ( naive ) independence assumptions between the features .",METHODOLOGY,Inverse Document,sentiment_analysis,8,82,1,0,,2.26E-05,0,negative,9.27E-06,1.80E-06,0.000506646,2.82E-08,3.33E-07,4.79E-06,4.21E-05,1.78E-05,1.15E-05,0.999229744,2.29E-08,0.000140441,3.55E-05
4771,sentiment_analysis8,130,"Under multinomial settings , the feature vectors represent the frequencies with which certain events have been generated by a multinomial ( p 1 , . . . , p n ) where pi is the probability that event i occurs .",METHODOLOGY,Inverse Document,sentiment_analysis,8,83,1,0,,1.22E-07,0,negative,2.73E-07,1.28E-07,1.14E-06,2.12E-10,4.45E-09,1.48E-07,3.22E-07,3.06E-06,3.61E-06,0.999988652,1.15E-10,2.56E-06,9.50E-08
4772,sentiment_analysis8,131,MNB is very popular for document classification task in text which too essentially is a multi - class classification problem .,METHODOLOGY,Inverse Document,sentiment_analysis,8,84,1,0,,3.71E-05,0,negative,3.33E-06,3.96E-07,1.38E-05,2.56E-08,7.68E-08,4.09E-06,7.37E-05,2.00E-05,7.99E-07,0.999360833,2.12E-07,0.000467821,5.49E-05
4773,sentiment_analysis8,132,e),METHODOLOGY,Inverse Document,sentiment_analysis,8,85,1,0,,1.44E-07,0,negative,1.29E-06,7.60E-09,5.72E-07,2.32E-10,4.11E-09,7.98E-08,4.46E-07,6.80E-07,1.17E-07,0.999980989,3.90E-11,1.57E-05,1.04E-07
4774,sentiment_analysis8,133,"Logistic Regression ( LR ) : LR is typically used for binary classification problems , that is , when we have only two labels .",METHODOLOGY,Inverse Document,sentiment_analysis,8,86,1,0,,0.001478701,0,negative,2.78E-06,8.06E-07,0.000120641,2.02E-08,7.60E-08,7.57E-06,0.000137668,4.25E-05,4.58E-06,0.999442784,4.85E-08,0.000167903,7.26E-05
4775,sentiment_analysis8,134,"In this work , LR is implemented in a one - vs - rest manner ; six classifiers have been trained for each class and finally , we consider the class that is predicted with the highest probability .",METHODOLOGY,Inverse Document,sentiment_analysis,8,87,1,0,,1.18E-05,0,negative,2.05E-05,4.66E-05,0.000578937,2.07E-08,1.32E-06,3.90E-06,5.37E-05,3.43E-05,7.44E-05,0.999010501,6.42E-09,0.000159041,1.68E-05
4776,sentiment_analysis8,135,"Having trained the above classifiers , we take ensemble of the best performing classifiers and use it for comparison with the current state - of - the - art for emotion recognition on the IEMOCAP dataset .",METHODOLOGY,Inverse Document,sentiment_analysis,8,88,1,0,,5.22E-07,0,negative,1.49E-06,1.67E-07,2.44E-06,1.07E-09,1.42E-07,6.02E-07,8.87E-06,6.77E-06,1.34E-07,0.999920917,1.59E-11,5.79E-05,5.56E-07
4777,sentiment_analysis8,136,D. Deep Learning Models,,,sentiment_analysis,8,0,1,0,,0.308472155,0,negative,9.22E-05,0.001035735,0.00077948,6.17E-06,3.12E-06,0.000305439,0.000641295,0.002828465,0.000491146,0.646578674,0.346103972,0.001068336,6.60E-05
4778,sentiment_analysis8,137,"In this section , we describe the deep learning models used .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,1,1,0,,2.25E-05,0,negative,1.49E-05,6.72E-05,9.27E-06,1.39E-06,2.48E-06,8.35E-05,3.95E-06,0.000269111,0.000195156,0.999330847,1.03E-05,1.13E-05,6.50E-07
4779,sentiment_analysis8,138,"Typically , Deep Neural Networks ( DNNs ) are trained in an end - to - end fashion and they are expected to "" figure out "" features completely on their own .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,2,1,0,,0.011624742,0,negative,1.37E-05,5.00E-05,2.28E-05,4.53E-06,1.31E-06,0.000102775,1.44E-05,0.000300487,0.000144661,0.995100338,0.004201301,3.50E-05,8.72E-06
4780,sentiment_analysis8,139,"However , training such a model can take a lot of time as well as computational resources .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,3,1,0,,0.000102758,0,negative,3.87E-05,1.05E-05,2.47E-06,4.99E-06,1.30E-06,6.32E-05,9.58E-06,0.000116005,2.84E-05,0.99818293,0.001476275,5.98E-05,5.78E-06
4781,sentiment_analysis8,140,"In order to minimize the computational overhead , we directly feed the handcrafted features as input to these models and compare their performance with the traditional end - to - end trained counterparts .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,4,1,0,,0.002089528,0,negative,0.000174347,0.000617729,0.000203255,3.12E-07,4.58E-06,2.88E-05,1.06E-05,0.000183258,0.00047916,0.99803546,1.19E-05,0.000250121,4.74E-07
4782,sentiment_analysis8,141,"In this work , we implement two types of models : a) Multi - Layer Perceptron ( MLP ) : MLP belongs to a class of feed - forward neural network .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,5,1,0,,0.113685576,0,negative,0.000122236,0.00167289,0.09116375,3.93E-06,9.26E-06,0.000646141,0.000252498,0.00169204,0.018624529,0.882718931,0.002811437,0.000222973,5.94E-05
4783,sentiment_analysis8,142,"It consists of at least three nodes : an input , a hidden and an output layer .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,6,1,0,,0.00734426,0,negative,0.000284957,0.001293088,0.003111476,7.35E-06,1.58E-05,0.000424344,6.31E-05,0.001388394,0.058516071,0.934446787,0.000323251,8.18E-05,4.35E-05
4784,sentiment_analysis8,143,All the nodes are interleaved with a non-linear activation function to stabilize the network during training time .,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,7,1,0,,0.024979757,0,negative,0.0004656,0.001980725,0.000294355,6.99E-06,1.24E-05,0.001500804,0.000107334,0.01850594,0.026086559,0.950896321,5.05E-05,7.46E-05,1.79E-05
4785,sentiment_analysis8,144,Their expressive power increases as we increase the number of hidden layers upto a certain extent .,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,8,1,0,,0.000731847,0,negative,0.001721553,4.96E-05,0.000157721,7.82E-07,3.69E-06,2.99E-05,1.56E-05,7.97E-05,0.000137902,0.995457967,2.96E-05,0.002314434,1.58E-06
4786,sentiment_analysis8,145,Their non-linear nature allows them to distinguish data that is not linearly separable .,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,9,1,0,,7.20E-05,0,negative,0.000222824,1.65E-05,7.87E-05,2.62E-07,1.52E-06,9.23E-06,1.86E-06,1.66E-05,0.000133738,0.99930541,9.80E-06,0.000203129,3.96E-07
4787,sentiment_analysis8,146,b ) Long Short Term Memory ( LSTM ) :,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,10,1,0,,0.37908991,0,negative,0.000336614,0.000265112,0.287613194,5.58E-07,2.36E-06,0.000155535,0.000189945,0.00016977,0.008391594,0.701378437,0.000630103,0.000840968,2.58E-05
4788,sentiment_analysis8,147,LSTMs were introduced for long - range context capturing in sequences .,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,11,1,0,,0.011551941,0,negative,8.77E-05,0.000147552,0.000583157,4.08E-06,3.31E-06,0.000128672,8.39E-05,0.000294727,0.001040222,0.976699174,0.020498214,0.000382804,4.65E-05
4789,sentiment_analysis8,148,"Unlike MLP , it has feedback connections that allow it to decide what information is important and what is not .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,12,1,0,,0.000449054,0,negative,0.000196241,0.000142721,0.003257028,5.05E-07,2.76E-06,3.66E-05,1.01E-05,8.36E-05,0.00694893,0.989213007,2.17E-05,8.31E-05,3.58E-06
4790,sentiment_analysis8,149,"It consists of a gating mechanism and there are three types of gates : input , forget and output .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,13,1,0,,0.001094918,0,negative,0.000767253,0.000435473,0.010586207,2.32E-06,1.49E-05,8.72E-05,2.70E-05,0.000159067,0.020208223,0.967421183,5.47E-05,0.0002222,1.43E-05
4791,sentiment_analysis8,150,Their equations are mentioned below :,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,14,1,0,,2.52E-06,0,negative,4.97E-06,1.03E-06,4.07E-06,3.31E-08,1.75E-07,4.25E-06,5.23E-07,9.02E-06,3.18E-05,0.999935763,6.02E-07,7.66E-06,6.66E-08
4792,sentiment_analysis8,151,"where initial values are c 0 = 0 and h 0 = 0 and denotes the element - wise product , t denotes the time step ( each element in a sequence belongs to onetime step ) , x t refers to the input vector to the LSTM unit , ft is the forget gate 's activation vector , it refers to the input gate 's activation vector , o t refers to the output gate 's activation vector , ht is the hidden state vector ( which is typically used to map a vector from the feature space to a lowerdimensional latent space , ) ct is the cell state vector and W , U and bare weight and bias matrices which need to be learned during training .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,15,1,0,,1.34E-05,0,negative,7.90E-06,1.91E-05,4.83E-06,2.40E-07,5.95E-07,4.14E-05,2.72E-06,0.0002541,0.000227728,0.999429214,2.38E-06,9.44E-06,3.53E-07
4793,sentiment_analysis8,152,"From , we see that an LSTM cell is able to keep track of hidden states at all time steps through the feedback mechanism .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,16,1,0,,0.000177374,0,negative,0.001359489,3.08E-05,3.06E-05,3.50E-07,2.06E-06,1.11E-05,6.56E-06,5.97E-05,0.000312529,0.997217071,3.87E-06,0.000964891,1.05E-06
4794,sentiment_analysis8,153,shows the network implemented in this work .,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,17,1,0,,7.91E-06,0,negative,3.26E-06,8.17E-06,1.14E-05,1.25E-07,4.15E-07,1.64E-05,2.22E-06,5.99E-05,0.000314422,0.999569013,4.23E-06,9.82E-06,6.46E-07
4795,sentiment_analysis8,154,We feed the feature vectors as input to the network and finally pass the output of the LSTM network through a softmax layer to get probability scores for each of the six emotion classes .,D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,18,1,0,,0.000347676,0,negative,2.10E-05,7.78E-05,2.06E-05,3.84E-07,1.38E-06,0.000290629,1.39E-05,0.002581798,0.001471108,0.995507941,1.64E-06,1.00E-05,1.75E-06
4796,sentiment_analysis8,155,"Since we are using feature vectors as input , we do not need another decoder network to transform it back from hidden to output space thereby reducing network size .",D. Deep Learning Models,D. Deep Learning Models,sentiment_analysis,8,19,1,0,,0.000125776,0,negative,0.000657429,0.000138236,0.000194569,4.96E-07,6.93E-06,1.35E-05,5.04E-06,5.67E-05,0.001527747,0.997170213,2.47E-06,0.000225319,1.29E-06
4797,sentiment_analysis8,156,E. Experiments,,,sentiment_analysis,8,0,1,0,,0.029993938,0,negative,0.002841388,0.000453399,5.47E-05,3.74E-06,2.16E-05,0.000103972,0.000575559,0.001320879,2.39E-05,0.966426344,0.000629367,0.027533227,1.19E-05
4798,sentiment_analysis8,157,"Here , we describe the three different settings we conducted our experiments in :",E. Experiments,E. Experiments,sentiment_analysis,8,1,1,0,,0.00040294,0,negative,0.004493001,1.63E-05,0.002574008,0.003579415,0.000403157,0.002907574,0.001197282,0.000175924,4.94E-05,0.928419916,5.17E-05,0.003141413,0.05299084
4799,sentiment_analysis8,158,"Audio- only : In this setting , we train all the classifiers using only the audio feature vectors described earlier .",E. Experiments,E. Experiments,sentiment_analysis,8,2,1,0,,0.346283749,0,baselines,0.007212491,0.00045055,0.393100193,0.00032906,0.000128305,0.005851028,0.019052478,0.003189851,0.000769335,0.306866325,0.000835356,0.040913863,0.221301163
4800,sentiment_analysis8,159,"Text - only : In this setting , we train all the classifiers using only the text feature vectors ( TFIDF vectors ) Audio + Text : In this setting , we fuse the feature vectors from the two modalities .",E. Experiments,E. Experiments,sentiment_analysis,8,3,1,0,,0.501994997,1,baselines,0.002699562,0.000152233,0.560957261,0.00011112,4.20E-05,0.003862513,0.019089395,0.001934533,0.000279522,0.195676089,0.000480007,0.026302727,0.188413055
4801,sentiment_analysis8,160,There have been some methods proposed to fuse vectors efficiently from multiple modalities but we simply concatenate the feature vectors from audio and text to obtain the combined feature vectors .,E. Experiments,E. Experiments,sentiment_analysis,8,4,1,0,,0.04435614,0,negative,0.003458116,4.57E-05,0.002467396,0.000277532,2.58E-05,0.004277312,0.003276301,0.002639298,0.000111301,0.667225629,0.003477128,0.014742072,0.297976405
4802,sentiment_analysis8,161,"Through this experiment , we would be able to infer how much information is contained in each of the modalities and how does fusion influence the model 's performance .",E. Experiments,E. Experiments,sentiment_analysis,8,5,1,0,,0.000599804,0,negative,0.002508154,5.63E-06,0.000277493,4.15E-06,7.12E-06,0.00027462,0.000226593,0.000239663,1.47E-05,0.987304643,5.08E-06,0.007640188,0.001492016
4803,sentiment_analysis8,162,F. Implementation Details,,,sentiment_analysis,8,0,1,0,,0.290399661,0,negative,0.000300435,0.001091686,1.44E-05,0.000185003,0.000101799,0.000698015,0.000590814,0.003592564,7.95E-05,0.991269616,0.001406742,0.000644908,2.45E-05
4804,sentiment_analysis8,163,"In this section , we describe the implementation details adopted in this work .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,1,1,0,,0.000176324,0,negative,0.000186037,0.000129851,3.58E-05,0.000361104,0.000392687,0.223919733,0.002411746,0.012270073,4.84E-05,0.759813101,9.82E-05,0.000197679,0.000135581
4805,sentiment_analysis8,164,"We use librosa , a Python library , to process the audio files and extract features from them .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,2,1,1,experimental-setup,0.561239027,1,experimental-setup,0.000145318,0.000295446,0.000334625,0.000469487,0.000179135,0.900432803,0.006166398,0.051285894,0.000228133,0.039942885,4.24E-05,5.70E-05,0.000420489
4806,sentiment_analysis8,165,"We use scikit - learn and xgboost [ 25 ] , the machine learning libraries for Python , to implement all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,3,1,1,experimental-setup,0.948033226,1,experimental-setup,1.10E-05,8.17E-06,1.02E-05,0.000152718,9.13E-06,0.975710854,0.002317304,0.016469965,5.94E-06,0.005230157,3.87E-06,4.77E-06,6.59E-05
4807,sentiment_analysis8,166,We use PyTorch to implement the LSTM classifiers described earlier .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,4,1,1,experimental-setup,0.580187384,1,experimental-setup,1.19E-05,7.22E-06,1.00E-05,0.000335202,1.20E-05,0.980784062,0.002172877,0.011425165,6.73E-06,0.005145417,4.12E-06,5.75E-06,7.95E-05
4808,sentiment_analysis8,167,"In order to regularize the hidden space of the LSTM classifiers , we use a shut - off mechanism , called dropout , where a fraction of neurons are not used for final prediction .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,5,1,1,experimental-setup,0.577431632,1,experimental-setup,0.001140944,0.00137787,0.000571317,5.67E-05,6.09E-05,0.686714768,0.004805391,0.269892677,0.002018965,0.032817133,4.02E-05,0.000126551,0.000376594
4809,sentiment_analysis8,168,This is shown to increase the robustness of the network and prevent overfitting .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,6,1,0,,0.024282031,0,negative,0.036133217,0.000787381,0.000828768,7.07E-05,0.000286287,0.069819238,0.003888728,0.015684828,0.000721426,0.858521909,0.000115726,0.012943846,0.000197939
4810,sentiment_analysis8,169,We randomly split our dataset into a train ( 80 % ) and test ( 20 % ) set .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,7,1,1,experimental-setup,0.145913544,0,experimental-setup,8.21E-05,7.18E-05,3.02E-05,3.09E-05,0.000136749,0.733442482,0.005182664,0.059517437,3.00E-05,0.201257016,1.04E-05,8.36E-05,0.000124697
4811,sentiment_analysis8,170,The same split is used for all the experiments to ensure a fair comparison .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,8,1,0,,0.013960449,0,experimental-setup,6.92E-05,5.61E-05,3.51E-05,3.24E-06,7.23E-06,0.727899568,0.005005411,0.123633304,3.86E-05,0.143127714,7.28E-06,7.43E-05,4.30E-05
4812,sentiment_analysis8,171,The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,9,1,1,experimental-setup,0.971502668,1,experimental-setup,6.16E-06,1.84E-06,2.67E-06,0.000211557,8.89E-06,0.988978276,0.002252351,0.004916739,1.40E-06,0.003524996,1.56E-06,6.65E-06,8.69E-05
4813,sentiment_analysis8,172,We stop the training when we do not see any improvement in validation performance for > 10 epochs .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,10,1,1,experimental-setup,0.434313678,0,experimental-setup,8.11E-05,2.77E-05,6.06E-06,7.91E-06,5.96E-06,0.810459457,0.002310643,0.17018663,1.46E-05,0.016779966,4.44E-06,2.81E-05,8.74E-05
4814,sentiment_analysis8,173,"Here , one epoch refers to one iteration over all the training samples .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,11,1,0,,0.001841999,0,experimental-setup,0.000197955,0.000276072,0.000103389,1.11E-05,1.70E-05,0.475792052,0.001938808,0.146469347,0.000999644,0.373943492,6.32E-05,6.96E-05,0.000118362
4815,sentiment_analysis8,174,Different batch sizes were used for different models .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,12,1,0,,0.0808626,0,experimental-setup,0.000273922,3.95E-05,2.23E-05,1.77E-05,1.80E-05,0.684135641,0.006226711,0.088063783,3.86E-05,0.22051043,3.83E-05,0.000455853,0.000159387
4816,sentiment_analysis8,175,Hyperparameters for the all the models under the three experiment settings could be found in the released repository .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,13,1,0,,0.048951219,0,experimental-setup,6.76E-05,2.99E-05,5.63E-06,4.01E-05,2.24E-05,0.821393544,0.001933458,0.087225735,1.87E-05,0.089130328,4.67E-06,6.45E-05,6.35E-05
4817,sentiment_analysis8,176,G. Evaluation Metrics :,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,14,1,0,,0.16932815,0,negative,0.005504401,0.000135621,0.000687904,5.31E-06,5.00E-05,0.064328281,0.016566092,0.007800951,4.70E-05,0.888561516,7.43E-05,0.016078357,0.0001602
4818,sentiment_analysis8,177,"In this section , we first describe the various evaluation metrics used and report results for the three experiment settings .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,15,1,0,,0.001987122,0,negative,0.000265034,0.000116582,2.16E-05,2.58E-05,0.000118918,0.115511581,0.002596643,0.014757979,3.35E-05,0.865572789,2.31E-05,0.00089459,6.19E-05
4819,sentiment_analysis8,178,a),F. Implementation Details,F. Implementation Details,sentiment_analysis,8,16,1,0,,0.000972419,0,negative,0.000639771,1.84E-05,0.000119772,9.34E-06,1.95E-05,0.112924927,0.003782261,0.007869247,5.54E-05,0.873575231,5.02E-05,0.000829307,0.000106704
4820,sentiment_analysis8,179,Accuracy : This refers to the percentage of test samples thatare classified correctly .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,17,1,0,,0.002891937,0,negative,9.82E-05,2.82E-05,8.81E-05,1.62E-05,3.19E-05,0.333496598,0.007796545,0.024022639,4.03E-05,0.633445311,9.64E-05,0.000627475,0.000212128
4821,sentiment_analysis8,180,b) Precision :,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,18,1,0,,0.757067656,1,negative,0.013126035,0.000345685,0.028475649,1.70E-05,0.000122646,0.116636884,0.04752577,0.008099293,0.00043697,0.756017074,0.0002016,0.028146734,0.000848695
4822,sentiment_analysis8,181,"This measure tells us out of all predictions , how many are actually present in the ground truth ( a.k.a. labels ) .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,19,1,0,,0.000623469,0,negative,0.000251608,8.17E-05,0.000152401,1.53E-05,7.55E-05,0.073714116,0.001549325,0.007034712,0.000153789,0.916317874,5.85E-05,0.000512415,8.28E-05
4823,sentiment_analysis8,182,It is calculated using the formula :,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,20,1,0,,0.000252148,0,negative,0.000200277,3.82E-05,8.47E-05,1.36E-05,3.49E-05,0.151465888,0.002003821,0.014523338,0.000131584,0.831217139,2.66E-05,0.000189103,7.08E-05
4824,sentiment_analysis8,183,c ),F. Implementation Details,F. Implementation Details,sentiment_analysis,8,21,1,0,,0.002472133,0,negative,0.001731192,2.11E-05,0.000594854,4.79E-06,2.56E-05,0.06601834,0.008112435,0.003926998,4.45E-05,0.91534486,2.33E-05,0.00401056,0.000141547
4825,sentiment_analysis8,184,Recall :,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,22,1,0,,0.000529078,0,negative,0.000142629,5.45E-06,9.09E-05,2.64E-06,1.25E-05,0.019406935,0.001692029,0.001038796,1.38E-05,0.977027229,2.39E-05,0.000503132,4.01E-05
4826,sentiment_analysis8,185,This measure tells us how many correct labels are present in the predicted output .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,23,1,0,,0.000614259,0,negative,0.000333045,7.18E-05,0.000143204,1.15E-05,5.39E-05,0.12205119,0.001989304,0.012948735,0.000192277,0.861704474,2.21E-05,0.00037494,0.000103445
4827,sentiment_analysis8,186,It is calculated using the formula :,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,24,1,0,,0.000250396,0,negative,0.000186031,3.53E-05,7.94E-05,1.20E-05,3.79E-05,0.136568209,0.002089016,0.01288962,0.000117382,0.847691153,1.76E-05,0.000198675,7.77E-05
4828,sentiment_analysis8,187,"Here , tp , f p , and f n stand for true positive , false positive and false negative respectively .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,25,1,0,,0.000195438,0,negative,3.66E-05,1.80E-05,1.05E-05,1.61E-06,7.52E-06,0.077815168,0.000565394,0.013945671,6.46E-05,0.907447035,4.84E-06,6.53E-05,1.78E-05
4829,sentiment_analysis8,188,We can compute these values from the confusion matrix .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,26,1,0,,0.000357068,0,negative,0.000100381,1.70E-05,2.48E-05,2.04E-06,1.03E-05,0.077251919,0.000980114,0.008281692,6.33E-05,0.913071153,3.68E-06,0.000172939,2.05E-05
4830,sentiment_analysis8,189,d ) F-score :,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,27,1,0,,0.263296435,0,negative,0.001795913,0.000271496,0.013888084,1.21E-06,2.20E-05,0.077053176,0.027549643,0.007195438,0.000491383,0.865301177,4.38E-05,0.006087795,0.000298921
4831,sentiment_analysis8,190,It is defined as the harmonic mean of precision and recall .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,28,1,0,,0.000389249,0,negative,0.000274366,0.000101267,0.000258388,2.84E-05,0.000118327,0.266505316,0.005147716,0.023269315,8.92E-05,0.703368929,2.86E-05,0.000565778,0.000244459
4832,sentiment_analysis8,191,This measure was included as accuracy is not a complete measure of a model 's predictive power but F-score is since it is more normalized .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,29,1,0,,0.000177998,0,negative,0.000234566,2.07E-05,5.30E-05,6.79E-06,8.70E-05,0.197263765,0.002730358,0.008638694,1.89E-05,0.790444273,3.24E-06,0.000447422,5.13E-05
4833,sentiment_analysis8,192,We compare our best performing models with the current state - of - the - art as mentioned in .,F. Implementation Details,F. Implementation Details,sentiment_analysis,8,30,1,0,,0.114435853,0,negative,3.37E-05,1.00E-05,1.54E-05,4.59E-06,5.69E-05,0.065319567,0.0040012,0.003097698,3.21E-06,0.926813146,2.14E-06,0.000594491,4.79E-05
4834,sentiment_analysis8,193,"They employ three types of recurrent encoders , namely , ARE , TRE and MDRE denoting Audio - , Text - and Multimodal Dual - Recurrent Encoders respectively .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,31,1,0,,0.172511536,0,negative,0.008183605,0.002548721,0.204436221,0.000123492,0.000860056,0.202202703,0.109268328,0.011938392,0.004607022,0.435646509,0.000119941,0.013112411,0.006952598
4835,sentiment_analysis8,194,"It is important to mention that only considers four emotions for classification , namely , angry , happy , sad and neutral as opposed to six in our case .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,32,1,0,,0.004197937,0,negative,0.000141193,2.25E-05,0.000104278,2.93E-05,0.000485305,0.052432162,0.003084395,0.002202544,8.08E-06,0.940479494,1.39E-05,0.000741897,0.000254977
4836,sentiment_analysis8,195,"In order to present a fair comparison of our method with theirs , we also run the experiments for the four classes ( models with code 4 - class in ) .",F. Implementation Details,F. Implementation Details,sentiment_analysis,8,33,1,0,,0.02497856,0,negative,0.000786911,5.03E-05,4.59E-05,3.33E-06,0.000102084,0.057362203,0.01826008,0.005291232,7.01E-06,0.914135383,1.26E-06,0.003846511,0.000107842
4837,sentiment_analysis8,196,V .,F. Implementation Details,,sentiment_analysis,8,34,1,0,,0.001026297,0,negative,0.000342261,1.64E-05,2.12E-05,1.05E-05,2.98E-05,0.098937746,0.001802559,0.008485813,3.53E-05,0.889789504,3.08E-06,0.000445382,8.04E-05
4838,sentiment_analysis8,197,RESULTS,,,sentiment_analysis,8,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
4839,sentiment_analysis8,198,"In this section , we discuss the performance of models described in Section IV .",RESULTS,RESULTS,sentiment_analysis,8,1,1,0,,0.041998018,0,negative,0.014258338,2.41E-05,0.000823745,6.78E-06,8.12E-06,6.68E-05,0.000793433,0.000253757,1.96E-05,0.875897159,2.14E-05,0.107746615,8.02E-05
4840,sentiment_analysis8,199,"From , we can see that our simpler and lighter ML models either outperform or are comparable to the much heavier current state - of - the art on this dataset .",RESULTS,RESULTS,sentiment_analysis,8,2,1,1,results,0.949698009,1,results,0.005897868,1.60E-06,5.76E-05,6.08E-06,1.67E-06,4.49E-05,0.00631138,0.000186593,1.06E-06,0.037845519,2.09E-05,0.949088665,0.000536201
4841,sentiment_analysis8,200,A more detailed analysis follows : a),RESULTS,RESULTS,sentiment_analysis,8,3,1,0,,0.022643991,0,negative,0.034595887,1.47E-05,0.00104722,2.90E-05,8.73E-06,0.000103992,0.000399782,0.000231401,7.09E-05,0.92726522,1.64E-05,0.036018357,0.000198506
4842,sentiment_analysis8,201,Audio - only results :,RESULTS,RESULTS,sentiment_analysis,8,4,1,1,results,0.97094316,1,results,0.001368699,8.48E-07,0.000167649,4.28E-07,3.34E-07,7.27E-06,0.003361118,3.26E-05,3.16E-07,0.040511324,2.77E-05,0.954340604,0.000181198
4843,sentiment_analysis8,202,Results are especially interesting for this setting .,RESULTS,RESULTS,sentiment_analysis,8,5,1,0,,0.016768614,0,negative,0.007071481,9.15E-06,0.000182336,1.47E-05,7.70E-06,0.000141765,0.001073707,0.000694295,1.75E-05,0.900927062,3.15E-05,0.089507164,0.000321591
4844,sentiment_analysis8,203,Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .,RESULTS,RESULTS,sentiment_analysis,8,6,1,1,results,0.901687972,1,results,0.004757896,4.66E-07,5.78E-05,4.74E-07,3.85E-07,5.54E-06,0.001527045,2.05E-05,1.69E-07,0.036547344,8.47E-06,0.956980176,9.38E-05
4845,sentiment_analysis8,204,"However , neither of them are able to beat the lighter E1 model ( Ensemble of RF , XGB and MLP ) which was trained on the eightdimensional audio feature vectors .",RESULTS,RESULTS,sentiment_analysis,8,7,1,0,,0.598754247,1,results,0.006055424,1.13E-06,0.001094882,1.02E-06,8.87E-07,1.18E-05,0.001648574,2.95E-05,7.13E-07,0.089285859,1.36E-05,0.90170234,0.000154216
4846,sentiment_analysis8,205,"A look at the confusion matrix ) reveals that detecting "" neutral "" or distinguishing between "" angry "" , "" happy "" and "" sad "" is the most difficult for the model .",RESULTS,RESULTS,sentiment_analysis,8,8,1,0,,0.562481227,1,results,0.237208378,5.14E-06,0.000386451,2.99E-06,4.11E-06,2.29E-05,0.001106471,7.74E-05,5.70E-06,0.264750397,6.91E-06,0.49633743,8.58E-05
4847,sentiment_analysis8,206,b),RESULTS,RESULTS,sentiment_analysis,8,9,1,0,,0.004301549,0,negative,0.011614763,8.48E-06,0.006644685,3.44E-06,3.70E-06,7.56E-05,0.000606037,0.000276421,3.74E-05,0.924461271,1.66E-05,0.055985355,0.000266316
4848,sentiment_analysis8,207,Text - only results :,RESULTS,RESULTS,sentiment_analysis,8,10,1,1,results,0.958667905,1,results,0.002026376,1.41E-06,0.000257738,8.73E-07,1.03E-06,8.95E-06,0.003751646,3.78E-05,5.37E-07,0.079448117,1.66E-05,0.914125309,0.000323613
4849,sentiment_analysis8,208,We observe that the performance of all the models for this setting is similar .,RESULTS,RESULTS,sentiment_analysis,8,11,1,1,results,0.569228366,1,results,0.00557312,2.27E-06,3.94E-05,2.92E-06,1.66E-06,4.10E-05,0.004613596,0.000285975,1.10E-06,0.129923151,1.31E-05,0.859140592,0.000362126
4850,sentiment_analysis8,209,This could be attributed to the richness of TFIDF vectors known to capture word - sentence correlation .,RESULTS,RESULTS,sentiment_analysis,8,12,1,0,,0.002366953,0,negative,0.003635488,1.06E-05,0.000488445,4.36E-06,4.21E-06,5.04E-05,0.000105095,0.000273334,3.64E-05,0.986838992,8.03E-06,0.008456992,8.77E-05
4851,sentiment_analysis8,210,We see from the confusion matrix ( ) that our text - based models are able to distinguish the six emotions fairly well along with the end - to - end trained TRE .,RESULTS,RESULTS,sentiment_analysis,8,13,1,0,,0.803691602,1,results,0.008648305,3.87E-07,2.46E-05,4.27E-07,3.86E-07,4.21E-06,0.001779215,2.38E-05,1.49E-07,0.030699145,2.14E-06,0.958723241,9.40E-05
4852,sentiment_analysis8,211,"We observe that "" sad "" is the toughest for textual features to identify very clearly .",RESULTS,RESULTS,sentiment_analysis,8,14,1,0,,0.704982711,1,results,0.091380874,6.26E-06,0.000131817,9.74E-05,3.10E-05,0.000231271,0.004978759,0.000688388,8.45E-06,0.441222864,1.14E-05,0.459395509,0.001816003
4853,sentiment_analysis8,212,c) Audio + Text results :,RESULTS,RESULTS,sentiment_analysis,8,15,1,1,results,0.958116268,1,results,0.001636674,6.31E-07,0.000435868,1.94E-07,2.58E-07,3.69E-06,0.00257901,1.25E-05,1.66E-07,0.037315797,5.30E-06,0.957897602,0.000112312
4854,sentiment_analysis8,213,We see that combining audio and text features gives us a boost of ? 14 % for all the metrics .,RESULTS,RESULTS,sentiment_analysis,8,16,1,1,results,0.970386813,1,results,0.135807749,5.91E-06,0.000190697,8.74E-06,3.65E-06,3.27E-05,0.005597827,0.000173036,5.15E-06,0.027271387,5.46E-06,0.829665643,0.001232086
4855,sentiment_analysis8,214,This is clear evidence of the strong correlation between text and speech features .,RESULTS,RESULTS,sentiment_analysis,8,17,1,0,,0.008214641,0,negative,0.025875601,4.93E-06,0.000251513,4.40E-06,6.12E-06,4.56E-05,0.000728461,0.000193878,1.11E-05,0.813870566,4.49E-06,0.158747011,0.000256359
4856,sentiment_analysis8,215,"Also , this is the only case when the recurrent encoders seem to perform slightly better in terms of accuracy but at the cost of precision .",RESULTS,RESULTS,sentiment_analysis,8,18,1,0,,0.463883497,0,results,0.054729981,2.91E-06,8.88E-05,6.33E-06,4.36E-06,2.42E-05,0.003216556,0.000127713,1.98E-06,0.144069622,4.81E-06,0.797363342,0.000359387
4857,sentiment_analysis8,216,The lower performance of E1 maybe be attributed to the trivial fusion method ( concatenation ) we use as simple concatenation for an ML model would still contain a lot of modality - specific connections instead of the desired inter-modal connections .,RESULTS,RESULTS,sentiment_analysis,8,19,1,0,,0.012508673,0,results,0.048840732,1.00E-06,0.000113648,1.48E-06,1.84E-06,8.39E-06,0.001402611,3.16E-05,6.99E-07,0.190342055,2.48E-06,0.759127163,0.000126278
4858,sentiment_analysis8,217,The promising result here is that combining features from both the modalities indeed helped to resolve the ambiguity observed for modality - specific models as shown in .,RESULTS,RESULTS,sentiment_analysis,8,20,1,0,,0.934677198,1,results,0.048985488,2.89E-06,0.000107354,1.67E-06,2.05E-06,1.17E-05,0.002096523,5.10E-05,1.73E-06,0.104924513,4.09E-06,0.843604501,0.000206505
4859,sentiment_analysis8,218,"We can say that the textual features helped incorrect classification of "" angry "" and "" happy "" classes whereas the audio features enabled the model to detect "" sad "" better .",RESULTS,RESULTS,sentiment_analysis,8,21,1,0,,0.52676924,1,results,0.196006714,3.91E-06,0.000235761,3.53E-06,4.84E-06,2.58E-05,0.000971391,9.97E-05,6.49E-06,0.359367591,2.95E-06,0.443079755,0.000191561
4860,sentiment_analysis8,219,"Overall , we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .",RESULTS,RESULTS,sentiment_analysis,8,22,1,1,results,0.637358841,1,results,0.011381088,1.02E-06,2.35E-05,2.53E-06,1.40E-06,1.50E-05,0.004994703,9.50E-05,5.31E-07,0.047628511,2.77E-06,0.935312078,0.00054184
4861,sentiment_analysis8,220,A. Most Important Features :,RESULTS,RESULTS,sentiment_analysis,8,23,1,0,,0.390014856,0,negative,0.077282092,2.32E-05,0.005264636,8.34E-06,9.15E-06,5.73E-05,0.001424481,0.000199001,3.53E-05,0.545748087,1.54E-05,0.3693551,0.000577923
4862,sentiment_analysis8,221,"In this section , we investigate which features contribute the most during prediction in this classification task .",RESULTS,RESULTS,sentiment_analysis,8,24,1,0,,0.098905462,0,negative,0.026583467,0.000114077,0.002529058,5.85E-06,2.24E-05,3.75E-05,0.000620426,0.000230335,7.67E-05,0.853679345,1.01E-05,0.115830531,0.000260228
4863,sentiment_analysis8,222,We chose the XGB model for this study and rank the eight audio features .,RESULTS,RESULTS,sentiment_analysis,8,25,1,0,,0.161031407,0,negative,0.00430825,0.000123527,0.002454357,4.14E-05,4.50E-05,0.003973069,0.007480314,0.031851981,0.000144915,0.931862113,6.94E-06,0.013733192,0.003974951
4864,sentiment_analysis8,223,"We see that Harmonic , which is directly related to the excitation in signals , contributes the most .",RESULTS,RESULTS,sentiment_analysis,8,26,1,0,,0.956226892,1,ablation-analysis,0.69983921,6.73E-06,0.000259423,8.74E-05,1.30E-05,6.53E-05,0.002928984,0.0003165,2.09E-05,0.079680436,1.63E-06,0.215270379,0.001510084
4865,sentiment_analysis8,224,"It is interesting to see that "" silence "" attributing to Pause , is almost as significant as standard deviation of the autocorrelated signal ( related to pitch ) .",RESULTS,RESULTS,sentiment_analysis,8,27,1,0,,0.112573176,0,ablation-analysis,0.559285949,4.48E-06,0.000262242,1.50E-05,9.73E-06,2.68E-05,0.002174564,0.000139942,6.88E-06,0.147156805,1.33E-06,0.290472121,0.000444228
4866,sentiment_analysis8,225,The low contribution of central moments is expected as a signal is very diverse and an global / coarse feature would be unable to identify the nuances present in it .,RESULTS,RESULTS,sentiment_analysis,8,28,1,0,,0.108634216,0,results,0.315839838,7.05E-06,0.000490225,1.04E-05,1.07E-05,2.89E-05,0.000963869,0.000144098,1.25E-05,0.324107899,1.89E-06,0.358053707,0.00032903
4867,sentiment_analysis8,226,VI .,RESULTS,RESULTS,sentiment_analysis,8,29,1,0,,0.00067074,0,negative,0.007141702,5.83E-06,0.00015031,7.47E-06,4.60E-06,7.57E-05,0.000224139,0.000536554,3.29E-05,0.979922071,1.11E-06,0.011584454,0.000313163
4868,sentiment_analysis8,227,CONCLUSION AND FUTURE WORK,RESULTS,RESULTS,sentiment_analysis,8,30,1,0,,0.001615902,0,negative,0.00455151,9.18E-06,0.000432033,7.49E-06,7.03E-06,0.000132791,0.001362358,0.000590445,4.26E-05,0.931040081,1.68E-05,0.060248498,0.001559188
4869,sentiment_analysis8,228,"In this work , we tackle the task of speech emotion recognition and study the contribution of different modalities towards ambiguity resolution on the IEMOCAP dataset .",RESULTS,RESULTS,sentiment_analysis,8,31,1,0,,0.542094526,1,results,0.025565541,0.000201814,0.004203712,0.000260869,0.000327226,0.00020449,0.014424883,0.000518743,4.01E-05,0.239352039,0.00014457,0.695506546,0.01924942
4870,sentiment_analysis8,229,"We compare , both , ML - and DL - based models and show that even lighter and more interpretable ML models can achieve performance close to DL - based models .",RESULTS,RESULTS,sentiment_analysis,8,32,1,0,,0.594165396,1,results,0.015996287,1.04E-05,0.000232789,2.01E-06,4.38E-06,1.60E-05,0.001808249,0.000119356,6.32E-06,0.297877367,2.81E-06,0.683563675,0.000360313
4871,sentiment_analysis8,230,We show that ensembling multiple ML models also lead to some improvement in the performance .,RESULTS,RESULTS,sentiment_analysis,8,33,1,0,,0.361063424,0,results,0.065590329,1.68E-05,0.000301501,3.71E-06,5.63E-06,1.88E-05,0.001593516,0.000208888,1.77E-05,0.322088396,1.96E-06,0.609606194,0.000546645
4872,sentiment_analysis8,231,We only extract a handful of time - domain features from audio signals .,RESULTS,RESULTS,sentiment_analysis,8,34,1,0,,0.04772834,0,negative,0.00817864,4.03E-05,0.001052189,0.000146072,8.00E-05,0.000485586,0.003632313,0.001935842,4.05E-05,0.867064221,5.54E-05,0.101770902,0.01551811
4873,sentiment_analysis8,232,"The audio feature - space could be made even richer if we could include some frequency - domain features too such as Mel - Frequency Cepstral Coefficients ( MFCC ) , Spectral Roll - off and additional timedomain features such as Zero Crossing Rate ( ZCR ) .",RESULTS,RESULTS,sentiment_analysis,8,35,1,0,,0.055515902,0,negative,0.012191711,3.72E-05,0.001887602,1.46E-05,1.81E-05,0.000113485,0.000838521,0.000663085,0.000163485,0.912542384,9.29E-06,0.069500829,0.002019678
4874,sentiment_analysis8,233,"Also , better fusion methods such as TFN and LMF could be employed for combining speech and text vectors more effectively .",RESULTS,RESULTS,sentiment_analysis,8,36,1,0,,0.179066647,0,negative,0.014904963,1.91E-05,0.000320165,4.39E-05,1.49E-05,0.000269019,0.002223567,0.002471027,6.47E-05,0.867474956,1.02E-05,0.107381182,0.004802286
4875,sentiment_analysis8,234,It would also be interesting to see the scaling in the performance of ML models v/s DL models if include more data .,RESULTS,RESULTS,sentiment_analysis,8,37,1,0,,0.00442251,0,negative,0.002701193,4.31E-06,0.000159764,3.19E-06,2.45E-06,6.34E-05,0.000641148,0.000569302,2.02E-05,0.962657686,2.89E-06,0.032486614,0.000687828
4876,sentiment_analysis13,1,title,,,sentiment_analysis,13,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
4877,sentiment_analysis13,2,BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis,title,title,sentiment_analysis,13,1,1,1,research-problem,0.989014612,1,research-problem,1.33E-07,1.29E-05,1.87E-07,2.88E-06,1.14E-06,7.21E-07,1.02E-05,3.05E-06,4.56E-07,0.003452153,0.996515198,4.89E-07,4.93E-07
4878,sentiment_analysis13,3,abstract,,,sentiment_analysis,13,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
4879,sentiment_analysis13,4,Question - answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making .,abstract,abstract,sentiment_analysis,13,1,1,0,,0.555867521,1,research-problem,1.46E-08,3.73E-06,6.47E-09,6.81E-07,2.34E-07,1.64E-07,3.33E-07,8.40E-07,1.57E-07,0.007924322,0.992069465,1.32E-08,3.51E-08
4880,sentiment_analysis13,5,"Inspired by the recent success of machine reading comprehension ( MRC ) on formal documents , this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions .",abstract,abstract,sentiment_analysis,13,2,1,0,,0.938895291,1,research-problem,9.88E-07,0.000990888,2.48E-06,4.18E-06,1.50E-05,1.21E-06,3.94E-06,8.00E-06,1.60E-05,0.018014251,0.980941586,9.13E-07,5.33E-07
4881,sentiment_analysis13,6,We call this problem Review Reading Comprehension ( RRC ) .,abstract,abstract,sentiment_analysis,13,3,1,0,,0.841073151,1,research-problem,1.28E-08,8.20E-06,1.26E-08,7.65E-07,3.66E-07,2.12E-07,4.28E-07,1.26E-06,4.88E-07,0.012628486,0.987359707,1.48E-08,4.11E-08
4882,sentiment_analysis13,7,"To the best of our knowledge , no existing work has been done on RRC .",abstract,abstract,sentiment_analysis,13,4,1,0,,0.002471949,0,research-problem,1.04E-07,0.000148833,1.25E-07,3.83E-06,3.70E-06,5.28E-06,1.46E-06,3.47E-05,1.19E-05,0.225659716,0.774130121,8.15E-08,1.41E-07
4883,sentiment_analysis13,8,"In this work , we first build an RRC dataset called ReviewRC based on a popular benchmark for aspectbased sentiment analysis .",abstract,abstract,sentiment_analysis,13,5,1,0,,0.521706567,1,negative,4.84E-05,0.16689074,0.000199348,0.002567928,0.225585068,0.000251634,0.000254629,0.000681015,0.000809918,0.492254119,0.110407382,2.87E-05,2.11E-05
4884,sentiment_analysis13,9,"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC .",abstract,abstract,sentiment_analysis,13,6,1,0,,0.536158716,1,approach,0.000130572,0.631536166,0.00014284,0.000137952,0.002060372,7.96E-05,5.01E-05,0.00100616,0.009473826,0.236358951,0.118983699,3.08E-05,8.97E-06
4885,sentiment_analysis13,10,"To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .",abstract,abstract,sentiment_analysis,13,7,1,1,research-problem,0.027975219,0,negative,4.33E-05,0.105042036,1.39E-05,2.86E-05,0.000818137,3.82E-05,1.85E-05,0.00071252,0.001191359,0.847399764,0.044669501,2.26E-05,1.60E-06
4886,sentiment_analysis13,11,Experimental results demonstrate that the proposed posttraining is highly effective 1 .,abstract,abstract,sentiment_analysis,13,8,1,0,,0.00255743,0,negative,0.000185384,0.001927033,3.01E-06,6.63E-06,0.000122514,9.49E-06,1.87E-05,0.000104691,5.59E-05,0.958149292,0.03930348,0.000112985,8.42E-07
4887,sentiment_analysis13,12,Introduction,,,sentiment_analysis,13,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
4888,sentiment_analysis13,13,"For online commerce , question - answering ( QA ) serves either as a standalone application of customer service or as a crucial component of a dialogue system that answers user questions .",Introduction,Introduction,sentiment_analysis,13,1,1,0,,0.966003985,1,research-problem,9.01E-07,0.000104196,2.86E-07,2.46E-05,3.15E-05,7.01E-06,1.93E-05,5.39E-06,1.98E-05,0.031662712,0.968119742,1.26E-06,3.28E-06
4889,sentiment_analysis13,14,Many intelligent personal assistants ( such as Amazon Alexa and Google Assistant ) support online shopping by allowing the user to speak directly to the assistants .,Introduction,Introduction,sentiment_analysis,13,2,1,0,,0.038839309,0,research-problem,4.04E-06,0.001075934,1.60E-06,6.56E-05,0.000242273,4.63E-05,3.16E-05,3.82E-05,0.000268958,0.202948292,0.795267972,3.29E-06,6.00E-06
4890,sentiment_analysis13,15,"One major hindrance for this mode of shopping is that such systems have limited capability to answer user questions about products ( or services ) , which are vital for customer decision making .",Introduction,Introduction,sentiment_analysis,13,3,1,0,,0.056768079,0,research-problem,4.46E-06,0.000811886,8.96E-07,0.000100882,0.000203474,4.42E-05,1.85E-05,3.30E-05,0.000205098,0.359583957,0.63898698,2.64E-06,4.08E-06
4891,sentiment_analysis13,16,"As such , an intelligent agent that can automatically answer customers ' questions is very important for the success of online businesses .",Introduction,Introduction,sentiment_analysis,13,4,1,0,,0.683405899,1,research-problem,1.59E-06,0.00074167,2.80E-07,3.09E-06,1.20E-05,8.70E-06,9.92E-06,2.52E-05,0.000649172,0.119661101,0.878883697,2.21E-06,1.42E-06
4892,sentiment_analysis13,17,"Given the ever - changing environment of products and services , it is very hard , if not impossible , to pre-compile an up - to - date and reliable knowledge base to cover a wide assortment of questions that customers may ask , such as in factoidbased KB - QA .",Introduction,Introduction,sentiment_analysis,13,5,1,0,,0.118067644,0,research-problem,8.96E-07,0.000437577,2.02E-07,4.72E-06,1.88E-05,8.94E-06,8.16E-06,1.66E-05,0.000128608,0.142861466,0.856511828,1.17E-06,1.10E-06
4893,sentiment_analysis13,18,"As a compromise , many online businesses leverage community question - answering ( CQA ) to crowdsource answers from existing customers .",Introduction,Introduction,sentiment_analysis,13,6,1,0,,0.75094322,1,research-problem,2.05E-06,0.000474087,6.48E-07,2.37E-05,6.59E-05,1.54E-05,2.29E-05,1.55E-05,6.93E-05,0.107375721,0.891929529,1.86E-06,3.48E-06
4894,sentiment_analysis13,19,"However , the problem with this approach is that many questions are not answered , and if they are answered , the answers are delayed , which is not suitable for interactive QA .",Introduction,Introduction,sentiment_analysis,13,7,1,0,,0.03130939,0,research-problem,3.19E-06,0.000477052,6.45E-07,1.66E-05,5.20E-05,1.48E-05,1.63E-05,1.60E-05,8.71E-05,0.235376812,0.763934412,2.75E-06,2.28E-06
4895,sentiment_analysis13,20,"In this paper , we explore the potential of using product reviews as a large source of user experiences that can be exploited to obtain answers to user questions .",Introduction,Introduction,sentiment_analysis,13,8,1,0,,0.943998987,1,approach,0.00014488,0.604129337,0.000174643,0.000284503,0.017979718,0.000213216,0.000333901,0.000385963,0.047773702,0.155441343,0.172980218,0.000109338,4.92E-05
4896,sentiment_analysis13,21,"Although there are existing studies that have used information retrieval ( IR ) techniques to find a whole review as the response to a user question , giving the whole review to the user is undesirable as it is quite time - consuming for the user to read it .",Introduction,Introduction,sentiment_analysis,13,9,1,0,,0.554362548,1,research-problem,1.80E-06,0.000286825,3.48E-07,2.18E-05,4.53E-05,1.53E-05,1.78E-05,1.61E-05,3.91E-05,0.167622393,0.831928856,1.77E-06,2.62E-06
4897,sentiment_analysis13,22,"Inspired by the success of Machine Reading Comphrenesions ( MRC ) , we propose a novel task called Review Reading Comprehension ( RRC ) as following .",Introduction,Introduction,sentiment_analysis,13,10,1,0,,0.959306721,1,research-problem,1.45E-05,0.013883431,2.22E-05,3.03E-05,0.000559531,2.39E-05,0.000154989,3.83E-05,0.001897675,0.093910159,0.889422283,2.83E-05,1.44E-05
4898,sentiment_analysis13,23,Problem Definition :,Introduction,Introduction,sentiment_analysis,13,11,1,0,,0.008304634,0,negative,2.68E-06,0.005262391,1.89E-06,4.49E-06,6.00E-05,4.80E-05,1.95E-05,9.65E-05,0.00786354,0.696283814,0.290351868,3.34E-06,1.98E-06
4899,sentiment_analysis13,24,"Given a question q = ( q 1 , . . . , q m ) from a customer ( or user ) about a product and a review d = ( d 1 , . . . , d n ) for that product containing the information to answer q , find a sequence of tokens ( a text span ) a = ( d s , . . . , d e ) ind that answers q correctly , where 1 ? s ? n , 1 ? e ? n , and s ?",Introduction,Introduction,sentiment_analysis,13,12,1,0,,0.113992979,0,negative,0.000128448,0.14149048,7.49E-05,7.21E-06,0.002020672,7.31E-05,0.000118854,0.000120414,0.077724903,0.739889563,0.038257501,8.86E-05,5.31E-06
4900,sentiment_analysis13,25,e.,Introduction,Introduction,sentiment_analysis,13,13,1,0,,0.001434176,0,negative,2.54E-05,0.001574749,2.89E-06,3.74E-05,0.000524118,0.000141372,2.27E-05,9.74E-05,0.002350725,0.993569479,0.001647851,4.02E-06,1.83E-06
4901,sentiment_analysis13,26,A sample laptop review is shown in .,Introduction,,sentiment_analysis,13,14,1,0,,0.003341116,0,negative,9.35E-05,0.013752527,5.36E-05,0.000625365,0.0452234,0.000897768,0.000320377,0.000249902,0.007531921,0.927792328,0.003386314,4.58E-05,2.72E-05
4902,sentiment_analysis13,27,We can see that customers may not only ask factoid Questions Q1 :,Introduction,A sample laptop review is shown in .,sentiment_analysis,13,15,1,0,,0.003099721,0,negative,4.85E-05,6.03E-05,1.87E-06,6.72E-07,7.21E-05,3.43E-05,7.44E-06,8.07E-07,0.000415966,0.996797301,0.002558667,8.49E-07,1.21E-06
4903,sentiment_analysis13,28,Does it have an internal hard drive ?,Introduction,A sample laptop review is shown in .,sentiment_analysis,13,16,1,0,,0.000749851,0,negative,2.76E-05,0.00011099,5.77E-06,2.23E-06,9.28E-05,9.77E-05,5.55E-06,2.37E-06,0.001924936,0.997312347,0.00041535,2.99E-07,2.00E-06
4904,sentiment_analysis13,29,Q2 : How large is the internal hard drive ?,Introduction,A sample laptop review is shown in .,sentiment_analysis,13,17,1,0,,0.003421484,0,negative,9.56E-05,0.000538907,6.41E-06,8.47E-06,0.000145877,0.000274122,2.20E-05,1.04E-05,0.003618156,0.99161162,0.003658587,1.04E-06,8.82E-06
4905,sentiment_analysis13,30,Q3 : is the capacity of the internal hard drive OK ?,Introduction,A sample laptop review is shown in .,sentiment_analysis,13,18,1,0,,0.005072741,0,negative,5.42E-05,0.000250913,4.04E-06,4.36E-06,8.55E-05,0.000187562,1.66E-05,6.96E-06,0.002370815,0.994047852,0.002964044,8.27E-07,6.30E-06
4906,sentiment_analysis13,31,Review Excellent value and a must buy for someone looking for a Macbook .,Introduction,A sample laptop review is shown in .,sentiment_analysis,13,19,1,0,,0.002196539,0,negative,0.000107642,0.000101578,9.37E-06,3.54E-06,0.000489093,0.000152414,2.13E-05,2.26E-06,0.001238363,0.997634701,0.000235188,1.30E-06,3.23E-06
4907,sentiment_analysis13,32,You ca n't get any better than this price and it come with A1 an internal disk drive .,Introduction,A sample laptop review is shown in .,sentiment_analysis,13,20,1,0,,0.000336124,0,negative,0.000101027,8.77E-05,8.85E-06,1.65E-05,0.001208075,0.000385769,2.23E-05,3.56E-06,0.000663884,0.997370737,0.000124683,7.57E-07,6.11E-06
4908,sentiment_analysis13,33,All the newer MacBooks do not .,Introduction,,sentiment_analysis,13,21,1,0,,0.00038902,0,negative,9.34E-06,0.000569623,2.15E-06,5.59E-05,0.001530322,0.000187987,5.21E-05,7.29E-05,0.000230476,0.996191902,0.001089586,5.23E-06,2.42E-06
4909,sentiment_analysis13,34,Plus you get 500 GB A2 which is also a great A3 feature .,Introduction,All the newer MacBooks do not .,sentiment_analysis,13,22,1,0,,0.019484965,0,negative,0.00150084,0.000808598,0.000232672,0.001607869,0.021318666,0.004816607,0.000585234,0.000291457,0.004507483,0.962907957,0.000656952,0.000198761,0.000566904
4910,sentiment_analysis13,35,"Also , the resale value on this will keep .",Introduction,All the newer MacBooks do not .,sentiment_analysis,13,23,1,0,,0.001177742,0,negative,0.000451551,0.000676635,9.01E-05,4.72E-06,0.000411568,9.24E-05,3.48E-05,3.64E-05,0.018978605,0.979035088,0.000124648,5.71E-05,6.39E-06
4911,sentiment_analysis13,36,"I highly recommend you get one before they are gone . questions such as the specs about some aspects of the laptop as in the first and second questions but also subjective or opinion questions about some aspects ( capacity of the hard drive ) , as in the third question .",Introduction,All the newer MacBooks do not .,sentiment_analysis,13,24,1,0,,0.000471868,0,negative,2.40E-05,3.39E-05,4.94E-06,1.22E-05,0.000247985,0.00015774,1.26E-05,2.16E-05,0.000264493,0.999141275,6.75E-05,7.36E-06,4.36E-06
4912,sentiment_analysis13,37,"RRC poses some domain challenges compared to the traditional MRC on Wikipedia , such as the need for rich product knowledge , informal text , and fine - grained opinions ( there is almost no subjective content in Wikipedia articles ) .",Introduction,All the newer MacBooks do not .,sentiment_analysis,13,25,1,0,,0.621357593,1,negative,0.00024209,0.001136466,7.05E-05,0.000286223,0.001978183,0.000599394,0.000788652,0.00010326,0.00054498,0.696109599,0.296751715,0.000357696,0.001031227
4913,sentiment_analysis13,38,Research also shows that yes / no questions are very frequent for products with complicated specifications .,Introduction,All the newer MacBooks do not .,sentiment_analysis,13,26,1,0,,0.018265858,0,negative,4.14E-05,8.72E-05,4.11E-06,8.32E-06,0.00033025,0.000104763,3.32E-05,2.72E-05,0.000144535,0.998264806,0.0009165,2.69E-05,1.08E-05
4914,sentiment_analysis13,39,"To the best of our knowledge , no existing work has been done in RRC .",Introduction,All the newer MacBooks do not .,sentiment_analysis,13,27,1,0,,0.002640144,0,negative,3.68E-05,0.000689051,5.73E-05,4.53E-05,0.000907213,0.000404721,9.64E-05,9.14E-05,0.001726894,0.986372519,0.009464008,3.27E-05,7.58E-05
4915,sentiment_analysis13,40,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",Introduction,All the newer MacBooks do not .,sentiment_analysis,13,28,1,1,dataset,0.801669187,1,dataset,0.000244543,0.012484181,0.000941596,0.000632624,0.909238574,0.000540925,0.000372534,5.29E-05,0.001483212,0.073584699,0.000141635,0.000149994,0.000132603
4916,sentiment_analysis13,41,We detail ReviewRC in Sec.,Introduction,,sentiment_analysis,13,29,1,0,,0.005133096,0,negative,0.000154567,0.022248732,5.70E-05,0.000118993,0.017169743,0.000269428,0.000114146,0.000116768,0.006105162,0.9533607,0.000243167,3.63E-05,5.37E-06
4917,sentiment_analysis13,42,"5 . Given the wide spectrum of domains ( types of products or services ) in online businesses and the prohibitive cost of annotation , ReviewRC can only be considered to have a limited number of annotated examples for supervised training , which still leaves the domain challenges partially unresolved .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,30,1,0,,0.009000906,0,negative,0.000512328,0.000286623,2.66E-05,0.000376161,0.004627684,0.000931867,0.000137823,4.51E-05,0.000233173,0.991384641,0.001247918,8.68E-06,0.000181372
4918,sentiment_analysis13,43,This work adopts BERT ) as the base model as it achieves the state - of the - art performance on MRC .,Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,31,1,1,model,0.790780533,1,model,0.000828306,0.186564891,0.018026848,1.05E-05,0.005059754,0.000348678,0.000575632,6.78E-05,0.692860713,0.094142077,0.001291504,2.94E-05,0.000193961
4919,sentiment_analysis13,44,"Although BERT aims to learn contextualized representations across a wide range of NLP tasks ( to be task - agnostic ) , leveraging BERT alone still leaves the domain challenges un - 2 http://alt.qcri.org/semeval2016/",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,32,1,0,,0.253999358,0,negative,0.000492376,0.000700449,0.000133642,0.000140306,0.004289493,0.000372881,0.00019317,1.90E-05,0.000456768,0.981712389,0.011241613,2.23E-05,0.000225649
4920,sentiment_analysis13,45,task5/.,Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,33,1,0,,0.026983868,0,negative,0.001028684,0.000179904,0.000253756,1.21E-05,0.00648782,0.000308001,0.000130159,1.10E-05,0.000567058,0.99095862,1.56E-05,1.96E-05,2.76E-05
4921,sentiment_analysis13,46,"We choose these review datasets to align RRC with existing research on sentiment analysis. resolved ( as BERT is trained on Wikipedia articles and has almost no understanding of opinion text ) , and it also introduces another challenge of task - awareness ( the RRC task ) , called the task challenge .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,34,1,0,,0.020174973,0,dataset,0.000102195,0.004939634,0.000355579,0.000188588,0.61288856,0.000562972,0.000178358,2.71E-05,0.000900768,0.379621766,0.000124283,1.06E-05,9.96E-05
4922,sentiment_analysis13,47,"This challenge arises when the taskagnostic BERT meets the limited number of finetuning examples in ReviewRC ( see Sec. 5 ) for RRC , which is insufficient to fine - tune BERT to ensure full task - awareness of the system 3 .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,35,1,0,,0.013088583,0,negative,0.000195302,0.000812774,3.01E-05,2.27E-05,0.000576127,0.000193349,6.73E-05,3.35E-05,0.001809103,0.993499012,0.002699052,8.33E-06,5.34E-05
4923,sentiment_analysis13,48,"To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,36,1,1,model,0.800788018,1,model,0.001659186,0.245393126,0.003151576,2.67E-05,0.010667887,0.000290664,0.000260028,7.45E-05,0.696405313,0.041708111,0.000208725,1.99E-05,0.000134269
4924,sentiment_analysis13,49,"This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,37,1,1,model,0.561084626,1,negative,0.004122663,0.091965851,0.034773834,5.86E-05,0.031970273,0.00109013,0.00083131,6.44E-05,0.349381691,0.483857372,0.00129362,6.08E-05,0.0005295
4925,sentiment_analysis13,50,"As a general - purpose approach , we show that the proposed method can also benefit ABSA tasks such as aspect extraction ( AE ) and aspect sentiment classification ( ASC ) .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,38,1,0,,0.417492069,0,negative,0.018141919,0.071071898,0.000836668,3.64E-05,0.005265672,0.000718279,0.003753889,0.000236997,0.093773112,0.803766555,0.001036441,0.000802431,0.000559727
4926,sentiment_analysis13,51,The main contributions of this paper are as follows .,Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,39,1,0,,0.000105631,0,negative,0.000123063,0.000555158,6.39E-05,8.20E-05,0.002491112,0.000737743,6.40E-05,5.71E-05,0.002486601,0.993193634,6.52E-05,1.73E-06,7.87E-05
4927,sentiment_analysis13,52,( 1 ) It proposes the new problem of review reading comprehension ( RRC ) .,Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,40,1,0,,0.388216434,0,negative,0.001123229,0.010150844,0.002486028,0.000122335,0.012508157,0.001269453,0.001496328,7.27E-05,0.011322004,0.929727092,0.028204774,5.99E-05,0.001457204
4928,sentiment_analysis13,53,"( 2 ) To solve this new problem , an annotated dataset for RRC is created .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,41,1,0,,0.034129926,0,negative,0.001004123,0.012474377,0.00201323,9.71E-06,0.023391615,0.000179549,0.000113705,1.42E-05,0.022439939,0.938044132,0.000249907,2.54E-05,4.01E-05
4929,sentiment_analysis13,54,"( 3 ) It proposes a general - purpose posttraining approach to improve RRC , AE , and ASC .",Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,42,1,0,,0.385449841,0,negative,0.008131326,0.031247138,0.037323997,2.62E-05,0.03293383,0.000736233,0.000734709,3.75E-05,0.145821983,0.74260536,0.000129275,5.68E-05,0.000215671
4930,sentiment_analysis13,55,Experimental results demonstrate that the proposed approach is effective .,Introduction,We detail ReviewRC in Sec.,sentiment_analysis,13,43,1,0,,0.004128565,0,negative,0.002568622,0.000769311,2.53E-05,2.40E-06,0.001224132,0.000135882,0.001036545,3.09E-05,0.000497099,0.99338434,1.92E-05,0.000250966,5.54E-05
4931,sentiment_analysis13,56,Related Works,,,sentiment_analysis,13,0,1,0,,0.000144394,0,negative,8.00E-06,1.03E-05,1.47E-06,1.57E-07,1.75E-07,1.81E-05,1.36E-05,0.000169567,3.75E-06,0.999096229,0.000638872,3.92E-05,4.90E-07
4932,sentiment_analysis13,136,Post - training,,,sentiment_analysis,13,0,1,0,,0.070044403,0,negative,0.000651712,0.000108398,0.000140674,1.79E-06,4.27E-06,0.000132671,0.000787997,0.001080418,4.32E-05,0.970978887,0.001197488,0.024855682,1.68E-05
4933,sentiment_analysis13,137,"As discussed in the introduction , fine - tuning BERT directly on the end task that has limited tuning data faces both domain challenges and taskawareness challenge .",Post - training,Post - training,sentiment_analysis,13,1,1,0,,0.079042818,0,negative,0.001650904,1.15E-05,0.001363901,1.60E-05,1.13E-06,9.28E-05,0.000709815,0.000417488,1.04E-05,0.973919254,0.001938556,0.019385052,0.000483248
4934,sentiment_analysis13,138,"To enhance the performance of RRC ( and also AE and ASC ) , we may need to reduce the bias introduced by non-review knowledge ( e.g. , from Wikipedia corpora ) and fuse domain knowledge ( DK ) ( from unsupervised domain data ) and task knowledge ( from supervised MRC task but out - of - domain data ) .",Post - training,Post - training,sentiment_analysis,13,2,1,0,,0.005542079,0,negative,0.0066067,2.31E-05,0.013071789,1.09E-05,2.82E-06,5.17E-05,0.001717541,0.000184807,1.10E-05,0.760773743,0.002124682,0.214747869,0.000673372
4935,sentiment_analysis13,139,"Given MRC is a general task with answers of questions covering almost all document contents , a large - scale MRC supervised corpus may also benefit AE and ASC .",Post - training,Post - training,sentiment_analysis,13,3,1,0,,0.171085991,0,negative,0.0045625,7.64E-06,0.012907697,4.71E-06,4.76E-06,3.70E-05,0.003411347,8.36E-05,3.62E-06,0.501321166,0.000246913,0.476815873,0.000593216
4936,sentiment_analysis13,140,"Eventually , we aim to have a general - purpose post - training strategy that can exploit the above two kinds of knowledge for end tasks .",Post - training,Post - training,sentiment_analysis,13,4,1,0,,0.005422823,0,negative,0.000848194,5.21E-05,0.004170917,6.91E-06,2.77E-06,6.86E-05,9.46E-05,0.000485265,0.000237546,0.992092618,2.01E-05,0.001840016,8.03E-05
4937,sentiment_analysis13,141,"To post-train on domain knowledge , we leverage the two novel pre-training objectives from BERT : masked language model ( MLM ) and next sentence 7 prediction ( NSP ) .",Post - training,Post - training,sentiment_analysis,13,5,1,0,,0.172153795,0,baselines,0.005625832,0.000478516,0.497029353,1.06E-05,1.76E-05,0.000131482,0.001859395,0.000701297,0.000391734,0.478119645,5.37E-05,0.015227253,0.000353583
4938,sentiment_analysis13,142,The former predicts randomly masked words and the latter detects whether two sides of the input are from the same document or not .,Post - training,Post - training,sentiment_analysis,13,6,1,0,,0.047296375,0,baselines,0.000320338,7.94E-06,0.790313644,2.97E-07,3.06E-07,1.88E-05,0.000139099,5.04E-05,0.000162555,0.208213948,9.55E-06,0.000722255,4.09E-05
4939,sentiment_analysis13,143,"A training example is formulated as ( [ CLS ] , x 1:j , [ SEP ] , x j+1:n , [ SEP ] ) , where x 1:n is a document ( with randomly masked words ) split into two sides x 1:j and x j+1:n and [ SEP ] separates those two .",Post - training,Post - training,sentiment_analysis,13,7,1,0,,0.000151474,0,negative,3.12E-05,4.31E-06,0.002697353,6.79E-07,4.64E-07,4.67E-05,0.000117079,0.000342624,3.20E-05,0.996096564,1.10E-05,0.000559774,6.03E-05
4940,sentiment_analysis13,144,MLM is crucial for injecting review domain knowledge and for alleviating the bias of the knowledge from Wikipedia .,Post - training,Post - training,sentiment_analysis,13,8,1,0,,0.502471263,1,negative,0.002152438,3.40E-05,0.012970095,9.24E-05,6.29E-06,0.000342812,0.016112927,0.000941225,2.41E-05,0.754034488,0.049890921,0.141697226,0.021700978
4941,sentiment_analysis13,145,"For example , in the Wikipedia domain , BERT may learn to guess the [ MASK ] in "" The [ MASK ] is bright "" as "" sun "" .",Post - training,Post - training,sentiment_analysis,13,9,1,0,,0.000110891,0,negative,9.13E-05,2.19E-06,0.003139887,2.20E-06,5.74E-07,5.77E-05,0.000167381,0.000186086,1.64E-05,0.995034327,2.08E-05,0.001148407,0.000132725
4942,sentiment_analysis13,146,"But in a laptop domain , it could be "" screen "" .",Post - training,Post - training,sentiment_analysis,13,10,1,0,,2.46E-05,0,negative,7.75E-05,2.56E-07,0.000196512,6.66E-07,2.03E-07,2.35E-05,3.24E-05,5.02E-05,2.33E-06,0.9991827,1.51E-06,0.000421679,1.05E-05
4943,sentiment_analysis13,147,"Further , if the [ MASK ] ed word is an opinion word in "" The touchscreen is [ MASK ] "" , this objective challenges BERT to learn the representations for fine - grained opinion words like "" great "" or "" terrible "" for .",Post - training,Post - training,sentiment_analysis,13,11,1,0,,0.00028095,0,negative,0.000197561,1.94E-06,0.000541616,1.63E-06,5.63E-07,1.45E-05,4.29E-05,6.17E-05,7.04E-06,0.996077327,2.31E-05,0.002982996,4.71E-05
4944,sentiment_analysis13,148,The objective of NSP further encourages BERT to learn contextual representation beyond word - level .,Post - training,Post - training,sentiment_analysis,13,12,1,0,,0.116405088,0,negative,0.002728404,1.33E-05,0.176391706,6.72E-07,1.17E-06,1.92E-05,0.000300097,5.86E-05,2.03E-05,0.800866878,2.85E-05,0.01949124,8.00E-05
4945,sentiment_analysis13,149,"In the context of reviews , NSP formulates a task of "" artificial review prediction "" , where a negative example is an original review but a positive example is a synthesized fake review by combining two different reviews .",Post - training,Post - training,sentiment_analysis,13,13,1,0,,0.094206658,0,negative,0.002123992,0.000174566,0.079063445,0.00022805,5.17E-05,0.000397071,0.018294317,0.001015456,0.00010212,0.736972224,0.01932554,0.114983274,0.027268291
4946,sentiment_analysis13,150,"This task exploits the rich relationships between two sides in the input , such as whether two sides of texts have the same rating or not ( when two reviews with different ratings are combined as a positive example ) , or whether two sides are targeting the same product or not ( when two reviews from different products are merged as a positive example ) .",Post - training,Post - training,sentiment_analysis,13,14,1,0,,0.002220902,0,negative,0.000653021,3.79E-05,0.015417011,2.52E-05,1.12E-05,0.000109361,0.001374497,0.000387468,4.11E-05,0.948337424,0.000582874,0.030891752,0.002131209
4947,sentiment_analysis13,151,"In summary , these two objectives encourage to learn a myriad of fine - grained features for potential end tasks .",Post - training,Post - training,sentiment_analysis,13,15,1,0,,0.00034612,0,negative,0.000412908,9.86E-06,0.001209485,2.14E-06,1.02E-06,1.92E-05,4.75E-05,0.000113517,7.03E-05,0.995401322,7.87E-06,0.0026432,6.17E-05
4948,sentiment_analysis13,152,"We let the loss function of MLM be L MLM and the loss function of next text piece prediction be L NSP , the total loss of the domain knowledge posttraining is L DK = L MLM + L NSP .",Post - training,Post - training,sentiment_analysis,13,16,1,0,,0.001354842,0,negative,0.000227952,3.80E-06,0.00167052,4.23E-07,4.79E-07,8.84E-05,0.000274975,0.001730077,2.91E-05,0.995241854,8.01E-07,0.00068034,5.13E-05
4949,sentiment_analysis13,153,"To post-train BERT on task - aware knowledge , we use SQuAD ( 1.1 ) , which is a popular largescale MRC dataset .",Post - training,Post - training,sentiment_analysis,13,17,1,0,,0.170917405,0,negative,0.001833233,0.000204023,0.063317944,2.19E-05,9.65E-05,0.000320375,0.007551791,0.001651636,6.11E-05,0.903208328,6.70E-06,0.020901955,0.000824625
4950,sentiment_analysis13,154,"Although BERT gains great success on SQuAD , this success is based on the huge amount of training examples of SQuAD ( 100,000 + ) .",Post - training,Post - training,sentiment_analysis,13,18,1,0,,0.000791546,0,negative,0.000384486,1.90E-06,0.002452886,1.80E-06,1.89E-06,4.37E-05,0.001687965,9.79E-05,1.55E-06,0.934896926,4.87E-05,0.059580808,0.000799488
4951,sentiment_analysis13,155,This amount is large enough to ameliorate the flaws of BERT that has almost no questions on the left side and no textual span predictions based on both the question and the document on the right side .,Post - training,Post - training,sentiment_analysis,13,19,1,0,,0.000118051,0,negative,0.001309523,3.16E-06,0.001178841,9.48E-07,5.52E-06,6.42E-05,0.00053828,0.000364844,8.15E-06,0.992874159,3.91E-07,0.003568961,8.30E-05
4952,sentiment_analysis13,156,"However , a small amount of finetuning examples is not sufficient to turn BERT to be more task - aware , as shown in Sec. 5 .",Post - training,Post - training,sentiment_analysis,13,20,1,0,,0.001676967,0,negative,0.074752829,2.34E-06,0.000532863,6.90E-07,1.64E-06,1.54E-05,0.001086621,0.000104953,2.27E-06,0.564273127,1.32E-06,0.359144118,8.18E-05
4953,sentiment_analysis13,157,"We let the loss on SQuAD be L MRC , which is in a similar setting as the loss L RRC for RRC .",Post - training,Post - training,sentiment_analysis,13,21,1,0,,0.000167706,0,negative,0.000252649,1.28E-05,0.027912719,2.52E-07,4.36E-07,3.18E-05,0.000227577,0.000396844,6.13E-05,0.969474821,1.43E-06,0.001601466,2.59E-05
4954,sentiment_analysis13,158,"As a result , the joint loss of post - training is defined as L = L DK + L MRC .",Post - training,Post - training,sentiment_analysis,13,22,1,0,,7.46E-05,0,negative,0.000112806,2.07E-06,0.001211465,8.45E-08,1.00E-07,7.91E-06,2.65E-05,9.93E-05,2.91E-05,0.998172608,3.36E-07,0.00033039,7.39E-06
4955,sentiment_analysis13,159,One major issue of post - training on such a loss is the prohibitive cost of GPU memory usage .,Post - training,Post - training,sentiment_analysis,13,23,1,0,,0.000138188,0,negative,0.001801968,4.93E-06,0.000821806,8.85E-06,1.55E-06,5.34E-05,0.000310118,0.000217843,1.56E-05,0.986455495,2.03E-05,0.009882996,0.00040508
4956,sentiment_analysis13,160,"Instead of updating parameters over a batch , we divide a batch into multiple sub-batches and accumulate gradients on those sub-batches before parameter updates .",Post - training,Post - training,sentiment_analysis,13,24,1,0,,0.02000914,0,negative,0.002758219,0.000180186,0.060385109,2.13E-06,5.84E-06,3.58E-05,0.000293211,0.000398263,0.000951206,0.932730559,2.68E-06,0.002120657,0.000136109
4957,sentiment_analysis13,161,This allows for a smaller subbatch to be consumed in each iteration .,Post - training,Post - training,sentiment_analysis,13,25,1,0,,0.000293107,0,negative,0.002606307,4.55E-06,0.006166606,2.13E-07,5.18E-07,7.22E-06,5.31E-05,6.75E-05,5.65E-05,0.989648758,1.77E-07,0.001372698,1.59E-05
4958,sentiment_analysis13,162,Algorithm 1 describes one training step and takes one batch of data on domain knowledge ( DK ) D DK and one batch of MRC training data D MRC to update the parameters ? of BERT .,Post - training,Post - training,sentiment_analysis,13,26,1,0,,0.000135787,0,negative,0.000205723,1.25E-05,0.006801178,8.77E-07,1.10E-06,1.77E-05,8.54E-05,0.000201798,0.000163753,0.991875998,1.40E-06,0.000555469,7.70E-05
4959,sentiment_analysis13,163,"In line 1 , it first initializes the gradients ? ?",Post - training,Post - training,sentiment_analysis,13,27,1,0,,0.010868235,0,negative,0.000144056,1.53E-06,0.008785072,6.40E-08,1.40E-07,8.23E-06,4.62E-05,7.68E-05,1.98E-05,0.990532747,1.40E-07,0.000374152,1.11E-05
4960,sentiment_analysis13,164,of all parameters as 0 to prepare gradient computation .,Post - training,Post - training,sentiment_analysis,13,28,1,0,,0.000679852,0,negative,0.000241103,4.99E-06,0.000624138,6.36E-07,5.53E-07,0.000174072,0.000351416,0.00600006,3.31E-05,0.99195681,3.54E-07,0.000538156,7.46E-05
4961,sentiment_analysis13,165,"Then in lines 2 and 3 , each batch of training data is split into u sub-batches .",Post - training,Post - training,sentiment_analysis,13,29,1,0,,0.004519906,0,negative,0.000526312,7.00E-06,0.002678447,1.88E-07,7.15E-07,1.98E-05,0.000144968,0.000343233,4.61E-05,0.995321866,1.63E-07,0.000867327,4.38E-05
4962,sentiment_analysis13,166,"Lines 4 - 7 spread the calculation of gradients to u iterations , where the data from each iteration of sub - batches are supposed",Post - training,Post - training,sentiment_analysis,13,30,1,0,,0.000338497,0,negative,0.000883474,2.32E-06,0.002753042,1.22E-07,4.45E-07,6.41E-06,4.38E-05,6.36E-05,2.15E-05,0.995433877,6.07E-08,0.000778738,1.27E-05
4963,sentiment_analysis13,167,to be able to fit into GPU memory .,Post - training,Post - training,sentiment_analysis,13,31,1,0,,0.00027911,0,negative,0.000197072,8.36E-07,0.00071562,4.61E-07,5.74E-07,9.39E-06,2.86E-05,5.25E-05,1.24E-05,0.99821625,1.93E-07,0.000732111,3.39E-05
4964,sentiment_analysis13,168,"In line 5 , it computes the partial joint loss L partial of two subbatches D DK , i and D MRC , i from the i - th iteration through forward pass .",Post - training,Post - training,sentiment_analysis,13,32,1,0,,0.000467525,0,negative,0.001236886,3.89E-06,0.03498823,1.17E-07,6.05E-07,6.31E-06,0.000101717,4.27E-05,3.83E-05,0.961354584,1.01E-07,0.002204315,2.23E-05
4965,sentiment_analysis13,169,"Note that the summation of two sub - batches ' losses is divided by u , which compensate the scale change introduced by gradient accumulation inline",Post - training,Post - training,sentiment_analysis,13,33,1,0,,0.00011973,0,negative,0.001090607,1.76E-06,0.00170038,1.32E-07,3.56E-07,6.50E-06,4.59E-05,6.56E-05,1.82E-05,0.996012059,5.74E-08,0.0010455,1.29E-05
4966,sentiment_analysis13,170,6 .,Post - training,Post - training,sentiment_analysis,13,34,1,0,,8.40E-05,0,negative,0.000157638,2.42E-07,9.34E-05,1.60E-07,1.40E-07,7.97E-06,3.23E-05,6.16E-05,2.88E-06,0.999205882,3.60E-08,0.000423265,1.45E-05
4967,sentiment_analysis13,171,Line 6 accumulates the gradients produced by backpropagation from the partial joint loss .,Post - training,Post - training,sentiment_analysis,13,35,1,0,,0.002083149,0,negative,0.001659678,5.03E-06,0.027854321,1.44E-07,6.65E-07,9.16E-06,0.000116255,7.90E-05,0.000116866,0.968769328,9.55E-08,0.001351753,3.77E-05
4968,sentiment_analysis13,172,"To this end , accumulating the gradients u times is equivalent to computing the gradients on the whole batch once .",Post - training,Post - training,sentiment_analysis,13,36,1,0,,0.000300674,0,negative,0.000133264,5.22E-06,0.000610072,2.75E-07,5.94E-07,1.64E-05,9.69E-05,0.000216588,3.67E-05,0.99803409,3.89E-07,0.000781498,6.79E-05
4969,sentiment_analysis13,173,But the subbatches and their intermediate hidden representations during the i - th forward pass can be discarded to save memory space .,Post - training,Post - training,sentiment_analysis,13,37,1,0,,7.98E-05,0,negative,0.000938137,6.18E-07,0.000371759,3.85E-08,1.67E-07,2.05E-06,1.92E-05,2.19E-05,4.50E-06,0.99720813,3.55E-08,0.001427547,6.00E-06
4970,sentiment_analysis13,174,Only the gradients ? ?,Post - training,Post - training,sentiment_analysis,13,38,1,0,,0.0114514,0,negative,7.39E-05,4.24E-07,0.000183693,2.69E-07,1.73E-07,1.07E-05,5.08E-05,7.84E-05,6.33E-06,0.9991467,1.75E-07,0.000412584,3.59E-05
4971,sentiment_analysis13,175,are kept throughout all iterations and used to update parameters ( based on the chosen optimizer ) inline 8 .,Post - training,Post - training,sentiment_analysis,13,39,1,0,,0.000197117,0,negative,0.000228851,4.17E-06,0.000810107,1.95E-07,5.01E-07,1.69E-05,0.000103609,0.000499257,3.08E-05,0.997797098,4.55E-08,0.000481228,2.73E-05
4972,sentiment_analysis13,176,We detail the hyper - parameter settings of this algorithm in Sec. 5.3 .,Post - training,Post - training,sentiment_analysis,13,40,1,0,,1.56E-05,0,negative,9.96E-05,1.48E-06,9.13E-05,2.52E-06,1.70E-06,2.64E-05,3.94E-05,0.000288995,4.70E-06,0.999093188,1.39E-08,0.000324715,2.60E-05
4973,sentiment_analysis13,177,Experiments,,,sentiment_analysis,13,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
4974,sentiment_analysis13,178,"We aim to answer the following research questions ( RQs ) in the experiment : RQ1 : what is the performance gain of posttraining for each review - based task , with respect to the state - of - the - art performance ?",Experiments,Experiments,sentiment_analysis,13,1,1,0,,0.010313597,0,negative,8.16E-05,0.000243263,0.000103645,4.55E-05,1.01E-05,0.005273437,0.000782661,0.055193444,0.000123843,0.934519311,0.003117212,0.00027471,0.000231222
4975,sentiment_analysis13,179,RQ2 : what is the performance of BERT 's pretrained weights on three review - based tasks without any domain and task adaptation ?,Experiments,Experiments,sentiment_analysis,13,2,1,0,,0.008477426,0,negative,0.000173228,0.000218125,0.000156258,4.96E-05,9.40E-06,0.006795648,0.001890312,0.075939825,8.30E-05,0.907304995,0.005988688,0.00093177,0.000459187
4976,sentiment_analysis13,180,"RQ3 : upon ablation studies of separate domain knowledge post - training and task - awareness posttraining , what is their respective contribution to the whole post - training performance gain ?",Experiments,Experiments,sentiment_analysis,13,3,1,0,,0.017492093,0,negative,0.000245987,0.000163222,0.000103575,8.69E-05,9.06E-06,0.012084339,0.00113535,0.104534305,0.000194003,0.878512886,0.001948096,0.000558555,0.000423672
4977,sentiment_analysis13,181,End Task Datasets,Experiments,Experiments,sentiment_analysis,13,4,1,0,,0.00302672,0,negative,6.68E-05,1.09E-05,0.000101371,3.27E-06,4.29E-06,0.002626541,0.001412826,0.00931685,5.39E-06,0.984186703,2.79E-05,0.00217321,6.39E-05
4978,sentiment_analysis13,182,"As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis , we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC .",Experiments,Experiments,sentiment_analysis,13,5,1,0,,0.006786002,0,negative,0.000461139,0.00199869,0.001402119,0.000772901,0.002925008,0.014473325,0.009057477,0.067752127,0.000155339,0.899029781,0.000191163,0.00105706,0.000723871
4979,sentiment_analysis13,183,We do not use SemEval 2014 Task 4 or SemEval 2015 Task 12 because these datasets do not come with the review ( document ) level XML tags to recover whole reviews from review sentences .,Experiments,Experiments,sentiment_analysis,13,6,1,0,,0.005013684,0,negative,0.000161589,0.000385164,0.000995975,5.56E-05,0.000341528,0.005234011,0.005311384,0.020832768,3.68E-05,0.964182772,0.000154719,0.002022922,0.000284859
4980,sentiment_analysis13,184,We keep the split of training and testing of the SemEval 2016 Task 5 datasets and annotate multiple QAs for each review following the way of constructing QAs for the SQuAD 1.1 datasets .,Experiments,Experiments,sentiment_analysis,13,7,1,0,,0.00434948,0,negative,0.000692956,0.000758005,0.001907173,0.000706145,0.00211044,0.011158448,0.009007978,0.022795912,9.35E-05,0.948696043,6.49E-05,0.001299669,0.000708786
4981,sentiment_analysis13,185,"To make sure our questions are close to realworld questions , 2 annotators are first exposed to 400 QAs from CQA ( under the laptop category in Amazon.com or popular restaurants in Yelp.com ) to get familiar with real questions .",Experiments,Experiments,sentiment_analysis,13,8,1,0,,0.006009859,0,negative,0.000472344,0.000545004,0.001239308,0.000158083,0.002021172,0.002390824,0.006519973,0.009223144,3.36E-05,0.974177721,0.00014193,0.002672164,0.00040473
4982,sentiment_analysis13,186,Then they are asked to read reviews and independently label textual spans and ask corresponding questions when they feel the textual spans contain valuable information that customers may care about .,Experiments,Experiments,sentiment_analysis,13,9,1,0,,0.00204751,0,negative,0.000451804,0.000819771,0.002923182,2.21E-05,0.000123496,0.00079635,0.000883758,0.007701588,0.000203728,0.982851055,0.000205873,0.002815144,0.000202164
4983,sentiment_analysis13,187,The textual spans are labeled to be as concise as possible but still human - readable .,Experiments,Experiments,sentiment_analysis,13,10,1,0,,0.059191129,0,negative,0.000410621,0.000744315,0.000557163,2.45E-05,8.78E-05,0.018044334,0.001068587,0.289880346,0.00045924,0.688048989,7.28E-06,0.000407425,0.000259432
4984,sentiment_analysis13,188,Note that the annotations for sentiment analysis tasks are not exposed to annotators to avoid biased annotation on RRC .,Experiments,Experiments,sentiment_analysis,13,11,1,0,,0.003604666,0,negative,0.000177494,3.00E-05,4.54E-05,3.71E-05,4.75E-05,0.001250349,0.000193704,0.0074253,1.20E-05,0.990369804,1.13E-05,0.000355661,4.44E-05
4985,sentiment_analysis13,189,"Since it is unlikely that the two annotators can label the same QAs ( the same questions with the same answer spans ) , they further mutually check each other 's annotations and dis agreements are discussed until agreements are reached .",Experiments,Experiments,sentiment_analysis,13,12,1,0,,0.007233277,0,negative,0.001320705,0.000440596,0.004891023,1.97E-05,0.000111665,0.000947541,0.00065619,0.006535531,0.000294171,0.981571504,6.12E-05,0.002904079,0.000246137
4986,sentiment_analysis13,190,Annotators are encouraged to label as many questions as possible from testing reviews to get more test examples .,Experiments,Experiments,sentiment_analysis,13,13,1,0,,0.045645896,0,negative,0.000450468,0.000354467,0.000792688,1.97E-05,7.67E-05,0.006762232,0.001454976,0.084342159,0.00017881,0.903827602,1.82E-05,0.001323467,0.000398482
4987,sentiment_analysis13,191,A training review is encouraged to have 2 questions ( training examples ) on average to have good coverage of reviews .,Experiments,Experiments,sentiment_analysis,13,14,1,0,,0.006908554,0,negative,0.000453501,0.000503157,0.001257216,7.61E-06,2.85E-05,0.011247997,0.001366202,0.280974071,0.000327881,0.70266149,1.51E-05,0.0008212,0.000336053
4988,sentiment_analysis13,192,The annotated data is in the format of SQuAD 1.1 to ensure compatibility with existing implementations of MRC models .,Experiments,Experiments,sentiment_analysis,13,15,1,0,,0.033953243,0,negative,0.000250544,0.000114158,0.000363352,0.000606288,0.000737908,0.019610108,0.00324078,0.04151982,6.76E-05,0.932423333,9.84E-06,0.000565054,0.00049122
4989,sentiment_analysis13,193,The statistics of the RRC dataset ( ReviewRC ) are shown in .,Experiments,Experiments,sentiment_analysis,13,16,1,0,,0.118665126,0,results,0.003867939,2.67E-05,0.001191715,3.01E-06,2.18E-05,0.000248363,0.011938441,0.0014666,2.91E-06,0.430138396,3.04E-05,0.550818705,0.000244895
4990,sentiment_analysis13,194,"Since SemEval datasets do not come with a validation set , we further split 20 % of reviews from the training set for validation .",Experiments,Experiments,sentiment_analysis,13,17,1,0,,0.018140936,0,negative,0.000780879,0.000379259,0.000392719,1.12E-05,8.63E-05,0.018980366,0.003465419,0.310472497,8.69E-05,0.66418451,3.08E-06,0.000840442,0.000316382
4991,sentiment_analysis13,195,Statistics of datasets for AE and ASC are given in .,Experiments,Experiments,sentiment_analysis,13,18,1,0,,0.010773415,0,negative,0.00021436,7.86E-06,7.25E-05,2.81E-06,9.09E-06,0.000857516,0.001696833,0.004476911,3.27E-06,0.982384377,1.42E-05,0.010179103,8.11E-05
4992,sentiment_analysis13,196,"For AE , we choose SemEval 2014 Task 4 for laptop and SemEval - 2016 Task 5 for restaurant to be consistent with and other previous works .",Experiments,Experiments,sentiment_analysis,13,19,1,0,,0.034505005,0,negative,0.000299454,0.000529197,0.000524991,3.25E-05,0.000283708,0.00892871,0.008513184,0.110922071,4.99E-05,0.865466262,1.08E-05,0.003802366,0.000636824
4993,sentiment_analysis13,197,"For ASC , we use SemEval 2014 Task 4 for both laptop and restaurant as existing research frequently uses this version .",Experiments,Experiments,sentiment_analysis,13,20,1,0,,0.017829132,0,negative,0.000320844,0.000467001,0.000915223,0.000114227,0.000806294,0.01236155,0.01381524,0.063146123,4.73E-05,0.90257744,1.88E-05,0.004243501,0.001166436
4994,sentiment_analysis13,198,We use 150 examples from the training set of all these datasets for validation .,Experiments,Experiments,sentiment_analysis,13,21,1,0,,0.023675833,0,hyperparameters,0.000157299,0.000177322,7.52E-05,1.80E-05,7.03E-05,0.028320929,0.004908894,0.525144268,3.00E-05,0.439815116,4.15E-06,0.000688308,0.000590161
4995,sentiment_analysis13,199,Post - training datasets,,,sentiment_analysis,13,0,1,0,,0.00520976,0,negative,9.60E-05,7.16E-05,0.000232894,1.26E-07,1.49E-06,7.11E-05,0.000200514,0.00062115,1.13E-05,0.994321728,0.000108897,0.004261916,1.33E-06
4996,sentiment_analysis13,200,"For domain knowledge post - training , we use Amazon laptop reviews and Yelp Dataset Challenge reviews 8 .",Post - training datasets,Post - training datasets,sentiment_analysis,13,1,1,0,,0.832507515,1,negative,9.35E-05,2.02E-06,0.019435209,7.46E-06,2.40E-06,0.00064096,0.002554167,0.00227063,2.03E-06,0.970223354,2.02E-06,0.002339164,0.002427039
4997,sentiment_analysis13,201,"For laptop , we filtered out reviewed products that have appeared in the validation / test reviews to avoid training bias for test data ( Yelp reviews do not have this issue as the source reviews of SemEval are not from Yelp ) .",Post - training datasets,Post - training datasets,sentiment_analysis,13,2,1,0,,0.002903329,0,negative,0.00027355,5.29E-07,0.012592926,5.64E-06,1.68E-06,0.000167568,0.000259192,0.000375416,1.47E-06,0.984217806,5.41E-07,0.001641736,0.000461943
4998,sentiment_analysis13,202,"Since the number of reviews is small , we choose a duplicate factor of 5 ( each review generates about 5 training examples ) during BERT data pre-processing .",Post - training datasets,Post - training datasets,sentiment_analysis,13,3,1,0,,0.552873014,1,negative,0.00067112,1.87E-05,0.00815488,1.34E-05,1.29E-06,0.008799099,0.016202284,0.20174601,2.18E-05,0.748647559,1.45E-05,0.00577738,0.009931871
4999,sentiment_analysis13,203,"This gives us 1,151,863 posttraining examples for laptop domain knowledge .",Post - training datasets,Post - training datasets,sentiment_analysis,13,4,1,0,,0.045380604,0,negative,0.000865159,1.26E-06,0.006809253,0.000191971,2.46E-05,0.000309942,0.001078649,0.000275372,2.76E-06,0.980264242,1.78E-06,0.003102562,0.007072474
5000,sentiment_analysis13,204,"For the restaurant domain , we use Yelp reviews from restaurant categories that the SemEval reviews also belong to .",Post - training datasets,Post - training datasets,sentiment_analysis,13,5,1,0,,0.120318736,0,negative,0.000214972,5.01E-06,0.049072362,1.78E-05,1.35E-05,0.000253894,0.003783222,0.000737539,2.93E-06,0.930992954,4.81E-06,0.009472102,0.005428925
5001,sentiment_analysis13,205,We choose 700K reviews to ensure it is large enough to generate training examples ( with a duplicate factor of 1 ) to cover all post - training steps that we can afford ( discussed in Section 5.3 ),Post - training datasets,Post - training datasets,sentiment_analysis,13,6,1,0,,0.080778513,0,negative,0.000204813,4.22E-06,0.008487578,4.80E-06,1.03E-06,0.002087875,0.005929454,0.026136029,6.10E-06,0.950445809,2.91E-06,0.002468082,0.004221291
5002,sentiment_analysis13,206,9 .,Post - training datasets,Post - training datasets,sentiment_analysis,13,7,1,0,,0.00041794,0,negative,0.000107344,1.84E-07,0.003248579,8.20E-07,5.52E-08,6.01E-05,5.60E-05,0.000364549,2.00E-06,0.995328766,3.62E-07,0.000643477,0.000187746
5003,sentiment_analysis13,207,"This gives us 2,677,025 post-training examples for restaurant domain knowledge learning .",Post - training datasets,Post - training datasets,sentiment_analysis,13,8,1,0,,0.06979584,0,negative,0.000625407,1.61E-06,0.006809177,0.00022708,3.33E-05,0.000349449,0.001166463,0.000325071,3.24E-06,0.976624478,1.59E-06,0.002688738,0.011144397
5004,sentiment_analysis13,208,"For MRC task - awareness post - training , we leverage SQuAD 1.1 ) that come with 87,599 training examples from 442 Wikipedia articles .",Post - training datasets,Post - training datasets,sentiment_analysis,13,9,1,0,,0.046179252,0,negative,0.001193126,1.93E-05,0.195429903,0.000379472,6.72E-05,0.00140681,0.013120745,0.001956231,1.70E-05,0.698450513,1.26E-05,0.019949036,0.0679981
5005,sentiment_analysis13,209,Hyper-parameters,Post - training datasets,,sentiment_analysis,13,10,1,0,,0.002505898,0,negative,8.53E-05,3.80E-06,0.012970746,1.68E-06,1.33E-07,0.0019759,0.001539113,0.059141434,4.08E-05,0.922263647,5.15E-06,0.000712621,0.001259683
5006,sentiment_analysis13,210,We adopt BERT BASE ( uncased ) as the basis for all experiments,Post - training datasets,Hyper-parameters,sentiment_analysis,13,11,1,1,hyperparameters,0.000311584,0,negative,1.26E-05,8.33E-05,0.012158264,7.20E-07,1.77E-06,0.001273021,0.000177731,0.085042826,1.84E-05,0.901013814,7.99E-06,0.000199601,9.97E-06
5007,sentiment_analysis13,211,"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .",Post - training datasets,Hyper-parameters,sentiment_analysis,13,12,1,1,hyperparameters,0.622493677,1,hyperparameters,5.16E-05,0.000290777,0.002122313,1.04E-05,2.81E-06,0.011866663,0.000192608,0.815564389,8.51E-05,0.169703868,5.64E-06,5.92E-05,4.45E-05
5008,sentiment_analysis13,212,"We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation .",Post - training datasets,Hyper-parameters,sentiment_analysis,13,13,1,1,hyperparameters,0.839087502,1,hyperparameters,6.04E-06,2.36E-05,3.29E-05,1.74E-06,2.67E-07,0.004747015,6.99E-05,0.98392491,5.93E-06,0.011154833,1.18E-06,2.05E-05,1.12E-05
5009,sentiment_analysis13,213,The maximum length of post -training is set to 320 with a batch size of 16 for each type of knowledge .,Post - training datasets,Hyper-parameters,sentiment_analysis,13,14,1,1,hyperparameters,0.775752267,1,hyperparameters,1.31E-06,9.77E-06,9.55E-06,4.57E-07,6.17E-08,0.00339506,3.91E-05,0.991523471,3.49E-06,0.005008471,6.33E-07,3.30E-06,5.32E-06
5010,sentiment_analysis13,214,"The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G .",Post - training datasets,Hyper-parameters,sentiment_analysis,13,15,1,1,hyperparameters,0.904438917,1,hyperparameters,1.21E-06,8.52E-06,8.15E-06,4.18E-07,7.03E-08,0.003473751,4.75E-05,0.988676421,2.59E-06,0.007767973,9.59E-07,6.89E-06,5.59E-06
5011,sentiment_analysis13,215,We use Adam optimizer and set the learning rate to be 3e - 5 .,Post - training datasets,Hyper-parameters,sentiment_analysis,13,16,1,1,hyperparameters,0.981567137,1,hyperparameters,1.05E-06,7.16E-06,4.80E-06,9.48E-07,6.92E-08,0.003414599,3.90E-05,0.994010703,2.48E-06,0.002509782,4.37E-07,2.67E-06,6.27E-06
5012,sentiment_analysis13,216,"We train 70,000 steps for the laptop domain and 140,000 steps for the restaurant domain , which roughly have one pass over the preprocessed data on the respective domain .",Post - training datasets,Hyper-parameters,sentiment_analysis,13,17,1,1,hyperparameters,0.074690393,0,hyperparameters,5.20E-05,9.63E-05,0.000382678,1.62E-05,1.38E-05,0.011250045,0.000408522,0.77127266,2.18E-05,0.216248841,2.56E-06,0.000146511,8.81E-05
5013,sentiment_analysis13,217,Compared Methods,,,sentiment_analysis,13,0,1,0,,0.003363972,0,negative,1.91E-05,0.000148937,7.51E-05,4.41E-07,6.71E-07,0.00022496,0.000164342,0.00299226,8.80E-05,0.993920181,0.001856336,0.00050613,3.53E-06
5014,sentiment_analysis13,218,"As BERT outperforms existing open source MRC baselines by a large margin , we do not intend to exhaust existing implementations but focus on variants of BERT introduced in this paper .",Compared Methods,Compared Methods,sentiment_analysis,13,1,1,0,,0.678929729,1,baselines,9.09E-05,3.47E-05,0.761730378,7.53E-07,9.21E-07,5.60E-05,0.000190151,0.000431222,6.17E-06,0.235345583,4.55E-05,0.00206185,5.92E-06
5015,sentiment_analysis13,219,DrQA is a baseline from the document reader 12 of DrQA .,Compared Methods,Compared Methods,sentiment_analysis,13,2,1,0,,0.596605612,1,baselines,1.20E-05,1.40E-06,0.989242557,2.96E-07,2.46E-07,1.33E-05,0.000832418,5.89E-05,2.38E-07,0.007639735,3.94E-05,0.002141757,1.77E-05
5016,sentiment_analysis13,220,We adopt this baseline because of its simple implementation for reproducibility .,Compared Methods,Compared Methods,sentiment_analysis,13,3,1,0,,0.042495303,0,baselines,1.51E-05,1.67E-05,0.49700449,9.07E-07,5.83E-07,0.000442469,0.000523057,0.006039898,1.55E-05,0.495396275,2.81E-05,0.000503406,1.35E-05
5017,sentiment_analysis13,221,We run the document reader with random initialization and train it directly on Review RC .,Compared Methods,Compared Methods,sentiment_analysis,13,4,1,0,,0.925757176,1,negative,8.23E-05,0.000218964,0.167303337,5.48E-06,1.93E-06,0.007812723,0.002421634,0.372541975,0.000146423,0.449150642,2.36E-05,0.000231528,5.94E-05
5018,sentiment_analysis13,222,"We use all default hyper - parameter settings for this baseline except the number of epochs , which is set as 60 for better convergence .",Compared Methods,Compared Methods,sentiment_analysis,13,5,1,0,,0.99268157,1,hyperparameters,4.84E-05,8.38E-05,0.007210439,1.01E-05,1.03E-06,0.01600923,0.002361983,0.875378426,3.90E-05,0.098586963,1.84E-05,0.000188417,6.39E-05
5019,sentiment_analysis13,223,DrQA + MRC is derived from the above baseline with official pre-trained weights on SQ u AD .,Compared Methods,Compared Methods,sentiment_analysis,13,6,1,0,,0.623945261,1,baselines,1.85E-06,1.44E-06,0.995941111,1.70E-08,2.97E-08,3.05E-06,6.30E-05,3.56E-05,2.66E-07,0.003828801,2.13E-06,0.000121509,1.17E-06
5020,sentiment_analysis13,224,We fine - tune document reader with Review RC .,Compared Methods,Compared Methods,sentiment_analysis,13,7,1,0,,0.298186725,0,baselines,0.000948365,6.53E-05,0.873814969,7.27E-06,4.04E-06,0.000176393,0.000615523,0.004403882,5.61E-05,0.115686983,1.20E-05,0.004129001,8.01E-05
5021,sentiment_analysis13,225,We expand the vocabulary of the embedding layer from the pre-trained model on ReviewRC since reviews may have words that are rare in Wikipedia and keep other hyper - parameters as their defaults .,Compared Methods,Compared Methods,sentiment_analysis,13,8,1,0,,0.973313891,1,negative,0.000683856,0.000182081,0.201605053,6.10E-06,3.17E-06,0.002685324,0.000998032,0.139829779,0.000148642,0.652922925,7.54E-06,0.000880698,4.68E-05
5022,sentiment_analysis13,226,"For AE and ASC , we summarize the scores of the state - of - the - arts on SemEval ( based the best of our knowledge ) for brevity .",Compared Methods,Compared Methods,sentiment_analysis,13,9,1,0,,0.068859897,0,negative,6.60E-05,1.05E-05,0.177340971,1.05E-06,2.19E-06,7.01E-05,0.000555462,0.000983066,2.11E-06,0.814165843,3.54E-05,0.00675127,1.60E-05
5023,sentiment_analysis13,227,DE - CNN reaches the state - of the - arts for AE by leveraging domain embeddings .,Compared Methods,Compared Methods,sentiment_analysis,13,10,1,0,,0.966461448,1,baselines,0.000320981,1.19E-05,0.754137156,1.39E-06,8.63E-07,4.79E-05,0.004586627,0.000601914,1.35E-06,0.046257237,0.000905997,0.192869275,0.000257421
5024,sentiment_analysis13,228,MGAN reaches the state - of - theart ASC on SemEval 2014 task 4 .,Compared Methods,Compared Methods,sentiment_analysis,13,11,1,0,,0.897091591,1,results,0.000347055,6.20E-06,0.240474568,2.08E-06,1.15E-06,9.72E-05,0.013301365,0.000970053,1.08E-06,0.052994036,0.000404317,0.690884559,0.000516373
5025,sentiment_analysis13,229,"Lastly , to answer RQ1 , RQ2 , and RQ3 , we have the following BERT variants .",Compared Methods,Compared Methods,sentiment_analysis,13,12,1,0,,0.013478003,0,negative,0.000216292,4.11E-06,0.356058335,2.18E-07,4.82E-07,1.54E-05,5.81E-05,0.00014992,2.68E-06,0.641309456,2.08E-06,0.002180612,2.24E-06
5026,sentiment_analysis13,230,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,Compared Methods,,sentiment_analysis,13,13,1,0,,0.98559463,1,baselines,1.39E-05,1.30E-06,0.997232046,2.04E-07,1.14E-07,5.13E-06,3.86E-05,2.79E-05,1.87E-06,0.002576301,7.05E-07,9.54E-05,6.44E-06
5027,sentiment_analysis13,231,weights and fine - tunes on all 3 end tasks .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,14,1,0,,0.002742934,0,negative,0.000107515,7.63E-06,0.01974761,8.11E-07,3.76E-07,0.002423048,0.000223411,0.015519418,6.60E-05,0.961340668,2.00E-06,0.000516605,4.49E-05
5028,sentiment_analysis13,232,We use this baseline to answer RQ2 and show that BERT 's pre-trained weights alone have limited performance gains on review - based tasks .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,15,1,0,,0.003682491,0,negative,0.000650988,2.65E-05,0.081043826,2.11E-06,5.52E-06,0.000194681,0.000539519,0.000475452,8.51E-06,0.864331243,9.92E-06,0.052622533,8.92E-05
5029,sentiment_analysis13,233,BERT - DK post - trains BERT 's weights only on domain knowledge ( reviews ) and fine - tunes on the 3 end tasks .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,16,1,0,,0.405323009,0,baselines,6.10E-05,8.45E-06,0.963018481,6.72E-07,8.14E-07,9.42E-05,0.000160477,0.000147581,1.07E-05,0.035878619,1.91E-06,0.000556304,6.08E-05
5030,sentiment_analysis13,234,We use BERT - DK and the following BERT - MRC to answer RQ3 .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,17,1,0,,0.004060816,0,negative,4.90E-05,4.68E-06,0.312038075,4.90E-07,1.14E-06,0.000153176,0.000312608,0.000229317,3.83E-06,0.681616542,9.38E-06,0.005491937,8.98E-05
5031,sentiment_analysis13,235,BERT - MRC post - trains BERT 's weights on SQuAD 1.1 and then fine - tunes on the 3 end tasks .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,18,1,0,,0.524900052,1,baselines,0.000136435,1.52E-05,0.917076867,5.95E-06,4.78E-06,0.00054763,0.001277361,0.00046983,1.19E-05,0.076430396,4.79E-06,0.003471238,0.000547657
5032,sentiment_analysis13,236,BERT - PT ( proposed method ) post -trains BERT 's weights using the joint post - training algorithm in Section 4 and then fine - tunes on the 3 end tasks .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,19,1,0,,0.88487651,1,baselines,2.32E-05,4.05E-06,0.971934179,1.33E-07,2.33E-07,2.53E-05,6.89E-05,5.50E-05,7.59E-06,0.02743813,1.54E-06,0.000416942,2.48E-05
5033,sentiment_analysis13,237,Evaluation Metrics and Model Selection,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,20,1,0,,0.009628361,0,negative,7.60E-05,2.18E-06,0.01237887,1.09E-06,9.12E-07,0.000211038,0.000476033,0.00042974,2.57E-06,0.963266075,3.30E-05,0.022874202,0.000248282
5034,sentiment_analysis13,238,"To be consistent with existing research on MRC , we use the same evaluation script from SQuAD 1.1 for RRC , which reports Exact Match ( EM ) and F1 scores .",Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,21,1,0,,0.000854591,0,negative,5.17E-05,3.94E-06,0.029264155,3.83E-07,2.42E-06,0.000108194,0.000290481,0.000207898,1.31E-06,0.963295523,8.97E-07,0.006745884,2.73E-05
5035,sentiment_analysis13,239,EM requires the answers to have exact string match with human annotated answer spans .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,22,1,0,,0.006778113,0,negative,0.000103933,1.24E-05,0.055402094,1.79E-05,4.98E-06,0.000801788,0.001546263,0.00099227,8.07E-06,0.910711653,0.001958766,0.020504593,0.007935322
5036,sentiment_analysis13,240,"F1 score is the averaged F 1 scores of individual answers , which is typically higher than EM and is the major metric .",Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,23,1,0,,0.000313228,0,negative,1.42E-05,4.60E-07,0.003637925,2.42E-07,5.14E-07,0.000135356,8.77E-05,0.000245437,6.98E-07,0.993815503,1.02E-06,0.002033131,2.78E-05
5037,sentiment_analysis13,241,Each individual F 1 score is the harmonic mean of individual precision and recall computed based on the number of overlapped words between the predicted answer and human annotated answers .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,24,1,0,,0.00024924,0,negative,4.13E-05,1.30E-06,0.005172065,1.53E-07,5.34E-07,0.000141263,4.45E-05,0.00041274,2.74E-06,0.992999558,1.71E-07,0.001172414,1.13E-05
5038,sentiment_analysis13,242,"For AE , we use the standard evaluation scripts come with the SemEval datasets and report the F1 score .",Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,25,1,0,,0.000372596,0,negative,1.53E-05,3.69E-07,0.001097502,1.60E-07,3.80E-07,7.71E-05,5.01E-05,0.000195746,3.30E-07,0.996790357,1.32E-07,0.001765213,7.40E-06
5039,sentiment_analysis13,243,"For ASC , we compute both accuracy and Macro - F1 over 3 classes of polarities , where Macro - F1 is the major metric as the imbalanced classes introduce biases on accuracy .",Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,26,1,0,,0.002645718,0,negative,0.000213828,1.90E-05,0.011383014,1.12E-06,6.44E-06,0.00015342,0.000312409,0.000597096,4.13E-06,0.967590774,9.63E-07,0.019656369,6.15E-05
5040,sentiment_analysis13,244,"To be consistent with existing research , examples belonging to the conflict polarity are dropped due to a very small number of examples .",Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,27,1,0,,0.001405457,0,negative,8.52E-05,7.62E-07,0.002873975,2.20E-07,1.85E-06,9.76E-05,2.79E-05,0.000199215,1.02E-06,0.995436444,4.69E-08,0.001267869,7.88E-06
5041,sentiment_analysis13,245,"We set the maximum number of epochs to 4 for BERT variants , though most runs converge just within 2 epochs .",Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,28,1,0,,0.648171376,1,negative,0.000327241,2.17E-05,0.004581956,1.75E-05,6.75E-06,0.044179326,0.004853114,0.19330201,1.82E-05,0.747657302,1.81E-06,0.003850189,0.001182843
5042,sentiment_analysis13,246,Results are reported as averages of 9 runs ( 9 different random seeds for random batch generation ) .,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,29,1,0,,0.003371371,0,negative,9.60E-06,5.46E-07,0.000268856,3.89E-07,2.96E-07,0.000410912,0.000148382,0.001577787,9.44E-07,0.995969399,4.23E-07,0.001579526,3.29E-05
5043,sentiment_analysis13,247,13,Compared Methods,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,sentiment_analysis,13,30,1,0,,4.18E-05,0,negative,9.64E-06,2.77E-07,0.000150258,4.61E-07,1.86E-07,9.88E-05,8.50E-06,0.000256312,2.30E-06,0.999332293,1.18E-07,0.000132396,8.50E-06
5044,sentiment_analysis13,248,Result Analysis,,,sentiment_analysis,13,0,1,0,,0.000601765,0,negative,1.37E-05,3.72E-05,1.33E-06,2.64E-07,1.89E-07,6.01E-05,7.71E-05,0.001320129,1.84E-05,0.993161332,0.00486798,0.000440059,2.18E-06
5045,sentiment_analysis13,249,"The results of RRC , AE and ASC are shown in Tables 4 , 5 and 6 , respectively .",Result Analysis,Result Analysis,sentiment_analysis,13,1,1,0,,0.213662122,0,results,0.00096243,7.91E-07,7.53E-05,3.06E-08,5.28E-08,1.10E-05,0.000398907,7.55E-05,8.47E-08,0.256121706,2.87E-05,0.742323726,1.73E-06
5046,sentiment_analysis13,250,"To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge .",Result Analysis,Result Analysis,sentiment_analysis,13,2,1,1,results,0.827842144,1,results,0.00566533,8.97E-07,3.46E-05,1.94E-07,2.24E-07,1.00E-05,0.000333488,5.44E-05,9.80E-08,0.081019661,1.01E-05,0.912867451,3.50E-06
5047,sentiment_analysis13,251,Domain Laptop,Result Analysis,,sentiment_analysis,13,3,1,0,,0.046045295,0,negative,0.005149341,6.12E-05,0.001191283,0.001275512,7.44E-05,0.016083471,0.023092468,0.010953296,5.59E-05,0.853321803,0.00083803,0.087229265,0.000674111
5048,sentiment_analysis13,252,"Rest. Methods EM F1 EM F1 DrQA 38.26 50.99 49.52 63.73 DrQA+MRC 40 To answer RQ2 , to our surprise we found that the vanilla pre-trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks .",Result Analysis,Domain Laptop,sentiment_analysis,13,4,1,1,results,0.068275076,0,results,0.000237317,1.13E-08,2.46E-05,7.39E-09,1.89E-09,5.15E-07,4.48E-05,1.33E-06,4.38E-09,0.093375576,1.28E-06,0.906311059,3.57E-06
5049,sentiment_analysis13,253,This justifies the need to adapt BERT to review - based tasks .,Result Analysis,Domain Laptop,sentiment_analysis,13,5,1,0,,0.003222605,0,negative,0.00015253,1.24E-07,2.22E-05,1.06E-07,8.69E-09,1.57E-06,1.43E-06,7.66E-06,5.74E-07,0.993863064,3.04E-06,0.005942982,4.71E-06
5050,sentiment_analysis13,254,"To answer RQ3 , we noticed that the roles of domain knowledge and task knowledge vary for different tasks and domains .",Result Analysis,Domain Laptop,sentiment_analysis,13,6,1,1,results,0.021831591,0,negative,0.000413707,7.17E-08,1.19E-05,1.28E-07,1.98E-08,2.34E-06,3.84E-06,5.27E-06,2.23E-07,0.978111845,8.44E-07,0.021444627,5.18E-06
5051,sentiment_analysis13,255,"For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post -training ( as indicated by BERT - MRC ) .",Result Analysis,Domain Laptop,sentiment_analysis,13,7,1,1,results,0.945242026,1,results,0.003289557,1.03E-07,4.81E-05,4.98E-08,1.42E-08,9.04E-07,4.19E-05,3.27E-06,5.82E-08,0.127546952,2.11E-06,0.869055901,1.10E-05
5052,sentiment_analysis13,256,The domain knowledge helps more for restaurant than for laptop .,Result Analysis,Domain Laptop,sentiment_analysis,13,8,1,0,,0.921329836,1,results,0.046911418,7.09E-08,1.94E-05,2.92E-06,6.80E-08,1.14E-05,0.000193708,1.82E-05,3.10E-07,0.104431034,6.12E-07,0.848202953,0.000207881
5053,sentiment_analysis13,257,"We suspect the reason is that certain types of knowledge ( such as specifications ) of laptop are already present in Wikipedia , whereas Wikipedia has little knowledge about restaurant .",Result Analysis,Domain Laptop,sentiment_analysis,13,9,1,0,,0.000176999,0,negative,6.25E-05,2.47E-08,5.55E-06,7.91E-08,1.16E-08,2.57E-06,1.65E-06,5.88E-06,1.46E-07,0.997455786,2.19E-07,0.002461829,3.74E-06
5054,sentiment_analysis13,258,We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .,Result Analysis,Domain Laptop,sentiment_analysis,13,10,1,1,results,0.806418747,1,results,0.00222336,3.06E-08,1.23E-05,1.92E-08,8.91E-09,7.64E-07,2.41E-05,2.39E-06,2.87E-08,0.248768919,3.60E-07,0.748963765,3.90E-06
5055,sentiment_analysis13,259,"For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .",Result Analysis,Domain Laptop,sentiment_analysis,13,11,1,1,results,0.95951779,1,results,0.009240531,5.44E-08,1.34E-05,7.10E-08,1.69E-08,8.87E-07,4.23E-05,2.86E-06,4.72E-08,0.101908229,7.43E-07,0.888773788,1.70E-05
5056,sentiment_analysis13,260,"BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .",Result Analysis,Domain Laptop,sentiment_analysis,13,12,1,1,results,0.818765823,1,results,0.000244236,1.84E-09,3.68E-06,7.63E-09,1.03E-09,1.86E-07,4.98E-05,4.74E-07,1.23E-09,0.008850345,9.73E-08,0.990841957,9.24E-06
5057,sentiment_analysis13,261,"We suspect that the improvements on laptop come from the fact that many answer spans in SQuAD are noun terms , which bear a closer relationship with laptop aspects .",Result Analysis,Domain Laptop,sentiment_analysis,13,13,1,0,,0.000376815,0,negative,0.000951602,3.14E-08,4.24E-05,4.28E-08,1.73E-08,1.47E-06,1.06E-05,2.53E-06,8.20E-08,0.807614356,3.81E-07,0.19136833,8.15E-06
5058,sentiment_analysis13,262,"For ASC , we observed that large - scale annotated MRC data is very useful .",Result Analysis,Domain Laptop,sentiment_analysis,13,14,1,1,results,0.784229107,1,results,0.001913622,2.93E-08,2.10E-05,4.87E-08,1.31E-08,8.24E-07,3.21E-05,1.87E-06,2.55E-08,0.106981043,9.81E-07,0.891027213,2.13E-05
5059,sentiment_analysis13,263,"We suspect the reason is that ASC can be interpreted as a special MRC problem , where all questions are about the polarity of a given aspect .",Result Analysis,Domain Laptop,sentiment_analysis,13,15,1,0,,0.000523873,0,negative,3.48E-05,6.23E-08,1.27E-05,1.20E-07,8.77E-09,1.99E-06,2.45E-06,5.81E-06,3.27E-07,0.994877253,5.30E-06,0.005042884,1.63E-05
5060,sentiment_analysis13,264,MRC training data may help BERT to understand the input format of ASC given their closer input formulation .,Result Analysis,Domain Laptop,sentiment_analysis,13,16,1,0,,0.000757064,0,negative,0.001032366,8.96E-08,0.000117341,3.17E-07,1.07E-07,2.98E-06,3.34E-05,4.26E-06,2.88E-07,0.682754307,9.66E-07,0.315995467,5.81E-05
5061,sentiment_analysis13,265,"Again , domain knowledge post - training also helps ASC .",Result Analysis,Domain Laptop,sentiment_analysis,13,17,1,0,,0.937431495,1,results,0.019392145,8.85E-08,2.22E-05,1.54E-06,6.51E-08,1.10E-05,0.000235943,2.40E-05,2.50E-07,0.126575281,5.06E-07,0.853512616,0.000224394
5062,sentiment_analysis13,266,We further investigated the errors from BERT - PT over the 3 tasks .,Result Analysis,Domain Laptop,sentiment_analysis,13,18,1,0,,0.006877943,0,negative,0.000350746,3.63E-08,9.14E-06,5.95E-09,8.04E-09,4.63E-07,6.11E-06,2.21E-06,3.57E-08,0.941722945,4.39E-08,0.057907339,9.23E-07
5063,sentiment_analysis13,267,The errors on RRC mainly come from boundaries of spans that are not concise enough and incorrect location of spans that may have certain nearby words related to the question .,Result Analysis,Domain Laptop,sentiment_analysis,13,19,1,1,results,0.056643475,0,negative,0.000442623,6.96E-08,2.04E-05,6.18E-07,4.96E-08,4.51E-06,2.48E-05,5.90E-06,1.75E-07,0.929295619,3.99E-06,0.070050345,0.00015086
5064,sentiment_analysis13,268,We believe precisely understanding user 's experience is challenging from only domain posttraining given limited help from the RRC data and no help from the Wikipedia data .,Result Analysis,Domain Laptop,sentiment_analysis,13,20,1,0,,0.004574292,0,negative,0.001064769,5.01E-08,1.11E-05,1.53E-07,3.87E-08,2.05E-06,1.44E-05,4.35E-06,9.88E-08,0.745079593,6.35E-07,0.25378975,3.30E-05
5065,sentiment_analysis13,269,"For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) .",Result Analysis,Domain Laptop,sentiment_analysis,13,21,1,1,results,0.043217471,0,negative,0.000272488,5.02E-08,6.86E-06,3.42E-06,6.07E-08,7.30E-06,1.24E-05,8.88E-06,2.53E-07,0.983655881,2.13E-06,0.015848047,0.000182185
5066,sentiment_analysis13,270,Restaurant suffers from rare aspects like the names of dishes .,Result Analysis,Domain Laptop,sentiment_analysis,13,22,1,0,,0.001621628,0,negative,0.000233788,4.99E-08,1.48E-05,5.21E-06,6.99E-08,9.15E-06,2.09E-05,6.17E-06,3.18E-07,0.973718191,8.34E-06,0.025538953,0.000443995
5067,sentiment_analysis13,271,"ASC tends to have more errors as the decision boundary between the negative and neutral examples is unclear ( e.g. , even annotators may not sure whether the reviewer shows no opinion or slight negative opinion when mentioning an aspect ) .",Result Analysis,Domain Laptop,sentiment_analysis,13,23,1,1,results,0.497124495,0,negative,0.002642888,1.17E-07,5.25E-05,1.14E-06,1.11E-07,3.56E-06,6.62E-05,4.35E-06,1.90E-07,0.589247784,3.71E-06,0.407704257,0.000273225
5068,sentiment_analysis13,272,"Also , BERT - PT has the problem of dealing with one sentence with two opposite opinions ( "" The screen is good but not for windows . "" ) .",Result Analysis,Domain Laptop,sentiment_analysis,13,24,1,0,,0.000495939,0,negative,9.36E-05,1.25E-07,9.08E-05,3.87E-07,4.11E-08,4.49E-06,2.27E-05,7.59E-06,3.39E-07,0.958022363,1.63E-05,0.041521848,0.000219447
5069,sentiment_analysis13,273,We believe that such training examples are rare .,Result Analysis,,sentiment_analysis,13,25,1,0,,0.002680674,0,negative,5.95E-05,1.16E-06,2.25E-06,3.23E-07,3.67E-07,2.55E-05,1.01E-05,0.000216463,9.36E-07,0.998764716,8.84E-07,0.00091706,7.92E-07
5070,sentiment_analysis13,274,Conclusions,,,sentiment_analysis,13,0,1,0,,0.000388901,0,negative,8.30E-05,6.93E-05,4.02E-06,8.07E-07,8.64E-07,3.36E-05,1.76E-05,0.00031599,6.31E-05,0.998796926,0.000458235,0.000155019,1.50E-06
5071,natural_language_inference28,1,title,,,natural_language_inference,28,1,1,0,,0.000701755,0,negative,8.67E-05,0.000310204,3.41E-06,0.00010693,6.41E-06,0.000664696,5.35E-05,0.005762623,0.000365664,0.991721448,0.000883565,2.01E-05,1.47E-05
5072,natural_language_inference28,2,ROTATIONAL UNIT OF MEMORY,,,natural_language_inference,28,2,1,0,,0.006957194,0,negative,0.001104638,0.001649156,0.001185942,0.000167286,5.59E-05,0.000968911,0.000951821,0.00412283,0.00828193,0.966314849,0.014047665,0.000942623,0.000206475
5073,natural_language_inference28,3,abstract,,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,3,1,0,,0.004275135,0,negative,0.000172564,0.000566819,6.30E-06,0.000186221,1.07E-05,0.001072161,0.000114264,0.009973721,0.000790861,0.985126726,0.001897692,4.59E-05,3.61E-05
5074,natural_language_inference28,4,The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks ( RNN ) to state - of - the - art performance in a variety of sequential tasks .,,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,4,1,1,research-problem,0.592254313,1,research-problem,3.42E-05,0.000169441,2.31E-05,1.52E-05,2.03E-06,0.000152321,0.000358591,0.000971368,0.000106011,0.489761252,0.508181697,0.000177914,4.69E-05
5075,natural_language_inference28,5,"However , RNN still have a limited capacity to manipulate long - term memory .",,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,5,1,1,research-problem,0.003387818,0,negative,7.12E-05,7.67E-05,1.44E-05,5.84E-06,2.40E-06,5.79E-05,0.000100322,0.000440304,3.74E-05,0.949425856,0.049562585,0.000193514,1.16E-05
5076,natural_language_inference28,6,To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms .,,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,6,1,0,,0.001605649,0,negative,0.000130913,6.60E-05,9.35E-06,1.67E-05,4.67E-06,8.23E-05,9.78E-05,0.000347288,2.90E-05,0.972777959,0.02623754,0.000187688,1.29E-05
5077,natural_language_inference28,7,In this paper we propose a novel RNN model that unifies the state - of - the - art approaches : Rotational Unit of Memory ( RUM ) .,,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,7,1,0,,0.948831819,1,negative,0.002882781,0.119437299,0.052131441,3.75E-05,0.000270924,0.000433017,0.001880478,0.004337703,0.031035027,0.63261611,0.137706651,0.016918178,0.000312893
5078,natural_language_inference28,8,"The core of RUM is its rotational operation , which is , naturally , a unitary matrix , providing architectures with the power to learn long - term dependencies by overcoming the vanishing and exploding gradients problem .",,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,8,1,0,,0.109273465,0,negative,0.00608599,0.020128508,0.006258144,3.71E-05,0.000252445,0.000300324,0.000245727,0.00208645,0.012740369,0.948223838,0.00080176,0.002787134,5.22E-05
5079,natural_language_inference28,9,"Moreover , the rotational unit also serves as associative memory .",,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,9,1,0,,0.049731642,0,negative,0.002303735,0.001143843,0.000632826,9.26E-06,2.10E-05,0.000109072,5.15E-05,0.000915875,0.004236296,0.989613675,0.000332557,0.000615064,1.54E-05
5080,natural_language_inference28,10,"We evaluate our model on synthetic memorization , question answering and language modeling tasks .",,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,10,1,0,,0.034679347,0,negative,0.000203384,0.003375299,4.19E-05,9.35E-06,0.000103907,0.000158111,0.000533932,0.002512087,8.92E-05,0.988469605,0.000902522,0.003576658,2.40E-05
5081,natural_language_inference28,11,RUM learns the Copying Memory task completely and improves the state - of - the - art result in the Recall task .,,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,11,1,0,,0.84405612,1,negative,0.006360545,0.001788142,0.005389663,5.45E-06,2.54E-05,0.000119634,0.004141725,0.001494249,0.000132403,0.54761078,0.010014085,0.422722373,0.000195562
5082,natural_language_inference28,12,RUM 's performance in the bAbI Question Answering task is comparable to that of models with attention mechanism .,,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,12,1,0,,0.83235272,1,results,0.002038071,0.000188518,0.000403874,3.10E-06,1.22E-05,4.27E-05,0.006759022,0.00049109,9.25E-06,0.263033448,0.006955323,0.719891974,0.000171462
5083,natural_language_inference28,13,"We also improve the state - of - the - art result to 1.189 bits - per- character ( BPC ) loss in the Character Level Penn Treebank ( PTB ) task , which is to signify the applications of RUM to real - world sequential data .",,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,13,1,0,,0.772194372,1,negative,0.006384563,0.005847963,0.000616552,2.11E-05,0.000106314,0.000198079,0.004421987,0.004474235,0.000213902,0.507950852,0.004295495,0.465129773,0.000339207
5084,natural_language_inference28,14,"The universality of our construction , at the core of RNN , establishes RUM as a promising approach to language modeling , speech recognition and machine translation .",,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,14,1,0,,0.030062902,0,negative,0.000316994,0.00043303,1.41E-05,1.62E-05,2.59E-05,5.42E-05,7.21E-05,0.000434408,0.000157307,0.995986415,0.00099386,0.001482469,1.30E-05
5085,natural_language_inference28,15,* equal contribution 1 ar Xiv:1710.09537v1 [ cs.LG ],,ROTATIONAL UNIT OF MEMORY,natural_language_inference,28,15,1,0,,0.015089792,0,negative,0.000516153,0.000639313,4.67E-05,0.057872955,0.000232442,0.005660586,0.000830371,0.00476093,0.000686237,0.921221174,0.007074637,0.000109594,0.000348953
5086,natural_language_inference28,16,INTRODUCTION,,,natural_language_inference,28,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
5087,natural_language_inference28,17,"Recurrent neural networks are widely used in a variety of machine learning applications such as language modeling ) , machine translation ) and speech recognition ) .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,1,1,0,,0.354971011,0,research-problem,9.46E-07,0.000386628,9.19E-07,2.34E-06,4.15E-06,1.24E-05,1.48E-05,2.18E-05,0.000761057,0.076836409,0.921955597,1.43E-06,1.61E-06
5088,natural_language_inference28,18,Their flexibility of taking inputs of dynamic length makes RNN particularly useful for these tasks .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,2,1,0,,0.009746024,0,negative,8.01E-05,0.011743231,3.58E-05,7.75E-05,0.000835649,0.000142492,5.02E-05,9.99E-05,0.013533858,0.790336684,0.183035607,2.24E-05,6.54E-06
5089,natural_language_inference28,19,"However , the traditional RNN models such as Long Short - Term Memory ( LSTM , ) and Gated Recurrent Unit ( GRU , ) exhibit some weaknesses that prevent them from achieving human level performance :",INTRODUCTION,INTRODUCTION,natural_language_inference,28,3,1,0,,0.027156414,0,research-problem,9.99E-06,0.000755572,3.00E-06,3.70E-06,2.26E-05,1.14E-05,2.15E-05,1.46E-05,0.00034436,0.338553843,0.660249661,8.26E-06,1.50E-06
5090,natural_language_inference28,20,"1 ) limited memory - they can only remember a hidden state , which usually occupies a small part of a model ; 2 ) gradient vanishing / explosion ) during training - trained with backpropagation through time the models fail to learn long - term dependencies .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,4,1,0,,0.167673498,0,negative,0.000982671,0.048736677,0.000272387,0.000518477,0.006931328,0.000561744,0.000129694,0.00023382,0.030823417,0.87648565,0.034247861,5.54E-05,2.08E-05
5091,natural_language_inference28,21,Several ways to address those problems are known .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,5,1,0,,0.007158668,0,negative,4.73E-06,0.00082093,9.16E-07,1.15E-05,3.81E-05,5.10E-05,1.54E-05,5.79E-05,0.000813817,0.703720182,0.294461229,2.80E-06,1.58E-06
5092,natural_language_inference28,22,"One solution is to use soft and local attention mechanisms ) , which is crucial for most modern applications of RNN .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,6,1,0,,0.020558068,0,negative,3.95E-05,0.002783781,8.51E-06,0.000365484,0.00049619,0.000221307,4.71E-05,0.000120576,0.001399845,0.67691338,0.317587735,6.70E-06,9.85E-06
5093,natural_language_inference28,23,"Nevertheless , researchers are still interested in improving basic RNN cell models to process sequential data better .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,7,1,0,,0.025301234,0,research-problem,6.06E-06,0.00096651,1.00E-06,5.76E-06,2.04E-05,2.99E-05,2.94E-05,5.44E-05,0.000726508,0.405554375,0.592596418,6.86E-06,2.41E-06
5094,natural_language_inference28,24,Numerous works ; ) use associative memory to span a large memory space .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,8,1,0,,0.017307506,0,negative,1.92E-05,0.005817978,1.56E-05,1.49E-05,9.91E-05,7.53E-05,5.03E-05,8.86E-05,0.004533676,0.528331976,0.460939111,9.73E-06,4.57E-06
5095,natural_language_inference28,25,"For example , a practical way to implement associative memory is to set weight matrices as trainable structures that change according to input instances for training .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,9,1,0,,0.006844316,0,negative,4.02E-06,0.00375719,3.12E-06,1.07E-05,2.82E-05,0.000176333,4.11E-05,0.000304498,0.011931861,0.593873172,0.389860429,4.69E-06,4.72E-06
5096,natural_language_inference28,26,"Furthermore , the recent concept of unitary or orthogonal evolution matrices ; ) also provides a theoretical and empirical solution to the problem of memorizing long - term dependencies .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,10,1,0,,0.133593773,0,negative,1.83E-05,0.013235378,2.23E-05,1.69E-05,0.000185527,7.50E-05,3.83E-05,9.38E-05,0.019408013,0.675109148,0.291783323,1.06E-05,3.36E-06
5097,natural_language_inference28,27,"Here , we propose a novel RNN cell that resolves simultaneously those weaknesses of basic RNN .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,11,1,1,model,0.956341204,1,model,4.33E-05,0.155766614,0.000295898,4.03E-06,0.000523539,3.16E-05,5.04E-05,4.32E-05,0.819215542,0.01736977,0.006634653,1.64E-05,4.97E-06
5098,natural_language_inference28,28,The Rotational Unit of Memory is a modified gated model whose rotational operation acts as associative memory and is strictly an orthogonal matrix .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,12,1,1,model,0.925008848,1,model,2.65E-05,0.031840747,0.000303054,3.74E-06,0.000259273,4.79E-05,2.73E-05,3.54E-05,0.949237295,0.017117405,0.001092708,4.92E-06,3.63E-06
5099,natural_language_inference28,29,We tested our model on several benchmarks .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,13,1,0,,0.005633787,0,negative,1.41E-05,0.014500899,3.60E-06,3.26E-05,0.001644247,0.001263254,0.000278616,0.001241439,0.003416993,0.972989509,0.004580123,2.62E-05,8.42E-06
5100,natural_language_inference28,30,RUM is able to solve the synthetic Copying Memory task while traditional LSTM and GRU fail .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,14,1,0,,0.100077013,0,negative,0.002021278,0.088346743,0.000401403,3.48E-05,0.001562151,0.000265872,0.008298474,0.00061133,0.018495598,0.746954845,0.125766817,0.007108161,0.000132567
5101,natural_language_inference28,31,"For synthetic Recall task , RUM exhibits a stronger ability to remember sequences , hence outperforming state - of - the - art RNN models such as Fastweight RNN ) and WeiNet ) .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,15,1,0,,0.234875779,0,negative,0.010356104,0.068624146,0.000241191,5.38E-05,0.002742427,0.000255095,0.015167991,0.000731525,0.010030211,0.832587703,0.031144098,0.02789987,0.000165861
5102,natural_language_inference28,32,By using RUM we achieve the state - of - the - art result in the real - world Character Level Penn Treebank task .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,16,1,0,,0.328585186,0,negative,0.001398585,0.195330021,0.000298306,2.44E-05,0.0020838,0.000221814,0.009922793,0.000684518,0.037418744,0.654054662,0.089756029,0.008636393,0.000169967
5103,natural_language_inference28,33,RUM also outperforms all basic RNN models in the bAbI question answering task .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,17,1,0,,0.241082985,0,negative,0.001605442,0.036536666,0.000210226,8.39E-05,0.002674876,0.000953397,0.06409773,0.002197956,0.004467116,0.791265135,0.066063711,0.029104976,0.000738838
5104,natural_language_inference28,34,"This performance is competitive with that of memory networks , which take advantage of attention mechanisms .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,18,1,0,,0.080179018,0,negative,9.05E-05,0.023594582,2.24E-05,2.01E-05,0.000576543,0.000366393,0.000856235,0.000702597,0.050590683,0.890882212,0.031907178,0.00035957,3.10E-05
5105,natural_language_inference28,35,Our contributions are as follows :,INTRODUCTION,INTRODUCTION,natural_language_inference,28,19,1,0,,0.00226048,0,negative,2.17E-05,0.009805355,1.23E-05,0.000403262,0.003333686,0.000506595,7.97E-05,0.000240908,0.005530885,0.968956477,0.011089513,7.67E-06,1.20E-05
5106,natural_language_inference28,36,1 .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,20,1,0,,0.00187482,0,negative,8.20E-06,0.002689131,1.93E-06,9.23E-06,0.000133776,0.000201362,2.09E-05,0.000247956,0.01101606,0.984681029,0.000986526,2.55E-06,1.33E-06
5107,natural_language_inference28,37,We develop the concept of the Rotational Unit that combines the memorization advantage of unitary / orthogonal matrices with the dynamic structure of associative memory ;,INTRODUCTION,INTRODUCTION,natural_language_inference,28,21,1,0,,0.917868483,1,model,1.79E-05,0.065337033,0.0001113,2.07E-06,0.000219071,2.16E-05,1.75E-05,2.44E-05,0.917237167,0.015869364,0.001135159,5.26E-06,2.20E-06
5108,natural_language_inference28,38,2 .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,22,1,0,,0.007280617,0,negative,1.27E-05,0.006255185,4.57E-06,5.33E-06,0.000122343,0.000294938,4.60E-05,0.000440625,0.038433506,0.953178583,0.001199138,4.67E-06,2.35E-06
5109,natural_language_inference28,39,We implement the rotational operation into a novel RNN model - RUM - which outperforms significantly the current frontier of models on a variety of sequential tasks .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,23,1,0,,0.922629391,1,model,3.60E-05,0.273557541,0.000233376,2.17E-06,0.000504727,5.04E-05,0.000150765,7.79E-05,0.705761147,0.018384188,0.0012144,1.96E-05,7.79E-06
5110,natural_language_inference28,40,MOTIVATION AND RELATED WORK,,,natural_language_inference,28,0,1,0,,0.00011597,0,negative,1.42E-05,4.53E-05,4.70E-06,8.89E-07,4.38E-07,2.89E-05,3.28E-05,0.000256847,2.28E-05,0.977040186,0.022485449,6.46E-05,2.97E-06
5111,natural_language_inference28,41,UNITARY APPROACH,,,natural_language_inference,28,0,1,0,,0.094368441,0,negative,0.000171707,0.00061804,0.000340699,3.86E-06,1.79E-06,0.000201406,0.000287755,0.002046866,0.000940035,0.877357423,0.116547257,0.001448592,3.46E-05
5112,natural_language_inference28,42,The problem of the gradient vanishing and exploding problem is well - known to obstruct the learning of long - term dependencies ) .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,1,1,0,,0.001143123,0,negative,1.32E-05,1.47E-05,2.46E-06,4.18E-06,2.77E-07,6.80E-06,2.23E-06,3.86E-05,1.48E-05,0.996842653,0.003041948,1.65E-05,1.66E-06
5113,natural_language_inference28,43,We will give a brief mathematical motivation of the problem .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,2,1,0,,9.77E-07,0,negative,2.49E-06,1.33E-05,7.35E-07,1.61E-07,1.35E-07,2.13E-06,1.92E-07,3.15E-05,2.73E-05,0.999912399,5.75E-06,3.83E-06,7.72E-08
5114,natural_language_inference28,44,Let 's assume the cost function is C.,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,3,1,0,,7.04E-06,0,negative,3.40E-06,8.33E-06,8.84E-07,3.85E-08,5.05E-08,2.75E-06,2.76E-07,8.17E-05,2.56E-05,0.999868607,2.97E-06,5.30E-06,5.58E-08
5115,natural_language_inference28,45,In order to evaluate ?C /?,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,4,1,0,,7.91E-06,0,negative,1.99E-05,2.51E-06,1.93E-06,4.45E-09,3.56E-08,2.63E-07,1.45E-07,4.67E-06,2.12E-06,0.9999164,7.80E-07,5.13E-05,7.53E-09
5116,natural_language_inference28,46,"W ij , one computes the derivative gradient using the chain rule :",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,5,1,0,,1.13E-05,0,negative,3.12E-05,1.81E-05,4.75E-05,3.11E-08,9.37E-08,2.12E-06,5.06E-07,2.43E-05,8.47E-05,0.999767463,2.57E-06,2.12E-05,5.78E-08
5117,natural_language_inference28,47,where D ( k ) = diag{ ? ( Wx ( k ) + Ah ( k?1 ) + b ) } is the Jacobian matrix of the point - wise nonlinearity .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,6,1,0,,3.34E-05,0,negative,1.37E-05,2.86E-05,5.41E-06,1.35E-07,2.56E-07,6.42E-06,9.50E-07,0.000125965,5.27E-05,0.99974514,3.80E-06,1.67E-05,1.70E-07
5118,natural_language_inference28,48,"As long as the eigenvalues of D ( k ) are of order unity , then if W has eigenvalues ?",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,7,1,0,,8.38E-06,0,negative,1.72E-05,4.16E-06,1.70E-06,4.95E-08,1.29E-07,2.37E-06,5.11E-07,2.80E-05,6.34E-06,0.99989764,2.25E-06,3.96E-05,5.70E-08
5119,natural_language_inference28,49,"i 1 , they will cause gradient explosion ?",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,8,1,0,,5.41E-06,0,negative,3.26E-06,1.33E-06,4.94E-07,5.71E-08,3.45E-08,1.75E-06,1.74E-07,1.63E-05,7.00E-06,0.999963252,9.59E-07,5.38E-06,3.41E-08
5120,natural_language_inference28,50,"C /?h ( T ) ? ? , while if W has eigenvalues ?",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,9,1,0,,1.06E-05,0,negative,5.06E-05,8.45E-06,4.25E-06,1.97E-07,4.62E-07,5.09E-06,1.36E-06,5.08E-05,1.14E-05,0.999776478,2.54E-06,8.82E-05,1.82E-07
5121,natural_language_inference28,51,"i 1 , they can cause gradient vanishing , ?C /?h ( T ) ?",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,10,1,0,,1.07E-05,0,negative,9.04E-05,7.10E-06,6.15E-06,1.04E-07,2.20E-07,3.04E-06,8.66E-07,3.42E-05,1.35E-05,0.999731322,2.03E-06,0.000110973,1.14E-07
5122,natural_language_inference28,52,0 . Either situation hampers the efficiency of RNN .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,11,1,0,,5.61E-06,0,negative,7.97E-05,1.62E-06,1.94E-06,7.04E-08,1.49E-07,1.52E-06,4.40E-07,1.34E-05,3.69E-06,0.999815943,1.13E-06,8.04E-05,7.26E-08
5123,natural_language_inference28,53,"LSTM is designed to solve this problem , but gradient clipping ) is still required for training .",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,12,1,0,,0.000137188,0,negative,6.19E-05,4.83E-05,9.65E-05,2.54E-06,1.34E-06,2.18E-05,7.60E-06,0.000176837,0.00010721,0.999216258,0.000193352,6.11E-05,5.22E-06
5124,natural_language_inference28,54,"Recently , by restraining the hidden - to - hidden matrix to be orthogonal or unitary , many models have overcome the problem of exploding and vanishing gradients .",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,13,1,0,,0.000145143,0,negative,3.18E-05,2.55E-05,3.65E-06,2.78E-06,6.67E-07,1.13E-05,2.68E-06,8.26E-05,3.53E-05,0.999392217,0.000365971,4.35E-05,2.11E-06
5125,natural_language_inference28,55,"Theoretically , unitary and orthogonal matrices will keep the norm of the gradient because the absolute value of their eigenvalues equals one .",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,14,1,0,,6.58E-05,0,negative,3.79E-05,1.52E-05,5.12E-06,1.17E-07,2.85E-07,3.49E-06,1.06E-06,7.24E-05,5.56E-05,0.999750504,3.60E-06,5.45E-05,2.48E-07
5126,natural_language_inference28,56,Several approaches have successfully developed the applications of unitary and orthogonal matrix to recurrent neural networks .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,15,1,0,,7.52E-05,0,negative,7.02E-06,1.27E-05,5.87E-06,1.35E-07,1.15E-07,2.85E-06,2.19E-06,3.43E-05,1.59E-05,0.999335114,0.000538425,4.47E-05,6.21E-07
5127,natural_language_inference28,57,use parameterizations to form the unitary spaces .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,16,1,0,,2.45E-06,0,negative,8.55E-06,2.27E-06,4.37E-06,7.43E-08,1.05E-07,5.12E-06,5.56E-07,3.67E-05,1.25E-05,0.999919674,2.82E-07,9.64E-06,1.05E-07
5128,natural_language_inference28,58,applies gradient projection onto a unitary manifold .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,17,1,0,,0.000429409,0,negative,0.000217221,6.69E-05,0.001001676,5.20E-07,2.21E-06,6.03E-06,2.89E-06,4.38E-05,0.000333629,0.998211934,3.17E-06,0.000108594,1.36E-06
5129,natural_language_inference28,59,"uses penalty terms as a regularization to restrain matrices to be unitary , hence accessing long - term memorization .",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,18,1,0,,0.000157656,0,negative,0.000252915,2.36E-05,0.000184451,1.30E-07,7.34E-07,3.57E-06,1.48E-06,3.66E-05,0.000113321,0.999269445,8.66E-07,0.0001124,4.33E-07
5130,natural_language_inference28,60,Only learning long - term dependencies is not sufficient for a powerful RNN. finds that the combination of unitary / orthogonal matrices with a gated mechanism improves the performance of RNN because of the benefits of a forgetting ability .,UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,19,1,0,,0.006593868,0,negative,0.000446455,7.80E-06,2.70E-05,1.05E-07,2.66E-07,1.91E-06,7.50E-06,2.11E-05,1.05E-05,0.995371599,3.82E-05,0.004066332,1.25E-06
5131,natural_language_inference28,61,"also points out the optimal way of such a unitary / gated combination : the unitary / orthogonal matrix should appear before the reset gate , which can then be followed by a modReLU activation .",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,20,1,0,,1.88E-06,0,negative,1.14E-05,2.36E-06,4.59E-06,6.52E-08,1.65E-07,1.65E-06,4.56E-07,1.29E-05,1.01E-05,0.999936184,6.61E-07,1.94E-05,9.17E-08
5132,natural_language_inference28,62,"In RUM we implement an orthogonal operation in the same place , but the construction of that matrix is completely different : instead of parameterizing the kernel , we encode a natural rotation , generated by the inputs and the hidden state .",UNITARY APPROACH,UNITARY APPROACH,natural_language_inference,28,21,1,0,,5.64E-06,0,negative,3.61E-05,7.52E-05,6.15E-05,5.01E-08,5.64E-07,1.24E-06,4.51E-07,1.80E-05,0.000298567,0.999477683,7.58E-07,2.98E-05,1.30E-07
5133,natural_language_inference28,63,ASSOCIATIVE MEMORY APPROACH,,,natural_language_inference,28,0,1,0,,0.059832062,0,research-problem,4.71E-05,0.000319959,0.000282577,3.26E-06,9.66E-07,0.000176926,0.000596488,0.001694511,0.000551963,0.379447525,0.615858607,0.000954071,6.60E-05
5134,natural_language_inference28,64,Limited memory in RNN is truly a shortage .,ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,1,1,0,,6.76E-05,0,negative,1.13E-05,3.20E-06,1.56E-06,1.24E-06,1.26E-07,5.22E-06,3.04E-06,1.42E-05,1.56E-05,0.999055511,0.000855658,3.29E-05,5.31E-07
5135,natural_language_inference28,65,Adding an external associative memory is a natural solution .,ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,2,1,0,,1.31E-05,0,negative,3.48E-05,4.09E-06,1.32E-06,1.18E-06,1.37E-07,9.60E-06,1.37E-06,3.20E-05,3.24E-05,0.999845886,1.73E-05,1.97E-05,1.74E-07
5136,natural_language_inference28,66,"For instance , the Neural Turing Machine ) and many other models have shown the power of using this technique .",ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,3,1,0,,2.61E-06,0,negative,3.59E-06,2.27E-06,2.15E-06,1.17E-07,2.78E-08,3.54E-06,2.74E-06,1.73E-05,1.46E-05,0.999592591,0.000342868,1.80E-05,1.66E-07
5137,natural_language_inference28,67,"While it expands the accessible memory space , the technique significantly increases the size of the model , therefore making the process of learning so many parameters harder .",ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,4,1,0,,2.67E-05,0,negative,0.000414376,2.00E-05,5.55E-06,9.88E-08,3.32E-07,1.04E-06,6.75E-07,7.33E-06,2.94E-05,0.999346654,4.15E-06,0.000170373,4.41E-08
5138,natural_language_inference28,68,"Now , we will briefly describe the concept of associative memory .",ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,5,1,0,,1.86E-06,0,negative,6.06E-07,1.82E-05,1.29E-06,1.12E-08,2.67E-08,7.99E-07,2.25E-07,1.34E-05,0.000165865,0.999777598,1.83E-05,3.59E-06,2.60E-08
5139,natural_language_inference28,69,"In basic RNN , ht = ? ( W x t + Ah t?1 + b ) where ht is the hidden state at time step t and x is the input data at each step .",ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,6,1,0,,2.34E-06,0,negative,6.01E-07,2.24E-05,2.07E-06,3.32E-08,2.51E-08,3.01E-06,8.99E-07,7.66E-05,0.000203057,0.999632535,5.47E-05,4.03E-06,8.58E-08
5140,natural_language_inference28,70,Here W and A are trainable parameters thatare fixed in the model .,ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,7,1,0,,8.58E-07,0,negative,4.92E-06,2.74E-05,1.23E-06,9.69E-08,9.18E-08,6.61E-06,8.73E-07,0.000200369,0.000154501,0.999596482,1.63E-06,5.71E-06,5.82E-08
5141,natural_language_inference28,71,A recent approach replaces A with a dynamic At ( as a function of time ) so that this matrix can serve as a memory state .,ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,8,1,0,,3.10E-06,0,negative,1.30E-05,3.03E-05,2.76E-05,2.45E-08,6.01E-08,7.16E-07,4.85E-07,9.04E-06,0.000104692,0.999761391,3.17E-05,2.10E-05,4.61E-08
5142,natural_language_inference28,72,"Thus , the memory size increases from O(N h ) to O (N 2 h ) , where N h is the hidden size .",ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,9,1,0,,0.000151318,0,negative,2.74E-05,3.61E-05,3.02E-06,1.01E-07,1.52E-07,4.87E-06,4.46E-06,9.59E-05,0.000133646,0.999519678,4.61E-05,0.000128183,3.56E-07
5143,natural_language_inference28,73,"In particular , At is determined by A t?1 , h t?1 and x t which can be apart of a multi - layer or a Hopfiled net .",ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,10,1,0,,2.28E-06,0,negative,4.25E-06,1.23E-05,1.12E-06,1.88E-08,4.75E-08,5.79E-07,1.85E-07,1.23E-05,0.00014151,0.999820475,7.73E-07,6.38E-06,1.22E-08
5144,natural_language_inference28,74,"By treating the RNN weights as memory determined by the current input data , a larger memory size is provided and less trainable parameters are required .",ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,11,1,0,,8.06E-05,0,negative,0.000131108,9.41E-05,5.09E-06,4.92E-08,3.54E-07,1.12E-06,6.78E-07,3.35E-05,0.000171434,0.999394846,1.63E-06,0.000166084,5.01E-08
5145,natural_language_inference28,75,This significantly increases the memorization ability of RNN .,ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,12,1,0,,7.75E-05,0,negative,0.000203477,5.48E-06,4.82E-06,2.41E-08,1.52E-07,4.18E-07,6.65E-07,4.54E-06,2.15E-05,0.999361395,9.15E-07,0.000396529,3.62E-08
5146,natural_language_inference28,76,Our model also falls into this category of associative memory through its rotational design of an orthogonal At matrix .,ASSOCIATIVE MEMORY APPROACH,ASSOCIATIVE MEMORY APPROACH,natural_language_inference,28,13,1,0,,5.96E-06,0,negative,3.56E-05,7.69E-05,0.000102428,5.02E-08,3.73E-07,1.55E-06,1.18E-06,1.06E-05,0.000869212,0.998827311,5.91E-06,6.87E-05,1.42E-07
5147,natural_language_inference28,77,METHODS,,,natural_language_inference,28,0,1,0,,2.98E-05,0,negative,4.58E-06,0.000323553,9.58E-06,8.17E-07,1.29E-06,6.39E-05,1.47E-05,0.001183839,0.000161795,0.996781926,0.001437094,1.57E-05,1.25E-06
5148,natural_language_inference28,78,The goal of this section is to suggest ways of engineering models that incorporate rotations as units of memory .,METHODS,METHODS,natural_language_inference,28,1,1,0,,0.132103281,0,negative,0.000292018,0.004239116,0.002271715,0.000121669,9.44E-05,0.000787105,0.000238967,0.009330257,0.003131378,0.974498033,0.004706912,0.000232828,5.56E-05
5149,natural_language_inference28,79,In the following discussion N x is the input size and N h is the hidden size .,METHODS,METHODS,natural_language_inference,28,2,1,0,,0.029160029,0,negative,6.23E-05,0.000980763,0.000480484,6.91E-06,9.69E-06,0.001305231,0.000127576,0.051490967,0.001657665,0.9434915,0.000314932,5.75E-05,1.45E-05
5150,natural_language_inference28,80,THE OPERATION,METHODS,,natural_language_inference,28,3,1,0,,0.029729309,0,negative,0.000130718,0.001238001,0.00219843,3.94E-05,4.36E-05,0.000994779,0.000448164,0.019255949,0.00475309,0.968602107,0.001993195,0.00021972,8.29E-05
5151,natural_language_inference28,81,Rotation,METHODS,THE OPERATION,natural_language_inference,28,4,1,0,,0.003366868,0,negative,0.001408606,0.000331259,0.000208125,1.25E-05,7.11E-05,0.000947839,8.51E-05,0.000491902,0.001956428,0.993704517,0.000697402,5.76E-05,2.77E-05
5152,natural_language_inference28,82,"The operation Rotation is an efficient encoder of an orthogonal operation , which acts as a unit of memory .",METHODS,THE OPERATION,natural_language_inference,28,5,1,0,,0.008478791,0,negative,0.004483988,0.004503484,0.036391631,5.24E-06,0.000119659,0.000580515,5.84E-05,0.000276038,0.072995476,0.880327293,0.000201258,4.02E-05,1.68E-05
5153,natural_language_inference28,83,"Rotation computes an orthogonal operator R ( a , b ) in RN h N h that represents the rotation between two non-collinear vectors a and bin the two - dimensional subspace span ( a , b ) of the Euclidean space RN h with distance .",METHODS,THE OPERATION,natural_language_inference,28,6,1,0,,0.002008477,0,negative,0.000688781,0.001580402,0.004754725,5.70E-06,9.01E-05,0.000552792,3.31E-05,0.000231995,0.006912273,0.984545165,0.000573106,1.91E-05,1.27E-05
5154,natural_language_inference28,84,"As a consequence , R can act as a kernel on a hidden state h.",METHODS,THE OPERATION,natural_language_inference,28,7,1,0,,0.000108514,0,negative,0.000221179,0.000113908,6.98E-05,1.89E-07,3.06E-06,3.93E-05,1.43E-06,4.14E-05,0.00113373,0.99835748,1.45E-05,3.65E-06,3.21E-07
5155,natural_language_inference28,85,"More formally , what we propose is a function",METHODS,THE OPERATION,natural_language_inference,28,8,1,0,,6.98E-05,0,negative,5.98E-05,0.000613942,7.08E-05,4.30E-07,3.48E-06,0.000132468,4.41E-06,0.000210283,0.009455331,0.98913525,0.000308973,3.02E-06,1.84E-06
5156,natural_language_inference28,86,such that after ortho-normalizing a and b to A practical advantage of Rotation is that it is both orthogonal and differentiable .,METHODS,THE OPERATION,natural_language_inference,28,9,1,0,,0.000180836,0,negative,0.000509587,0.000148195,5.78E-05,1.48E-06,2.80E-05,0.000107807,4.52E-06,8.88E-05,0.000307461,0.998708878,2.13E-05,1.52E-05,8.97E-07
5157,natural_language_inference28,87,"On one hand , it is a composition of differentiable sub-operations , which enables learning via backpropagation .",METHODS,THE OPERATION,natural_language_inference,28,10,1,0,,0.004500877,0,negative,0.004093188,0.011121211,0.009898635,1.80E-06,7.29E-05,0.000216474,2.74E-05,0.000230796,0.088227303,0.885911814,0.000129817,6.20E-05,6.70E-06
5158,natural_language_inference28,88,"On the other hand , it preserves the norm of the hidden state , hence it can yield more stable gradients .",METHODS,THE OPERATION,natural_language_inference,28,11,1,0,,0.033137081,0,negative,0.098529607,0.000489259,0.000375891,5.45E-06,8.74E-05,0.000143229,3.15E-05,0.000100528,0.000432166,0.899122921,5.33E-05,0.000624066,4.74E-06
5159,natural_language_inference28,89,We were motivated to find differentiable implementations of unitary ( orthogonal in particular ) operations in existing toolkits for deep learning .,METHODS,THE OPERATION,natural_language_inference,28,12,1,0,,0.00091654,0,negative,7.86E-05,0.000505797,2.02E-05,5.22E-06,1.80E-05,0.000407327,1.32E-05,0.000322774,0.000478743,0.996083504,0.002053258,6.66E-06,6.76E-06
5160,natural_language_inference28,90,Our conclusion is that Rotation can be implemented in various frameworks thatare utilized for RNN and other deep learning architectures .,METHODS,THE OPERATION,natural_language_inference,28,13,1,0,,0.017460055,0,negative,0.001288579,0.000177575,2.52E-05,5.35E-06,4.16E-05,0.000251934,1.11E-05,9.59E-05,0.000260048,0.997758638,4.06E-05,4.07E-05,2.74E-06
5161,natural_language_inference28,91,"Indeed , Rotation is not constrained to parameterize a unitary structure , but instead it produces an orthogonal matrix from simple components in the cell , which makes it useful for experimentation .",METHODS,THE OPERATION,natural_language_inference,28,14,1,0,,0.000151578,0,negative,0.000768426,8.41E-05,4.61E-05,4.52E-07,1.70E-05,4.55E-05,2.46E-06,2.37E-05,0.000112777,0.99887447,5.43E-06,1.94E-05,2.77E-07
5162,natural_language_inference28,92,We implement Rotation together with its action on a hidden state efficiently .,METHODS,THE OPERATION,natural_language_inference,28,15,1,0,,0.029077945,0,negative,0.002517204,0.024146578,0.000779829,2.31E-05,0.000373867,0.00080885,6.97E-05,0.000940724,0.088427304,0.881426438,0.000346719,9.69E-05,4.27E-05
5163,natural_language_inference28,93,We do not need to compute the matrix R t before we rotate .,METHODS,THE OPERATION,natural_language_inference,28,16,1,0,,8.38E-05,0,negative,0.000522679,0.00018156,4.58E-05,1.83E-06,3.18E-05,0.000174411,7.43E-06,0.000117854,0.000734988,0.99815887,8.11E-06,1.27E-05,1.90E-06
5164,natural_language_inference28,94,Instead we can directly apply the RHS of equation to the hidden state .,METHODS,THE OPERATION,natural_language_inference,28,17,1,0,,0.000184747,0,negative,0.000223616,0.000117757,6.13E-05,2.81E-07,4.84E-06,4.25E-05,1.92E-06,3.88E-05,0.000935836,0.998558922,8.52E-06,5.03E-06,5.85E-07
5165,natural_language_inference28,95,"Hence , the memory complexity of our algorithm is the RHS of ( 1 ) .",METHODS,THE OPERATION,natural_language_inference,28,18,1,0,,0.000487031,0,negative,0.000553712,0.002235963,8.11E-05,2.38E-06,2.75E-05,0.001012707,8.19E-05,0.001835365,0.006167368,0.987508791,0.000422548,4.59E-05,2.47E-05
5166,natural_language_inference28,96,"Note that we only use two trainable vectors in RN h to generate orthogonal weights in RN h N h , which means the model has O (N 2 h ) degrees of freedom for a single unit of memory .",METHODS,THE OPERATION,natural_language_inference,28,19,1,0,,0.000418275,0,negative,0.001150307,0.000501866,0.000133524,5.87E-07,2.30E-05,0.000332805,2.80E-05,0.000384294,0.002071945,0.995328064,1.28E-05,2.85E-05,4.18E-06
5167,natural_language_inference28,97,"Likewise , the time complexity is O ( N b N 2 h ) .",METHODS,THE OPERATION,natural_language_inference,28,20,1,0,,0.000902323,0,negative,0.000405239,0.000320253,6.62E-05,5.22E-07,1.09E-05,0.000588093,5.35E-05,0.000611465,0.001948639,0.995909388,5.09E-05,2.44E-05,1.05E-05
5168,natural_language_inference28,98,"Thus , Rotation is a universal operation that enables implementations suitable to any neural network model with backpropagation .",METHODS,THE OPERATION,natural_language_inference,28,21,1,0,,0.003053542,0,negative,0.000413217,0.001153669,0.000113817,5.01E-06,8.30E-05,0.000214255,1.34E-05,0.000194312,0.00638998,0.991343831,4.52E-05,2.49E-05,5.36E-06
5169,natural_language_inference28,99,THE RUM ARCHITECTURE,,,natural_language_inference,28,0,1,0,,0.658323811,1,negative,0.000150493,0.002955269,0.001323344,1.47E-05,7.10E-06,0.000819046,0.001026619,0.007700215,0.012274809,0.767408014,0.204961222,0.001161524,0.000197671
5170,natural_language_inference28,100,We propose the Recurrent Unit of Memory as the first example of an application of Rotation to a recurrent cell .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,1,1,0,,0.000172481,0,negative,4.32E-05,0.001069826,9.23E-05,1.41E-06,3.29E-06,8.73E-06,6.63E-06,0.000100316,0.006918921,0.987753285,0.003852862,0.000147484,1.79E-06
5171,natural_language_inference28,101,is a sketch of the connections in the cell .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,2,1,0,,1.90E-06,0,negative,1.13E-05,2.52E-05,4.59E-06,5.06E-07,4.14E-07,6.46E-06,1.18E-06,6.88E-05,0.000491792,0.999369185,1.10E-05,9.42E-06,1.85E-07
5172,natural_language_inference28,102,RUM consists of an update gate u ?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,3,1,0,,1.67E-05,0,negative,2.38E-05,0.000119109,0.00016528,8.55E-08,4.01E-07,3.25E-06,1.82E-06,4.43E-05,0.001404111,0.998149848,4.52E-05,4.26E-05,1.98E-07
5173,natural_language_inference28,103,RN h that has the same function as in GRU .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,4,1,0,,4.96E-06,0,negative,4.37E-05,2.67E-05,4.09E-05,2.13E-08,1.71E-07,7.36E-07,7.61E-07,1.08E-05,5.44E-05,0.999680185,1.02E-05,0.000131421,2.83E-08
5174,natural_language_inference28,104,"Instead of a reset gate , however , the model learns a memory target variable ?",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,5,1,0,,3.16E-05,0,negative,2.56E-06,1.61E-05,9.72E-07,3.42E-08,4.27E-08,1.24E-06,1.61E-07,2.33E-05,0.000335381,0.999610219,6.92E-06,3.07E-06,2.23E-08
5175,natural_language_inference28,105,?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,6,1,0,,9.31E-07,0,negative,2.16E-06,3.18E-06,2.83E-07,2.62E-08,2.68E-08,1.51E-06,2.85E-07,1.87E-05,4.03E-05,0.999921938,5.93E-06,5.65E-06,2.20E-08
5176,natural_language_inference28,106,RN h .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,7,1,0,,1.67E-05,0,negative,2.44E-05,1.25E-05,2.85E-06,3.98E-07,2.89E-07,4.74E-06,2.13E-06,4.78E-05,0.000104636,0.999680792,4.50E-05,7.43E-05,2.93E-07
5177,natural_language_inference28,107,RUM also learns to embed the input vector x ?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,8,1,0,,2.40E-05,0,negative,2.52E-05,0.000172285,0.000162295,3.41E-08,2.55E-07,2.02E-06,1.04E-06,4.34E-05,0.002562197,0.99697599,1.75E-05,3.77E-05,1.11E-07
5178,natural_language_inference28,108,R Nx into RN h to yield ?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,9,1,0,,3.37E-06,0,negative,6.10E-05,1.56E-05,3.01E-06,2.31E-07,5.17E-07,3.56E-06,2.05E-06,4.42E-05,6.72E-05,0.999702183,8.82E-06,9.15E-05,1.32E-07
5179,natural_language_inference28,109,?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,10,1,0,,9.12E-07,0,negative,2.09E-06,3.27E-06,2.93E-07,2.64E-08,3.26E-08,1.48E-06,3.05E-07,1.91E-05,4.44E-05,0.999918879,3.73E-06,6.38E-06,2.44E-08
5180,natural_language_inference28,110,RN h .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,11,1,0,,1.50E-05,0,negative,2.37E-05,1.28E-05,2.83E-06,3.87E-07,3.42E-07,4.63E-06,2.13E-06,4.94E-05,0.000116498,0.999677921,2.73E-05,8.18E-05,3.10E-07
5181,natural_language_inference28,111,"Hence Rotation encodes the rotation between the embedded input and the target , which is accumulated to the associative memory unit R t ?",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,12,1,0,,2.07E-05,0,negative,1.69E-05,5.89E-05,2.88E-05,2.98E-08,2.06E-07,1.50E-06,7.13E-07,2.11E-05,0.000693836,0.999140667,5.01E-06,3.24E-05,5.87E-08
5182,natural_language_inference28,112,RN h N h ( originally initialized to the identity matrix ) .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,13,1,0,,8.18E-06,0,negative,4.36E-05,3.30E-05,1.45E-05,3.47E-08,2.20E-07,1.32E-06,1.07E-06,3.15E-05,0.000150452,0.999610136,4.29E-06,0.000109884,6.26E-08
5183,natural_language_inference28,113,Here ?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,14,1,0,,1.43E-06,0,negative,2.34E-06,1.42E-05,3.31E-07,1.68E-07,9.48E-08,5.08E-06,7.73E-07,0.000106965,0.000168697,0.999685585,9.18E-06,6.51E-06,1.15E-07
5184,natural_language_inference28,114,is a non-negative integer that is a hyper - parameter of the model .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,15,1,0,,3.23E-05,0,negative,1.35E-05,0.000357211,2.08E-06,6.50E-07,6.21E-07,3.89E-05,4.24E-06,0.004085763,0.002078985,0.993385332,1.39E-05,1.80E-05,8.12E-07
5185,natural_language_inference28,115,"From here , the orthogonal R t acts on the state h to produce an evolved hidden stateh .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,16,1,0,,3.74E-06,0,negative,7.22E-06,2.67E-05,4.84E-06,2.79E-08,1.15E-07,8.77E-07,2.71E-07,2.19E-05,0.000666427,0.999264888,6.26E-07,6.08E-06,3.09E-08
5186,natural_language_inference28,116,"Finally RUM obtains the new hidden state via u , just as in GRU .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,17,1,0,,2.67E-05,0,negative,4.00E-05,9.49E-05,0.000106946,2.09E-08,3.17E-07,8.82E-07,1.05E-06,1.46E-05,0.001524186,0.998150184,1.95E-06,6.49E-05,7.97E-08
5187,natural_language_inference28,117,The RUM equations are as follows,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,18,1,0,,1.48E-05,0,negative,8.02E-06,3.56E-05,7.32E-06,3.45E-08,1.52E-07,1.47E-06,6.30E-07,2.44E-05,0.00242903,0.997470484,6.90E-06,1.58E-05,1.23E-07
5188,natural_language_inference28,118,initial update gate and memory target ;,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,19,1,0,,0.000152408,0,negative,1.83E-05,0.0004665,1.21E-05,4.26E-07,1.11E-06,2.24E-05,8.78E-06,0.00140837,0.005693327,0.992256324,3.90E-05,7.18E-05,1.55E-06
5189,natural_language_inference28,119,?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,20,1,0,,8.43E-07,0,negative,1.76E-06,2.94E-06,2.97E-07,2.25E-08,4.17E-08,1.27E-06,3.40E-07,1.83E-05,4.37E-05,0.999922486,1.17E-06,7.62E-06,2.85E-08
5190,natural_language_inference28,120,activation of the update gate ;,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,21,1,0,,1.30E-05,0,negative,0.000361309,0.000123412,0.000147064,1.84E-07,1.78E-06,2.88E-06,2.42E-06,3.39E-05,0.001956147,0.997020452,6.62E-06,0.0003434,4.26E-07
5191,natural_language_inference28,121,We have introduced time subscripts to demonstrate the recurrence relations .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,22,1,0,,1.84E-06,0,negative,3.52E-06,1.99E-05,7.72E-07,6.87E-08,2.29E-07,3.97E-06,4.75E-07,7.67E-05,0.000317105,0.999570473,3.97E-07,6.32E-06,4.98E-08
5192,natural_language_inference28,122,The kernels have dimensions given by,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,23,1,0,,6.71E-05,0,negative,3.18E-05,0.000568521,4.30E-06,1.20E-06,2.40E-06,0.000276515,3.34E-05,0.012250337,0.002501569,0.984261361,7.33E-06,5.80E-05,3.22E-06
5193,natural_language_inference28,123,The norm ?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,24,1,0,,0.001519959,0,negative,1.12E-06,1.36E-05,6.90E-07,4.32E-08,6.42E-08,3.83E-06,1.13E-06,9.96E-05,0.000189758,0.999675145,5.84E-06,9.03E-06,1.22E-07
5194,natural_language_inference28,124,is a scalar hyper - parameter of the RUM model .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,25,1,0,,7.26E-05,0,negative,1.66E-05,0.000337362,2.42E-06,8.04E-07,1.05E-06,3.86E-05,6.21E-06,0.004199803,0.002074203,0.993292539,3.95E-06,2.52E-05,1.23E-06
5195,natural_language_inference28,125,"The orthogonal matrix R ( ? t , ? ) conceptually takes the place of a kernel acting on the hidden state in GRU .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,26,1,0,,4.66E-06,0,negative,1.03E-05,4.01E-05,4.10E-06,8.11E-08,4.17E-07,2.66E-06,5.94E-07,5.55E-05,0.000552747,0.999322788,4.67E-07,1.01E-05,8.12E-08
5196,natural_language_inference28,126,"This is the most efficient place to introduce an orthogonal operation , as the Gated Orthogonal Recurrent Unit ( GORU , ) experiments suggest .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,27,1,0,,1.74E-05,0,negative,4.83E-05,4.00E-05,3.67E-05,1.24E-07,8.56E-07,1.66E-06,1.25E-06,1.66E-05,0.000353175,0.999415036,2.47E-06,8.36E-05,1.73E-07
5197,natural_language_inference28,127,"The difference with the GORU cell is that GORU parameterizes and learns the kernel as an orthogonal matrix , while RUM does not parameterize the rotation R. Instead , RUM learns ? , which together with x , determines R . The orthogonal matrix keeps the norm of the vectors , so we experiment with a ReLU activation instead of the conventional tanh in gated mechanisms .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,28,1,0,,1.81E-05,0,negative,1.64E-05,0.000483485,8.08E-05,7.47E-08,1.14E-06,3.68E-06,2.01E-06,8.33E-05,0.002773629,0.996517947,3.31E-06,3.40E-05,2.59E-07
5198,natural_language_inference28,128,"Even though R is an orthogonal element of RUM , the norm of ht is not stable because of the ReLU activation .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,29,1,0,,9.21E-06,0,negative,0.001973451,5.62E-05,1.32E-05,1.38E-07,1.69E-06,1.76E-06,1.03E-05,4.42E-05,5.68E-05,0.98956386,5.45E-06,0.008272471,3.88E-07
5199,natural_language_inference28,129,"Therefore , we suggest normalizing the hidden state ht to a have norm ?.",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,30,1,0,,1.48E-05,0,negative,4.26E-05,3.09E-05,2.07E-06,1.23E-07,5.18E-07,3.05E-06,7.34E-07,0.000117149,0.000150831,0.999618856,2.79E-07,3.28E-05,1.23E-07
5200,natural_language_inference28,130,"We call this technique time normalization as we usually feed mini-batches to the RNN during learning that have the shape ( N b , NT ) , where Nb is the size of the batch and NT is the length of the sequence that we feed in .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,31,1,0,,9.44E-06,0,negative,4.40E-05,0.000463477,9.33E-05,1.62E-07,3.10E-06,2.45E-06,2.71E-06,4.66E-05,0.001115401,0.998080385,5.25E-06,0.000142784,3.99E-07
5201,natural_language_inference28,131,Time normalization happens along the sequence dimension as opposed to the batch dimension in batch normalization .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,32,1,0,,0.000105052,0,negative,6.25E-05,0.000832888,8.05E-05,1.01E-06,5.38E-06,9.39E-06,4.47E-06,0.000199454,0.0040586,0.994663811,5.38E-06,7.52E-05,1.42E-06
5202,natural_language_inference28,132,Choosing appropriate ? for the RUM model stabilizes learning and ensures the eigenvalues of the kernels are bounded from above .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,33,1,0,,0.001504935,0,negative,0.00057517,0.000593756,8.45E-06,2.27E-06,8.86E-06,2.29E-05,1.91E-05,0.001189103,0.001002859,0.995147713,3.04E-06,0.001422779,3.97E-06
5203,natural_language_inference28,133,This in turn means that the smaller ?,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,34,1,0,,3.52E-07,0,negative,6.10E-05,9.91E-06,1.98E-06,1.90E-08,1.78E-07,6.30E-07,4.10E-07,1.37E-05,7.02E-05,0.999774678,1.49E-07,6.71E-05,2.51E-08
5204,natural_language_inference28,134,"is , the more we reduce the effect of exploding gradients .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,35,1,0,,3.34E-05,0,negative,0.001059868,2.19E-05,9.88E-06,9.93E-07,2.35E-06,4.30E-06,1.97E-06,3.66E-05,9.29E-05,0.998474295,2.43E-07,0.0002944,1.95E-07
5205,natural_language_inference28,135,"Finally , even though RUM uses an update gate , it is not a standard gated mechanism , as it does not have a reset gate .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,36,1,0,,1.45E-06,0,negative,5.50E-05,3.63E-05,5.52E-05,9.04E-08,9.08E-07,1.04E-06,1.13E-06,1.10E-05,0.000202672,0.999525337,2.61E-06,0.000108428,2.31E-07
5206,natural_language_inference28,136,Instead we suggest utilizing additional memory via the target vector ? .,THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,37,1,0,,3.96E-05,0,negative,1.41E-05,1.86E-05,1.42E-06,8.06E-07,6.13E-07,5.86E-06,9.68E-07,8.71E-05,0.000163866,0.999684032,9.77E-07,2.13E-05,3.34E-07
5207,natural_language_inference28,137,"By feeding inputs to RUM , ?",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,38,1,0,,4.89E-06,0,negative,3.94E-05,1.55E-05,1.16E-05,6.74E-08,6.92E-07,1.03E-06,8.50E-07,1.75E-05,0.000145168,0.999693234,1.84E-07,7.46E-05,1.22E-07
5208,natural_language_inference28,138,"adapts to encode rotations , which align the hidden states in desired locations in RN h , without changing the norm of h .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,39,1,0,,3.75E-05,0,negative,7.08E-05,0.000380881,0.00032983,2.74E-07,4.51E-06,4.55E-06,5.61E-06,9.14E-05,0.00352611,0.995438133,2.36E-06,0.000144174,1.31E-06
5209,natural_language_inference28,139,"We believe that the unit of memory R t gives advantage to RUM over other gated mechanisms , such as LSTM and GRU .",THE RUM ARCHITECTURE,THE RUM ARCHITECTURE,natural_language_inference,28,40,1,0,,0.000316404,0,negative,0.000475163,2.32E-05,1.29E-05,7.81E-08,9.73E-07,2.42E-06,7.33E-06,3.06E-05,0.000109749,0.997353318,4.23E-07,0.001983504,3.46E-07
5210,natural_language_inference28,140,EXPERIMENTS,,,natural_language_inference,28,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
5211,natural_language_inference28,141,"Firstly , we test RUM 's memorization capacity on the Copying Memory Task .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,1,1,0,,0.03894587,0,negative,0.014370745,0.007237814,0.007935149,0.000284383,0.000898434,0.005142224,0.01503454,0.048253841,0.000456313,0.870403179,0.001202387,0.027064963,0.001716028
5212,natural_language_inference28,142,"Secondly , we signify the superiority of RUM by obtaining a state - of - the - art result in the Associative Recall Task .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,2,1,0,,0.02474018,0,negative,0.021484046,0.001210762,0.005992346,7.18E-05,0.000206712,0.001578092,0.006621352,0.010848555,0.000138882,0.819872477,0.000353457,0.130882588,0.000738972
5213,natural_language_inference28,143,"Thirdly , we show that even without external memory , RUM achieves comparable to state - of - the - art results in the bAbI Question Answering data set .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,3,1,0,,0.774523412,1,results,0.002997606,4.89E-05,0.00045705,8.63E-06,1.22E-05,0.000344359,0.021890767,0.003761168,3.91E-06,0.065895942,0.000353239,0.903250037,0.000976145
5214,natural_language_inference28,144,"Finally , we utilize RUM 's rotational memory to reach 1.189 BPC in the Character Level Penn Treebank .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,4,1,0,,0.531396847,1,hyperparameters,0.015748002,0.004538395,0.019428905,0.000306768,0.000598534,0.058747229,0.139745855,0.384623067,0.000609215,0.290705592,0.000398782,0.072044863,0.012504794
5215,natural_language_inference28,145,"We experiment with ? = 0 RUM and ? = 1 RUM , the latter model corresponding to tuning in the rotational associative memory .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,5,1,0,,0.029688132,0,hyperparameters,0.000568094,0.000480556,0.001134011,1.68E-05,1.69E-05,0.068772096,0.005314716,0.576303856,0.000251804,0.345851403,8.16E-05,0.000877557,0.000330664
5216,natural_language_inference28,146,COPYING MEMORY TASK,EXPERIMENTS,,natural_language_inference,28,6,1,1,experiments,0.649990867,1,negative,0.000903011,0.00020978,0.003780558,7.88E-05,3.76E-05,0.004569524,0.219659807,0.034223987,6.73E-05,0.511708256,0.058306834,0.156159847,0.010294682
5217,natural_language_inference28,147,"A standard way to evaluate the memory capacity of a neural network is to test its performance in the Copying Memory Task , ) .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,7,1,0,,0.028225679,0,negative,0.000248576,1.92E-06,2.25E-05,4.48E-06,6.40E-07,8.15E-05,0.003742245,5.15E-05,1.01E-05,0.990005783,0.000898277,0.000880665,0.004051852
5218,natural_language_inference28,148,We follow the setup in .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,8,1,0,,0.025318832,0,negative,0.000443768,3.99E-06,0.000112695,9.00E-05,1.43E-05,0.000244228,0.00212341,5.48E-05,2.76E-05,0.994765834,7.01E-06,0.000397442,0.001714885
5219,natural_language_inference28,149,The objective of the RNN is to remember ( copy ) information received T time steps earlier ( see section A for details about the data ) .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,9,1,0,,0.019222735,0,negative,0.000355438,2.86E-05,0.000523224,4.25E-06,3.45E-06,0.000111382,0.001677939,7.87E-05,0.001261165,0.993175682,0.000151373,0.000250384,0.002378461
5220,natural_language_inference28,150,Our results in this task demonstrate :,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,10,1,0,,0.123603532,0,negative,0.016201261,2.95E-06,0.000109773,1.59E-06,4.00E-06,5.09E-05,0.063703636,2.10E-05,5.04E-06,0.822509073,3.88E-05,0.091415325,0.005936705
5221,natural_language_inference28,151,"1 . RUM utilizes a different representation of memory that outperforms those of LSTM and GRU ; 2 . RUM solves the task completely , despite its update gate , which does not allow all of the information encoded in the hidden stay to pass through .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,11,1,1,experiments,0.960963027,1,negative,0.054689534,7.04E-06,0.00032464,4.60E-06,4.81E-06,0.000109394,0.166314763,4.72E-05,8.22E-06,0.481520534,4.25E-05,0.288707535,0.008219187
5222,natural_language_inference28,152,The only other gated RNN model successful at copying is GORU .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,12,1,0,,0.318815946,0,negative,0.00042733,1.12E-06,0.000915919,1.25E-06,1.51E-06,0.000132534,0.022778034,3.04E-05,9.25E-06,0.970093911,2.08E-05,0.001619752,0.003968138
5223,natural_language_inference28,153,"reveals that LSTM and GRU hit a predictable baseline , which is equivalent to random guessing .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,13,1,0,,0.680215687,1,negative,0.04506223,1.96E-06,0.000278297,4.85E-07,3.41E-06,2.11E-05,0.039222889,8.74E-06,3.30E-06,0.819416805,2.56E-06,0.095525083,0.000453101
5224,natural_language_inference28,154,"RUM falls bellow the baseline , and subsequently learns the task by achieving zero loss after a few thousands iterations .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,14,1,0,,0.316988166,0,negative,0.110677886,4.39E-06,0.000427445,3.45E-06,5.98E-06,0.000122629,0.344812049,6.22E-05,6.70E-06,0.368710855,6.08E-06,0.159864874,0.015295487
5225,natural_language_inference28,155,With the help of figure 2 we will explain how the additional hyper - parameters for RUM affect its training .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,15,1,0,,0.001948345,0,negative,0.000439785,5.08E-07,9.50E-06,4.92E-07,7.84E-07,9.58E-06,0.000182546,7.10E-06,6.97E-06,0.999141265,3.33E-07,0.000169492,3.17E-05
5226,natural_language_inference28,156,We observe that when we remove the normalization (? = N / A ) then RUM learns more quickly than the case of requiring a norm ? = 1.0 .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,16,1,0,,0.357020078,0,negative,0.064854502,3.56E-06,5.05E-05,3.62E-06,4.16E-06,0.000197537,0.176875195,0.000154218,1.13E-05,0.696024354,9.29E-06,0.053447121,0.008364596
5227,natural_language_inference28,157,"At the same time , though , the training entails more fluctuations .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,17,1,0,,0.015538873,0,negative,0.008503265,5.94E-07,1.68E-05,3.84E-07,1.14E-06,7.83E-06,0.001114768,5.28E-06,2.82E-06,0.986535968,9.26E-07,0.003695338,0.00011488
5228,natural_language_inference28,158,Hence we believe that choosing a finite ?,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,18,1,0,,0.008229006,0,negative,0.000328124,2.99E-07,4.09E-06,5.34E-07,3.98E-07,1.99E-05,0.000625391,1.45E-05,2.63E-06,0.99855987,1.41E-06,0.000349292,9.35E-05
5229,natural_language_inference28,159,to normalize the hidden state is an important tool for stable learning .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,19,1,0,,0.180878038,0,negative,0.004742695,3.45E-06,6.39E-05,9.28E-06,4.49E-06,6.81E-05,0.004172252,3.67E-05,1.41E-05,0.981163622,5.03E-05,0.005686116,0.003984926
5230,natural_language_inference28,160,"Moreover , it is necessary for the NLP task in this paper ( see section 4.4 ) : for our character level predictions we use large hidden sizes , which if left unnormalized , can make the cross entropy loss blowup .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,20,1,0,,0.03783357,0,negative,0.007021113,2.70E-06,4.14E-05,3.12E-06,4.22E-06,3.93E-05,0.002133188,1.85E-05,8.56E-06,0.986300266,2.93E-06,0.003945078,0.00047965
5231,natural_language_inference28,161,We also observe the benefits of tuning in the associative rotational memory .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,21,1,0,,0.889447786,1,negative,0.124658586,3.20E-06,0.000112518,9.52E-07,3.69E-06,2.78E-05,0.022780044,1.45E-05,1.45E-05,0.80276782,1.68E-06,0.048845481,0.000769217
5232,natural_language_inference28,162,"Indeed , a ? = 1 RUM has a smaller hidden size , N h = 100 , yet it learns much more quickly than a ? = 0 RUM .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,22,1,0,,0.02176748,0,negative,0.01619283,1.32E-06,5.33E-05,1.37E-06,2.70E-06,0.000116682,0.130322782,6.70E-05,3.27E-06,0.803177796,3.52E-06,0.046133474,0.003923967
5233,natural_language_inference28,163,It is possible that the accumulation of phase via ? = 1 to enable faster long - term dependence learning than the ? = 0 case .,EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,23,1,0,,0.016245804,0,negative,0.001952529,1.12E-06,2.45E-05,5.79E-07,8.93E-07,4.40E-05,0.002764945,3.34E-05,1.61E-05,0.994166624,9.20E-07,0.000678455,0.000315902
5234,natural_language_inference28,164,"Either way , both models overcome the vanishing / exploding gradients , and eventually learn the task completely .",EXPERIMENTS,COPYING MEMORY TASK,natural_language_inference,28,24,1,0,,0.444265763,0,negative,0.000891292,1.38E-06,1.68E-05,4.38E-07,5.57E-07,1.37E-05,0.00122992,1.66E-05,2.86E-05,0.99689836,6.58E-07,0.000716833,0.000184811
5235,natural_language_inference28,165,ASSOCIATIVE RECALL TASK,EXPERIMENTS,,natural_language_inference,28,25,1,1,experiments,0.584508682,1,negative,0.0008775,0.00014553,0.00250593,4.18E-05,5.63E-05,0.003577743,0.126483923,0.02707943,6.59E-05,0.587441361,0.00174326,0.240065974,0.009915312
5236,natural_language_inference28,166,Another important synthetic task to test the memory ability of recurrent neural network is the Associative Recall .,EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,26,1,0,,0.042478698,0,negative,0.000220046,7.31E-07,1.72E-05,1.28E-06,5.22E-07,2.74E-05,0.002392197,1.35E-05,3.72E-06,0.986950665,0.000103679,0.000887988,0.009381027
5237,natural_language_inference28,167,This task requires RNN to remember the whole sequence of the data and perform extra logic on the sequence .,EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,27,1,0,,0.021996117,0,negative,0.000505924,1.33E-06,4.44E-05,5.93E-06,2.99E-06,3.67E-05,0.000598829,1.25E-05,2.11E-05,0.989627343,1.33E-05,0.000446534,0.008683179
5238,natural_language_inference28,168,We follow the same setting as in and and modify the original task so that it can test for longer sequences .,EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,28,1,0,,0.003728417,0,negative,0.000561111,2.58E-06,0.000227751,4.55E-07,4.30E-06,1.69E-05,0.001293464,7.35E-06,1.13E-05,0.996987343,2.52E-07,0.000407612,0.000479553
5239,natural_language_inference28,169,"In detail , the RNN is fed into a sequence of characters , e.g. "" a1s2d3f4 g5 ? ? d "" .",EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,29,1,0,,0.009517542,0,negative,0.000199928,2.32E-06,0.000307008,1.55E-07,1.68E-06,1.16E-05,0.000657879,7.78E-06,9.25E-05,0.998247718,2.38E-07,0.000128334,0.000342801
5240,natural_language_inference28,170,"The RNN is supposed to output the character based on the "" key "" which is located at the end of the sequence .",EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,30,1,0,,0.033714682,0,negative,0.000137374,3.86E-06,0.000233583,3.88E-07,1.10E-06,1.96E-05,0.00041772,2.16E-05,0.000518735,0.997461482,8.38E-07,4.34E-05,0.001140257
5241,natural_language_inference28,171,"The RNN needs to look back into the sequence and find the "" key "" and then to retrieve the next character .",EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,31,1,0,,0.009342445,0,negative,0.000324159,1.91E-06,0.000190329,2.49E-07,1.28E-06,9.72E-06,0.000433744,5.68E-06,9.51E-05,0.997872604,8.05E-07,0.000140344,0.000924098
5242,natural_language_inference28,172,"In this example , the correct answer is "" 3 "" .",EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,32,1,0,,0.006350491,0,negative,3.01E-05,4.86E-08,2.51E-06,4.44E-08,3.00E-07,5.73E-06,0.0002741,3.30E-06,4.75E-07,0.999373051,7.90E-08,0.000120816,0.000189454
5243,natural_language_inference28,173,See section B for further details about the data .,EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,33,1,0,,0.000671761,0,negative,3.53E-05,2.94E-08,9.91E-07,2.05E-07,7.50E-07,3.91E-06,6.16E-05,1.99E-06,1.59E-07,0.999818931,6.26E-09,3.68E-05,3.93E-05
5244,natural_language_inference28,174,"In this experiment , we compare RUM to an LSTM , , a Fast - weight RNN ) and a recent successful RNN WeiNet ) .",EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,34,1,0,,0.218678776,0,negative,0.002104764,1.89E-05,0.003726629,2.42E-07,5.11E-06,2.78E-05,0.045207509,8.95E-06,3.61E-05,0.940766364,1.59E-06,0.005392098,0.002703897
5245,natural_language_inference28,175,All the models have the same hidden state N h = 50 for different lengths T .,EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,35,1,1,experiments,0.666301591,1,negative,0.001163797,1.13E-05,5.37E-05,5.33E-06,5.23E-06,0.001861322,0.228361746,0.003356655,5.09E-05,0.705829187,1.16E-06,0.000616217,0.058683548
5246,natural_language_inference28,176,We use a batch size 128 .,EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,36,1,1,experiments,0.985340372,1,tasks,0.001377276,2.14E-05,9.49E-05,4.21E-05,1.64E-05,0.005876463,0.344542181,0.006779459,6.00E-05,0.232885911,1.07E-06,0.000567183,0.407735711
5247,natural_language_inference28,177,The optimizer is RMSProp with a learning rate 0.001 .,EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,37,1,1,experiments,0.991043748,1,tasks,0.001285417,2.48E-05,0.000189568,0.00013212,2.13E-05,0.010220407,0.293945418,0.009942302,0.000100383,0.176022572,9.62E-07,0.00033929,0.507775419
5248,natural_language_inference28,178,"We find that LSTM fails to learn the task , because of its lack of sufficient memory capacity .",EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,38,1,1,experiments,0.403984656,0,negative,0.038036384,1.09E-06,1.81E-05,9.94E-07,5.81E-06,2.52E-05,0.024863613,1.58E-05,1.33E-06,0.90100857,2.25E-07,0.030049805,0.005972998
5249,natural_language_inference28,179,"NTM and Fast - weight RNN fail longer tasks , which means they can not learn to manipulate their memory efficiently .",EXPERIMENTS,ASSOCIATIVE RECALL TASK,natural_language_inference,28,39,1,1,experiments,0.750217535,1,negative,0.001535805,1.64E-07,3.39E-05,1.58E-07,5.83E-07,8.96E-06,0.004044409,2.73E-06,4.12E-07,0.984160707,9.14E-07,0.007046566,0.003164694
5250,natural_language_inference28,180,QUESTION ANSWERING,EXPERIMENTS,,natural_language_inference,28,40,1,1,experiments,0.953802068,1,experiments,0.002124028,9.21E-05,0.000733313,0.004482002,0.002415237,0.004087199,0.548827311,0.009793978,1.05E-05,0.057927867,0.000218598,0.225105297,0.14418253
5251,natural_language_inference28,181,Question answering remains one of the most important applicable tasks in NLP .,EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,41,1,0,,0.80994958,1,experiments,0.000556316,2.08E-06,1.85E-05,7.13E-06,1.29E-05,0.000375049,0.563533946,3.56E-06,1.74E-06,0.293414777,8.03E-05,0.000305612,0.141688121
5252,natural_language_inference28,182,Almost all stateof - the - art performance is achieved by the means of attention mechanisms .,EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,42,1,0,,0.63687946,1,negative,0.000302254,7.59E-07,1.22E-05,1.34E-06,4.00E-06,0.000511864,0.30191714,5.42E-06,3.64E-06,0.682173346,3.75E-06,0.000112321,0.01495193
5253,natural_language_inference28,183,Few works have been done to improve the performance by developing stronger RNN .,EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,43,1,0,,0.029242991,0,negative,0.000416225,7.97E-07,9.81E-06,1.20E-06,5.72E-06,0.000350861,0.156816494,5.14E-06,1.72E-06,0.833875744,2.98E-06,8.51E-05,0.008428248
5254,natural_language_inference28,184,"Here , we tested RUM on the b AbI Question Answering data set ) to demonstrate its ability to memorize and reason without any attention .",EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,44,1,0,,0.531664986,1,experiments,0.000927951,2.03E-05,0.000169814,2.31E-07,2.35E-05,0.000110771,0.865422293,1.85E-06,1.16E-05,0.129332654,3.92E-07,0.000252801,0.003725803
5255,natural_language_inference28,185,"In this task , we train 20 sub - tasks jointly for each model .",EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,45,1,0,,0.601857492,1,experiments,0.000276702,4.88E-06,9.20E-06,4.61E-07,2.43E-05,0.000793906,0.89578178,2.87E-05,5.96E-06,0.092244787,2.34E-08,1.92E-05,0.010810125
5256,natural_language_inference28,186,See section C for detailed experimental settings and results on each sub-task .,EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,46,1,0,,0.001283707,0,negative,5.22E-05,1.85E-07,1.33E-06,4.95E-07,2.31E-05,9.29E-05,0.011695525,1.05E-06,2.14E-07,0.988013487,3.30E-09,7.25E-06,0.000112256
5257,natural_language_inference28,187,"We compare our model with several baselines : a simple LSTM , an End - to - end Memory Network ) and a GORU .",EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,47,1,1,experiments,0.896408161,1,experiments,0.000342115,9.03E-06,0.000123257,6.70E-08,8.72E-06,0.00017423,0.744570134,2.61E-06,1.08E-05,0.253727863,3.10E-08,6.51E-05,0.000966036
5258,natural_language_inference28,188,"We find that RUM outperforms significantly LSTM and GORU and achieves competitive result with those of MemN2N , which has an attention mechanism .",EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,48,1,1,experiments,0.974305575,1,experiments,0.001655234,1.39E-07,8.79E-07,5.79E-08,4.79E-07,1.96E-05,0.986694569,6.04E-07,6.94E-08,0.007731118,9.78E-09,0.00064534,0.003251861
5259,natural_language_inference28,189,We summarize the results in .,EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,49,1,0,,0.005798725,0,negative,0.000263608,1.48E-07,2.51E-06,6.81E-07,1.74E-05,8.21E-05,0.018369135,4.50E-07,2.97E-07,0.980875667,1.65E-08,1.91E-05,0.000368908
5260,natural_language_inference28,190,"We emphasize that for some sub - tasks in the table , which require large memory , RUM outperforms models with attention mechanisms ( Mem N2N ) .",EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,50,1,0,,0.969677449,1,experiments,0.003022162,1.59E-07,1.67E-06,7.84E-08,1.26E-06,4.06E-05,0.965773805,7.84E-07,8.51E-08,0.02942577,9.72E-09,0.000453819,0.00127976
5261,natural_language_inference28,191,Model,EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,51,1,0,,0.018873684,0,negative,0.000449401,5.70E-06,0.000312658,8.26E-07,9.92E-06,0.000457588,0.403569683,7.94E-06,0.000306802,0.564694435,8.67E-07,7.63E-05,0.03010783
5262,natural_language_inference28,192,Test Accuracy ( % ) LSTM ) 49 GORU ) 60 MemN2N ) 86 RUM ( ours ) 73.2,EXPERIMENTS,QUESTION ANSWERING,natural_language_inference,28,52,1,0,,0.143423783,0,experiments,0.000172607,7.05E-08,5.84E-06,9.43E-08,2.93E-06,0.000118971,0.762971824,1.29E-06,1.09E-07,0.230187245,1.67E-08,0.000184938,0.006354065
5263,natural_language_inference28,193,CHARACTER LEVEL LANGUAGE MODELING,,,natural_language_inference,28,0,1,1,experiments,0.919729562,1,research-problem,0.002339443,0.000264501,0.000701753,0.001652985,0.000337793,0.001097951,0.273850119,0.001752839,2.47E-05,0.168353113,0.436597557,0.10814569,0.004881587
5264,natural_language_inference28,194,The rotational unit of memory is a natural architecture that can learn long - term structure in data while avoiding significant overfitting .,CHARACTER LEVEL LANGUAGE MODELING,CHARACTER LEVEL LANGUAGE MODELING,natural_language_inference,28,1,1,0,,0.015283991,0,negative,0.001139751,5.64E-05,0.005952847,1.21E-05,2.36E-05,0.002078287,0.435637207,4.75E-05,0.003713687,0.550016809,8.43E-05,0.000223061,0.00101454
5265,natural_language_inference28,195,"Perhaps , the best way to demonstrate this unique property , among other RNN models , is to test RUM on real world character level NLP tasks .",CHARACTER LEVEL LANGUAGE MODELING,CHARACTER LEVEL LANGUAGE MODELING,natural_language_inference,28,2,1,0,,0.000925488,0,negative,3.34E-05,3.09E-07,1.59E-06,1.27E-07,1.62E-07,7.11E-05,0.031350246,3.81E-06,2.36E-06,0.968397843,3.22E-05,8.44E-05,2.25E-05
5266,natural_language_inference28,196,PENN TREEBANK CORPUS DATA SET,CHARACTER LEVEL LANGUAGE MODELING,,natural_language_inference,28,3,1,1,experiments,0.441424475,0,experiments,5.61E-05,2.83E-07,1.29E-05,2.68E-07,2.04E-06,5.25E-05,0.929885731,1.41E-06,2.56E-07,0.067899794,1.45E-05,0.000799789,0.001274372
5267,natural_language_inference28,197,The corpus is a collection of articles in The Wall Street Journal ) .,CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,4,1,0,,0.005393282,0,negative,0.000156922,1.17E-07,5.79E-05,3.76E-05,1.15E-05,4.44E-05,0.000941434,2.57E-06,1.05E-06,0.997689543,7.29E-07,0.000497981,0.000558301
5268,natural_language_inference28,198,The text is in English and its vocabulary consists of 10000 words .,CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,5,1,0,,0.137212594,0,negative,8.57E-05,1.07E-06,5.58E-05,2.34E-06,1.44E-06,0.000274983,0.013768136,0.000175189,7.05E-06,0.983466427,1.65E-06,0.000697181,0.001463041
5269,natural_language_inference28,199,"We split the data into train , validation and test sets according to .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,6,1,0,,0.003851538,0,negative,5.86E-05,2.72E-07,1.12E-05,1.84E-07,1.19E-07,4.28E-05,0.002685134,7.08E-05,3.04E-06,0.996790605,2.78E-07,0.000279249,5.77E-05
5270,natural_language_inference28,200,We train by feeding mini-batches of size Nb that consist of sequences of T consecutive characters .,CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,7,1,0,,0.047218137,0,negative,0.000300695,6.98E-06,6.36E-05,3.07E-06,5.64E-07,0.000700582,0.044341704,0.002408026,8.90E-05,0.949136959,5.80E-06,0.000817848,0.002125105
5271,natural_language_inference28,201,"We incorporate RUM into the state - of - the - art high - level model : Fast - Slow RNN ( FS - RNN , ) .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,8,1,0,,0.905854582,1,negative,0.001510149,4.99E-05,0.097745802,1.55E-06,1.37E-06,0.000115071,0.067240269,7.07E-05,0.000823109,0.823236245,4.65E-05,0.006720153,0.002439222
5272,natural_language_inference28,202,"The FS - RNN -k architecture consists of two hierarchical layers : one of them is a "" fast "" layer that connects k RNN cells F 1 , . . .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,9,1,0,,0.005283289,0,negative,0.001294532,1.69E-05,0.032038839,3.13E-06,1.85E-06,8.80E-05,0.009048915,3.83E-05,0.001447337,0.950638745,2.69E-05,0.003040921,0.00231561
5273,natural_language_inference28,203,"F kin series ; the other is a "" slow "" layer that consists of a single RNN cell S.",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,10,1,0,,0.01001349,0,negative,0.00079888,9.83E-07,0.002826261,3.31E-07,3.95E-07,8.63E-06,0.001412281,5.42E-06,4.03E-05,0.992455956,1.16E-06,0.002329919,0.000119514
5274,natural_language_inference28,204,The organization is roughly as follows :,CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,11,1,0,,0.000618237,0,negative,9.36E-05,2.92E-07,0.000163599,3.17E-07,2.57E-07,6.33E-06,0.000483617,2.91E-06,1.22E-05,0.998404044,8.53E-07,0.000669307,0.000162659
5275,natural_language_inference28,205,F 1 receives the input from the mini-batch and feeds it s state into S ; S feeds it s state into F 2 ; the output of F k is the probability distribution of the predicted character .,CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,12,1,0,,0.014899344,0,negative,0.000118958,2.60E-07,0.000585979,3.04E-08,4.65E-08,2.27E-06,0.000399901,1.86E-06,1.49E-05,0.998182784,3.01E-07,0.000672629,2.01E-05
5276,natural_language_inference28,206,"outlines the performance of some FS - RNN models along with other results in the PTB data set , in which we present the improved test BPC .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,13,1,0,,0.079081886,0,negative,0.000386677,6.99E-08,7.34E-05,2.33E-08,5.55E-08,9.48E-07,0.001418332,6.29E-07,2.18E-07,0.965821646,3.60E-07,0.032268439,2.92E-05
5277,natural_language_inference28,207,"achieve their record with FS - LSTM - 2 , by setting F 1 , 2 and S to LSTM .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,14,1,0,,0.008301049,0,negative,4.98E-05,5.09E-08,4.46E-05,6.48E-08,3.72E-08,6.26E-06,0.001418101,2.81E-06,4.78E-07,0.996097963,9.90E-07,0.002250217,0.000128572
5278,natural_language_inference28,208,"The authors in the same paper suggest that the "" slow "" cell has the function of capturing long - term dependencies from the data .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,15,1,0,,0.001082013,0,negative,8.21E-05,1.39E-07,3.16E-05,1.74E-07,4.48E-08,7.33E-06,0.00049337,6.47E-06,5.26E-06,0.998762081,1.19E-06,0.000475005,0.000135199
5279,natural_language_inference28,209,"Hence , it is natural to set S to be a RUM , given its memorization advantages .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,16,1,0,,0.000803111,0,negative,4.60E-05,1.83E-07,1.23E-05,1.61E-08,2.12E-08,1.43E-06,0.000143865,3.36E-06,4.00E-06,0.999482389,1.14E-07,0.000301768,4.50E-06
5280,natural_language_inference28,210,"In particular , we experiment with FS - RUM - 2 , for which S is a RUM and F 1 , 2 are LSTM .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,17,1,0,,0.274955284,0,negative,0.00029576,1.39E-06,0.000488493,1.96E-07,4.41E-07,2.35E-05,0.033175368,1.62E-05,2.92E-06,0.956344006,4.30E-07,0.009385831,0.000265501
5281,natural_language_inference28,211,"Additionally , we test the performance of a simple RUM and a two - layer RUM .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,18,1,0,,0.457911589,0,negative,0.000513347,1.34E-06,9.60E-05,8.11E-08,1.92E-07,5.90E-06,0.007080047,7.73E-06,3.80E-06,0.982857836,2.63E-07,0.009357939,7.55E-05
5282,natural_language_inference28,212,"As the models are prone to overfitting , for each of our models we follow the experimental settings for regularization in , presented in section D .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,19,1,0,,0.001140084,0,negative,6.23E-05,2.09E-07,1.10E-05,7.27E-08,6.88E-08,9.15E-06,0.001422061,1.50E-05,1.19E-06,0.998080908,4.00E-08,0.000381103,1.69E-05
5283,natural_language_inference28,213,Those techniques work particularly well in combination with the rotational structure of RUM .,CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,20,1,0,,0.020942209,0,negative,0.001472884,2.01E-07,5.92E-05,6.22E-07,2.73E-07,8.81E-06,0.002343341,3.64E-06,1.61E-06,0.970989477,6.73E-07,0.024813848,0.000305396
5284,natural_language_inference28,214,"More specifically , FS - RUM - 2 needs more than 350 epochs to converge by following a suitable learning rate pattern ( see in the appendix ) .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,21,1,0,,0.396927372,0,negative,0.003295305,1.86E-06,5.51E-05,2.22E-06,1.66E-06,0.000121594,0.111235923,0.000162967,6.37E-06,0.850164533,1.58E-06,0.0237329,0.011218027
5285,natural_language_inference28,215,"FS - RUM - 2 generalizes better than other gated models , such as GRU and LSTM , because it learns efficient patterns for activation in its kernels .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,22,1,1,experiments,0.968120463,1,results,0.003323399,9.06E-08,2.15E-05,1.66E-07,6.52E-08,6.17E-06,0.081935327,6.36E-06,1.00E-07,0.152553613,3.24E-07,0.760401064,0.001751818
5286,natural_language_inference28,216,"Such a skill is useful for the large Penn Treebank data set , as with its special diagonal structure , the RUM cell in FS - RUM - 2 activates almost all neurons in the hidden state .",CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,23,1,0,,0.00737665,0,negative,0.001484211,1.37E-07,2.75E-05,5.04E-08,1.03E-07,2.32E-06,0.00169543,1.70E-06,7.90E-07,0.979374045,1.42E-07,0.017359835,5.37E-05
5287,natural_language_inference28,217,We discuss this representational advantage in section 5.1 .,CHARACTER LEVEL LANGUAGE MODELING,PENN TREEBANK CORPUS DATA SET,natural_language_inference,28,24,1,0,,0.00077968,0,negative,6.32E-05,4.86E-08,5.99E-06,3.89E-08,3.56E-08,1.38E-06,0.000143135,1.58E-06,1.17E-06,0.999382393,4.73E-08,0.000384268,1.67E-05
5288,natural_language_inference28,218,DISCUSSION,CHARACTER LEVEL LANGUAGE MODELING,,natural_language_inference,28,25,1,0,,0.0002698,0,negative,0.000104764,5.83E-07,3.14E-06,4.20E-07,9.92E-07,0.00022412,0.108064415,7.03E-06,7.27E-06,0.89142267,1.42E-06,6.45E-05,9.87E-05
5289,natural_language_inference28,219,VISUAL ANALYSIS,CHARACTER LEVEL LANGUAGE MODELING,,natural_language_inference,28,26,1,0,,0.004608581,0,experiments,0.000101211,1.44E-07,4.69E-06,3.33E-07,1.14E-06,0.000292444,0.634084045,3.82E-06,7.06E-07,0.3649361,3.81E-07,0.000373573,0.000201405
5290,natural_language_inference28,220,One advantage of the Rotational Unit of Memory is that it allows the model to encode information in the phase of the hidden state .,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,27,1,0,,0.006153899,0,negative,0.00081352,7.94E-07,8.56E-05,9.49E-08,2.64E-07,5.12E-06,0.000838132,1.16E-06,2.40E-05,0.997396621,5.31E-07,0.000823956,1.02E-05
5291,natural_language_inference28,221,"In order to demonstrate the structure behind such learning , we look at the kernels that generate the target memory ?",CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,28,1,0,,0.000116542,0,negative,6.57E-06,3.40E-08,6.47E-07,2.94E-08,2.31E-08,3.17E-06,4.51E-05,9.58E-07,8.78E-07,0.99992997,6.42E-08,1.14E-05,1.14E-06
5292,natural_language_inference28,222,in the RUM model .,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,29,1,0,,0.029014339,0,negative,0.000146703,1.52E-07,2.88E-05,1.43E-08,6.61E-08,2.66E-06,0.000346111,9.84E-07,7.33E-06,0.999307794,7.26E-08,0.000155561,3.79E-06
5293,natural_language_inference28,223,is a visualization for the Recall task that demonstrates the diagonal structure of W,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,30,1,0,,0.002109332,0,negative,7.60E-05,1.43E-07,1.28E-05,3.34E-08,1.25E-07,2.90E-06,0.00024861,7.09E-07,3.02E-06,0.999461446,1.29E-07,0.000190657,3.46E-06
5294,natural_language_inference28,224,hh which generates ?,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,31,1,0,,0.002437259,0,negative,1.66E-05,2.29E-08,3.05E-06,1.66E-08,4.20E-08,2.89E-06,0.00018454,5.12E-07,5.99E-07,0.999739106,3.98E-08,5.01E-05,2.40E-06
5295,natural_language_inference28,225,( a diagonal structure is also present W,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,32,1,0,,0.000380157,0,negative,0.000173195,4.37E-08,7.06E-06,1.26E-08,5.09E-08,1.90E-06,0.000173973,4.19E-07,1.35E-06,0.99950732,3.28E-08,0.000133205,1.45E-06
5296,natural_language_inference28,226,"hh , but it is contrasted less ) .",CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,33,1,0,,0.000355089,0,negative,1.68E-05,9.93E-09,8.85E-07,2.46E-08,6.46E-08,2.05E-06,0.000137683,3.02E-07,1.28E-07,0.999750799,3.78E-08,8.88E-05,2.33E-06
5297,natural_language_inference28,227,One way to interpret the importance of the diagonal contrast is that each neuron in the hidden state plays an important role for learning since each element on the diagonal activates a distinct neuron .,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,34,1,0,,0.000371543,0,negative,0.000230541,1.27E-07,2.81E-06,8.13E-09,3.96E-08,1.41E-06,0.000233753,5.95E-07,1.96E-06,0.999380335,4.25E-08,0.000146896,1.49E-06
5298,natural_language_inference28,228,"Therefore , it seems that RUM utilizes the capacity of the hidden state almost completely .",CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,35,1,0,,0.126429367,0,negative,0.003697468,2.56E-07,9.10E-06,3.20E-08,2.03E-07,2.97E-06,0.002017517,1.02E-06,1.24E-06,0.991522271,8.99E-08,0.002739137,8.69E-06
5299,natural_language_inference28,229,"For this reason , we might consider RUM as an architecture that is close to the theoretical optimum of the representational power of RNN models .",CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,36,1,0,,0.000616193,0,negative,7.51E-05,1.04E-07,9.59E-06,1.02E-08,6.68E-08,1.98E-06,0.000190765,4.98E-07,3.57E-06,0.999606532,4.90E-08,0.000109447,2.28E-06
5300,natural_language_inference28,230,"Moreover , the diagonal structure is not task specific .",CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,37,1,0,,0.000458253,0,negative,8.72E-05,1.30E-07,5.27E-06,1.54E-08,7.35E-08,1.58E-06,0.000198908,5.08E-07,8.57E-07,0.999457371,8.53E-08,0.000245745,2.28E-06
5301,natural_language_inference28,231,"For example , in ( b ) we observe a particular W",CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,38,1,0,,4.11E-05,0,negative,2.41E-05,2.14E-08,1.53E-06,1.10E-09,1.42E-08,2.77E-07,5.01E-05,1.03E-07,4.98E-07,0.999863904,5.00E-09,5.92E-05,2.19E-07
5302,natural_language_inference28,232,-2,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,39,1,0,,0.000776999,0,negative,2.01E-05,3.55E-08,1.04E-06,2.53E-08,4.35E-08,2.37E-06,0.0001169,9.91E-07,1.01E-06,0.999824265,1.96E-08,2.99E-05,3.29E-06
5303,natural_language_inference28,233,hh for the target ? on the Penn Treebank task .,CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,40,1,0,,0.007294108,0,negative,3.22E-05,5.34E-08,3.01E-06,7.89E-09,2.59E-08,4.79E-06,0.007470287,1.23E-06,5.32E-07,0.991485879,4.06E-07,0.000953264,4.83E-05
5304,natural_language_inference28,234,"The way we interpret the meaning of the diagonal structure , combined with the off - diagonal activations , is that probably they encode grammar and vocabulary , as well as the links between various components of language .",CHARACTER LEVEL LANGUAGE MODELING,VISUAL ANALYSIS,natural_language_inference,28,41,1,0,,0.000107881,0,negative,3.12E-05,7.19E-08,2.87E-06,4.35E-09,4.02E-08,7.54E-07,5.39E-05,2.29E-07,3.74E-06,0.999874369,8.75E-09,3.21E-05,7.49E-07
5305,natural_language_inference28,235,THEORETICAL ANALYSIS,CHARACTER LEVEL LANGUAGE MODELING,,natural_language_inference,28,42,1,0,,0.00032796,0,negative,3.47E-05,1.22E-07,2.15E-06,2.60E-08,1.95E-07,8.31E-05,0.362914754,3.08E-06,2.04E-06,0.636763654,1.07E-07,8.70E-05,0.000109038
5306,natural_language_inference28,236,"It is natural to view the Rotational Unit of Memory and many other approaches using orthogonal matrices to fall into the category of phase - encoding architectures : R = R (? ) , where ?",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,43,1,0,,1.66E-05,0,negative,3.69E-06,3.92E-07,7.26E-06,1.99E-09,1.34E-08,7.69E-07,6.66E-06,1.09E-06,8.50E-06,0.999936256,9.50E-08,3.51E-05,1.23E-07
5307,natural_language_inference28,237,is a phase information matrix .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,44,1,0,,0.000311844,0,negative,1.50E-05,5.53E-07,1.92E-06,3.04E-08,1.09E-07,2.17E-06,6.06E-06,1.07E-05,9.82E-06,0.99993462,3.17E-08,1.86E-05,3.88E-07
5308,natural_language_inference28,238,"For instance , we can parameterize any orthogonal matrix according to the Efficient Unitary Neural Networks ) architecture : R = N i =0 U 0 (? i ) , where U 0 is a block diagonal matrix containing N / 2 numbers of 2 - by - 2 rotations .",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,45,1,0,,5.60E-06,0,negative,2.81E-06,2.62E-07,3.78E-06,7.40E-09,2.85E-08,2.73E-06,1.06E-05,4.66E-06,9.60E-06,0.999950293,4.38E-08,1.48E-05,3.36E-07
5309,natural_language_inference28,239,The component ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,46,1,0,,0.001494116,0,negative,2.42E-06,5.32E-08,5.52E-07,7.46E-09,1.51E-08,1.33E-06,5.26E-06,2.84E-06,1.76E-06,0.999973542,1.48E-08,1.20E-05,2.27E-07
5310,natural_language_inference28,240,i is an one - by - ( N / 2 ) parameter vector .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,47,1,0,,1.59E-05,0,negative,7.59E-06,5.37E-07,9.29E-07,1.69E-08,8.03E-08,3.54E-06,9.98E-06,2.39E-05,6.36E-06,0.999927616,8.87E-09,1.91E-05,3.26E-07
5311,natural_language_inference28,241,"Therefore , the rotational memory equation in our model can be represented as",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,48,1,0,,2.49E-05,0,negative,1.93E-05,9.05E-07,8.08E-06,9.73E-09,8.09E-08,1.63E-06,1.42E-05,4.12E-06,3.86E-05,0.999848656,7.46E-08,6.37E-05,6.74E-07
5312,natural_language_inference28,242,where ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,49,1,0,,9.67E-07,0,negative,1.08E-06,4.00E-08,2.38E-07,6.60E-09,1.06E-08,1.05E-06,2.09E-06,2.36E-06,8.56E-07,0.999986658,1.07E-08,5.50E-06,1.04E-07
5313,natural_language_inference28,243,tare rotational memory phase vectors at time t and ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,50,1,0,,1.29E-05,0,negative,1.13E-05,7.64E-08,1.17E-06,3.37E-09,4.20E-08,7.24E-07,8.22E-06,1.79E-06,6.91E-07,0.999903303,5.00E-09,7.25E-05,1.75E-07
5314,natural_language_inference28,244,represents the phases generated by the operation Rotation correspondingly .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,51,1,0,,7.92E-05,0,negative,1.69E-06,8.81E-08,1.05E-06,2.21E-09,1.64E-08,5.54E-07,2.10E-06,1.91E-06,4.78E-06,0.999982348,4.24E-09,5.36E-06,9.09E-08
5315,natural_language_inference28,245,Note that each element of the matrix multiplication U 0 (? i ) U 0 ( ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,52,1,0,,4.67E-06,0,negative,1.76E-05,1.20E-07,1.86E-06,2.51E-09,4.19E-08,5.96E-07,6.16E-06,1.07E-06,1.19E-06,0.999904502,4.12E-09,6.68E-05,9.71E-08
5316,natural_language_inference28,246,i ) only depends on one element from ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,53,1,0,,8.40E-06,0,negative,1.83E-05,7.17E-08,2.19E-06,3.26E-09,3.85E-08,4.27E-07,4.72E-06,8.07E-07,8.94E-07,0.999899738,3.90E-09,7.26E-05,1.34E-07
5317,natural_language_inference28,247,i and ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,54,1,0,,8.05E-06,0,negative,2.42E-06,9.80E-09,1.28E-07,2.47E-09,1.51E-08,5.83E-07,2.73E-06,8.78E-07,1.14E-07,0.999971022,1.36E-09,2.20E-05,6.70E-08
5318,natural_language_inference28,248,i each .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,55,1,0,,8.96E-06,0,negative,5.28E-06,2.95E-08,2.64E-07,1.17E-09,1.39E-08,3.60E-07,3.79E-06,1.19E-06,4.65E-07,0.999961612,9.27E-10,2.69E-05,7.15E-08
5319,natural_language_inference28,249,"This means that , to cancel out one element ?",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,56,1,0,,2.02E-06,0,negative,1.21E-06,1.80E-08,1.72E-07,4.31E-09,1.58E-08,4.07E-07,7.95E-07,6.08E-07,4.05E-07,0.999991598,2.70E-09,4.70E-06,6.22E-08
5320,natural_language_inference28,250,"i , the model only needs to learn to express ?",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,57,1,0,,9.02E-06,0,negative,3.85E-06,8.92E-08,3.99E-07,6.39E-08,4.41E-08,5.25E-06,1.18E-05,1.05E-05,8.86E-07,0.999943837,1.69E-08,2.23E-05,9.54E-07
5321,natural_language_inference28,251,i as the negation of ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,58,1,0,,5.01E-06,0,negative,1.77E-06,2.02E-08,2.80E-07,8.82E-09,3.77E-08,1.15E-06,4.03E-06,1.88E-06,2.09E-07,0.999973557,2.20E-09,1.68E-05,2.26E-07
5322,natural_language_inference28,252,i .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,59,1,0,,1.76E-06,0,negative,5.60E-06,2.94E-08,2.63E-07,1.06E-08,3.02E-08,1.23E-06,3.72E-06,2.36E-06,3.85E-07,0.999966942,1.43E-09,1.92E-05,2.45E-07
5323,natural_language_inference28,253,"As a result , our RNN implementation does not require a reset gate , as in GRU or GORU , because the forgetting mechanism is automatically embedded into the representation ( 2 ) of phase - encoding .",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,60,1,0,,2.59E-05,0,negative,0.000548407,5.40E-06,1.71E-05,1.64E-08,6.60E-07,1.20E-06,3.23E-05,3.06E-06,2.93E-05,0.998921257,1.25E-08,0.000440185,1.11E-06
5324,natural_language_inference28,254,"Thus , the concept of phase - encoding is simply a special sampling on manifolds generated by the special orthogonal Lie group SO ( N ) .",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,61,1,0,,3.64E-05,0,negative,6.87E-06,6.74E-07,5.23E-06,2.10E-08,1.17E-07,1.34E-06,8.65E-06,3.04E-06,7.63E-06,0.999906245,4.04E-08,5.90E-05,1.18E-06
5325,natural_language_inference28,255,"Now , let N = N h be the hidden size .",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,62,1,0,,4.31E-06,0,negative,2.13E-06,5.19E-08,2.52E-07,1.21E-09,1.54E-08,6.35E-07,6.35E-06,3.60E-06,1.07E-06,0.999973294,1.06E-09,1.24E-05,1.57E-07
5326,natural_language_inference28,256,One way to extend the current RUM model is to allow for ?,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,63,1,0,,8.17E-06,0,negative,5.22E-06,6.23E-08,1.18E-06,1.46E-08,4.81E-08,1.42E-06,3.88E-06,2.59E-06,1.42E-06,0.999973247,2.93E-09,1.06E-05,3.34E-07
5327,natural_language_inference28,257,"to be any real number in the associative memory equation R t = ( R t?1 ) ? Rotation ( ? t , ? t ) .",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,64,1,0,,2.79E-06,0,negative,2.29E-06,4.59E-08,3.75E-07,3.37E-09,3.47E-08,9.60E-07,3.24E-06,2.99E-06,6.08E-07,0.99998,9.34E-10,9.30E-06,1.43E-07
5328,natural_language_inference28,258,This will expand the representational power of the rotational unit .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,65,1,0,,2.31E-05,0,negative,1.39E-05,6.97E-08,1.27E-06,7.39E-09,7.60E-08,7.97E-07,5.27E-06,1.22E-06,2.14E-06,0.999947796,1.04E-09,2.71E-05,3.31E-07
5329,natural_language_inference28,259,"The difficulty is to mathematically define the raising of a matrix to are al power , which is equivalent to defining a logarithm of a matrix .",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,66,1,0,,3.19E-06,0,negative,2.48E-06,8.61E-08,5.33E-07,2.25E-08,4.72E-08,1.51E-06,5.39E-06,2.33E-06,6.82E-07,0.999962807,1.95E-08,2.34E-05,7.48E-07
5330,natural_language_inference28,260,"Again , rotations prove to be a natural choice since they are elements of SO ( N h ) , and their logarithms correspond to elements of the vector space of the Lie algebra so ( N h ) , associatied to SO ( N h ) .",CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,67,1,0,,4.55E-06,0,negative,4.11E-06,5.09E-08,5.89E-07,2.42E-09,4.21E-08,6.15E-07,3.74E-06,1.14E-06,4.44E-07,0.999964374,3.30E-10,2.48E-05,1.05E-07
5331,natural_language_inference28,261,For the training of all models we use RMSProp optimization with a learning rate of 0.001 and a decay rate of 0.9 ; the batch size Nb is 128 .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,68,1,0,,0.360256022,0,negative,0.000214058,3.19E-05,2.42E-05,2.77E-05,1.70E-05,0.010188525,0.044465054,0.040279363,3.10E-05,0.902004726,3.93E-08,0.000377413,0.002338951
5332,natural_language_inference28,262,We observe that it is necessary to tune in the associative memory via ? = 1 since ? =,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,69,1,0,,1.90E-06,0,negative,8.59E-05,1.23E-07,3.91E-07,1.79E-08,1.62E-07,2.39E-06,2.44E-05,7.89E-06,6.37E-07,0.999757835,1.02E-09,0.000119392,9.10E-07
5333,natural_language_inference28,263,0 RUM does not learn the task .,CHARACTER LEVEL LANGUAGE MODELING,THEORETICAL ANALYSIS,natural_language_inference,28,70,1,0,,3.10E-06,0,negative,1.01E-05,3.23E-08,3.20E-06,1.73E-09,6.81E-08,3.43E-07,6.26E-06,4.46E-07,3.73E-07,0.999927641,3.82E-10,5.12E-05,3.13E-07
5334,natural_language_inference28,264,FUTURE WORK,CHARACTER LEVEL LANGUAGE MODELING,,natural_language_inference,28,71,1,0,,0.000450514,0,negative,1.99E-05,6.25E-08,1.41E-06,8.26E-09,1.74E-07,6.36E-05,0.361199151,1.57E-06,5.69E-07,0.638464943,4.61E-09,5.88E-05,0.000189812
5335,natural_language_inference28,265,D CHARACTER LEVEL PENN TREEBANK TASK,CHARACTER LEVEL LANGUAGE MODELING,,natural_language_inference,28,72,1,0,,0.001155707,0,experiments,1.14E-05,2.51E-08,2.42E-06,2.95E-08,1.94E-07,2.45E-05,0.973824899,2.66E-07,2.44E-08,0.020909603,1.96E-08,0.000283483,0.004943072
5336,natural_language_inference28,266,"For all RNN cells we apply layer normalization ) to the cells and to the LSTM gates and RUM 's update gate and target memory , zoneout ) to the recurrent connections , and dropout ) to the FS - RNN .",CHARACTER LEVEL LANGUAGE MODELING,D CHARACTER LEVEL PENN TREEBANK TASK,natural_language_inference,28,73,1,0,,0.002469404,0,negative,0.001772145,9.28E-06,0.000196239,9.41E-07,7.83E-06,0.000296016,0.119661771,0.000146542,6.83E-05,0.87361292,4.41E-09,0.000256534,0.003971478
5337,natural_language_inference28,267,For training we use Adam optimization ( Kingma & Ba ) .,CHARACTER LEVEL LANGUAGE MODELING,D CHARACTER LEVEL PENN TREEBANK TASK,natural_language_inference,28,74,1,0,,0.490004339,0,experiments,0.000155721,4.17E-06,1.92E-05,2.77E-06,4.35E-06,0.002470957,0.687805964,0.001926741,1.07E-05,0.283042043,3.67E-09,0.000111945,0.02444539
5338,natural_language_inference28,268,We apply gradient clipping with maximal norm of the gradients equal to 1.0 . lists the hyper - parameters we use for our models .,CHARACTER LEVEL LANGUAGE MODELING,D CHARACTER LEVEL PENN TREEBANK TASK,natural_language_inference,28,75,1,0,,0.073133282,0,experiments,0.000168547,4.14E-06,1.59E-05,2.31E-06,3.91E-06,0.00228168,0.625807924,0.002180209,1.13E-05,0.349490298,3.16E-09,0.000148989,0.019884736
5339,natural_language_inference28,269,We embed the inputs into a higher - dimensional space .,CHARACTER LEVEL LANGUAGE MODELING,D CHARACTER LEVEL PENN TREEBANK TASK,natural_language_inference,28,76,1,0,,0.000502406,0,negative,6.01E-05,3.04E-06,6.50E-05,5.77E-08,7.14E-07,1.58E-05,0.00640491,1.73E-05,0.00013094,0.992634572,2.42E-09,6.81E-05,0.000599541
5340,natural_language_inference28,270,The output of each models passes through a softmax layer ; then the probabilities are evaluated by a standard cross entropy loss function .,CHARACTER LEVEL LANGUAGE MODELING,D CHARACTER LEVEL PENN TREEBANK TASK,natural_language_inference,28,77,1,0,,0.000429169,0,negative,0.000210536,2.38E-06,0.00020059,2.41E-08,6.56E-07,9.64E-06,0.00609122,5.74E-06,0.000209003,0.992874357,1.54E-09,6.31E-05,0.000332772
5341,natural_language_inference28,271,The bits - per-character ( BPC ) loss is simply the cross entropy with a binary logarithm .,CHARACTER LEVEL LANGUAGE MODELING,D CHARACTER LEVEL PENN TREEBANK TASK,natural_language_inference,28,78,1,0,,0.001588757,0,negative,1.47E-05,4.29E-07,0.000109757,2.62E-08,3.13E-07,1.04E-05,0.008597857,3.94E-06,5.68E-06,0.990207076,8.62E-09,8.92E-05,0.000960586
5342,relation-classification0,1,title,,,relation-classification,0,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
5343,relation-classification0,2,End - to - End Relation Extraction using LSTMs on Sequences and Tree Structures,title,title,relation-classification,0,1,1,1,research-problem,0.989613553,1,research-problem,1.51E-08,9.58E-06,4.30E-08,2.27E-08,2.17E-08,4.39E-08,2.93E-07,8.52E-07,1.58E-06,0.00193043,0.998057018,8.28E-08,1.99E-08
5344,relation-classification0,3,abstract,,,relation-classification,0,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
5345,relation-classification0,4,We present a novel end - to - end neural model to extract entities and relations between them .,abstract,abstract,relation-classification,0,1,1,1,research-problem,0.816708795,1,research-problem,9.78E-07,0.003469423,1.68E-05,1.07E-06,5.54E-06,1.85E-06,2.83E-06,2.42E-05,0.000834708,0.017434808,0.97820636,9.39E-07,5.45E-07
5346,relation-classification0,5,Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM - RNNs on bidirectional sequential LSTM - RNNs .,abstract,abstract,relation-classification,0,2,1,0,,0.290737118,0,research-problem,2.06E-05,0.080101172,0.000398576,1.09E-05,0.000100536,3.85E-05,1.92E-05,0.000311499,0.072259538,0.198589274,0.648137543,8.42E-06,4.24E-06
5347,relation-classification0,6,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,abstract,abstract,relation-classification,0,3,1,1,research-problem,0.116698717,0,negative,1.62E-05,0.110685589,4.59E-05,1.33E-05,0.000193112,5.15E-05,4.17E-06,0.000496316,0.082159095,0.78039321,0.025938098,2.55E-06,9.08E-07
5348,relation-classification0,7,We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .,abstract,abstract,relation-classification,0,4,1,0,,0.046933373,0,negative,6.99E-05,0.282589505,0.000122876,0.000182465,0.001599981,0.000184782,4.32E-05,0.001918551,0.034575496,0.581362018,0.097319316,2.10E-05,1.09E-05
5349,relation-classification0,8,"Our model improves over the stateof - the - art feature - based model on end -toend relation extraction , achieving 12.1 % and 5.7 % relative error reductions in F1score on ACE2005 and ACE2004 , respectively .",abstract,abstract,relation-classification,0,5,1,0,,0.029125973,0,research-problem,0.0002669,0.002974201,2.09E-05,8.36E-05,0.000261493,5.37E-05,0.000575358,0.000589159,5.12E-05,0.452567891,0.540246529,0.002275382,3.36E-05
5350,relation-classification0,9,We also show that our LSTM - RNN based model compares favorably to the state - of - the - art CNN based model ( in F1-score ) on nominal relation classification ( Sem Eval - 2010 Task 8 ) .,abstract,abstract,relation-classification,0,6,1,0,,0.027401516,0,negative,0.00016417,0.003539052,1.03E-05,2.42E-05,0.000106302,4.82E-05,0.000365731,0.00082289,5.48E-05,0.554039628,0.438835381,0.001976821,1.24E-05
5351,relation-classification0,10,"Finally , we present an extensive ablation analysis of several model components .",abstract,abstract,relation-classification,0,7,1,0,,0.00162103,0,negative,6.10E-05,0.008780215,7.33E-06,0.000458518,0.002695936,4.42E-05,3.88E-06,8.60E-05,0.000565853,0.981281777,0.006012125,2.46E-06,6.48E-07
5352,relation-classification0,11,Introduction,,,relation-classification,0,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
5353,relation-classification0,12,Extracting semantic relations between entities in text is an important and well - studied task in information extraction and natural language processing ( NLP ) .,Introduction,Introduction,relation-classification,0,1,1,1,research-problem,0.950099772,1,research-problem,1.16E-06,0.000133462,4.10E-07,2.12E-05,3.53E-05,4.61E-06,1.57E-05,3.56E-06,2.59E-05,0.02525728,0.97449747,1.15E-06,2.69E-06
5354,relation-classification0,13,"Traditional systems treat this task as a pipeline of two separated tasks , i.e. , named entity recognition ( NER ) ) and relation extraction , but recent studies show that end - to - end ( joint ) modeling of entity and relation is important for high performance since relations interact closely with entity information .",Introduction,Introduction,relation-classification,0,2,1,1,research-problem,0.926902071,1,research-problem,6.03E-07,0.000125567,2.38E-07,3.32E-06,6.92E-06,2.10E-06,5.57E-06,3.03E-06,3.48E-05,0.027201009,0.972615298,6.95E-07,8.84E-07
5355,relation-classification0,14,"For instance , to learn that Toefting and Bolton have an Organization - Affiliation ( ORG - AFF ) relation in the sentence Toefting transferred to Bolton , the entity information that Toefting and Bolton are Person and Organization entities is important .",Introduction,Introduction,relation-classification,0,3,1,0,,0.01114311,0,negative,2.88E-05,0.006589993,5.39E-06,1.64E-05,0.000710855,4.78E-05,2.12E-05,5.68E-05,0.004855929,0.855391992,0.132260391,1.22E-05,2.27E-06
5356,relation-classification0,15,"Extraction of these entities is in turn encouraged by the presence of the context words transferred to , which indicate an employment relation .",Introduction,Introduction,relation-classification,0,4,1,0,,0.425800883,0,negative,0.000292508,0.040280517,9.69E-05,0.000115542,0.021266242,0.0001145,6.33E-05,5.02E-05,0.016180585,0.890843053,0.030646154,4.49E-05,5.60E-06
5357,relation-classification0,16,Previous joint models have employed feature - based structured learning .,Introduction,Introduction,relation-classification,0,5,1,0,,0.040124485,0,negative,5.74E-05,0.026512136,6.16E-05,0.000155533,0.000915184,0.000373112,0.000111223,0.000281533,0.013899744,0.625955241,0.331642594,1.67E-05,1.80E-05
5358,relation-classification0,17,An alternative approach to this end - to - end relation extraction task is to employ automatic feature learning via neural network ( NN ) based models .,Introduction,Introduction,relation-classification,0,6,1,0,,0.699229564,1,research-problem,2.75E-06,0.000819153,1.27E-06,3.38E-05,8.30E-05,1.90E-05,1.91E-05,1.84E-05,0.000167899,0.129819858,0.869009689,2.37E-06,3.73E-06
5359,relation-classification0,18,There are two ways to represent relations between entities using neural networks : recurrent / recursive neural networks ( RNNs ) and convolutional neural networks ( CNNs ) .,Introduction,Introduction,relation-classification,0,7,1,0,,0.783913871,1,research-problem,4.64E-06,0.004107734,1.55E-05,8.13E-06,3.17E-05,9.41E-05,5.20E-05,0.000135428,0.019099543,0.314592153,0.661848186,4.58E-06,6.27E-06
5360,relation-classification0,19,"Among these , RNNs can directly represent essential linguistic structures , i.e. , word sequences and constituent / dependency trees .",Introduction,Introduction,relation-classification,0,8,1,0,,0.880666196,1,negative,1.09E-05,0.007009238,9.81E-06,3.16E-06,4.92E-05,3.50E-05,3.00E-05,6.54E-05,0.019395038,0.56270648,0.410674725,8.78E-06,2.18E-06
5361,relation-classification0,20,"Despite this representation ability , for relation classification tasks , the previously reported performance using long short - term memory ( LSTM ) based RNNs is worse than one using CNNs .",Introduction,Introduction,relation-classification,0,9,1,0,,0.075874503,0,research-problem,5.36E-06,0.00058665,9.16E-07,6.99E-06,1.78E-05,1.66E-05,2.56E-05,2.90E-05,0.000249896,0.316809559,0.68224138,7.83E-06,2.37E-06
5362,relation-classification0,21,"These previous LSTM - based systems mostly include limited linguistic structures and neural architectures , and do not model entities and relations jointly .",Introduction,Introduction,relation-classification,0,10,1,0,,0.052632474,0,research-problem,6.11E-06,0.004660339,4.56E-06,1.79E-05,7.89E-05,0.000115514,7.54E-05,0.000173561,0.002828268,0.44245714,0.549566871,8.75E-06,6.69E-06
5363,relation-classification0,22,We are able to achieve improvements over state - of - the - art models via endto - end modeling of entities and relations based on richer LSTM - RNN architectures that incorporate complementary linguistic structures .,Introduction,Introduction,relation-classification,0,11,1,0,,0.222761588,0,negative,0.00066578,0.284435807,0.000184354,5.78E-05,0.003289256,0.000163338,0.001363163,0.000393035,0.158108697,0.456638465,0.092964451,0.001663009,7.28E-05
5364,relation-classification0,23,Word sequence and tree structure are known to be complementary information for extracting relations .,Introduction,Introduction,relation-classification,0,12,1,0,,0.023011567,0,negative,1.20E-05,0.002424525,6.50E-06,3.37E-05,0.000338211,0.000116532,6.24E-05,7.48E-05,0.001810416,0.673716855,0.321385659,1.12E-05,7.19E-06
5365,relation-classification0,24,"For instance , dependencies between words are not enough to predict that source and U.S. have an ORG - AFF relation in the sentence "" This is ... "" , one U.S. source said , and the context word said is required for this prediction .",Introduction,Introduction,relation-classification,0,13,1,0,,0.001619594,0,negative,9.75E-06,0.001268493,2.13E-06,2.58E-05,0.000422908,0.000118988,2.85E-05,5.46E-05,0.001224901,0.968843309,0.027992912,5.44E-06,2.33E-06
5366,relation-classification0,25,"Many traditional , feature - based relation classification models extract features from both sequences and parse trees .",Introduction,Introduction,relation-classification,0,14,1,0,,0.304410056,0,research-problem,4.63E-06,0.002830869,4.61E-06,1.98E-05,0.000109201,6.91E-05,5.22E-05,6.91E-05,0.000947911,0.35671289,0.639168378,5.26E-06,6.00E-06
5367,relation-classification0,26,"However , previous RNNbased models focus on only one of these linguistic structures .",Introduction,Introduction,relation-classification,0,15,1,0,,0.026192333,0,negative,5.59E-06,0.002764785,3.71E-06,1.95E-05,8.51E-05,9.34E-05,7.14E-05,0.000114275,0.001580773,0.503742787,0.491504693,7.44E-06,6.57E-06
5368,relation-classification0,27,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,Introduction,Introduction,relation-classification,0,16,1,1,model,0.95317975,1,model,3.88E-05,0.250912993,0.000292744,4.35E-06,0.000428075,3.86E-05,0.000107638,7.16E-05,0.696164952,0.026601536,0.025299206,2.91E-05,1.04E-05
5369,relation-classification0,28,Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,Introduction,Introduction,relation-classification,0,17,1,1,model,0.954355693,1,model,5.26E-06,0.033260078,6.21E-05,2.61E-07,4.33E-05,6.24E-06,6.47E-06,8.19E-06,0.961906015,0.004290694,0.000408825,1.90E-06,7.18E-07
5370,relation-classification0,29,"Our model first detects entities and then extracts relations between the detected entities using a single incrementally - decoded NN structure , and the NN parameters are jointly updated using both entity and relation labels .",Introduction,Introduction,relation-classification,0,18,1,1,model,0.946925429,1,model,4.47E-06,0.043843441,2.81E-05,2.99E-07,4.59E-05,6.88E-06,4.92E-06,1.46E-05,0.951908376,0.003916126,0.000225232,9.49E-07,6.72E-07
5371,relation-classification0,30,"Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .",Introduction,Introduction,relation-classification,0,19,1,1,model,0.950932454,1,model,4.25E-05,0.319718007,0.000119588,4.78E-06,0.001074429,3.56E-05,2.88E-05,6.90E-05,0.669196847,0.009491667,0.000207764,7.56E-06,3.47E-06
5372,relation-classification0,31,"These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .",Introduction,Introduction,relation-classification,0,20,1,1,model,0.593315857,1,model,0.000682069,0.328740997,0.00030064,0.000109603,0.013951557,0.000219931,0.000227593,0.000199477,0.336405222,0.315829278,0.003133202,0.000178225,2.22E-05
5373,relation-classification0,32,"On end - to - end relation extraction , we improve over the state - of - the - art feature - based model , with 12.1 % ( ACE2005 ) and 5.7 % ( ACE2004 ) relative error reductions in F1-score .",Introduction,Introduction,relation-classification,0,21,1,0,,0.143656896,0,negative,0.008318542,0.165212829,0.00035031,0.000150368,0.008944847,0.000684272,0.046960155,0.001693782,0.014096275,0.699306728,0.017953378,0.035696911,0.000631605
5374,relation-classification0,33,"On nominal relation classification ( Sem Eval - 2010 Task 8 ) , our model compares favorably to the state - of - the - art CNNbased model in F1-score .",Introduction,Introduction,relation-classification,0,22,1,0,,0.06841854,0,negative,0.001165296,0.100958045,0.000145551,5.01E-05,0.003819216,0.000469887,0.025118524,0.001240387,0.005936178,0.812411247,0.028390892,0.020025831,0.000268824
5375,relation-classification0,34,"Finally , we also ablate and compare our various model components , which leads to some key findings ( both positive and negative ) about the contribution and effectiveness of different RNN structures , input dependency relation structures , different parsing models , external resources , and joint learning settings .",Introduction,Introduction,relation-classification,0,23,1,0,,0.013935921,0,negative,0.000267441,0.046598919,2.47E-05,4.49E-05,0.003541262,0.000224729,0.000224889,0.000258426,0.016673103,0.930750234,0.001275023,0.000109788,6.59E-06
5376,relation-classification0,35,Related Work,,,relation-classification,0,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
5377,relation-classification0,49,Model,,,relation-classification,0,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
5378,relation-classification0,50,"We design our model with LSTM - RNNs that represent both word sequences and dependency tree structures , and perform end - to - end extraction of relations between entities on top of these RNNs.",Model,Model,relation-classification,0,1,1,0,,0.198509133,0,model,0.000733416,0.023670955,0.020804293,3.22E-05,0.00018834,0.000603238,0.000709186,0.003749476,0.491659907,0.456054329,0.000512668,0.001110603,0.00017134
5379,relation-classification0,51,illustrates the overview of the model .,Model,Model,relation-classification,0,2,1,0,,0.000888756,0,negative,2.61E-05,4.13E-05,3.43E-05,1.36E-06,1.64E-06,3.41E-05,1.50E-05,0.000239771,0.003871856,0.995643984,1.93E-05,6.75E-05,3.74E-06
5380,relation-classification0,52,"The model mainly consists of three representation layers : a word embeddings layer ( embedding layer ) , a word sequence based LSTM - RNN layer ( sequence layer ) , and finally a dependency subtree based LSTM - RNN layer ( dependency layer ) .",Model,Model,relation-classification,0,3,1,0,,0.542373578,1,model,0.000528949,0.003695297,0.010367215,1.28E-05,3.24E-05,0.000273995,0.000267412,0.002001647,0.643953721,0.337970388,0.000369697,0.000364655,0.000161893
5381,relation-classification0,53,"During decoding , we build greedy , left - to - right entity detection on the sequence layer and realize relation classification on the dependency layers , where each subtree based LSTM - RNN corresponds to a relation candidate between two detected entities .",Model,Model,relation-classification,0,4,1,0,,0.340701648,0,model,0.000801507,0.008429817,0.006933174,2.52E-06,2.29E-05,5.42E-05,8.81E-05,0.00065419,0.552662775,0.429590142,0.000139697,0.000597919,2.30E-05
5382,relation-classification0,54,"After decoding the entire model structure , we update the parameters simultaneously via backpropagation through time ( BPTT ) .",Model,Model,relation-classification,0,5,1,0,,0.416291041,0,negative,0.000673682,0.007154675,0.00908028,1.73E-06,1.71E-05,4.50E-05,8.86E-05,0.000550945,0.372989145,0.608715622,0.000143227,0.000522924,1.70E-05
5383,relation-classification0,55,"The dependency layers are stacked on the sequence layer , so the embedding and sequence layers are shared by both entity detection and relation classification , and the shared parameters are affected by both entity and relation labels .",Model,Model,relation-classification,0,6,1,0,,0.242366526,0,negative,0.001130498,0.004541733,0.003091287,7.08E-06,2.29E-05,0.00019441,0.000169693,0.003440049,0.418354745,0.568543404,5.02E-05,0.000418397,3.56E-05
5384,relation-classification0,56,Embedding Layer,Model,,relation-classification,0,7,1,0,,0.760665644,1,negative,0.000454088,0.001806283,0.003286241,1.84E-05,2.29E-05,0.000683781,0.000794395,0.008386299,0.419392342,0.562659118,0.000754041,0.001214285,0.000527779
5385,relation-classification0,57,The embedding layer handles embedding representations .,Model,Embedding Layer,relation-classification,0,8,1,0,,0.001388298,0,negative,6.19E-05,0.001226703,0.000387355,3.36E-06,4.80E-06,0.000203908,2.41E-05,0.007710365,0.131965434,0.858314093,5.45E-05,2.90E-05,1.45E-05
5386,relation-classification0,58,"n w , n p , n d and n e - dimensional vectors v , v ( p ) , v and v ( e ) are embedded to words , part - of - speech ( POS ) tags , dependency types , and entity labels , respectively .",Model,Embedding Layer,relation-classification,0,9,1,0,,1.23E-05,0,negative,1.62E-06,0.000131516,1.09E-05,5.91E-08,2.35E-07,2.91E-05,1.77E-06,0.001269692,0.004312686,0.994227777,1.11E-05,3.33E-06,2.20E-07
5387,relation-classification0,59,Sequence Layer,Model,,relation-classification,0,10,1,0,,0.285049563,0,negative,0.001050865,0.001482449,0.009715361,9.51E-05,8.19E-05,0.001022307,0.001813031,0.003323061,0.416914266,0.558962578,0.000832127,0.003051669,0.001655214
5388,relation-classification0,60,The sequence layer represents words in a linear sequence using the representations from the embedding layer .,Model,Sequence Layer,relation-classification,0,11,1,0,,0.001370811,0,negative,8.81E-05,0.001404467,0.000990103,3.00E-06,6.68E-06,4.47E-05,2.87E-05,0.000636147,0.267099036,0.729569852,5.76E-05,5.62E-05,1.54E-05
5389,relation-classification0,61,"This layer represents sentential context information and maintains entities , as shown in bottom - left part of .",Model,Sequence Layer,relation-classification,0,12,1,0,,7.69E-05,0,negative,3.66E-05,0.000313064,0.000225758,8.52E-07,2.60E-06,1.31E-05,7.96E-06,0.000136997,0.094237028,0.904951161,3.11E-05,3.92E-05,4.53E-06
5390,relation-classification0,62,We represent the word sequence in a sentence with bidirectional LSTM - RNNs .,Model,Sequence Layer,relation-classification,0,13,1,0,,0.000271266,0,negative,3.82E-05,0.002274306,0.002036887,4.70E-07,4.45E-06,2.04E-05,2.31E-05,0.000300684,0.207345262,0.78780224,6.04E-05,8.86E-05,4.88E-06
5391,relation-classification0,63,"The LSTM unit at t-th word consists of a collection of n ls - dimensional vectors : an input gate it , a forget gate ft , an output gate o t , a memory cell ct , and a hidden state ht .",Model,Sequence Layer,relation-classification,0,14,1,0,,6.46E-06,0,negative,8.15E-06,0.00034927,3.27E-05,2.87E-07,1.18E-06,8.21E-06,4.14E-06,0.000216988,0.035660742,0.963676969,2.25E-05,1.74E-05,1.39E-06
5392,relation-classification0,64,"The unit receives an n-dimensional input vector x t , the previous hidden state h t?1 , and the memory cell c t?1 , and calculates the new vectors using the following equations :",Model,Sequence Layer,relation-classification,0,15,1,0,,1.90E-06,0,negative,5.81E-06,9.29E-05,3.98E-05,3.05E-08,2.33E-07,1.66E-06,1.27E-06,3.31E-05,0.008818924,0.990981481,7.01E-06,1.77E-05,1.69E-07
5393,relation-classification0,65,-1,Model,Sequence Layer,relation-classification,0,16,1,0,,2.15E-06,0,negative,4.14E-06,1.35E-05,1.67E-06,2.04E-07,2.59E-07,2.74E-06,1.13E-06,6.48E-05,0.000644208,0.999253841,2.38E-06,1.09E-05,2.52E-07
5394,relation-classification0,66,where ?,Model,Sequence Layer,relation-classification,0,17,1,0,,1.18E-07,0,negative,1.47E-06,7.12E-06,9.18E-07,1.13E-07,9.82E-08,2.68E-06,7.38E-07,4.42E-05,0.000371219,0.99956349,2.72E-06,5.11E-06,1.19E-07
5395,relation-classification0,67,"denotes the logistic function , denotes element - wise multiplication , W and U are weight matrices , and bare bias vectors .",Model,Sequence Layer,relation-classification,0,18,1,0,,6.30E-06,0,negative,3.98E-06,0.000129034,7.13E-06,1.89E-07,3.93E-07,8.92E-06,2.86E-06,0.000418576,0.005937957,0.993479535,4.27E-06,6.73E-06,4.27E-07
5396,relation-classification0,68,The LSTM unit at t-th word receives the concatenation of word and POS embeddings as its input vector :,Model,Sequence Layer,relation-classification,0,19,1,0,,9.69E-06,0,negative,4.06E-06,9.17E-05,2.86E-05,5.78E-08,2.36E-07,7.11E-06,2.92E-06,0.000202498,0.014040842,0.985609265,3.67E-06,8.66E-06,4.39E-07
5397,relation-classification0,69,.,Model,Sequence Layer,relation-classification,0,20,1,0,,2.29E-07,0,negative,1.21E-06,3.13E-06,3.43E-07,1.61E-08,3.55E-08,6.87E-07,2.48E-07,1.56E-05,0.000281416,0.999692889,4.89E-07,3.86E-06,3.50E-08
5398,relation-classification0,70,We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ?,Model,Sequence Layer,relation-classification,0,21,1,0,,5.66E-06,0,negative,7.60E-06,0.000112802,3.57E-05,4.17E-08,3.71E-07,4.98E-06,3.25E-06,0.000138274,0.010778603,0.988905281,1.14E-06,1.17E-05,2.71E-07
5399,relation-classification0,71,"ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ?",Model,Sequence Layer,relation-classification,0,22,1,0,,3.33E-06,0,negative,1.76E-05,3.69E-05,1.37E-05,2.02E-07,1.06E-06,4.43E-06,4.14E-06,7.12E-05,0.000938951,0.998798864,2.20E-06,0.000110235,4.35E-07
5400,relation-classification0,72,"ht , and pass it to the subsequent layers .",Model,Sequence Layer,relation-classification,0,23,1,0,,2.76E-06,0,negative,3.49E-05,2.71E-05,8.93E-05,7.56E-08,7.93E-07,1.47E-06,2.31E-06,1.95E-05,0.001518467,0.998199814,1.02E-06,0.000104787,3.76E-07
5401,relation-classification0,73,Entity Detection,Model,,relation-classification,0,24,1,0,,0.166819174,0,negative,0.001470886,0.000844116,0.003086623,7.79E-05,0.000184477,0.000496879,0.015436753,0.003320913,0.005003621,0.61893801,0.001074361,0.343519634,0.006545781
5402,relation-classification0,74,We treat entity detection as a sequence labeling task .,Model,Entity Detection,relation-classification,0,25,1,0,,0.000445449,0,negative,0.000171943,0.000171779,0.000242861,5.45E-06,9.67E-06,6.80E-05,0.001085612,9.96E-05,0.000243646,0.988653911,0.00016127,0.008952321,0.000133896
5403,relation-classification0,75,"We assign an entity tag to each word using a commonly used encoding scheme BILOU ( Begin , Inside , Last , Outside , Unit ) ( Ratinov and , where each entity tag represents the entity type and the position of a word in the entity .",Model,Entity Detection,relation-classification,0,26,1,0,,3.60E-05,0,negative,3.42E-06,7.01E-06,1.35E-05,8.35E-08,2.30E-07,4.17E-05,4.21E-05,0.000104937,0.000494767,0.99928038,2.31E-07,1.01E-05,1.60E-06
5404,relation-classification0,76,"For example , in , we assign B - PER and L - PER ( which denote the beginning and last words of a person entity type , respectively ) to each word in Sidney Yates to represent this phrase as a PER ( person ) entity type .",Model,Entity Detection,relation-classification,0,27,1,0,,1.76E-05,0,negative,1.06E-06,3.04E-07,2.88E-06,5.66E-09,2.92E-08,6.37E-06,1.17E-05,9.84E-06,1.84E-05,0.999939296,2.94E-08,9.96E-06,1.60E-07
5405,relation-classification0,77,We perform entity detection on top of the sequence layer .,Model,Entity Detection,relation-classification,0,28,1,0,,0.007309719,0,negative,0.000324705,0.00013479,0.001546958,4.03E-07,3.27E-06,3.30E-05,0.000449946,3.99E-05,0.005273909,0.991202106,1.43E-06,0.000960686,2.89E-05
5406,relation-classification0,78,We employ a two - layered NN with an n he - dimensional hidden layer h ( e ) and a softmax output layer for entity detection .,Model,Entity Detection,relation-classification,0,29,1,0,,0.001364934,0,negative,5.86E-05,0.000232384,0.000408735,9.61E-07,2.65E-06,0.000130717,0.000718903,0.000401756,0.007770308,0.989938195,7.68E-06,0.000262723,6.63E-05
5407,relation-classification0,79,"Here , Ware weight matrices and bare bias vectors .",Model,Entity Detection,relation-classification,0,30,1,0,,8.18E-06,0,negative,3.54E-06,2.84E-07,3.99E-07,1.09E-08,2.24E-08,5.73E-06,5.18E-06,2.28E-05,1.08E-05,0.99994427,6.70E-09,6.88E-06,6.11E-08
5408,relation-classification0,80,"We assign entity labels to words in a greedy , left - to - right manner .",Model,Entity Detection,relation-classification,0,31,1,0,,0.001555248,0,negative,7.95E-05,0.000112286,0.000162555,6.35E-08,8.65E-07,1.40E-05,8.68E-05,3.98E-05,0.002583806,0.996785062,2.85E-07,0.000133135,1.93E-06
5409,relation-classification0,81,"1 During this decoding , we use the predicted label of a word to predict the label of the next word so as to take label dependencies into account .",Model,Entity Detection,relation-classification,0,32,1,0,,2.65E-05,0,negative,3.10E-06,6.79E-07,9.28E-06,3.02E-08,7.32E-08,3.06E-06,5.87E-06,4.67E-06,8.37E-05,0.999880676,4.00E-08,8.51E-06,3.56E-07
5410,relation-classification0,82,The NN above receives the concatenation of its corresponding outputs in the sequence layer and the label embedding for its previous word ) .,Model,Entity Detection,relation-classification,0,33,1,0,,2.69E-05,0,negative,4.89E-06,8.09E-07,2.20E-05,2.91E-09,2.15E-08,1.60E-06,5.59E-06,2.69E-06,0.000163793,0.999787272,2.79E-08,1.12E-05,1.74E-07
5411,relation-classification0,83,Dependency Layer,Model,,relation-classification,0,34,1,0,,0.079065488,0,negative,0.000910274,0.000594346,0.007083333,1.45E-05,3.84E-05,0.000282534,0.001301018,0.001701594,0.268875955,0.713677153,5.49E-05,0.004279443,0.001186649
5412,relation-classification0,84,"The dependency layer represents a relation between a pair of two target words ( corresponding to a relation candidate in relation classification ) in the dependency tree , and is in charge of relationspecific representations , as is shown in top - right part of .",Model,Dependency Layer,relation-classification,0,35,1,0,,7.42E-06,0,negative,4.35E-05,0.000140152,0.000187736,3.16E-07,1.49E-06,3.53E-06,3.54E-06,6.89E-05,0.039810768,0.959699889,1.65E-06,3.49E-05,3.69E-06
5413,relation-classification0,85,"This layer mainly focuses on the shortest path between a pair of target words in the dependency tree ( i.e. , the path between the least common node and the two target words ) since these paths are shown to be effective in relation classification .",Model,Dependency Layer,relation-classification,0,36,1,0,,0.000180107,0,negative,5.25E-05,0.000401885,0.000583064,6.07E-08,1.51E-06,1.27E-06,3.79E-06,2.24E-05,0.023428205,0.975344509,3.41E-06,0.000156009,1.41E-06
5414,relation-classification0,86,"For example , we show the shortest path between Yates and Chicago in the bottom of , and this path well captures the key phrase of their relation , i.e. , born in .",Model,Dependency Layer,relation-classification,0,37,1,0,,2.60E-07,0,negative,4.75E-06,1.98E-06,2.80E-06,6.71E-08,5.46E-07,9.65E-07,9.36E-07,1.09E-05,9.34E-05,0.999852674,4.44E-08,3.07E-05,1.71E-07
5415,relation-classification0,87,"We employ bidirectional tree - structured LSTM - RNNs ( i.e. , bottom - up and top - down ) to represent a relation candidate by capturing the dependency structure around the target word pair .",Model,Dependency Layer,relation-classification,0,38,1,0,,0.001958686,0,negative,6.20E-05,0.003135245,0.001353654,2.31E-07,6.19E-06,5.24E-06,1.85E-05,0.000114702,0.173887242,0.82123951,4.96E-06,0.000166379,6.14E-06
5416,relation-classification0,88,This bidirectional structure propagates to each node not only the information from the leaves but also information from the root .,Model,Dependency Layer,relation-classification,0,39,1,0,,9.07E-06,0,negative,7.61E-06,7.08E-05,5.70E-05,3.47E-08,4.54E-07,6.62E-07,8.98E-07,1.32E-05,0.027121416,0.972710589,7.48E-07,1.60E-05,6.20E-07
5417,relation-classification0,89,"This is especially important for relation classification , which makes use of argument nodes near the bottom of the tree , and our top - down LSTM - RNN sends information from the top of the tree to such near - leaf nodes ( unlike in standard bottom - up LSTM - RNNs ) .",Model,Dependency Layer,relation-classification,0,40,1,0,,8.52E-06,0,negative,3.03E-05,6.07E-05,2.70E-05,3.46E-08,7.45E-07,4.38E-07,1.01E-06,7.22E-06,0.000950189,0.998669432,6.92E-07,0.00025201,2.59E-07
5418,relation-classification0,90,2 Note that the two variants of tree - structured LSTM - RNNs by are notable to represent our target structures which have a variable number of typed children : the Child - Sum Tree - LSTM does not deal with types and the N - ary Tree assumes a fixed number of children .,Model,Dependency Layer,relation-classification,0,41,1,0,,1.51E-06,0,negative,7.97E-06,8.35E-06,4.41E-05,1.44E-08,3.09E-07,7.31E-07,1.70E-06,1.04E-05,0.000180313,0.999667567,6.18E-08,7.84E-05,1.29E-07
5419,relation-classification0,91,We thus propose a new variant of tree - structured LSTM - RNN that shares weight matrices U s for same - type children and also allows variable number of children .,Model,Dependency Layer,relation-classification,0,42,1,0,,0.000109097,0,negative,0.000111964,0.002109894,0.000793053,2.75E-07,6.93E-06,2.93E-06,1.40E-05,5.10E-05,0.038513771,0.957809529,9.28E-06,0.000572817,4.52E-06
5420,relation-classification0,92,"For this variant , we calculate n lt - dimensional vectors in the LSTM unit at t-th node with C ( t ) children using following equations :",Model,Dependency Layer,relation-classification,0,43,1,0,,1.96E-06,0,negative,6.06E-06,9.21E-06,1.75E-05,2.13E-09,7.64E-08,1.77E-07,3.58E-07,5.10E-06,0.000563754,0.999383995,1.54E-08,1.37E-05,3.22E-08
5421,relation-classification0,93,where m ( ) is a type mapping function .,Model,Dependency Layer,relation-classification,0,44,1,0,,2.16E-07,0,negative,1.02E-06,1.59E-06,1.11E-06,4.06E-09,2.75E-08,1.41E-07,1.31E-07,5.88E-06,0.000121414,0.999864955,3.64E-08,3.65E-06,4.12E-08
5422,relation-classification0,94,"To investigate appropriate structures to represent relations between two target word pairs , we experiment with three structure options .",Model,Dependency Layer,relation-classification,0,45,1,0,,4.81E-05,0,negative,1.78E-05,0.000132156,8.23E-06,9.01E-08,2.82E-06,2.80E-06,6.87E-06,0.000151039,0.000484273,0.999036273,8.43E-08,0.000157055,4.77E-07
5423,relation-classification0,95,"We primarily employ the shortest path structure ( SP - Tree ) , which captures the core dependency path between a target word pair and is widely used in relation classification models , e.g. , .",Model,Dependency Layer,relation-classification,0,46,1,0,,0.000730746,0,negative,7.07E-05,0.002318329,0.001136536,1.80E-07,6.92E-06,5.42E-06,2.79E-05,0.000125283,0.017591079,0.978272033,1.93E-06,0.000439921,3.77E-06
5424,relation-classification0,96,We also try two other dependency structures : SubTree and Full - Tree .,Model,Dependency Layer,relation-classification,0,47,1,0,,0.022530595,0,negative,0.000153244,0.000457253,0.002659099,1.14E-07,6.02E-06,5.68E-06,5.01E-05,6.73E-05,0.01096344,0.984832077,7.11E-07,0.000800132,4.82E-06
5425,relation-classification0,97,SubTree is the subtree under the lowest common ancestor of the target word pair .,Model,Dependency Layer,relation-classification,0,48,1,0,,4.83E-06,0,negative,2.38E-06,1.76E-05,3.94E-05,2.29E-08,2.73E-07,9.22E-07,1.64E-06,2.30E-05,0.002411971,0.997486596,1.77E-07,1.53E-05,7.53E-07
5426,relation-classification0,98,This provides additional modifier information to the path and the word pair in SPTree .,Model,Dependency Layer,relation-classification,0,49,1,0,,8.36E-07,0,negative,2.94E-05,8.07E-06,1.14E-05,1.15E-08,2.63E-07,2.19E-07,3.78E-07,5.19E-06,0.000702012,0.999201416,9.25E-09,4.15E-05,8.52E-08
5427,relation-classification0,99,FullTree is the full dependency tree .,Model,Dependency Layer,relation-classification,0,50,1,0,,0.004097596,0,negative,1.04E-05,6.63E-05,0.001774076,8.00E-08,1.26E-06,4.56E-06,2.57E-05,7.99E-05,0.009257223,0.988632618,9.29E-07,0.000134813,1.21E-05
5428,relation-classification0,100,This captures context from the entire sentence .,Model,Dependency Layer,relation-classification,0,51,1,0,,1.28E-05,0,negative,7.82E-06,1.96E-05,6.84E-05,1.36E-08,4.76E-07,3.15E-07,6.22E-07,6.92E-06,0.003188502,0.996689574,2.24E-08,1.74E-05,3.61E-07
5429,relation-classification0,101,"While we use one node type for SPTree , we define two node types for SubTree and FullTree , i.e. , one for nodes on shortest paths and one for all other nodes .",Model,Dependency Layer,relation-classification,0,52,1,0,,2.04E-06,0,negative,9.65E-06,0.000116961,2.96E-05,2.80E-08,8.23E-07,1.71E-06,3.57E-06,9.87E-05,0.003054905,0.996655443,3.49E-08,2.83E-05,3.59E-07
5430,relation-classification0,102,We use the type mapping function m ( ) to distinguish these two nodes types .,Model,Dependency Layer,relation-classification,0,53,1,0,,3.90E-07,0,negative,1.43E-06,1.30E-05,3.54E-06,9.40E-09,1.41E-07,5.04E-07,4.91E-07,3.02E-05,0.001083141,0.998864141,1.41E-08,3.29E-06,1.55E-07
5431,relation-classification0,103,Stacking Sequence and Dependency Layers,Model,,relation-classification,0,54,1,0,,0.177528986,0,negative,0.001060899,0.000275903,0.007595742,3.32E-06,2.13E-05,5.85E-05,0.000800071,0.000363357,0.08535381,0.89418236,8.54E-06,0.009768856,0.000507407
5432,relation-classification0,104,We stack the dependency layers ( corresponding to relation candidates ) on top of the sequence layer to incorporate both word sequence and dependency tree structure information into the output .,Model,Stacking Sequence and Dependency Layers,relation-classification,0,55,1,0,,0.000125745,0,negative,3.42E-05,0.000282328,0.000182191,3.19E-07,3.78E-06,4.72E-06,1.28E-05,0.000221188,0.068146135,0.931071146,2.18E-07,2.95E-05,1.14E-05
5433,relation-classification0,105,The dependency - layer LSTM unit at the t - th word receives as input,Model,Stacking Sequence and Dependency Layers,relation-classification,0,56,1,0,,6.91E-07,0,negative,2.14E-06,5.54E-06,4.96E-06,1.03E-08,1.17E-07,4.97E-07,1.25E-06,2.76E-05,0.000941467,0.999005294,2.32E-08,1.07E-05,4.05E-07
5434,relation-classification0,106,"i.e. , the concatenation of its corresponding hidden state vectors st in the sequence layer , dependency type embedding v",Model,Stacking Sequence and Dependency Layers,relation-classification,0,57,1,0,,2.11E-07,0,negative,4.04E-06,1.01E-05,5.30E-06,1.96E-08,2.02E-07,3.01E-07,7.55E-07,1.99E-05,0.000418781,0.999520842,2.60E-08,1.95E-05,2.67E-07
5435,relation-classification0,107,Relation Classification,Model,,relation-classification,0,58,1,0,,0.002929041,0,negative,5.33E-05,3.67E-05,0.000418942,9.38E-07,1.22E-05,7.43E-05,0.001221444,0.000753706,0.001268716,0.985464062,1.58E-06,0.010308884,0.000385272
5436,relation-classification0,108,"We incrementally build relation candidates using all possible combinations of the last words of detected entities , i.e. , words with L or U labels in the BILOU scheme , during decoding .",Model,Relation Classification,relation-classification,0,59,1,0,,1.68E-05,0,negative,3.04E-05,2.67E-05,0.000169413,6.73E-09,2.56E-07,1.17E-06,1.76E-05,1.34E-05,0.000447984,0.999190507,1.02E-08,0.000101318,1.28E-06
5437,relation-classification0,109,"For instance , in , we build a relation candidate using Yates with an L - PER label and Chicago with an U - LOC label .",Model,Relation Classification,relation-classification,0,60,1,0,,8.11E-08,0,negative,1.21E-06,4.59E-08,7.81E-06,1.95E-09,8.30E-08,5.36E-07,4.00E-06,1.51E-06,2.66E-07,0.999929022,5.91E-10,5.53E-05,2.18E-07
5438,relation-classification0,110,"For each relation candidate , we realize the dependency layer d p ( described above ) corresponding to the path between the word pair pin the relation candidate , and the NN receives a relation candidate vector constructed from the output of the dependency tree layer , and predicts its relation label .",Model,Relation Classification,relation-classification,0,61,1,0,,4.11E-07,0,negative,2.75E-06,3.46E-07,1.25E-05,5.83E-10,1.10E-08,1.48E-07,1.08E-06,1.56E-06,2.01E-05,0.999947039,1.27E-09,1.44E-05,1.05E-07
5439,relation-classification0,111,We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation .,Model,Relation Classification,relation-classification,0,62,1,0,,2.01E-07,0,negative,1.39E-06,3.23E-07,1.02E-05,3.56E-10,1.43E-08,1.44E-07,7.43E-07,1.54E-06,7.65E-06,0.999968504,3.96E-10,9.48E-06,4.88E-08
5440,relation-classification0,112,"We represent relation labels by type and direction , except for negative relations that have no direction .",Model,Relation Classification,relation-classification,0,63,1,0,,3.80E-07,0,negative,2.85E-07,2.89E-07,2.54E-06,2.04E-09,1.38E-08,1.50E-06,2.17E-06,1.88E-05,1.57E-05,0.999955688,6.17E-10,2.87E-06,2.13E-07
5441,relation-classification0,113,"The relation candidate vector is constructed as the concatenation d p = [?h p A ; ?h p 1 ; ?h p 2 ] , where ?h p",Model,Relation Classification,relation-classification,0,64,1,0,,1.77E-07,0,negative,1.72E-06,1.49E-07,4.87E-06,2.85E-10,1.08E-08,2.61E-07,1.63E-06,1.96E-06,2.75E-06,0.999971274,2.19E-10,1.53E-05,5.05E-08
5442,relation-classification0,114,A is the hidden state vector of the top LSTM,Model,Relation Classification,relation-classification,0,65,1,0,,1.16E-07,0,negative,5.31E-07,9.40E-08,6.12E-07,1.78E-09,6.62E-09,9.08E-07,2.65E-06,2.24E-05,2.92E-06,0.999963687,7.38E-10,5.94E-06,2.40E-07
5443,relation-classification0,115,We use the dependency to the parent since the number of children varies .,Model,Relation Classification,relation-classification,0,66,1,0,,2.70E-07,0,negative,2.74E-06,1.49E-07,5.66E-06,6.04E-10,1.02E-08,4.29E-07,1.99E-06,4.85E-06,4.83E-06,0.999968924,2.01E-10,1.03E-05,8.90E-08
5444,relation-classification0,116,"Dependency types can also be incorporated into m ( ) , but this did not help in initial experiments .",Model,Relation Classification,relation-classification,0,67,1,0,,1.67E-07,0,negative,3.64E-06,3.06E-08,5.82E-07,4.88E-09,9.74E-09,2.14E-06,3.89E-06,7.81E-06,1.11E-06,0.999962423,7.18E-10,1.81E-05,2.91E-07
5445,relation-classification0,117,"unit in the bottom - up LSTM - RNN ( representing the lowest common ancestor of the target word pair p ) , and ?h p 1 , ?h p 2 are the hidden state vectors of the two LSTM units representing the first and second target words in the top - down LSTM - RNN .",Model,Relation Classification,relation-classification,0,68,1,0,,1.61E-07,0,negative,2.19E-06,1.31E-07,1.15E-05,5.99E-10,1.61E-08,2.35E-07,2.66E-06,1.75E-06,4.68E-06,0.999947168,3.89E-10,2.95E-05,1.55E-07
5446,relation-classification0,118,All the corresponding arrows are shown in .,Model,Relation Classification,relation-classification,0,69,1,0,,3.15E-08,0,negative,1.47E-07,7.01E-09,1.16E-07,7.70E-10,2.85E-09,2.08E-07,3.06E-07,1.26E-06,4.17E-07,0.999995756,7.68E-11,1.75E-06,2.76E-08
5447,relation-classification0,119,"Similarly to the entity detection , we employ a two - layered NN with an n hr -dimensional hidden layer h ( r ) and a softmax output layer ( with weight matrices W , bias vectors b ) .",Model,Relation Classification,relation-classification,0,70,1,0,,5.70E-06,0,negative,1.56E-05,9.99E-06,0.000242147,1.55E-08,2.04E-07,3.12E-06,3.66E-05,2.69E-05,0.000230474,0.999358003,8.88E-09,7.03E-05,6.63E-06
5448,relation-classification0,120,"We construct the input d p for relation classification from tree - structured LSTM - RNNs stacked on sequential LSTM - RNNs , so the contribution of sequence layer to the input is indirect .",Model,Relation Classification,relation-classification,0,71,1,0,,3.94E-07,0,negative,6.59E-06,1.34E-06,1.99E-05,3.70E-09,1.63E-07,6.85E-07,7.60E-06,8.06E-06,6.62E-06,0.999895807,4.92E-10,5.26E-05,5.83E-07
5449,relation-classification0,121,"Furthermore , our model uses words for representing entities , so it can not fully use the entity information .",Model,Relation Classification,relation-classification,0,72,1,0,,3.30E-06,0,negative,1.99E-05,2.68E-07,6.62E-06,3.60E-09,8.53E-08,4.37E-07,2.53E-06,2.01E-06,1.39E-06,0.999852387,5.83E-10,0.000114037,2.93E-07
5450,relation-classification0,122,"To alleviate these problems , we directly concatenate the average of hidden state vectors for each entity from the sequence layer to the input d p to relation classification , i.e. , d p =",Model,Relation Classification,relation-classification,0,73,1,0,,8.79E-08,0,negative,3.19E-06,2.88E-07,5.12E-06,3.55E-10,1.31E-08,1.68E-07,1.54E-06,1.98E-06,3.25E-06,0.999970271,1.15E-10,1.41E-05,7.43E-08
5451,relation-classification0,123,", where I p 1 and I p 2 represent sets of word indices in the first and second entities .",Model,Relation Classification,relation-classification,0,74,1,0,,1.40E-08,0,negative,6.74E-07,1.57E-08,5.71E-07,1.28E-10,3.48E-09,4.96E-08,2.15E-07,4.09E-07,3.22E-07,0.999991341,3.19E-11,6.39E-06,1.06E-08
5452,relation-classification0,124,"Also , we assign two labels to each word pair in prediction since we consider both left - to - right and right - to - left directions .",Model,Relation Classification,relation-classification,0,75,1,0,,2.36E-07,0,negative,3.09E-06,1.11E-06,1.66E-05,4.51E-10,2.38E-08,3.26E-07,3.09E-06,5.84E-06,4.05E-05,0.999918089,1.34E-10,1.11E-05,1.71E-07
5453,relation-classification0,125,"When the predicted labels are inconsistent , we select the positive and more confident label , similar to .",Model,Relation Classification,relation-classification,0,76,1,0,,9.35E-08,0,negative,1.03E-06,6.80E-08,1.64E-06,3.80E-10,1.08E-08,1.53E-07,6.30E-07,2.95E-06,1.14E-06,0.999987653,2.75E-11,4.66E-06,6.79E-08
5454,relation-classification0,126,Training,,,relation-classification,0,0,1,0,,0.134389038,0,negative,3.65E-05,0.000486702,1.13E-05,6.34E-06,4.70E-06,0.000594961,0.000209756,0.011401758,0.000268358,0.981655362,0.005022667,0.000285532,1.61E-05
5455,relation-classification0,127,"We update the model parameters including weights , biases , and embeddings by BPTT and Adam ( Kingma and Ba , 2015 ) with gradient clipping , parameter averaging , and L2-regularization ( we regularize weights W and U , not the bias terms b ) .",Training,Training,relation-classification,0,1,1,0,,0.901024194,1,hyperparameters,6.26E-05,0.000286165,7.01E-05,1.61E-05,3.59E-06,0.134405142,0.002151521,0.80023397,0.000204319,0.062498414,7.71E-06,1.80E-05,4.24E-05
5456,relation-classification0,128,We also apply dropout to the embedding layer and to the final hidden layers for entity detection and relation classification .,Training,Training,relation-classification,0,2,1,0,,0.87204526,1,hyperparameters,0.003059512,0.013105122,0.022937181,6.76E-05,6.09E-05,0.056239362,0.008419833,0.497013903,0.015490062,0.381539058,0.000284324,0.001260109,0.000523007
5457,relation-classification0,129,"We employ two enhancements , scheduled sampling and entity pretraining , to alleviate the problem of unreliable prediction of entities in the early stage of training , and to encourage building positive relation instances from the detected entities .",Training,Training,relation-classification,0,3,1,0,,0.753997988,1,negative,0.008729234,0.049266908,0.026189073,0.000291436,0.000524688,0.046851011,0.012052362,0.358720035,0.016414515,0.474865027,0.000364873,0.00474592,0.000984918
5458,relation-classification0,130,"In scheduled sampling , we use gold labels as prediction in the probability of i that depends on the number of epochs i during training if the gold labels are legal .",Training,Training,relation-classification,0,4,1,0,,0.001553849,0,negative,0.000184866,0.002218211,0.007523655,5.16E-06,1.13E-05,0.012600738,0.001300802,0.114571201,0.001822163,0.859284869,0.000118024,0.00031367,4.53E-05
5459,relation-classification0,131,"As for i , we choose the inverse sigmoid decay i = k / ( k + exp ( i / k ) ) , where k( ?",Training,Training,relation-classification,0,5,1,0,,0.004124501,0,negative,0.000137358,8.59E-05,0.00010424,2.88E-06,2.60E-06,0.087592505,0.002002177,0.408793893,6.93E-05,0.5010052,1.21E-05,0.000172404,1.94E-05
5460,relation-classification0,132,1 ) is a hyper - parameter that adjusts how often we use the gold labels as prediction .,Training,Training,relation-classification,0,6,1,0,,0.005695674,0,hyperparameters,6.59E-05,0.000242716,0.000119035,5.23E-06,2.54E-06,0.048680061,0.000607195,0.608743856,0.000403404,0.341046061,1.27E-05,4.24E-05,2.89E-05
5461,relation-classification0,133,"Entity pretraining is inspired by , and we pretrain the entity detection model using the training data before training the entire model parameters .",Training,Training,relation-classification,0,7,1,0,,0.004912869,0,negative,0.000357251,0.00171531,0.008542499,3.62E-05,0.000126502,0.016090902,0.002739232,0.049964238,0.000572988,0.919135195,5.71E-05,0.00054309,0.000119578
5462,relation-classification0,134,Results and Discussion,,,relation-classification,0,0,1,0,,0.005562826,0,negative,0.000289338,0.000152875,1.75E-05,1.27E-06,2.39E-06,4.28E-05,0.000328215,0.000502027,2.34E-05,0.983648035,0.005859673,0.009125785,6.73E-06
5463,relation-classification0,135,Data and Task Settings,Results and Discussion,,relation-classification,0,1,1,0,,0.002976445,0,negative,0.000515695,8.76E-05,0.000250368,2.00E-05,1.61E-05,0.000393997,0.000664642,0.001380789,0.000139933,0.990500594,0.000324914,0.005555362,0.00015007
5464,relation-classification0,136,"We evaluate on three datasets : ACE05 and ACE04 for end - to - end relation extraction , and SemEval - 2010 Task 8 for relation classification .",Results and Discussion,Data and Task Settings,relation-classification,0,2,1,1,,0.008573395,0,negative,0.000747971,5.62E-06,0.000641517,8.44E-07,6.75E-07,3.37E-05,0.00046185,7.43E-05,1.26E-06,0.88115491,1.85E-05,0.116791793,6.71E-05
5465,relation-classification0,137,"We use the first two datasets as our primary target , and use the last one to thoroughly analyze and ablate the relation classification part of our model .",Results and Discussion,Data and Task Settings,relation-classification,0,3,1,0,,8.01E-05,0,negative,0.00039862,1.47E-06,0.00016619,1.25E-06,1.23E-06,2.51E-05,4.52E-05,3.31E-05,1.25E-06,0.994800723,1.75E-06,0.004503765,2.03E-05
5466,relation-classification0,138,ACE05 defines 7 coarse - grained entity types and 6 coarse - grained relation types between entities .,Results and Discussion,Data and Task Settings,relation-classification,0,4,1,0,,0.001894449,0,negative,0.002103264,1.35E-06,0.004227324,1.81E-05,6.49E-06,0.000227616,0.001861173,6.43E-05,2.38E-06,0.859915487,3.87E-05,0.128714155,0.002819659
5467,relation-classification0,139,"We use the same data splits , preprocessing , and task settings as .",Results and Discussion,Data and Task Settings,relation-classification,0,5,1,0,,8.24E-05,0,negative,0.000482508,6.93E-07,0.000174691,9.01E-07,3.71E-07,5.30E-05,0.000103081,7.27E-05,9.85E-07,0.99447746,9.41E-07,0.004615834,1.69E-05
5468,relation-classification0,140,We report the primary micro F1 -scores as well as micro precision and recall on both entity and relation extraction to better explain model performance .,Results and Discussion,Data and Task Settings,relation-classification,0,6,1,0,,5.35E-05,0,negative,0.002790154,1.35E-06,0.000213944,2.08E-06,1.84E-06,1.75E-05,8.70E-05,1.89E-05,1.18E-06,0.938710669,2.40E-06,0.058116717,3.62E-05
5469,relation-classification0,141,We treat an entity as correct when it s type and the region of its head are correct .,Results and Discussion,Data and Task Settings,relation-classification,0,7,1,0,,1.65E-05,0,negative,0.000107472,7.31E-07,0.000100359,3.16E-07,1.54E-07,3.06E-05,1.13E-05,8.10E-05,4.30E-06,0.998446726,1.26E-06,0.001198595,1.72E-05
5470,relation-classification0,142,We treat a relation as correct when it s type and argument entities are correct ; we thus treat all non-negative relations on wrong entities as false positives .,Results and Discussion,Data and Task Settings,relation-classification,0,8,1,0,,1.95E-05,0,negative,0.000226601,6.10E-07,0.000298813,6.73E-08,6.77E-08,8.02E-06,5.37E-06,2.70E-05,3.18E-06,0.997851846,5.49E-07,0.001573871,3.98E-06
5471,relation-classification0,143,"ACE04 defines the same 7 coarse - grained entity types as ACE05 , but defines 7 coarse - grained relation types .",Results and Discussion,Data and Task Settings,relation-classification,0,9,1,0,,5.60E-05,0,negative,0.000713408,7.49E-07,0.003202177,7.49E-07,1.14E-06,5.94E-05,0.000320532,5.08E-05,1.55E-06,0.96740222,2.35E-06,0.028082364,0.000162625
5472,relation-classification0,144,"We follow the cross-validation setting of Chan and and , and the preprocessing and evaluation metrics of ACE05 .",Results and Discussion,Data and Task Settings,relation-classification,0,10,1,0,,5.19E-06,0,negative,0.000178033,1.25E-06,0.000148443,1.02E-06,4.77E-07,5.66E-05,5.35E-05,9.57E-05,1.84E-06,0.997362443,8.97E-07,0.002074407,2.54E-05
5473,relation-classification0,145,SemEval-2010,Results and Discussion,,relation-classification,0,11,1,0,,0.101384947,0,negative,0.003006638,4.74E-05,0.00069497,1.08E-05,4.67E-06,0.000105612,0.002400192,0.000523069,0.000156569,0.859741287,0.001921987,0.129935342,0.001451522
5474,relation-classification0,146,Task 8 defines 9 relation types between nominals and a tenth type,Results and Discussion,SemEval-2010,relation-classification,0,12,1,0,,0.000133537,0,negative,0.000380723,9.29E-07,0.000183649,8.65E-07,5.45E-07,1.25E-05,8.58E-05,1.86E-05,4.56E-06,0.939417687,1.23E-05,0.059813872,6.80E-05
5475,relation-classification0,147,Other when two nouns have none of these relations .,Results and Discussion,SemEval-2010,relation-classification,0,13,1,0,,1.36E-05,0,negative,4.67E-05,2.10E-07,2.01E-05,9.61E-08,5.29E-08,2.49E-06,4.46E-06,7.05E-06,1.52E-06,0.997603565,6.56E-07,0.002311202,1.88E-06
5476,relation-classification0,148,"We treat this Other type as a negative relation type , and no direction is considered .",Results and Discussion,SemEval-2010,relation-classification,0,14,1,0,,1.50E-05,0,negative,0.000307742,1.45E-06,0.000218453,5.71E-08,1.12E-07,2.03E-06,4.49E-06,1.05E-05,7.63E-06,0.996046269,3.79E-07,0.003399702,1.23E-06
5477,relation-classification0,149,"The dataset consists of 8,000 training and 2,717 test sentences , and each sentence is annotated with a relation between two given nominals .",Results and Discussion,SemEval-2010,relation-classification,0,15,1,0,,0.001491125,0,negative,0.001291497,3.26E-06,0.0001774,3.11E-05,5.69E-05,4.03E-05,0.000157272,2.90E-05,3.85E-06,0.97972294,1.46E-06,0.018405187,7.98E-05
5478,relation-classification0,150,We randomly selected 800 sentences from the training set as our development set .,Results and Discussion,SemEval-2010,relation-classification,0,16,1,0,,0.002032176,0,negative,0.000208803,4.61E-06,1.69E-05,6.80E-07,1.47E-06,4.04E-05,0.0001039,0.000413756,4.79E-06,0.992953732,8.20E-07,0.006223541,2.65E-05
5479,relation-classification0,151,"We followed the official task setting , and report the official macro -averaged F1 - score ( Macro - F1 ) on the 9 relation types .",Results and Discussion,SemEval-2010,relation-classification,0,17,1,0,,5.23E-05,0,negative,4.15E-05,2.99E-07,7.52E-06,4.33E-08,1.79E-07,1.48E-06,1.12E-05,6.69E-06,2.65E-07,0.991197012,1.37E-07,0.008732384,1.26E-06
5480,relation-classification0,152,"For more details of the data and task settings , please refer to the supplementary material .",Results and Discussion,SemEval-2010,relation-classification,0,18,1,0,,1.95E-05,0,negative,3.74E-05,3.52E-07,2.79E-06,8.01E-07,1.43E-07,4.52E-06,6.02E-06,2.85E-05,1.71E-06,0.998315953,1.77E-07,0.001599572,2.05E-06
5481,relation-classification0,153,Experimental Settings,,,relation-classification,0,0,1,0,,0.000906053,0,negative,0.00032846,0.00081167,3.21E-05,0.01550276,0.000514535,0.007724999,0.001542594,0.005401491,0.000154886,0.96543545,0.002319407,0.000151089,8.06E-05
5482,relation-classification0,154,We implemented our model using the cnn library .,Experimental Settings,Experimental Settings,relation-classification,0,1,1,1,experimental-setup,0.983379171,1,experimental-setup,5.59E-07,3.38E-07,1.14E-06,1.84E-05,1.07E-06,0.994970899,0.000229695,0.003373476,2.51E-07,0.001402637,2.25E-07,1.31E-07,1.22E-06
5483,relation-classification0,155,"We parsed the texts using the Stanford neural dependency parser 7 ( Chen and Manning , 2014 ) with the original Stanford Dependencies .",Experimental Settings,Experimental Settings,relation-classification,0,2,1,1,experimental-setup,0.974899193,1,experimental-setup,1.35E-06,1.05E-06,6.88E-06,3.87E-06,8.23E-06,0.979320835,0.000952202,0.006437161,6.61E-07,0.013263336,1.01E-06,7.45E-07,2.67E-06
5484,relation-classification0,156,"Based on preliminary tuning , we fixed embedding dimensions n w to 200 , n p , n d , n e to 25 , and dimensions of intermediate layers ( n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers ) to 100 .",Experimental Settings,Experimental Settings,relation-classification,0,3,1,1,experimental-setup,0.991970548,1,experimental-setup,1.23E-06,6.91E-07,5.30E-07,2.28E-07,1.23E-07,0.932695935,0.000383351,0.06520563,5.80E-07,0.001709712,2.68E-07,6.85E-07,1.04E-06
5485,relation-classification0,157,We initialized word vectors via word2 vec trained on Wikipedia 8 and randomly initialized all other parameters .,Experimental Settings,Experimental Settings,relation-classification,0,4,1,1,experimental-setup,0.988947973,1,experimental-setup,4.76E-07,4.82E-07,4.52E-07,1.23E-07,6.58E-08,0.948369388,0.000221093,0.050104092,4.94E-07,0.001302521,1.03E-07,2.30E-07,4.81E-07
5486,relation-classification0,158,We tuned hyper - parameters using development sets for ACE05 and SemEval - 2010 Task 8 to achieve high primary ( Micro - and Macro - ) F1-scores .,Experimental Settings,Experimental Settings,relation-classification,0,5,1,0,,0.792551114,1,experimental-setup,2.11E-06,1.57E-06,9.05E-07,4.58E-07,3.93E-07,0.952211045,0.000237598,0.043091954,1.07E-06,0.004450931,2.65E-07,8.87E-07,8.11E-07
5487,relation-classification0,159,"9 For ACE04 , we directly employed the best parameters for ACE05 .",Experimental Settings,Experimental Settings,relation-classification,0,6,1,0,,0.297416931,0,experimental-setup,4.81E-06,8.75E-07,6.81E-06,1.19E-06,2.32E-06,0.967412554,0.000599199,0.010328868,6.29E-07,0.021638655,3.65E-07,2.63E-06,1.09E-06
5488,relation-classification0,160,The hyperparameter settings are shown in the supplementary material .,Experimental Settings,Experimental Settings,relation-classification,0,7,1,0,,0.627383085,1,experimental-setup,1.68E-06,1.52E-06,7.71E-07,4.38E-07,2.80E-07,0.937187681,0.000225379,0.054423244,1.52E-06,0.00815551,3.38E-07,8.81E-07,7.54E-07
5489,relation-classification0,161,For SemEval-2010,Experimental Settings,,relation-classification,0,8,1,0,,0.08350007,0,negative,0.000575929,1.97E-05,0.000834281,1.88E-05,4.19E-05,0.298952503,0.11453817,0.01095129,1.66E-05,0.567893529,0.002199713,0.003717652,0.000239819
5490,relation-classification0,162,"Task 8 , we also omitted the entity detection and label embeddings since only target nominals are annotated and the task defines no entity types .",Experimental Settings,For SemEval-2010,relation-classification,0,9,1,0,,0.006684835,0,negative,0.000471501,1.33E-06,0.000485738,3.91E-07,1.08E-05,0.008070291,0.334696011,5.57E-05,7.66E-07,0.653437053,1.16E-06,0.002725079,4.42E-05
5491,relation-classification0,163,Our statistical significance results are based on the Approximate Randomization ( AR ) test .,Experimental Settings,For SemEval-2010,relation-classification,0,10,1,0,,0.029010823,0,negative,9.51E-05,1.91E-05,0.000350844,1.69E-06,1.71E-05,0.028567528,0.152955505,0.000498652,1.52E-05,0.817205537,7.66E-06,0.000208456,5.76E-05
5492,relation-classification0,164,End - to - end Relation Extraction Results,Experimental Settings,For SemEval-2010,relation-classification,0,11,1,0,,0.665852044,1,experiments,0.000108806,6.77E-07,6.65E-05,3.01E-08,2.70E-07,0.0009264,0.924071479,1.61E-05,3.78E-07,0.064868184,8.41E-06,0.009864789,6.80E-05
5493,relation-classification0,165,"Table 1 compares our model with the state - of - theart feature - based model of on final test sets , and shows that our model performs better than the state - of - the - art model .",Experimental Settings,For SemEval-2010,relation-classification,0,12,1,1,results,0.861697729,1,experiments,0.000179617,1.99E-07,9.01E-06,9.25E-08,4.86E-07,0.001663191,0.918086928,3.74E-05,4.06E-08,0.060484214,7.19E-07,0.019478146,6.00E-05
5494,relation-classification0,166,"To analyze the contributions and effects of the various components of our end - to - end relation extraction model , we perform ablation tests on the ACE05 development set ( ) .",Experimental Settings,For SemEval-2010,relation-classification,0,13,1,1,ablation-analysis,0.179069013,0,negative,0.000568001,1.04E-05,0.000270917,1.04E-06,4.72E-05,0.005261538,0.404938004,5.13E-05,2.00E-06,0.586596147,1.13E-06,0.002187913,6.45E-05
5495,relation-classification0,167,"The performance slightly degraded without scheduled sampling , and the performance significantly degraded when we removed entity pretraining or removed both ( p < 0.05 ) .",Experimental Settings,For SemEval-2010,relation-classification,0,14,1,1,ablation-analysis,0.646236757,1,experiments,0.021620846,2.96E-06,0.000139307,6.75E-06,6.35E-05,0.008311611,0.541172245,6.82E-05,1.32E-06,0.417020398,2.72E-06,0.011267908,0.000322221
5496,relation-classification0,168,"This is reasonable because the model can only create relation instances when both of the entities are found and , without these enhancements , it may get too late to find some relations .",Experimental Settings,For SemEval-2010,relation-classification,0,15,1,0,,0.000581721,0,negative,0.000118508,5.88E-07,1.84E-05,3.56E-07,2.43E-06,0.004389843,0.015737551,4.00E-05,1.12E-06,0.979553307,1.16E-06,0.000127702,9.04E-06
5497,relation-classification0,169,Removing label embeddings did not affect 6 https://github.com/clab/cnn,Experimental Settings,For SemEval-2010,relation-classification,0,16,1,0,,0.770908515,1,experiments,0.004795001,1.23E-06,3.24E-05,2.68E-06,7.18E-06,0.006334022,0.947205503,7.73E-05,3.76E-07,0.032347075,1.16E-06,0.008740826,0.000455262
5498,relation-classification0,170,7 http://nlp.stanford.edu/software/,Experimental Settings,For SemEval-2010,relation-classification,0,17,1,0,,0.01419319,0,negative,9.07E-05,7.67E-07,0.000143538,0.000274324,0.000378966,0.079565338,0.063053305,3.97E-05,8.43E-07,0.856253767,5.16E-07,5.75E-05,0.00014073
5499,relation-classification0,171,stanford-corenlp-full-2015-04-20.zip,Experimental Settings,For SemEval-2010,relation-classification,0,18,1,0,,0.010770386,0,negative,3.39E-05,6.40E-07,6.37E-05,0.000382378,0.00019262,0.060745454,0.162464887,5.23E-05,8.83E-07,0.774830337,5.55E-06,0.000150613,0.001076767
5500,relation-classification0,172,8 https://dumps.wikimedia.org/enwiki/ 20150901/,Experimental Settings,For SemEval-2010,relation-classification,0,19,1,0,,0.022975866,0,negative,3.46E-05,6.54E-07,2.56E-05,0.006948901,0.00085563,0.180568377,0.028785032,4.30E-05,6.72E-07,0.78244218,4.54E-07,2.03E-05,0.000274727
5501,relation-classification0,173,9,Experimental Settings,For SemEval-2010,relation-classification,0,20,1,0,,4.39E-05,0,negative,4.20E-06,1.91E-07,3.71E-06,1.35E-07,4.09E-07,0.006642221,0.005173444,5.09E-05,1.84E-06,0.988110358,2.59E-07,8.67E-06,3.68E-06
5502,relation-classification0,174,"We did not tune the precision - recall trade - offs , but doing so can specifically improve precision further .",Experimental Settings,For SemEval-2010,relation-classification,0,21,1,0,,0.000799531,0,negative,0.00010822,1.77E-06,3.13E-05,2.51E-06,1.73E-05,0.043078882,0.047609829,0.00016333,1.92E-06,0.908838908,5.13E-07,0.000106026,3.95E-05
5503,relation-classification0,175,"Other work on ACE is not comparable or performs worse than the model by the entity detection performance , but this degraded the recall in relation classification .",Experimental Settings,For SemEval-2010,relation-classification,0,22,1,0,,0.002706642,0,negative,1.12E-05,2.67E-07,0.000147867,4.31E-07,3.34E-06,0.003543001,0.329201608,1.62E-05,1.65E-07,0.666364735,1.97E-05,0.000472082,0.000219419
5504,relation-classification0,176,This indicates that entity label information is helpful in detecting relations .,Experimental Settings,For SemEval-2010,relation-classification,0,23,1,0,,0.278650414,0,experiments,0.005961641,2.53E-06,0.000133844,5.72E-07,1.68E-05,0.002432843,0.547370429,3.46E-05,1.27E-06,0.429255499,6.01E-07,0.014682722,0.000106699
5505,relation-classification0,177,"We also show the performance without sharing parameters , i.e. , embedding and sequence layers , for detecting entities and relations ( ? Shared parameters ) ; we first train the entity detection model , detect entities with the model , and build a separate relation extraction model using the detected entities , i.e. , without entity detection .",Experimental Settings,For SemEval-2010,relation-classification,0,24,1,0,,0.299309171,0,experiments,0.000215506,1.07E-05,0.000490902,1.35E-07,3.22E-06,0.004651593,0.759737858,7.07E-05,5.16E-06,0.232813793,7.04E-07,0.001939586,6.01E-05
5506,relation-classification0,178,This setting can be regarded as a pipeline model since two separate models are trained sequentially .,Experimental Settings,For SemEval-2010,relation-classification,0,25,1,0,,0.003279552,0,negative,9.26E-05,3.22E-05,0.007254337,7.03E-07,6.93E-06,0.013604988,0.109674631,0.000150316,0.000741725,0.868161209,3.96E-06,0.000143929,0.000132455
5507,relation-classification0,179,"Without the shared parameters , both the performance in entity detection and relation classification drops slightly , although the differences are not significant .",Experimental Settings,For SemEval-2010,relation-classification,0,26,1,0,,0.959231909,1,experiments,0.00934803,8.27E-07,2.59E-05,3.16E-07,2.35E-06,0.00067675,0.954463091,1.84E-05,1.95E-07,0.017177734,4.53E-07,0.017945001,0.00034096
5508,relation-classification0,180,"When we removed all the enhancements , i.e. , scheduled sampling , entity pretraining , label embedding , and shared parameters , the performance is significantly worse than SP - Tree ( p < 0.01 ) , showing that these enhancements provide complementary benefits to end - to - end relation extraction .",Experimental Settings,For SemEval-2010,relation-classification,0,27,1,1,ablation-analysis,0.967997644,1,experiments,0.010909033,1.43E-06,3.92E-05,4.21E-07,4.99E-06,0.000784637,0.933485535,1.85E-05,2.94E-07,0.025895222,3.49E-07,0.02851142,0.000349004
5509,relation-classification0,181,"Next , we show the performance with different LSTM - RNN structures in .",Experimental Settings,For SemEval-2010,relation-classification,0,28,1,0,,0.205949593,0,negative,0.000181866,7.51E-07,1.32E-05,2.39E-07,3.65E-06,0.003731204,0.185489583,4.50E-05,3.83E-07,0.80863173,1.97E-07,0.001880253,2.20E-05
5510,relation-classification0,182,"We first compare the three input dependency structures ( SPTree , SubTree , FullTree ) for tree - structured LSTM - RNNs .",Experimental Settings,For SemEval-2010,relation-classification,0,29,1,0,,0.42181189,0,experiments,0.000133107,2.49E-05,0.00034917,1.77E-07,6.67E-06,0.006589368,0.680008654,9.29E-05,5.96E-06,0.31137207,1.04E-06,0.001343434,7.26E-05
5511,relation-classification0,183,"Performances on these three structures are almost same when we distinguish the nodes in the shortest paths from other nodes , but when we do not distinguish them ( - SP ) , the information outside of the shortest path , i.e. , FullTree ( - SP ) , significantly hurts performance ( p < 0.05 ) .",Experimental Settings,For SemEval-2010,relation-classification,0,30,1,0,,0.936040611,1,experiments,0.001062883,4.12E-07,2.35E-05,5.90E-08,1.08E-06,0.000431575,0.96096983,8.19E-06,1.01E-07,0.024316559,1.50E-07,0.013066586,0.000119059
5512,relation-classification0,184,We then compare our tree - structured LSTM - RNN ( SPTree ) with the Child - Sum treestructured LSTM - RNN on the shortest path of .,Experimental Settings,For SemEval-2010,relation-classification,0,31,1,0,,0.069784324,0,experiments,5.09E-05,8.61E-06,0.001094357,6.38E-08,3.37E-06,0.007405675,0.766374709,6.31E-05,7.33E-06,0.224621623,2.23E-07,0.000308898,6.12E-05
5513,relation-classification0,185,"Child - Sum performs worse than our SPTree model , but not with as big of a decrease as above .",Experimental Settings,For SemEval-2010,relation-classification,0,32,1,0,,0.405211254,0,experiments,0.000166442,8.92E-08,6.26E-06,1.72E-07,7.03E-07,0.000990992,0.985629184,1.37E-05,1.84E-08,0.006612466,4.73E-08,0.006295967,0.000283966
5514,relation-classification0,186,This maybe because the difference in the models appears only on nodes that have multiple children and all the nodes except for the least common node have one child .,Experimental Settings,For SemEval-2010,relation-classification,0,33,1,0,,0.000258046,0,negative,4.05E-05,5.06E-07,3.81E-05,1.15E-07,2.22E-06,0.002484064,0.022276775,1.72E-05,1.22E-06,0.975042478,1.26E-07,8.59E-05,1.08E-05
5515,relation-classification0,187,We finally show results with two counterparts of sequence - based LSTM - RNNs using the shortest path ( last two rows in ) .,Experimental Settings,For SemEval-2010,relation-classification,0,34,1,0,,0.19614626,0,negative,0.00012933,8.43E-07,0.000102928,4.15E-08,2.07E-06,0.001234245,0.413065488,1.00E-05,6.39E-07,0.581999568,1.14E-07,0.003425342,2.94E-05
5516,relation-classification0,188,SPSeq is a bidirectional LSTM - RNN on the shortest path .,Experimental Settings,For SemEval-2010,relation-classification,0,35,1,0,,0.2977098,0,experiments,3.77E-05,1.25E-05,0.105907625,1.84E-07,3.74E-06,0.006222249,0.828578958,4.47E-05,0.000126889,0.058279562,8.87E-07,0.000176704,0.000608224
5517,relation-classification0,189,The LSTM unit receives input from the sequence layer concatenated with embeddings for the surrounding dependency types and directions .,Experimental Settings,For SemEval-2010,relation-classification,0,36,1,0,,0.033082813,0,negative,8.52E-05,2.01E-05,0.00153672,1.58E-06,9.28E-06,0.094154745,0.385232965,0.001519658,0.000819247,0.515698305,7.68E-07,3.79E-05,0.000883503
5518,relation-classification0,190,We concatenate the outputs of the two RNNs for the relation candidate .,Experimental Settings,For SemEval-2010,relation-classification,0,37,1,0,,0.001765798,0,negative,4.75E-05,9.04E-06,0.001471116,1.34E-07,3.52E-06,0.01506006,0.139820331,0.000203719,0.000225187,0.842996763,1.77E-07,4.06E-05,0.00012186
5519,relation-classification0,191,SPX u is our adaptation of the shortest path LSTM - RNN proposed by to match our sequence - layer based model .,Experimental Settings,For SemEval-2010,relation-classification,0,38,1,0,,0.002522401,0,experiments,9.59E-05,8.43E-06,0.00666247,8.30E-07,2.91E-05,0.016238734,0.818294225,9.92E-05,1.76E-05,0.157793036,1.09E-07,0.000254753,0.000505532
5520,relation-classification0,192,11 This has two LSTM - RNNs for the left and right subpaths of the shortest path .,Experimental Settings,For SemEval-2010,relation-classification,0,39,1,0,,0.00201236,0,negative,4.66E-05,2.73E-06,0.009521705,3.15E-07,1.16E-05,0.01700172,0.302827002,5.80E-05,3.92E-05,0.670185004,1.16E-07,9.73E-05,0.000208672
5521,relation-classification0,193,"We first calculate the max pooling of the LSTM units for each of these two RNNs , and then concatenate the outputs of the pooling for the relation candidate .",Experimental Settings,For SemEval-2010,relation-classification,0,40,1,0,,0.011283729,0,negative,0.000137461,2.31E-05,0.003045982,1.74E-07,7.07E-06,0.014523724,0.361284475,0.000184793,0.000245113,0.620207299,1.96E-07,0.000104637,0.00023595
5522,relation-classification0,194,The comparison with these sequence - based LSTM - RNNs indicates that a tree - structured LSTM - RNN is comparable to sequence - based ones in representing shortest paths .,Experimental Settings,For SemEval-2010,relation-classification,0,41,1,0,,0.664382738,1,experiments,0.000191052,1.81E-07,2.93E-05,1.38E-08,4.21E-07,0.000466972,0.909965078,6.06E-06,7.16E-08,0.077572555,6.08E-08,0.011713282,5.49E-05
5523,relation-classification0,195,"Overall , the performance comparison of the LSTM - RNN structures in show that for end - to - end relation extraction , selecting the appropriate tree structure representation of the input ( i.e. , the shortest path ) is more important than the choice of the LSTM - RNN structure on that input ( i.e. , sequential versus tree - based ) .",Experimental Settings,For SemEval-2010,relation-classification,0,42,1,0,,0.704562612,1,experiments,0.000479607,2.31E-07,9.14E-06,5.16E-08,8.97E-07,0.000357796,0.950705929,6.82E-06,4.66E-08,0.030465964,8.31E-08,0.017833681,0.000139754
5524,relation-classification0,196,Relation Classification Analysis Results,,,relation-classification,0,0,1,0,,0.002611977,0,negative,7.13E-05,3.19E-05,1.68E-05,7.06E-07,8.17E-07,4.69E-05,0.000196491,0.000326294,1.74E-05,0.980455424,0.016464721,0.002363242,8.05E-06
5525,relation-classification0,197,"To thoroughly analyze the relation classification part alone , e.g. , comparing different LSTM structures , architecture components such as hidden layers and input information , and classification task settings , we use the SemEval - 2010 Task 8 .",Relation Classification Analysis Results,Relation Classification Analysis Results,relation-classification,0,1,1,0,,0.000248753,0,negative,0.000292652,3.32E-05,0.000165476,1.73E-07,4.08E-06,8.29E-06,5.09E-05,2.83E-05,2.86E-06,0.998871453,9.71E-06,0.000532511,4.06E-07
5526,relation-classification0,198,"This dataset , often used to evaluate NN models for relation classification , annotates only relation - related nominals ( unlike ACE datasets ) , so we can focus cleanly on the relation classification part .",Relation Classification Analysis Results,Relation Classification Analysis Results,relation-classification,0,2,1,0,,0.000753952,0,negative,0.000284121,2.19E-06,0.000126255,2.40E-06,4.11E-05,1.38E-05,2.85E-05,4.12E-06,7.45E-07,0.9993911,2.06E-06,0.00010323,4.80E-07
5527,relation-classification0,199,Settings,Relation Classification Analysis Results,,relation-classification,0,3,1,0,,1.63E-05,0,negative,1.27E-05,5.32E-06,4.11E-06,2.34E-07,2.41E-07,1.37E-05,5.12E-06,5.91E-05,5.85E-06,0.999868447,4.06E-06,2.10E-05,1.60E-07
5528,relation-classification0,200,Macro - F1 No External Knowledge Resources Our Model ( SPTree ) 0.844 dos 0.841 0.840 + Word,Relation Classification Analysis Results,Settings,relation-classification,0,4,1,0,,0.000174605,0,negative,0.000396772,2.22E-05,6.54E-05,1.22E-06,1.23E-05,0.002601539,0.000244963,0.000698752,1.49E-05,0.995783361,7.54E-05,8.15E-05,1.76E-06
5529,relation-classification0,201,Net Our Model ( SPTree + WordNet ) 0.855 0.856 0.837 We first report official test set results in Table 4 .,Relation Classification Analysis Results,Settings,relation-classification,0,5,1,0,,0.007363265,0,negative,0.001003931,2.43E-05,4.12E-05,2.19E-06,2.28E-05,0.000509597,0.000900603,0.000163419,3.54E-06,0.995042187,0.000813695,0.001467707,4.88E-06
5530,relation-classification0,202,"Our novel LSTM - RNN model is comparable to both the state - of - the - art CNN - based models on this task with or without external sources , i.e. , WordNet , unlike the previous best LSTM - RNN model .",Relation Classification Analysis Results,Settings,relation-classification,0,6,1,0,,0.512656049,1,negative,0.009815359,0.001516009,0.000335121,3.53E-05,0.000178143,0.002012915,0.007493132,0.001697069,0.000142185,0.931481948,0.009266409,0.035891579,0.000134875
5531,relation-classification0,203,"Next , we compare different LSTM - RNN structures in .",Relation Classification Analysis Results,Settings,relation-classification,0,7,1,0,,0.092584021,0,negative,0.000147999,6.86E-05,5.16E-05,4.20E-08,1.94E-06,6.43E-05,1.72E-05,4.84E-05,1.20E-05,0.999523608,2.22E-05,4.20E-05,5.32E-08
5532,relation-classification0,204,"As for the three input dependency structures ( SPTree , SubTree , FullTree ) , Full",Relation Classification Analysis Results,Settings,relation-classification,0,8,1,0,,0.003071961,0,negative,0.000272782,0.000147746,0.003132567,1.56E-07,7.67E-06,0.000283768,9.87E-05,7.86E-05,0.000138767,0.995646565,0.000116084,7.61E-05,4.60E-07
5533,relation-classification0,205,"Tree performs significantly worse than other structures regardless of whether or not we distinguish the nodes in the shortest paths from the other nodes , which hints that the information outside of the shortest path significantly hurts the performance ( p < 0.05 ) .",Relation Classification Analysis Results,Settings,relation-classification,0,9,1,0,,0.855042044,1,negative,0.029604187,0.000169139,0.000206958,7.10E-06,3.57E-05,0.001405687,0.006026753,0.000783124,1.36E-05,0.920775386,0.001811401,0.039123337,3.76E-05
5534,relation-classification0,206,We also compare our treestructured LSTM - RNN ( SPTree ) with sequencebased LSTM - RNNs ( SPSeq and SPXu ) and treestructured LSTM - RNNs ( Child - Sum ) .,Relation Classification Analysis Results,Settings,relation-classification,0,10,1,0,,0.576031931,1,negative,0.000585623,0.008005696,0.023409374,1.65E-06,0.0001144,0.00112792,0.000873216,0.000599457,0.001746666,0.962916816,0.000419654,0.000194478,5.05E-06
5535,relation-classification0,207,"All these LSTM - RNNs perform slightly worse than our SP - 12 When incorporating WordNet information into our model , we prepared embeddings for WordNet hypernyms extracted by SuperSenseTagger and concatenated the embeddings to the input vector ( the concatenation of word and POS embeddings ) of the sequence LSTM .",Relation Classification Analysis Results,Settings,relation-classification,0,11,1,0,,0.297556312,0,negative,0.018372193,0.000241227,0.0001987,2.50E-05,0.000173805,0.004546302,0.00900322,0.001748514,1.63E-05,0.9370169,0.000728974,0.027856705,7.22E-05
5536,relation-classification0,208,We tuned the dimension of the WordNet embeddings and set it to 15 using the development dataset .,Relation Classification Analysis Results,Settings,relation-classification,0,12,1,0,,0.120550166,0,negative,0.000187811,0.002068026,2.89E-05,4.11E-05,0.000101286,0.174435031,0.001373446,0.205519702,0.000699547,0.615398184,8.93E-05,1.64E-05,4.13E-05
5537,relation-classification0,209,"0.848 produces different results on FullTree as compared to the results on ACE05 in , the trend still holds that selecting the appropriate tree structure representation of the input is more important than the choice of the LSTM - RNN structure on that input .",Relation Classification Analysis Results,Settings,relation-classification,0,13,1,0,,0.065045172,0,negative,0.099413394,7.65E-05,7.31E-05,2.20E-06,2.96E-05,0.000329908,0.001357656,0.00023671,7.98E-06,0.881413071,0.000157165,0.016895443,7.24E-06
5538,relation-classification0,210,"Finally , summarizes the contribution of several model components and training settings on SemEval relation classification .",Relation Classification Analysis Results,Settings,relation-classification,0,14,1,0,,6.83E-05,0,negative,0.000146492,4.75E-05,9.23E-06,5.52E-06,7.16E-05,8.49E-05,7.04E-06,1.65E-05,1.76E-05,0.999536606,4.85E-05,8.02E-06,5.02E-07
5539,relation-classification0,211,"We first remove the hidden layer by directly connecting the LSTM - RNN layers to the softmax layers , and found that this slightly degraded performance , but the difference was small .",Relation Classification Analysis Results,Settings,relation-classification,0,15,1,0,,0.080404463,0,negative,0.035702082,0.000299705,7.59E-05,1.85E-05,0.00030539,0.000885077,0.000103459,0.00019874,9.90E-05,0.96212676,1.75E-05,0.000163845,4.11E-06
5540,relation-classification0,212,We then skip the sequence layer and directly use the word and POS embeddings for the dependency layer .,Relation Classification Analysis Results,Settings,relation-classification,0,16,1,0,,0.026642983,0,negative,0.004524584,0.006490386,0.003899127,3.01E-06,5.21E-05,0.000954369,0.000113449,0.000819283,0.04649802,0.936547659,5.57E-05,3.46E-05,7.64E-06
5541,relation-classification0,213,"Removing the sequence layer 13 or entity - related information from the sequence layer ( ? Pair ) slightly degraded performance , and , on removing both , the performance dropped significantly ( p < 0.05 ) .",Relation Classification Analysis Results,Settings,relation-classification,0,17,1,0,,0.115486055,0,ablation-analysis,0.552847164,9.14E-05,0.000138603,1.35E-05,0.000325774,0.00033972,0.000445404,5.24E-05,2.09E-05,0.443448465,2.31E-05,0.002247146,6.46E-06
5542,relation-classification0,214,This indicates that the sequence layer is necessary but the last words of nominals are almost enough for expressing the relations in this task .,Relation Classification Analysis Results,Settings,relation-classification,0,18,1,0,,6.99E-05,0,negative,0.001938921,2.05E-05,1.27E-05,2.99E-07,1.59E-05,6.03E-05,1.07E-05,2.18E-05,1.26E-05,0.997867167,3.55E-06,3.53E-05,1.49E-07
5543,relation-classification0,215,"When we replace the Stanford neural dependency parser with the Stanford lexicalized PCFG parser ( Stanford PCFG ) , the performance slightly dropped , but the difference was small .",Relation Classification Analysis Results,Settings,relation-classification,0,19,1,0,,0.191803459,0,negative,0.028283461,2.18E-05,5.34E-05,1.28E-06,9.34E-05,9.69E-05,0.000172168,1.90E-05,4.61E-06,0.970214254,1.97E-05,0.001017803,2.07E-06
5544,relation-classification0,216,This indicates that the selection of parsing models is not critical .,Relation Classification Analysis Results,Settings,relation-classification,0,20,1,0,,0.00057612,0,negative,0.000867231,7.30E-06,3.61E-06,1.76E-07,1.15E-05,2.68E-05,6.10E-06,1.02E-05,2.63E-06,0.999040345,1.61E-06,2.25E-05,6.61E-08
5545,relation-classification0,217,"We also included WordNet , and this slightly improved the performance ( + WordNet ) , but the difference was small .",Relation Classification Analysis Results,Settings,relation-classification,0,21,1,0,,0.001715014,0,negative,0.031235526,9.01E-05,0.000330378,6.39E-06,0.000320474,0.001844337,0.001174979,0.000193521,2.67E-05,0.962876491,1.94E-05,0.001870246,1.15E-05
5546,relation-classification0,218,"Lastly , for the generation of relation candidates , generating only leftto - right candidates slightly degraded the perfor- mance , but the difference was small and hence the creation of right - to - left candidates was not critical .",Relation Classification Analysis Results,Settings,relation-classification,0,22,1,0,,0.718816736,1,negative,0.436564173,7.11E-05,6.14E-05,1.46E-05,0.000213495,0.000518636,0.002006392,0.000143312,5.64E-06,0.546316291,5.68E-05,0.014003803,2.44E-05
5547,relation-classification0,219,"Treating the inverse relation candidate as a negative instance ( Negative sampling ) also performed comparably to other generation methods in our model , which showed a significance improvement over generating only left - to - right candidates ) .",Relation Classification Analysis Results,Settings,relation-classification,0,23,1,0,,0.154881583,0,negative,0.032253758,8.46E-05,6.18E-05,3.77E-06,9.15E-05,0.000551756,0.002021877,0.000194115,6.33E-06,0.9469667,4.55E-05,0.017707062,1.13E-05
5548,relation-classification0,220,Conclusion,,,relation-classification,0,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
5549,text_generation5,1,title,,,text_generation,5,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
5550,text_generation5,2,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,title,title,text_generation,5,1,1,1,research-problem,0.998764759,1,research-problem,6.86E-08,3.33E-05,2.28E-07,8.96E-08,6.60E-08,2.50E-07,1.30E-06,5.09E-06,7.75E-06,0.002981541,0.996969938,3.53E-07,7.84E-08
5551,text_generation5,3,abstract,,,text_generation,5,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
5552,text_generation5,4,"Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .",abstract,abstract,text_generation,5,1,1,1,research-problem,0.714471352,1,research-problem,3.89E-08,8.98E-06,1.37E-08,4.08E-07,1.46E-07,2.49E-07,3.78E-07,2.15E-06,3.43E-07,0.015538153,0.984449061,4.33E-08,3.57E-08
5553,text_generation5,5,"This negative result is so far poorly understood , but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder .",abstract,abstract,text_generation,5,2,1,0,,0.003517813,0,research-problem,3.91E-07,9.32E-05,9.30E-08,2.54E-06,2.39E-06,5.03E-06,1.05E-06,4.04E-05,2.12E-05,0.492171334,0.507662099,1.86E-07,1.17E-07
5554,text_generation5,6,"In this paper , we experiment with a new type of decoder for VAE : a dilated CNN .",abstract,abstract,text_generation,5,3,1,0,,0.811981288,1,research-problem,3.15E-05,0.224574033,0.000269319,2.25E-05,0.000388005,3.32E-05,4.23E-05,0.000395465,0.00834402,0.120706561,0.645166253,2.16E-05,5.13E-06
5555,text_generation5,7,"By changing the decoder 's dilation architecture , we control the size of context from previously generated words .",abstract,abstract,text_generation,5,4,1,0,,0.186446022,0,negative,8.19E-05,0.120391356,2.97E-05,4.59E-05,0.000468748,5.42E-05,7.52E-06,0.000806665,0.015584985,0.82329602,0.039221491,9.69E-06,1.85E-06
5556,text_generation5,8,"In experiments , we find that there is a trade - off between contextual capacity of the decoder and effective use of encoding information .",abstract,abstract,text_generation,5,5,1,0,,0.012533567,0,negative,1.39E-05,0.008677893,1.04E-06,3.12E-05,6.17E-05,4.77E-05,4.77E-06,0.000899793,0.000656544,0.835826942,0.153773227,4.16E-06,1.06E-06
5557,text_generation5,9,"We show that when carefully managed , VAEs can outperform LSTM language models .",abstract,abstract,text_generation,5,6,1,0,,0.365411831,0,negative,7.46E-05,0.022307175,1.14E-05,1.33E-05,9.02E-05,2.51E-05,2.88E-05,0.000600236,0.000960139,0.534511373,0.44128006,9.48E-05,2.85E-06
5558,text_generation5,10,"We demonstrate perplexity gains on two datasets , representing the first positive language modeling result with VAE .",abstract,abstract,text_generation,5,7,1,0,,0.021204143,0,negative,5.34E-05,0.061952677,2.79E-05,1.75E-05,0.000429076,2.48E-05,7.07E-05,0.00059979,0.000853771,0.645984201,0.289814652,0.000167025,4.48E-06
5559,text_generation5,11,"Further , we conduct an in - depth investigation of the use of VAE ( with our new decoding architecture ) for semi-supervised and unsupervised labeling tasks , demonstrating gains over several strong baselines .",abstract,abstract,text_generation,5,8,1,0,,0.021136991,0,negative,0.000147252,0.127085869,5.36E-05,7.96E-05,0.002946737,5.04E-05,4.25E-05,0.000462412,0.001324141,0.827604181,0.040149154,5.08E-05,3.28E-06
5560,text_generation5,12,Introduction,,,text_generation,5,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
5561,text_generation5,13,"Generative models play an important role in NLP , both in their use as language models and because of their ability to effectively learn from unlabeled data .",Introduction,Introduction,text_generation,5,1,1,0,,0.194472236,0,research-problem,7.01E-07,0.000183891,3.27E-07,2.42E-06,4.80E-06,4.83E-06,8.15E-06,7.35E-06,8.89E-05,0.05497976,0.944716945,9.27E-07,9.77E-07
5562,text_generation5,14,"By parameterzing generative models using neural nets , recent work has proposed model classes thatare particularly expressive and can pontentially model a wide range of phenomena in language and other modalities .",Introduction,Introduction,text_generation,5,2,1,0,,0.482155224,0,research-problem,7.41E-06,0.004752236,2.55E-06,2.32E-05,6.98E-05,4.04E-05,2.03E-05,8.48E-05,0.002992838,0.293274583,0.698724609,4.18E-06,3.15E-06
5563,text_generation5,15,We focus on a specific instance 1 Carnegie Mellon University .,Introduction,Introduction,text_generation,5,3,1,0,,0.01741853,0,negative,9.53E-05,0.118217637,5.36E-05,6.82E-05,0.007403405,8.32E-05,6.74E-05,0.000123209,0.015232704,0.810182291,0.048421604,4.60E-05,5.30E-06
5564,text_generation5,16,Correspondence to : Zichao Yang < zichaoy@cs.cmu.edu >.,Introduction,,text_generation,5,4,1,0,,0.022820472,0,negative,3.07E-05,0.002134783,1.02E-05,0.027615814,0.004287085,0.001722195,0.00011894,0.000275637,0.001980925,0.826981745,0.134800781,3.88E-06,3.73E-05
5565,text_generation5,17,"Proceedings of the 34 th International Conference on Machine Learning , Sydney , Australia , PMLR 70 , 2017 .",Introduction,Correspondence to : Zichao Yang < zichaoy@cs.cmu.edu >.,text_generation,5,5,1,0,,0.032140836,0,negative,3.73E-05,0.001850863,2.65E-05,0.00016589,0.001819186,0.000342551,0.000164437,4.85E-05,0.003112003,0.587652335,0.404750674,1.32E-05,1.65E-05
5566,text_generation5,18,Copyright 2017 by the author ( s ) .,Introduction,,text_generation,5,6,1,0,,0.024087527,0,negative,8.32E-06,0.019731072,5.45E-06,1.80E-05,8.95E-05,0.000741293,6.91E-05,0.00173622,0.174854176,0.771266412,0.031470374,3.96E-06,6.09E-06
5567,text_generation5,19,of this class : the variational autoencoder 1 ( VAE ) .,Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,7,1,0,,0.054431441,0,negative,3.87E-06,0.005179606,4.36E-05,2.89E-05,0.000158853,0.000257755,4.40E-05,0.000415583,0.006860235,0.542763568,0.444231973,5.12E-06,6.97E-06
5568,text_generation5,20,The generative story behind the VAE ( to be described in detail in the next section ) is simple :,Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,8,1,0,,0.155772203,0,negative,6.58E-05,0.061994969,0.000307688,5.68E-06,0.000668403,9.77E-05,1.90E-05,0.000224054,0.281437244,0.62510006,0.030051207,2.57E-05,2.58E-06
5569,text_generation5,21,"First , a continuous latent representation is sampled from a multivariate Gaussian .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,9,1,0,,0.833552617,1,model,4.34E-05,0.336204507,0.000198567,5.13E-06,0.001237105,7.11E-05,1.43E-05,0.00031878,0.630206155,0.031134551,0.000557874,6.44E-06,2.09E-06
5570,text_generation5,22,"Then , an output is sampled from a distribution parameterized by a neural decoder , conditioned on the latent representation .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,10,1,0,,0.517471187,1,model,4.66E-06,0.051080489,8.01E-05,6.55E-07,0.000119544,3.05E-05,2.70E-06,0.0001671,0.857168501,0.090533674,0.000810481,1.13E-06,4.00E-07
5571,text_generation5,23,"The latent representation ( treated as a latent variable during training ) is intended to give the model more expressive capacity when compared with simpler neural generative models - for example , conditional language models .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,11,1,0,,0.754454245,1,model,2.30E-05,0.139937725,9.90E-05,6.87E-06,0.000767621,0.000128501,8.89E-06,0.000654355,0.785365151,0.072611978,0.000391141,4.08E-06,1.70E-06
5572,text_generation5,24,"The choice of decoding architecture and final output distribution , which connect the latent representation to output , depends on the kind of data being modeled .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,12,1,0,,0.069971857,0,negative,8.69E-06,0.012426873,4.41E-06,0.000196427,0.001387691,0.0004812,1.31E-05,0.001063234,0.005659016,0.967423425,0.011326025,6.05E-06,3.85E-06
5573,text_generation5,25,The VAE owes it s name to an accompanying variational technique ) that has been successfully used to train such models on image data .,Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,13,1,0,,0.346942842,0,negative,3.76E-05,0.083216229,0.000669505,7.31E-05,0.004177854,0.000215858,5.62E-05,0.000305454,0.054410258,0.693276842,0.163515531,3.49E-05,1.07E-05
5574,text_generation5,26,The application of VAEs to text data has been far less successful .,Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,14,1,0,,0.215900985,0,research-problem,2.00E-06,0.000553884,2.48E-06,2.27E-05,0.000208802,7.04E-05,3.53E-05,7.52E-05,8.88E-05,0.370914059,0.628015612,6.23E-06,4.52E-06
5575,text_generation5,27,"The obvious choice for decoding architecture for a textual VAE is an LSTM , a typical workhorse in NLP .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,15,1,0,,0.19730604,0,negative,5.58E-06,0.005587259,2.52E-05,0.000123803,0.000565477,0.000398442,5.83E-05,0.000469437,0.005904052,0.565495654,0.421343856,7.63E-06,1.53E-05
5576,text_generation5,28,"However , found that using an LSTM - VAE for text modeling yields higher perplexity on held - out data than using an LSTM language model .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,16,1,0,,0.016979005,0,negative,2.00E-05,0.001220973,2.87E-05,5.33E-05,0.002284502,0.000457644,0.000131591,0.000318761,0.000297831,0.947084351,0.048022315,6.88E-05,1.13E-05
5577,text_generation5,29,"In particular , they observe that the LSTM decoder in VAE does not make effective use of the latent representation during training and , as a result , VAE collapses into a simple language model .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,17,1,0,,0.08833632,0,negative,0.000129982,0.022722578,5.00E-05,5.09E-05,0.002796225,0.000170086,3.45E-05,0.000320224,0.004826498,0.940972716,0.027862304,5.93E-05,4.76E-06
5578,text_generation5,30,Related work has used simpler decoders that model text as a bag of words .,Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,18,1,0,,0.034235372,0,negative,6.00E-06,0.002353093,1.37E-05,0.000113973,0.000860651,0.000484397,5.11E-05,0.000427438,0.001053037,0.888492101,0.106125673,8.11E-06,1.07E-05
5579,text_generation5,31,"Their results indicate better use of latent representations , but their decoders can not effectively model longer - range dependencies in text and thus underperform in terms of final perplexity .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,19,1,0,,0.005100994,0,negative,6.08E-05,0.002828985,2.54E-05,1.81E-05,0.001798963,0.000157883,4.64E-05,0.000183734,0.000631202,0.986545314,0.007578555,0.000121239,3.46E-06
5580,text_generation5,32,"Motivated by these observations , we hypothesize that the contextual capacity of the decoder plays an important role in whether VAEs effectively condition on the latent representation when trained on text data .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,20,1,0,,0.215908946,0,negative,8.79E-05,0.238991293,9.87E-05,3.44E-05,0.005485319,0.000178722,2.15E-05,0.000616515,0.218057128,0.533194038,0.003187177,4.27E-05,4.71E-06
5581,text_generation5,33,"We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,21,1,1,model,0.9417719,1,approach,9.72E-05,0.529230241,0.002016012,0.000137964,0.014866429,0.000334978,0.000126869,0.000513863,0.36011978,0.081124403,0.01132044,7.37E-05,3.81E-05
5582,text_generation5,34,"In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,22,1,1,model,0.797440441,1,approach,6.18E-05,0.649007416,0.000228883,2.19E-05,0.005785929,0.000151904,3.53E-05,0.000634302,0.294847823,0.048935436,0.000262974,2.16E-05,4.62E-06
5583,text_generation5,35,"In the two extremes , depending on the choice of dilation , the CNN decoder can reproduce a simple MLP using a bags of words representation of text , or can reproduce the long - range dependence of recurrent architectures ( like an LSTM ) by conditioning on the entire history .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,23,1,0,,0.773042253,1,model,2.63E-05,0.06350705,0.000680423,1.64E-06,0.000533794,4.27E-05,1.60E-05,9.73E-05,0.758334966,0.175109327,0.001627214,2.17E-05,1.61E-06
5584,text_generation5,36,"Thus , by choosing a dilated CNN as the decoder , we are able to conduct experiments where we vary contextual capacity , finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,24,1,0,,0.801891876,1,approach,0.000368558,0.485797617,0.000174166,7.15E-05,0.021590609,0.000308968,8.25E-05,0.00107106,0.178707499,0.31111786,0.000505376,0.000194197,1.00E-05
5585,text_generation5,37,"We demonstrate that when this trade - off is correctly managed , textual VAEs can perform substantially better than simple LSTM language models , a finding consistent with recent image modeling experiments using variational lossy autoencoders .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,25,1,0,,0.602162907,1,approach,0.00057975,0.433831894,0.000141925,6.06E-05,0.005458199,0.000356906,0.000214922,0.002367313,0.132312672,0.41626795,0.007674344,0.000705354,2.81E-05
5586,text_generation5,38,"We goon to show that VAEs with carefully selected CNN decoders can be quite effective for semi-supervised classification and unsupervised clustering , outperforming several strong baselines ( from ) on both text categorization and sentiment analysis .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,26,1,0,,0.686324742,1,negative,0.000741984,0.354505181,0.000734795,5.58E-05,0.01679674,0.000347058,0.00079226,0.001006402,0.056081083,0.551055209,0.014880325,0.002960908,4.23E-05
5587,text_generation5,39,Our contributions are as follows :,Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,27,1,0,,0.002238597,0,negative,7.80E-06,0.004352199,1.31E-05,0.000555337,0.008885094,0.000647459,3.11E-05,0.000351109,0.001923769,0.980738556,0.002481336,5.01E-06,8.11E-06
5588,text_generation5,40,"First , we propose the use of a dilated CNN as a new decoder for VAE .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,28,1,0,,0.937299698,1,approach,0.000272285,0.720281508,0.000827742,0.000102542,0.02343715,0.000243457,0.000148029,0.000592994,0.158771579,0.093663443,0.00149056,0.000147539,2.12E-05
5589,text_generation5,41,"We then empirically evaluate several dilation architectures with different capacities , finding that reduced contextual capacity leads to stronger reliance on latent representations .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,29,1,0,,0.456975719,0,approach,0.00020456,0.542745989,0.000231998,1.71E-05,0.01005242,0.000334632,0.000135465,0.001300721,0.131018293,0.313313078,0.000434517,0.000203171,8.07E-06
5590,text_generation5,42,"By picking a decoder with suitable contextual capacity , we find our VAE performs better than LSTM language models on two data sets .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,30,1,0,,0.089114293,0,negative,0.000366014,0.049853703,4.07E-05,0.000243617,0.008904706,0.003301876,0.001581415,0.01593312,0.005662401,0.9088504,0.002526502,0.002630918,0.000104662
5591,text_generation5,43,We also explore the use of dilated CNN VAEs for semi-supervised classification and find they perform better than strong baselines from .,Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,31,1,0,,0.198928818,0,negative,0.000732154,0.293881116,0.000412594,9.75E-05,0.024552167,0.001273437,0.001367951,0.003841498,0.020786762,0.648597759,0.001500957,0.002897552,5.85E-05
5592,text_generation5,44,"Finally , we verify that the same framework can be used effectively for unsupervised clustering .",Introduction,Copyright 2017 by the author ( s ) .,text_generation,5,32,1,0,,0.026145612,0,negative,7.34E-05,0.038452557,2.82E-05,5.62E-05,0.003764307,0.000405464,9.35E-05,0.001305697,0.022652302,0.931267187,0.001755581,0.000135746,9.87E-06
5593,text_generation5,45,Model,,,text_generation,5,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
5594,text_generation5,46,"In this section , we begin by providing background on the use of variational autoencoders for language modeling .",Model,Model,text_generation,5,1,1,0,,0.007757243,0,negative,0.000310774,0.003860281,0.000314972,0.000112816,9.89E-05,0.000484512,0.000291301,0.003828959,0.017322702,0.971048793,0.001095718,0.001120015,0.000110257
5595,text_generation5,47,Then we introduce the dilated CNN architecture that we will use as a new decoder for VAE in experiments .,Model,Model,text_generation,5,2,1,0,,0.055263208,0,negative,0.000478615,0.001605302,0.001287169,4.96E-06,1.84E-05,5.06E-05,4.87E-05,0.000542315,0.041796667,0.953082333,0.000100527,0.000971015,1.34E-05
5596,text_generation5,48,"Finally , we describe the generalization of VAE that we will use to conduct experiments on semi-supervised classification .",Model,Model,text_generation,5,3,1,0,,0.001253206,0,negative,0.000498569,0.0020439,0.001347895,1.12E-05,7.63E-05,6.70E-05,0.000100332,0.000562074,0.012018784,0.9809405,8.64E-05,0.002229491,1.76E-05
5597,text_generation5,49,Background on Variational Autoencoders,,,text_generation,5,0,1,0,,0.163508318,0,research-problem,2.05E-05,0.000135798,7.20E-05,1.81E-06,6.18E-07,4.91E-05,0.000308497,0.00043911,5.18E-05,0.392136228,0.606407926,0.000356474,2.02E-05
5598,text_generation5,145,Experiments,,,text_generation,5,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
5599,text_generation5,146,Data sets,Experiments,,text_generation,5,1,1,0,,0.000951646,0,negative,4.33E-05,1.60E-05,0.00076108,8.36E-07,2.70E-06,0.00156173,0.003291912,0.008266378,4.75E-06,0.983366758,9.10E-05,0.002552389,4.12E-05
5600,text_generation5,147,"Since we would like to investigate VAEs for language modeling and semi-supervised classification , the data sets should be suitable for both purposes .",Experiments,Data sets,text_generation,5,2,1,0,,0.001847785,0,negative,3.99E-06,1.53E-07,2.73E-06,9.16E-09,1.85E-08,1.26E-05,3.82E-06,2.47E-05,4.44E-08,0.999887854,2.76E-06,6.12E-05,4.80E-08
5601,text_generation5,148,"We use two large scale document classification data sets : Yahoo Answer and Yelp15 review , representing topic classification and sentiment classification data sets respectively ..",Experiments,Data sets,text_generation,5,3,1,0,,0.002304431,0,negative,4.11E-05,8.62E-06,0.000204409,2.28E-06,7.99E-06,0.000478753,0.000365121,0.000247754,1.04E-06,0.998314554,1.81E-05,0.000305641,4.63E-06
5602,text_generation5,149,We report negative log likelihood ( NLL ) and perplexity ( PPL ) on the test set .,Experiments,Data sets,text_generation,5,4,1,0,,0.000801485,0,negative,9.30E-05,7.60E-07,2.73E-05,2.80E-08,1.39E-07,3.91E-05,4.53E-05,5.98E-05,1.07E-07,0.99796881,8.14E-07,0.001764495,2.56E-07
5603,text_generation5,150,The KL component of NLL is given in parentheses .,Experiments,Data sets,text_generation,5,5,1,0,,1.39E-05,0,negative,3.95E-06,1.40E-07,4.70E-06,2.85E-08,1.02E-08,0.000332684,1.10E-05,0.000405185,8.22E-07,0.999226436,6.45E-07,1.42E-05,2.07E-07
5604,text_generation5,151,Size indicates the effective filter size .,Experiments,,text_generation,5,6,1,0,,0.035105519,0,negative,0.001090149,0.000250017,0.00102086,4.32E-05,2.58E-05,0.013875927,0.00171804,0.165363942,0.000376228,0.814075417,0.00014348,0.00149847,0.000518474
5605,text_generation5,152,VAE + init indicates pretraining of only the encoder using an LSTM LM .,Experiments,Size indicates the effective filter size .,text_generation,5,7,1,0,,0.002651541,0,negative,0.001212859,4.25E-06,0.002924022,4.74E-05,1.68E-05,0.027630596,0.000665446,0.001883618,2.37E-05,0.898630838,8.96E-06,0.000282755,0.066668703
5606,text_generation5,153,Model configurations and Training details,Experiments,,text_generation,5,8,1,0,,0.03564418,0,negative,0.000214056,0.000624655,0.000547625,0.000251559,8.35E-05,0.055473578,0.008492786,0.261490464,0.000386288,0.66985636,0.000291864,0.001047503,0.001239808
5607,text_generation5,154,We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders .,Experiments,Model configurations and Training details,text_generation,5,9,1,1,hyperparameters,0.956039374,1,experimental-setup,0.00024488,4.16E-05,0.018762453,7.07E-06,7.54E-06,0.900743148,0.011139624,0.030553303,8.02E-05,0.038082183,2.42E-06,7.88E-05,0.000256819
5608,text_generation5,155,"For CNNs , we explore several different configurations .",Experiments,,text_generation,5,10,1,1,hyperparameters,0.767286493,1,hyperparameters,0.000964754,0.00129042,0.002789547,3.45E-05,3.20E-05,0.065652146,0.007631993,0.586006827,0.000456648,0.333063463,5.39E-05,0.001320954,0.00070284
5609,text_generation5,156,"We set the convolution filter size to be 3 and gradually increase the depth and dilation from [ 1 , 2 , 4 ] , ] to .",Experiments,"For CNNs , we explore several different configurations .",text_generation,5,11,1,1,hyperparameters,0.799359317,1,experimental-setup,0.002576564,3.20E-05,0.000371761,8.23E-05,2.10E-05,0.482401178,0.012566247,0.154243947,5.35E-05,0.149738466,1.49E-05,0.001332686,0.196565492
5610,text_generation5,157,"They represent small , medium and large model and we name them as SCNN , MCNN and LCNN .",Experiments,"For CNNs , we explore several different configurations .",text_generation,5,12,1,0,,0.013317533,0,negative,0.001413775,8.07E-06,0.09195748,1.37E-05,1.35E-05,0.009689179,0.000995787,0.000793922,7.27E-05,0.86212014,1.54E-05,0.002214515,0.030691964
5611,text_generation5,158,We also explore a very large model with dilations and name it as VLCNN .,Experiments,"For CNNs , we explore several different configurations .",text_generation,5,13,1,0,,0.153301559,0,baselines,0.007751663,8.31E-05,0.558446479,2.62E-05,3.23E-05,0.013023263,0.00579223,0.001301247,0.000319709,0.346706219,3.55E-05,0.012928911,0.053553081
5612,text_generation5,159,"The effective filter size are 15 , 63 , 125 and 187 respectively .",Experiments,"For CNNs , we explore several different configurations .",text_generation,5,14,1,0,,0.672060316,1,experimental-setup,0.003830924,3.48E-05,0.000551421,9.72E-05,2.89E-05,0.380532694,0.009622625,0.120928971,8.55E-05,0.194471896,1.10E-05,0.001718578,0.288085488
5613,text_generation5,160,"We use the last hidden state of the encoder LSTM and feed it though an MLP to get the mean and variance of q ( z|x ) , from which we sample z and then feed it through an MLP to get the starting state of decoder .",Experiments,"For CNNs , we explore several different configurations .",text_generation,5,15,1,0,,0.001034537,0,negative,0.002822627,9.69E-06,0.010418309,7.73E-06,8.87E-06,0.013474257,0.000690977,0.002540139,9.07E-05,0.953070925,4.20E-06,0.001483905,0.015377668
5614,text_generation5,161,"For the LSTM decoder , we follow to use it as the initial state of LSTM and feed it to every step of LSTM .",Experiments,"For CNNs , we explore several different configurations .",text_generation,5,16,1,0,,0.004765235,0,negative,0.001127919,1.04E-05,0.003474053,9.94E-06,6.89E-06,0.011594696,0.00030089,0.004137342,0.000167725,0.966072613,2.76E-06,0.000520032,0.012574754
5615,text_generation5,162,"For the CNN decoder , we concatenate it with the word embedding of every decoder input .",Experiments,"For CNNs , we explore several different configurations .",text_generation,5,17,1,0,,0.045241815,0,negative,0.008840433,7.79E-05,0.024247378,4.26E-05,3.16E-05,0.078835433,0.002785942,0.023655169,0.000878345,0.773504076,7.90E-06,0.001788051,0.085305134
5616,text_generation5,163,The architecture of the Semi-supervised VAE basically follows that of the VAE .,Experiments,"For CNNs , we explore several different configurations .",text_generation,5,18,1,0,,0.001480785,0,negative,0.003374011,6.25E-05,0.15947263,2.62E-05,2.09E-05,0.008027104,0.001185018,0.001103673,0.001440463,0.735663589,6.17E-05,0.005860763,0.0837014
5617,text_generation5,164,We feed the last hidden state of the encoder LSTM through a two layer MLP then a softmax to get q ( y|x ) .,Experiments,"For CNNs , we explore several different configurations .",text_generation,5,19,1,0,,0.001918262,0,negative,0.009753969,2.95E-05,0.018313072,3.35E-05,1.96E-05,0.056328931,0.001844993,0.012170151,0.000489387,0.794144952,6.13E-06,0.001883815,0.104982115
5618,text_generation5,165,We use Gumbel - softmax to sample y from q ( y|x ) .,Experiments,"For CNNs , we explore several different configurations .",text_generation,5,20,1,1,hyperparameters,0.003965503,0,negative,0.002306542,2.97E-05,0.005975603,2.69E-05,1.27E-05,0.158411046,0.004799615,0.044801649,9.42E-05,0.654606218,4.73E-06,0.000955029,0.127976022
5619,text_generation5,166,"We then concatenate y with the last hidden state of encoder LSTM and feed them throught an MLP to get the mean and variance of q ( z|y , x ) .",Experiments,"For CNNs , we explore several different configurations .",text_generation,5,21,1,0,,0.000835437,0,negative,0.002246949,4.74E-06,0.008591617,2.90E-06,4.38E-06,0.004711799,0.000370502,0.000793901,5.44E-05,0.973303802,1.50E-06,0.001690906,0.008222636
5620,text_generation5,167,y and z together are used as the starting state of the decoder .,Experiments,"For CNNs , we explore several different configurations .",text_generation,5,22,1,0,,0.000687476,0,negative,0.000674594,2.19E-06,0.002715941,1.51E-06,2.97E-06,0.003134513,0.000159476,0.000789142,3.66E-05,0.988310142,7.17E-07,0.000800969,0.00337124
5621,text_generation5,168,We use a vocabulary size of 20 k for both data sets and set the word embedding dimension to be 512 .,Experiments,"For CNNs , we explore several different configurations .",text_generation,5,23,1,1,hyperparameters,0.691573231,1,experimental-setup,0.000971657,2.17E-05,0.000296145,4.08E-05,1.40E-05,0.383747509,0.017871061,0.122388125,3.39E-05,0.135803195,4.60E-06,0.000949142,0.33785823
5622,text_generation5,169,The LSTM dimension is 1024 .,Experiments,,text_generation,5,24,1,0,,0.918693528,1,hyperparameters,3.65E-05,6.51E-05,2.68E-05,1.28E-05,3.41E-06,0.050203323,0.004526463,0.934784552,4.33E-05,0.009498289,4.50E-06,7.97E-05,0.000715103
5623,text_generation5,170,"The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally , as shown in Section 2.3 .",Experiments,The LSTM dimension is 1024 .,text_generation,5,25,1,1,hyperparameters,0.902736881,1,experimental-setup,4.80E-05,6.63E-06,8.82E-05,8.65E-06,5.72E-06,0.858952077,0.006233113,0.10254054,7.64E-06,0.027901586,4.45E-07,2.66E-05,0.00418073
5624,text_generation5,171,We select the dimension of z from .,Experiments,,text_generation,5,26,1,0,,0.007064195,0,hyperparameters,0.000167155,0.000114908,3.01E-05,1.46E-05,1.66E-05,0.019974467,0.001482645,0.701639367,5.28E-05,0.275745458,4.46E-06,0.000420242,0.000337201
5625,text_generation5,172,We find our model is not sensitive to this parameter .,Experiments,We select the dimension of z from .,text_generation,5,27,1,0,,0.000881391,0,negative,0.027491707,1.35E-05,0.00050951,0.000191223,0.000178916,0.02567716,0.001883366,0.003104604,1.99E-05,0.685975463,5.22E-06,0.011315307,0.243634153
5626,text_generation5,173,"We use Adam to optimize all models and the learning rate is selected from [ 2e - 3 , 1 e - 3 , 7.5 e - 4 ] and ?",Experiments,We select the dimension of z from .,text_generation,5,28,1,1,hyperparameters,0.933268108,1,experimental-setup,0.000530938,1.22E-05,6.45E-05,3.41E-05,1.13E-05,0.475772378,0.008253425,0.169602193,1.28E-05,0.061352662,1.02E-06,0.000141716,0.284210817
5627,text_generation5,174,"1 is selected from [ 0.5 , 0.9 ] .",Experiments,We select the dimension of z from .,text_generation,5,29,1,0,,0.146699462,0,experimental-setup,0.002326339,1.81E-05,0.000103203,3.10E-05,2.23E-05,0.324012334,0.00750821,0.201345351,1.93E-05,0.24817751,1.19E-06,0.000652771,0.215782404
5628,text_generation5,175,"Empirically , we find learning rate 1e - 3 and ?1 = 0.5 to perform the best .",Experiments,We select the dimension of z from .,text_generation,5,30,1,1,hyperparameters,0.635486424,1,tasks,0.002246149,1.55E-05,5.27E-05,7.59E-05,3.73E-05,0.347754841,0.007504276,0.124452506,9.18E-06,0.088944379,6.45E-07,0.000652671,0.428254037
5629,text_generation5,176,"We select dropout ratio of LSTMs ( both encoder and decoder ) from [ 0.3 , 0.5 ] .",Experiments,We select the dimension of z from .,text_generation,5,31,1,1,hyperparameters,0.947911744,1,tasks,0.000646395,1.69E-05,8.20E-05,6.41E-05,1.72E-05,0.257477712,0.007998637,0.124114699,1.94E-05,0.023130447,5.69E-07,0.000147198,0.586284732
5630,text_generation5,177,"Following , we also use drop word for the LSTM decoder , the drop word ratio is selected from [ 0 , 0.3 , 0.5 , 0.7 ] .",Experiments,We select the dimension of z from .,text_generation,5,32,1,1,hyperparameters,0.905276292,1,tasks,0.00481471,4.66E-05,0.000784415,3.92E-05,4.34E-05,0.264810817,0.014256493,0.107886815,4.82E-05,0.147267213,7.30E-07,0.000564155,0.459437282
5631,text_generation5,178,"For the CNN decoder , we use a dropout ratio of 0.1 at each layer .",Experiments,We select the dimension of z from .,text_generation,5,33,1,1,hyperparameters,0.92829931,1,tasks,0.000678688,1.86E-05,0.000114372,9.68E-05,2.26E-05,0.237828321,0.006369308,0.102590561,2.04E-05,0.013571858,2.71E-07,9.27E-05,0.638595499
5632,text_generation5,179,We do not use drop word for CNN decoders .,Experiments,We select the dimension of z from .,text_generation,5,34,1,0,,0.001576929,0,negative,0.020456186,1.76E-05,0.010848501,2.65E-05,0.000105152,0.011754732,0.001040899,0.000926492,3.24E-05,0.874870059,7.98E-07,0.004040512,0.075880074
5633,text_generation5,180,We use batch size of 32 and all model are trained for 40 epochs .,Experiments,We select the dimension of z from .,text_generation,5,35,1,1,hyperparameters,0.941338979,1,tasks,0.000337424,8.33E-06,6.36E-05,3.91E-05,1.17E-05,0.189506725,0.006744037,0.063404457,8.46E-06,0.018495353,3.18E-07,8.00E-05,0.721300483
5634,text_generation5,181,We start to half the learning rate every 2 epochs after epoch 30 .,Experiments,We select the dimension of z from .,text_generation,5,36,1,0,,0.155239362,0,tasks,0.002549562,1.67E-05,0.000219,2.70E-05,2.21E-05,0.127008923,0.009809077,0.039233088,1.98E-05,0.033883413,3.84E-07,0.000223444,0.786987528
5635,text_generation5,182,"Following , we use KL cost annealing strategy .",Experiments,,text_generation,5,37,1,1,hyperparameters,0.198340081,0,negative,0.005537392,0.003420289,0.014641709,0.000117162,0.000322222,0.012087778,0.005732593,0.285081738,0.002863639,0.663238004,5.12E-06,0.001920744,0.005031605
5636,text_generation5,183,We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .,Experiments,"Following , we use KL cost annealing strategy .",text_generation,5,38,1,1,hyperparameters,0.092905895,0,tasks,0.001299876,6.51E-06,4.28E-05,9.03E-06,8.78E-06,0.025358433,0.004799666,0.009415835,1.80E-05,0.030069489,1.46E-07,9.04E-05,0.928881106
5637,text_generation5,184,"We treat T as a hyper parameter and select it from [ 10 k , 40 k , 80 k ] .",Experiments,"Following , we use KL cost annealing strategy .",text_generation,5,39,1,0,,0.008087043,0,tasks,0.001015481,6.43E-06,4.62E-05,1.06E-05,1.20E-05,0.03225821,0.005162895,0.010940537,1.39E-05,0.070177626,1.36E-07,0.000125041,0.880230956
5638,text_generation5,185,Language modeling results,,,text_generation,5,0,1,0,,0.708843795,1,results,0.000504428,9.72E-05,0.000138538,4.76E-06,5.88E-06,8.43E-05,0.019687351,0.000849116,5.37E-06,0.355123946,0.17835105,0.444931672,0.000216305
5639,text_generation5,186,The results for language modeling are shown in .,Language modeling results,,text_generation,5,1,1,1,results,0.304438909,0,negative,0.000231134,5.98E-08,0.000131858,3.73E-08,1.31E-08,2.80E-06,0.005579543,3.49E-06,1.04E-07,0.567406329,9.76E-06,0.426578295,5.66E-05
5640,text_generation5,187,We report the negative log likelihood ( NLL ) and perplexity ( PPL ) of the test set .,Language modeling results,The results for language modeling are shown in .,text_generation,5,2,1,0,,0.00413428,0,negative,0.00023835,1.76E-07,3.01E-05,1.02E-07,5.60E-08,2.11E-06,0.000177663,6.16E-06,1.78E-07,0.868538869,9.26E-07,0.130964725,4.06E-05
5641,text_generation5,188,"For the NLL of VAEs , we decompose it into reconstruction loss and KL divergence and report the KL divergence in the parenthesis .",Language modeling results,The results for language modeling are shown in .,text_generation,5,3,1,0,,0.000700586,0,negative,0.00029803,1.31E-06,0.000368367,2.63E-07,5.70E-08,5.36E-06,7.02E-05,2.67E-05,4.02E-06,0.981118719,3.06E-06,0.018063492,4.04E-05
5642,text_generation5,189,"To better visualize these results , we plot the results of Yahoo data set ( We can see that LSTM - VAE is worse than LSTM - LM in terms of NLL and the KL term is nearly zero , which verifies the finding of .",Language modeling results,The results for language modeling are shown in .,text_generation,5,4,1,0,,0.663801141,1,results,0.000589625,8.30E-09,4.56E-06,1.33E-08,3.39E-09,4.09E-07,0.000341375,1.22E-06,5.87E-09,0.090836983,2.42E-07,0.908202678,2.29E-05
5643,text_generation5,190,"When we use CNNs as the decoders for VAEs , we can see improvement over pure CNN LMs .",Language modeling results,The results for language modeling are shown in .,text_generation,5,5,1,0,,0.96554233,1,results,0.000524953,6.75E-09,5.86E-06,2.02E-08,2.93E-09,3.54E-07,0.000394596,1.02E-06,7.48E-09,0.021765286,1.93E-07,0.977242539,6.52E-05
5644,text_generation5,191,"For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .",Language modeling results,The results for language modeling are shown in .,text_generation,5,6,1,1,results,0.968579964,1,results,0.000484733,6.12E-09,3.06E-06,1.92E-08,2.91E-09,2.53E-07,0.000580121,8.24E-07,4.41E-09,0.011146652,1.26E-07,0.987689896,9.43E-05
5645,text_generation5,192,The improvement is big for small models and gradually decreases as we increase the decoder model contextual capacity .,Language modeling results,The results for language modeling are shown in .,text_generation,5,7,1,0,,0.910216913,1,results,0.011251113,1.82E-07,2.63E-05,1.33E-06,9.01E-08,5.12E-06,0.000981916,1.81E-05,6.42E-07,0.119623561,1.53E-06,0.867444856,0.000645332
5646,text_generation5,193,"When the model is as large as VLCNN , the improvement diminishes and the VAE result is almost the same with LM result .",Language modeling results,The results for language modeling are shown in .,text_generation,5,8,1,0,,0.886507279,1,results,0.012839472,4.46E-08,1.28E-05,1.49E-07,2.27E-08,1.17E-06,0.000802727,3.95E-06,6.88E-08,0.055038617,4.72E-07,0.93112117,0.000179373
5647,text_generation5,194,"This is also reflected in the KL term , SCNN - VAE has the largest KL of 13.3 and VLCNN - VAE has the smallest KL of 0.7 .",Language modeling results,The results for language modeling are shown in .,text_generation,5,9,1,0,,0.470494744,0,results,0.008567584,4.06E-08,2.30E-05,3.51E-07,4.85E-08,2.39E-06,0.000865729,3.96E-06,9.50E-08,0.13079794,4.25E-07,0.859569651,0.000168813
5648,text_generation5,195,"When LCNN is used as the decoder , we obtain an optimal trade off between using contextual information and latent representation .",Language modeling results,The results for language modeling are shown in .,text_generation,5,10,1,1,results,0.350869025,0,results,0.003597542,1.04E-06,7.73E-05,9.19E-07,1.36E-07,5.90E-06,0.000516664,3.81E-05,2.83E-06,0.397798894,3.15E-06,0.597684771,0.000272843
5649,text_generation5,196,"LCNN - VAE achieves a NLL of 333.9 , which improves over LSTM - LM with NLL of 334.9 .",Language modeling results,The results for language modeling are shown in .,text_generation,5,11,1,1,results,0.94769014,1,results,0.000237475,1.20E-08,9.31E-06,3.98E-08,6.10E-09,5.50E-07,0.001149635,1.55E-06,8.54E-09,0.013931544,1.73E-07,0.984318008,0.000351685
5650,text_generation5,197,"We find that if we initialize the parameters of LSTM encoder with parameters of LSTM language model , we can improve the VAE results further .",Language modeling results,The results for language modeling are shown in .,text_generation,5,12,1,0,,0.720963935,1,results,0.003387779,3.38E-07,1.26E-05,1.11E-06,1.33E-07,1.02E-05,0.000997654,6.18E-05,9.16E-07,0.369465498,1.49E-06,0.625232552,0.000827916
5651,text_generation5,198,This indicates better encoder model is also a key factor for VAEs to work well .,Language modeling results,The results for language modeling are shown in .,text_generation,5,13,1,0,,0.010367205,0,negative,0.003424864,7.67E-08,3.31E-05,1.52E-07,4.41E-08,1.85E-06,0.000137481,3.93E-06,3.05E-07,0.693566323,4.23E-07,0.30276142,7.00E-05
5652,text_generation5,199,"Combined with encoder initialization , LCNN - VAE improves over LSTM - LM from 334.9 to 332.1 in NLL and from 66.2 to 63.9 in PPL .",Language modeling results,The results for language modeling are shown in .,text_generation,5,14,1,0,,0.945456421,1,results,0.001782861,3.08E-08,8.52E-06,1.31E-07,1.45E-08,8.05E-07,0.000920794,2.73E-06,4.18E-08,0.0227446,1.72E-07,0.974149891,0.000389406
5653,text_generation5,200,Similar results for the sentiment data set are shown in .,Language modeling results,The results for language modeling are shown in .,text_generation,5,15,1,0,,0.135940141,0,results,0.000271266,7.30E-09,1.02E-05,7.33E-09,3.75E-09,3.34E-07,0.000241003,7.39E-07,7.74E-09,0.270778387,9.88E-08,0.728675495,2.24E-05
5654,text_generation5,201,LCNN - VAE improves over LSTM - LM from 362.7 to 359.1 in NLL and from 42.6 to 41.1 in PPL .,Language modeling results,The results for language modeling are shown in .,text_generation,5,16,1,0,,0.939615834,1,results,0.00032914,7.08E-09,4.57E-06,3.71E-08,5.08E-09,3.78E-07,0.000871868,1.05E-06,6.74E-09,0.014283388,8.32E-08,0.98418113,0.000328333
5655,text_generation5,202,Latent representation visualization :,Language modeling results,The results for language modeling are shown in .,text_generation,5,17,1,0,,0.091051442,0,results,0.000322938,3.65E-07,0.003060557,8.99E-08,1.93E-08,2.36E-06,0.001113704,3.45E-06,1.09E-06,0.477478308,1.54E-05,0.517477932,0.000523755
5656,text_generation5,203,"In order to visualize the latent representation , we set the dimension of z to be 2 and plot the mean of posterior probability q ( z | x ) , as shown in .",Language modeling results,The results for language modeling are shown in .,text_generation,5,18,1,0,,0.000988333,0,negative,0.000230082,1.28E-07,1.05E-05,1.79E-07,4.05E-08,1.15E-05,0.000309581,7.02E-05,7.44E-07,0.971678316,3.61E-07,0.027553932,0.000134492
5657,text_generation5,204,We can see distinct different characteristics of topic and sentiment representation .,Language modeling results,The results for language modeling are shown in .,text_generation,5,19,1,0,,0.001351347,0,negative,0.000170078,3.22E-08,1.45E-05,3.37E-08,1.26E-08,9.04E-07,4.40E-05,2.04E-06,1.78E-07,0.918761784,2.29E-07,0.080963533,4.26E-05
5658,text_generation5,205,"In , we can see that documents of different topics fall into different clusters , while in , documents of different ratings form a continuum , they lie continuously on the xaxis as the review rating increases . :",Language modeling results,The results for language modeling are shown in .,text_generation,5,20,1,0,,0.006650478,0,negative,0.002983532,7.74E-08,5.65E-05,1.02E-07,3.36E-08,1.35E-06,0.00026995,3.68E-06,2.42E-07,0.505904592,2.82E-07,0.4906829,9.68E-05
5659,text_generation5,206,Semi-supervised VAE ablation results on Yahoo .,Language modeling results,,text_generation,5,21,1,0,,0.339365695,0,results,0.000559704,5.69E-08,0.000183242,2.91E-08,2.36E-08,9.32E-07,0.008695349,1.12E-06,4.30E-08,0.140414573,9.56E-07,0.850041492,0.000102484
5660,text_generation5,207,We report both the NLL and classification accuracy of the test data .,Language modeling results,Semi-supervised VAE ablation results on Yahoo .,text_generation,5,22,1,0,,0.006948045,0,negative,0.000161894,1.26E-08,6.81E-06,4.25E-08,3.61E-08,9.50E-07,7.30E-05,1.14E-06,6.99E-08,0.994336349,1.67E-08,0.00524123,0.000178497
5661,text_generation5,208,Accuracy is in percentage .,Language modeling results,,text_generation,5,23,1,0,,0.0070175,0,negative,2.28E-05,6.46E-08,1.04E-05,7.51E-08,1.87E-08,8.50E-06,0.001186571,2.73E-05,8.05E-07,0.994261231,3.07E-07,0.004451825,3.02E-05
5662,text_generation5,209,Number of labeled samples is fixed to be 500 .,Language modeling results,Accuracy is in percentage .,text_generation,5,24,1,0,,0.524164116,1,negative,0.000466176,3.09E-06,3.47E-05,2.56E-06,5.54E-07,0.00046722,0.006404849,0.0069702,1.04E-05,0.978667437,1.34E-06,0.004162329,0.002809123
5663,text_generation5,210,Semi-supervised VAE results,Language modeling results,Accuracy is in percentage .,text_generation,5,25,1,0,,0.836826678,1,results,0.000342577,6.81E-08,4.78E-05,1.23E-08,1.23E-08,1.08E-06,0.003629273,2.08E-06,3.48E-08,0.196120543,1.17E-06,0.799752174,0.000103147
5664,text_generation5,211,"Motivated by the success of VAEs for language modeling , we continue to explore VAEs for semi-supervised learning .",Language modeling results,Accuracy is in percentage .,text_generation,5,26,1,0,,0.142032123,0,negative,0.000262695,9.08E-07,0.000197662,5.89E-07,1.85E-07,1.15E-05,0.000952766,1.59E-05,8.47E-07,0.947622155,5.13E-05,0.048783024,0.00210048
5665,text_generation5,212,"Following that of , we set the number of labeled samples to be 100 , 500 , 1000 and 2000 respectively .",Language modeling results,Accuracy is in percentage .,text_generation,5,27,1,0,,0.079123056,0,negative,0.000370426,7.80E-07,1.12E-05,4.31E-07,2.53E-07,8.72E-05,0.001678141,0.000822606,1.78E-06,0.994270768,9.88E-08,0.002501384,0.000254957
5666,text_generation5,213,Ablation Study :,Language modeling results,Accuracy is in percentage .,text_generation,5,28,1,0,,0.0270062,0,negative,0.009337015,2.84E-07,0.002512783,3.83E-07,4.40E-07,5.06E-06,0.000385469,2.68E-06,1.71E-06,0.911479026,6.72E-07,0.076019268,0.000255208
5667,text_generation5,214,"At first , we would like to explore the effect of different decoders for semi-supervised classification .",Language modeling results,Accuracy is in percentage .,text_generation,5,29,1,0,,0.054970728,0,negative,0.000137291,7.59E-08,9.14E-06,1.67E-08,1.70E-08,1.63E-06,5.96E-05,4.31E-06,3.77E-07,0.995139781,7.65E-08,0.004630804,1.69E-05
5668,text_generation5,215,We fix the number of labeled samples to be 500 and report both classification accuracy and NLL of the test set of Yahoo data set in 5 .,Language modeling results,Accuracy is in percentage .,text_generation,5,30,1,0,,0.265966138,0,negative,0.000231654,6.99E-07,1.08E-05,3.47E-07,2.70E-07,8.62E-05,0.002098504,0.000902943,1.48E-06,0.993277962,1.05E-07,0.002829375,0.000559674
5669,text_generation5,216,We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .,Language modeling results,Accuracy is in percentage .,text_generation,5,31,1,1,ablation-analysis,0.843098436,1,results,0.000691402,8.38E-09,4.12E-06,4.96E-09,8.82E-09,4.63E-07,0.002116749,1.21E-06,3.47E-09,0.101129861,2.16E-08,0.895984868,7.13E-05
5670,text_generation5,217,The accuracy decreases as we gradually increase the decoder contextual capacity .,Language modeling results,Accuracy is in percentage .,text_generation,5,32,1,0,,0.753889857,1,negative,0.041721285,2.08E-07,1.86E-05,2.45E-07,1.79E-07,3.48E-06,0.001678842,1.36E-05,3.47E-07,0.491900784,2.11E-07,0.46441568,0.000246592
5671,text_generation5,218,"On the other hand , LCNN - VAE - Semi has the best NLL result .",Language modeling results,Accuracy is in percentage .,text_generation,5,33,1,1,ablation-analysis,0.801559185,1,results,0.000919429,6.54E-09,4.69E-06,6.15E-09,8.65E-09,4.30E-07,0.001875012,1.01E-06,2.37E-09,0.070935353,1.59E-08,0.926199904,6.41E-05
5672,text_generation5,219,"This classification accuracy and NLL trade off once again verifies our conjecture : with small contextual window size , the decoder is forced to use the encoder information , hence the latent representation is better , they denotes the LSTM is initialized with a sequence autoencoder and a language model .",Language modeling results,Accuracy is in percentage .,text_generation,5,34,1,0,,0.027842574,0,negative,0.006978428,4.25E-08,9.52E-06,1.94E-08,3.84E-08,1.04E-06,0.000728559,2.75E-06,5.88E-08,0.690646382,3.74E-08,0.301595773,3.73E-05
5673,text_generation5,220,learned .,Language modeling results,Accuracy is in percentage .,text_generation,5,35,1,0,,0.000921379,0,negative,3.47E-05,1.50E-08,4.52E-06,9.81E-09,2.18E-08,6.75E-07,7.33E-06,2.55E-06,1.39E-07,0.999692335,2.45E-09,0.000254373,3.32E-06
5674,text_generation5,221,Comparing the NLL results of Comparison with related methods :,Language modeling results,Accuracy is in percentage .,text_generation,5,36,1,0,,0.032896675,0,negative,0.000126541,2.83E-08,0.000116196,1.09E-09,4.86E-09,3.45E-07,0.000452632,4.80E-07,4.30E-08,0.938932083,1.08E-07,0.060355383,1.62E-05
5675,text_generation5,222,"We compare Semisupervised VAE with the methods from , which represent the previous state - of - the - art for semisupervised sequence learning .",Language modeling results,Accuracy is in percentage .,text_generation,5,37,1,0,,0.569464082,1,negative,0.000386444,4.71E-06,0.000700009,3.48E-07,1.32E-06,1.13E-05,0.002180765,1.94E-05,1.40E-06,0.954645724,7.32E-07,0.041633603,0.000414236
5676,text_generation5,223,pre-trains a classifier by initializing the parameters of a classifier with that of a language model or a sequence autoencoder .,Language modeling results,Accuracy is in percentage .,text_generation,5,38,1,0,,0.037753934,0,negative,0.000121135,3.06E-07,0.000725934,5.19E-08,1.24E-07,1.84E-06,4.31E-05,7.42E-06,4.14E-06,0.998289392,2.20E-08,0.000736648,6.99E-05
5677,text_generation5,224,They find it improves the classification accuracy significantly .,Language modeling results,,text_generation,5,39,1,0,,0.174770317,0,negative,0.002109946,4.33E-08,2.02E-05,2.99E-08,4.86E-08,8.86E-07,0.001409108,2.14E-06,8.80E-08,0.80613971,3.75E-08,0.190284083,3.37E-05
5678,text_generation5,225,"Since SCNN - VAE - Semi performs the best according to Table 5 , we fix the decoder to be SCNN in this part .",Language modeling results,They find it improves the classification accuracy significantly .,text_generation,5,40,1,0,,0.688159517,1,negative,0.030809486,1.86E-06,0.000141288,1.38E-07,2.41E-06,2.89E-05,0.006021847,1.66E-05,1.18E-06,0.940964585,4.63E-08,0.021573249,0.000438393
5679,text_generation5,226,The detailed comparison is in .,Language modeling results,,text_generation,5,41,1,0,,0.002688369,0,negative,7.08E-05,2.31E-08,1.49E-05,5.19E-08,5.34E-08,9.80E-07,0.000129665,1.64E-06,1.78E-07,0.996486728,1.30E-08,0.003280984,1.40E-05
5680,text_generation5,227,We can see that semisupervised VAE performs better than LM - LSTM and LA - LSTM from .,Language modeling results,The detailed comparison is in .,text_generation,5,42,1,0,,0.789386372,1,results,0.000339202,3.71E-09,1.05E-06,1.24E-08,3.71E-09,4.75E-07,0.002149121,1.60E-06,2.46E-09,0.045133926,9.69E-09,0.952217291,0.0001573
5681,text_generation5,228,We also initialize the encoder of the VAE with parameters from LM and find classification accuracy further improves .,Language modeling results,The detailed comparison is in .,text_generation,5,43,1,0,,0.17156071,0,negative,0.004208356,2.26E-06,5.94E-05,1.12E-06,5.07E-07,2.14E-05,0.003462257,0.000358325,4.50E-06,0.892526018,4.92E-08,0.098173766,0.001182017
5682,text_generation5,229,We also see the advantage of SCNN - VAE - Semi over LM - LSTM is greater when the number of labeled samples is smaller .,Language modeling results,The detailed comparison is in .,text_generation,5,44,1,0,,0.611668316,1,results,0.001058314,8.66E-09,2.76E-06,1.23E-08,6.87E-09,3.42E-07,0.001330392,1.47E-06,6.67E-09,0.082303447,8.71E-09,0.915205599,9.76E-05
5683,text_generation5,230,The advantage decreases as we increase the number of labeled samples .,Language modeling results,The detailed comparison is in .,text_generation,5,45,1,0,,0.220604467,0,negative,0.002560424,8.30E-08,1.82E-05,1.64E-07,6.72E-08,1.93E-06,0.000494721,1.05E-05,3.46E-07,0.849379399,3.29E-08,0.147325284,0.000208847
5684,text_generation5,231,"When we set the number of labeled samples to be 25 k , the SCNN - VAE - Semi achieves an accuracy of 70.4 , which is similar to LM - LSTM with an accuracy of 70.5 .",Language modeling results,The detailed comparison is in .,text_generation,5,46,1,0,,0.817440771,1,results,0.000267303,4.09E-09,1.51E-06,7.73E-09,4.74E-09,3.07E-07,0.001994653,1.28E-06,2.60E-09,0.048800563,4.47E-09,0.94872958,0.000204777
5685,text_generation5,232,"Also , SCNN - VAE - Semi performs better on Yahoo data set than Yelp data set .",Language modeling results,The detailed comparison is in .,text_generation,5,47,1,0,,0.692559732,1,results,0.00015811,1.63E-09,1.03E-06,5.43E-09,1.99E-09,2.67E-07,0.001585068,8.94E-07,1.25E-09,0.036043819,3.53E-09,0.96205624,0.000154556
5686,text_generation5,233,"For Yelp , SCNN - VAE - Semi is a little bit worse than LM - LSTM if the number of labeled samples is greater than 100 , but becomes better when we initialize the encoder .",Language modeling results,The detailed comparison is in .,text_generation,5,48,1,0,,0.803056731,1,results,0.000610696,8.15E-09,2.33E-06,2.40E-08,8.45E-09,9.81E-07,0.003380222,4.12E-06,6.07E-09,0.088022791,7.77E-09,0.907663712,0.000315087
5687,text_generation5,234,explains this observation .,Language modeling results,The detailed comparison is in .,text_generation,5,49,1,0,,5.61E-05,0,negative,3.14E-05,1.34E-08,5.00E-06,4.95E-08,1.92E-08,6.35E-07,1.45E-05,2.22E-06,1.87E-07,0.998916783,4.33E-09,0.000998292,3.09E-05
5688,text_generation5,235,It shows the documents are coupled together and are harder to classify .,Language modeling results,The detailed comparison is in .,text_generation,5,50,1,0,,0.024838555,0,negative,4.52E-05,8.01E-09,1.03E-05,1.38E-08,1.80E-08,4.25E-07,1.55E-05,7.99E-07,9.23E-08,0.997630751,2.53E-09,0.002277589,1.94E-05
5689,text_generation5,236,"Also , the latent representation contains information other than sentiment , which may not be useful for classification .",Language modeling results,The detailed comparison is in .,text_generation,5,51,1,0,,0.005342767,0,negative,0.00037853,1.84E-08,9.16E-06,7.78E-09,1.76E-08,2.30E-07,1.80E-05,6.36E-07,1.06E-07,0.98254047,2.28E-09,0.017044806,8.03E-06
5690,text_generation5,237,Unsupervised clustering results,,,text_generation,5,0,1,0,,0.7953464,1,negative,7.54E-05,0.000288992,6.70E-05,5.88E-07,5.82E-07,5.19E-05,0.000480009,0.00099951,9.95E-05,0.656157148,0.323370668,0.018391126,1.75E-05
5691,text_generation5,238,We also explored using the same framework for unsupervised clustering .,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,1,1,0,,1.27E-05,0,negative,8.35E-05,5.53E-07,3.47E-05,5.72E-08,2.05E-08,3.77E-06,1.80E-06,1.96E-05,2.17E-06,0.999286709,1.74E-06,0.000565169,2.90E-07
5692,text_generation5,239,We compare with the baselines that ex - tract the feature with existing models and then run Gaussian Mixture Model ( GMM ) on these features .,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,2,1,0,,0.000210381,0,negative,0.000275685,1.54E-05,0.000564364,1.24E-07,1.76E-07,5.85E-06,1.34E-05,5.96E-05,6.00E-06,0.992590044,3.86E-06,0.006464694,8.26E-07
5693,text_generation5,240,We find empirically that simply using the features does not perform well since the features are high dimensional .,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,3,1,0,,0.110228946,0,negative,0.001343386,1.42E-06,6.93E-06,2.34E-06,9.62E-08,6.99E-05,0.000132067,0.000833086,2.52E-06,0.878881612,2.91E-05,0.118676063,2.15E-05
5694,text_generation5,241,"We run a PCA on these features , the dimension of PCA is selected from .",Unsupervised clustering results,Unsupervised clustering results,text_generation,5,4,1,0,,5.65E-05,0,negative,7.96E-05,2.79E-06,1.63E-05,1.51E-07,5.95E-08,2.55E-05,5.10E-06,0.000548955,9.25E-06,0.999060133,6.56E-07,0.000250466,1.05E-06
5695,text_generation5,242,"Since GMM can easily get stuck in poor local optimum , we run each model ten times and report the best result .",Unsupervised clustering results,Unsupervised clustering results,text_generation,5,5,1,0,,0.000901796,0,negative,4.70E-05,4.03E-07,1.61E-06,3.89E-08,1.34E-08,1.31E-05,8.06E-06,0.000205428,4.93E-07,0.998460479,8.30E-07,0.001262055,4.89E-07
5696,text_generation5,243,We find directly optimizing U ( x ) does not perform well for unsupervised clustering and we need to initialize the encoder with LSTM language model .,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,6,1,0,,0.000404413,0,negative,0.000852771,3.98E-07,6.18E-06,5.31E-07,4.57E-08,8.65E-06,1.68E-05,6.98E-05,8.47E-07,0.97206336,1.39E-05,0.026961077,5.64E-06
5697,text_generation5,244,The model only works well for Yahoo data set .,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,7,1,0,,0.333262861,0,results,0.001041041,3.39E-07,1.51E-05,9.85E-07,4.92E-08,2.15E-05,0.000299785,0.000159624,4.65E-07,0.184037726,1.53E-05,0.814365836,4.22E-05
5698,text_generation5,245,This is potentially because shows that sentiment latent representations does not fall into clusters .,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,8,1,0,,1.03E-05,0,negative,9.70E-05,6.15E-08,2.60E-06,1.05E-08,6.08E-09,4.07E-07,4.60E-07,4.05E-06,1.80E-07,0.99881207,1.24E-07,0.001082981,6.99E-08
5699,text_generation5,246,?,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,9,1,0,,1.23E-06,0,negative,7.10E-06,4.28E-08,9.09E-07,9.19E-09,2.15E-09,7.04E-07,3.90E-07,6.76E-06,3.74E-07,0.999795372,2.55E-07,0.000188004,8.01E-08
5700,text_generation5,247,"in Equation 5 is a sensitive parameter , we select it from the range between 0.5 and 1.5 with an interval of 0.1 .",Unsupervised clustering results,Unsupervised clustering results,text_generation,5,10,1,0,,8.97E-06,0,negative,0.000200485,2.85E-06,4.56E-06,5.27E-07,1.11E-07,7.53E-05,1.49E-05,0.002895524,6.58E-06,0.995901424,7.31E-07,0.000894644,2.42E-06
5701,text_generation5,248,"We use the following evaluation protocol : after we finish training , for cluster i , we find out the validation sample x n from cluster i that has the best q ( y i | x ) and assign the label of x n to all samples in cluster i .",Unsupervised clustering results,Unsupervised clustering results,text_generation,5,11,1,0,,1.22E-05,0,negative,3.93E-05,1.64E-06,2.73E-05,1.05E-08,1.52E-08,7.62E-07,1.02E-06,2.30E-05,2.65E-06,0.998975424,3.77E-07,0.000928349,1.53E-07
5702,text_generation5,249,We then compute the test accuracy based on this assignment .,Unsupervised clustering results,Unsupervised clustering results,text_generation,5,12,1,0,,1.33E-06,0,negative,2.79E-05,6.46E-07,1.66E-05,2.45E-09,6.83E-09,1.13E-07,1.73E-07,2.69E-06,2.85E-06,0.999131958,2.17E-07,0.00081679,5.36E-08
5703,text_generation5,250,The detailed results are in .,Unsupervised clustering results,,text_generation,5,13,1,0,,2.19E-05,0,negative,0.000135403,5.17E-08,5.05E-06,1.74E-08,1.23E-08,5.12E-07,1.41E-06,3.13E-06,2.44E-07,0.995747718,9.58E-08,0.004106169,1.83E-07
5704,text_generation5,251,We can see SCNN - VAE - Unsup + init performs better than other baselines .,Unsupervised clustering results,The detailed results are in .,text_generation,5,14,1,0,,0.861589687,1,results,0.000182581,2.18E-09,1.38E-06,7.15E-09,9.90E-10,1.82E-07,4.82E-05,5.88E-07,1.64E-09,0.020501138,5.08E-08,0.979250656,1.53E-05
5705,text_generation5,252,"LSTM+ GMM performs very bad probably because the feature dimension is 1024 and is too high for GMM , even though we already used PCA to reduce the dimension .",Unsupervised clustering results,The detailed results are in .,text_generation,5,15,1,0,,0.814538109,1,results,0.001102655,1.89E-08,5.82E-06,2.31E-07,1.33E-08,3.20E-06,0.000181897,7.02E-06,2.93E-08,0.057919309,3.50E-07,0.940557248,0.000222207
5706,text_generation5,253,Conditional text generation,Unsupervised clustering results,,text_generation,5,16,1,0,,0.064750171,0,results,0.000247012,3.41E-06,0.00033075,9.93E-07,2.08E-07,9.35E-06,0.0010622,6.57E-05,2.22E-06,0.292942954,0.000749247,0.704399976,0.000186001
5707,text_generation5,254,"With the semi-supervised VAE , we are able to generate text conditional on the label .",Unsupervised clustering results,Conditional text generation,text_generation,5,17,1,0,,3.39E-05,0,negative,0.000312081,2.93E-06,0.000121476,5.08E-08,1.14E-07,8.17E-07,9.04E-06,5.05E-06,4.51E-06,0.987975771,1.02E-05,0.011556217,1.73E-06
5708,text_generation5,255,"Due to space limitation , we only show one example of 1 star the food was good but the service was horrible .",Unsupervised clustering results,Conditional text generation,text_generation,5,18,1,0,,9.41E-07,0,negative,1.55E-05,6.38E-08,1.97E-06,1.90E-07,1.68E-07,2.18E-06,1.44E-06,4.09E-06,5.97E-07,0.999891209,3.16E-08,8.23E-05,2.47E-07
5709,text_generation5,256,took forever to get our food .,Unsupervised clustering results,Conditional text generation,text_generation,5,19,1,0,,3.65E-07,0,negative,1.17E-05,6.34E-08,1.11E-06,6.40E-08,2.36E-08,1.93E-06,1.30E-06,6.56E-06,9.98E-07,0.999915139,1.72E-07,6.06E-05,2.99E-07
5710,text_generation5,257,we had to ask twice for our check after we got our food .,Unsupervised clustering results,Conditional text generation,text_generation,5,20,1,0,,5.97E-07,0,negative,1.55E-05,5.60E-08,7.89E-07,9.71E-08,4.29E-08,1.71E-06,1.20E-06,4.50E-06,8.33E-07,0.999903223,1.90E-07,7.15E-05,4.00E-07
5711,text_generation5,258,will not return .,Unsupervised clustering results,Conditional text generation,text_generation,5,21,1,0,,3.04E-07,0,negative,2.54E-06,2.04E-08,6.24E-07,4.03E-09,2.50E-09,2.35E-07,4.07E-07,1.61E-06,4.53E-07,0.999958344,7.51E-08,3.56E-05,6.78E-08
5712,text_generation5,259,"2 star the food was good , but the service was terrible .",Unsupervised clustering results,Conditional text generation,text_generation,5,22,1,0,,5.85E-06,0,negative,2.29E-05,3.46E-08,1.81E-06,3.54E-08,2.88E-08,1.25E-06,1.41E-06,3.86E-06,4.03E-07,0.999814802,4.55E-08,0.000153141,2.33E-07
5713,text_generation5,260,took forever to get someone to take our drink order .,Unsupervised clustering results,Conditional text generation,text_generation,5,23,1,0,,2.61E-07,0,negative,8.37E-06,3.12E-08,8.55E-07,4.34E-08,1.66E-08,9.82E-07,6.87E-07,3.03E-06,5.43E-07,0.999938834,6.67E-08,4.63E-05,2.01E-07
5714,text_generation5,261,had to ask 3 times to get the check .,Unsupervised clustering results,Conditional text generation,text_generation,5,24,1,0,,1.61E-07,0,negative,6.92E-06,2.32E-08,6.30E-07,1.36E-08,1.48E-08,6.73E-07,6.40E-07,2.53E-06,3.17E-07,0.99993722,3.64E-08,5.09E-05,1.18E-07
5715,text_generation5,262,"food was ok , nothing to write about .",Unsupervised clustering results,Conditional text generation,text_generation,5,25,1,0,,5.57E-07,0,negative,7.68E-06,4.04E-08,9.72E-07,3.73E-08,2.00E-08,1.48E-06,1.54E-06,5.43E-06,6.23E-07,0.999898612,9.44E-08,8.31E-05,3.75E-07
5716,text_generation5,263,3 star came here for the first time last night .,Unsupervised clustering results,Conditional text generation,text_generation,5,26,1,0,,1.07E-05,0,negative,5.73E-05,3.89E-07,8.94E-06,3.75E-07,1.71E-07,6.45E-06,8.71E-06,2.73E-05,4.44E-06,0.999305495,5.49E-07,0.000573569,6.27E-06
5717,text_generation5,264,food was good .,Unsupervised clustering results,Conditional text generation,text_generation,5,27,1,0,,7.34E-06,0,negative,1.89E-05,8.94E-08,2.56E-06,2.35E-08,2.32E-08,2.32E-06,1.07E-05,1.31E-05,8.95E-07,0.999159526,3.04E-07,0.000790318,1.28E-06
5718,text_generation5,265,service was a little slow .,Unsupervised clustering results,Conditional text generation,text_generation,5,28,1,0,,2.18E-06,0,negative,9.97E-05,8.11E-08,2.51E-06,2.30E-07,1.51E-07,4.07E-06,8.42E-06,8.56E-06,1.08E-06,0.999132736,1.46E-07,0.000739516,2.78E-06
5719,text_generation5,266,food was just ok .,Unsupervised clustering results,Conditional text generation,text_generation,5,29,1,0,,1.13E-06,0,negative,6.37E-06,5.85E-08,1.06E-06,2.70E-08,1.52E-08,1.62E-06,2.14E-06,8.60E-06,8.65E-07,0.999883924,1.20E-07,9.47E-05,4.79E-07
5720,text_generation5,267,"4 star food was good , service was a little slow , but the food was pretty good .",Unsupervised clustering results,Conditional text generation,text_generation,5,30,1,0,,6.56E-06,0,negative,2.86E-05,1.46E-08,1.18E-06,1.34E-08,2.64E-08,7.46E-07,2.48E-06,2.47E-06,1.23E-07,0.999546596,1.78E-08,0.000417488,2.53E-07
5721,text_generation5,268,i had the grilled chicken sandwich and it was really good .,Unsupervised clustering results,Conditional text generation,text_generation,5,31,1,0,,2.59E-06,0,negative,1.35E-05,6.68E-08,1.38E-06,5.96E-08,2.92E-08,2.32E-06,2.66E-06,9.93E-06,7.77E-07,0.999812351,1.03E-07,0.000155869,9.93E-07
5722,text_generation5,269,will definitely be back !,Unsupervised clustering results,Conditional text generation,text_generation,5,32,1,0,,9.90E-07,0,negative,4.85E-06,5.26E-08,1.03E-06,1.45E-08,7.29E-09,8.28E-07,1.39E-06,6.36E-06,1.58E-06,0.999937896,6.39E-08,4.55E-05,4.33E-07
5723,text_generation5,270,"5 star food was very good , service was fast and friendly .",Unsupervised clustering results,Conditional text generation,text_generation,5,33,1,0,,0.000349026,0,negative,0.000140302,7.53E-08,2.98E-06,8.73E-08,6.55E-08,4.35E-06,4.47E-05,2.32E-05,5.42E-07,0.992116468,1.21E-07,0.007660087,7.01E-06
5724,text_generation5,271,food was very good as well .,Unsupervised clustering results,Conditional text generation,text_generation,5,34,1,0,,2.25E-05,0,negative,2.51E-05,6.73E-08,2.04E-06,3.20E-08,2.82E-08,2.59E-06,1.82E-05,1.69E-05,7.38E-07,0.998145163,1.48E-07,0.001786034,2.95E-06
5725,text_generation5,272,will be back !:,Unsupervised clustering results,Conditional text generation,text_generation,5,35,1,0,,1.81E-06,0,negative,4.72E-06,4.46E-08,1.46E-06,1.67E-08,1.05E-08,6.66E-07,1.07E-06,4.03E-06,1.47E-06,0.999944757,3.22E-08,4.13E-05,4.16E-07
5726,text_generation5,273,Text generated by conditioning on sentiment label .,Unsupervised clustering results,,text_generation,5,36,1,0,,1.65E-05,0,negative,5.29E-05,1.25E-07,4.85E-05,3.29E-08,9.11E-08,1.48E-06,8.17E-06,1.37E-05,4.43E-07,0.991842757,3.37E-08,0.008029173,2.70E-06
5727,text_generation5,274,generated reviews conditioning on review rating in .,Unsupervised clustering results,Text generated by conditioning on sentiment label .,text_generation,5,37,1,0,,2.55E-05,0,negative,5.63E-05,1.54E-07,6.29E-05,2.66E-08,1.15E-07,1.19E-06,7.40E-06,4.92E-06,1.34E-06,0.996943127,1.25E-07,0.002913496,8.90E-06
5728,text_generation5,275,"For each group of generated text , we fix z and vary the label y , while picking x via beam search with a beam size of 10 .",Unsupervised clustering results,Text generated by conditioning on sentiment label .,text_generation,5,38,1,0,,2.09E-05,0,negative,0.000123109,1.25E-06,7.16E-06,8.15E-08,2.33E-07,3.15E-05,7.35E-05,0.000499413,3.18E-06,0.99873997,2.19E-08,0.000505286,1.53E-05
5729,text_generation5,276,Related work,,,text_generation,5,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
5730,text_generation5,293,Conclusion,,,text_generation,5,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
5731,natural_language_inference11,1,title,,,natural_language_inference,11,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
5732,natural_language_inference11,2,Dynamic Integration of Background Knowledge in Neural NLU Systems,title,,natural_language_inference,11,1,1,1,research-problem,0.995909411,1,research-problem,1.61E-08,8.86E-06,5.99E-08,1.67E-08,1.20E-08,6.34E-08,3.23E-07,1.42E-06,3.83E-06,0.001921826,0.998063481,7.26E-08,1.94E-08
5733,natural_language_inference11,3,abstract,,,natural_language_inference,11,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
5734,natural_language_inference11,4,"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time .",abstract,abstract,natural_language_inference,11,1,1,1,research-problem,0.384133981,0,research-problem,1.47E-07,0.000101453,8.35E-08,2.37E-05,1.82E-05,3.51E-06,1.26E-06,1.18E-05,4.38E-06,0.155560389,0.844274714,5.68E-08,2.03E-07
5735,natural_language_inference11,5,We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models .,abstract,abstract,natural_language_inference,11,2,1,0,,0.531507146,1,research-problem,3.09E-06,0.007823769,2.83E-05,2.08E-06,1.49E-05,2.84E-06,3.88E-06,2.81E-05,0.002360563,0.037250355,0.95247931,1.94E-06,8.77E-07
5736,natural_language_inference11,6,A general - purpose reading module reads background knowledge in the form of freetext statements ( together with task - specific text inputs ) and yields refined word representations to a task - specific NLU architecture that reprocesses the task inputs with these representations .,abstract,abstract,natural_language_inference,11,3,1,0,,0.26032566,0,negative,1.42E-05,0.080668165,0.000524366,8.55E-06,0.000175891,5.02E-05,1.65E-05,0.000332566,0.126889837,0.475636942,0.315675138,4.54E-06,3.18E-06
5737,natural_language_inference11,7,Experiments on document question answering ( DQA ) and recognizing textual entailment ( RTE ) demonstrate the effectiveness and flexibility of the approach .,abstract,abstract,natural_language_inference,11,4,1,0,,0.416112745,0,research-problem,1.03E-07,2.33E-05,6.96E-08,1.95E-06,3.33E-06,4.18E-07,1.87E-06,1.66E-06,3.29E-07,0.022294682,0.977672052,1.39E-07,1.31E-07
5738,natural_language_inference11,8,Analysis shows that our model learns to exploit knowledge in a semantically appropriate way .,abstract,abstract,natural_language_inference,11,5,1,0,,0.010535246,0,negative,0.000198983,0.002798709,5.03E-06,4.96E-05,0.00053697,1.90E-05,4.75E-06,7.11E-05,0.000168984,0.986665132,0.009472019,9.27E-06,4.61E-07
5739,natural_language_inference11,9,"Sungjin Ahn , Heeyoul Choi , Tanel Prnamaa , and Yoshua Bengio. 2016 .",abstract,abstract,natural_language_inference,11,6,1,0,,0.042689413,0,negative,0.000180214,0.008532299,0.000155648,1.19E-05,0.00019542,2.29E-05,2.74E-05,0.000141997,0.000996746,0.76284091,0.226848172,4.49E-05,1.50E-06
5740,natural_language_inference11,10,A neural knowledge language model .,abstract,abstract,natural_language_inference,11,7,1,0,,0.002376314,0,research-problem,2.59E-07,0.000127374,1.84E-06,1.84E-07,3.83E-07,1.84E-06,1.60E-06,2.25E-05,0.000222844,0.046450792,0.953169752,4.74E-07,1.58E-07
5741,natural_language_inference11,11,arXiv .,abstract,abstract,natural_language_inference,11,8,1,0,,0.000355303,0,negative,1.03E-06,0.000133977,4.20E-07,1.38E-05,4.43E-05,1.75E-05,1.61E-06,4.80E-05,4.31E-05,0.988544055,0.011151691,3.92E-07,1.41E-07
5742,natural_language_inference11,12,Introduction,,,natural_language_inference,11,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
5743,natural_language_inference11,13,"Understanding natural language depends crucially on common - sense and background knowledge , for example , knowledge about what concepts are expressed by the words being read , and what relations hold between these concepts ( relational knowledge ) .",Introduction,Introduction,natural_language_inference,11,1,1,0,,0.565454713,1,research-problem,8.88E-07,0.000167888,2.79E-07,1.40E-05,3.44E-05,7.27E-06,7.99E-06,6.41E-06,4.86E-05,0.079650678,0.920059094,9.65E-07,1.53E-06
5744,natural_language_inference11,14,"As a simple illustration , if an agent needs to understand that the statement "" King Farouk signed his abdication "" is entailed by "" King Farouk was exiled to France in 1952 , after signing his resignation "" , it must know ( among other things ) that abdication means resignation of a king .",Introduction,Introduction,natural_language_inference,11,2,1,0,,0.002899357,0,negative,3.66E-06,0.002314751,1.12E-06,2.11E-05,0.000237909,7.53E-05,1.19E-05,7.71E-05,0.004412603,0.883640296,0.10920084,1.79E-06,1.61E-06
5745,natural_language_inference11,15,"In most neural natural language understanding ( NLU ) systems , the requisite background knowl - edge is implicitly encoded in the models ' parameters .",Introduction,Introduction,natural_language_inference,11,3,1,0,,0.952687594,1,research-problem,1.24E-06,0.000823709,8.85E-07,6.67E-06,2.02E-05,8.73E-06,1.18E-05,1.42E-05,0.000318947,0.063673121,0.935117054,1.49E-06,1.95E-06
5746,natural_language_inference11,16,"That is , what background knowledge is present has been learned from task supervision and also by pre-training word embeddings ( where distributional properties correlate with certain kinds of useful background knowledge , such as semantic relatedness ) .",Introduction,Introduction,natural_language_inference,11,4,1,0,,0.091095364,0,negative,3.30E-05,0.050221437,1.81E-05,5.68E-05,0.005082817,9.87E-05,1.62E-05,0.00010692,0.022804241,0.906908647,0.014643482,7.38E-06,2.26E-06
5747,natural_language_inference11,17,"However , acquisition of background knowledge from static training corpora is limiting for two reasons .",Introduction,Introduction,natural_language_inference,11,5,1,0,,0.037753878,0,research-problem,6.52E-06,0.001318657,1.95E-06,2.81E-05,0.000117138,2.71E-05,1.63E-05,2.89E-05,0.000371751,0.396296962,0.601780485,3.47E-06,2.65E-06
5748,natural_language_inference11,18,"First , it is unreasonable to expect that all background knowledge that could be important for solving an NLU task can be extracted from a limited amount of training data .",Introduction,Introduction,natural_language_inference,11,6,1,0,,0.007503433,0,negative,3.70E-05,0.007138395,3.86E-06,5.61E-05,0.001256024,5.89E-05,1.40E-05,6.34E-05,0.001818431,0.971822728,0.017721807,7.97E-06,1.32E-06
5749,natural_language_inference11,19,"Second , as the world changes , the facts that may influence how a text is understood will likewise change .",Introduction,Introduction,natural_language_inference,11,7,1,0,,0.00683571,0,negative,2.61E-05,0.00350286,7.03E-06,0.000150172,0.002578584,0.000161321,2.21E-05,6.16E-05,0.003910412,0.982184722,0.007389106,3.79E-06,2.17E-06
5750,natural_language_inference11,20,"In short : building suitably large corpora to capture all relevant information , and keeping the corpus and derived models up to date with changes to the world would be impractical .",Introduction,Introduction,natural_language_inference,11,8,1,0,,0.005761556,0,negative,5.72E-06,0.000621653,7.85E-07,0.000113604,0.000143656,9.77E-05,3.17E-05,7.87E-05,0.000268709,0.542505999,0.456121909,4.02E-06,5.85E-06
5751,natural_language_inference11,21,"In this paper , we develop a new architecture for dynamically incorporating external background knowledge in NLU models .",Introduction,Introduction,natural_language_inference,11,9,1,1,model,0.963990005,1,model,3.37E-05,0.183813947,0.00018923,2.73E-06,0.000204985,2.52E-05,5.26E-05,4.88E-05,0.766452087,0.016907012,0.032247221,1.70E-05,5.48E-06
5752,natural_language_inference11,22,"Rather than relying only on static knowledge implicitly present in the training data , supplementary knowledge is retrieved from external knowledge sources ( in this paper , ConceptNet and Wikipedia ) to assist with understanding text inputs .",Introduction,Introduction,natural_language_inference,11,10,1,1,model,0.721799936,1,approach,0.000114451,0.576293942,0.000235407,3.16E-05,0.014037133,8.83E-05,5.78E-05,0.000121826,0.293701078,0.111075139,0.004207988,2.91E-05,6.23E-06
5753,natural_language_inference11,23,"Since NLU systems must already read and understand text inputs , we assume that background knowledge will likewise be provided in text form ( 2 ) .",Introduction,Introduction,natural_language_inference,11,11,1,0,,0.058743135,0,negative,4.55E-06,0.10679595,1.38E-05,5.52E-06,0.000302175,0.00024764,3.65E-05,0.000845601,0.437684468,0.441021785,0.013034277,4.20E-06,3.58E-06
5754,natural_language_inference11,24,The retrieved supplementary texts are read together with the task inputs by an initial reading module whose outputs are contextually refined word embeddings ( 3 ) .,Introduction,Introduction,natural_language_inference,11,12,1,1,model,0.91562431,1,model,5.96E-06,0.043236166,3.79E-05,2.49E-07,5.15E-05,8.80E-06,6.00E-06,1.80E-05,0.945863847,0.010211674,0.000557658,1.68E-06,5.54E-07
5755,natural_language_inference11,25,These refined embeddings are then used as input to a task - specific NLU architecture ( any architecture that reads text as a sequence of word embeddings can be used here ) .,Introduction,Introduction,natural_language_inference,11,13,1,1,model,0.908686784,1,model,3.58E-06,0.040240968,2.96E-05,8.02E-07,7.93E-05,2.84E-05,1.00E-05,6.35E-05,0.929965916,0.028764889,0.00081065,1.28E-06,1.09E-06
5756,natural_language_inference11,26,"The initial reading module and the task module are learnt jointly , end - to - end .",Introduction,Introduction,natural_language_inference,11,14,1,1,model,0.940000349,1,model,2.02E-06,0.032923615,1.62E-05,6.51E-08,1.49E-05,2.78E-06,2.08E-06,7.78E-06,0.963234526,0.003612759,0.000182533,5.18E-07,1.90E-07
5757,natural_language_inference11,27,We experiment with several different datasets on the tasks of document question answering ( DQA ) and recognizing textual entailment evaluating the impact of our proposed solution with both basic task architectures and a sophisticated task architecture for RTE ( 4 ) .,Introduction,Introduction,natural_language_inference,11,15,1,0,,0.223539486,0,approach,8.80E-05,0.466524064,0.00035324,0.00017225,0.122803268,0.000435207,0.001697973,0.000428781,0.013446575,0.364800387,0.028877043,0.000304613,6.86E-05
5758,natural_language_inference11,28,We find that our embedding refinement strategy is effective ( 5 ) .,Introduction,Introduction,natural_language_inference,11,16,1,0,,0.03055464,0,negative,0.001898447,0.090643699,6.06E-05,0.000217071,0.005590461,0.001439229,0.002615355,0.004354195,0.016074946,0.865534107,0.010507283,0.000984007,8.06E-05
5759,natural_language_inference11,29,"On four competitive benchmarks , we show that refinement helps .",Introduction,Introduction,natural_language_inference,11,17,1,0,,0.042985941,0,negative,0.006437325,0.222860578,0.000125373,0.000147075,0.004383654,0.000719643,0.002678775,0.001745985,0.036161818,0.714869277,0.007895569,0.001893787,8.11E-05
5760,natural_language_inference11,30,"First , simply refining the embeddings just using the context ( and no additional background information ) can improve performance significantly , but adding background knowledge helps further .",Introduction,Introduction,natural_language_inference,11,18,1,0,,0.138016843,0,negative,0.062423503,0.061053865,0.000240182,0.002587939,0.018731557,0.003498065,0.038524241,0.005011367,0.003639194,0.781567949,0.008061192,0.013771798,0.000889149
5761,natural_language_inference11,31,"Our results are competitive with the best systems , achieving a new state of the art on the recent TriviaQA benchmarks .",Introduction,Introduction,natural_language_inference,11,19,1,0,,0.041468802,0,negative,0.000579524,0.029415172,5.93E-05,0.000159978,0.004549025,0.000751134,0.015571616,0.001240865,0.004292988,0.913793794,0.019162645,0.010123139,0.00030083
5762,natural_language_inference11,32,"Our success on this task is especially noteworthy because the task - specific architecture is a simple reading architecture , in particular a single layer BiLSTM with a feed - forward neural network for span prediction .",Introduction,Introduction,natural_language_inference,11,20,1,0,,0.181660516,0,negative,7.46E-05,0.023714846,4.33E-05,5.47E-05,0.005856438,0.000202054,0.000489729,0.000116234,0.005521612,0.933912468,0.029728644,0.000266465,1.89E-05
5763,natural_language_inference11,33,"Finally , we provide an analysis demonstrating that our systems are able to exploit background knowledge in a semantically appropriate manner ( 5.3 ) .",Introduction,Introduction,natural_language_inference,11,21,1,0,,0.009082415,0,negative,0.00023251,0.01040909,1.42E-05,0.000547767,0.017821113,0.000430701,0.000117336,0.000169391,0.002774182,0.966973322,0.000473067,3.05E-05,6.77E-06
5764,natural_language_inference11,34,"It includes , for instance , an experiment showing that our system is capable of making appropriate counterfactual inferences when provided with "" alternative facts "" .",Introduction,Introduction,natural_language_inference,11,22,1,0,,0.006252749,0,negative,7.18E-06,0.000477492,1.62E-06,0.000127645,0.003425859,0.000109957,3.53E-05,3.93E-05,0.000101837,0.994260122,0.001408146,3.34E-06,2.20E-06
5765,natural_language_inference11,35,External Knowledge as Supplementary,Introduction,,natural_language_inference,11,23,1,0,,0.038662282,0,negative,0.000175053,0.018932345,9.39E-05,0.000123978,0.00313391,0.000807764,0.00046495,0.000580087,0.062875763,0.906866245,0.005810143,0.000108634,2.72E-05
5766,natural_language_inference11,36,Text Inputs,Introduction,,natural_language_inference,11,24,1,0,,0.108502328,0,negative,6.28E-05,0.004758556,4.85E-05,0.000473772,0.01479061,0.001510492,0.005771114,0.000638959,0.001303406,0.952534903,0.017243148,0.000688963,0.000174826
5767,natural_language_inference11,37,"Knowledge resources make information that could potentially be useful for improving NLU available in a variety different formats , such as natural language text , ( subject , predicate , object ) triples , relational data bases , and other structured formats .",Introduction,Text Inputs,natural_language_inference,11,25,1,0,,0.000213199,0,negative,3.70E-06,4.08E-05,1.14E-06,4.21E-06,0.000206496,1.72E-05,3.89E-06,2.98E-06,2.40E-05,0.998877169,0.000816682,1.21E-06,4.87E-07
5768,natural_language_inference11,38,"Rather than tailoring our solution to a particular structured representation , we assume that all supplementary information either already exists in natural language statements ( e.g. , encyclopedias ) or can easily be recoded as natural language .",Introduction,Text Inputs,natural_language_inference,11,26,1,0,,8.47E-06,0,negative,1.51E-05,0.007526303,1.58E-05,5.19E-07,0.000167879,2.10E-05,2.84E-06,3.10E-05,0.013205565,0.978976968,3.39E-05,3.01E-06,1.67E-07
5769,natural_language_inference11,39,"Furthermore , while mapping from unstructured to structured representations is hard , the inverse problem is easy .",Introduction,Text Inputs,natural_language_inference,11,27,1,0,,8.91E-05,0,negative,2.05E-06,1.54E-05,1.23E-07,8.23E-07,4.93E-06,6.14E-06,1.10E-06,2.49E-06,1.27E-05,0.998637614,0.001315946,5.88E-07,1.07E-07
5770,natural_language_inference11,40,"For example , given a triple ( abdication , ISA , resignation ) we can construct the free - text assertion "" Abdication is a resignation . "" using simple rules .",Introduction,Text Inputs,natural_language_inference,11,28,1,0,,6.66E-06,0,negative,1.16E-06,2.33E-05,3.90E-07,1.89E-07,1.08E-05,1.59E-05,1.63E-06,5.30E-06,0.000168332,0.999722573,4.99E-05,3.98E-07,1.03E-07
5771,natural_language_inference11,41,"Finally , the freetext format means that knowledge that exists only in unstructured text form such as encyclopedic knowledge ( e.g. , Wikipedia ) is usable by our system .",Introduction,Text Inputs,natural_language_inference,11,29,1,0,,1.12E-05,0,negative,6.88E-05,0.000291547,1.89E-05,1.02E-06,0.000714019,3.23E-05,7.29E-06,4.17E-06,0.000175694,0.998668609,5.63E-06,1.18E-05,1.47E-07
5772,natural_language_inference11,42,"An important question that remains to be answered is : given some text that is to be understood , what supplementary knowledge should be incorporated ?",Introduction,Text Inputs,natural_language_inference,11,30,1,0,,5.01E-06,0,negative,2.24E-06,1.87E-05,1.48E-07,1.27E-06,7.73E-06,2.08E-05,1.44E-06,7.41E-06,4.56E-05,0.999757731,0.000136328,4.46E-07,1.27E-07
5773,natural_language_inference11,43,"The retrieval and preparation of contextually relevant information from knowledge sources is a complex research topic by itself , and there are several statistical and more recently neural approaches as well as approaches based on reinforcement learning .",Introduction,Text Inputs,natural_language_inference,11,31,1,0,,0.005204463,0,negative,6.61E-06,0.000102381,1.30E-06,8.15E-06,4.02E-05,3.89E-05,2.36E-05,1.22E-05,3.96E-05,0.959785202,0.039935253,3.73E-06,2.92E-06
5774,natural_language_inference11,44,"Rather than learning both how to incorporate relevant information and which information is relevant , we use a heuristic retrieval mechanism ( 4 ) and focus on the integration model .",Introduction,Text Inputs,natural_language_inference,11,32,1,0,,2.05E-05,0,negative,9.42E-05,0.019985977,0.000167688,2.95E-07,0.000113651,2.50E-05,1.16E-05,3.21E-05,0.092398338,0.887127679,3.52E-05,7.74E-06,4.70E-07
5775,natural_language_inference11,45,"In the next section , we turn to the question of how to leverage the retrieved supplementary knowledge ( encoded as text ) in a NLU system .",Introduction,Text Inputs,natural_language_inference,11,33,1,0,,4.08E-05,0,negative,1.26E-05,0.0006494,2.65E-06,1.99E-07,4.62E-05,5.06E-06,1.27E-06,4.32E-06,0.000744819,0.99850074,2.95E-05,3.22E-06,7.64E-08
5776,natural_language_inference11,46,Refining Word Embeddings by Reading,Introduction,,natural_language_inference,11,34,1,0,,0.751492402,1,negative,2.53E-05,0.023210516,3.75E-05,1.55E-05,0.000152607,0.000453533,0.003496101,0.000807534,0.055244967,0.755381176,0.16079236,0.000261971,0.000120954
5777,natural_language_inference11,47,Virtually every NLU task - from document classification to translation to question answeringshould in theory be able to benefit from supplementary knowledge .,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,35,1,0,,0.000157649,0,negative,7.89E-06,2.95E-05,5.28E-06,1.05E-05,2.26E-05,9.72E-05,0.000303594,2.51E-05,6.18E-05,0.995975705,0.003375794,6.09E-05,2.40E-05
5778,natural_language_inference11,48,"While one could develop custom architectures for each task so as to read supplementary inputs , we would like ours to augment any existing NLU task architectures with the ability to read relevant information with minimal effort .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,36,1,0,,3.40E-06,0,negative,5.36E-06,2.65E-05,2.28E-06,1.06E-06,9.85E-06,4.91E-05,3.03E-05,2.34E-05,0.000364519,0.999462257,1.07E-05,1.37E-05,1.03E-06
5779,natural_language_inference11,49,"To realize this goal , we adopt the strategy of refining word embeddings ; that is , we replace static word embeddings with embeddings thatare functions of the task inputs and any supplementary inputs .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,37,1,0,,0.000876841,0,negative,0.000747614,0.010693244,0.000482407,2.21E-06,0.000278986,7.74E-05,0.000285498,8.40E-05,0.107124864,0.880091647,8.82E-06,0.000117428,5.87E-06
5780,natural_language_inference11,50,"Word embeddings can be considered a simple form of key - value memory stores that , in our case , not only contain general - purpose knowledge ( as in typical neural NLU systems ) but also contextual information ( including background knowledge ) .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,38,1,0,,4.17E-05,0,negative,9.88E-06,0.000117329,3.56E-05,7.53E-07,1.35E-05,6.36E-05,7.76E-05,2.95E-05,0.004758511,0.994819377,5.16E-05,1.80E-05,4.67E-06
5781,natural_language_inference11,51,The use of word - embeddings as memory has the advantage that it is transparent to the task - architecture which kinds of embeddings ( refined or unrefined ) are used .,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,39,1,0,,0.000292172,0,negative,0.000184558,0.000454127,0.000200607,5.73E-07,4.37E-05,2.67E-05,9.07E-05,1.16E-05,0.004869968,0.993966355,1.14E-05,0.000137731,1.97E-06
5782,natural_language_inference11,52,Our incremental refinement process encodes input texts followed by updates on the word embedding matrix in multiple reading steps .,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,40,1,0,,0.001418128,0,negative,0.00020347,0.007133179,0.000704258,1.11E-06,0.000135343,4.52E-05,0.000223762,3.63E-05,0.346761631,0.644638658,2.38E-05,8.23E-05,1.09E-05
5783,natural_language_inference11,53,"Words are first represented non-contextually ( i.e. , standard word embeddings ) , which can be conceived of as the columns in an embedding matrix E 0 .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,41,1,0,,0.00011967,0,negative,2.61E-05,0.000756983,0.000106037,1.55E-07,1.91E-05,2.50E-05,5.65E-05,2.57E-05,0.042437108,0.956529994,3.38E-06,1.26E-05,1.43E-06
5784,natural_language_inference11,54,At each progressive reading step ?,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,42,1,0,,5.51E-06,0,negative,2.07E-06,5.94E-06,1.11E-06,5.62E-08,8.74E-07,9.77E-06,1.68E-05,7.68E-06,0.000174915,0.999772526,2.32E-06,5.65E-06,2.41E-07
5785,natural_language_inference11,55,"1 , a new embedding matrix",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,43,1,0,,5.26E-05,0,negative,3.19E-05,8.48E-05,7.19E-06,6.32E-07,1.34E-05,6.95E-05,0.000114955,0.000135193,0.002481556,0.997039646,2.42E-06,1.67E-05,2.13E-06
5786,natural_language_inference11,56,"E is constructed by refining the embeddings from the previous step E ? 1 using ( userspecified ) contextual information X for reading step , which is a set of natural language sequences ( i.e. , texts ) .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,44,1,0,,4.65E-06,0,negative,1.95E-05,6.13E-05,1.54E-05,6.32E-08,1.39E-05,4.88E-06,1.79E-05,6.00E-06,0.000525746,0.99931602,3.72E-07,1.86E-05,2.65E-07
5787,natural_language_inference11,57,An illustration of our incremental refinement strategy can be found in .,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,45,1,0,,1.17E-06,0,negative,3.00E-06,5.25E-06,3.28E-06,2.22E-07,8.17E-06,1.06E-05,7.90E-06,3.84E-06,0.000222955,0.99973122,4.97E-07,2.75E-06,3.33E-07
5788,natural_language_inference11,58,"In the following , we define this procedure formally .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,46,1,0,,2.97E-07,0,negative,8.91E-07,1.14E-05,5.02E-07,5.71E-08,1.79E-06,6.43E-06,6.48E-06,9.04E-06,0.000185351,0.999775649,2.04E-07,2.12E-06,1.16E-07
5789,natural_language_inference11,59,We denote the hidden dimensionality of ... :,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,47,1,0,,1.40E-06,0,negative,2.43E-06,1.14E-05,1.02E-06,1.15E-07,3.10E-06,2.94E-05,1.95E-05,3.49E-05,0.00024265,0.999652989,2.60E-07,1.95E-06,3.13E-07
5790,natural_language_inference11,60,"Illustration of our context - dependent , refinement strategy for word representations on an example from the SNLI dataset comprising the premise ( X 1 = {p} ) , hypothesis ( X 2 = {q} ) and additional external information inform of free - text assertions from ConceptNet ( X 1 = A ) .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,48,1,0,,4.39E-06,0,negative,4.17E-05,7.85E-05,6.39E-05,1.51E-07,1.16E-05,1.78E-05,0.000268751,8.45E-06,0.001122722,0.998203124,1.25E-05,0.000164968,5.81E-06
5791,natural_language_inference11,61,"Note that for the QA task there would be another stage that additionally integrates Wikipedia abstracts of answer candidates ( X 4 = W , see 4 ) .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,49,1,0,,7.39E-06,0,negative,1.60E-05,7.23E-06,9.39E-06,4.17E-08,3.89E-06,7.43E-06,2.16E-05,2.81E-06,0.000222295,0.999692165,3.50E-07,1.64E-05,3.63E-07
5792,natural_language_inference11,62,The reading architecture constructs refinements of word representations incrementally ( conceptually represented as columns in a series of embedding matrices ),Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,50,1,0,,0.000160101,0,negative,2.42E-05,0.000239805,0.000468648,7.94E-07,2.88E-05,4.98E-05,0.000152294,2.02E-05,0.0536776,0.945285456,1.78E-05,1.66E-05,1.79E-05
5793,natural_language_inference11,63,"E are incrementally refined by reading the input text and textual renderings of relevant background knowledge before computing the representations used by the task model ( in this figure , RTE ) .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,51,1,0,,1.26E-05,0,negative,6.91E-05,0.000397677,0.000393048,1.33E-07,2.81E-05,1.74E-05,0.000129232,1.09E-05,0.022866318,0.976032972,1.92E-06,4.90E-05,4.17E-06
5794,natural_language_inference11,64,our model by n and a fully - connected layer by,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,52,1,0,,9.47E-06,0,negative,2.64E-05,0.000195005,4.82E-05,1.29E-06,4.07E-05,0.000215655,0.000445718,0.000207434,0.011212791,0.987563273,3.89E-06,1.65E-05,2.30E-05
5795,natural_language_inference11,65,Unrefined Word Embeddings ( E 0 ),Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,53,1,0,,3.80E-05,0,negative,2.23E-05,3.48E-05,8.51E-05,2.87E-08,2.56E-06,1.37E-05,0.000249772,6.90E-06,0.001189696,0.998304944,1.42E-06,8.70E-05,1.83E-06
5796,natural_language_inference11,66,"The first representation level consists of noncontextual word representations , that is , word representations that do not depend on any input ; these can be conceived of as an embedding matrix E 0 whose columns are indexed by words in ? * .",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,54,1,0,,8.02E-05,0,negative,3.42E-05,0.000150631,0.000192702,5.10E-07,5.21E-05,4.70E-05,0.000132079,2.16E-05,0.008122926,0.991223875,8.53E-07,1.46E-05,6.92E-06
5797,natural_language_inference11,67,"The non-contextual word representation e 0 w for a single word w is computed by using a gated combination of fixed , pre-trained word vectors e p w ?",Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,55,1,0,,2.56E-06,0,negative,3.00E-06,1.01E-05,2.30E-06,3.04E-08,3.45E-06,6.82E-06,1.12E-05,7.34E-06,0.000165265,0.999787177,6.07E-08,3.08E-06,1.57E-07
5798,natural_language_inference11,68,Rn with learned character - based embeddings e char w ?,Introduction,Refining Word Embeddings by Reading,natural_language_inference,11,56,1,0,,3.05E-05,0,negative,1.31E-06,1.46E-05,7.44E-06,8.44E-08,2.50E-06,3.23E-05,0.000139053,2.41E-05,0.000144246,0.999609225,4.10E-06,1.76E-05,3.38E-06
5799,natural_language_inference11,69,Rn .,Introduction,,natural_language_inference,11,57,1,0,,0.00081427,0,negative,7.35E-06,0.001123494,6.16E-06,3.70E-06,0.000238555,0.00024956,0.000211741,0.000232216,0.002315845,0.995574046,2.32E-05,9.34E-06,4.80E-06
5800,natural_language_inference11,70,We compute e char w using a single - layer convolutional neural network with n convolutional filters of width 5 followed by a max - pooling operation overtime .,Introduction,Rn .,natural_language_inference,11,58,1,0,,0.071319051,0,negative,6.64E-05,0.005852118,6.13E-05,5.07E-05,0.005343041,0.028732446,0.002481899,0.021465987,0.008018628,0.92749762,3.25E-06,1.25E-05,0.000414082
5801,natural_language_inference11,71,The formal definition of this combination is given in Eq.,Introduction,Rn .,natural_language_inference,11,59,1,0,,0.000667663,0,negative,1.13E-05,0.000233404,3.47E-05,1.01E-07,8.78E-05,1.19E-05,4.14E-06,4.67E-06,0.0033155,0.996291597,5.22E-07,3.89E-06,4.66E-07
5802,natural_language_inference11,72,1 .,Introduction,Rn .,natural_language_inference,11,60,1,0,,0.000127242,0,negative,3.01E-06,2.30E-05,7.44E-07,4.00E-07,2.96E-05,3.91E-05,2.71E-06,2.16E-05,0.000235037,0.999643353,2.34E-07,5.62E-07,6.03E-07
5803,natural_language_inference11,73,"In order to compute contextually refined word embeddings E given prior representations E ? 1 we assume a given set of texts X = {x 1 , x 2 , . . .} thatare to be read at refinement iteration .",Introduction,Rn .,natural_language_inference,11,61,1,0,,0.000855236,0,negative,5.17E-06,0.001574143,7.84E-06,9.53E-07,0.000699851,4.83E-05,8.90E-06,7.46E-05,0.003256397,0.994317981,9.12E-07,2.73E-06,2.31E-06
5804,natural_language_inference11,74,Each text xi is a sequence of word tokens .,Introduction,Rn .,natural_language_inference,11,62,1,0,,0.000657119,0,negative,6.68E-06,0.000605095,1.35E-05,3.12E-07,0.0001893,3.81E-05,7.56E-06,3.67E-05,0.01037477,0.98872277,7.17E-07,1.85E-06,2.71E-06
5805,natural_language_inference11,75,"We embed all tokens of every x i using the embedding matrix from the previous layer , E ?1 .",Introduction,Rn .,natural_language_inference,11,63,1,0,,0.011706165,0,negative,9.29E-05,0.004951725,3.61E-05,2.83E-06,0.000915558,0.000827694,0.00019292,0.0014367,0.025673763,0.965836829,5.60E-07,6.45E-06,2.59E-05
5806,natural_language_inference11,76,"To each word , we concatenate a one - hot vector of length L with position set to 1 , indicating which layer is currently being processed .",Introduction,Rn .,natural_language_inference,11,64,1,0,,0.051403815,0,negative,4.75E-05,0.008716785,4.77E-05,2.40E-06,0.000742958,0.000987011,0.000211354,0.002015284,0.101982265,0.88520158,1.24E-06,4.19E-06,3.98E-05
5807,natural_language_inference11,77,1,Introduction,Rn .,natural_language_inference,11,65,1,0,,6.23E-05,0,negative,1.84E-06,1.62E-05,6.66E-07,2.72E-07,2.11E-05,2.53E-05,2.27E-06,1.43E-05,0.000289731,0.999627351,1.12E-07,3.34E-07,5.35E-07
5808,natural_language_inference11,78,"Stacking the vectors into a matrix , we obtain a X i ?",Introduction,Rn .,natural_language_inference,11,66,1,0,,0.001227904,0,negative,3.95E-05,0.000317817,1.63E-05,2.32E-07,0.000119436,3.99E-05,2.39E-05,1.88E-05,0.002177745,0.99723361,3.57E-07,1.07E-05,1.64E-06
5809,natural_language_inference11,79,R d|x i | .,Introduction,Rn .,natural_language_inference,11,67,1,0,,0.000175694,0,negative,8.35E-06,2.11E-05,2.38E-06,3.65E-07,5.29E-05,3.01E-05,8.14E-06,1.21E-05,7.75E-05,0.999782677,1.74E-07,3.05E-06,1.16E-06
5810,natural_language_inference11,80,"This matrix is processed by a bidirectional recurrent neural network , a BiLSTM ( Hochreiter and Schmidhuber , 1997 ) in this work .",Introduction,Rn .,natural_language_inference,11,68,1,0,,0.063581301,0,negative,2.30E-05,0.003531413,0.000233212,5.03E-07,0.000392966,6.33E-05,2.70E-05,4.35E-05,0.180340271,0.815328524,1.16E-06,3.51E-06,1.16E-05
5811,natural_language_inference11,81,The resulting output is further projected to X i by a fully - connected layer with ReLU activation ( Eq. 2 ) .,Introduction,Rn .,natural_language_inference,11,69,1,0,,0.005120201,0,negative,2.66E-05,0.001318792,8.70E-05,3.31E-07,0.000157363,5.71E-05,2.28E-05,5.98E-05,0.049639504,0.948620205,3.49E-07,2.59E-06,7.42E-06
5812,natural_language_inference11,82,"To finally update the previous embedding e ? 1 w of word w , we initially maxpool all representations of occurrences matching the lemma of win every x ?",Introduction,Rn .,natural_language_inference,11,70,1,0,,0.002854832,0,negative,0.00011971,0.003803254,0.000124311,2.84E-07,0.000354382,8.77E-05,7.50E-05,7.11E-05,0.036325862,0.95902202,3.15E-07,1.02E-05,5.83E-06
5813,natural_language_inference11,83,X resulting in w ( Eq. 3 ) .,Introduction,Rn .,natural_language_inference,11,71,1,0,,0.001568386,0,negative,1.95E-05,8.27E-05,1.33E-05,3.46E-08,3.58E-05,7.02E-06,5.11E-06,3.90E-06,0.000585165,0.999242646,4.76E-08,4.42E-06,3.57E-07
5814,natural_language_inference11,84,"Finally , we combine the previous representation e ? 1 w with w to form an updated representation e w via a gated addition .",Introduction,Rn .,natural_language_inference,11,72,1,0,,0.005231191,0,negative,3.25E-05,0.001517897,0.000138797,4.08E-07,0.00029833,4.13E-05,1.67E-05,3.01E-05,0.048625928,0.949289247,2.44E-07,3.25E-06,5.28E-06
5815,natural_language_inference11,85,This lets the model determine how much to revise the previous embedding with the newly read information ( Eq. 5 ) .,Introduction,Rn .,natural_language_inference,11,73,1,0,,0.001142552,0,negative,2.34E-05,0.000912413,3.85E-05,3.32E-08,4.38E-05,7.26E-06,4.76E-06,8.03E-06,0.028786467,0.970172115,8.73E-08,2.55E-06,6.30E-07
5816,natural_language_inference11,86,"Note that we soften the matching condition for w using lemmatization , 2 lemma ( w ) , during the pooling operation of Eq. 3 because contextual information about certain words is usually independent of the current word form w they appear in .",Introduction,Rn .,natural_language_inference,11,74,1,0,,0.002397344,0,negative,0.000136014,0.002716566,5.88E-05,2.98E-07,0.000339039,4.48E-05,2.77E-05,4.21E-05,0.012686601,0.983935652,8.48E-08,9.99E-06,2.33E-06
5817,natural_language_inference11,87,"As a consequence , this minor linguistic preprocessing step allows for additional interaction between tokens of the same lemma .",Introduction,Rn .,natural_language_inference,11,75,1,0,,0.013675977,0,negative,0.000797026,0.000443188,2.33E-05,2.29E-06,0.001201895,3.75E-05,1.42E-05,1.20E-05,0.001257981,0.996194775,8.05E-08,1.36E-05,2.19E-06
5818,natural_language_inference11,88,"Pooling over lemma-occurrences effectively connects different text passages ( even across texts ) thatare otherwise disconnected , mitigating the problems arising from long - distance dependencies .",Introduction,Rn .,natural_language_inference,11,76,1,0,,0.375824786,0,negative,0.001696358,0.009174092,0.000593832,4.68E-06,0.002020331,0.000216226,0.000936397,0.000101943,0.017218647,0.967352944,2.68E-06,0.000560781,0.000121095
5819,natural_language_inference11,89,"This is reminiscent of the ( soft ) attention mechanism used in reading comprehension models ( e.g. , ; ) .",Introduction,Rn .,natural_language_inference,11,77,1,0,,0.000378321,0,negative,3.31E-06,0.000104739,9.00E-06,1.14E-07,1.96E-05,2.05E-05,6.66E-06,1.52E-05,0.003486941,0.996330757,3.06E-07,8.89E-07,2.00E-06
5820,natural_language_inference11,90,"However , our setup is more general as it allows for the connection of multiple passages ( via pooling ) at once and is able to deal with multiple inputs which is necessary to make use of additional input texts such as relevant background knowledge .",Introduction,Rn .,natural_language_inference,11,78,1,0,,0.047179918,0,negative,4.64E-05,0.005061394,6.79E-05,3.11E-06,0.001686769,9.20E-05,2.92E-05,5.83E-05,0.008098806,0.984837455,2.27E-07,1.13E-05,7.15E-06
5821,natural_language_inference11,91,Experimental Setup,,,natural_language_inference,11,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
5822,natural_language_inference11,92,We run experiments on four benchmarks for two standard NLU tasks : recognizing textual entailment ( RTE ) and document question answering ( DQA ) .,Experimental Setup,Experimental Setup,natural_language_inference,11,1,1,0,,0.294284765,0,negative,0.000265688,0.000699246,0.032911084,2.47E-05,5.45E-05,0.015864088,0.012055754,0.038768609,4.57E-05,0.891125983,0.000560411,0.007525985,9.82E-05
5823,natural_language_inference11,93,In the following we describe our experimental setup .,Experimental Setup,Experimental Setup,natural_language_inference,11,2,1,0,,0.002942472,0,negative,4.92E-05,6.51E-05,0.00067765,2.45E-05,7.16E-06,0.01108054,0.000398615,0.026514769,4.02E-05,0.960966254,2.01E-05,0.000144996,1.10E-05
5824,natural_language_inference11,94,Task - specific,Experimental Setup,Experimental Setup,natural_language_inference,11,3,1,0,,0.197033245,0,negative,0.002377423,4.90E-05,0.04670345,4.79E-05,1.88E-05,0.023147105,0.006877721,0.028673252,7.35E-05,0.871194388,9.22E-05,0.02058511,0.000160158
5825,natural_language_inference11,95,Models,,,natural_language_inference,11,0,1,0,,0.013378693,0,negative,0.001769058,0.000281584,0.001312991,0.00016118,0.000150821,0.001858418,0.032853537,0.00468419,0.00015464,0.882857056,0.025147519,0.048083361,0.000685644
5826,natural_language_inference11,96,"Since we wish to assess the value of the proposed embedding refinement strategy , we focus on relatively simple task architectures .",Models,Models,natural_language_inference,11,1,1,0,,0.048508536,0,negative,0.000598401,0.000104547,0.01481849,2.35E-06,6.86E-06,0.000629184,0.059941428,0.00052203,0.000197078,0.92229381,2.21E-05,0.000791615,7.21E-05
5827,natural_language_inference11,97,"We use single - layer bidirectional LSTMs ( BiLSTMs ) as encoders of the inputs represented by the refined or unrefined embeddings with a task - specific , feed - forward network for the final prediction .",Models,Models,natural_language_inference,11,2,1,0,,0.943309949,1,baselines,0.000531368,0.000160787,0.616702423,5.42E-06,4.14E-06,0.00292055,0.133400574,0.002121405,0.014468367,0.22887236,2.96E-05,8.42E-05,0.000698731
5828,natural_language_inference11,98,Such models are general reading architectures .,Models,Models,natural_language_inference,11,3,1,0,,0.011802888,0,negative,0.000111468,6.58E-06,0.148005639,4.53E-06,1.71E-06,0.001303163,0.090280448,0.000278415,0.000511703,0.758520724,0.000117936,0.000185496,0.000672187
5829,natural_language_inference11,99,"To demonstrate that our reading module can be integrated into arbitrary task architectures , we also add our refinement module to a reimplementation of a state of the art architecture for RTE called ESIM .",Models,Models,natural_language_inference,11,4,1,0,,0.52944124,1,baselines,0.002847228,0.00020421,0.544055711,1.50E-06,7.10E-06,0.000217726,0.057560403,0.00011361,0.006476381,0.387247174,2.05E-05,0.001046246,0.000202228
5830,natural_language_inference11,100,We refer the interested reader to the ESIM paper for details of the model .,Models,Models,natural_language_inference,11,5,1,0,,0.006617732,0,negative,3.90E-05,1.58E-06,0.000459193,3.92E-06,1.48E-06,0.000219953,0.003564314,8.91E-05,2.40E-05,0.995509837,1.63E-06,6.36E-05,2.24E-05
5831,natural_language_inference11,101,All models are trained end - to - end jointly with the refinement module using a dimensionality of n = 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints .,Models,Models,natural_language_inference,11,6,1,1,hyperparameters,0.94322271,1,experiments,0.00021135,2.99E-05,0.002989386,8.61E-06,3.95E-06,0.016013606,0.897110651,0.011696505,0.00014156,0.070298036,3.57E-06,9.07E-05,0.001402174
5832,natural_language_inference11,102,All baselines operate on the unrefined word embeddings E 0 described in 3.1 .,Models,Models,natural_language_inference,11,7,1,1,hyperparameters,0.270154225,0,negative,0.00014966,3.42E-05,0.029163808,1.69E-06,2.26E-06,0.003078342,0.229013048,0.00381673,0.000229515,0.734216349,2.73E-06,0.000122114,0.000169517
5833,natural_language_inference11,103,For the DQA baseline system we add the lemma in - question feature ( liq ) suggested in .,Models,Models,natural_language_inference,11,8,1,1,hyperparameters,0.868352554,1,baselines,0.001323972,2.76E-05,0.649360842,8.06E-07,2.97E-06,0.000491721,0.118550758,0.000196304,0.000443135,0.228947962,2.27E-06,0.000470386,0.000181288
5834,natural_language_inference11,104,"Implementation details for the BiL - STM task architectures , as well as training details , are available in Appendix A.",Models,Models,natural_language_inference,11,9,1,0,,0.016699118,0,negative,0.000108075,4.67E-06,0.001264377,0.000130033,3.49E-05,0.004008148,0.098837165,0.000383501,2.55E-05,0.894221317,7.29E-06,0.000304722,0.000670285
5835,natural_language_inference11,105,Question Answering,Models,,natural_language_inference,11,10,1,0,,0.962064551,1,experiments,0.000589685,2.04E-06,0.00109377,6.82E-05,5.86E-05,0.000171996,0.949827901,1.66E-05,1.45E-06,0.018617364,5.54E-05,0.010852605,0.018644413
5836,natural_language_inference11,106,Table 1 presents our results on two question answering benchmarks .,Models,Question Answering,natural_language_inference,11,11,1,0,,0.116328079,0,experiments,0.000293343,4.13E-07,7.81E-06,1.04E-07,1.81E-06,7.16E-05,0.598858148,2.81E-06,3.47E-07,0.398117522,1.97E-06,0.002166508,0.000477658
5837,natural_language_inference11,107,"The results demonstrate that the introduction of the refinement module helps consistently , and further improvements come from using commonsense knowledge from Concept - 6 Statistics were extracted from the DBpedia Anchor Text dataset ( http://downloads.dbpedia.org/ 2016-10/core-i18n/en/anchor_text_en.ttl. bz2 ) .",Models,Question Answering,natural_language_inference,11,12,1,0,,0.791300118,1,experiments,0.03690998,1.10E-06,4.89E-06,9.18E-07,2.16E-06,8.03E-05,0.940553058,4.70E-06,1.05E-06,0.017300106,7.46E-07,0.003268351,0.001872651
5838,natural_language_inference11,108,Recognizing Textual Entailment,Models,,natural_language_inference,11,13,1,0,,0.948075595,1,experiments,0.000468029,2.95E-06,0.002060817,4.40E-06,6.22E-06,5.66E-05,0.935755042,1.75E-05,1.17E-06,0.020565597,0.000129308,0.031612687,0.009319685
5839,natural_language_inference11,109,Supplementary Knowledge Sources,Models,,natural_language_inference,11,14,1,0,,0.06950826,0,negative,0.000321155,3.45E-06,0.001034657,1.25E-05,8.79E-06,0.000447472,0.060950965,0.000287307,4.30E-05,0.935281376,4.28E-06,0.001226201,0.000378889
5840,natural_language_inference11,110,We use,Models,,natural_language_inference,11,15,1,0,,0.01212329,0,negative,0.000291525,1.56E-06,0.001299632,1.26E-06,3.66E-06,0.000258767,0.019140725,0.000127129,1.28E-05,0.978584606,2.56E-07,0.000239675,3.84E-05
5841,natural_language_inference11,111,"ConceptNet 3 ( Speer and Havasi , 2012 ) , a freelyavailable , multi-lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources , such as Wiktionary , Open Multilingual WordNet , OpenCyc and DBpedia .",Models,We use,natural_language_inference,11,16,1,0,,0.099826131,0,baselines,0.000222106,1.59E-05,0.738680165,5.83E-05,2.98E-05,0.000547303,0.001826052,0.000668413,0.00016994,0.238312809,7.95E-06,0.000831827,0.018629429
5842,natural_language_inference11,112,It presents information in the form of relational triples .,Models,We use,natural_language_inference,11,17,1,0,,0.088376818,0,baselines,0.000164055,3.54E-06,0.926606432,8.01E-07,1.19E-06,1.27E-05,8.81E-05,1.94E-05,0.000125117,0.072150967,2.15E-06,0.00040114,0.000424522
5843,natural_language_inference11,113,"4 Additionally , we exploit Wikipedia abstracts in our DQA experiments as described below .",Models,We use,natural_language_inference,11,18,1,0,,0.069854986,0,negative,0.000360314,9.82E-06,0.134282247,1.46E-05,2.89E-05,0.000114475,0.000220442,0.000253638,8.25E-05,0.863044844,6.77E-07,0.000656213,0.000931354
5844,natural_language_inference11,114,ConceptNet Integration,Models,,natural_language_inference,11,19,1,0,,0.289879382,0,experiments,0.000905582,5.79E-06,0.187425078,6.96E-07,1.28E-06,0.000160446,0.536241862,4.89E-05,0.00011457,0.257437204,1.78E-05,0.015341693,0.00229908
5845,natural_language_inference11,115,"Here we describe the heuristic we use to obtain plausibly relevant supplementary knowledge for understanding a text pair ( p , q ) from ConceptNet .",Models,ConceptNet Integration,natural_language_inference,11,20,1,0,,1.61E-05,0,negative,0.000357897,5.50E-06,0.002084472,1.97E-07,3.10E-07,2.14E-06,3.22E-05,1.18E-05,3.83E-05,0.996329033,1.02E-06,0.001109566,2.76E-05
5846,natural_language_inference11,116,Our hypothesis is that relations that link words and phrases across p and q are likely to be most valuable .,Models,ConceptNet Integration,natural_language_inference,11,21,1,0,,1.33E-05,0,negative,7.23E-06,2.30E-07,6.17E-05,5.17E-08,1.58E-08,2.85E-06,6.69E-06,3.18E-05,1.12E-05,0.999838578,1.34E-07,2.60E-05,1.35E-05
5847,natural_language_inference11,117,"Because assertions a in ConceptNet come inform of ( subject , predicate , object ) - triples ( s , r , o ) , we retrieve all assertions for which s appears in q and o appears in p , or vice versa .",Models,ConceptNet Integration,natural_language_inference,11,22,1,0,,3.11E-05,0,negative,2.92E-05,1.33E-07,0.000241922,1.38E-08,3.41E-08,4.74E-07,2.28E-06,2.10E-06,4.76E-06,0.999649486,2.24E-08,6.67E-05,2.91E-06
5848,natural_language_inference11,118,"Because still too many such assertions might be retrieved for an instance , we rank all retrievals based on their respective subject and object .",Models,ConceptNet Integration,natural_language_inference,11,23,1,0,,2.18E-05,0,negative,1.93E-05,5.88E-08,9.94E-05,2.75E-09,1.13E-08,3.86E-07,3.21E-06,2.10E-06,1.24E-06,0.99983009,2.00E-09,4.38E-05,4.46E-07
5849,natural_language_inference11,119,"The ranking score we use is the inverse product of appearances of the subject and the object in the KB , that is score ( a ) = ( a I (s a = s a ) a I ( o a = o a ) ) ? 1 , where I denotes the indicator function .",Models,ConceptNet Integration,natural_language_inference,11,24,1,0,,3.85E-06,0,negative,3.42E-06,6.99E-08,9.59E-05,3.32E-09,3.21E-09,5.82E-07,2.92E-06,5.45E-06,4.50E-06,0.999875942,2.25E-08,1.00E-05,1.20E-06
5850,natural_language_inference11,120,"During training and evaluation we retain the top -k assertions , using k = 50 for DQA and k = 20 for RTE .",Models,ConceptNet Integration,natural_language_inference,11,25,1,0,,0.003046936,0,negative,4.57E-05,5.35E-07,8.06E-05,6.71E-08,5.49E-08,3.49E-05,0.000291914,0.000445482,5.95E-06,0.998967805,1.79E-08,6.47E-05,6.23E-05
5851,natural_language_inference11,121,Note that fewer or even no assertions might be retrieved for a particular instance during training and testing .,Models,ConceptNet Integration,natural_language_inference,11,26,1,0,,3.38E-06,0,negative,1.05E-05,7.13E-09,5.23E-06,1.70E-09,1.43E-09,9.20E-08,3.44E-07,6.72E-07,3.68E-07,0.99996645,1.80E-09,1.61E-05,2.15E-07
5852,natural_language_inference11,122,Wikipedia Integration,Models,,natural_language_inference,11,27,1,0,,0.603505315,1,experiments,0.001182522,2.32E-06,0.008070782,5.73E-06,2.26E-05,0.000220056,0.796258319,4.56E-05,6.41E-06,0.142817354,4.04E-06,0.037717362,0.013646968
5853,natural_language_inference11,123,Here we describe the heuristic we use to obtain plausibly relevant supplementary knowledge from Wikipedia .,Models,Wikipedia Integration,natural_language_inference,11,28,1,0,,0.000348514,0,negative,0.000848628,9.84E-06,0.000793897,2.70E-07,2.99E-06,9.92E-06,0.0009846,7.31E-06,3.92E-05,0.99706392,2.05E-07,0.000197627,4.16E-05
5854,natural_language_inference11,124,We wish to use Wikipedia abstracts 5 as an addi-tional knowledge source to gather more information about the top answer predictions of our DQA model .,Models,Wikipedia Integration,natural_language_inference,11,29,1,0,,0.001258292,0,negative,0.000123735,1.02E-07,4.85E-05,1.53E-07,6.95E-07,9.11E-06,0.000453668,2.60E-06,7.98E-07,0.999110732,1.19E-07,0.000142025,0.00010773
5855,natural_language_inference11,125,"To this end , we let the system first predict the top - 16 answer spans without any information from Wikipedia .",Models,Wikipedia Integration,natural_language_inference,11,30,1,0,,0.001200188,0,negative,0.000177153,1.39E-06,0.001408612,1.05E-08,3.08E-07,3.24E-06,0.000348726,2.31E-06,2.49E-05,0.998003754,9.49E-09,2.44E-05,5.17E-06
5856,natural_language_inference11,126,"For each answer candidate string , we collect abstracts for their 3 most frequently linked Wikipedia entries .",Models,Wikipedia Integration,natural_language_inference,11,31,1,0,,0.002110167,0,negative,0.000145794,6.75E-07,0.000273493,1.26E-07,1.73E-05,8.72E-06,0.000609879,3.03E-06,1.65E-06,0.998867015,6.10E-09,4.80E-05,2.43E-05
5857,natural_language_inference11,127,6,Models,Wikipedia Integration,natural_language_inference,11,32,1,0,,2.16E-05,0,negative,9.18E-06,5.86E-09,1.91E-06,2.55E-09,5.75E-09,8.86E-07,3.21E-05,5.39E-07,3.04E-07,0.999950143,2.83E-09,3.78E-06,1.18E-06
5858,natural_language_inference11,128,"Using more than only the most frequently linked Wikipedia entry for a given answer string , lets us mitigate problems arising from polysemous entity names , although it does mean the refinement model needs to be selective in extracting relevant information .",Models,Wikipedia Integration,natural_language_inference,11,33,1,0,,0.00018315,0,negative,0.005130501,2.79E-07,0.00018253,1.32E-08,3.03E-07,1.42E-06,0.001022976,4.99E-07,1.64E-06,0.99240601,3.15E-08,0.001241533,1.23E-05
5859,natural_language_inference11,129,The refinement module additionally reads the initial 50 tokens of each retrieved Wikipedia abstract and computes the final predictions .,Models,Wikipedia Integration,natural_language_inference,11,34,1,0,,0.024260868,0,negative,0.000409612,1.56E-06,0.013838236,1.04E-08,1.72E-07,4.35E-06,0.001054304,2.01E-06,0.000166841,0.98446815,2.17E-08,2.78E-05,2.69E-05
5860,natural_language_inference11,130,Refinement Order,Models,,natural_language_inference,11,35,1,0,,0.014405099,0,negative,0.000306202,3.96E-06,0.010211757,1.10E-06,2.98E-06,0.000261777,0.090871324,0.00014461,0.000345402,0.896436833,4.96E-07,0.000616523,0.000797045
5861,natural_language_inference11,131,"When employing our embedding - refinement strategy , we first read the document ( p ) followed by the question ( q ) in case of DQA , and the premise ( p ) followed by the hypothesis ( q ) for RTE , that is , X 1 = {p} and X 2 = {q}.",Models,Refinement Order,natural_language_inference,11,36,1,0,,1.62E-07,0,negative,4.72E-06,2.55E-07,8.70E-06,7.37E-10,8.84E-09,2.45E-07,5.98E-07,5.41E-06,2.41E-06,0.999892851,5.05E-09,8.48E-05,4.70E-08
5862,natural_language_inference11,132,"Additional knowledge in the form of a set of assertions A is integrated after reading the task - specific input for both DQA and RTE , that is , X 3 = A. Finally , for DQA we additionally add Wikipedia abstracts as background knowledge as described previously , that is , X 4 = W. In preliminary experiments we found that the final performance is not significantly sensitive to the order of presentation so we decided to fix our order as defined above .",Models,Refinement Order,natural_language_inference,11,37,1,0,,6.23E-07,0,negative,4.72E-05,9.91E-07,4.21E-05,1.40E-08,9.51E-08,2.97E-06,6.56E-06,3.75E-05,9.55E-06,0.999536674,9.32E-09,0.000315834,5.47E-07
5863,natural_language_inference11,133,Results,,,natural_language_inference,11,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
5864,natural_language_inference11,134,This section presents results .,Results,Results,natural_language_inference,11,1,1,0,,0.014047031,0,negative,0.002242833,4.78E-06,0.000106016,3.79E-06,4.61E-06,4.80E-05,0.000328557,0.000196325,6.39E-06,0.963525715,8.29E-06,0.033433251,9.14E-05
5865,natural_language_inference11,135,"We provide ablations for a total of 7 task - dataset - model combinations and compare our final results to other works on the most recent benchmark datasets for each task ( Trivi aQA and MultiNLI ) , demonstrating that our results are competitive , and in some cases , state of the art , even without sophisticated task architectures .",Results,Results,natural_language_inference,11,2,1,0,,0.830925601,1,results,0.040134308,5.96E-05,0.000464776,2.13E-05,3.00E-05,7.10E-05,0.003779463,0.000299982,1.75E-05,0.363090809,3.41E-05,0.59164994,0.00034741
5866,natural_language_inference11,136,Model,,,natural_language_inference,11,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
5867,natural_language_inference11,137,SQuAD T - Wiki T - Web Net ( A ) .,Model,Model,natural_language_inference,11,1,1,0,,0.001906796,0,negative,0.000857796,0.000213333,0.005195738,6.45E-05,4.44E-05,0.001043227,0.004324701,0.002181919,0.006289039,0.949864251,0.000576129,0.028688996,0.000655927
5868,natural_language_inference11,138,"Wikipedia ( W ) yields further , significant improvements on TriviaQA , slightly outperforming the current state of the art model .",Model,Model,natural_language_inference,11,2,1,1,results,0.705065855,1,results,0.007044655,4.38E-05,0.000628697,9.60E-06,1.52E-05,4.76E-05,0.003445817,0.000226176,6.26E-05,0.027608631,6.57E-05,0.960405357,0.000396076
5869,natural_language_inference11,139,"This is especially noteworthy given the simplicity of our QA architecture ( i.e. , a single layer BiL - STM ) compared to the previous SotA attained by .",Model,Model,natural_language_inference,11,3,1,0,,0.005381796,0,negative,0.001465509,0.000209224,0.00073105,1.24E-06,5.86E-06,2.06E-05,5.52E-05,0.000103071,0.0018205,0.987875511,8.35E-05,0.007623175,5.55E-06
5870,natural_language_inference11,140,"The development results on SQuAD 7 show the same pattern of improvement , but here the results are slightly worse than the model of Clark and Gardner , and they are way off from the current best - known results ( currently at 87 % F1 ) ; 8 however , our intention with these experiments is to show of the value that external knowledge and our refinement process can bring , not to compete with highly tuned task architectures on a single dataset .",Model,Model,natural_language_inference,11,4,1,0,,0.081239064,0,negative,0.002004756,5.94E-05,0.000489495,1.49E-06,6.42E-06,4.83E-05,0.001363687,0.000257534,0.000139859,0.557011458,7.16E-05,0.438505286,4.06E-05
5871,natural_language_inference11,141,Controlling for computation .,Model,Model,natural_language_inference,11,5,1,0,,0.001048945,0,negative,0.000104139,6.60E-05,8.15E-05,3.76E-06,3.64E-06,7.69E-05,6.05E-05,0.000424107,0.002340138,0.996406216,3.04E-05,0.000394257,8.48E-06
5872,natural_language_inference11,142,One potential explanation for the improvement obtained using the We do not report test set results for SQuAD due to restrictions on code sharing .,Model,Model,natural_language_inference,11,6,1,0,,0.000910015,0,negative,0.000471357,7.70E-05,0.000170597,5.94E-06,8.27E-06,0.000117376,0.000173701,0.000685637,0.000796016,0.994538704,6.27E-05,0.002872653,2.01E-05
5873,natural_language_inference11,143,"8 https://rajpurkar.github.io/SQuAD-explorer/ refinement module is that we are enabling more computation over the information present in the inputs , that is , we are effectively using a deeper architecture .",Model,Model,natural_language_inference,11,7,1,0,,0.035611041,0,negative,0.00436737,0.000326532,0.007333832,2.30E-05,4.51E-05,0.000196901,0.000265661,0.000583575,0.01998258,0.96382347,2.16E-05,0.002989928,4.04E-05
5874,natural_language_inference11,144,"To test whether this might be the case , we also ran an experiment with a 2 - layer BiLSTM ( + liq ) .",Model,Model,natural_language_inference,11,8,1,0,,0.007621571,0,negative,0.000771842,0.000150696,0.00065359,8.95E-07,8.06E-06,8.20E-05,0.000231751,0.00043975,0.001408237,0.992598741,9.86E-06,0.003639396,5.23E-06
5875,natural_language_inference11,145,This setup exhibits similar computational complexity and number of parameters to BiLSTM + p + q.,Model,Model,natural_language_inference,11,9,1,0,,0.001106109,0,negative,0.001616866,0.000830853,0.002875683,1.02E-05,8.69E-05,0.000183141,0.000450329,0.000919332,0.007446881,0.979547466,1.76E-05,0.005988739,2.60E-05
5876,natural_language_inference11,146,"We found that the second layer did not improve performance , suggesting that pooling over word / lemma occurrences in a given context between layers , is a powerful , yet simple technique .",Model,Model,natural_language_inference,11,10,1,0,,0.305589662,0,negative,0.065953022,0.000443626,0.00055789,2.90E-05,5.49E-05,0.000356228,0.001157764,0.001893203,0.00381976,0.790862299,0.000107364,0.134607045,0.000157974
5877,natural_language_inference11,147,shows the results of our RTE experiments .,Model,Model,natural_language_inference,11,11,1,1,results,0.005611452,0,negative,0.00012042,1.35E-05,9.62E-05,1.06E-06,7.76E-06,3.43E-05,0.000114106,0.000174375,0.000177295,0.996849755,3.01E-06,0.002402127,6.01E-06
5878,natural_language_inference11,148,"In general , the introduction of our refinement strategy almost always helps , both with and without external knowledge .",Model,Model,natural_language_inference,11,12,1,1,results,0.697433178,1,results,0.053656442,0.000156422,0.000181137,5.44E-05,5.77E-05,0.000562889,0.009564158,0.005048562,0.00044479,0.176054152,6.50E-05,0.75335335,0.000800934
5879,natural_language_inference11,149,"When providing additional background knowledge from ConceptNet , our BiLSTM based models improve substantially , while the ESIM - based models improve only on the more difficult MultiNLI dataset .",Model,Model,natural_language_inference,11,13,1,1,results,0.432541397,0,results,0.028409053,7.02E-05,0.000147638,3.78E-05,2.42E-05,0.000217651,0.006296337,0.002052363,0.000211359,0.047599367,3.39E-05,0.913930731,0.000969346
5880,natural_language_inference11,150,"Compared to previously published state of the art systems , our models acquit themselves quite well on the MultiNLI benchmark , and competitively on the SNLI benchmark .",Model,Model,natural_language_inference,11,14,1,1,results,0.330929604,0,results,0.001531388,2.68E-05,8.06E-05,4.12E-06,8.25E-06,5.19E-05,0.003083106,0.000385876,5.12E-05,0.070691446,2.14E-05,0.923798547,0.000265312
5881,natural_language_inference11,151,"In parallel to this work , Gong et al. ( 2017 ) developed a novel task - specific architecture for RTE that achieves slightly better performance on MultiNLI than our ESIM + p + q + A based models .",Model,Model,natural_language_inference,11,15,1,0,,0.019550782,0,negative,0.000514419,0.002557652,0.019759168,8.56E-06,3.15E-05,0.000182157,0.000794664,0.000722537,0.064616399,0.899670777,0.001331716,0.009541634,0.000268817
5882,natural_language_inference11,152,9,Model,Model,natural_language_inference,11,16,1,0,,5.89E-05,0,negative,1.45E-05,1.07E-05,6.28E-06,4.29E-07,6.54E-07,2.02E-05,9.72E-06,0.000180424,0.000794956,0.998911764,1.80E-06,4.72E-05,1.38E-06
5883,natural_language_inference11,153,"It draws attention to the fact that when using our knowledge - enhanced embed - ding module , on the MultiNLI , the basic BiLSTM task model outperforms the task - specific ESIM model , which is architecturally much more complex and designed specifically for the RTE task .",Model,Model,natural_language_inference,11,17,1,0,,0.511801489,1,results,0.045173674,8.54E-05,0.000415583,7.39E-06,3.02E-05,9.74E-05,0.002506936,0.000828962,0.000270585,0.279212343,1.08E-05,0.671236919,0.00012388
5884,natural_language_inference11,154,"We do find that there is little impact of using external knowledge on the RTE task with ESIM , although the refinement strategy helps using just p + q.",Model,Model,natural_language_inference,11,18,1,1,results,0.450678408,0,results,0.023400621,5.91E-05,0.000144136,5.91E-06,2.61E-05,6.65E-05,0.003110875,0.000713203,9.64E-05,0.153346905,1.47E-05,0.818831149,0.00018438
5885,natural_language_inference11,155,"A more detailed set of experiments reported in Appendix B shows that by impoverishing the amount of training data and information present in the GloVe embeddings , the positive impact of supplemental information becomes much more pronounced .",Model,Model,natural_language_inference,11,19,1,0,,0.040989682,0,negative,0.035012309,0.000120546,0.000153875,7.66E-06,5.64E-05,7.64E-05,0.001254915,0.000723547,0.000324521,0.748347777,7.88E-06,0.213844735,6.94E-05
5886,natural_language_inference11,156,"These results suggest that ESIM is able to learn important background information from the large - scale datasets and from pretrained embeddings , but this can be supplemented when necessary .",Model,Model,natural_language_inference,11,20,1,0,,0.094735939,0,negative,0.037214805,0.000568633,0.000486053,1.35E-05,7.20E-05,0.000112414,0.001088307,0.000998963,0.002472834,0.717353062,3.24E-05,0.239464156,0.000122868
5887,natural_language_inference11,157,"Nevertheless , both ESIM and our BiL - STM models when trained with knowledge from ConceptNet are sensitive to the semantics of the provided assertions as demonstrated in our analysis in 5.3 .",Model,Model,natural_language_inference,11,21,1,1,results,0.060313815,0,negative,0.03699821,0.000157005,0.000437787,5.32E-06,4.28E-05,4.99E-05,0.000727685,0.000398268,0.000432058,0.700368437,1.55E-05,0.260320698,4.63E-05
5888,natural_language_inference11,158,We argue that this is a desirable side effect because it makes the predictions of our model more interpretable than those not trained with knowledge .,Model,Model,natural_language_inference,11,22,1,0,,0.000751383,0,negative,0.000264405,5.65E-05,3.88E-05,1.91E-06,5.59E-06,3.27E-05,3.37E-05,0.000532319,0.001165994,0.99722231,4.08E-06,0.000635495,6.21E-06
5889,natural_language_inference11,159,"Furthermore , increasing the coverage of assertions in ConceptNet would most likely yield improved performance even without retraining our models .",Model,Model,natural_language_inference,11,23,1,1,results,0.017050469,0,negative,0.001525258,3.13E-05,3.42E-05,7.61E-06,1.05E-05,0.000144203,0.000705669,0.001463292,0.000417048,0.959572594,1.35E-05,0.035984658,9.01E-05
5890,natural_language_inference11,160,"Finally , we remark that despite careful tuning , our re-implementation of ESIM fails to match the 88 % reported in Chen et al. ( 2017 ) by 0.8 % ; however , with MultiNLI , we find that our implementation of ESIM performs considerably better ( by approximately 5 % ) .",Model,Model,natural_language_inference,11,24,1,0,,0.410080152,0,results,0.029014665,6.20E-05,0.000172971,1.29E-05,4.13E-05,0.000113948,0.00495364,0.000925591,0.000117343,0.138107681,7.45E-06,0.826159345,0.000311156
5891,natural_language_inference11,161,"The instability of the results suggests , as well as the failure of a custom RTEarchitecture to consistently perform well suggests that current SotA RTE models maybe overfit to the SNLI dataset .",Model,Model,natural_language_inference,11,25,1,0,,0.006266368,0,negative,0.004402265,4.44E-05,0.000107339,8.13E-06,2.69E-05,9.37E-05,0.000779153,0.000530813,0.0002488,0.86314235,1.25E-05,0.130520784,8.29E-05
5892,natural_language_inference11,162,Qualitative Analysis,Model,,natural_language_inference,11,26,1,0,,0.004804752,0,negative,0.000381358,0.000243367,0.000267346,5.65E-05,4.84E-05,0.000879831,0.002289462,0.005993893,0.007019753,0.974330932,0.00016568,0.007616462,0.000707043
5893,natural_language_inference11,163,"Although our empirical results show our knowledge - incorporation approach improves performance , in this section we attempt to assess whether we are learning to use the provided knowledge in a semantically appropriate way .",Model,Qualitative Analysis,natural_language_inference,11,27,1,0,,1.07E-05,0,negative,0.002455104,6.86E-05,8.20E-06,5.82E-08,9.89E-06,4.88E-06,4.88E-05,1.05E-05,1.56E-05,0.996180442,1.73E-06,0.001195935,2.20E-07
5894,natural_language_inference11,164,RTE,Model,,natural_language_inference,11,28,1,0,,0.002187544,0,negative,9.74E-05,5.05E-05,3.96E-05,9.67E-06,9.70E-06,0.000145326,0.000169805,0.001429839,0.002714125,0.994528334,8.62E-06,0.000742165,5.49E-05
5895,natural_language_inference11,165,"To test our models sensitivity towards the semantics of the assertions for recognizing textual entailment , we run an experiment in which we swap the synonym with the antonym predicate in the provided assertions during test time .",Model,RTE,natural_language_inference,11,29,1,0,,9.32E-05,0,negative,4.19E-05,1.65E-05,1.53E-05,4.80E-08,1.45E-06,2.34E-06,1.51E-05,4.91E-05,1.71E-05,0.998519878,8.06E-08,0.001320823,3.80E-07
5896,natural_language_inference11,166,We hypothesize that in many cases these two predicates are very important for predicting either contradic - tion or entailment .,Model,RTE,natural_language_inference,11,30,1,0,,2.24E-06,0,negative,3.89E-06,1.63E-06,1.40E-06,2.56E-08,8.52E-08,1.08E-06,6.73E-07,2.41E-05,3.32E-05,0.999898777,5.81E-08,3.50E-05,1.24E-07
5897,natural_language_inference11,167,"Indeed , there is a strong performance drop of about 10 % on MultiNLI examples for both the BiLSTM and the ESIM model for which either a synonym or an antonym-assertion is present .",Model,RTE,natural_language_inference,11,31,1,0,,0.042979733,0,negative,0.035453476,8.32E-06,3.27E-05,1.78E-06,3.12E-06,1.90E-05,0.000558389,0.000266752,1.89E-05,0.699437691,1.06E-06,0.264168248,3.06E-05
5898,natural_language_inference11,168,This very large drop clearly shows that our models are sensitive to the semantics of the provided knowledge .,Model,RTE,natural_language_inference,11,32,1,0,,5.94E-05,0,negative,0.001808488,1.48E-06,1.29E-05,1.55E-07,1.08E-06,2.71E-06,2.59E-05,2.01E-05,8.15E-06,0.98472522,7.06E-08,0.013392654,1.07E-06
5899,natural_language_inference11,169,Examples of prediction changes are presented in .,Model,RTE,natural_language_inference,11,33,1,0,,7.57E-06,0,negative,1.31E-05,5.30E-07,3.25E-06,1.31E-07,1.88E-07,2.02E-06,3.57E-06,1.86E-05,1.26E-05,0.999716604,1.09E-07,0.000228746,5.49E-07
5900,natural_language_inference11,170,"They demonstrate that the system has learned to trust the presented assertions to the point that it will make appropriate counterfactual inferences - that is , the change in knowledge has caused the change in prediction .",Model,RTE,natural_language_inference,11,34,1,0,,1.66E-06,0,negative,2.72E-05,1.93E-06,6.62E-06,3.11E-08,1.48E-07,7.66E-07,1.25E-06,1.22E-05,4.41E-05,0.999766049,4.38E-08,0.000139525,2.13E-07
5901,natural_language_inference11,171,For the interested reader we provide additional RTE analysis results in Appendix C DQA,Model,RTE,natural_language_inference,11,35,1,0,,5.42E-06,0,negative,7.62E-06,6.66E-07,1.77E-06,1.30E-07,7.36E-08,3.05E-06,7.12E-06,4.59E-05,1.61E-05,0.999632649,1.46E-07,0.000283904,8.83E-07
5902,natural_language_inference11,172,The following is an example question from the TriviaQA dataset :,Model,RTE,natural_language_inference,11,36,1,0,,1.47E-06,0,negative,1.12E-06,1.88E-07,1.68E-06,1.79E-08,8.41E-08,9.81E-07,4.11E-06,1.04E-05,1.70E-06,0.999783122,1.36E-07,0.000196088,3.89E-07
5903,natural_language_inference11,173,"Answer candidates ( i.e. , Denmark , Corfu , Greece , Vanuata ) were obtained from the top predicted answer spans computed by our model excluding Wikipedia ( i.e. , BiLSTM + p + q + A ) .",Model,RTE,natural_language_inference,11,37,1,0,,6.49E-06,0,negative,2.72E-06,9.89E-07,7.12E-06,4.12E-08,9.43E-07,9.40E-06,1.21E-05,9.74E-05,6.94E-06,0.999750525,9.95E-09,0.000111309,5.05E-07
5904,natural_language_inference11,174,"Their corresponding abstracts were retrieved from Wikipedia and then given to our model in a second pass ( i.e. , BiLSTM + p + q + A + W ) .",Model,RTE,natural_language_inference,11,38,1,0,,2.06E-05,0,negative,1.73E-05,1.98E-06,1.49E-05,1.58E-07,6.82E-06,5.39E-06,9.76E-06,4.08E-05,9.15E-06,0.999650059,8.44E-09,0.000243174,6.27E-07
5905,natural_language_inference11,175,"In this example , the final best prediction of the model changes from Denmark to Corfu after integrating the abstracts ( here , the abstract clearly states that Corfu is an island ) .",Model,RTE,natural_language_inference,11,39,1,0,,2.00E-06,0,negative,8.54E-06,5.71E-07,1.06E-05,1.08E-08,1.20E-07,8.18E-07,4.42E-06,1.17E-05,1.16E-05,0.999658542,2.04E-08,0.000292595,3.63E-07
5906,natural_language_inference11,176,"We studied a total of 25 similar answer changes , 14 of which went from incorrect to correct , and 11 of which went from correct to incorrect .",Model,RTE,natural_language_inference,11,40,1,0,,9.73E-05,0,negative,4.57E-05,9.11E-06,1.51E-05,3.47E-06,4.79E-05,1.90E-05,4.46E-05,0.000105211,1.68E-05,0.998829031,9.98E-08,0.000858725,5.32E-06
5907,natural_language_inference11,177,"In 11 of the 14 corrections , obvious information is present in the Wikipedia abstracts that reinforced the correct answer .",Model,RTE,natural_language_inference,11,41,1,0,,5.66E-05,0,negative,0.000769571,1.80E-06,2.46E-05,9.67E-06,2.76E-05,1.93E-05,3.88E-05,2.84E-05,1.11E-05,0.996287763,2.72E-08,0.002776798,4.50E-06
5908,natural_language_inference11,178,"Where the system was confused by the answers ( i.e. , when the abstracts switched the production from correct to incorrect ) , no obvious information was present in 8 of the 11 cases , suggesting that the model had difficulty coping with unrelated background information .",Model,RTE,natural_language_inference,11,42,1,0,,6.30E-05,0,negative,0.000104034,1.38E-06,5.43E-06,3.39E-07,2.26E-06,8.13E-06,1.97E-05,4.59E-05,6.95E-06,0.997259027,3.24E-08,0.002545612,1.16E-06
5909,natural_language_inference11,179,"In 3 of the 11 , plausibly relevant information was present in the abstract of the correct answer , yet the model still made the incorrect answer change .",Model,RTE,natural_language_inference,11,43,1,0,,2.80E-05,0,negative,0.000688989,2.15E-06,1.49E-05,3.42E-06,1.22E-05,1.58E-05,3.34E-05,4.70E-05,1.47E-05,0.996889157,2.17E-08,0.002275632,2.75E-06
5910,natural_language_inference11,180,The existence of counterfactual inferences in RTE and the tendency to use reinforcing informa - p:,Model,RTE,natural_language_inference,11,44,1,0,,7.12E-06,0,negative,7.57E-06,7.41E-06,1.14E-05,5.94E-08,1.19E-07,2.21E-06,1.55E-05,5.40E-05,5.43E-05,0.999249948,9.53E-06,0.000584419,3.53E-06
5911,natural_language_inference11,181,His off - the - cuff style seems amateurish the net cost of operations .,Model,RTE,natural_language_inference,11,45,1,0,,1.23E-06,0,negative,4.87E-06,3.91E-07,2.13E-06,6.98E-08,1.06E-07,2.36E-06,2.46E-06,2.21E-05,1.06E-05,0.999873752,2.74E-08,8.07E-05,4.46E-07
5912,natural_language_inference11,182,but uh these guys tion about candidate answers in DQA suggest that our knowledge incorporating strategy is exploiting heterogeneous knowledge sources in semantically sensible ways .,Model,RTE,natural_language_inference,11,46,1,0,,2.10E-05,0,negative,5.72E-06,1.07E-06,3.78E-06,8.12E-08,1.02E-07,1.88E-06,1.12E-05,2.29E-05,6.86E-06,0.999059279,4.13E-06,0.000880154,2.83E-06
5913,natural_language_inference11,183,Related Work,,,natural_language_inference,11,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
5914,natural_language_inference11,192,Conclusion,,,natural_language_inference,11,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
5915,natural_language_inference11,198,A Implementation Details,,,natural_language_inference,11,0,1,0,,0.001181912,0,negative,8.83E-05,0.001021637,1.64E-05,0.000210437,0.000103111,0.000708779,0.000454491,0.003601627,0.000158694,0.989483574,0.0038694,0.000248988,3.46E-05
5916,natural_language_inference11,199,All our models were trained with 3 different random seeds and the top performance is reported 10 .,A Implementation Details,A Implementation Details,natural_language_inference,11,1,1,0,,0.849515052,1,experimental-setup,1.31E-05,7.89E-06,5.05E-06,3.42E-06,1.89E-06,0.839773221,0.004684162,0.109779932,5.11E-06,0.045672037,6.14E-06,2.45E-05,2.36E-05
5917,natural_language_inference11,200,An overview of hyper - parameters used in our experiments can be found in .,A Implementation Details,A Implementation Details,natural_language_inference,11,2,1,0,,0.000449144,0,negative,3.80E-05,1.63E-05,7.76E-06,3.35E-05,1.46E-05,0.228703429,0.000671833,0.015562273,2.00E-05,0.754878639,1.40E-05,2.93E-05,1.04E-05
5918,natural_language_inference11,201,"In the following we explain the detailed implementation of our two task - specific , baseline models .",A Implementation Details,A Implementation Details,natural_language_inference,11,3,1,0,,0.00032634,0,negative,5.02E-05,0.00013052,5.61E-05,0.000106269,9.73E-05,0.18381934,0.001157918,0.012622087,0.000155678,0.801704694,3.62E-05,3.46E-05,2.91E-05
5919,natural_language_inference11,202,"We assume to have computed the contextually ( un - ) refined word representations depending on the setup and embedded our input sequences q = ( q 1 , ... , q L Q ) and p = ( p 1 , ... , p LP ) to Q ? R nL Q and P ?",A Implementation Details,A Implementation Details,natural_language_inference,11,4,1,0,,0.001840371,0,negative,0.000176206,0.000100399,9.77E-05,5.50E-06,1.92E-05,0.1379303,0.002290936,0.026325177,0.0001386,0.832658748,3.60E-05,0.00020507,1.61E-05
5920,natural_language_inference11,203,"R nL P , respectively .",A Implementation Details,A Implementation Details,natural_language_inference,11,5,1,0,,0.442910752,0,negative,0.000428207,4.03E-05,9.87E-05,3.14E-06,7.96E-06,0.055096658,0.002331887,0.015451554,8.34E-05,0.92591393,7.89E-05,0.000446751,1.87E-05
5921,natural_language_inference11,204,The word representation update gate in Eq. 4 is initialized with a bias of 1 to refine representations only slightly in the beginning of training .,A Implementation Details,A Implementation Details,natural_language_inference,11,6,1,0,,0.869303969,1,experimental-setup,1.91E-05,5.70E-05,8.97E-06,2.16E-06,1.38E-06,0.608866197,0.001716492,0.376660362,9.16E-05,0.012539741,6.00E-06,6.31E-06,2.47E-05
5922,natural_language_inference11,205,"In the following as before , we denote the hidden dimensionality of our model by n and a fully - connected layer by",A Implementation Details,A Implementation Details,natural_language_inference,11,7,1,0,,0.01115918,0,experimental-setup,6.51E-05,0.000153751,2.07E-05,1.46E-05,1.29E-05,0.612872386,0.002939206,0.217163937,0.000265149,0.16633154,4.74E-05,4.42E-05,6.92E-05
5923,natural_language_inference11,206,A.1 Question Answering Encoding,A Implementation Details,,natural_language_inference,11,8,1,0,,0.274981569,0,negative,0.00057486,0.000305731,0.001386387,2.20E-05,2.15E-05,0.079483705,0.065930536,0.019946503,0.000557203,0.794583318,0.02696421,0.009072923,0.001151054
5924,natural_language_inference11,207,In the DQA task q refers to the question and p to the supporting text .,A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,9,1,0,,4.41E-06,0,negative,5.57E-06,5.69E-06,4.25E-06,2.04E-06,2.04E-06,0.000132314,8.02E-05,2.76E-05,3.41E-05,0.998912874,0.000751307,2.96E-05,1.24E-05
5925,natural_language_inference11,208,"For our baseline ( i.e. , BiLSTM + liq ) we additionally concatenate a binary feature top and q indicating whether the corresponding token lemma appeared in the question .",A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,10,1,0,,0.0005684,0,negative,0.000490112,0.000394224,0.001967521,2.03E-06,1.41E-05,0.002013251,0.001142083,0.000619912,0.010297859,0.982875965,2.53E-05,0.000136512,2.12E-05
5926,natural_language_inference11,209,"However , it is omitted in the following for the sake of brevity .",A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,11,1,0,,3.54E-06,0,negative,8.71E-06,1.89E-06,1.66E-06,1.25E-06,1.61E-06,0.000103885,1.69E-05,1.55E-05,1.12E-05,0.999824068,2.39E-06,1.06E-05,4.81E-07
5927,natural_language_inference11,210,At first we process both sequences by identical BiLSTMs in parallel ( Eq. 6 ) followed by a linear projection and a tanh nonlinearity ( Eq.,A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,12,1,0,,0.000700181,0,negative,0.001915222,0.000235139,0.00652899,1.42E-06,1.70E-05,0.000603033,0.001776606,9.11E-05,0.008065948,0.980043925,3.75E-05,0.000657261,2.69E-05
5928,natural_language_inference11,211,7 ) .,A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,13,1,0,,4.57E-05,0,negative,0.000302688,1.67E-06,3.62E-05,3.45E-07,2.55E-06,9.45E-05,0.000116587,1.02E-05,1.19E-05,0.999107925,1.70E-06,0.000312485,1.25E-06
5929,natural_language_inference11,212,U ?,A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,14,1,0,,1.73E-05,0,negative,1.47E-05,4.82E-07,2.80E-06,2.63E-07,5.06E-07,0.000175275,0.000114294,2.20E-05,5.53E-06,0.999617394,3.57E-06,4.19E-05,1.28E-06
5930,natural_language_inference11,213,R n2n is initialized by [ I ; I ] where I ?,A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,15,1,0,,1.27E-05,0,negative,4.04E-05,3.48E-06,5.36E-05,1.51E-07,1.12E-06,0.000193596,0.000280222,3.38E-05,3.53E-05,0.999222065,6.21E-06,0.000127972,2.17E-06
5931,natural_language_inference11,214,R nn is the identity matrix .,A Implementation Details,A.1 Question Answering Encoding,natural_language_inference,11,16,1,0,,5.36E-05,0,negative,1.32E-05,5.93E-06,6.13E-06,4.15E-07,9.24E-07,0.000479446,0.000191419,0.000192304,9.21E-05,0.998978662,8.76E-06,2.76E-05,3.02E-06
5932,natural_language_inference11,215,Prediction,A Implementation Details,,natural_language_inference,11,17,1,0,,0.300866904,0,negative,0.000167127,3.10E-05,0.000248957,0.000103143,0.000214858,0.297525941,0.312521151,0.025041804,3.11E-05,0.350257854,0.000142857,0.010910933,0.002803208
5933,natural_language_inference11,216,Our prediction - or answer layer is similar to .,A Implementation Details,Prediction,natural_language_inference,11,18,1,0,,2.61E-05,0,negative,0.000113329,1.01E-05,0.000938858,3.45E-06,4.00E-06,0.004271464,0.086972365,0.000205071,0.001683043,0.904352926,6.10E-06,3.18E-05,0.001407563
5934,natural_language_inference11,217,"We first compute a weighted , n-dimensional representationq of the processed questionQ ( Eq. 8 ) .",A Implementation Details,Prediction,natural_language_inference,11,19,1,0,,8.15E-05,0,negative,7.80E-05,1.74E-05,0.000821508,7.89E-08,9.29E-07,0.000324566,0.014050411,2.84E-05,0.000947433,0.983664862,9.61E-07,2.12E-05,4.43E-05
5935,natural_language_inference11,218,"Result variations were small , that is within less than a percentage point in all experiments .",A Implementation Details,Prediction,natural_language_inference,11,20,1,0,,0.002178084,0,negative,0.000259964,2.86E-07,3.22E-06,4.80E-07,1.39E-06,0.000575301,0.364140116,3.10E-05,5.42E-07,0.632162408,5.20E-07,0.002542982,0.000281793
5936,natural_language_inference11,219,"The probability distributions p s / p e for the start / end location of the answer is computed by a 2 - layer MLP with a ReLU activated , hidden layer s j as follows :",A Implementation Details,Prediction,natural_language_inference,11,21,1,0,,4.31E-06,0,negative,1.47E-05,6.16E-07,3.48E-05,1.71E-08,1.61E-07,0.000146299,0.004216558,1.07E-05,2.53E-05,0.995539767,9.06E-08,7.07E-06,4.03E-06
5937,natural_language_inference11,220,"The model is trained to maximize the loglikelihood of the correct answer spans by computing the sum of the correct span probabilities p s ( i ) p e ( k ) for span ( i , k) under our model ( Eq. 9 ) .",A Implementation Details,Prediction,natural_language_inference,11,22,1,0,,1.27E-05,0,negative,1.01E-05,5.29E-06,6.96E-05,5.09E-08,2.90E-07,0.000541955,0.008588741,8.24E-05,0.000450207,0.990222022,4.98E-07,2.96E-06,2.59E-05
5938,natural_language_inference11,221,"During evaluation we extract the span ( i , k) with the best score and maximum token length k ?",A Implementation Details,Prediction,natural_language_inference,11,23,1,0,,2.78E-05,0,negative,2.79E-05,2.12E-07,8.37E-06,1.51E-08,1.97E-07,0.000110126,0.013132879,7.01E-06,8.35E-07,0.986655216,5.40E-08,5.12E-05,5.93E-06
5939,natural_language_inference11,222,i ? 16 for SQuAD and k ?,A Implementation Details,Prediction,natural_language_inference,11,24,1,0,,0.000216794,0,experiments,4.61E-05,8.44E-07,6.22E-06,1.71E-07,4.25E-07,0.007313049,0.659212873,0.000567512,4.20E-06,0.332474913,3.76E-07,6.28E-05,0.000310571
5940,natural_language_inference11,223,i ? 8 for TriviaQA .,A Implementation Details,Prediction,natural_language_inference,11,25,1,0,,8.48E-05,0,negative,6.51E-05,1.61E-07,8.76E-06,4.48E-08,3.80E-07,0.000448149,0.04593747,2.47E-05,1.05E-06,0.953399586,6.20E-08,9.39E-05,2.07E-05
5941,natural_language_inference11,224,TriviaQA,A Implementation Details,,natural_language_inference,11,26,1,0,,0.056402505,0,negative,0.001216277,2.84E-05,0.000861388,0.000171823,0.000734448,0.182497571,0.087765894,0.006711342,3.73E-05,0.702792134,3.46E-05,0.015243732,0.001905102
5942,natural_language_inference11,225,Properly training a QA system on TriviaQA is much more challenging than SQuAD because of the large document sizes and the use of multiple paragraphs .,A Implementation Details,TriviaQA,natural_language_inference,11,27,1,0,,0.029467296,0,negative,0.000264532,1.60E-06,2.02E-05,1.10E-05,1.83E-05,0.000981806,0.247330511,1.52E-05,1.22E-06,0.709626176,0.000285407,0.000975846,0.040468264
5943,natural_language_inference11,226,"Therefore , we adopt the approach of Clark and Gardner ( 2017 ) who were the first to properly train neural QA models on Trivi - a QA .",A Implementation Details,TriviaQA,natural_language_inference,11,28,1,0,,0.000355168,0,negative,0.001494264,0.000374033,0.006926664,2.56E-06,0.000101757,0.001666887,0.188120881,5.92E-05,0.000656846,0.798029996,7.35E-06,0.000359187,0.00220037
5944,natural_language_inference11,227,"It relies on splitting documents and merging paragraphs up to a certain maximum token length ( 600 per paragraph in our experiments ) , and only retaining the top -k paragraphs ( 6 in our case ) for prediction .",A Implementation Details,TriviaQA,natural_language_inference,11,29,1,0,,0.020891271,0,negative,0.006471763,0.000178325,0.02148452,4.88E-06,0.000142403,0.002082262,0.287617662,4.46E-05,0.000518073,0.675744626,4.23E-06,0.000667775,0.005038847
5945,natural_language_inference11,228,Paragraphs are ranked using the tfidf cosine similarity between question and paragraph .,A Implementation Details,TriviaQA,natural_language_inference,11,30,1,0,,0.008400129,0,negative,0.000402367,1.50E-05,0.000967353,8.11E-07,5.92E-05,0.001655025,0.114776578,2.79E-05,0.000121639,0.87962528,5.81E-07,0.000172968,0.002175272
5946,natural_language_inference11,229,To speedup training only 2 paragraphs out of the top 4 / 6 for the W eb / W ikipedia datasets were sampled .,A Implementation Details,TriviaQA,natural_language_inference,11,31,1,0,,0.001240976,0,negative,0.000308748,4.73E-06,5.49E-05,7.88E-06,0.00044871,0.009023886,0.100484911,8.74E-05,6.56E-06,0.887201955,5.73E-08,2.38E-05,0.002346375
5947,natural_language_inference11,230,The only architectural difference for this multi-paragraph setup is that we encode multiple p for each question q and the softmax of Eq.,A Implementation Details,TriviaQA,natural_language_inference,11,32,1,0,,0.000102165,0,negative,0.006292575,0.000122375,0.002197143,1.66E-06,4.62E-05,0.00081058,0.100493233,2.31E-05,0.000615718,0.887687824,2.15E-06,0.000509698,0.001197693
5948,natural_language_inference11,231,9 is taken over all tokens of all paragraphs instead of only a single paragraph .,A Implementation Details,TriviaQA,natural_language_inference,11,33,1,0,,7.82E-05,0,negative,0.000121689,1.29E-06,4.45E-05,8.26E-08,2.58E-06,0.000507294,0.013771291,1.51E-05,1.76E-05,0.985424858,4.85E-08,1.36E-05,8.01E-05
5949,natural_language_inference11,232,"For further details , we refer the interested reader to Clark and Gardner ( 2017 ) who explain this process in more detail .",A Implementation Details,TriviaQA,natural_language_inference,11,34,1,0,,1.35E-05,0,negative,3.46E-05,4.74E-07,6.46E-06,3.82E-06,1.02E-05,0.000549803,0.001929599,6.46E-06,4.67E-06,0.997261161,1.49E-07,5.51E-06,0.000187026
5950,natural_language_inference11,233,For optimization we employed ADAM with a learning rate of 10 ?3 which was halved when performance dropped between checkpoint ( ckpt ) intervals .,A Implementation Details,TriviaQA,natural_language_inference,11,35,1,0,,0.612910644,1,experiments,0.000161688,1.10E-05,1.39E-05,1.06E-05,1.65E-05,0.090664174,0.823150268,0.003150042,3.55E-05,0.066863955,2.32E-07,1.62E-05,0.015905962
5951,natural_language_inference11,234,We use 300 - dimensional wordembeddings from as pre-trained word embeddings in all experiments .,A Implementation Details,TriviaQA,natural_language_inference,11,36,1,0,,0.448918906,0,experiments,8.71E-05,1.04E-05,1.05E-05,1.39E-06,4.20E-06,0.037105554,0.88663857,0.002288355,4.83E-05,0.064216307,2.06E-07,1.29E-05,0.009576264
5952,natural_language_inference11,235,For regularization we make use of dropout on the computed non-contextual word representations e w defined in 3.1 with the same dropout mask for all words in a batch .,A Implementation Details,TriviaQA,natural_language_inference,11,37,1,0,,0.028641391,0,experiments,0.001361763,6.59E-05,8.54E-05,1.60E-05,7.69E-05,0.036712715,0.675396703,0.002359153,0.00022902,0.271269397,2.07E-07,3.95E-05,0.012387339
5953,natural_language_inference11,236,For QA we additionally applied dropout on the projections computed in Eq.,A Implementation Details,TriviaQA,natural_language_inference,11,38,1,0,,0.014262466,0,experiments,0.011378104,8.95E-05,0.002563502,1.35E-06,5.34E-05,0.002791912,0.528604587,6.77E-05,0.000337243,0.450758092,4.27E-07,0.00048159,0.002872586
5954,natural_language_inference11,237,7 .,A Implementation Details,TriviaQA,natural_language_inference,11,39,1,0,,3.63E-05,0,negative,6.12E-05,1.54E-07,2.87E-06,6.60E-08,5.54E-07,0.000191883,0.004533855,4.98E-06,3.02E-06,0.99513871,2.28E-08,8.34E-06,5.43E-05
5955,natural_language_inference11,238,A.2 Recognizing Textual Entailment,A Implementation Details,,natural_language_inference,11,40,1,0,,0.028301827,0,negative,0.00055628,4.55E-05,0.000279112,1.27E-05,4.78E-05,0.042996613,0.137601936,0.005575814,8.05E-05,0.791004795,0.000196184,0.01984442,0.001758233
5956,natural_language_inference11,239,"Encoding Analogous to DQA we encode our input sequences by BiLSTMs , however , for RTE we use conditional encoding instead .",A Implementation Details,A.2 Recognizing Textual Entailment,natural_language_inference,11,41,1,0,,0.000156041,0,negative,0.000445392,4.36E-05,0.005363471,5.74E-07,7.76E-06,0.000140758,0.01176636,1.01E-05,0.000837482,0.979833535,7.28E-06,0.00055305,0.000990669
5957,natural_language_inference11,240,"Therefore , we initially process the embedded hypothesis Q by a BiLSTM and use the respective end states of the forward and backward LSTM as initial states for the forward and backward LSTM that processes the embedded premise P.",A Implementation Details,A.2 Recognizing Textual Entailment,natural_language_inference,11,42,1,0,,1.58E-05,0,negative,4.99E-05,9.23E-06,5.52E-05,4.91E-08,1.04E-06,4.19E-05,0.000879679,1.22E-05,0.000519948,0.998364022,2.33E-07,1.43E-05,5.22E-05
5958,natural_language_inference11,241,Prediction,A Implementation Details,,natural_language_inference,11,43,1,0,,0.208223136,0,experiments,0.000111835,2.28E-05,0.000194063,6.02E-05,0.000485313,0.199403877,0.437852401,0.01668963,1.90E-05,0.320652492,8.86E-06,0.017376459,0.007123169
5959,natural_language_inference11,242,"We concatenate the outputs of the forward and backward LSTMs processing the premise p , i.e. , pf wt ;p bw t ?",A Implementation Details,Prediction,natural_language_inference,11,44,1,0,,1.04E-05,0,negative,1.52E-05,5.01E-07,2.56E-05,7.13E-09,2.20E-07,0.000137837,0.014666297,8.90E-06,1.18E-05,0.985108908,5.86E-09,9.50E-06,1.51E-05
5960,natural_language_inference11,243,R 2 n and run each of the resulting LP outputs through a fully - connected layer with ReLU activation ( h t ) followed by a max - pooling operation overtime resulting in a hidden state h ?,A Implementation Details,Prediction,natural_language_inference,11,45,1,0,,2.12E-05,0,negative,4.86E-05,4.32E-07,1.28E-05,6.34E-08,1.05E-06,0.000481128,0.133662416,3.18E-05,2.28E-06,0.86551814,3.17E-08,8.82E-05,0.000153109
5961,natural_language_inference11,244,Rn .,A Implementation Details,,natural_language_inference,11,46,1,0,,0.001047946,0,negative,4.00E-05,5.51E-06,2.07E-05,2.28E-06,1.30E-05,0.031208277,0.00364792,0.003452603,2.17E-05,0.961236794,1.46E-06,0.000292539,5.73E-05
5962,natural_language_inference11,245,"Finally , h is used to predict the RTE label as follows :",A Implementation Details,Rn .,natural_language_inference,11,47,1,0,,0.003603303,0,negative,0.000292009,4.23E-06,0.001517013,6.26E-08,2.09E-06,5.36E-05,0.000783274,1.68E-05,0.000164671,0.996899316,7.05E-08,4.71E-05,0.000219808
5963,natural_language_inference11,246,"The probability of choosing category c ? { entailment , contradiction , neutral } is defined in Eq. 10 .",A Implementation Details,Rn .,natural_language_inference,11,48,1,0,,0.001397507,0,negative,5.00E-05,1.14E-06,3.27E-05,1.12E-07,1.41E-06,0.000133055,0.000684162,7.70E-05,2.22E-05,0.99868723,4.02E-08,1.54E-05,0.000295506
5964,natural_language_inference11,247,"Finally , the model is trained to maximize the log-likelihood of the correct category label given probability distribution p.",A Implementation Details,Rn .,natural_language_inference,11,49,1,0,,0.001675697,0,negative,0.000248346,3.33E-05,0.001402794,8.81E-07,1.02E-05,0.000413769,0.001866314,0.000308344,0.002571167,0.990260569,2.53E-07,1.80E-05,0.002866055
5965,natural_language_inference11,248,B Reducing Training Data & Dimensionality of Pre-trained Word Embeddings,A Implementation Details,Rn .,natural_language_inference,11,50,1,0,,0.197059688,0,negative,0.013513485,6.04E-05,0.002048864,4.53E-06,3.15E-05,0.000808793,0.072667559,0.000191486,0.000263689,0.851656292,2.63E-06,0.004951212,0.053799562
5966,natural_language_inference11,249,We find that there is only little impact when using external knowledge on the RTE task when using a more sophisticated task model such as ESIM .,A Implementation Details,Rn .,natural_language_inference,11,51,1,0,,0.604702205,1,experiments,0.091360182,8.43E-06,0.000179889,1.23E-05,6.89E-05,0.001419896,0.353448413,0.000352786,1.47E-05,0.342946103,6.80E-07,0.032446693,0.177741083
5967,natural_language_inference11,250,"We hypothesize that the attention mechanisms within ESIM together with powerful , pre-trained word representations allow for the recovery of some important lexical relations when trained on a large dataset .",A Implementation Details,Rn .,natural_language_inference,11,52,1,0,,0.011286938,0,negative,0.000290936,1.20E-05,0.000360565,4.23E-06,2.20E-05,0.000544338,0.002435005,0.000174943,0.000269295,0.985244995,4.30E-07,9.33E-05,0.010547859
5968,natural_language_inference11,251,It follows that by reducing the number of training data and impoverishing pre-trained word representations the impact of using external knowledge should become larger .,A Implementation Details,Rn .,natural_language_inference,11,53,1,0,,0.010985093,0,negative,0.030181848,2.82E-06,9.96E-05,1.37E-06,1.73E-05,0.000162263,0.004786781,3.52E-05,1.46E-05,0.961078974,7.47E-08,0.00096387,0.002655397
5969,natural_language_inference11,252,"To test this hypothesis , we gradually impoverish pre-trained word embeddings by reducing their dimensionality with PCA while reducing the number of training instances at the same time .",A Implementation Details,Rn .,natural_language_inference,11,54,1,0,,0.385648526,0,negative,0.014311089,0.000295482,0.001994494,4.26E-06,0.000183913,0.001133852,0.032574338,0.00046178,0.000620572,0.936128414,1.10E-07,0.000566495,0.011725203
5970,natural_language_inference11,253,11 Our joint data and dimensionality reduction results are presented in .,A Implementation Details,Rn .,natural_language_inference,11,55,1,0,,0.023331695,0,negative,0.000219579,5.94E-07,0.000195884,2.47E-06,2.19E-05,0.000293121,0.001942048,1.68E-05,4.48E-06,0.99298898,6.36E-08,0.000200715,0.004113387
5971,natural_language_inference11,254,They show that there is indeed a slightly larger benefit when employing background knowledge from ConcepNet ( A ) in the more impoverished settings with largest improvements when using around 10 k examples and reduced dimensionality to 10 .,A Implementation Details,Rn .,natural_language_inference,11,56,1,0,,0.04118758,0,negative,0.062448762,8.30E-06,0.00054385,1.25E-06,2.51E-05,0.000252433,0.134499275,5.09E-05,1.12E-05,0.740718314,2.18E-07,0.039727588,0.021712763
5972,natural_language_inference11,255,"However , we observe that the biggest over all impact over the baseline ESIM model stems from our contextual refinement strategy ( i.e. , reading only the premise p and hypothesis q) which is especially pronounced for the 1 k and 3 k experiments .",A Implementation Details,Rn .,natural_language_inference,11,57,1,0,,0.58826264,1,ablation-analysis,0.626686738,8.98E-06,0.000261004,6.90E-06,7.03E-05,0.000359878,0.186144871,7.64E-05,7.94E-06,0.11296415,1.05E-07,0.025604005,0.047808715
5973,natural_language_inference11,256,This highlights once more the usefulness of our refinement strategy even without the use of additional knowledge .,A Implementation Details,Rn .,natural_language_inference,11,58,1,0,,0.006568069,0,negative,0.001938875,5.85E-07,3.43E-05,3.68E-07,7.06E-06,6.73E-05,0.00193319,6.89E-06,3.29E-06,0.994346536,2.39E-08,0.000494216,0.001167303
5974,natural_language_inference11,257,C Further Analysis of Knowledge Utilization in RTE,A Implementation Details,,natural_language_inference,11,59,1,0,,0.033871117,0,negative,0.000137074,1.18E-05,4.01E-05,7.75E-07,1.02E-05,0.015195267,0.055690675,0.004233058,4.44E-05,0.91772924,3.43E-06,0.00626074,0.000643198
5975,natural_language_inference11,258,Is additional knowledge used ?,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,60,1,0,,7.98E-06,0,negative,7.33E-06,1.94E-07,9.29E-07,3.33E-08,2.98E-07,9.05E-05,0.001318494,1.35E-05,1.10E-06,0.998543526,1.03E-08,1.03E-05,1.38E-05
5976,natural_language_inference11,259,"To verify whether and how our models make use of additional knowledge , we conducted several experiments .",A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,61,1,0,,3.94E-05,0,negative,8.98E-06,7.22E-08,3.67E-07,6.16E-09,1.62E-06,2.60E-05,0.001247425,1.66E-06,5.44E-08,0.998688964,3.75E-10,2.30E-05,1.84E-06
5977,natural_language_inference11,260,"First , we evaluated models trained with knowledge on our tasks while not providing any knowledge at test time .",A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,62,1,0,,0.000186983,0,negative,0.000138534,1.80E-06,1.48E-05,6.70E-08,2.59E-05,9.53E-05,0.007533682,5.07E-06,5.23E-07,0.99199172,1.60E-09,0.000181399,1.12E-05
5978,natural_language_inference11,261,"This ablation drops performance by 3.7 - 3.9 % accuracy on MultiNLI , and by 4 % F1 on SQuAD .",A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,63,1,0,,0.013601068,0,negative,0.049238282,7.13E-06,0.000243424,1.31E-06,1.00E-04,0.000219807,0.040284031,1.03E-05,6.72E-06,0.907053359,1.75E-08,0.002386159,0.000449472
5979,natural_language_inference11,262,This indicates the model is refining the representations using the provided assertions in a useful way .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,64,1,0,,6.31E-05,0,negative,1.58E-05,6.33E-08,8.20E-07,1.11E-08,5.33E-07,1.74E-05,0.000287355,1.68E-06,3.94E-07,0.999661842,9.81E-10,1.08E-05,3.32E-06
5980,natural_language_inference11,263,What knowledge is used ?,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,65,1,0,,6.02E-06,0,negative,3.40E-06,1.00E-07,5.38E-07,2.49E-08,3.44E-07,6.80E-05,0.000670549,9.13E-06,4.07E-07,0.999233467,3.06E-09,4.84E-06,9.19E-06
5981,natural_language_inference11,264,After establishing that our models are somehow sensitive to seman - :,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,66,1,0,,1.21E-05,0,negative,2.79E-05,4.07E-08,9.16E-07,2.15E-09,1.98E-07,7.28E-06,0.000545784,6.23E-07,1.59E-07,0.999381518,6.53E-10,3.38E-05,1.79E-06
5982,natural_language_inference11,265,Development set results for MultiNLI ( Matched + Mismatched ) when reducing training data and embedding dimensionality with PCA .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,67,1,0,,0.000897994,0,negative,9.73E-05,1.77E-07,4.27E-05,2.93E-09,7.95E-07,2.79E-05,0.077223935,1.71E-06,1.67E-07,0.920317891,2.46E-09,0.002232298,5.52E-05
5983,natural_language_inference11,266,In parenthesis we report the relative differences to the respective result directly above .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,68,1,0,,2.32E-05,0,negative,4.77E-06,4.30E-08,5.64E-07,1.13E-08,1.55E-06,2.86E-05,0.000888271,2.22E-06,8.51E-08,0.999051858,2.07E-10,1.87E-05,3.26E-06
5984,natural_language_inference11,267,tics we wanted to find out which type of knowledge is important for which task .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,69,1,0,,1.15E-05,0,negative,3.19E-06,2.74E-08,8.50E-07,5.10E-09,2.09E-06,2.10E-05,0.000410324,7.42E-07,3.99E-08,0.999547736,3.67E-10,1.15E-05,2.49E-06
5985,natural_language_inference11,268,For this analysis we exclude assertions including the most prominent predicates in our knowledge base individually when evaluating our models .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,70,1,0,,4.11E-05,0,negative,1.24E-05,1.85E-07,1.99E-06,7.47E-09,6.70E-06,3.64E-05,0.000909051,3.29E-06,1.87E-07,0.999012245,1.36E-10,1.43E-05,3.30E-06
5986,natural_language_inference11,269,The results are presented in .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,71,1,0,,7.31E-05,0,negative,1.62E-05,1.50E-08,7.92E-07,9.33E-10,3.27E-07,5.57E-06,0.001426249,3.87E-07,2.62E-08,0.99841608,1.51E-10,0.000132267,2.13E-06
5987,natural_language_inference11,270,They demonstrate that the biggest performance drop in total ( blue bars ) stems from related to assertions .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,72,1,0,,2.03E-05,0,negative,0.000356318,7.75E-08,1.14E-05,3.62E-09,1.44E-06,7.69E-06,0.002004485,3.23E-07,9.00E-08,0.99738996,3.80E-10,0.000223347,4.90E-06
5988,natural_language_inference11,271,This very prominent predicate appears much more frequently than other assertions and helps connecting related parts of the 2 input sequences with each other .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,73,1,0,,2.17E-05,0,negative,0.00024881,2.35E-07,1.29E-05,4.30E-08,1.99E-05,3.12E-05,0.002220256,9.95E-07,3.45E-07,0.997352854,2.22E-10,9.91E-05,1.33E-05
5989,natural_language_inference11,272,We believe that related to assertions offer benefits mainly from a modeling perspective by strongly connecting the input sequences with each other and thus bridging long - range dependencies similar to attention .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,74,1,0,,0.000217571,0,negative,0.000113896,1.05E-07,3.21E-06,3.03E-09,3.67E-07,1.17E-05,0.002113503,8.79E-07,2.72E-07,0.997551963,8.02E-10,0.000196292,7.83E-06
5990,natural_language_inference11,273,Looking at the relative drops obtained by normalizing the performance differences on the actually affected examples ( green ) we find that our models depend highly on the presence of antonym and synonym assertions for all tasks as well as partially on is a and derived from assertions .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,75,1,0,,0.030244075,0,negative,0.033685965,1.15E-06,1.45E-05,6.92E-08,9.51E-06,7.58E-05,0.326791748,9.53E-06,3.43E-07,0.60501006,2.98E-09,0.034004443,0.000396828
5991,natural_language_inference11,274,This is an interesting finding which shows that the sensitivity of our models is selective wrt .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,76,1,0,,0.000141273,0,negative,0.001950729,5.90E-07,9.87E-06,1.72E-08,3.11E-06,3.36E-05,0.016514898,3.33E-06,5.84E-07,0.980184299,1.42E-09,0.00126842,3.05E-05
5992,natural_language_inference11,275,the type of knowledge and task .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,77,1,0,,1.42E-05,0,negative,2.76E-06,5.40E-08,7.13E-07,8.91E-09,5.02E-07,2.86E-05,0.000541033,3.38E-06,2.17E-07,0.999405642,3.13E-10,9.71E-06,7.37E-06
5993,natural_language_inference11,276,The fact that the largest relative impact stems from antonyms is very interesting because it is known that such information is hard to capture with distributional semantics contained in pre-trained word embeddings .,A Implementation Details,C Further Analysis of Knowledge Utilization in RTE,natural_language_inference,11,78,1,0,,0.000357693,0,negative,0.004423355,4.87E-07,9.08E-06,1.43E-08,6.65E-06,2.47E-05,0.027825767,2.22E-06,2.01E-07,0.964509885,6.68E-10,0.003153864,4.37E-05
5994,topic_models0,1,title,,,topic_models,0,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
5995,topic_models0,2,Learning document embeddings along with their uncertainties,title,,topic_models,0,1,1,1,research-problem,0.879315993,1,research-problem,6.80E-08,0.000142263,1.18E-07,7.40E-08,7.53E-08,4.82E-07,4.83E-07,1.42E-05,4.55E-05,0.011707781,0.988088663,2.48E-07,5.19E-08
5996,topic_models0,3,abstract,,,topic_models,0,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
5997,topic_models0,4,Majority of the text modelling techniques yield only point - estimates of document embeddings and lack in capturing the uncertainty of the estimates .,abstract,abstract,topic_models,0,1,1,0,,0.091437062,0,research-problem,2.10E-07,2.92E-05,5.90E-08,1.74E-05,4.76E-06,4.83E-06,1.28E-06,1.14E-05,1.42E-06,0.14375314,0.856176076,6.49E-08,1.68E-07
5998,topic_models0,5,These uncertainties give a notion of how well the embeddings represent a document .,abstract,abstract,topic_models,0,2,1,0,,0.002653793,0,negative,4.40E-06,0.002223546,3.59E-06,3.90E-06,4.03E-05,8.91E-06,7.49E-07,5.96E-05,0.000717619,0.971479399,0.025457105,7.33E-07,1.11E-07
5999,topic_models0,6,"We present Bayesian subspace multinomial model ( Bayesian SMM ) , a generative log - linear model that learns to represent documents in the form of Gaussian distributions , thereby encoding the uncertainty in its covariance .",abstract,abstract,topic_models,0,3,1,0,,0.797262007,1,research-problem,7.77E-06,0.038522104,0.000204244,3.61E-06,3.91E-05,9.69E-06,1.63E-05,0.000116009,0.004338463,0.056018898,0.900713598,8.13E-06,2.04E-06
6000,topic_models0,7,"Additionally , in the proposed Bayesian SMM , we address a commonly encountered problem of intractability that appears during variational inference in mixed - logit models .",abstract,abstract,topic_models,0,4,1,0,,0.0951544,0,negative,2.99E-05,0.177993651,3.07E-05,3.24E-05,0.000306092,2.81E-05,9.34E-06,0.000371204,0.007542712,0.44145967,0.372186362,7.82E-06,2.07E-06
6001,topic_models0,8,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,abstract,abstract,topic_models,0,5,1,1,research-problem,0.306684687,0,research-problem,1.95E-05,0.171310808,0.000204351,1.25E-05,0.00018658,2.40E-05,1.75E-05,0.00034525,0.01882048,0.211714028,0.597331323,1.08E-05,2.89E-06
6002,topic_models0,9,Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state - of - the - art neural variational document model on ( Fisher ) speech and ( 20 Newsgroups ) text corpora .,abstract,abstract,topic_models,0,6,1,0,,0.018590148,0,negative,0.00016191,0.003261138,4.12E-06,7.37E-06,3.99E-05,2.74E-05,8.44E-05,0.000667866,7.90E-05,0.753589414,0.241302043,0.000772031,3.32E-06
6003,topic_models0,10,Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data .,abstract,abstract,topic_models,0,7,1,0,,0.002049683,0,negative,6.80E-05,0.005011826,1.67E-06,9.04E-06,0.000174998,1.56E-05,2.18E-05,0.000225015,7.35E-05,0.947681307,0.046630572,8.56E-05,1.01E-06
6004,topic_models0,11,The topic ID results show that the proposed model is outperforms state - of - the - art unsupervised topic models and achieve comparable results to the state - of - the - art fully supervised discriminative models .,abstract,abstract,topic_models,0,8,1,0,,0.021898845,0,negative,0.000507856,0.00566046,1.37E-05,4.31E-05,0.000235481,7.51E-05,0.000506815,0.001386897,8.47E-05,0.808606229,0.17926249,0.003599515,1.78E-05
6005,topic_models0,12,narrative,abstract,abstract,topic_models,0,9,1,0,,0.007485599,0,research-problem,1.36E-05,0.002275309,2.91E-05,6.80E-05,0.000162142,8.41E-05,4.98E-05,0.000268374,0.001571194,0.415939205,0.579522026,1.18E-05,5.35E-06
6006,topic_models0,13,"Learning document embeddings along with their uncertainties Santosh Kesiraju , Old?ich Plchot , Luk Burget , and Suryakanth V Gangashetty",abstract,abstract,topic_models,0,10,1,0,,0.028303368,0,research-problem,8.17E-08,6.76E-05,1.11E-07,2.10E-07,1.35E-07,6.22E-07,1.33E-06,1.23E-05,2.01E-05,0.013091789,0.98680515,3.17E-07,1.20E-07
6007,topic_models0,14,Abstract - Majority of the text modelling techniques yield only point - estimates of document embeddings and lack in capturing the uncertainty of the estimates .,abstract,abstract,topic_models,0,11,1,0,,0.039253936,0,research-problem,6.49E-07,7.25E-05,4.06E-07,0.0004385,1.56E-05,2.32E-05,9.20E-06,2.51E-05,8.97E-06,0.06756574,0.931838297,1.39E-07,1.72E-06
6008,topic_models0,15,These uncertainties give a notion of how well the embeddings represent a document .,abstract,abstract,topic_models,0,12,1,0,,0.001749318,0,negative,5.15E-06,0.0033623,4.50E-06,5.11E-06,8.21E-05,1.13E-05,1.04E-06,7.79E-05,0.00131382,0.986245833,0.008889707,1.12E-06,1.54E-07
6009,topic_models0,16,"We present Bayesian subspace multinomial model ( Bayesian SMM ) , a generative log - linear model that learns to represent documents in the form of Gaussian distributions , thereby encoding the uncertainty in its covariance .",abstract,abstract,topic_models,0,13,1,0,,0.76168524,1,research-problem,2.16E-05,0.141966338,0.000568363,1.05E-05,0.000170189,2.89E-05,5.05E-05,0.000326642,0.01939902,0.128517122,0.708908787,2.54E-05,6.61E-06
6010,topic_models0,17,"Additionally , in the proposed Bayesian SMM , we address a commonly encountered problem of intractability that appears during variational inference in mixed - logit models .",abstract,abstract,topic_models,0,14,1,0,,0.078078465,0,negative,4.36E-05,0.319456989,4.64E-05,5.01E-05,0.000677041,4.31E-05,1.58E-05,0.000562739,0.016187166,0.506440499,0.156459536,1.35E-05,3.53E-06
6011,topic_models0,18,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,abstract,abstract,topic_models,0,15,1,1,research-problem,0.334034802,0,approach,3.32E-05,0.378616359,0.000333378,2.18E-05,0.000495746,4.30E-05,3.32E-05,0.0005846,0.050786741,0.297923706,0.271101178,2.16E-05,5.68E-06
6012,topic_models0,19,Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state - of - the - art neural variational document model on ( Fisher ) speech and ( 20 Newsgroups ) text corpora .,abstract,abstract,topic_models,0,16,1,0,,0.017171771,0,negative,0.000232685,0.005493939,6.10E-06,1.05E-05,9.21E-05,4.28E-05,0.000160191,0.001038518,0.000142909,0.903679278,0.08756918,0.001526214,5.58E-06
6013,topic_models0,20,Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data .,abstract,abstract,topic_models,0,17,1,0,,0.001853744,0,negative,8.47E-05,0.007337439,2.17E-06,1.13E-05,0.000355697,2.14E-05,3.47E-05,0.000306257,0.000110212,0.976859407,0.014730257,0.000144889,1.57E-06
6014,topic_models0,21,The topic ID results show that the proposed model is outperforms state - of - the - art unsupervised topic models and achieve comparable results to the state - of - the - art fully supervised discriminative models .,abstract,abstract,topic_models,0,18,1,0,,0.0202687,0,negative,0.000707567,0.009386135,1.97E-05,6.04E-05,0.000519475,0.000119965,0.000950127,0.002211618,0.000143313,0.914462694,0.064768699,0.006620177,3.01E-05
6015,topic_models0,22,"Index Terms - Bayesian methods , embeddings , topic identification",abstract,abstract,topic_models,0,19,1,0,,0.021222417,0,research-problem,1.03E-06,0.000744882,1.24E-06,5.40E-05,2.09E-05,4.32E-05,1.79E-05,0.000151325,0.000263728,0.292508225,0.706189991,1.19E-06,2.43E-06
6016,topic_models0,23,I. INTRODUCTION,,,topic_models,0,0,1,0,,0.000196916,0,negative,0.000783432,0.00042639,3.86E-05,0.010720081,7.66E-05,0.002184256,0.00040315,0.002121679,0.000468556,0.974938341,0.007667188,4.70E-05,0.000124609
6017,topic_models0,24,"L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -.",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,1,1,1,research-problem,0.341982345,0,research-problem,3.34E-07,0.000128694,1.87E-07,8.61E-07,1.72E-06,2.59E-06,2.17E-06,4.16E-06,0.000153808,0.034074897,0.965629897,3.34E-07,3.39E-07
6018,topic_models0,25,These embeddings elicit the latent semantic relations present among the co-occurring words in a sentence or bag - of - words from a document .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,2,1,0,,0.871685232,1,model,1.85E-06,0.029944297,1.03E-05,1.93E-07,2.26E-05,8.88E-06,1.22E-06,2.53E-05,0.950194988,0.017968035,0.001821904,2.92E-07,1.57E-07
6019,topic_models0,26,"Majority of the techniques for learning these embeddings are based on two complementary ideologies , ( i ) topic modelling , and ( ii ) word prediction .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,3,1,0,,0.295803767,0,research-problem,1.38E-06,0.000761543,8.43E-07,4.10E-05,6.00E-05,4.43E-05,6.83E-06,3.14E-05,0.000254477,0.203332347,0.795463614,5.08E-07,1.66E-06
6020,topic_models0,27,The former methods are primarily built on top of bag - of - words model and tend to capture higher level semantics such as topics .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,4,1,0,,0.032191215,0,negative,7.90E-06,0.003345717,5.37E-06,0.000166474,0.000386551,0.000223775,2.79E-05,0.00014203,0.001434401,0.51320076,0.481050719,2.82E-06,5.62E-06
6021,topic_models0,28,The latter techniques capture lower level semantics by exploiting the contextual information of words in a sequence -.,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,5,1,0,,0.046350092,0,research-problem,5.91E-06,0.004654289,6.21E-06,3.81E-05,0.000139763,0.000148995,1.84E-05,0.000116123,0.007588975,0.490330989,0.49694663,2.27E-06,3.37E-06
6022,topic_models0,29,"On the other hand , there is a growing interest towards developing pre-trained language models , , thatare then finetuned for specific tasks such as document classification , question answering , named entity recognition , etc .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,6,1,0,,0.513424292,1,research-problem,9.73E-07,0.000329347,3.09E-07,7.28E-06,1.36E-05,1.29E-05,4.77E-06,1.59E-05,0.000208575,0.140153587,0.859251287,6.16E-07,8.29E-07
6023,topic_models0,30,Although these models achieve state - of - the - art results in several NLP tasks ; they require enormous computational resources to train .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,7,1,0,,0.029642805,0,research-problem,1.21E-06,0.000465529,3.25E-07,1.37E-05,3.76E-05,2.18E-05,9.04E-06,2.66E-05,0.000137387,0.225363529,0.773921071,8.80E-07,1.33E-06
6024,topic_models0,31,Latent variable models are a popular choice in unsupervised learning ; where the observed data is assumed to be S. generated through the latent variables according to a stochastic process .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,8,1,0,,0.048292878,0,research-problem,8.67E-07,0.0017984,1.22E-06,3.73E-06,8.59E-06,2.33E-05,8.45E-06,5.15E-05,0.003578548,0.124151668,0.870371602,8.56E-07,1.22E-06
6025,topic_models0,32,"The goal is then to estimate the model parameters , and also the latent variables .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,9,1,0,,0.06833554,0,negative,7.27E-06,0.061668306,3.83E-06,1.73E-05,0.000249001,0.000104915,1.09E-05,0.00036321,0.181972657,0.653022732,0.102575512,2.48E-06,1.87E-06
6026,topic_models0,33,"In probabilistic topic models ( PTMs ) the latent variables are attributed to topics , and the generative process assumes that every topic is a sample from a distribution over words in the vocabulary and documents are generated from the distribution of ( latent ) topics .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,10,1,0,,0.772464218,1,research-problem,2.79E-06,0.016245252,5.05E-06,1.50E-05,8.07E-05,4.82E-05,1.59E-05,0.000102234,0.01796696,0.230147127,0.735365764,2.12E-06,2.89E-06
6027,topic_models0,34,"Recent works showed that auto - encoders can also be seen as generative models for images and text , .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,11,1,0,,0.265995737,0,research-problem,2.04E-06,0.001886971,1.95E-06,6.41E-06,2.39E-05,5.82E-05,1.48E-05,6.54E-05,0.007083343,0.395846237,0.595007657,1.45E-06,1.70E-06
6028,topic_models0,35,"Generative models allows us to incorporate prior information about the latent variables , and with the help of variational Bayes ( VB ) techniques , , , one can infer posterior distribution over the latent variables instead of just point - estimates .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,12,1,0,,0.896127381,1,negative,3.38E-05,0.119113375,5.05E-05,2.78E-05,0.000620702,7.28E-05,2.18E-05,0.00015076,0.194248972,0.504880286,0.180766991,9.14E-06,3.15E-06
6029,topic_models0,36,The posterior distribution captures uncertainty of the latent variable estimates while trying to explain ( fit ) the observed data and our prior belief .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,13,1,0,,0.81451539,1,model,2.24E-06,0.030783336,5.56E-06,3.72E-07,3.52E-05,1.92E-05,2.45E-06,6.86E-05,0.936041171,0.032306548,0.000734789,3.80E-07,2.13E-07
6030,topic_models0,37,"In the context of text modelling , these latent variables are seen as embeddings .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,14,1,0,,0.560127541,1,negative,6.59E-06,0.075286387,1.35E-05,1.93E-05,0.000247343,0.000167872,2.26E-05,0.000356578,0.198024573,0.536853696,0.188993358,4.18E-06,4.10E-06
6031,topic_models0,38,"In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a generative model for bag - ofwords representation of documents .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,15,1,1,model,0.971274568,1,model,3.44E-05,0.364860643,0.000282909,4.68E-06,0.000441675,3.43E-05,5.92E-05,7.03E-05,0.581633362,0.023972181,0.028588091,1.28E-05,5.34E-06
6032,topic_models0,39,"We show that our model can learn to represent each document in the form of a Gaussian distribution , thereby encoding the uncertainty in its covariance .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,16,1,1,model,0.831396587,1,model,1.48E-05,0.132055916,1.91E-05,1.06E-06,0.000106658,1.16E-05,5.32E-06,3.70E-05,0.825697892,0.037625337,0.004421137,3.61E-06,6.67E-07
6033,topic_models0,40,"Further , we propose a generative Gaussian classifier that exploits this uncertainty for topic identification ( ID ) .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,17,1,1,model,0.940832718,1,model,1.77E-05,0.328883095,0.000100953,1.81E-06,0.000330308,1.56E-05,1.52E-05,3.59E-05,0.649899659,0.016078215,0.004615222,4.88E-06,1.46E-06
6034,topic_models0,41,"The proposed VB framework can be extended in a straightforward way for subspace n-gram model , that can model n-gram distribution of words in sentences .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,18,1,1,model,0.861222163,1,model,9.04E-06,0.160946368,2.12E-05,1.88E-06,0.000124952,2.27E-05,7.09E-06,6.21E-05,0.773158368,0.057133433,0.008508179,3.62E-06,1.06E-06
6035,topic_models0,42,"Earlier , ( non-Bayesian ) SMM was used for learning document embeddings in an unsupervised fashion .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,19,1,0,,0.225540621,0,research-problem,2.83E-06,0.003302635,4.61E-06,5.74E-05,0.00013139,0.000220885,4.91E-05,0.000174668,0.002361449,0.439085773,0.554599526,2.53E-06,7.17E-06
6036,topic_models0,43,"They were then used for training linear classifiers for topic ID from spoken and textual documents , .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,20,1,0,,0.373174015,0,model,1.25E-05,0.087494315,6.15E-05,1.81E-05,0.0019725,0.000288311,3.84E-05,0.000254639,0.54397963,0.358985821,0.006887011,3.56E-06,3.63E-06
6037,topic_models0,44,"However , one of the limitations was that the learned document embeddings ( also termed as document i-vectors ) were only point - estimates and were prone to over-fitting , especially for shorter documents .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,21,1,0,,0.034517314,0,negative,9.55E-06,0.015363817,5.89E-06,3.42E-05,0.000741013,0.000106664,2.86E-05,0.000102323,0.004617527,0.873383888,0.105598354,5.26E-06,2.94E-06
6038,topic_models0,45,Our proposed model can overcome this problem by capturing the uncertainty of the embeddings in the form of posterior distributions .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,22,1,0,,0.870983214,1,model,8.16E-05,0.276982992,4.69E-05,1.77E-05,0.001532378,7.40E-05,1.79E-05,0.000123388,0.541614821,0.175514707,0.003978208,1.27E-05,2.66E-06
6039,topic_models0,46,"Given the significant prior research in PTMs and related algorithms for learning representations , it is important to draw precise relations between the presented model and former works .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,23,1,0,,0.0335714,0,negative,2.02E-06,0.005069692,9.47E-07,8.48E-06,0.000140156,5.85E-05,6.50E-06,8.12E-05,0.008986253,0.961986878,0.02365745,1.35E-06,5.79E-07
6040,topic_models0,47,"We do this from the following viewpoints : ( a ) Graphical models illustrating the dependency of random and observed variables , ( b ) assumptions of distributions over random variables and their limitations , and ( c ) approximations made during the inference and their consequences .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,24,1,0,,0.38659315,0,negative,2.62E-05,0.320621359,2.34E-05,0.000108959,0.00610993,0.000265406,3.90E-05,0.000423544,0.229208838,0.435117527,0.00804195,8.13E-06,5.81E-06
6041,topic_models0,48,"The contributions of this paper are as follows : ( a ) we present Bayesian subspace multinomial model and analyse its relation to popular models such as latent Dirichlet allocation ( LDA ) , correlated topic model ( CTM ) , paragraph vector ( PV - DBOW ) and neural variational document model ( NVDM ) , ( b ) we adapt tricks from for faster and efficient variational inference of the proposed model , ( c ) we combine optimization techniques from , and use them to train the proposed model , ( d ) we propose a generative Gaussian classifier that exploits uncertainty in the posterior distribution of document embeddings , ( e ) we provide experimental results on both text and speech data showing that the proposed document representations achieve state - of - theart perplexity scores , and ( f ) with our proposed classification systems , we illustrate robustness of the model to over-fitting and at the same time obtain superior classification results when compared systems based on state - of - the - art unsupervised models .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,25,1,0,,0.775137521,1,approach,9.09E-05,0.587713995,0.000294462,1.61E-05,0.00387666,7.96E-05,0.00010205,9.03E-05,0.290569211,0.110720184,0.006399519,4.09E-05,5.99E-06
6042,topic_models0,49,"We begin with the description of Bayesian SMM in Section II , followed by VB for the model in Section III .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,26,1,0,,0.053510963,0,negative,1.12E-05,0.100575669,1.46E-05,0.00022989,0.006574276,0.000574925,5.49E-05,0.000716572,0.06129788,0.824232343,0.00570328,5.29E-06,9.19E-06
6043,topic_models0,50,The complete VB training procedure and algorithm is presented in Section III - A .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,27,1,0,,0.296238111,0,negative,7.04E-06,0.150202263,1.11E-05,2.21E-05,0.001466149,0.000190229,2.76E-05,0.000589147,0.357453768,0.487366993,0.002657068,3.05E-06,3.51E-06
6044,topic_models0,51,The procedure for inferring the document embedding posterior distributions for ( unseen ) documents is described in Section III - B. Section IV presents a generative Gaussian classifier that exploits the uncertainty encoded in document embedding posterior distributions .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,28,1,0,,0.756717306,1,model,8.79E-06,0.267918873,2.88E-05,1.37E-06,0.000433756,1.70E-05,9.53E-06,4.48E-05,0.684556518,0.046224478,0.000751811,3.40E-06,8.64E-07
6045,topic_models0,52,Relationship between Bayesian SMM and existing popular topic models is described in Section V .,I. INTRODUCTION,I. INTRODUCTION,topic_models,0,29,1,0,,0.04059697,0,negative,1.84E-05,0.092380799,4.57E-05,5.80E-06,0.001062941,0.000105337,3.21E-05,0.000119219,0.174580781,0.722096478,0.009542347,7.79E-06,2.32E-06
6046,topic_models0,53,"Experimental details are given in Section VI , followed by results and analysis in Section VII .",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,30,1,0,,0.006430171,0,negative,1.27E-05,0.018952249,1.31E-05,6.91E-05,0.030352737,0.000182163,3.67E-05,7.93E-05,0.005159991,0.94490898,0.000224611,6.54E-06,1.90E-06
6047,topic_models0,54,"Finally , we conclude and discuss directions for future research in Section VIII",I. INTRODUCTION,I. INTRODUCTION,topic_models,0,31,1,0,,0.002158207,0,negative,2.37E-05,0.008103706,8.66E-06,0.000935871,0.007240184,0.00159703,0.000113257,0.000671816,0.004455617,0.975884529,0.000949619,5.58E-06,1.05E-05
6048,topic_models0,55,II .,I. INTRODUCTION,,topic_models,0,32,1,0,,0.002171706,0,negative,3.18E-05,0.009596486,2.25E-05,3.35E-05,0.001825546,0.000269008,3.13E-05,0.000137698,0.020707184,0.966935468,0.000403536,3.92E-06,1.98E-06
6049,topic_models0,56,BAYESIAN SUBSPACE MULTINOMIAL MODEL,I. INTRODUCTION,II .,topic_models,0,33,1,0,,0.07623867,0,negative,1.46E-05,0.050297512,0.000114455,1.81E-05,0.000314397,0.000446331,0.000169376,0.000511134,0.251034267,0.548936705,0.148093096,2.99E-05,2.01E-05
6050,topic_models0,57,Our generative probabilistic model assumes that the training data ( bag - of - words ) were generated as follows :,I. INTRODUCTION,II .,topic_models,0,34,1,0,,0.264488105,0,model,1.75E-06,0.071167254,1.02E-05,7.27E-07,0.000160052,0.000174645,6.70E-06,0.000676584,0.621873774,0.305563976,0.000363277,4.70E-07,5.95E-07
6051,topic_models0,58,"For each document , a K-dimensional latent vector w is generated from isotropic Gaussian prior with mean 0 and precision ?:",I. INTRODUCTION,II .,topic_models,0,35,1,0,,0.153390823,0,negative,7.61E-06,0.153614322,8.98E-06,8.41E-06,0.001162256,0.002738822,6.78E-05,0.012013239,0.171309926,0.658453199,0.000607641,3.23E-06,4.54E-06
6052,topic_models0,59,"The latent vector w is a low dimensional embedding ( K V ) of document - specific distribution of words , where V is the size of the vocabulary .",I. INTRODUCTION,II .,topic_models,0,36,1,0,,0.40722965,0,negative,7.92E-06,0.160217922,1.81E-05,2.80E-05,0.002175754,0.001281378,3.34E-05,0.004038582,0.387030461,0.444854173,0.000307981,1.72E-06,4.64E-06
6053,topic_models0,60,"More precisely , for each document , the Vdimensional vector of word probabilities is calculated as :",I. INTRODUCTION,II .,topic_models,0,37,1,0,,0.033141077,0,negative,4.14E-06,0.023598839,9.68E-06,9.83E-07,0.00044962,8.06E-05,3.66E-06,0.000106694,0.094462041,0.88107147,0.000210583,1.38E-06,2.85E-07
6054,topic_models0,61,"where {m , T } are parameters of the model .",I. INTRODUCTION,II .,topic_models,0,38,1,0,,0.002897403,0,negative,2.21E-06,0.007833508,1.71E-06,4.51E-06,0.000480397,0.000120709,3.22E-06,0.00021606,0.014522637,0.976703356,0.000110724,6.27E-07,3.25E-07
6055,topic_models0,62,The vector m known as universal background model represents log unigram probabilities of words .,I. INTRODUCTION,II .,topic_models,0,39,1,0,,0.097557427,0,negative,3.60E-06,0.044474401,1.65E-05,3.02E-06,0.000403294,0.000631093,1.99E-05,0.00173642,0.403163025,0.549257062,0.000289029,8.07E-07,1.89E-06
6056,topic_models0,63,"T known as total variability matrix , is a low - rank matrix defining subspace of document - specific distributions .",I. INTRODUCTION,II .,topic_models,0,40,1,0,,0.047592222,0,negative,1.11E-05,0.044255551,7.01E-05,1.63E-05,0.005713645,0.000248472,1.44E-05,0.000176965,0.06399168,0.884985284,0.000511668,2.79E-06,2.11E-06
6057,topic_models0,64,"Finally , for each document , a vector of word counts x ( bagof - words ) is sampled from Multinomial distribution :",I. INTRODUCTION,II .,topic_models,0,41,1,0,,0.044423557,0,negative,4.23E-06,0.08897436,1.80E-05,8.08E-07,0.000433219,0.000168211,1.01E-05,0.000487623,0.425701888,0.484079806,0.00012019,1.02E-06,5.70E-07
6058,topic_models0,65,where N is the number of words in the document .,I. INTRODUCTION,II .,topic_models,0,42,1,0,,0.002945347,0,negative,1.96E-06,0.003941273,1.72E-06,1.84E-06,0.000297488,0.000127101,4.99E-06,0.00020513,0.006918425,0.988354515,0.000144433,6.98E-07,4.27E-07
6059,topic_models0,66,"The above described generative process fully defines our Bayesian model , which we will now use to address the following problems : given training data X , we estimate model parameters {m , T } and , for any given document x , we infer posterior distribution over corresponding document embedding Note that such distribution also encodes the inferred uncertainty about such representation .",I. INTRODUCTION,II .,topic_models,0,43,1,0,,0.165040104,0,negative,2.12E-06,0.045410155,7.18E-06,3.97E-06,0.000623945,0.000111112,5.45E-06,0.000205602,0.348693861,0.604767443,0.000167603,7.53E-07,8.01E-07
6060,topic_models0,67,"Using Bayes ' rule , the posterior distribution of document embedding w is written as 1 :",I. INTRODUCTION,II .,topic_models,0,44,1,0,,0.015184363,0,negative,1.51E-06,0.015914063,5.00E-06,1.10E-06,0.000204305,0.000173938,6.99E-06,0.000408592,0.09213795,0.890900797,0.000244257,8.47E-07,6.47E-07
6061,topic_models0,68,"In numerator of ( 4 ) , p ( w ) represents prior distribution of document embeddings ( 1 ) and p ( x |w ) represents the likelihood of observed data .",I. INTRODUCTION,II .,topic_models,0,45,1,0,,0.002979813,0,negative,3.75E-06,0.0083862,2.91E-06,3.29E-06,0.000556363,0.000176036,3.94E-06,0.000286257,0.031017685,0.959532651,2.99E-05,6.62E-07,3.89E-07
6062,topic_models0,69,"According to our generative process , we assume that every document x is a sample from Multinomial distribution ( 3 ) , and the log -likelihood is given as follows :",I. INTRODUCTION,II .,topic_models,0,46,1,0,,0.008383333,0,negative,3.19E-06,0.033131134,5.94E-06,7.73E-07,0.000401555,8.76E-05,5.31E-06,0.000270562,0.084935172,0.881065681,9.14E-05,1.26E-06,3.95E-07
6063,topic_models0,70,where ti represents a row in matrix T .,I. INTRODUCTION,II .,topic_models,0,47,1,0,,0.001037781,0,negative,1.58E-06,0.002200829,1.57E-06,1.32E-06,0.000398457,4.61E-05,1.77E-06,3.98E-05,0.004336475,0.992940884,3.06E-05,5.36E-07,1.61E-07
6064,topic_models0,71,The problem arises while computing the denominator in .,I. INTRODUCTION,,topic_models,0,48,1,0,,0.003562903,0,negative,3.10E-06,0.002671141,2.21E-06,3.25E-05,0.000377771,0.000257129,4.05E-05,0.000234284,0.002517749,0.993152594,0.000705503,2.59E-06,2.97E-06
6065,topic_models0,72,It involves solving the integral over a product of likelihood term containing the softmax function and Gaussian distribution ( prior ) .,I. INTRODUCTION,The problem arises while computing the denominator in .,topic_models,0,49,1,0,,0.045940453,0,negative,0.000148935,0.005625176,0.000172762,5.03E-05,0.00377583,0.000404226,3.50E-05,0.000136071,0.034705949,0.954846355,5.72E-05,1.71E-05,2.51E-05
6066,topic_models0,73,There exists no analytical form for this integral .,I. INTRODUCTION,,topic_models,0,50,1,0,,0.013189558,0,negative,1.64E-06,0.007197763,5.04E-06,1.13E-05,0.000309719,0.000206283,5.78E-05,0.00036571,0.009574729,0.979619571,0.002643288,3.48E-06,3.68E-06
6067,topic_models0,74,"This is a generic problem that arises while performing Bayesian inference for mixed - logit models , , multi-class logistic regression or any other model where likelihood function and prior are not conjugate to each other .",I. INTRODUCTION,There exists no analytical form for this integral .,topic_models,0,51,1,0,,0.011930869,0,negative,1.37E-05,0.001051527,1.89E-05,0.000152598,0.000624176,0.000463541,3.56E-05,0.000179215,0.00213528,0.994277698,0.000996225,1.47E-05,3.68E-05
6068,topic_models0,75,"In such cases , one can resort to variational inference and find an approximation to the posterior distribution p ( w|x ) .",I. INTRODUCTION,There exists no analytical form for this integral .,topic_models,0,52,1,0,,0.005369117,0,negative,1.98E-05,0.001192524,1.32E-05,2.42E-05,0.000327376,0.00029373,2.61E-05,0.000287037,0.004118461,0.99359314,7.68E-05,1.25E-05,1.50E-05
6069,topic_models0,76,"This approximation to the true posterior is referred as variational distribution q ( w ) , and is obtained by minimizing the Kullback - Leibler ( KL ) divergence , D KL ( q || p ) between the approximate and true posterior .",I. INTRODUCTION,There exists no analytical form for this integral .,topic_models,0,53,1,0,,0.085355345,0,negative,1.76E-05,0.014503047,0.000161556,5.75E-07,0.000257193,4.25E-05,1.34E-05,6.27E-05,0.398914089,0.585994264,2.22E-05,6.65E-06,4.21E-06
6070,topic_models0,77,We can express log marginal ( evidence ) of the data as :,I. INTRODUCTION,There exists no analytical form for this integral .,topic_models,0,54,1,0,,0.007858121,0,negative,3.57E-05,0.001166035,5.00E-05,7.71E-07,0.000151311,4.48E-05,9.52E-06,4.23E-05,0.031840964,0.966640544,7.07E-06,8.12E-06,2.79E-06
6071,topic_models0,78,Here H [ q ] represents the entropy of q ( w ) .,I. INTRODUCTION,There exists no analytical form for this integral .,topic_models,0,55,1,0,,0.004383776,0,negative,6.28E-06,0.000257743,7.84E-06,3.11E-07,6.35E-05,3.05E-05,4.13E-06,3.10E-05,0.005114294,0.994478434,2.28E-06,2.39E-06,1.29E-06
6072,topic_models0,79,"Given the data x , log p ( x ) is a constant with respect tow , and D KL ( q || p ) can be minimized by maximizing L ( q ) , which is known as Evidence Lower BOund ( ELBO ) for a document .",I. INTRODUCTION,There exists no analytical form for this integral .,topic_models,0,56,1,0,,0.020409552,0,negative,2.91E-05,0.004152173,5.45E-05,6.28E-07,0.000179543,3.00E-05,1.23E-05,5.24E-05,0.03098777,0.964444044,3.23E-05,2.16E-05,3.75E-06
6073,topic_models0,80,"This is the standard formulation of variational Bayes , where the problem of finding an approximate posterior is transformed into optimization of the functional L ( q ) .",I. INTRODUCTION,There exists no analytical form for this integral .,topic_models,0,57,1,0,,0.030570608,0,negative,2.39E-05,0.007245472,0.000135558,4.90E-06,0.000378901,0.000162029,3.49E-05,0.000209666,0.065103453,0.926571847,9.05E-05,1.45E-05,2.43E-05
6074,topic_models0,81,III .,I. INTRODUCTION,,topic_models,0,58,1,0,,0.001473251,0,negative,3.60E-05,0.00743186,2.40E-05,1.00E-05,0.001789394,0.000297653,7.81E-05,0.000149358,0.017922108,0.972241,1.34E-05,4.76E-06,2.37E-06
6075,topic_models0,82,VARIATIONAL BAYES,I. INTRODUCTION,III .,topic_models,0,59,1,0,,0.175285967,0,negative,5.76E-05,0.079837129,0.00105894,6.87E-05,0.013904386,0.006627414,0.004575084,0.001841349,0.058453836,0.831863876,0.001359492,0.00015651,0.000195704
6076,topic_models0,83,"In this section , using the VB framework , we derive and explain the procedure for estimating model parameters {m , T } and inferring the variational distribution , q ( w ) .",I. INTRODUCTION,III .,topic_models,0,60,1,0,,0.024584251,0,negative,1.42E-05,0.423963726,4.28E-05,1.60E-05,0.020110801,0.000253238,4.00E-05,0.000257331,0.082465543,0.472789384,3.85E-05,5.92E-06,2.47E-06
6077,topic_models0,84,"Before proceeding , we note that our model assumes that all documents and the corresponding document embeddings ( latent variables ) are independent .",I. INTRODUCTION,III .,topic_models,0,61,1,0,,0.057378656,0,negative,1.07E-05,0.036488885,1.14E-05,7.38E-06,0.003774409,0.000162601,8.03E-06,0.000108707,0.041012462,0.918404997,8.31E-06,1.40E-06,7.16E-07
6078,topic_models0,85,This can be seen from the graphical model in .,I. INTRODUCTION,III .,topic_models,0,62,1,0,,0.001705372,0,negative,5.08E-06,0.001871408,4.14E-06,3.58E-07,0.000207512,4.25E-05,3.98E-06,1.98E-05,0.011129295,0.986707401,7.50E-06,9.09E-07,1.58E-07
6079,topic_models0,86,"Hence , we derive the inference only for one document embedding w , given an observed vector of word counts x .",I. INTRODUCTION,III .,topic_models,0,63,1,0,,0.011711015,0,negative,2.31E-05,0.14940662,3.40E-05,4.36E-07,0.001020338,4.08E-05,1.05E-05,5.33E-05,0.23236659,0.617014536,2.59E-05,3.43E-06,5.18E-07
6080,topic_models0,87,"We chose the variational distribution q ( w ) to be Gaussian , with mean ?",I. INTRODUCTION,III .,topic_models,0,64,1,0,,0.156794803,0,negative,1.34E-05,0.057801294,1.19E-05,2.02E-05,0.00423597,0.033929186,0.001240494,0.041321853,0.030684627,0.830694078,1.50E-05,3.93E-06,2.81E-05
6081,topic_models0,88,"and precision ? , i.e. , q ( w ) = N ( w | ? , ? ?1 ) . The functional L ( q ) now becomes :",I. INTRODUCTION,III .,topic_models,0,65,1,0,,0.026929945,0,negative,5.44E-05,0.020233865,9.92E-05,3.20E-06,0.006840375,0.000314155,6.88E-05,7.45E-05,0.013585641,0.958701884,1.13E-05,1.14E-05,1.29E-06
6082,topic_models0,89,The term A from ( 11 ) is the negative KL divergence between the variational distribution q ( w ) and the documentindependent prior from ( 1 ) .,I. INTRODUCTION,III .,topic_models,0,66,1,0,,0.011723908,0,negative,1.77E-05,0.032415913,1.92E-05,7.21E-06,0.004470869,0.000589268,3.41E-05,0.000542327,0.029547137,0.932348458,4.11E-06,1.61E-06,2.05E-06
6083,topic_models0,90,This can be computed analytically as :,I. INTRODUCTION,III .,topic_models,0,67,1,0,,0.001325721,0,negative,3.22E-06,0.001281927,4.52E-06,1.34E-06,0.000684784,0.000129255,6.13E-06,4.15E-05,0.003421408,0.994422246,2.81E-06,5.50E-07,3.00E-07
6084,topic_models0,91,where K denotes the document embedding dimensionality .,I. INTRODUCTION,III .,topic_models,0,68,1,0,,0.007967971,0,negative,5.27E-06,0.007132031,3.39E-06,2.87E-06,0.001268817,0.000378273,2.41E-05,0.000527518,0.004332847,0.986318028,4.46E-06,9.64E-07,1.43E-06
6085,topic_models0,92,The term B from ( 11 ) is the expectation over log -likelihood of a document :,I. INTRODUCTION,III .,topic_models,0,69,1,0,,0.005096026,0,negative,5.61E-06,0.008969012,1.28E-05,5.10E-07,0.000813876,0.000128722,1.41E-05,9.95E-05,0.017430482,0.972520421,3.30E-06,1.08E-06,5.63E-07
6086,topic_models0,93,"( 13 ) involves solving the expectation over log - sum - exp operation ( denoted by F ) , which is intractable .",I. INTRODUCTION,III .,topic_models,0,70,1,0,,0.014099501,0,negative,9.36E-06,0.013670972,1.29E-05,1.09E-06,0.001301564,0.000134349,2.62E-05,8.93E-05,0.00751779,0.977210923,2.13E-05,3.05E-06,1.17E-06
6087,topic_models0,94,"It appears when dealing with variational inference in mixed - logit models , .",I. INTRODUCTION,III .,topic_models,0,71,1,0,,0.005499517,0,negative,6.76E-07,0.001003582,2.15E-06,9.47E-06,0.00044071,0.00048863,2.78E-05,0.000144592,0.000315365,0.997440311,0.000123853,6.72E-07,2.15E-06
6088,topic_models0,95,"We can approximate F with empirical expectation using samples from q ( w ) , but F is a function of q ( w ) , whose parameters we are seeking by optimizing L ( q ) .",I. INTRODUCTION,III .,topic_models,0,72,1,0,,0.003299732,0,negative,2.12E-06,0.004470236,2.77E-06,5.76E-07,0.000472627,0.000147246,1.56E-05,0.000118611,0.002891151,0.99187226,5.31E-06,1.04E-06,4.69E-07
6089,topic_models0,96,The corresponding gradients of L ( q ) with respect to q ( w ) will exhibit high variance if we directly take samples from q ( w ) for the empirical expectation .,I. INTRODUCTION,III .,topic_models,0,73,1,0,,0.015912584,0,negative,5.13E-05,0.008064581,8.63E-06,1.16E-06,0.002875189,0.000118864,3.52E-05,6.81E-05,0.001737318,0.987029585,2.19E-06,7.32E-06,5.32E-07
6090,topic_models0,97,"To overcome this , we will reparametrize the random variable w .",I. INTRODUCTION,III .,topic_models,0,74,1,0,,0.009180408,0,negative,5.76E-05,0.032251902,5.16E-05,3.21E-06,0.006179402,0.000266753,2.47E-05,0.000106511,0.035119209,0.925935516,4.47E-07,2.41E-06,7.24E-07
6091,topic_models0,98,This is done by introducing a differentiable function g over another random variable .,I. INTRODUCTION,III .,topic_models,0,75,1,0,,0.031912365,0,negative,1.43E-05,0.038267467,3.16E-05,1.22E-06,0.002755442,9.94E-05,1.16E-05,7.06E-05,0.07376286,0.884982779,1.03E-06,1.05E-06,5.95E-07
6092,topic_models0,99,"If p ( ) = N ( 0 , I ) , then ,",I. INTRODUCTION,III .,topic_models,0,76,1,0,,0.000238439,0,negative,1.21E-06,0.000545612,1.33E-06,4.44E-07,0.000518095,7.65E-05,4.47E-06,2.86E-05,0.000509057,0.998313734,4.56E-07,3.05E-07,1.51E-07
6093,topic_models0,100,where L is the Cholesky factor of ? ?,I. INTRODUCTION,III .,topic_models,0,77,1,0,,0.000807185,0,negative,4.83E-06,0.001483642,4.32E-06,4.49E-06,0.002837839,0.00090925,4.87E-05,0.000246569,0.000620094,0.993836827,8.81E-07,9.49E-07,1.59E-06
6094,topic_models0,101,1,I. INTRODUCTION,III .,topic_models,0,78,1,0,,0.000121751,0,negative,6.39E-07,0.000296938,6.72E-07,1.20E-06,0.000305692,0.000145288,4.56E-06,4.35E-05,0.000707456,0.998493454,3.00E-07,1.04E-07,2.34E-07
6095,topic_models0,102,.,I. INTRODUCTION,III .,topic_models,0,79,1,0,,0.000115929,0,negative,3.48E-07,0.000197218,3.09E-07,1.72E-07,9.75E-05,5.52E-05,2.70E-06,2.81E-05,0.00054811,0.999069929,2.41E-07,9.42E-08,9.91E-08
6096,topic_models0,103,"Using this reparametrization of w , we obtain the following approximation :",I. INTRODUCTION,III .,topic_models,0,80,1,0,,0.003812092,0,negative,5.66E-05,0.016039907,4.34E-05,1.05E-06,0.003093687,0.000203077,9.69E-05,8.17E-05,0.013592089,0.966779865,1.84E-06,8.34E-06,1.53E-06
6097,topic_models0,104,where R denotes the total number of samples r from p( ) .,I. INTRODUCTION,III .,topic_models,0,81,1,0,,0.000773716,0,negative,1.37E-06,0.00075718,1.29E-06,1.07E-06,0.000841553,0.000161807,1.05E-05,7.63E-05,0.00043992,0.997707716,4.65E-07,3.81E-07,4.04E-07
6098,topic_models0,105,"Combining , and , we get the approximation to L ( q ) .",I. INTRODUCTION,III .,topic_models,0,82,1,0,,0.016880062,0,negative,9.33E-06,0.004772944,5.86E-06,5.49E-06,0.0031199,0.000684611,7.69E-05,0.00028128,0.004313527,0.986723776,1.12E-06,2.11E-06,3.19E-06
6099,topic_models0,106,"We will introduce the document suffix d , to make the notation explicit :",I. INTRODUCTION,III .,topic_models,0,83,1,0,,0.000566398,0,negative,4.16E-07,0.001342627,2.76E-06,1.96E-06,0.001322879,0.000365597,1.18E-05,0.000115057,0.001372395,0.995463288,3.10E-07,1.46E-07,7.26E-07
6100,topic_models0,107,"For the entire training data X , the complete ELBO will be simply the summation over all the documents , i.e. , d L ( q d ) .",I. INTRODUCTION,III .,topic_models,0,84,1,0,,0.004353583,0,negative,4.14E-06,0.024382966,2.71E-05,4.59E-07,0.001207614,0.000185592,3.85E-05,0.000194525,0.05399673,0.919957653,1.69E-06,1.23E-06,1.84E-06
6101,topic_models0,108,A. Training,,,topic_models,0,0,1,0,,0.15692037,0,negative,0.000209353,0.00405191,9.08E-05,2.01E-05,2.17E-05,0.000853993,0.000342357,0.020148109,0.001332582,0.962084503,0.010045405,0.000742343,5.68E-05
6102,topic_models0,109,The variational Bayes ( VB ) training procedure for Bayesian SMM is stochastic because of the sampling involved in the re-parametrization trick .,A. Training,A. Training,topic_models,0,1,1,0,,0.02216299,0,negative,0.000369262,0.001505615,0.000121994,0.000120804,4.57E-05,0.000946929,0.000600436,0.003746165,0.000416506,0.588723274,0.402520045,0.000562041,0.000321238
6103,topic_models0,110,"Like the standard VB approach , we optimize ELBO alternately with respect to q ( w ) and {m , T }.",A. Training,A. Training,topic_models,0,2,1,0,,0.000817177,0,negative,0.003232318,0.034816878,0.002617529,1.54E-05,9.90E-05,0.002823616,0.000263706,0.029617059,0.032815993,0.892502293,0.0004686,0.000672774,5.48E-05
6104,topic_models0,111,"Since we do not have closed form update equations , we perform gradient - based updates .",A. Training,A. Training,topic_models,0,3,1,0,,0.000789829,0,negative,0.006468886,0.012838558,0.001609316,1.60E-05,0.0001074,0.002471278,0.000202672,0.020756957,0.012475377,0.942179756,9.05E-05,0.000748629,3.47E-05
6105,topic_models0,112,"Additionally , we regularize rows in matrix T while optimizing .",A. Training,A. Training,topic_models,0,4,1,0,,0.01523125,0,negative,0.004469404,0.006127714,0.000544384,1.22E-05,7.67E-05,0.001950235,0.000114264,0.017394597,0.011059156,0.95761515,4.21E-05,0.000567078,2.70E-05
6106,topic_models0,113,"Thus , the final objective function becomes ,",A. Training,,topic_models,0,5,1,0,,7.19E-05,0,negative,0.000437328,8.81E-05,1.58E-05,1.62E-06,4.38E-06,0.000247756,1.98E-05,0.001155394,0.000375657,0.997355056,4.32E-05,0.000251909,3.91E-06
6107,topic_models0,114,"where we have added the term for 1 regularization of rows in matrix T , with corresponding weight ?.",A. Training,"Thus , the final objective function becomes ,",topic_models,0,6,1,0,,7.55E-05,0,negative,0.001376693,1.37E-05,0.000176701,1.50E-06,1.42E-06,3.85E-05,2.63E-06,0.000339949,0.000122389,0.997734332,2.97E-06,0.0001654,2.38E-05
6108,topic_models0,115,The same regularization was previously used for non Bayesian SMM in .,A. Training,"Thus , the final objective function becomes ,",topic_models,0,7,1,0,,0.000343256,0,negative,0.000726972,7.59E-05,0.000845979,1.06E-05,3.48E-06,0.000215081,1.56E-05,0.001957174,0.000462221,0.994800228,0.000289274,0.000228963,0.000368587
6109,topic_models0,116,This can also be seen as obtaining a maximum a posteriori estimate of T with Laplace priors .,A. Training,"Thus , the final objective function becomes ,",topic_models,0,8,1,0,,0.000304431,0,negative,0.000219569,5.82E-05,0.000437888,1.09E-06,1.34E-06,1.38E-05,1.40E-06,0.000186157,0.0005049,0.998351893,4.93E-05,0.000130705,4.39E-05
6110,topic_models0,117,1 ) Parameter initialization :,A. Training,"Thus , the final objective function becomes ,",topic_models,0,9,1,0,,0.317525308,0,negative,0.000461731,0.000782912,0.000248049,1.36E-05,6.98E-06,0.001145882,4.08E-05,0.080163934,0.003849826,0.912551773,9.83E-05,0.000149228,0.000486997
6111,topic_models0,118,The vector m is initialized to log uni-gram probabilities estimated from training data .,A. Training,"Thus , the final objective function becomes ,",topic_models,0,10,1,0,,0.002993623,0,negative,0.000402607,0.000646403,0.000159782,7.32E-06,5.82E-06,0.00286632,6.56E-05,0.195500151,0.002624383,0.797177638,4.26E-05,0.000101029,0.000400261
6112,topic_models0,119,"The values in matrix T are randomly initialized from N ( 0 , 0.001 ) .",A. Training,"Thus , the final objective function becomes ,",topic_models,0,11,1,0,,0.199424488,0,negative,0.000448182,0.000423479,5.78E-05,1.73E-05,8.20E-06,0.008758037,0.000207808,0.489214014,0.000760244,0.498990176,3.38E-05,0.00014202,0.000938972
6113,topic_models0,120,"The prior over latent variables p ( w ) is set to isotropic Gaussian distribution with mean 0 and ? = { 1 , 10 }.",A. Training,"Thus , the final objective function becomes ,",topic_models,0,12,1,0,,0.100859302,0,negative,0.000656745,0.000479197,0.00011562,1.66E-05,8.92E-06,0.0084994,0.000226537,0.413990466,0.001030542,0.573748559,3.17E-05,0.00020033,0.00099533
6114,topic_models0,121,"The variational distribution q ( w ) is initialized to N ( 0 , diag ( 0.1 ) ) .",A. Training,"Thus , the final objective function becomes ,",topic_models,0,13,1,0,,0.341715057,0,negative,0.000777868,0.000591659,0.000120812,1.83E-05,1.07E-05,0.008415668,0.000238506,0.484278455,0.001282476,0.502827529,3.02E-05,0.000177538,0.001230258
6115,topic_models0,122,"Later in Section VII - A , we will show that initializing the posterior to a sharper Gaussian distribution helps to speedup the convergence .",A. Training,"Thus , the final objective function becomes ,",topic_models,0,14,1,0,,0.007602536,0,negative,0.059292999,0.000129423,0.00022766,8.73E-06,1.81E-05,8.09E-05,4.02E-05,0.000897545,0.000155698,0.922319543,7.33E-06,0.016707428,0.000114399
6116,topic_models0,123,2 ) Optimization :,A. Training,"Thus , the final objective function becomes ,",topic_models,0,15,1,0,,0.252739547,0,negative,0.000712995,0.000272231,0.005139712,6.48E-07,2.28E-06,3.69E-05,6.37E-06,0.000753739,0.001655388,0.990764316,3.69E-05,0.000567244,5.13E-05
6117,topic_models0,124,The gradient - based updates are done by ADAM optimization scheme ; in addition to the following tricks :,A. Training,"Thus , the final objective function becomes ,",topic_models,0,16,1,0,,0.128266402,0,negative,0.009053843,0.001121414,0.002490238,2.26E-05,3.56E-05,0.000763255,6.33E-05,0.018074591,0.003631445,0.963953475,9.57E-06,0.000342612,0.000438205
6118,topic_models0,125,We simplified the variational distribution q ( w ) by making its precision matrix ?,A. Training,"Thus , the final objective function becomes ,",topic_models,0,17,1,0,,8.57E-05,0,negative,0.001971375,1.80E-05,0.000153828,2.65E-06,6.53E-06,6.40E-05,5.00E-06,0.000470054,0.000105696,0.996855407,1.00E-06,0.000309018,3.74E-05
6119,topic_models0,126,diagonal,A. Training,"Thus , the final objective function becomes ,",topic_models,0,18,1,0,,0.000206609,0,negative,0.003068482,1.60E-05,0.000870911,7.64E-06,8.69E-06,0.000117352,2.82E-05,0.00044878,0.00039243,0.993186648,6.10E-06,0.001377596,0.000471136
6120,topic_models0,127,2 .,A. Training,"Thus , the final objective function becomes ,",topic_models,0,19,1,0,,2.18E-05,0,negative,0.000151021,5.47E-06,2.06E-05,5.06E-07,5.65E-07,2.53E-05,1.22E-06,0.000312789,6.71E-05,0.999361012,1.11E-06,3.32E-05,2.01E-05
6121,topic_models0,128,"Further , while updating it , we used log standard deviation parametrization , i.e. ,",A. Training,"Thus , the final objective function becomes ,",topic_models,0,20,1,0,,3.70E-05,0,negative,0.00141181,4.55E-05,0.000182402,4.58E-06,1.03E-05,0.000224256,1.12E-05,0.003402632,0.00018562,0.994290439,1.06E-06,0.000154036,7.62E-05
6122,topic_models0,129,The gradients of the objective ( 16 ) w.r.t. the mean ?,A. Training,"Thus , the final objective function becomes ,",topic_models,0,21,1,0,,0.000113239,0,negative,0.000250419,9.69E-06,2.73E-05,6.23E-07,1.44E-06,1.90E-05,1.83E-06,0.000343348,3.80E-05,0.999125286,1.19E-06,0.000166103,1.57E-05
6123,topic_models0,130,is given as follows :,A. Training,"Thus , the final objective function becomes ,",topic_models,0,22,1,0,,9.26E-05,0,negative,0.000167421,1.41E-05,0.000221695,2.74E-07,7.33E-07,5.57E-06,7.56E-07,0.000110638,0.000300538,0.999092022,1.91E-06,6.86E-05,1.58E-05
6124,topic_models0,131,"where ,",A. Training,"Thus , the final objective function becomes ,",topic_models,0,23,1,0,,5.73E-06,0,negative,3.26E-05,1.94E-06,6.23E-06,3.64E-07,3.41E-07,1.09E-05,5.50E-07,0.000142377,2.43E-05,0.999752359,4.53E-07,1.96E-05,8.06E-06
6125,topic_models0,132,The gradient w.r.t log standard deviation ?,A. Training,"Thus , the final objective function becomes ,",topic_models,0,24,1,0,,0.00281364,0,negative,0.000189601,2.22E-05,4.08E-05,4.37E-06,2.68E-06,9.12E-05,6.28E-06,0.001888243,0.00017668,0.997286506,4.48E-06,0.000107429,0.000179461
6126,topic_models0,133,is given as :,A. Training,"Thus , the final objective function becomes ,",topic_models,0,25,1,0,,8.15E-05,0,negative,0.000101395,9.88E-06,5.31E-05,5.75E-07,9.21E-07,9.55E-06,8.25E-07,0.00020227,0.000194989,0.999358356,1.66E-06,4.02E-05,2.62E-05
6127,topic_models0,134,"where 1 represents a column vector of ones , denotes element - wise product , and exp is element - wise exponential operation .",A. Training,"Thus , the final objective function becomes ,",topic_models,0,26,1,0,,7.71E-05,0,negative,0.000108987,1.46E-05,3.03E-05,1.34E-06,1.79E-06,3.86E-05,1.96E-06,0.000925852,0.000129022,0.998665825,8.84E-07,3.09E-05,4.99E-05
6128,topic_models0,135,The 1 regularization term makes the objective function ( 17 ) discontinuous ( non-differentiable ) at points where it crosses the orthant .,A. Training,"Thus , the final objective function becomes ,",topic_models,0,27,1,0,,0.007791158,0,negative,0.012114448,0.000188313,0.000445085,1.96E-05,2.17E-05,0.000322043,2.72E-05,0.00919905,0.001759193,0.974788115,2.26E-06,0.000536439,0.000576553
6129,topic_models0,136,"Hence , we used sub-gradients and employed orthant - wise learning .",A. Training,"Thus , the final objective function becomes ,",topic_models,0,28,1,0,,0.002743357,0,negative,0.001199761,5.11E-05,0.000220765,5.52E-06,8.86E-06,0.000137705,6.28E-06,0.001912276,0.000345999,0.995843072,1.12E-06,0.000108856,0.000158696
6130,topic_models0,137,The gradient of the objective w.r.t. a row ti in matrix T is computed as follows :,A. Training,"Thus , the final objective function becomes ,",topic_models,0,29,1,0,,4.42E-05,0,negative,0.000206503,1.61E-05,9.89E-05,4.85E-07,1.58E-06,1.25E-05,1.53E-06,0.000238948,0.000120809,0.999150377,9.55E-07,0.000113033,3.83E-05
6131,topic_models0,138,"Here , sign and exp operate element - wise .",A. Training,"Thus , the final objective function becomes ,",topic_models,0,30,1,0,,0.0004731,0,negative,0.000161434,2.36E-05,9.25E-05,1.58E-06,2.27E-06,2.72E-05,1.76E-06,0.000566461,0.000432831,0.998601955,4.83E-07,3.55E-05,5.25E-05
6132,topic_models0,139,The sub-gradient ?t i is defined as :,A. Training,"Thus , the final objective function becomes ,",topic_models,0,31,1,0,,7.60E-05,0,negative,0.000192593,4.98E-06,5.78E-05,6.19E-07,1.24E-06,1.19E-05,1.30E-06,0.000173847,3.62E-05,0.999373682,4.33E-07,0.000115942,2.94E-05
6133,topic_models0,140,"Finally , the rows in matrix T are updated according to ,",A. Training,"Thus , the final objective function becomes ,",topic_models,0,32,1,0,,8.92E-05,0,negative,0.000438794,1.15E-05,0.000150717,3.19E-07,1.53E-06,9.63E-06,1.39E-06,0.000192402,0.000202675,0.998843711,2.55E-07,0.000125779,2.13E-05
6134,topic_models0,141,"where , d i is the step in ascent direction ,",A. Training,"Thus , the final objective function becomes ,",topic_models,0,33,1,0,,6.49E-06,0,negative,0.000234145,5.02E-06,5.13E-05,8.57E-07,1.70E-06,1.41E-05,1.40E-06,0.000189615,5.61E-05,0.999314615,3.85E-07,9.20E-05,3.88E-05
6135,topic_models0,142,"Here , ?",A. Training,"Thus , the final objective function becomes ,",topic_models,0,34,1,0,,7.44E-06,0,negative,3.59E-05,1.20E-06,6.00E-06,1.09E-06,1.34E-06,1.70E-05,8.15E-07,0.000131182,9.32E-06,0.99974712,1.25E-07,2.57E-05,2.33E-05
6136,topic_models0,143,"is the learning rate , f i and s i represents bias corrected first and second moments ( as required by ADAM ) of sub -gradient ?t",A. Training,"Thus , the final objective function becomes ,",topic_models,0,35,1,0,,3.38E-05,0,negative,4.38E-05,5.51E-06,9.08E-06,1.69E-06,1.34E-06,1.62E-05,1.07E-06,0.000391037,3.45E-05,0.999417457,5.26E-07,3.48E-05,4.29E-05
6137,topic_models0,144,i respectively .,A. Training,"Thus , the final objective function becomes ,",topic_models,0,36,1,0,,0.005537716,0,negative,8.52E-05,1.50E-06,7.22E-06,1.01E-07,3.31E-07,4.54E-06,7.51E-07,0.000122373,1.48E-05,0.999655438,9.10E-08,9.73E-05,1.04E-05
6138,topic_models0,145,"PO represents orthant projection , which ensures that the update step does not cross the point of non-differentiability .",A. Training,"Thus , the final objective function becomes ,",topic_models,0,37,1,0,,0.004175296,0,negative,0.000395098,0.000118418,0.0029242,5.47E-06,1.18E-05,9.59E-05,1.67E-05,0.001528628,0.002414436,0.991026694,4.69E-06,0.000199486,0.001258582
6139,topic_models0,146,"It is defined as ,",A. Training,,topic_models,0,38,1,0,,2.10E-06,0,negative,1.38E-05,6.49E-06,1.72E-06,1.45E-07,1.02E-06,5.22E-05,5.34E-06,0.000343173,5.41E-05,0.999492018,7.46E-07,2.81E-05,1.13E-06
6140,topic_models0,147,"The orthant projection introduces explicit zeros in the estimated T matrix and , results in sparse solution .",A. Training,"It is defined as ,",topic_models,0,39,1,0,,0.148089093,0,negative,0.065755656,0.000113806,0.001350516,1.78E-05,4.46E-05,0.000129207,5.31E-05,0.002894286,0.000240479,0.917984668,5.92E-07,0.011169324,0.000245927
6141,topic_models0,148,"Unlike in , we do not require to apply the sign projection , because both the gradient ?t i and step d point to the same orthant ( due to properties of ADAM ) .",A. Training,"It is defined as ,",topic_models,0,40,1,0,,0.000244117,0,negative,0.001088268,0.000113262,0.000318247,1.06E-06,8.12E-06,1.65E-05,5.64E-06,0.00103476,0.000178555,0.996253855,1.62E-07,0.000964042,1.75E-05
6142,topic_models0,149,The stochastic VB training is outlined in Algorithm 1 . compute sub-gradients ?t,A. Training,"It is defined as ,",topic_models,0,41,1,0,,0.000391731,0,negative,6.37E-05,1.83E-05,8.02E-05,3.46E-07,1.44E-06,3.49E-05,6.32E-06,0.002922549,6.77E-05,0.996496332,2.21E-07,0.000272525,3.55E-05
6143,topic_models0,150,i using and 12 update rows in T using ( 24 ) 13 until convergence or max iterations,A. Training,"It is defined as ,",topic_models,0,42,1,0,,0.00097248,0,negative,0.000583795,1.66E-05,0.000222029,2.48E-07,3.06E-06,1.19E-05,7.22E-06,0.000489385,3.50E-05,0.996404441,1.28E-07,0.002208933,1.72E-05
6144,topic_models0,151,B. Inferring embeddings for new documents,A. Training,,topic_models,0,43,1,0,,0.007993215,0,negative,0.000226804,0.001959322,0.000927258,1.39E-06,2.10E-05,0.00029668,0.000330835,0.002905089,0.005593035,0.983101063,0.000135536,0.004287065,0.000214932
6145,topic_models0,152,"After obtaining the model parameters from VB training , we can infer ( extract ) the posterior distribution of document embedding q ( w ) for any given document x .",A. Training,B. Inferring embeddings for new documents,topic_models,0,44,1,0,,6.13E-06,0,negative,7.96E-06,1.09E-05,5.77E-06,1.66E-08,5.12E-07,7.50E-06,1.25E-06,8.68E-05,5.01E-05,0.999784719,6.18E-08,4.38E-05,6.18E-07
6146,topic_models0,153,This is done by iteratively updating the parameters of q ( w ) that maximize L ( q ) from .,A. Training,B. Inferring embeddings for new documents,topic_models,0,45,1,0,,9.37E-06,0,negative,1.89E-05,4.24E-05,1.37E-05,9.27E-08,1.81E-06,2.77E-05,3.13E-06,0.000383526,0.000189005,0.999292986,5.12E-08,2.47E-05,1.98E-06
6147,topic_models0,154,These updates are performed by following the same ADAM optimization scheme as in training .,A. Training,B. Inferring embeddings for new documents,topic_models,0,46,1,0,,0.000115613,0,negative,5.41E-05,0.000171965,1.70E-05,6.47E-07,6.06E-06,0.0007897,4.94E-05,0.017428624,0.000540715,0.980895147,6.10E-08,3.20E-05,1.45E-05
6148,topic_models0,155,"Note that the embeddings are extracted by maximizing the ELBO , that does not involve any supervision ( topic labels ) .",A. Training,B. Inferring embeddings for new documents,topic_models,0,47,1,0,,2.82E-05,0,negative,1.59E-05,7.11E-06,5.01E-06,2.44E-08,5.81E-07,1.04E-05,1.14E-06,0.000126305,2.99E-05,0.99977851,7.53E-09,2.48E-05,3.42E-07
6149,topic_models0,156,These embeddings which are in the form of posterior distributions will be used as input features for training topic ID classifiers .,A. Training,B. Inferring embeddings for new documents,topic_models,0,48,1,0,,6.54E-05,0,negative,5.28E-06,1.68E-05,6.92E-06,5.40E-08,1.05E-06,3.02E-05,1.76E-06,0.000391952,0.000196981,0.999336196,2.40E-08,1.12E-05,1.51E-06
6150,topic_models0,157,"Alternatively , one can use only the mean of the posterior distributions as point estimates of document embeddings .",A. Training,B. Inferring embeddings for new documents,topic_models,0,49,1,0,,5.53E-06,0,negative,2.96E-06,6.78E-06,3.06E-06,9.75E-08,4.47E-07,4.36E-05,3.02E-06,0.000490531,3.15E-05,0.999393218,1.16E-07,2.20E-05,2.73E-06
6151,topic_models0,158,IV .,A. Training,,topic_models,0,50,1,0,,1.55E-05,0,negative,0.000253669,1.52E-05,6.48E-06,4.55E-07,5.14E-06,0.0001254,2.06E-05,0.000657797,6.59E-05,0.998625493,8.26E-08,0.00021925,4.54E-06
6152,topic_models0,159,GAUSSIAN CLASSIFIER WITH UNCERTAINTY,A. Training,IV .,topic_models,0,51,1,0,,0.001164264,0,negative,0.000615639,0.00036092,0.001318056,3.21E-06,1.16E-05,0.000175002,0.000795065,0.000492973,0.008223564,0.964146756,0.00032053,0.000388784,0.023147855
6153,topic_models0,160,"In this section , we will present a generative Gaussian classifier that exploits the uncertainty in posterior distributions of document embedding .",A. Training,IV .,topic_models,0,52,1,0,,0.003020455,0,negative,0.004110266,0.006030905,0.016067942,4.91E-06,0.000236246,9.44E-05,0.000230054,0.000178312,0.061930078,0.908150868,1.56E-05,0.000266768,0.002683691
6154,topic_models0,161,"Moreover , it also exploits the same uncertainty while computing the posterior probability of class labels .",A. Training,IV .,topic_models,0,53,1,0,,7.13E-05,0,negative,0.001617715,0.000212472,0.002122285,5.26E-07,2.37E-05,1.03E-05,1.69E-05,1.72E-05,0.003300819,0.992468364,9.84E-07,7.51E-05,0.000133555
6155,topic_models0,162,"The proposed classifier is called Gaussian linear classifier with uncertainty ( GLCU ) and is inspired by , .",A. Training,IV .,topic_models,0,54,1,0,,0.000653609,0,negative,0.000737201,0.001264799,0.017842092,7.29E-07,5.70E-05,3.77E-05,7.26E-05,8.74E-05,0.022813532,0.956421949,2.91E-06,3.91E-05,0.000623015
6156,topic_models0,163,It can be seen as an extension to the simple Gaussian linear classifier ( GLC ) .,A. Training,IV .,topic_models,0,55,1,0,,0.000146545,0,negative,0.00111206,0.00032908,0.026704184,1.02E-06,4.18E-05,3.25E-05,6.24E-05,3.03E-05,0.010642197,0.960180588,4.07E-06,6.23E-05,0.000797419
6157,topic_models0,164,Let = 1 . . .,A. Training,,topic_models,0,56,1,0,,4.69E-06,0,negative,1.58E-05,6.10E-06,1.51E-06,3.13E-07,3.48E-06,0.000105869,1.20E-05,0.000658632,2.51E-05,0.999102461,4.85E-08,6.51E-05,3.57E-06
6158,topic_models0,165,"L denote class labels , d = 1 . . .",A. Training,Let = 1 . . .,topic_models,0,57,1,0,,3.33E-05,0,negative,8.54E-05,4.07E-07,3.40E-05,1.32E-07,2.75E-07,3.13E-06,1.16E-06,0.000114784,3.50E-06,0.999382767,4.31E-09,0.000153145,0.000221286
6159,topic_models0,166,"D represent document indices , and h d represent the class label of document din one - hot encoding .",A. Training,Let = 1 . . .,topic_models,0,58,1,0,,0.000225195,0,negative,7.98E-05,6.19E-07,8.64E-05,1.45E-07,3.15E-07,3.05E-06,1.29E-06,0.000135716,1.15E-05,0.999212938,9.45E-09,0.000123162,0.000345076
6160,topic_models0,167,"GLC assumes that every class is Gaussian distributed with a specific mean , and a shared precision matrix D.",A. Training,Let = 1 . . .,topic_models,0,59,1,0,,0.00144147,0,negative,0.000424964,2.36E-05,0.013051921,1.76E-06,4.56E-06,4.18E-05,4.05E-05,0.000584566,0.000166409,0.972269954,6.51E-07,0.00120614,0.012183212
6161,topic_models0,168,"Let M denote a matrix of class means , with representing a column .",A. Training,Let = 1 . . .,topic_models,0,60,1,0,,9.10E-05,0,negative,2.34E-05,2.23E-07,1.67E-05,5.26E-08,1.22E-07,1.66E-06,5.19E-07,7.30E-05,4.40E-06,0.999697775,3.10E-09,5.71E-05,0.000125017
6162,topic_models0,169,GLC is described by the following model :,A. Training,Let = 1 . . .,topic_models,0,61,1,0,,0.00278989,0,negative,0.000258981,3.34E-06,0.004619374,3.33E-07,8.96E-07,9.70E-06,1.27E-05,0.000147302,0.000405436,0.98926127,3.17E-07,0.000656618,0.004623762
6163,topic_models0,170,where,A. Training,Let = 1 . . .,topic_models,0,62,1,0,,2.75E-06,0,negative,1.61E-05,5.00E-08,6.23E-06,4.91E-08,4.30E-08,1.72E-06,4.69E-07,4.54E-05,1.19E-06,0.99979763,1.30E-09,4.03E-05,9.08E-05
6164,topic_models0,171,"In our case , however , the training examples come in the form of posterior distributions ,",A. Training,Let = 1 . . .,topic_models,0,63,1,0,,4.64E-05,0,negative,3.29E-05,3.11E-07,2.44E-05,5.06E-08,1.30E-07,1.36E-06,3.89E-07,5.08E-05,5.75E-06,0.999725246,3.47E-09,5.73E-05,0.000101409
6165,topic_models0,172,) as extracted using our Bayesian SMM .,A. Training,Let = 1 . . .,topic_models,0,64,1,0,,0.000110525,0,negative,0.000846883,1.97E-07,0.000150162,5.61E-08,4.32E-07,1.52E-06,1.77E-06,2.10E-05,1.73E-06,0.997645477,1.51E-09,0.001190553,0.000140252
6166,topic_models0,173,"In such case , the proper ML training procedure should maximize the expected classconditional likelihood , with the expectation over w d calculated for each training example with respect to its posterior distri-",A. Training,Let = 1 . . .,topic_models,0,65,1,0,,3.72E-05,0,negative,0.000129775,3.38E-07,8.98E-05,1.72E-08,1.11E-07,6.48E-07,7.35E-07,1.48E-05,3.87E-06,0.99922107,4.49E-09,0.000471339,6.75E-05
6167,topic_models0,174,"However , it is more convenient to introduce an equivalent model , where the observations are the means ?",A. Training,Let = 1 . . .,topic_models,0,66,1,0,,2.06E-05,0,negative,2.16E-05,1.26E-07,2.00E-05,8.59E-08,6.57E-08,2.06E-06,7.12E-07,5.29E-05,4.06E-06,0.999514016,9.62E-09,7.01E-05,0.000314254
6168,topic_models0,175,d of the posteriors q ( w d ) and the uncertainty encoded in ? ?,A. Training,Let = 1 . . .,topic_models,0,67,1,0,,4.17E-05,0,negative,0.000105773,1.61E-07,3.50E-05,4.13E-07,4.55E-07,6.67E-06,3.22E-06,0.000115566,1.15E-06,0.998116239,2.26E-09,0.000278139,0.001337212
6169,topic_models0,176,"1 dis introduced into the model through the latent variable yd as ,",A. Training,Let = 1 . . .,topic_models,0,68,1,0,,2.30E-05,0,negative,0.000214432,2.87E-07,6.36E-05,1.58E-07,2.95E-07,3.49E-06,1.54E-06,0.000123832,5.70E-06,0.999040408,1.97E-09,0.000196856,0.000349413
6170,topic_models0,177,The resulting model is called GLCU .,A. Training,,topic_models,0,69,1,0,,0.0004548,0,negative,3.00E-05,9.23E-05,0.000191837,7.21E-08,4.35E-06,3.78E-05,4.65E-05,0.000362863,0.001533238,0.997472308,1.12E-07,0.000200992,2.76E-05
6171,topic_models0,178,"Since the random variables yd and dare Gaussiandistributed , the resulting class conditional likelihood is obtained using convolution of two Gaussians , i.e ,",A. Training,The resulting model is called GLCU .,topic_models,0,70,1,0,,2.10E-06,0,negative,0.000282916,1.48E-06,8.81E-05,8.15E-08,6.87E-07,3.25E-06,3.25E-06,4.85E-05,3.71E-05,0.998826542,5.65E-09,0.000622097,8.59E-05
6172,topic_models0,179,"GLCU can be trained by estimating its parameters ? , that maximize the class conditional likelihood of training data .",A. Training,The resulting model is called GLCU .,topic_models,0,71,1,0,,1.09E-05,0,negative,4.41E-05,2.98E-06,0.000100719,4.24E-07,9.59E-07,1.35E-05,4.87E-06,0.000289267,8.50E-05,0.99783715,3.64E-08,0.000192676,0.001428255
6173,topic_models0,180,This can be done efficiently by using the following EM algorithm .,A. Training,The resulting model is called GLCU .,topic_models,0,72,1,0,,4.90E-06,0,negative,7.20E-05,3.00E-06,0.000111291,1.11E-07,7.70E-07,7.98E-06,4.28E-06,0.000112253,0.000110841,0.99915366,1.19E-08,0.000132876,0.000290927
6174,topic_models0,181,A. EM algorithm,A. Training,,topic_models,0,73,1,0,,0.00275157,0,negative,6.26E-05,0.000259324,0.000265669,4.04E-06,3.49E-05,0.001091896,0.000789925,0.008147464,0.001080552,0.986657442,2.75E-06,0.000719479,0.000884003
6175,topic_models0,182,"In the E-step , we calculate the posterior distribution of latent variables :",A. Training,A. EM algorithm,topic_models,0,74,1,0,,3.36E-06,0,negative,4.56E-06,2.47E-05,1.68E-05,6.01E-09,9.49E-07,5.83E-06,1.59E-06,0.000221014,7.72E-05,0.999625872,2.35E-09,2.05E-05,8.87E-07
6176,topic_models0,183,"where ,",A. Training,A. EM algorithm,topic_models,0,75,1,0,,7.17E-08,0,negative,1.78E-07,2.83E-07,1.63E-07,9.68E-09,1.34E-07,8.63E-06,3.62E-07,0.000169828,1.62E-06,0.99981602,7.44E-10,2.58E-06,1.85E-07
6177,topic_models0,184,"In the M-step , we maximize the auxiliary function Q with respect to model parameters ?.",A. Training,A. EM algorithm,topic_models,0,76,1,0,,2.55E-06,0,negative,6.79E-06,6.51E-05,1.10E-05,3.58E-08,2.91E-06,1.67E-05,2.91E-06,0.00097606,0.000113594,0.998779539,4.12E-09,2.26E-05,2.84E-06
6178,topic_models0,185,"It is the expectation of log joint - probability with respect to p ( y d | ? d ) , i.e. ,",A. Training,A. EM algorithm,topic_models,0,77,1,0,,5.63E-08,0,negative,7.59E-07,1.80E-06,2.20E-06,1.23E-08,5.18E-07,5.97E-06,6.13E-07,0.00013402,4.75E-06,0.999843853,1.19E-09,5.06E-06,4.39E-07
6179,topic_models0,186,"Maximizing the auxiliary function Q w.r.t. ? , we have :",A. Training,A. EM algorithm,topic_models,0,78,1,0,,5.33E-07,0,negative,7.51E-06,3.75E-06,3.64E-06,7.56E-08,1.54E-06,1.65E-05,3.46E-06,0.000306427,5.80E-06,0.999548389,5.26E-09,0.000100047,2.85E-06
6180,topic_models0,187,"where , ad = u d ? (? d ? d ) , and , I is the set of documents from class .",A. Training,A. EM algorithm,topic_models,0,79,1,0,,5.90E-08,0,negative,4.20E-07,1.01E-06,1.01E-06,1.21E-08,8.41E-07,9.60E-06,8.83E-07,0.00021228,1.60E-06,0.999763378,8.15E-10,8.36E-06,5.98E-07
6181,topic_models0,188,"To train the GLCU model , we alternate between E-step and M- step until convergence .",A. Training,A. EM algorithm,topic_models,0,80,1,0,,0.001225302,0,negative,2.40E-05,0.00020664,1.12E-05,5.30E-07,1.95E-05,0.001530516,0.000119523,0.129337933,0.000140242,0.868522244,4.26E-09,2.80E-05,5.97E-05
6182,topic_models0,189,"Given a test document embedding posterior distribution q ( w ) = N ( w | ? , ? ? 1 ) , we compute the class conditional likelihood according to , and the posterior probability of a class C k is obtained by applying the Bayes ' rule :",A. Training,A. EM algorithm,topic_models,0,81,1,0,,7.88E-08,0,negative,1.08E-06,1.79E-06,4.17E-06,3.78E-09,4.25E-07,3.48E-06,5.81E-07,6.70E-05,6.07E-06,0.999905543,6.09E-10,9.44E-06,3.77E-07
6183,topic_models0,190,V .,A. Training,,topic_models,0,82,1,0,,1.35E-05,0,negative,2.65E-05,1.98E-06,1.51E-06,2.01E-07,2.36E-06,8.03E-05,1.53E-05,0.000436258,9.02E-06,0.999371751,2.06E-09,4.62E-05,8.58E-06
6184,topic_models0,191,RELATED MODELS,A. Training,V .,topic_models,0,83,1,0,,2.74E-07,0,negative,0.000130827,5.66E-07,0.000307241,8.36E-08,6.38E-06,1.08E-05,2.18E-05,5.29E-06,8.11E-06,0.997515277,6.63E-09,5.10E-05,0.001942575
6185,topic_models0,192,"In this section , we review and relate some of the popular PTMs and neural network based document models .",A. Training,V .,topic_models,0,84,1,0,,1.67E-06,0,negative,0.000108777,8.26E-06,5.09E-05,5.47E-07,2.59E-05,3.00E-05,1.51E-05,3.85E-05,1.14E-05,0.998730921,1.36E-08,1.70E-05,0.000962743
6186,topic_models0,193,"We begin with a brief review of LDA , a probabilistic generative model for bag - of - words representation of documents .",A. Training,V .,topic_models,0,85,1,0,,2.61E-06,0,negative,0.000124791,9.18E-06,7.95E-05,6.42E-06,5.38E-05,9.33E-05,1.91E-05,9.40E-05,2.16E-05,0.99495985,1.81E-08,1.28E-05,0.004525596
6187,topic_models0,194,A. Latent Dirichlet allocation,A. Training,,topic_models,0,86,1,0,,0.003538253,0,negative,0.000430766,0.000424904,0.006558598,2.74E-06,9.31E-05,0.000645329,0.003211961,0.001960087,0.001028976,0.975567216,1.08E-06,0.005910572,0.004164705
6188,topic_models0,195,Let ?,A. Training,A. Latent Dirichlet allocation,topic_models,0,87,1,0,,7.17E-08,0,negative,6.43E-07,6.87E-09,1.08E-07,1.02E-09,2.36E-08,7.27E-07,2.27E-07,7.99E-07,2.11E-07,0.999995855,5.05E-11,9.66E-07,4.33E-07
6189,topic_models0,196,1:K represent K topics .,A. Training,A. Latent Dirichlet allocation,topic_models,0,88,1,0,,9.30E-07,0,negative,1.98E-06,1.43E-07,3.21E-06,4.95E-10,3.47E-08,6.39E-07,7.49E-07,1.19E-06,1.25E-05,0.999975989,1.82E-10,2.27E-06,1.26E-06
6190,topic_models0,197,LDA assumes that every topic ?,A. Training,A. Latent Dirichlet allocation,topic_models,0,89,1,0,,1.72E-05,0,negative,8.06E-06,2.62E-07,2.46E-05,4.29E-08,5.60E-07,1.61E-05,1.95E-05,5.70E-06,4.35E-06,0.999527767,1.44E-08,3.47E-05,0.000358334
6191,topic_models0,198,k is a distribution over a fixed vocabulary of size V .,A. Training,A. Latent Dirichlet allocation,topic_models,0,90,1,0,,9.06E-08,0,negative,1.52E-06,1.63E-07,1.19E-06,1.71E-09,1.07E-07,1.21E-06,7.60E-07,4.28E-06,4.33E-06,0.999983044,8.49E-11,1.56E-06,1.83E-06
6192,topic_models0,199,Every document dis generated by a two step process :,A. Training,A. Latent Dirichlet allocation,topic_models,0,91,1,0,,1.41E-05,0,negative,5.98E-05,4.02E-06,0.000487091,5.77E-09,1.40E-06,2.17E-06,1.05E-05,1.86E-06,0.000267801,0.999087439,2.89E-09,4.40E-05,3.39E-05
6193,topic_models0,200,"First , a document - specific vector ( embedding ) representing a distribution over K topics is sampled , i.e. , ? d ? Dir ( ? ) .",A. Training,A. Latent Dirichlet allocation,topic_models,0,92,1,0,,3.41E-07,0,negative,2.78E-05,3.35E-06,2.14E-05,2.70E-09,9.15E-07,1.40E-06,3.21E-06,3.75E-06,4.22E-05,0.999886035,1.08E-10,6.52E-06,3.44E-06
6194,topic_models0,201,"Then , for each word in the document d , a topic indicator variable z i is sampled : z i ? Multi (? d ; 1 ) and the word xi is in turn sampled from the topic - specific distribution : x i ? Multi (? zi ; 1 ) .",A. Training,A. Latent Dirichlet allocation,topic_models,0,93,1,0,,2.53E-07,0,negative,1.61E-06,1.52E-07,7.14E-07,1.73E-09,1.12E-07,1.89E-06,1.02E-06,7.21E-06,5.53E-06,0.999978395,3.49E-11,9.50E-07,2.41E-06
6195,topic_models0,202,The topic ( ? ) and document ( ? ) vectors live in ( V ? 1 ) and ( K ?1 ) simplexes respectively .,A. Training,A. Latent Dirichlet allocation,topic_models,0,94,1,0,,3.78E-06,0,negative,3.46E-06,3.87E-07,1.47E-06,4.56E-09,2.14E-07,1.12E-05,7.98E-06,4.14E-05,1.25E-05,0.999904464,1.08E-10,3.46E-06,1.34E-05
6196,topic_models0,203,"For every word x i in document d , there is a discrete latent variable z i that tells which topic was responsible for generating the word .",A. Training,A. Latent Dirichlet allocation,topic_models,0,95,1,0,,3.73E-08,0,negative,1.15E-06,1.13E-07,8.24E-07,1.64E-09,7.88E-08,6.34E-07,3.63E-07,2.05E-06,7.24E-06,0.999985435,4.84E-11,8.73E-07,1.23E-06
6197,topic_models0,204,This can be seen from the respective graphical model in .,A. Training,A. Latent Dirichlet allocation,topic_models,0,96,1,0,,2.37E-08,0,negative,9.29E-07,9.31E-09,2.08E-07,2.65E-10,1.04E-08,1.25E-07,8.60E-08,1.78E-07,1.20E-06,0.99999624,1.98E-11,8.33E-07,1.78E-07
6198,topic_models0,205,"During inference , the generative process is inverted to obtain posterior distribution over latent variables , p (? , z | x , ? , ? 1:K ) , given the observed data and prior belief .",A. Training,A. Latent Dirichlet allocation,topic_models,0,97,1,0,,4.78E-07,0,negative,2.41E-06,3.74E-07,2.22E-06,2.14E-09,1.12E-07,7.67E-07,6.02E-07,3.41E-06,2.68E-05,0.999959866,6.43E-11,7.65E-07,2.64E-06
6199,topic_models0,206,"Since the true posterior is intractable , Blei resorted to variational inference which finds an approximation to the true posterior as a variational distribution q ( ? , z ) .",A. Training,A. Latent Dirichlet allocation,topic_models,0,98,1,0,,2.17E-07,0,negative,3.42E-06,1.49E-07,3.13E-06,1.37E-08,2.28E-07,3.92E-06,2.25E-06,2.86E-06,1.75E-06,0.999950702,1.58E-09,6.05E-06,2.55E-05
6200,topic_models0,207,"Further , meanfield approximation was made to make the inference tractable , i.e. , q (? , z ) = q (? ) i q ( z i ) .",A. Training,A. Latent Dirichlet allocation,topic_models,0,99,1,0,,5.79E-08,0,negative,2.73E-05,7.88E-07,8.12E-06,2.60E-08,1.30E-06,8.13E-06,3.05E-06,9.85E-06,2.34E-05,0.999903064,7.36E-11,2.84E-06,1.21E-05
6201,topic_models0,208,"In the original model proposed by Blei , the parameters ?",A. Training,A. Latent Dirichlet allocation,topic_models,0,100,1,0,,1.07E-07,0,negative,3.12E-06,7.35E-08,4.33E-07,2.22E-09,1.48E-07,2.50E-06,1.55E-06,6.99E-06,8.90E-07,0.999979339,2.39E-11,2.71E-06,2.24E-06
6202,topic_models0,209,were obtained using maximum likelihood approach .,A. Training,A. Latent Dirichlet allocation,topic_models,0,101,1,0,,1.51E-05,0,negative,3.43E-06,8.20E-08,7.18E-07,1.03E-08,2.96E-07,1.37E-05,4.75E-06,1.82E-05,1.53E-06,0.999944869,2.86E-11,2.74E-06,9.63E-06
6203,topic_models0,210,The choice of Dirichlet distribution for q ( ? ) simplifies the inference process because of the Dirichlet - Multinomial conjugacy .,A. Training,A. Latent Dirichlet allocation,topic_models,0,102,1,0,,5.66E-05,0,negative,0.001688779,7.84E-07,3.55E-06,4.48E-08,1.30E-06,5.20E-06,1.91E-05,6.66E-06,3.56E-06,0.998038295,8.53E-11,0.000208164,2.46E-05
6204,topic_models0,211,"However , the assumption of Dirichlet distribution causes limitations to the model , and q ( ? ) can not capture correlations between topics in each document .",A. Training,A. Latent Dirichlet allocation,topic_models,0,103,1,0,,3.91E-07,0,negative,3.08E-06,2.17E-08,5.50E-07,1.59E-09,2.94E-08,5.33E-07,7.86E-07,5.46E-07,2.53E-07,0.999978937,2.98E-10,1.14E-05,3.90E-06
6205,topic_models0,212,"This was the motivation for Blei to model documents with Gaussian distributions , and the resulting model is called correlated topic model ( CTM ) .",A. Training,A. Latent Dirichlet allocation,topic_models,0,104,1,0,,1.54E-05,0,negative,2.02E-06,1.02E-07,5.96E-06,4.74E-08,3.43E-07,1.12E-05,6.14E-06,4.38E-06,1.50E-06,0.999792452,2.72E-09,7.42E-06,0.000168452
6206,topic_models0,213,"The generative process for a document in CTM is same as in LDA , except for document vectors are now drawn from Gaussian , i.e. ,",A. Training,A. Latent Dirichlet allocation,topic_models,0,105,1,0,,1.43E-07,0,negative,5.06E-06,4.54E-07,4.88E-06,1.63E-09,1.65E-07,2.22E-06,3.09E-06,6.57E-06,2.42E-05,0.999944019,4.03E-11,2.23E-06,7.09E-06
6207,topic_models0,214,"In this formulation , the document embeddings ?",A. Training,A. Latent Dirichlet allocation,topic_models,0,106,1,0,,2.75E-07,0,negative,4.42E-07,2.72E-08,2.36E-07,4.46E-10,2.87E-08,4.16E-07,2.33E-07,1.33E-06,7.22E-07,0.999995495,5.35E-12,5.06E-07,5.62E-07
6208,topic_models0,215,are no longer in the ( K ?,A. Training,A. Latent Dirichlet allocation,topic_models,0,107,1,0,,3.83E-08,0,negative,3.07E-06,1.94E-08,1.45E-06,1.46E-10,5.47E-08,1.65E-07,4.36E-07,1.67E-07,3.37E-07,0.999990254,3.30E-12,3.64E-06,4.13E-07
6209,topic_models0,216,"1 ) simplex , rather they are dependent through the logistic normal .",A. Training,A. Latent Dirichlet allocation,topic_models,0,108,1,0,,1.05E-07,0,negative,1.77E-05,4.79E-08,1.01E-05,3.70E-10,1.25E-07,3.67E-07,9.96E-07,1.99E-07,1.07E-06,0.999955139,7.58E-12,1.32E-05,1.10E-06
6210,topic_models0,217,This is the same as in our proposed Bayesian SMM ( 1 ) .,A. Training,A. Latent Dirichlet allocation,topic_models,0,109,1,0,,3.87E-08,0,negative,1.49E-06,9.77E-08,2.37E-06,2.60E-10,3.99E-08,2.04E-07,4.50E-07,4.21E-07,8.61E-06,0.999983787,1.20E-11,1.22E-06,1.31E-06
6211,topic_models0,218,The advantage is that the document vectors can model the correlations in topics .,A. Training,A. Latent Dirichlet allocation,topic_models,0,110,1,0,,2.55E-05,0,negative,2.94E-05,2.75E-07,1.64E-05,3.81E-09,3.43E-07,1.06E-06,2.88E-06,8.55E-07,6.33E-06,0.999908402,5.02E-11,1.99E-05,1.41E-05
6212,topic_models0,219,"The topic distributions over vocabulary ? , however , still remained Discrete .",A. Training,A. Latent Dirichlet allocation,topic_models,0,111,1,0,,8.01E-07,0,negative,6.44E-07,1.52E-08,3.55E-07,4.36E-09,5.34E-08,3.34E-06,1.63E-06,2.49E-06,2.41E-07,0.999976773,8.19E-11,2.42E-06,1.20E-05
6213,topic_models0,220,"In Bayesian SMM , the topic - word distributions ( T ) are not Discrete , hence it can model the correlations between words and ( latent ) topics .",A. Training,A. Latent Dirichlet allocation,topic_models,0,112,1,0,,1.20E-06,0,negative,5.36E-06,6.01E-07,1.20E-05,2.45E-08,2.89E-07,7.37E-06,1.11E-05,9.40E-06,5.73E-06,0.999751221,2.12E-09,1.68E-05,0.00018012
6214,topic_models0,221,"The variational inference in CTM is similar to that of LDA including the mean - field approximation , because of the discrete latent variable z ) .",A. Training,A. Latent Dirichlet allocation,topic_models,0,113,1,0,,2.18E-07,0,negative,3.24E-06,3.56E-07,6.36E-06,2.56E-09,1.45E-07,1.15E-06,2.46E-06,2.23E-06,4.65E-06,0.999953737,1.27E-10,6.48E-06,1.92E-05
6215,topic_models0,222,An additional problem is dealing with the non-conjugacy .,A. Training,,topic_models,0,114,1,0,,1.41E-05,0,negative,1.72E-05,6.42E-07,1.54E-06,2.16E-07,3.16E-06,4.34E-05,2.36E-05,0.000201944,1.30E-06,0.999608883,2.27E-10,6.67E-05,3.14E-05
6216,topic_models0,223,"More specifically , it is the intractability while solving the expectation over log -sumexp function ( see F from ) .",A. Training,An additional problem is dealing with the non-conjugacy .,topic_models,0,115,1,0,,1.86E-05,0,negative,1.72E-05,7.33E-08,1.27E-05,1.42E-07,1.03E-06,1.26E-05,7.96E-06,7.14E-06,5.55E-07,0.994767808,3.45E-10,5.05E-06,0.005167702
6217,topic_models0,224,"Blei used Jensen 's inequality to form an upper bound on F , and this in - turn acted as lower bound on ELBO .",A. Training,An additional problem is dealing with the non-conjugacy .,topic_models,0,116,1,0,,9.53E-06,0,negative,6.76E-05,2.49E-07,4.06E-05,4.77E-07,5.59E-06,6.49E-05,3.61E-05,4.40E-05,1.95E-06,0.970785577,1.51E-10,9.61E-06,0.028943342
6218,topic_models0,225,"In our proposed Bayesian SMM , we also encountered the same problem , and we approximated F using the re-parametrization trick ( Section III ) .",A. Training,An additional problem is dealing with the non-conjugacy .,topic_models,0,117,1,0,,2.65E-05,0,negative,9.73E-05,4.59E-07,2.64E-05,6.48E-08,2.64E-06,1.10E-05,1.07E-05,1.02E-05,2.28E-06,0.996987342,7.66E-11,1.04E-05,0.002841187
6219,topic_models0,226,There exist similar approximation techniques based on Quasi Monte Carlo sampling .,A. Training,An additional problem is dealing with the non-conjugacy .,topic_models,0,118,1,0,,3.95E-05,0,negative,2.85E-05,1.66E-07,2.74E-05,1.52E-07,1.05E-06,4.44E-05,4.28E-05,4.50E-05,1.21E-06,0.984810644,1.93E-10,9.73E-06,0.014989015
6220,topic_models0,227,"Unlike in LDA or CTM , Bayesian SMM does not require to make mean - field approximation , because the topic - word mixture is not Discrete thus eliminating the need for discrete latent variable z .",A. Training,An additional problem is dealing with the non-conjugacy .,topic_models,0,119,1,0,,3.71E-05,0,negative,0.001409109,4.93E-06,0.000238499,4.08E-08,5.22E-06,6.66E-06,5.96E-05,8.94E-06,1.70E-05,0.994704217,1.54E-10,0.000116983,0.003428786
6221,topic_models0,228,C. Subspace multinomial model,,,topic_models,0,0,1,0,,0.815088239,1,negative,0.000505528,0.004963262,0.020718335,2.51E-06,4.35E-06,0.00027058,0.0012984,0.004209427,0.002718114,0.760054765,0.184444023,0.020740745,7.00E-05
6222,topic_models0,229,SMM is a log - linear model ; originally proposed for modelling discrete prosodic features for the task of speaker verification .,C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,1,1,0,,0.048541036,0,negative,0.000290768,4.54E-05,0.062086342,3.62E-05,1.19E-06,0.000189779,0.000242651,0.000561462,0.000908569,0.909644648,0.010133954,0.009051202,0.0068078
6223,topic_models0,230,"Later , it was used for phonotatic language recognition and eventually for topic identification and document clustering , .",C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,2,1,0,,9.86E-05,0,negative,0.000173336,1.32E-06,0.000318189,9.71E-06,2.42E-07,4.46E-05,2.09E-05,9.33E-05,5.34E-05,0.997601387,7.00E-05,0.001331318,0.0002822
6224,topic_models0,231,Similar model was proposed by Maas for unsupervised learning of word representations .,C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,3,1,0,,0.000143504,0,negative,7.73E-05,3.25E-06,0.000808405,4.02E-06,1.74E-07,4.47E-05,1.87E-05,0.000224767,0.000169663,0.997108919,0.000224175,0.000960504,0.000355347
6225,topic_models0,232,One of the major differences among these works is the type of regularization used for matrix T .,C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,4,1,0,,1.90E-05,0,negative,5.81E-05,1.60E-06,5.68E-05,1.10E-06,6.95E-08,9.41E-06,4.57E-06,7.82E-05,1.74E-05,0.998209019,1.95E-05,0.001508659,3.56E-05
6226,topic_models0,233,Another major difference is in obtaining embeddings w d for a given test document .,C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,5,1,0,,3.70E-05,0,negative,0.0001126,2.70E-06,0.000250541,6.59E-07,9.46E-08,5.02E-06,3.72E-06,3.98E-05,2.90E-05,0.997037124,1.68E-05,0.002476345,2.56E-05
6227,topic_models0,234,"Maas obtained them by projecting the vector of word counts x d onto the matrix T , i.e. , w d = T x d , whereas , extracted the embeddings by maximizing regularized log - likelihood function .",C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,6,1,0,,0.0002324,0,negative,0.000190677,4.22E-06,0.001544839,4.20E-07,2.51E-07,5.84E-06,3.74E-06,4.72E-05,5.97E-05,0.99616284,7.90E-06,0.001954895,1.74E-05
6228,topic_models0,235,The embeddings extracted using SMM are prone to over-fitting .,C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,7,1,0,,0.129030932,0,negative,0.000527616,3.59E-06,6.34E-05,8.97E-06,2.54E-07,2.61E-05,1.63E-05,0.000209302,3.46E-05,0.991395369,5.01E-05,0.007388001,0.000276298
6229,topic_models0,236,Our Bayesian SMM overcomes this problem by capturing the uncertainty of document embeddings in the posterior distribution .,C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,8,1,0,,0.008871955,0,negative,0.001977289,0.00016584,0.002904155,5.75E-06,2.46E-06,2.00E-05,2.59E-05,0.000254109,0.001153886,0.957059136,0.000107914,0.036107202,0.00021639
6230,topic_models0,237,Our experimental analysis in section VII - C illustrates the robustness of Bayesian SMM .,C. Subspace multinomial model,C. Subspace multinomial model,topic_models,0,9,1,0,,0.004009193,0,negative,0.003383979,7.34E-07,3.59E-05,3.88E-07,1.16E-07,4.46E-06,2.51E-05,3.27E-05,3.37E-06,0.83725077,1.89E-06,0.159238521,2.22E-05
6231,topic_models0,238,D. Paragraph vector,C. Subspace multinomial model,,topic_models,0,10,1,0,,0.007944035,0,negative,0.000117611,1.49E-05,0.006498176,2.20E-07,9.14E-08,1.47E-05,2.31E-05,0.000214159,0.000751783,0.984982406,6.00E-05,0.007230035,9.29E-05
6232,topic_models0,239,"Paragraph vector bag - of - words ( PV - DBOW ) is also a log - linear model , which is trained stochastically to maximize the likelihood of a set of words from a given document .",C. Subspace multinomial model,D. Paragraph vector,topic_models,0,11,1,0,,0.001706781,0,negative,7.69E-05,3.27E-05,0.284891512,1.26E-06,3.24E-07,4.05E-05,3.49E-05,0.000304145,0.001228378,0.712227102,0.000231829,0.000391439,0.000538989
6233,topic_models0,240,"SMM can be seen as a special case of PV - DBOW , since it maximizes the likelihood of all the words in a document .",C. Subspace multinomial model,D. Paragraph vector,topic_models,0,12,1,0,,6.78E-05,0,negative,4.24E-05,1.32E-05,0.008827389,1.79E-07,8.02E-08,3.01E-06,2.72E-06,4.23E-05,0.000222546,0.989889921,0.000111081,0.000813426,3.17E-05
6234,topic_models0,241,E .,C. Subspace multinomial model,,topic_models,0,13,1,0,,5.67E-06,0,negative,9.63E-05,1.66E-07,9.11E-06,3.81E-07,2.67E-08,2.05E-06,7.55E-07,1.73E-05,9.71E-06,0.999583498,1.58E-07,0.000274579,5.98E-06
6235,topic_models0,242,Neural network based models,,,topic_models,0,0,1,0,,0.567334208,1,negative,3.16E-05,0.000290506,0.000554857,1.43E-06,5.60E-07,0.000183922,0.00062966,0.002473808,0.000760342,0.500503784,0.493315238,0.001204653,4.96E-05
6236,topic_models0,243,Neural variational document model ( NVDM ) is an adaptation of variational auto - encoders for document modelling .,Neural network based models,Neural network based models,topic_models,0,1,1,0,,0.055642061,0,negative,3.56E-05,0.000169998,0.015043595,1.04E-06,2.15E-07,2.27E-05,0.000240547,0.00029759,0.000383488,0.949435304,0.032651105,0.001696642,2.22E-05
6237,topic_models0,244,"The encoder models the posterior distribution of latent variables given the input , i.e. , p ? ( z | x ) , and the decoder models distribution of input data given the latent variable , i.e. , p ? ( x | z ) .",Neural network based models,Neural network based models,topic_models,0,2,1,0,,2.03E-05,0,negative,3.82E-06,1.12E-05,0.000100295,5.52E-09,6.38E-09,7.27E-07,3.78E-07,3.25E-05,0.000417266,0.999426015,1.61E-06,6.17E-06,2.58E-08
6238,topic_models0,245,"In NVDM , the authors used bag - of - words as input , while their encoder and decoders are two - layer feed - forward neural networks .",Neural network based models,Neural network based models,topic_models,0,3,1,0,,1.10E-05,0,negative,5.26E-06,1.88E-05,0.000285783,1.43E-07,4.62E-08,8.28E-06,8.58E-06,0.000134714,7.25E-05,0.999312618,0.000108402,4.43E-05,5.43E-07
6239,topic_models0,246,"The decoder part of NVDM is similar to Bayesian SMM , as both the models maximize expected loglikelihood of data , assuming Multinomial distribution .",Neural network based models,Neural network based models,topic_models,0,4,1,0,,0.000153185,0,negative,7.94E-05,0.000138303,0.002215394,1.91E-07,2.33E-07,5.12E-06,5.72E-06,7.05E-05,0.001438007,0.995885329,2.57E-05,0.000135273,7.43E-07
6240,topic_models0,247,"In simple terms , Bayesian SMM is a decoder with a single feed forward layer .",Neural network based models,Neural network based models,topic_models,0,5,1,0,,3.49E-05,0,negative,1.04E-05,2.34E-05,0.000790206,6.18E-08,4.36E-08,1.75E-06,2.60E-06,2.35E-05,0.000200543,0.998789645,7.80E-05,7.95E-05,3.23E-07
6241,topic_models0,248,"For a given test document , in NVDM , the approximate posterior distribution of latent variables is obtained directly by forward propagating through the encoder ; whereas in Bayesian SMM , it is obtained by iteratively optimizing ELBO .",Neural network based models,Neural network based models,topic_models,0,6,1,0,,7.22E-06,0,negative,1.33E-05,4.64E-05,4.24E-05,2.71E-08,4.36E-08,6.64E-07,8.09E-07,3.40E-05,5.16E-05,0.999684705,1.79E-05,0.000108153,5.83E-08
6242,topic_models0,249,The experiments in Section VII show that the posterior distributions obtained from Bayesian SMM represent the data better as compared to the ones obtained directly from the encoder of NVDM .,Neural network based models,Neural network based models,topic_models,0,7,1,0,,0.009848743,0,negative,0.000286128,8.91E-07,6.39E-06,5.36E-09,1.04E-08,8.00E-07,1.47E-05,1.88E-05,6.27E-07,0.914259416,3.49E-06,0.085408609,1.20E-07
6243,topic_models0,250,F. Sparsity in topic models,Neural network based models,,topic_models,0,8,1,0,,3.59E-05,0,negative,7.15E-06,1.49E-06,1.30E-05,4.09E-08,8.14E-09,1.53E-06,7.52E-06,2.48E-05,5.03E-06,0.999340853,0.000123979,0.000474218,4.47E-07
6244,topic_models0,251,"Sparsity is often one of the desired properties in topic models , .",Neural network based models,F. Sparsity in topic models,topic_models,0,9,1,0,,0.00045894,0,negative,1.56E-05,2.52E-06,1.88E-05,9.02E-07,6.81E-08,1.12E-05,1.32E-05,5.46E-05,5.94E-06,0.998142905,0.00144522,0.000286209,2.96E-06
6245,topic_models0,252,"Sparse coding inspired topic model was proposed by , where the authors have obtained sparse representations for both documents and words .",Neural network based models,F. Sparsity in topic models,topic_models,0,10,1,0,,0.001762105,0,negative,3.02E-05,8.84E-06,0.001832848,1.22E-06,1.84E-07,4.89E-05,0.000113825,0.000170013,5.12E-05,0.993999299,0.00285844,0.000864179,2.08E-05
6246,topic_models0,253,"1 regularization over T for SMM ( 1 SMM ) was observed to yield better results when compared to LDA , STC and 2 regularized SMM ( 2 SMM ) .",Neural network based models,F. Sparsity in topic models,topic_models,0,11,1,0,,0.371385351,0,negative,0.006518813,1.03E-06,5.03E-05,1.46E-07,1.11E-07,9.25E-06,0.00019111,6.39E-05,7.51E-07,0.554912654,1.12E-05,0.438238157,2.62E-06
6247,topic_models0,254,Relation between SMM and sparse additive generative model ( SAGE ) was explained in .,Neural network based models,F. Sparsity in topic models,topic_models,0,12,1,0,,0.000193308,0,negative,2.54E-05,2.36E-06,5.61E-05,1.54E-06,1.46E-07,1.35E-05,1.33E-05,2.99E-05,1.27E-05,0.999178166,0.000471362,0.000191975,3.45E-06
6248,topic_models0,255,"In , the authors proposed an algorithm to obtain sparse document embeddings ( called sparse composite document vector ( SCDV ) ) from pre-trained word embeddings .",Neural network based models,F. Sparsity in topic models,topic_models,0,13,1,0,,0.003739618,0,negative,0.000154494,6.38E-05,0.00153865,2.80E-06,1.07E-06,3.73E-05,6.48E-05,0.000186531,7.89E-05,0.995270631,0.000959218,0.001628174,1.36E-05
6249,topic_models0,256,"In our proposed Bayesian SMM , we introduce sparsity into the model parameters T by applying 1 regularization and using orthantwise learning .",Neural network based models,F. Sparsity in topic models,topic_models,0,14,1,0,,0.002193748,0,negative,0.000911439,0.000267826,0.000307914,1.40E-06,2.22E-06,2.07E-05,1.56E-05,0.000350239,0.000248687,0.996669058,9.69E-06,0.001193286,1.94E-06
6250,topic_models0,257,VI .,Neural network based models,,topic_models,0,15,1,0,,4.77E-07,0,negative,7.90E-06,2.61E-07,4.65E-07,6.74E-09,4.74E-09,6.20E-07,3.23E-07,1.11E-05,3.42E-06,0.999952386,4.99E-08,2.35E-05,1.46E-08
6251,topic_models0,258,EXPERIMENTS A. Datasets,Neural network based models,VI .,topic_models,0,16,1,0,,5.46E-06,0,negative,2.19E-05,5.65E-06,4.41E-05,9.24E-09,1.58E-07,2.09E-06,5.42E-06,1.22E-05,4.52E-06,0.999573679,4.07E-06,0.000326028,8.53E-08
6252,topic_models0,259,We have conducted experiments on both speech and text corpora .,Neural network based models,VI .,topic_models,0,17,1,0,,2.57E-05,0,negative,1.27E-05,2.16E-05,1.75E-05,8.76E-08,3.31E-06,5.51E-06,7.50E-06,2.73E-05,6.16E-06,0.999672131,2.12E-06,0.000223818,1.86E-07
6253,topic_models0,260,"The speech data used is Fisher phase 1 corpus 3 , which is a collection of 5850 conversational telephone speech recordings with a closed set of 40 topics .",Neural network based models,VI .,topic_models,0,18,1,0,,9.29E-05,0,negative,1.65E-05,5.10E-05,8.08E-05,4.19E-06,0.000199841,5.83E-05,3.21E-05,9.84E-05,1.47E-05,0.999343166,1.28E-06,9.83E-05,1.51E-06
6254,topic_models0,261,Each conversation is approximately 10 minutes long with two sides of the call and is supposedly about one topic .,Neural network based models,VI .,topic_models,0,19,1,0,,2.08E-06,0,negative,3.63E-05,3.05E-05,4.05E-05,3.13E-06,0.000119554,2.91E-05,1.51E-05,5.67E-05,1.53E-05,0.999578346,8.69E-07,7.37E-05,9.45E-07
6255,topic_models0,262,"We considered each side of the call ( recording ) as an independent document , which resulted in a total of 11700 documents .",Neural network based models,VI .,topic_models,0,20,1,0,,0.000101265,0,negative,6.43E-05,3.12E-05,2.07E-05,7.48E-06,0.000182594,8.21E-05,1.84E-05,9.60E-05,2.16E-05,0.999430956,5.05E-07,4.32E-05,9.98E-07
6256,topic_models0,263,"presents the details of data splits ; they are the same as used in earlier research , , .",Neural network based models,VI .,topic_models,0,21,1,0,,2.21E-07,0,negative,1.93E-06,2.17E-06,1.06E-06,1.46E-08,1.75E-07,8.64E-07,2.78E-07,8.62E-06,4.31E-06,0.999976257,1.10E-07,4.20E-06,1.01E-08
6257,topic_models0,264,"Our preprocessing involved removing punctuation and special characters , but we did not remove any stop words .",Neural network based models,VI .,topic_models,0,22,1,0,,2.81E-05,0,negative,5.38E-05,2.31E-06,3.08E-06,1.72E-06,8.23E-06,1.68E-05,1.33E-06,1.28E-05,6.84E-06,0.999881139,2.57E-07,1.16E-05,1.28E-07
6258,topic_models0,265,"Using Kaldi open - source toolkit , we trained a sequence discriminative DNN - HMM automatic speech recognizer ( ASR ) system to obtain automatic transcriptions .",Neural network based models,VI .,topic_models,0,23,1,0,,0.000885098,0,negative,0.000241073,0.003823775,0.005684084,1.03E-05,0.000259239,0.000154134,0.000343024,0.000664652,0.001662899,0.984943843,8.99E-05,0.002077453,4.57E-05
6259,topic_models0,266,The ASR system resulted in 18 % word - errorrate on a held - out test set .,Neural network based models,VI .,topic_models,0,24,1,0,,0.019661292,0,negative,0.001589503,4.21E-05,8.16E-05,4.12E-07,3.20E-06,2.49E-05,0.000474787,0.000220162,2.59E-05,0.845553963,2.27E-05,0.151945476,1.51E-05
6260,topic_models0,267,We report experimental results on both manual and automatic transcriptions .,Neural network based models,VI .,topic_models,0,25,1,0,,3.10E-05,0,negative,1.77E-05,1.19E-05,1.34E-05,3.71E-08,2.19E-06,1.68E-06,5.56E-06,1.22E-05,3.26E-06,0.999142137,4.89E-07,0.000789237,1.35E-07
6261,topic_models0,268,"The vocabulary size while using manual transcriptions was 24854 , for automatic , it was 18292 , and the average document length is 830 , and 856 words respectively .",Neural network based models,VI .,topic_models,0,26,1,0,,0.000134169,0,negative,1.68E-05,4.48E-05,2.49E-05,1.46E-06,5.09E-05,0.000175584,5.33E-05,0.000736069,2.72E-05,0.998807853,4.67E-07,5.82E-05,2.28E-06
6262,topic_models0,269,"The text corpus used is 20 Newsgroups 4 , which contains 11314 training and 7532 test documents over 20 topics .",Neural network based models,VI .,topic_models,0,27,1,0,,0.000195758,0,negative,1.27E-05,4.56E-05,2.99E-05,3.43E-06,0.000223453,7.15E-05,2.38E-05,0.000176926,1.08E-05,0.999341114,2.85E-07,5.90E-05,1.50E-06
6263,topic_models0,270,"Our preprocessing involved removing punctuation and words that do not occur in at least two documents , which resulted in a vocabulary of 56433 words .",Neural network based models,VI .,topic_models,0,28,1,0,,0.000381155,0,negative,0.000204573,0.000103632,2.48E-05,2.25E-05,0.000229744,0.00024039,3.18E-05,0.000407584,8.44E-05,0.998577831,7.18E-07,6.84E-05,3.53E-06
6264,topic_models0,271,The average document length is 290 words .,Neural network based models,,topic_models,0,29,1,0,,0.00058616,0,negative,9.11E-06,9.35E-06,6.76E-06,1.97E-07,9.95E-07,4.09E-05,5.12E-05,0.0012014,1.40E-05,0.998391007,1.07E-07,0.000273201,1.77E-06
6265,topic_models0,272,B. Hyper- parameters of Bayesian SMM,Neural network based models,The average document length is 290 words .,topic_models,0,30,1,0,,0.01371036,0,negative,2.57E-05,2.58E-06,0.004218377,1.88E-08,2.44E-08,3.90E-06,1.13E-05,5.14E-05,2.61E-05,0.994737773,9.17E-07,0.000917806,4.07E-06
6266,topic_models0,273,"In our topic ID experiments , we observed that the embedding dimension ( K ) and regularization weight ( ? ) for rows in matrix T are the two important hyper - parameters .",Neural network based models,The average document length is 290 words .,topic_models,0,31,1,0,,0.016164164,0,negative,0.000135176,2.99E-06,7.68E-05,1.30E-07,1.97E-07,9.68E-06,1.39E-05,0.000250317,5.87E-06,0.998013465,2.29E-07,0.001485188,6.13E-06
6267,topic_models0,274,"The embedding dimension was chosen from K = { 100 , . . . , 800 } , and regularization weight from ? = { 0.0001 , . . . , 10.0 }.",Neural network based models,The average document length is 290 words .,topic_models,0,32,1,1,hyperparameters,0.883030685,1,negative,4.88E-05,9.22E-06,7.77E-05,1.78E-06,3.19E-07,0.002354604,0.000686342,0.112022015,3.91E-05,0.884276073,3.34E-07,0.000298884,0.000184841
6268,topic_models0,275,C. Proposed topic ID systems,Neural network based models,,topic_models,0,33,1,0,,1.15E-05,0,negative,3.50E-06,6.78E-06,2.90E-05,1.04E-08,2.17E-08,1.14E-06,7.76E-06,4.28E-05,4.26E-05,0.999093217,1.20E-05,0.000760289,8.60E-07
6269,topic_models0,276,Our Bayesian SMM is an unsupervised model trained iteratively by optimizing the ELBO ; it does not necessarily correlate with the performance of topic ID .,Neural network based models,C. Proposed topic ID systems,topic_models,0,34,1,0,,1.89E-06,0,negative,0.000196445,3.18E-05,0.000106241,4.33E-08,3.43E-07,1.35E-06,4.94E-06,2.19E-05,5.16E-05,0.997273375,3.98E-07,0.002310806,8.06E-07
6270,topic_models0,277,"It is valid for SMM , NVDM or any other generative model trained without supervision .",Neural network based models,C. Proposed topic ID systems,topic_models,0,35,1,0,,5.82E-07,0,negative,3.03E-06,2.87E-07,1.76E-06,2.87E-09,5.96E-09,2.48E-07,3.22E-07,4.94E-06,2.23E-06,0.999920938,4.62E-08,6.61E-05,4.68E-08
6271,topic_models0,278,"A typical way to overcome this problem is to have an early stopping mechanism ( ESM ) , which requires to evaluate the topic ID accuracy on a held - out ( or crossvalidation ) set at regular intervals during the training .",Neural network based models,C. Proposed topic ID systems,topic_models,0,36,1,0,,5.20E-06,0,negative,1.37E-05,1.89E-06,2.37E-06,1.18E-07,4.39E-08,2.05E-06,1.26E-06,3.83E-05,7.49E-06,0.999847385,1.58E-06,8.30E-05,8.68E-07
6272,topic_models0,279,It can then be used to stop the training earlier if needed .,Neural network based models,C. Proposed topic ID systems,topic_models,0,37,1,0,,2.43E-07,0,negative,6.12E-06,2.12E-07,1.25E-06,1.43E-09,6.02E-09,9.90E-08,7.82E-08,2.52E-06,3.29E-06,0.99996948,2.92E-09,1.69E-05,1.13E-08
6273,topic_models0,280,"Using the above described scheme , we trained three different classifiers : ( i ) Gaussian linear classifier ( GLC ) , ( ii ) multi-class logistic regression ( LR ) , and , ( iii ) Gaussian linear classifier with uncertainty ( GLCU ) .",Neural network based models,C. Proposed topic ID systems,topic_models,0,38,1,0,,0.000937816,0,negative,3.85E-05,1.42E-05,0.000148112,9.41E-09,1.47E-07,1.88E-06,6.06E-06,3.08E-05,4.24E-05,0.999410128,4.18E-08,0.000307429,3.31E-07
6274,topic_models0,281,Note that GLC and LR can not exploit the uncertainty in the document embeddings ; and are trained using only the mean parameter ?,Neural network based models,C. Proposed topic ID systems,topic_models,0,39,1,0,,1.75E-06,0,negative,5.53E-06,1.30E-07,7.13E-07,2.99E-09,5.72E-09,2.10E-07,1.56E-07,3.21E-06,5.91E-07,0.999943287,1.13E-08,4.61E-05,1.95E-08
6275,topic_models0,282,"of the posterior distributions ; whereas GLCU is trained using the full posterior distribution q ( w ) , i.e. , along with the uncertainties of document embeddings as described in Section IV .",Neural network based models,C. Proposed topic ID systems,topic_models,0,40,1,0,,3.89E-07,0,negative,3.15E-05,8.16E-07,7.13E-06,2.45E-09,2.95E-08,1.97E-07,3.55E-07,3.69E-06,2.57E-06,0.999854618,4.34E-09,9.91E-05,2.59E-08
6276,topic_models0,283,"GLC and GLCU does not have any hyper - parameters to tune , while the 2 regularization weight of LR was tuned using crossvalidation experiments .",Neural network based models,C. Proposed topic ID systems,topic_models,0,41,1,0,,6.01E-06,0,negative,3.01E-05,1.74E-06,6.70E-06,2.76E-08,8.78E-08,4.29E-06,2.95E-06,0.000120893,3.83E-06,0.999758314,3.68E-09,7.09E-05,1.59E-07
6277,topic_models0,284,D. Baseline topic ID systems,Neural network based models,,topic_models,0,42,1,0,,0.000240316,0,negative,7.91E-06,5.50E-06,0.000111141,1.70E-08,5.22E-08,2.11E-06,6.99E-05,5.15E-05,1.06E-05,0.991281042,9.12E-06,0.008448164,2.96E-06
6278,topic_models0,285,"1 ) NVDM : Since NVDM and our proposed Bayesian SMM share similarities , we chose to extract the embeddings from NVDM and use them for training linear classifiers .",Neural network based models,D. Baseline topic ID systems,topic_models,0,43,1,1,baselines,0.033517032,0,negative,0.000367635,8.80E-06,0.00468064,1.56E-07,6.38E-07,4.18E-06,2.89E-05,2.15E-05,1.78E-05,0.990561503,1.26E-07,0.004298156,9.99E-06
6279,topic_models0,286,"Given a trained NVDM model , embeddings for any test document can be extracted just by forward propagating through the encoder .",Neural network based models,D. Baseline topic ID systems,topic_models,0,44,1,0,,4.32E-06,0,negative,7.77E-05,8.34E-07,8.34E-05,1.13E-08,4.81E-08,1.71E-07,7.03E-07,2.73E-06,9.86E-06,0.998899432,1.74E-08,0.000924576,5.48E-07
6280,topic_models0,287,"Although this is computationally cheaper , one needs to decide when to stop training , as a fully converged NVDM may not yield optimal embeddings for discriminative tasks such as topic ID .",Neural network based models,D. Baseline topic ID systems,topic_models,0,45,1,0,,9.47E-07,0,negative,5.25E-05,1.90E-07,4.65E-06,7.35E-08,2.82E-08,1.07E-06,1.38E-06,1.56E-05,1.32E-06,0.99921666,4.56E-08,0.000704108,2.36E-06
6281,topic_models0,288,"Hence , we used the same early stopping mechanism as described in earlier section .",Neural network based models,D. Baseline topic ID systems,topic_models,0,46,1,0,,2.95E-07,0,negative,2.67E-05,1.42E-07,3.07E-05,1.18E-08,3.39E-08,5.18E-07,8.48E-07,5.89E-06,1.89E-06,0.999830075,1.60E-09,0.000102836,3.50E-07
6282,topic_models0,289,"We used the same three classifier pipelines ( LR , GLC , GLCU ) as we used for Bayesian SMM .",Neural network based models,D. Baseline topic ID systems,topic_models,0,47,1,0,,5.79E-06,0,negative,2.62E-05,8.59E-07,0.000188871,3.26E-08,1.36E-07,3.67E-06,1.25E-05,3.09E-05,5.56E-06,0.999461551,8.37E-09,0.000266455,3.29E-06
6283,topic_models0,290,"Our architecture and training scheme are similar to ones proposed in , i.e. , two feed forward layers with either 500 or 1000 hidden units and { sigmoid , ReLU , tanh } activation functions .",Neural network based models,D. Baseline topic ID systems,topic_models,0,48,1,0,,3.43E-05,0,negative,0.000133748,1.18E-05,0.000220041,3.07E-05,2.71E-06,0.00019435,9.37E-05,0.001547186,5.47E-05,0.997073506,6.83E-08,0.000385424,0.000252032
6284,topic_models0,291,"The latent dimension was chosen from K = { 100 , . . . , 800 }.",Neural network based models,D. Baseline topic ID systems,topic_models,0,49,1,0,,0.001037841,0,negative,4.69E-05,3.84E-06,1.39E-05,7.16E-07,2.47E-07,0.000180019,0.000185453,0.011290857,1.87E-05,0.987455629,7.00E-08,0.000702051,0.000101629
6285,topic_models0,292,The hyper - parameters were tuned based on cross-validation experiments .,Neural network based models,D. Baseline topic ID systems,topic_models,0,50,1,0,,0.000138482,0,negative,2.44E-05,2.22E-06,7.43E-06,2.33E-07,1.20E-07,3.55E-05,2.55E-05,0.002927545,1.46E-05,0.996757146,1.72E-08,0.000189875,1.54E-05
6286,topic_models0,293,"2 ) SMM : Our second baseline system is non-Bayesian SMM with 1 regularization over the rows in T matrix , i.e. , 1 SMM .",Neural network based models,D. Baseline topic ID systems,topic_models,0,51,1,1,baselines,0.029994205,0,negative,9.54E-05,6.29E-06,0.035958454,5.11E-08,2.88E-07,4.13E-06,7.57E-05,2.22E-05,3.23E-05,0.960160178,1.01E-07,0.003621385,2.35E-05
6287,topic_models0,294,"It was trained with hyper - parameters such as embedding dimension K = { 100 , . . . , 800 } , and regularization weight ? = { 0.0001 , . . . , 10.0 }.",Neural network based models,D. Baseline topic ID systems,topic_models,0,52,1,0,,0.002092922,0,negative,5.70E-05,6.97E-06,2.95E-05,2.96E-06,6.94E-07,0.000374802,0.000240501,0.018448381,2.54E-05,0.980104606,5.05E-08,0.000489234,0.000219866
6288,topic_models0,295,The embeddings obtained from SMM were then used to train GLC and LR classifiers .,Neural network based models,D. Baseline topic ID systems,topic_models,0,53,1,0,,6.98E-05,0,negative,1.79E-05,7.93E-07,3.46E-05,2.57E-08,7.52E-08,1.71E-06,3.39E-06,6.03E-05,1.32E-05,0.999745706,3.15E-09,0.000119051,3.30E-06
6289,topic_models0,296,"Note that we can not use GLCU here , because SMM yields only point - estimates of embeddings .",Neural network based models,D. Baseline topic ID systems,topic_models,0,54,1,0,,4.18E-07,0,negative,3.41E-05,7.29E-08,1.16E-05,2.54E-09,8.00E-09,1.00E-07,3.06E-07,1.25E-06,5.47E-07,0.999434394,1.76E-09,0.000517471,1.35E-07
6290,topic_models0,297,We used the same early stopping mechanism to train the classifiers .,Neural network based models,D. Baseline topic ID systems,topic_models,0,55,1,0,,2.12E-06,0,negative,1.06E-05,5.72E-07,3.34E-05,6.12E-08,5.29E-08,5.38E-06,6.66E-06,0.000130485,7.91E-06,0.999734568,3.94E-09,6.49E-05,5.43E-06
6291,topic_models0,298,"The experimental analysis in Section VII - C shows that Bayesian SMM is more robust to over-fitting when compared to SMM and NVDM , and does not require an early stopping mechanism .",Neural network based models,D. Baseline topic ID systems,topic_models,0,56,1,0,,0.001595068,0,negative,0.00226396,2.77E-07,1.45E-05,5.75E-08,7.61E-08,1.30E-06,0.000102134,1.75E-05,4.51E-07,0.725330273,1.81E-08,0.272254484,1.50E-05
6292,topic_models0,299,3 ) ULMFiT : The third baseline system is the universal language model fine - tuned for classification ( ULMFiT ) .,Neural network based models,D. Baseline topic ID systems,topic_models,0,57,1,1,baselines,0.086377197,0,negative,4.25E-05,1.06E-06,0.02145172,4.94E-08,1.97E-07,3.50E-06,0.000130236,1.52E-05,7.29E-06,0.968947608,5.78E-08,0.009311522,8.91E-05
6293,topic_models0,300,The pre-trained 5 model consists of 3 BiLSTM layers .,Neural network based models,D. Baseline topic ID systems,topic_models,0,58,1,0,,7.14E-05,0,negative,0.000283883,9.86E-06,0.000326809,4.98E-06,1.43E-06,0.000195363,0.00031576,0.006318624,0.000191028,0.990406233,5.41E-08,0.000524866,0.001421111
6294,topic_models0,301,Finetuning the model involves two steps : ( a ) fine - tuning LM on the target dataset and ( b ) training classifier ( MLP layer ) on the target dataset .,Neural network based models,D. Baseline topic ID systems,topic_models,0,59,1,0,,5.67E-05,0,negative,7.36E-05,1.23E-06,8.37E-05,2.12E-08,8.80E-08,3.19E-07,8.45E-07,6.53E-06,1.16E-05,0.999632937,3.85E-09,0.00018753,1.69E-06
6295,topic_models0,302,We trained several models with various drop - out rates .,Neural network based models,D. Baseline topic ID systems,topic_models,0,60,1,0,,2.13E-05,0,negative,3.44E-05,9.65E-07,1.98E-05,1.90E-07,2.74E-07,1.57E-05,3.10E-05,0.000527153,5.74E-06,0.999102961,2.78E-09,0.000229719,3.21E-05
6296,topic_models0,303,"More specifically , the LM was fine - tuned for 15 epochs 6 , with drop - out rates from : { 0.2 , . . . , 0.6 }.",Neural network based models,D. Baseline topic ID systems,topic_models,0,61,1,0,,0.001207794,0,negative,0.000377401,1.19E-05,7.38E-05,5.90E-06,2.11E-06,0.000308228,0.000573397,0.012985317,3.12E-05,0.983579075,2.50E-08,0.001252062,0.000799636
6297,topic_models0,304,"The classifier was fine - tuned for 50 epochs with drop - out rates from : { 0.2 , . . . , 0.6 }.",Neural network based models,D. Baseline topic ID systems,topic_models,0,62,1,0,,0.002436939,0,negative,0.000110434,6.12E-06,3.74E-05,2.66E-06,8.65E-07,0.000235956,0.000279085,0.012251372,3.18E-05,0.986019675,2.08E-08,0.000460778,0.000563828
6298,topic_models0,305,"A held - out development set was used to tune the hyper - parameters ( drop - out rates , and fine - tuning epochs ) .",Neural network based models,D. Baseline topic ID systems,topic_models,0,63,1,0,,0.000175197,0,negative,7.21E-06,4.81E-07,1.05E-05,2.11E-08,1.06E-07,2.22E-06,4.88E-06,0.000142343,2.44E-06,0.999770501,3.76E-10,5.62E-05,3.06E-06
6299,topic_models0,306,4 ) TF - IDF :,Neural network based models,D. Baseline topic ID systems,topic_models,0,64,1,1,baselines,0.012303673,0,negative,5.61E-05,3.68E-06,0.024993622,1.41E-08,1.25E-07,1.08E-06,4.20E-05,6.95E-06,5.68E-05,0.970925058,1.37E-07,0.003881662,3.28E-05
6300,topic_models0,307,"The fourth baseline system is a standard term frequency - inverse document frequency ( TF - IDF ) based document representation , followed by multi-class logistic regression ( LR ) .",Neural network based models,D. Baseline topic ID systems,topic_models,0,65,1,1,baselines,0.004405764,0,negative,6.09E-05,2.99E-06,0.020085903,3.77E-08,3.06E-07,2.11E-06,5.41E-05,1.14E-05,2.35E-05,0.9771723,3.09E-08,0.002545639,4.09E-05
6301,topic_models0,308,"Although TF - IDF is not a topic model , the classification performance of TF - IDF based systems are often close to state - of - the - art systems .",Neural network based models,D. Baseline topic ID systems,topic_models,0,66,1,0,,0.000156694,0,negative,1.12E-05,8.77E-08,1.23E-05,4.11E-08,3.35E-08,7.99E-07,1.03E-05,6.14E-06,2.23E-07,0.993686801,1.92E-07,0.006231793,4.01E-05
6302,topic_models0,309,VII .,Neural network based models,,topic_models,0,67,1,0,,1.33E-07,0,negative,1.66E-06,6.29E-08,2.27E-07,1.39E-09,4.70E-09,1.09E-07,1.67E-07,4.85E-06,1.13E-06,0.999977277,1.95E-10,1.45E-05,2.15E-08
6303,topic_models0,310,RESULTS AND DISCUSSION,Neural network based models,VII .,topic_models,0,68,1,0,,1.07E-06,0,negative,1.19E-05,1.46E-06,4.86E-06,4.71E-09,1.15E-07,4.95E-07,8.83E-06,8.83E-06,3.09E-06,0.999107843,1.91E-08,0.000852179,3.82E-07
6304,topic_models0,311,A. Convergence rate of Bayesian SMM,Neural network based models,VII .,topic_models,0,69,1,0,,1.03E-05,0,negative,3.58E-05,4.93E-05,0.000228551,2.84E-08,7.07E-07,1.44E-06,1.88E-05,2.98E-05,1.00E-04,0.998360199,3.56E-07,0.001173041,2.08E-06
6305,topic_models0,312,We observed that the posterior distributions extracted using Bayesian SMM are always much sharper than standard Normal distribution .,Neural network based models,VII .,topic_models,0,70,1,0,,1.46E-05,0,negative,3.84E-05,9.95E-07,1.66E-06,5.64E-09,1.67E-07,7.43E-07,5.91E-06,1.83E-05,9.43E-07,0.998698568,7.63E-09,0.001233975,3.06E-07
6306,topic_models0,313,"Hence we initialized the variational distribution to N ( 0 , diag ( 0.1 ) ) to speedup the convergence .",Neural network based models,VII .,topic_models,0,71,1,0,,0.000270085,0,negative,8.07E-06,7.52E-06,2.84E-06,9.21E-08,9.65E-07,5.94E-05,1.98E-05,0.003348179,1.96E-05,0.996518926,1.34E-09,1.27E-05,1.99E-06
6307,topic_models0,314,shows objective ( ELBO ) plotted for two different initializations of variational distribution .,Neural network based models,VII .,topic_models,0,72,1,0,,4.19E-06,0,negative,3.99E-06,3.18E-07,1.96E-06,1.90E-09,2.88E-08,5.85E-07,3.31E-06,2.00E-05,2.05E-06,0.999870787,2.46E-09,9.67E-05,2.64E-07
6308,topic_models0,315,"Here , the model was trained on 20 Newsgroups corpus , with the embedding dimension K = 100 , regularization weight ? = 1.0 and prior set to standard Normal .",Neural network based models,VII .,topic_models,0,73,1,0,,0.001290979,0,negative,2.93E-06,1.57E-05,5.06E-06,1.69E-07,1.77E-06,8.26E-05,5.17E-05,0.006928,1.36E-05,0.992871153,3.19E-09,2.20E-05,5.31E-06
6309,topic_models0,316,"We can observe that the model initialized to N ( 0 , diag ( 0.1 ) ) converges faster as compared to the one initialized to standard Normal .",Neural network based models,VII .,topic_models,0,74,1,0,,0.000615344,0,negative,0.000395469,3.23E-06,3.21E-06,4.53E-08,4.96E-07,5.47E-06,0.000176889,0.000206535,2.36E-06,0.985004636,7.76E-09,0.014196668,4.99E-06
6310,topic_models0,317,"In all the further experiments , we initialized 7 both the prior and variational distributions to N ( 0 , diag ( 0.1 ) ) .",Neural network based models,VII .,topic_models,0,75,1,0,,0.001528737,0,negative,8.14E-06,2.37E-05,4.36E-06,3.00E-07,1.48E-06,0.000260421,0.000114529,0.018897218,3.75E-05,0.980613015,4.30E-09,3.02E-05,9.20E-06
6311,topic_models0,318,B. Perplexity,Neural network based models,,topic_models,0,76,1,0,,2.97E-05,0,negative,3.98E-06,3.44E-07,1.06E-05,3.05E-10,1.13E-08,7.76E-08,5.90E-06,4.61E-06,1.18E-06,0.997484554,3.59E-09,0.002488484,2.43E-07
6312,topic_models0,319,"Perplexity is an intrinsic measure for topic models , .",Neural network based models,B. Perplexity,topic_models,0,77,1,0,,1.95E-06,0,negative,1.02E-06,2.06E-08,5.67E-06,3.43E-09,4.92E-09,1.53E-07,1.62E-06,2.28E-06,1.91E-07,0.999672076,1.19E-08,0.000314229,2.73E-06
6313,topic_models0,320,It is computed as an average of every test document according to :,Neural network based models,B. Perplexity,topic_models,0,78,1,0,,9.16E-08,0,negative,1.67E-06,8.30E-09,5.90E-06,6.29E-11,1.20E-09,1.12E-08,7.03E-08,2.00E-07,2.52E-07,0.999967394,2.13E-11,2.45E-05,1.99E-08
6314,topic_models0,321,or for an entire test corpus according to :,Neural network based models,B. Perplexity,topic_models,0,79,1,0,,5.97E-08,0,negative,7.22E-07,3.22E-09,5.30E-06,1.72E-11,6.28E-10,6.16E-09,9.43E-08,8.99E-08,4.66E-08,0.999916107,1.73E-11,7.76E-05,1.21E-08
6315,topic_models0,322,where,Neural network based models,B. Perplexity,topic_models,0,80,1,0,,2.53E-08,0,negative,1.36E-07,9.35E-10,1.36E-07,1.02E-10,2.08E-10,1.39E-08,3.85E-08,3.67E-07,4.28E-08,0.999992828,1.04E-11,6.42E-06,1.66E-08
6316,topic_models0,323,Nd is the number of word tokens in document d.,Neural network based models,B. Perplexity,topic_models,0,81,1,0,,2.62E-07,0,negative,2.82E-07,6.45E-09,7.55E-07,9.30E-11,8.90E-10,2.28E-08,1.18E-07,1.23E-06,1.30E-07,0.99997852,3.21E-11,1.89E-05,7.90E-08
6317,topic_models0,324,"In our case , log p ( x ) from ( 9 ) can not be evaluated , because the KL divergence from variational distribution q to the true posterior p can not be computed ; as the true posterior is intractable .",Neural network based models,B. Perplexity,topic_models,0,82,1,0,,2.98E-07,0,negative,1.24E-06,3.96E-09,6.08E-07,7.40E-11,6.92E-10,9.10E-09,9.69E-08,2.26E-07,4.44E-08,0.999870591,4.25E-11,0.000127143,3.33E-08
6318,topic_models0,325,"We can only compute L ( q ) , which is a lower bound on log p ( x ) ; thus the resulting perplexity values act as upper bounds .",Neural network based models,B. Perplexity,topic_models,0,83,1,0,,1.90E-07,0,negative,1.36E-05,1.45E-08,3.17E-06,1.56E-10,3.11E-09,2.15E-08,6.00E-07,4.11E-07,2.09E-07,0.99954253,4.93E-11,0.000439286,1.61E-07
6319,topic_models0,326,This is true for NVDM or any other model in the VB framework where the true posterior is intractable .,Neural network based models,B. Perplexity,topic_models,0,84,1,0,,4.01E-07,0,negative,6.65E-07,5.93E-09,9.14E-07,3.79E-10,9.32E-10,3.72E-08,2.32E-07,1.09E-06,1.37E-07,0.999951471,1.09E-10,4.53E-05,1.75E-07
6320,topic_models0,327,"We estimated L ( q ) from ( 16 ) using 32 samples , i.e. , R = 32 , in order to compute perplexity .",Neural network based models,B. Perplexity,topic_models,0,85,1,0,,4.92E-05,0,negative,2.50E-06,8.14E-08,1.92E-06,6.54E-09,2.89E-08,5.24E-06,3.06E-05,0.000244927,3.25E-07,0.999598418,7.67E-11,0.000106937,9.01E-06
6321,topic_models0,328,"In , the authors used 20 samples .",Neural network based models,,topic_models,0,86,1,0,,1.06E-06,0,negative,1.56E-06,6.39E-08,9.41E-07,3.49E-09,8.00E-08,2.08E-07,6.39E-07,1.21E-05,2.75E-07,0.999961336,2.14E-11,2.27E-05,9.55E-08
6322,topic_models0,329,We present the comparison of 20 Newsgroups test data perplexities obtained using Bayesian SMM and NVDM in .,Neural network based models,"In , the authors used 20 samples .",topic_models,0,87,1,0,,0.018018474,0,negative,3.78E-05,1.54E-05,0.001055926,1.22E-07,1.20E-05,8.07E-06,0.000193177,0.000109994,2.72E-06,0.991327182,4.86E-09,0.007155364,8.23E-05
6323,topic_models0,330,It shows the perplexities of 20 Newsgroups corpus under full and a limited vocabulary of 2000 words .,Neural network based models,"In , the authors used 20 samples .",topic_models,0,88,1,0,,0.000452918,0,negative,2.09E-06,1.02E-07,5.72E-05,3.27E-08,2.57E-06,5.63E-06,1.73E-05,5.82E-05,1.39E-07,0.999611735,5.81E-11,0.000229639,1.54E-05
6324,topic_models0,331,We also show the perplexity computed using the maximum likelihood probabilities estimated on the test data .,Neural network based models,"In , the authors used 20 samples .",topic_models,0,89,1,0,,0.000107708,0,negative,5.67E-07,4.52E-08,1.99E-06,4.15E-10,2.59E-08,5.64E-07,2.21E-06,2.81E-05,1.31E-07,0.99988894,1.36E-11,7.71E-05,4.01E-07
6325,topic_models0,332,It acts as the lower bound on the test perplexities .,Neural network based models,"In , the authors used 20 samples .",topic_models,0,90,1,0,,6.04E-05,0,negative,1.76E-06,2.71E-07,1.57E-05,4.78E-09,1.63E-07,7.67E-07,9.59E-07,4.49E-05,1.90E-06,0.999916215,3.88E-11,1.61E-05,1.29E-06
6326,topic_models0,333,"NVDM was shown to achieve superior perplexity scores when compared to LDA , docNADE , Deep Auto Regressive Neural Network models .",Neural network based models,"In , the authors used 20 samples .",topic_models,0,91,1,0,,0.716346998,1,negative,0.000614541,9.92E-07,0.000249621,2.28E-08,7.29E-07,5.56E-06,0.00137985,8.95E-05,7.01E-07,0.596557189,1.20E-09,0.400876013,0.000225256
6327,topic_models0,334,"To the best of our knowledge , our model achieves state - of - the - art perplexity scores on 20 Newsgroups corpus under limited and full vocabulary conditions .",Neural network based models,"In , the authors used 20 samples .",topic_models,0,92,1,0,,0.051060044,0,negative,5.28E-05,2.00E-06,0.000211204,4.27E-08,1.60E-06,7.45E-06,0.001142957,0.000205122,7.16E-07,0.890578231,2.72E-09,0.107427003,0.000370809
6328,topic_models0,335,"In further investigation , we trained both Bayesian SMM and NVDM until convergence .",Neural network based models,"In , the authors used 20 samples .",topic_models,0,93,1,0,,0.009229938,0,negative,1.30E-05,2.03E-06,3.97E-05,3.66E-08,1.57E-06,3.15E-05,4.95E-05,0.001968562,4.69E-06,0.997819916,4.98E-11,4.78E-05,2.17E-05
6329,topic_models0,336,"At regular checkpoints during the training , we froze the model , extracted the embeddings for both training and test data , and computed the perplexities ; shown in .",Neural network based models,"In , the authors used 20 samples .",topic_models,0,94,1,0,,0.005531288,0,negative,5.52E-05,4.56E-07,0.0001392,4.10E-09,8.12E-07,1.14E-06,1.23E-05,1.81E-05,1.01E-06,0.999231027,1.68E-11,0.000538533,2.31E-06
6330,topic_models0,337,We can observe that both the Bayesian SMM and NVDM fit the training data equally well ( low perplexities ) .,Neural network based models,"In , the authors used 20 samples .",topic_models,0,95,1,0,,0.677487627,1,negative,0.000137544,4.49E-07,1.27E-05,3.85E-08,3.84E-07,1.74E-05,0.000932129,0.000771366,3.36E-07,0.931912631,5.16E-10,0.066058145,0.000156904
6331,topic_models0,338,"However , in the case of NVDM , the perplexity of test data increases after certain number of iterations ; suggesting that NVDM fails to generalize and over-fits on the training data .",Neural network based models,"In , the authors used 20 samples .",topic_models,0,96,1,0,,0.012964116,0,negative,0.000423017,8.29E-07,7.55E-05,9.35E-09,5.30E-07,1.64E-06,7.36E-05,4.16E-05,4.60E-07,0.984226268,4.58E-10,0.015140388,1.62E-05
6332,topic_models0,339,"In the case of Bayesian SMM , the perplexity of the test data decreases and remains stable , illustrating the robustness of our model .",Neural network based models,"In , the authors used 20 samples .",topic_models,0,97,1,0,,0.709363074,1,negative,0.010325359,2.75E-06,0.000167691,9.70E-08,2.61E-06,1.34E-05,0.002231049,0.00041879,1.35E-06,0.738604775,7.42E-10,0.247917205,0.000314937
6333,topic_models0,340,C. Early stopping mechanism for topic ID systems,Neural network based models,,topic_models,0,98,1,0,,9.56E-06,0,negative,2.94E-06,1.06E-06,2.12E-05,4.87E-10,1.23E-08,1.75E-07,1.60E-05,1.67E-05,6.49E-06,0.998549252,2.63E-09,0.001384641,1.47E-06
6334,topic_models0,341,The embeddings extracted from a model trained purely in an unsupervised fashion does not necessarily yield optimum results when used in a supervised scenario .,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,99,1,0,,1.11E-05,0,negative,1.16E-05,1.75E-08,2.34E-07,6.56E-10,4.88E-09,1.43E-07,8.57E-06,4.44E-06,6.56E-08,0.995015876,6.66E-11,0.004958553,5.15E-07
6335,topic_models0,342,"As discussed earlier in Sections VI - C , and VI - D , an early stopping mechanism ( ESM ) during the training of an unsupervised model ( eg : NVDM , SMM , and Bayesian SMM ) is required to get optimal performance from the subsequent topic ID system .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,100,1,0,,2.67E-06,0,negative,8.73E-06,1.56E-07,6.80E-07,1.18E-09,1.20E-08,9.87E-08,1.20E-06,6.95E-06,9.42E-07,0.999744417,8.51E-11,0.000236521,2.90E-07
6336,topic_models0,343,The following experiment illustrates the idea of ESM :,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,101,1,0,,7.11E-07,0,negative,1.60E-06,1.92E-08,2.70E-06,3.92E-11,4.42E-09,1.35E-08,1.41E-06,2.21E-07,5.35E-08,0.999435183,1.28E-11,0.000558737,5.69E-08
6337,topic_models0,344,"We trained SMM , Bayesian SMM and NVDM on Fisher data until convergence .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,102,1,0,,0.000279714,0,negative,4.36E-06,5.95E-07,3.82E-06,4.16E-08,1.90E-07,2.05E-05,7.52E-05,0.000798117,1.83E-06,0.999019542,2.51E-11,6.60E-05,9.73E-06
6338,topic_models0,345,"At regular checkpoints during the training , we froze the model , extracted the embeddings for both training and test data .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,103,1,0,,7.89E-06,0,negative,5.53E-06,4.32E-07,6.56E-06,8.23E-10,5.22E-08,2.24E-07,3.88E-06,9.96E-06,2.61E-06,0.999916431,4.17E-12,5.40E-05,3.54E-07
6339,topic_models0,346,"We chose GLC for SMM , GLCU for NVDM , and Bayesian SMM as topic ID classifiers .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,104,1,0,,0.000682719,0,negative,2.24E-06,8.14E-07,1.65E-05,7.58E-09,1.47E-07,1.63E-05,0.000130982,0.000508557,2.51E-06,0.999238678,2.20E-11,7.53E-05,7.90E-06
6340,topic_models0,347,We then evaluated the topic ID accuracy on the cross-validation 8 represents the best cross-validation score and the corresponding test score obtained using the early stopping mechanism ( ESM ) .,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,105,1,0,,6.01E-07,0,negative,7.10E-07,2.41E-08,4.80E-07,5.85E-11,9.85E-09,3.19E-08,8.89E-07,1.13E-06,5.87E-08,0.999912003,1.09E-12,8.46E-05,3.24E-08
6341,topic_models0,348,The embedding dimension was set to 100 for all the models .,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,106,1,0,,0.002961589,0,negative,4.34E-06,2.08E-06,2.05E-06,1.17E-07,1.88E-07,0.00016442,0.000602815,0.031047557,8.31E-06,0.967948625,1.43E-10,0.000145697,7.38E-05
6342,topic_models0,349,and test sets .,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,107,1,0,,6.99E-07,0,negative,9.07E-08,2.87E-09,2.52E-07,2.98E-11,6.15E-09,1.54E-08,3.44E-07,6.91E-07,9.21E-09,0.999972726,2.43E-13,2.58E-05,1.47E-08
6343,topic_models0,350,shows the topic ID accuracy on crossvalidation and test sets obtained at regular checkpoints for all the three models .,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,108,1,0,,2.02E-06,0,negative,6.79E-07,5.72E-09,2.35E-07,1.02E-10,2.84E-09,1.08E-07,6.70E-06,3.44E-06,2.13E-08,0.999447092,4.58E-12,0.00054143,2.89E-07
6344,topic_models0,351,The circular dot ( ) represents the best cross -validation score and the corresponding test score that is obtained by employing ESM .,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,109,1,0,,1.14E-07,0,negative,2.73E-07,1.99E-08,1.89E-07,1.58E-10,4.10E-09,9.75E-08,5.14E-07,7.29E-06,2.17E-07,0.999978624,1.24E-12,1.27E-05,5.96E-08
6345,topic_models0,352,"In case of ( non-Bayesian ) SMM , the test accuracy drops significantly after certain number of iterations ; suggesting the strong need of ESM .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,110,1,0,,0.006758887,0,negative,0.002038824,1.38E-07,2.47E-06,8.78E-09,9.76E-08,6.80E-07,0.000185515,1.43E-05,1.20E-07,0.938240308,6.88E-11,0.059509582,7.95E-06
6346,topic_models0,353,The crossvalidation accuracies of NVDM and Bayesian SMM are similar and remain consistent over the iterations .,Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,111,1,0,,0.001924441,0,negative,0.000138533,1.25E-07,1.53E-06,6.70E-09,5.07E-08,1.39E-06,0.00079691,6.54E-05,7.65E-08,0.89684394,3.38E-11,0.102129931,2.21E-05
6347,topic_models0,354,"However , the test accuracy of NVDM is much lower than that of Bayesian SMM and also decreases over the iterations .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,112,1,0,,1.30E-06,0,negative,6.70E-05,4.61E-08,1.07E-06,1.06E-09,2.19E-08,1.28E-07,4.27E-05,4.18E-06,3.47E-08,0.977722108,5.02E-11,0.02216061,2.13E-06
6348,topic_models0,355,"On the other hand , the test accuracy of Bayesian SMM increases and stays consistent .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,113,1,0,,0.00018149,0,negative,0.000649206,2.12E-07,3.16E-06,1.26E-08,8.55E-08,9.41E-07,0.000301738,3.39E-05,2.17E-07,0.952025397,8.32E-11,0.046967237,1.79E-05
6349,topic_models0,356,"It shows the robustness of our proposed model , which in addition , does not require any ESM .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,114,1,0,,7.77E-06,0,negative,1.22E-05,7.18E-09,1.91E-07,1.48E-10,4.00E-09,3.46E-08,6.19E-06,1.01E-06,2.07E-08,0.997273336,2.17E-12,0.00270684,1.78E-07
6350,topic_models0,357,"In all the further topic ID experiments , we report classification results for Bayesian SMM without ESM ; while the results for SMM , and NVDM are with ESM .",Neural network based models,C. Early stopping mechanism for topic ID systems,topic_models,0,115,1,0,,5.58E-06,0,negative,8.71E-07,2.78E-08,1.04E-06,3.64E-11,7.87E-09,3.55E-08,4.74E-06,9.03E-07,2.11E-08,0.999616743,7.02E-13,0.00037555,6.00E-08
6351,topic_models0,358,D. Topic ID results,,,topic_models,0,0,1,0,,0.466756275,0,results,0.001245845,0.000240162,0.000162425,6.34E-07,2.05E-06,3.66E-05,0.001581576,0.000691095,9.30E-06,0.425675989,0.014240045,0.556089362,2.49E-05
6352,topic_models0,359,This section presents the topic ID results in terms of classification accuracy ( in % ) and cross-entropy ( CE ) on the test sets .,D. Topic ID results,D. Topic ID results,topic_models,0,1,1,0,,0.122311503,0,results,0.000446392,8.47E-08,1.69E-05,4.03E-08,1.53E-08,6.38E-07,8.87E-05,2.68E-06,4.19E-08,0.195840091,1.16E-06,0.803578985,2.43E-05
6353,topic_models0,360,Cross - entropy gives a notion of how confident the classifier is about its prediction .,D. Topic ID results,D. Topic ID results,topic_models,0,2,1,0,,0.001672275,0,negative,0.000841295,7.94E-07,0.001995952,1.02E-06,1.07E-07,1.15E-05,2.78E-05,3.13E-05,7.95E-06,0.956439039,5.14E-06,0.04046154,0.000176473
6354,topic_models0,361,A well calibrated classifier tends to have lower cross - entropy .,D. Topic ID results,D. Topic ID results,topic_models,0,3,1,0,,0.099803155,0,results,0.003572497,2.00E-07,0.000124814,1.74E-07,4.22E-08,2.25E-06,3.97E-05,8.98E-06,4.75E-07,0.468452017,1.88E-06,0.527752148,4.48E-05
6355,topic_models0,362,"presents the classification results on Fisher speech corpora with manual and automatic transcriptions , where the first two rows are the results from earlier published works .",D. Topic ID results,D. Topic ID results,topic_models,0,4,1,1,results,0.01788601,0,results,0.000492007,7.28E-08,0.000228064,1.44E-07,4.71E-08,1.49E-06,9.21E-05,2.30E-06,1.12E-07,0.291060886,2.77E-06,0.707970703,0.000149295
6356,topic_models0,363,"Hazen , used discriminative vocabulary selection followed by a nave Bayes ( NB ) classifier .",D. Topic ID results,D. Topic ID results,topic_models,0,5,1,0,,0.016335496,0,negative,0.003522493,1.18E-06,0.00985024,1.81E-05,5.84E-07,8.31E-05,0.000231403,6.14E-05,1.31E-05,0.855977566,3.14E-05,0.125060934,0.005148477
6357,topic_models0,364,Having a limited ( small ) vocabulary is the major drawback of this approach .,D. Topic ID results,D. Topic ID results,topic_models,0,6,1,0,,0.001349045,0,negative,0.002140534,6.17E-07,0.000143729,2.84E-05,1.68E-07,2.84E-05,8.09E-05,3.29E-05,6.51E-06,0.894473327,8.18E-05,0.097999219,0.004983563
6358,topic_models0,365,"Although we have used the same training and test splits , May had slightly larger vocabulary than ours , and their best system is similar to our baseline TF - IDF based system .",D. Topic ID results,D. Topic ID results,topic_models,0,7,1,0,,9.77E-05,0,negative,0.000827843,9.53E-08,0.000196297,4.97E-07,9.48E-08,5.81E-06,9.48E-05,6.60E-06,3.35E-07,0.710777235,1.46E-06,0.287923127,0.00016578
6359,topic_models0,366,The remaining rows in show our baselines and proposed systems .,D. Topic ID results,D. Topic ID results,topic_models,0,8,1,0,,0.000626987,0,negative,0.00019852,1.14E-07,2.31E-05,5.52E-07,5.81E-08,4.53E-06,1.12E-05,1.41E-05,1.12E-06,0.988250201,2.11E-07,0.011445192,5.12E-05
6360,topic_models0,367,"We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .",D. Topic ID results,D. Topic ID results,topic_models,0,9,1,1,results,0.654170544,1,results,0.00071876,1.06E-08,3.22E-06,8.52E-08,5.22E-09,4.22E-07,0.000129453,1.22E-06,1.03E-08,0.012879136,2.03E-07,0.986154505,0.000112974
6361,topic_models0,368,"To the best of our knowledge , the proposed systems achieve the best classification results on Fisher corpora with the current set - up , i.e. , treating each side of the conversation as an independent document .",D. Topic ID results,D. Topic ID results,topic_models,0,10,1,0,,0.265083532,0,results,0.000188897,1.59E-07,6.49E-05,1.87E-07,4.18E-08,2.50E-06,0.000169629,6.83E-06,1.09E-07,0.255143802,5.77E-06,0.744099753,0.000317457
6362,topic_models0,369,It can be observed ULMFiT has the lowest cross - entropy among all the systems .,D. Topic ID results,D. Topic ID results,topic_models,0,11,1,0,,0.841534631,1,results,0.00153978,1.54E-08,6.22E-06,4.87E-08,6.86E-09,4.25E-07,8.24E-05,1.64E-06,1.46E-08,0.047400753,1.70E-07,0.950923049,4.54E-05
6363,topic_models0,370,presents classification results on 20 Newsgroups dataset .,D. Topic ID results,D. Topic ID results,topic_models,0,12,1,1,results,0.185763023,0,results,0.000308142,1.22E-07,0.00030637,1.68E-07,3.66E-08,1.94E-06,0.000159104,4.03E-06,1.31E-07,0.175787345,5.00E-06,0.822893214,0.000534401
6364,topic_models0,371,The first three rows give the results as reported in earlier works .,D. Topic ID results,D. Topic ID results,topic_models,0,13,1,0,,0.000509809,0,negative,0.00017523,4.08E-08,1.03E-05,1.23E-07,3.09E-08,1.64E-06,1.54E-05,5.15E-06,2.23E-07,0.949118355,1.38E-07,0.050623311,5.00E-05
6365,topic_models0,372,"Pappagari et al. , proposed a CNN - based discriminative model trained to jointly optimize categorical cross - entropy loss for classification task along with binary cross - entropy for verification task .",D. Topic ID results,D. Topic ID results,topic_models,0,14,1,0,,0.010588278,0,negative,0.001516431,3.78E-06,0.002974325,2.71E-05,7.55E-07,8.07E-05,0.000245597,0.000212178,3.00E-05,0.898585451,9.03E-05,0.077907478,0.018325906
6366,topic_models0,373,"Sparse composite document vector ( SCDV ) exploits pre-trained word embeddings to obtain sparse document embeddings , whereas neural tensor skip - gram model ( NTSG ) extends the idea of a skipgram model for obtaining document embeddings .",D. Topic ID results,D. Topic ID results,topic_models,0,15,1,0,,0.645209108,1,negative,0.001128771,7.87E-06,0.105299345,6.81E-06,4.50E-07,5.59E-05,0.00063706,0.000107195,3.51E-05,0.7038225,0.000312129,0.177235739,0.011351177
6367,topic_models0,374,"The authors in ( SCDV ) have shown superior classification results as compared to paragraph vector , LDA , NTSG , and other systems .",D. Topic ID results,D. Topic ID results,topic_models,0,16,1,0,,0.066825274,0,results,0.000321271,1.30E-07,0.00042059,4.24E-07,5.85E-08,4.94E-06,0.000161865,7.25E-06,2.91E-07,0.440317386,9.42E-06,0.557024609,0.001731772
6368,topic_models0,375,The next rows in present our baselines and proposed systems .,D. Topic ID results,D. Topic ID results,topic_models,0,17,1,0,,0.000300028,0,negative,0.000384215,2.18E-07,2.76E-05,2.96E-06,1.44E-07,7.32E-06,1.11E-05,2.10E-05,3.29E-06,0.989638597,2.36E-07,0.009685084,0.000218224
6369,topic_models0,376,"We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models , except for the purely discriminative CNN model .",D. Topic ID results,D. Topic ID results,topic_models,0,18,1,1,results,0.910291383,1,results,0.000366952,1.73E-08,3.48E-06,2.43E-07,8.52E-09,1.53E-06,0.000352315,5.70E-06,2.10E-08,0.017465891,3.28E-07,0.981300961,0.000502556
6370,topic_models0,377,"We can also see that all the topic ID systems based on Bayesian SMM are consistently better than variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM .",D. Topic ID results,D. Topic ID results,topic_models,0,19,1,1,results,0.93390393,1,results,0.000637906,3.58E-08,5.20E-06,2.75E-07,1.42E-08,1.81E-06,0.000365292,7.61E-06,3.53E-08,0.027993175,3.70E-07,0.970632437,0.000355842
6371,topic_models0,378,"The advantages of the proposed Bayesian SMM are summarized as follows : ( a ) the document embeddings are Gaussian distributed which enables to train simple generative classifiers like GLC , or GLCU ; that can extended to newer classes easily , ( b ) although the Bayesian is trained in an unsupervised fashion , it does not require any early stopping mechanism to yield optimal topic ID results ; document embeddings extracted from a fully converged or model can be directly used for classification tasks without any fine - tuning .",D. Topic ID results,D. Topic ID results,topic_models,0,20,1,0,,0.136723289,0,results,0.007551392,7.20E-06,0.000692276,3.54E-06,5.77E-07,1.05E-05,0.000197397,4.14E-05,1.73E-05,0.480425698,9.23E-06,0.510232548,0.000810944
6372,topic_models0,379,E. Uncertainty in document embeddings,D. Topic ID results,,topic_models,0,21,1,0,,0.011520531,0,results,0.00058627,2.68E-07,0.000322889,4.27E-07,4.41E-08,3.17E-06,0.000146288,8.69E-06,7.29E-07,0.351535177,1.79E-05,0.645773499,0.001604634
6373,topic_models0,380,The uncertainty captured in the posterior distribution of document embeddings correlates strongly with size of the document .,D. Topic ID results,E. Uncertainty in document embeddings,topic_models,0,22,1,0,,0.00544371,0,negative,0.013803767,2.59E-06,2.17E-05,6.76E-07,9.45E-07,5.60E-06,4.97E-05,1.30E-05,1.20E-05,0.968914439,1.72E-06,0.017149205,2.46E-05
6374,topic_models0,381,The trace of the covariance matrix of the inferred posterior distributions gives us the notion of such a correlation .,D. Topic ID results,E. Uncertainty in document embeddings,topic_models,0,23,1,0,,1.33E-05,0,negative,4.35E-05,2.60E-07,4.36E-06,4.56E-08,9.15E-08,1.16E-06,1.16E-06,2.62E-06,5.01E-06,0.99987103,2.17E-07,6.92E-05,1.33E-06
6375,topic_models0,382,shows an example of uncertainty captured in the embeddings .,D. Topic ID results,E. Uncertainty in document embeddings,topic_models,0,24,1,0,,5.29E-05,0,negative,4.03E-05,4.15E-08,4.27E-06,9.79E-09,3.43E-08,6.80E-07,1.77E-06,1.10E-06,1.09E-06,0.999876489,2.35E-08,7.33E-05,9.18E-07
6376,topic_models0,383,"Here , the Bayesian SMM was trained on 20 Newsgroups with an embedding dimension of 100 .",D. Topic ID results,E. Uncertainty in document embeddings,topic_models,0,25,1,0,,0.034977587,0,negative,0.00018391,6.78E-06,2.10E-05,1.00E-06,2.43E-06,0.000176766,0.000394625,0.000584875,1.37E-05,0.998133031,5.00E-07,0.000361862,0.000119526
6377,topic_models0,384,VIII .,D. Topic ID results,,topic_models,0,26,1,0,,8.41E-05,0,negative,0.000317939,2.17E-07,1.82E-05,1.25E-06,5.19E-08,7.65E-06,1.41E-05,3.85E-05,2.65E-06,0.991388074,2.82E-07,0.00797262,0.000238528
6378,topic_models0,385,CONCLUSIONS AND FUTURE WORK,D. Topic ID results,VIII .,topic_models,0,27,1,0,,1.08E-05,0,negative,0.002804181,3.66E-06,9.66E-05,6.94E-07,1.74E-06,2.00E-05,3.55E-05,9.68E-06,7.71E-05,0.995410123,2.41E-05,0.00031193,0.001204626
6379,topic_models0,386,We have presented a generative model for learning document representations ( embeddings ) and their uncertainties .,D. Topic ID results,VIII .,topic_models,0,28,1,0,,0.000636739,0,negative,0.008757332,0.000282611,0.003950705,2.39E-05,2.44E-05,7.08E-05,0.000135465,4.50E-05,0.001454861,0.95909863,0.004094512,0.001390729,0.020671141
6380,topic_models0,387,Our proposed model achieved state - of - the - art perplexity results on the standard 20 Newsgroups and Fisher datasets .,D. Topic ID results,VIII .,topic_models,0,29,1,0,,0.022257177,0,negative,0.026647562,1.27E-05,0.000138814,1.84E-05,1.67E-05,0.000127389,0.002670234,9.63E-05,2.25E-05,0.793980314,6.31E-05,0.058633003,0.117573014
6381,topic_models0,388,"Next , we have shown that the proposed model is robust to overfitting and unlike in SMM and NVDM , it does not require any early stopping mechanism for topic ID .",D. Topic ID results,VIII .,topic_models,0,30,1,0,,0.009625042,0,negative,0.195285553,0.000105214,0.000253222,1.13E-05,2.41E-05,4.33E-05,0.000494764,5.26E-05,0.00017968,0.787334861,2.12E-05,0.013655782,0.002538473
6382,topic_models0,389,We proposed an extension to simple Gaussian linear classifier that exploits the uncertainty in document embeddings and achieves better crossentropy scores on the test data as compared to the simple GLC .,D. Topic ID results,VIII .,topic_models,0,31,1,0,,0.010986602,0,negative,0.059915854,0.000657884,0.015083728,1.52E-05,0.000106573,6.53E-05,0.00011077,3.84E-05,0.006120212,0.911355276,0.000119256,0.002041799,0.004369707
6383,topic_models0,390,"Using simple linear classifiers on the obtained document embeddings , we achieved superior classification results on Fisher speech 20 Newsgroups text corpora .",D. Topic ID results,VIII .,topic_models,0,32,1,0,,0.035975826,0,negative,0.063862224,2.79E-05,0.000214758,3.42E-06,8.42E-06,4.15E-05,0.001373846,3.37E-05,5.46E-05,0.858319159,4.17E-05,0.063890184,0.012128674
6384,topic_models0,391,We also addressed a commonly encountered problem of intractability while performing variational inference in mixed - logit models by using the re-parametrization trick .,D. Topic ID results,VIII .,topic_models,0,33,1,0,,0.000281203,0,negative,0.033502202,0.000350757,0.001263898,3.63E-05,0.000117596,5.52E-05,7.00E-05,3.92E-05,0.00090186,0.958165904,7.26E-05,0.001461484,0.003963061
6385,topic_models0,392,This idea can be translated in a straightforwardly for subspace n-gram model for learning sentence embeddings and also for learning word embeddings along with their uncertainties .,D. Topic ID results,VIII .,topic_models,0,34,1,0,,2.56E-05,0,negative,0.001085228,1.41E-05,0.000221351,1.79E-06,3.92E-06,8.20E-06,3.67E-06,8.86E-06,0.000583236,0.997625883,4.42E-06,4.06E-05,0.000398789
6386,topic_models0,393,"The proposed Bayesian SMM can be extended to have topic - specific priors for document embeddings , which enables to encode topic label uncertainty explicitly in the document embeddings .",D. Topic ID results,VIII .,topic_models,0,35,1,0,,0.001937752,0,negative,0.011862971,0.000491659,0.001549927,4.36E-06,2.05E-05,1.62E-05,2.08E-05,3.38E-05,0.013485926,0.971474508,1.96E-05,0.000308621,0.000711135
6387,topic_models0,394,"There exists other scoring mechanisms that exploit the uncertainty in embeddings , which we plan to explore in our future works .",D. Topic ID results,VIII .,topic_models,0,36,1,0,,5.72E-05,0,negative,0.001026685,6.03E-06,7.57E-05,2.49E-06,2.28E-06,3.35E-05,2.68E-05,3.71E-05,0.000101184,0.997021129,8.21E-06,9.70E-05,0.001561967
6388,topic_models0,395,APPENDIX A GRADIENTS OF LOWER BOUND,D. Topic ID results,VIII .,topic_models,0,37,1,0,,5.38E-05,0,negative,0.003146019,5.86E-06,0.000234373,7.34E-06,6.54E-06,3.17E-05,3.29E-05,2.23E-05,0.000246904,0.991001249,1.02E-05,0.000219703,0.005034905
6389,topic_models0,396,The variational distribution is diagonal with the following parametrization :,D. Topic ID results,VIII .,topic_models,0,38,1,0,,0.000147851,0,negative,0.005253865,0.000140289,0.000375269,6.33E-06,2.24E-05,9.92E-05,0.000101612,0.000439246,0.001492669,0.989733884,1.98E-06,8.88E-05,0.002244488
6390,topic_models0,397,The lower bound for a single document is :,D. Topic ID results,VIII .,topic_models,0,39,1,0,,5.73E-05,0,negative,0.001256526,4.68E-06,0.000215236,3.54E-07,1.81E-06,7.24E-06,1.33E-05,7.70E-06,0.000166781,0.997416265,3.40E-06,0.00012386,0.000782836
6391,topic_models0,398,where g( ) = ? + diag ( exp{ ?} ),D. Topic ID results,VIII .,topic_models,0,40,1,0,,9.21E-06,0,negative,0.000734796,2.26E-06,2.52E-05,4.58E-06,8.53E-06,3.22E-05,8.17E-06,2.98E-05,1.68E-05,0.998564861,5.09E-07,2.50E-05,0.000547254
6392,topic_models0,399,.,D. Topic ID results,VIII .,topic_models,0,41,1,0,,7.54E-07,0,negative,0.000100614,3.16E-07,4.96E-06,8.80E-08,2.55E-07,1.46E-06,7.31E-07,2.13E-06,1.43E-05,0.999835031,1.27E-07,4.57E-06,3.54E-05
6393,topic_models0,400,It is convenient to have the following derivatives :,D. Topic ID results,VIII .,topic_models,0,42,1,0,,1.88E-06,0,negative,0.000915209,1.57E-06,6.13E-05,6.07E-07,2.60E-06,4.17E-06,2.25E-06,3.47E-06,3.68E-05,0.998885912,1.76E-07,2.52E-05,6.08E-05
6394,topic_models0,401,Derivatives of the parameters of variational distribution :,D. Topic ID results,VIII .,topic_models,0,43,1,0,,0.000120093,0,negative,0.002330081,5.36E-06,0.000259109,1.30E-06,5.77E-06,9.79E-06,8.40E-06,1.09E-05,0.000173497,0.996748937,4.42E-07,4.58E-05,0.000400586
6395,topic_models0,402,Taking derivative of the objective function ( ) with respect to mean parameter ?,D. Topic ID results,VIII .,topic_models,0,44,1,0,,5.85E-06,0,negative,0.00071513,8.12E-07,1.38E-05,5.21E-07,1.27E-06,5.50E-06,4.46E-06,6.09E-06,1.30E-05,0.998968105,3.58E-07,4.38E-05,0.000227172
6396,topic_models0,403,and using :,D. Topic ID results,VIII .,topic_models,0,45,1,0,,5.14E-06,0,negative,0.000633794,4.64E-07,7.78E-05,2.48E-07,1.65E-06,3.21E-06,1.95E-06,1.46E-06,1.65E-05,0.999151937,1.10E-07,2.27E-05,8.82E-05
6397,topic_models0,404,Taking the derivative of objective function ( ( 44 ) ) with respect to ?,D. Topic ID results,VIII .,topic_models,0,46,1,0,,8.01E-06,0,negative,0.003084512,1.92E-06,3.84E-05,3.80E-07,1.93E-06,3.40E-06,8.69E-06,3.43E-06,2.04E-05,0.996510262,4.38E-07,0.000159336,0.000166885
6398,topic_models0,405,and using ( 47 ) :,D. Topic ID results,VIII .,topic_models,0,47,1,0,,7.20E-06,0,negative,0.001777697,1.40E-06,6.04E-05,6.16E-07,2.32E-06,4.83E-06,6.87E-06,3.38E-06,2.74E-05,0.997729259,2.68E-07,8.46E-05,0.000300981
6399,topic_models0,406,Derivatives of the model parameters :,D. Topic ID results,VIII .,topic_models,0,48,1,0,,0.000135553,0,negative,0.004209085,3.35E-06,0.000394701,3.63E-07,2.57E-06,4.59E-06,9.46E-06,3.53E-06,0.000167141,0.994907014,3.08E-07,9.85E-05,0.000199336
6400,topic_models0,407,Taking the derivative of complete objective with respect to a row t k from matrix T :,D. Topic ID results,VIII .,topic_models,0,49,1,0,,4.05E-06,0,negative,0.001878518,1.57E-06,8.17E-05,1.89E-07,1.53E-06,2.24E-06,4.60E-06,1.95E-06,3.31E-05,0.997781768,1.49E-07,7.12E-05,0.000141552
6401,topic_models0,408,APPENDIX B EM ALGORITHM FOR GLCU E - STEP :,D. Topic ID results,VIII .,topic_models,0,50,1,0,,0.000943491,0,negative,0.001033408,1.00E-05,0.000323118,6.19E-07,1.71E-06,1.34E-05,6.16E-05,2.21E-05,0.000445685,0.994927732,3.21E-06,0.00013734,0.003019956
6402,topic_models0,409,"Obtaining the posterior distribution of latent variable p ( y d | ? d , ? ) .",D. Topic ID results,VIII .,topic_models,0,51,1,0,,8.37E-06,0,negative,0.000327441,2.85E-06,4.16E-05,5.33E-07,9.34E-07,1.27E-05,1.94E-05,2.32E-05,5.02E-05,0.996703131,2.36E-06,5.30E-05,0.002762697
6403,topic_models0,410,Using the results from,,,topic_models,0,0,1,0,,0.001560194,0,negative,0.000303651,2.43E-05,3.02E-05,2.07E-07,1.32E-06,1.80E-05,4.20E-05,0.000130011,7.14E-06,0.996465663,7.77E-05,0.002899006,8.47E-07
6404,topic_models0,411,resulting in :,Using the results from,Using the results from,topic_models,0,1,1,0,,0.00049128,0,negative,0.004213305,2.32E-06,0.006606102,2.03E-07,2.09E-07,1.79E-05,1.31E-05,3.09E-05,1.31E-05,0.988029525,3.52E-06,0.001055379,1.43E-05
6405,topic_models0,412,Taking derivative with respect to shared precision matrix D and equating it to zero :,Using the results from,Using the results from,topic_models,0,2,1,0,,0.001319485,0,negative,0.007505223,1.27E-05,0.005262906,3.82E-06,1.45E-06,9.02E-05,5.66E-05,0.000154768,4.93E-05,0.984951142,2.07E-05,0.001620506,0.000270648
6406,text-classification0,1,title,,,text-classification,0,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
6407,text-classification0,2,Character - level Convolutional Networks for Text Classification,title,,text-classification,0,1,1,1,research-problem,0.998390948,1,research-problem,3.51E-08,5.91E-06,1.12E-07,6.13E-08,4.21E-08,9.62E-08,1.41E-06,1.20E-06,8.26E-07,0.001523674,0.99846631,2.50E-07,6.33E-08
6408,text-classification0,3,*,title,Character - level Convolutional Networks for Text Classification,text-classification,0,2,1,0,,1.61E-05,0,negative,5.06E-06,3.73E-06,8.05E-07,1.40E-07,8.70E-07,1.77E-05,3.20E-05,5.95E-06,9.25E-05,0.999557569,0.000280631,2.89E-06,1.56E-07
6409,text-classification0,4,abstract,,,text-classification,0,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
6410,text-classification0,5,This article offers an empirical exploration on the use of character - level convolutional networks ( ConvNets ) for text classification .,abstract,abstract,text-classification,0,1,1,0,,0.895783157,1,research-problem,4.61E-06,0.007846655,1.15E-05,2.01E-05,0.000122458,6.24E-06,9.43E-06,4.39E-05,9.60E-05,0.083729023,0.908106245,2.49E-06,1.32E-06
6411,text-classification0,6,We constructed several largescale datasets to show that character - level convolutional networks could achieve state - of - the - art or competitive results .,abstract,abstract,text-classification,0,2,1,0,,0.018033456,0,negative,5.09E-06,0.00879686,6.59E-06,1.53E-05,0.00060173,2.89E-05,7.44E-06,0.000142727,0.000188491,0.933807836,0.056394592,4.03E-06,4.15E-07
6412,text-classification0,7,"Comparisons are offered against traditional models such as bag of words , n-grams and their TFIDF variants , and deep learning models such as word - based ConvNets and recurrent neural networks .",abstract,abstract,text-classification,0,3,1,0,,0.002597172,0,negative,8.91E-06,0.039140473,1.33E-05,1.13E-05,0.000232825,4.33E-05,1.65E-05,0.000628874,0.000911862,0.853565828,0.10541714,8.81E-06,9.00E-07
6413,text-classification0,8,There are also related works that use character - level features for language processing .,abstract,abstract,text-classification,0,4,1,0,,0.001230843,0,research-problem,9.30E-08,3.11E-05,5.70E-08,6.49E-07,6.42E-07,2.22E-06,1.03E-06,1.58E-05,4.14E-06,0.172903078,0.827041006,9.13E-08,6.60E-08
6414,text-classification0,9,"These include using character - level n-grams with linear classifiers [ 15 ] , and incorporating character - level features to ConvNets [ 28 ] [ 29 ] .",abstract,abstract,text-classification,0,5,1,0,,0.010748509,0,negative,6.03E-06,0.001396535,2.92E-06,4.15E-05,5.35E-05,4.15E-05,7.89E-06,0.000145134,0.00015513,0.670712428,0.327435586,9.68E-07,9.01E-07
6415,text-classification0,10,"In particular , these ConvNet approaches use words as a basis , in which character - level features extracted at word [ 28 ] or word n-gram [ 29 ] level form a distributed representation .",abstract,abstract,text-classification,0,6,1,0,,0.035279685,0,research-problem,5.39E-07,0.000432565,1.03E-06,6.18E-06,6.46E-06,1.34E-05,2.42E-06,6.37E-05,8.20E-05,0.310893226,0.688498093,1.89E-07,2.96E-07
6416,text-classification0,11,Improvements for part - of - speech tagging and information retrieval were observed .,abstract,abstract,text-classification,0,7,1,0,,0.016377954,0,negative,5.12E-05,0.00030412,1.47E-06,7.74E-05,0.00015688,2.54E-05,2.42E-05,9.07E-05,2.28E-05,0.805841089,0.193376218,2.63E-05,2.23E-06
6417,text-classification0,12,Introduction,,,text-classification,0,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
6418,text-classification0,13,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",Introduction,Introduction,text-classification,0,1,1,1,research-problem,0.788437412,1,research-problem,7.81E-07,9.64E-05,3.33E-07,1.35E-05,2.23E-05,4.69E-06,1.39E-05,3.89E-06,1.75E-05,0.027772442,0.972051075,9.90E-07,2.19E-06
6419,text-classification0,14,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,Introduction,Introduction,text-classification,0,2,1,0,,0.726852324,1,research-problem,4.42E-07,0.000117771,1.45E-07,3.32E-06,5.00E-06,5.42E-06,6.42E-06,7.05E-06,4.60E-05,0.060243877,0.939563108,5.99E-07,8.81E-07
6420,text-classification0,15,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",Introduction,Introduction,text-classification,0,3,1,0,,0.22732891,0,research-problem,1.18E-06,0.000188039,3.80E-07,9.52E-06,2.79E-05,1.03E-05,1.42E-05,8.74E-06,3.56E-05,0.097605701,0.902095462,1.26E-06,1.83E-06
6421,text-classification0,16,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",Introduction,Introduction,text-classification,0,4,1,0,,0.017914559,0,research-problem,2.36E-06,0.000575126,1.18E-06,6.23E-06,1.36E-05,2.73E-05,1.63E-05,2.98E-05,0.000714052,0.219454941,0.779155056,2.19E-06,1.99E-06
6422,text-classification0,17,"In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",Introduction,Introduction,text-classification,0,5,1,0,,0.01577926,0,research-problem,5.25E-06,0.002894738,7.59E-06,2.42E-05,5.63E-05,0.000178706,6.03E-05,0.00015409,0.00889666,0.3859773,0.601734019,4.11E-06,6.75E-06
6423,text-classification0,18,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",Introduction,Introduction,text-classification,0,6,1,1,model,0.910558874,1,approach,0.000146257,0.542051941,0.000275829,2.99E-05,0.002848083,7.44E-05,0.000181408,0.000128459,0.224915565,0.100595466,0.128643346,9.23E-05,1.70E-05
6424,text-classification0,19,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,Introduction,Introduction,text-classification,0,7,1,1,model,0.056839452,0,negative,1.16E-05,0.028420362,1.21E-05,3.05E-05,0.006079852,0.000143756,4.20E-05,0.000142311,0.005044353,0.938109307,0.021952327,8.58E-06,2.98E-06
6425,text-classification0,20,"Historically we know that ConvNets usually require large - scale datasets to work , therefore we also build several of them .",Introduction,Introduction,text-classification,0,8,1,0,,0.371392134,0,negative,6.43E-06,0.004892884,4.92E-06,3.56E-05,0.000172748,0.000236233,5.94E-05,0.000255874,0.00347012,0.691694934,0.299157643,6.15E-06,7.06E-06
6426,text-classification0,21,An extensive set of comparisons is offered with traditional models and other deep learning models .,Introduction,Introduction,text-classification,0,9,1,0,,0.008324426,0,negative,2.99E-05,0.042055677,1.38E-05,4.04E-05,0.003133417,0.000159555,4.80E-05,0.000215102,0.012331974,0.932331969,0.009619339,1.75E-05,3.26E-06
6427,text-classification0,22,Applying convolutional networks to text classification or natural language processing at large was explored in literature .,Introduction,Introduction,text-classification,0,10,1,0,,0.409038435,0,research-problem,1.10E-06,0.000455329,4.83E-07,1.35E-06,4.61E-06,9.25E-06,1.47E-05,2.04E-05,0.000255026,0.121541285,0.87769324,1.94E-06,1.25E-06
6428,text-classification0,23,"It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words , without any knowledge on the syntactic or semantic structures of a language .",Introduction,Introduction,text-classification,0,11,1,0,,0.403418545,0,research-problem,3.76E-06,0.003086999,2.29E-06,1.85E-06,1.29E-05,2.90E-05,2.82E-05,7.33E-05,0.004712596,0.34653242,0.645508634,6.07E-06,2.02E-06
6429,text-classification0,24,These approaches have been proven to be competitive to traditional models .,Introduction,Introduction,text-classification,0,12,1,0,,0.030028224,0,negative,1.49E-05,0.002054608,2.67E-06,7.52E-06,5.80E-05,6.80E-05,5.74E-05,9.47E-05,0.002137133,0.864899795,0.13058725,1.51E-05,3.02E-06
6430,text-classification0,25,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,Introduction,Introduction,text-classification,0,13,1,0,,0.019080603,0,negative,6.11E-06,0.000877847,1.62E-06,7.93E-06,7.98E-05,4.09E-05,3.08E-05,4.16E-05,0.000323805,0.787185888,0.211395729,5.91E-06,2.13E-06
6431,text-classification0,26,"This simplification of engineering could be crucial for a single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",Introduction,Introduction,text-classification,0,14,1,0,,0.057811045,0,negative,0.000103879,0.006257231,6.62E-06,5.09E-05,0.00104774,0.000116038,2.62E-05,8.89E-05,0.006138353,0.981868315,0.004278268,1.54E-05,2.23E-06
6432,text-classification0,27,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,Introduction,Introduction,text-classification,0,15,1,0,,0.409337366,0,negative,2.91E-05,0.005281397,7.87E-06,1.55E-05,0.000268167,8.13E-05,5.22E-05,6.95E-05,0.00380922,0.90774102,0.082620653,2.04E-05,3.57E-06
6433,text-classification0,28,Character - level Convolutional Networks,Introduction,Introduction,text-classification,0,16,1,0,,0.127392062,0,model,1.72E-05,0.01962875,9.88E-05,8.97E-07,1.92E-05,4.86E-05,0.000235569,9.30E-05,0.62075252,0.141863039,0.217183114,4.72E-05,1.21E-05
6434,text-classification0,29,"In this section , we introduce the design of character - level ConvNets for text classification .",Introduction,Introduction,text-classification,0,17,1,0,,0.718782635,1,approach,9.13E-05,0.72115641,0.000143013,2.57E-05,0.003493608,0.000101772,0.000224696,0.000210879,0.16172227,0.090155792,0.022588557,6.87E-05,1.74E-05
6435,text-classification0,30,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .",Introduction,Introduction,text-classification,0,18,1,0,,0.953148224,1,model,1.65E-05,0.144594461,4.60E-05,1.69E-06,0.000288415,2.01E-05,1.06E-05,3.96E-05,0.844387074,0.010395559,0.000196066,2.74E-06,1.13E-06
6436,text-classification0,31,Key Modules,Introduction,,text-classification,0,19,1,0,,0.018023269,0,negative,6.94E-05,0.02196737,6.33E-05,5.13E-05,0.001750984,0.000439563,0.000171011,0.000347074,0.145888221,0.827173856,0.002040117,2.70E-05,1.09E-05
6437,text-classification0,32,"The main component is the temporal convolutional module , which simply computes a 1 - D convolution .",Introduction,Key Modules,text-classification,0,20,1,0,,0.577290812,1,model,0.000301711,0.056508574,0.000891685,3.99E-06,0.00047364,4.89E-05,7.83E-06,9.01E-05,0.637065508,0.304289181,0.000307749,8.14E-06,2.98E-06
6438,text-classification0,33,Suppose we have a discrete input function g ( x ) ?,Introduction,Key Modules,text-classification,0,21,1,0,,0.000152398,0,negative,2.34E-06,0.000259143,5.31E-07,1.60E-07,6.22E-06,5.68E-06,2.62E-07,2.57E-05,0.000997578,0.998559772,0.000142037,5.62E-07,4.84E-08
6439,text-classification0,34,"[ 1 , l ] ?",Introduction,Key Modules,text-classification,0,22,1,0,,0.000596688,0,negative,3.14E-06,5.39E-05,8.43E-07,2.29E-07,1.20E-05,6.27E-06,2.95E-07,1.17E-05,0.000113632,0.999775187,2.21E-05,6.51E-07,3.67E-08
6440,text-classification0,35,Rand a discrete kernel function,Introduction,Key Modules,text-classification,0,23,1,0,,0.002988804,0,negative,1.24E-05,0.005515832,1.24E-05,2.05E-06,3.12E-05,0.000241285,7.08E-06,0.002098925,0.031075919,0.960735643,0.000263984,1.95E-06,1.36E-06
6441,text-classification0,36,where c = k ? d + 1 is an offset constant .,Introduction,Key Modules,text-classification,0,24,1,0,,0.000136918,0,negative,2.78E-05,0.001050356,3.52E-06,1.02E-06,3.71E-05,2.92E-05,1.42E-06,0.000137701,0.002909019,0.995723898,7.62E-05,2.52E-06,2.82E-07
6442,text-classification0,37,"Just as in traditional convolutional networks in vision , the module is parameterized by a set of such kernel functions f ij ( x ) ( i = 1 , 2 , . . . , m and j = 1 , 2 , . . . , n) which we call weights , on a set of inputs g i ( x ) and outputs h j ( y ) .",Introduction,Key Modules,text-classification,0,25,1,0,,0.000195803,0,negative,1.02E-05,0.011119851,1.46E-05,1.15E-06,5.83E-05,1.76E-05,8.51E-07,0.000107357,0.10579785,0.882790074,8.10E-05,8.18E-07,3.07E-07
6443,text-classification0,38,"We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .",Introduction,Key Modules,text-classification,0,26,1,0,,9.78E-05,0,negative,2.59E-06,0.000572978,1.27E-06,2.42E-07,1.76E-05,8.69E-06,2.63E-07,3.95E-05,0.002420715,0.996919704,1.59E-05,3.92E-07,5.60E-08
6444,text-classification0,39,The outputs h j ( y ) is obtained by a sum over i of the convolutions between g i ( x ) and f ij ( x ) .,Introduction,Key Modules,text-classification,0,27,1,0,,0.000201033,0,negative,9.90E-06,0.001003904,8.34E-06,1.12E-07,1.70E-05,5.17E-06,3.11E-07,1.65E-05,0.009071011,0.98985413,1.28E-05,8.07E-07,4.73E-08
6445,text-classification0,40,One key module that helped us to train deeper models is temporal max - pooling .,Introduction,Key Modules,text-classification,0,28,1,0,,0.047585868,0,negative,0.000320478,0.000765826,5.41E-05,1.13E-05,0.00066116,4.70E-05,2.95E-06,1.65E-05,0.002007711,0.996030996,7.38E-05,7.14E-06,9.83E-07
6446,text-classification0,41,It is the 1 - D version of the max - pooling module used in computer vision .,Introduction,Key Modules,text-classification,0,29,1,0,,0.006245901,0,negative,0.000129059,0.028192914,0.005470662,4.65E-06,0.000775177,0.000114982,2.12E-05,0.00012394,0.198086027,0.766800619,0.000263132,1.18E-05,5.93E-06
6447,text-classification0,42,Given a discrete input function g ( x ) ?,Introduction,Key Modules,text-classification,0,30,1,0,,6.58E-05,0,negative,3.13E-06,0.000316279,1.01E-06,1.30E-07,8.90E-06,5.09E-06,3.54E-07,2.76E-05,0.00109266,0.998502949,4.10E-05,8.80E-07,5.37E-08
6448,text-classification0,43,"[ 1 , l ] ?",Introduction,Key Modules,text-classification,0,31,1,0,,0.000402429,0,negative,2.68E-06,4.83E-05,9.34E-07,1.97E-07,1.51E-05,6.66E-06,3.54E-07,1.22E-05,0.00010173,0.999804185,6.82E-06,6.71E-07,4.60E-08
6449,text-classification0,44,"R , the max - pooling function h ( y ) ?",Introduction,Key Modules,text-classification,0,32,1,0,,0.000493365,0,negative,1.46E-05,0.000889205,2.89E-06,3.04E-06,0.000104563,0.000127811,5.28E-06,0.000424729,0.000808312,0.997560686,5.49E-05,3.08E-06,9.08E-07
6450,text-classification0,45,"[ 1 , ( l ? k) / d + 1 ] ?",Introduction,Key Modules,text-classification,0,33,1,0,,7.48E-05,0,negative,9.89E-06,8.12E-05,1.24E-06,5.34E-07,2.68E-05,1.48E-05,9.19E-07,2.68E-05,0.000137059,0.999684917,1.41E-05,1.58E-06,1.44E-07
6451,text-classification0,46,R of g ( x ) is defined as,Introduction,Key Modules,text-classification,0,34,1,0,,0.000145076,0,negative,1.82E-05,0.000354434,9.50E-06,2.28E-07,3.32E-05,3.94E-06,5.96E-07,9.28E-06,0.000783914,0.998751127,3.23E-05,3.17E-06,9.04E-08
6452,text-classification0,47,where c = k ? d + 1 is an offset constant .,Introduction,Key Modules,text-classification,0,35,1,0,,8.71E-05,0,negative,2.28E-05,0.000876867,3.61E-06,8.35E-07,4.90E-05,3.05E-05,1.87E-06,0.00014011,0.002351057,0.996502567,1.81E-05,2.38E-06,3.54E-07
6453,text-classification0,48,"This very pooling module enabled us to train ConvNets deeper than 6 layers , where all others fail .",Introduction,Key Modules,text-classification,0,36,1,0,,0.191875349,0,negative,0.000954693,0.017387595,0.000135333,0.000319486,0.008334493,0.001937193,8.18E-05,0.001657426,0.033370405,0.935741556,3.26E-05,1.80E-05,2.94E-05
6454,text-classification0,49,The analysis by might shed some light on this .,Introduction,Key Modules,text-classification,0,37,1,0,,3.09E-05,0,negative,9.54E-06,5.30E-05,3.94E-07,1.02E-06,2.53E-05,1.34E-05,3.58E-07,1.53E-05,0.000141297,0.999736524,3.30E-06,4.25E-07,9.30E-08
6455,text-classification0,50,"The non-linearity used in our model is the rectifier or thresholding function h ( x ) = max {0 , x} , which makes our convolutional layers similar to rectified linear units ( ReLUs ) .",Introduction,Key Modules,text-classification,0,38,1,0,,0.006535641,0,negative,3.10E-05,0.012648665,2.19E-05,3.94E-05,0.000513052,0.000942473,2.27E-05,0.005361038,0.028859636,0.951529455,2.16E-05,2.52E-06,6.63E-06
6456,text-classification0,51,"The algorithm used is stochastic gradient descent ( SGD ) with a minibatch of size 128 , using momentum 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times .",Introduction,Key Modules,text-classification,0,39,1,0,,0.584488493,1,negative,0.000115231,0.026772403,0.000123214,0.005154669,0.017255257,0.097441192,0.001255548,0.068643753,0.005630222,0.777164331,8.44E-05,1.48E-05,0.000344963
6457,text-classification0,52,Each epoch takes a fixed number of random training samples uniformly sampled across classes .,Introduction,Key Modules,text-classification,0,40,1,0,,0.014272846,0,negative,0.000167963,0.096076007,6.85E-05,7.11E-06,0.000824494,0.000364981,5.72E-05,0.004265186,0.16335775,0.734758231,3.53E-05,8.94E-06,8.37E-06
6458,text-classification0,53,This number will later be detailed for each dataset sparately .,Introduction,Key Modules,text-classification,0,41,1,0,,2.72E-05,0,negative,5.81E-06,7.38E-05,6.90E-07,3.56E-07,6.65E-05,6.74E-06,2.46E-07,1.12E-05,0.000165333,0.999668483,3.40E-07,4.64E-07,3.03E-08
6459,text-classification0,54,The implementation is done using Torch 7 .,Introduction,Key Modules,text-classification,0,42,1,0,,0.239856728,0,negative,0.000104379,0.00257181,0.000118144,0.05382105,0.079814161,0.071224417,0.000359696,0.002649976,0.000683775,0.788453226,2.62E-05,9.87E-06,0.000163279
6460,text-classification0,55,Character quantization,Introduction,,text-classification,0,43,1,0,,0.06111135,0,negative,0.000126808,0.045394437,0.000272046,5.79E-05,0.002853577,0.002306114,0.109703252,0.002444165,0.01339513,0.797993668,0.018697231,0.00539398,0.001361677
6461,text-classification0,56,Our models accept a sequence of encoded characters as input .,Introduction,Character quantization,text-classification,0,44,1,0,,0.001396953,0,negative,8.92E-06,8.08E-06,4.72E-05,6.16E-08,5.83E-07,4.64E-06,6.43E-06,1.10E-05,0.001359267,0.998539716,3.98E-07,9.67E-06,3.94E-06
6462,text-classification0,57,"The encoding is done by prescribing an alphabet of size m for the input language , and then quantize each character using 1 - of - m encoding ( or "" one - hot "" encoding ) .",Introduction,Character quantization,text-classification,0,45,1,0,,0.016386181,0,negative,4.75E-05,5.12E-05,0.000197598,1.46E-07,2.14E-06,5.73E-06,1.54E-05,1.29E-05,0.002377461,0.997258702,5.30E-07,2.47E-05,6.01E-06
6463,text-classification0,58,"Then , the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l 0 .",Introduction,Character quantization,text-classification,0,46,1,0,,0.000308507,0,negative,1.07E-05,1.73E-06,6.05E-06,1.22E-08,2.13E-07,1.03E-06,2.57E-06,3.24E-06,0.000111484,0.999851825,4.00E-08,1.07E-05,4.41E-07
6464,text-classification0,59,"Any character exceeding length l 0 is ignored , and any characters thatare not in the alphabet including blank characters are quantized as all - zero vectors .",Introduction,Character quantization,text-classification,0,47,1,0,,0.000216491,0,negative,3.03E-05,2.66E-06,1.67E-05,1.12E-08,2.94E-07,2.49E-06,7.24E-06,4.76E-06,7.75E-05,0.999839649,2.24E-08,1.79E-05,4.35E-07
6465,text-classification0,60,"The character quantization order is backward so that the latest reading on characters is always placed near the begin of the output , making it easy for fully connected layers to associate weights with the latest reading .",Introduction,Character quantization,text-classification,0,48,1,0,,0.004880605,0,negative,9.89E-05,6.53E-05,8.12E-05,1.17E-06,5.03E-06,0.000133688,0.000199864,0.000440687,0.001480469,0.997416468,3.38E-07,3.59E-05,4.11E-05
6466,text-classification0,61,Later we also compare with models that use a different alphabet in which we distinguish between upper-case and lower - case letters .,Introduction,Character quantization,text-classification,0,49,1,0,,0.000332789,0,negative,1.04E-05,1.00E-06,1.66E-05,3.54E-09,1.05E-07,5.95E-07,4.79E-06,7.01E-07,1.18E-05,0.999901205,3.83E-08,5.25E-05,2.15E-07
6467,text-classification0,62,Model Design,,,text-classification,0,0,1,0,,0.026782024,0,negative,0.000381913,0.000215127,3.13E-05,0.00021625,3.41E-05,0.000793352,0.00057269,0.002435985,0.000307328,0.989058205,0.005315204,0.000569231,6.93E-05
6468,text-classification0,63,We designed 2 ConvNets - one large and one small .,Model Design,Model Design,text-classification,0,1,1,0,,0.040766879,0,negative,0.000953482,0.00534086,0.001578357,0.000152427,0.001402246,0.107079663,0.008406154,0.028385923,0.011113699,0.83514672,0.000270227,5.32E-05,0.000117038
6469,text-classification0,64,They are both 9 layers deep with 6 convolutional layers and 3 fully - connected layers .,Model Design,Model Design,text-classification,0,2,1,0,,0.737831465,1,negative,0.001404308,0.004300294,0.001594168,0.004159819,0.002435628,0.441257653,0.020122252,0.037774996,0.008328333,0.477559174,0.000496537,5.93E-05,0.000507566
6470,text-classification0,65,gives an illustration .,Model Design,Model Design,text-classification,0,3,1,0,,3.98E-05,0,negative,2.47E-05,5.86E-06,3.35E-06,1.05E-07,1.86E-06,3.00E-05,4.80E-06,1.61E-05,2.15E-05,0.999875617,1.41E-05,1.86E-06,5.66E-08
6471,text-classification0,66,Some Text,Model Design,,text-classification,0,4,1,0,,1.24E-05,0,negative,3.53E-05,7.58E-06,1.33E-06,6.07E-06,7.26E-06,0.000359239,2.48E-05,7.23E-05,3.49E-05,0.999364637,8.39E-05,2.03E-06,7.22E-07
6472,text-classification0,67,Convolutions,Model Design,,text-classification,0,5,1,0,,0.006796777,0,negative,0.000682193,0.000142802,0.001009582,9.04E-07,5.27E-06,0.000453756,0.000982124,0.000145057,0.002039656,0.989964237,0.004266457,0.0002998,8.16E-06
6473,text-classification0,68,Max - pooling Length Feature Quantization ...,Model Design,Convolutions,text-classification,0,6,1,0,,0.000830158,0,negative,0.000249607,1.35E-06,0.000687152,6.77E-07,8.63E-08,2.13E-05,2.43E-05,5.45E-05,5.58E-05,0.998626952,7.13E-06,0.000261613,9.53E-06
6474,text-classification0,69,Conv. and Pool. layers,Model Design,,text-classification,0,7,1,0,,0.642207528,1,negative,0.003346154,0.00059596,0.00161751,0.000282011,0.000489464,0.013479704,0.001299136,0.001696695,0.007897116,0.968946263,0.000203629,6.83E-05,7.81E-05
6475,text-classification0,70,Fully - connected :,Model Design,Conv. and Pool. layers,text-classification,0,8,1,0,,0.816419608,1,negative,0.000200802,0.005115973,0.307424325,2.08E-06,2.01E-05,0.001107137,0.00073224,0.002774647,0.202937601,0.47832122,0.000951089,0.000370644,4.21E-05
6476,text-classification0,71,Illustration of our model,Model Design,Conv. and Pool. layers,text-classification,0,9,1,0,,8.19E-05,0,negative,4.80E-05,0.000603777,0.000189686,2.91E-05,2.80E-05,0.000709634,0.000105927,0.00285219,0.045882896,0.948144928,0.00124117,7.73E-05,8.73E-05
6477,text-classification0,72,"The input have number of features equal to 70 due to our character quantization method , and the input feature length is 1014 .",Model Design,Conv. and Pool. layers,text-classification,0,10,1,0,,0.133435803,0,negative,1.34E-05,0.001016424,4.30E-05,1.01E-05,2.09E-05,0.035605988,0.000705439,0.466047121,0.003616588,0.492813388,4.55E-05,2.42E-05,3.81E-05
6478,text-classification0,73,It seems that 1014 characters could already capture most of the texts of interest .,Model Design,Conv. and Pool. layers,text-classification,0,11,1,0,,0.001454711,0,negative,0.000174117,0.000187963,3.81E-05,0.000119287,0.00020151,0.003602217,0.000273323,0.006529207,0.002637049,0.985722801,0.000194248,0.000229867,9.03E-05
6479,text-classification0,74,We also insert 2 dropout modules in between the 3 fully - connected layers to regularize .,Model Design,Conv. and Pool. layers,text-classification,0,12,1,0,,0.491046645,0,hyperparameters,0.001194937,0.025576014,0.00111013,0.000395038,0.000435411,0.027947476,0.002293894,0.387121598,0.231152102,0.321898568,0.00013643,0.000120661,0.000617741
6480,text-classification0,75,They have dropout probability of 0.5 .,Model Design,Conv. and Pool. layers,text-classification,0,13,1,0,,0.037805443,0,negative,0.000104671,0.001877204,0.000335813,6.99E-05,9.97E-05,0.031190162,0.001051999,0.286346468,0.016421994,0.662337974,2.75E-05,3.58E-05,0.000100868
6481,text-classification0,76,"lists the configurations for convolutional layers , and table 2 lists the configurations for fully - connected ( linear ) layers .",Model Design,Conv. and Pool. layers,text-classification,0,14,1,0,,0.00285109,0,negative,6.11E-05,0.000135157,7.12E-05,1.29E-06,2.06E-05,0.00042755,5.00E-05,0.00246413,0.000620516,0.996019224,3.47E-06,0.000124253,1.49E-06
6482,text-classification0,77,We initialize the weights using a Gaussian distribution .,Model Design,Conv. and Pool. layers,text-classification,0,15,1,0,,0.195928702,0,hyperparameters,1.55E-05,0.001462318,3.54E-05,4.11E-06,6.20E-06,0.023319483,0.000348795,0.481162757,0.018868138,0.47471174,3.14E-05,9.65E-06,2.45E-05
6483,text-classification0,78,"The mean and standard deviation used for initializing the large model is ( 0 , 0.02 ) and small model ( 0 , 0.05 ) . :",Model Design,Conv. and Pool. layers,text-classification,0,16,1,0,,0.001179092,0,negative,3.05E-05,0.000262764,3.13E-05,3.10E-06,7.29E-06,0.017465697,0.000309556,0.175044378,0.002270555,0.804525203,8.91E-06,2.74E-05,1.33E-05
6484,text-classification0,79,Fully - connected layers used in our experiments .,Model Design,Conv. and Pool. layers,text-classification,0,17,1,0,,0.013733327,0,negative,3.58E-05,0.000976395,0.000150541,2.07E-05,1.92E-05,0.038023641,0.001129094,0.327784913,0.016941883,0.614696828,7.39E-05,4.48E-05,0.000102466
6485,text-classification0,80,The number of output units for the last layer is determined by the problem .,Model Design,Conv. and Pool. layers,text-classification,0,18,1,0,,7.28E-05,0,negative,1.95E-05,0.00053443,1.99E-05,3.65E-06,7.11E-06,0.001851592,5.36E-05,0.034586412,0.007170632,0.955700817,2.76E-05,1.65E-05,8.23E-06
6486,text-classification0,81,"For example , for a 10 - class classification problem it will be 10 .",Model Design,Conv. and Pool. layers,text-classification,0,19,1,0,,1.61E-05,0,negative,4.15E-06,7.73E-05,1.07E-05,3.96E-07,3.23E-06,0.00132681,3.15E-05,0.011424027,0.001044038,0.986060342,7.42E-06,7.49E-06,2.63E-06
6487,text-classification0,82,Depends on the problem,Model Design,,text-classification,0,20,1,0,,2.79E-05,0,negative,5.80E-05,3.89E-05,6.62E-06,6.60E-06,3.06E-05,0.000268387,3.31E-05,0.00010067,0.000132014,0.999297184,1.58E-05,1.05E-05,1.46E-06
6488,text-classification0,83,"For different problems the input lengths maybe different ( for example in our case l 0 = 1014 ) , and so are the frame lengths .",Model Design,Depends on the problem,text-classification,0,21,1,0,,2.23E-05,0,negative,1.15E-05,6.40E-06,5.48E-07,1.27E-07,1.05E-06,0.000276135,6.72E-06,0.000332609,5.79E-06,0.999335548,3.35E-06,1.99E-05,2.87E-07
6489,text-classification0,84,"From our model design , it is easy to know that given input length l 0 , the output frame length after the last convolutional layer ( but before any of the fully - connected layers ) isl 6 = ( l 0 ? 96 ) / 27 .",Model Design,Depends on the problem,text-classification,0,22,1,0,,2.34E-05,0,negative,9.25E-05,3.52E-05,3.55E-06,4.80E-07,1.23E-05,0.000634847,2.62E-05,0.000774047,1.98E-05,0.998291127,3.66E-06,0.00010517,1.05E-06
6490,text-classification0,85,This number multiplied with the frame size at layer 6 will give the input dimension the first fully - connected layer accepts .,Model Design,Depends on the problem,text-classification,0,23,1,0,,3.98E-05,0,negative,0.000230046,0.00018491,2.52E-05,1.24E-06,3.01E-05,0.005137581,0.000133512,0.005921784,0.000349533,0.987918066,2.79E-06,5.67E-05,8.49E-06
6491,text-classification0,86,Layer Output Units Large Output Units,Model Design,,text-classification,0,24,1,0,,0.032815871,0,negative,0.0010973,0.000836952,0.000844621,4.85E-05,0.000264702,0.006335637,0.001719314,0.002699274,0.027509399,0.958354403,8.93E-05,7.45E-05,0.000126198
6492,text-classification0,87,Data Augmentation using Thesaurus,Model Design,,text-classification,0,25,1,0,,0.152165302,0,negative,0.001467792,0.001577691,0.000564662,1.32E-05,6.64E-05,0.000881208,0.034343444,0.000694901,0.001455422,0.855626866,0.095373863,0.007304283,0.000630279
6493,text-classification0,88,Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models .,Model Design,Data Augmentation using Thesaurus,text-classification,0,26,1,0,,8.38E-05,0,negative,1.32E-05,8.81E-07,7.10E-07,2.24E-07,8.62E-08,1.40E-05,6.89E-05,2.19E-05,3.75E-06,0.999757676,3.63E-05,7.96E-05,2.71E-06
6494,text-classification0,89,These techniques usually work well when we could find appropriate invariance properties that the model should possess .,Model Design,Data Augmentation using Thesaurus,text-classification,0,27,1,0,,2.09E-05,0,negative,1.91E-05,3.21E-07,5.23E-07,2.96E-07,1.11E-07,1.57E-05,3.10E-05,1.03E-05,2.64E-06,0.999859607,2.51E-06,5.71E-05,8.20E-07
6495,text-classification0,90,"In terms of texts , it is not reasonable to augment the data using signal transformations as done in image or speech recognition , because the exact order of characters may form rigorous syntactic and semantic meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,28,1,0,,5.49E-07,0,negative,4.32E-06,5.85E-08,3.63E-07,9.54E-09,5.93E-08,9.52E-07,5.96E-06,5.21E-07,4.32E-07,0.999936078,5.20E-07,5.06E-05,9.07E-08
6496,text-classification0,91,"Therefore , the best way to do data augmentation would have been using human rephrases of sentences , but this is unrealistic and expensive due the large volume of samples in our datasets .",Model Design,Data Augmentation using Thesaurus,text-classification,0,29,1,0,,7.33E-07,0,negative,9.89E-06,8.09E-08,2.69E-07,1.61E-08,6.56E-08,2.51E-06,8.04E-06,1.92E-06,4.78E-07,0.999945588,1.88E-07,3.08E-05,1.09E-07
6497,text-classification0,92,"As a result , the most natural choice in data augmentation for us is to replace words or phrases with their synonyms .",Model Design,Data Augmentation using Thesaurus,text-classification,0,30,1,0,,3.41E-06,0,negative,6.37E-06,4.58E-07,5.52E-07,1.75E-07,1.22E-07,4.93E-06,1.79E-05,4.55E-06,1.99E-06,0.999914513,1.62E-05,3.12E-05,1.13E-06
6498,text-classification0,93,"We experimented data augmentation by using an English thesaurus , which is obtained from the mytheas component used in LibreOffice 1 project .",Model Design,Data Augmentation using Thesaurus,text-classification,0,31,1,0,,4.99E-05,0,negative,0.000150048,1.64E-05,2.40E-05,1.80E-07,9.88E-06,2.40E-05,0.000286409,2.09E-05,1.23E-05,0.999102905,1.30E-07,0.00035099,1.84E-06
6499,text-classification0,94,"That thesaurus in turn was obtained from Word - Net , where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,32,1,0,,1.69E-06,0,negative,8.02E-06,7.14E-07,1.00E-05,2.82E-07,8.01E-06,2.61E-05,2.41E-05,6.00E-06,6.16E-06,0.999901067,2.48E-08,8.82E-06,6.84E-07
6500,text-classification0,95,"To decide on how many words to replace , we extract all replaceable words from the given text and randomly chooser of them to be replaced .",Model Design,Data Augmentation using Thesaurus,text-classification,0,33,1,0,,7.21E-06,0,negative,4.31E-05,2.07E-05,1.32E-05,1.42E-08,6.62E-07,2.78E-06,2.35E-05,1.06E-05,0.000121924,0.999733702,4.77E-08,2.96E-05,2.00E-07
6501,text-classification0,96,The probability of number r is determined by a geometric distribution with parameter pin which P [ r ] ? pr .,Model Design,Data Augmentation using Thesaurus,text-classification,0,34,1,0,,4.01E-07,0,negative,1.63E-06,2.02E-07,3.97E-07,3.62E-09,2.16E-08,1.64E-06,3.52E-06,4.71E-06,8.28E-06,0.999976913,1.61E-08,2.63E-06,3.75E-08
6502,text-classification0,97,The index s of the synonym chosen given a word is also determined by a another geometric distribution in which P [ s ] ? q s .,Model Design,Data Augmentation using Thesaurus,text-classification,0,35,1,0,,1.95E-07,0,negative,3.37E-06,1.81E-07,5.50E-07,2.67E-09,2.37E-08,6.20E-07,1.98E-06,1.44E-06,6.21E-06,0.999980897,1.15E-08,4.69E-06,2.40E-08
6503,text-classification0,98,"This way , the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,36,1,0,,9.79E-06,0,negative,0.000272394,7.77E-07,3.89E-06,6.01E-09,9.79E-08,7.01E-07,7.51E-06,1.49E-06,2.27E-05,0.999613592,1.98E-08,7.68E-05,7.58E-08
6504,text-classification0,99,We will report the results using this new data augmentation technique with p = 0.5 and q = 0.5 .,Model Design,Data Augmentation using Thesaurus,text-classification,0,37,1,0,,8.76E-06,0,negative,2.80E-05,9.44E-08,4.51E-07,1.38E-09,2.86E-08,7.20E-07,3.03E-05,1.13E-06,3.85E-07,0.999757595,3.33E-09,0.000181246,4.56E-08
6505,text-classification0,100,Comparison Models,,,text-classification,0,0,1,0,,0.003668464,0,negative,4.40E-05,6.03E-05,0.000129932,3.12E-07,4.63E-07,0.000188924,0.000253909,0.001850929,4.60E-05,0.995139376,0.000842548,0.001440338,2.96E-06
6506,text-classification0,101,"To offer fair comparisons to competitive models , we conducted a series of experiments with both traditional and deep learning methods .",Comparison Models,Comparison Models,text-classification,0,1,1,0,,0.067890901,0,negative,0.00021644,9.09E-06,0.251633579,1.96E-06,1.51E-06,8.07E-05,0.000655505,0.000749272,3.50E-06,0.74475057,2.99E-06,0.001880066,1.48E-05
6507,text-classification0,102,"We tried our best to choose models that can provide comparable and competitive results , and the results are reported faithfully without any model selection .",Comparison Models,Comparison Models,text-classification,0,2,1,0,,0.411302364,0,negative,0.000299228,3.65E-06,0.0978873,3.84E-06,6.10E-07,0.000631209,0.001405758,0.004348468,4.04E-06,0.888406982,8.83E-06,0.006938718,6.14E-05
6508,text-classification0,103,Traditional Methods,,,text-classification,0,0,1,0,,0.050164264,0,negative,3.98E-05,0.000117706,7.99E-05,1.66E-06,2.48E-06,0.00019686,0.000410462,0.001319879,3.22E-05,0.986850895,0.010291466,0.000646611,1.01E-05
6509,text-classification0,104,We refer to traditional methods as those that using a hand - crafted feature extractor and a linear classifier .,Traditional Methods,Traditional Methods,text-classification,0,1,1,0,,0.001290689,0,negative,3.98E-05,3.79E-05,0.00643441,3.72E-06,2.60E-06,0.000347356,0.000369773,0.001114333,3.23E-05,0.991247064,0.000250595,0.000110421,9.65E-06
6510,text-classification0,105,The classifier used is a multinomial logistic regression in all these models .,Traditional Methods,Traditional Methods,text-classification,0,2,1,0,,0.230327474,0,negative,0.000131929,0.000301694,0.045215794,9.87E-06,5.26E-06,0.024377884,0.006597177,0.146360035,0.000591957,0.776214838,3.95E-05,9.35E-05,6.06E-05
6511,text-classification0,106,Bag - of - words and its TFIDF .,Traditional Methods,Traditional Methods,text-classification,0,3,1,0,,0.008008105,0,baselines,8.39E-05,1.79E-05,0.605014354,1.17E-07,1.62E-07,4.15E-05,0.000857695,0.000126865,2.98E-05,0.390295587,0.001097852,0.002422245,1.20E-05
6512,text-classification0,107,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",Traditional Methods,Traditional Methods,text-classification,0,4,1,0,,0.080840806,0,negative,0.000149145,0.00052681,0.012116003,5.42E-06,1.14E-05,0.011047905,0.005060781,0.136593308,0.000378354,0.833913998,1.64E-05,0.000144992,3.55E-05
6513,text-classification0,108,"For the normal bag - of - words , we use the counts of each word as the features .",Traditional Methods,Traditional Methods,text-classification,0,5,1,0,,0.102372091,0,negative,6.13E-05,0.000157275,0.038454339,2.62E-07,8.31E-07,0.000798896,0.000276085,0.011171726,0.000557,0.948481675,4.23E-06,3.39E-05,2.53E-06
6514,text-classification0,109,"For the TFIDF ( term-frequency inverse - document - frequency ) version , we use the counts as the term-frequency .",Traditional Methods,Traditional Methods,text-classification,0,6,1,0,,0.357017704,0,baselines,3.51E-05,6.71E-05,0.759322481,9.43E-08,5.22E-07,9.12E-05,0.00029926,0.000309455,7.64E-05,0.239674722,1.11E-05,0.000109972,2.47E-06
6515,text-classification0,110,The inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset .,Traditional Methods,Traditional Methods,text-classification,0,7,1,0,,0.013087887,0,negative,5.09E-05,7.38E-05,0.045130261,2.01E-06,2.10E-06,0.001917652,0.000975227,0.008189852,0.00031914,0.943203552,4.89E-05,6.57E-05,2.10E-05
6516,text-classification0,111,The features are normalized by dividing the largest feature value .,Traditional Methods,Traditional Methods,text-classification,0,8,1,0,,0.015126611,0,negative,9.03E-05,8.36E-05,0.010098751,5.15E-07,1.30E-06,0.001784566,0.0003773,0.015625142,0.000418924,0.971470172,2.72E-06,4.19E-05,4.83E-06
6517,text-classification0,112,Bag - of - ngrams and its TFIDF .,Traditional Methods,Traditional Methods,text-classification,0,9,1,0,,0.008896727,0,baselines,0.000101441,1.61E-05,0.668097986,7.40E-08,1.79E-07,2.49E-05,0.001060493,7.15E-05,1.91E-05,0.326086151,0.000732151,0.003778584,1.14E-05
6518,text-classification0,113,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",Traditional Methods,Traditional Methods,text-classification,0,10,1,0,,0.188720749,0,negative,7.48E-05,0.000230409,0.008621444,5.39E-06,1.60E-05,0.010878856,0.005340623,0.088645219,0.000178168,0.885849869,5.24E-06,0.00011456,3.94E-05
6519,text-classification0,114,The feature values are computed the same way as in the bag - of - words model .,Traditional Methods,Traditional Methods,text-classification,0,11,1,0,,0.001220269,0,negative,8.58E-05,7.27E-05,0.014476187,2.82E-07,7.49E-07,0.000605082,0.000234893,0.004828023,0.000546707,0.979099055,1.57E-06,4.53E-05,3.64E-06
6520,text-classification0,115,Bag - of - means on word embedding .,Traditional Methods,Traditional Methods,text-classification,0,12,1,0,,0.022652044,0,baselines,9.79E-05,4.98E-05,0.827275106,2.93E-07,3.87E-07,5.73E-05,0.001819804,0.000207673,6.44E-05,0.165167378,0.000784809,0.00444403,3.11E-05
6521,text-classification0,116,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",Traditional Methods,Traditional Methods,text-classification,0,13,1,0,,0.000451852,0,baselines,0.000184216,0.000110176,0.620281193,8.12E-07,8.47E-06,7.27E-05,0.000359198,0.00016749,0.000253437,0.378347626,2.70E-06,0.000203409,8.60E-06
6522,text-classification0,117,We take into consideration all the words that appeared more than 5 times in the training subset .,Traditional Methods,Traditional Methods,text-classification,0,14,1,0,,0.002592883,0,negative,0.000124137,4.91E-05,0.002047665,1.28E-06,1.06E-05,0.001070827,0.00054435,0.008546018,6.27E-05,0.987425619,5.86E-07,0.000110725,6.43E-06
6523,text-classification0,118,The dimension of the embedding is 300 .,Traditional Methods,Traditional Methods,text-classification,0,15,1,0,,0.862768374,1,hyperparameters,0.000139485,0.000276923,0.001567995,2.00E-05,6.17E-06,0.041862507,0.012391131,0.744716397,0.000351707,0.198091997,3.35E-05,0.000246534,0.000295705
6524,text-classification0,119,The bag - of - means features are computed the same way as in the bag - of - words model .,Traditional Methods,Traditional Methods,text-classification,0,16,1,0,,0.003134482,0,negative,0.000243618,7.59E-05,0.027126066,4.93E-07,2.03E-06,0.000452642,0.000409675,0.00270663,0.000447206,0.968419308,7.02E-07,0.000109251,6.45E-06
6525,text-classification0,120,The number of means is 5000 .,Traditional Methods,Traditional Methods,text-classification,0,17,1,0,,0.54875057,1,hyperparameters,0.000121304,0.00017866,0.001755054,7.45E-06,4.48E-06,0.028936968,0.009423355,0.52556189,0.000280691,0.433272056,2.24E-05,0.000258051,0.000177635
6526,text-classification0,121,Deep Learning Methods,,,text-classification,0,0,1,0,,0.359151529,0,negative,3.45E-05,0.000271938,0.000110369,4.16E-06,1.04E-06,0.000251751,0.00088824,0.002525396,0.000125578,0.497834222,0.496523444,0.001363013,6.63E-05
6527,text-classification0,122,Recently deep learning methods have started to be applied to text classification .,Deep Learning Methods,Deep Learning Methods,text-classification,0,1,1,0,,0.005247977,0,negative,6.20E-06,6.95E-06,9.36E-06,5.72E-07,8.50E-08,2.80E-05,5.82E-05,0.000127803,3.65E-06,0.932812826,0.066827506,0.00011599,2.96E-06
6528,text-classification0,123,"We choose two simple and representative models for comparison , in which one is word - based ConvNet and the other a simple long - short term memory ( LSTM ) recurrent neural network model .",Deep Learning Methods,Deep Learning Methods,text-classification,0,2,1,0,,0.009682648,0,negative,2.36E-05,0.000104814,0.002410639,2.20E-07,1.08E-06,0.000129417,0.000127619,0.000512498,4.26E-05,0.996489008,1.63E-05,0.00014167,4.37E-07
6529,text-classification0,124,Word - based ConvNets .,Deep Learning Methods,Deep Learning Methods,text-classification,0,3,1,0,,0.001479485,0,negative,2.19E-05,1.00E-05,0.030787111,1.60E-08,2.66E-08,2.92E-05,6.64E-05,7.22E-05,5.44E-05,0.968262102,0.000272526,0.000423629,4.89E-07
6530,text-classification0,125,"Among the large number of recent works on word - based ConvNets for text classification , one of the differences is the choice of using pretrained or end - to - end learned word representations .",Deep Learning Methods,Deep Learning Methods,text-classification,0,4,1,0,,0.000669414,0,negative,6.85E-06,5.59E-06,1.31E-05,3.24E-07,8.57E-08,1.38E-05,2.33E-05,6.23E-05,1.60E-06,0.991775711,0.007995852,0.00010073,6.19E-07
6531,text-classification0,126,We offer comparisons with both using the pretrained word2vec embedding and using lookup tables .,Deep Learning Methods,Deep Learning Methods,text-classification,0,5,1,0,,0.000122295,0,negative,2.24E-05,6.27E-06,9.32E-06,2.50E-08,9.63E-08,9.64E-06,7.19E-06,5.43E-05,2.80E-06,0.999683623,1.22E-06,0.000203105,2.92E-08
6532,text-classification0,127,"The embedding size is 300 in both cases , in the same way as our bagof - means model .",Deep Learning Methods,Deep Learning Methods,text-classification,0,6,1,0,,0.036423123,0,negative,2.80E-05,0.000163356,4.41E-05,3.31E-06,7.06E-07,0.023121449,0.001088531,0.291243572,0.000228304,0.683937333,5.77E-05,7.43E-05,9.39E-06
6533,text-classification0,128,"To ensure fair comparison , the models for each case are of the same size as our character - level ConvNets , in terms of both the number of layers and each layer 's output size .",Deep Learning Methods,Deep Learning Methods,text-classification,0,7,1,0,,1.42E-05,0,negative,6.23E-06,1.53E-05,2.92E-05,9.81E-08,2.71E-07,0.000345605,6.29E-05,0.001556876,1.98E-05,0.997942436,1.33E-06,1.99E-05,1.33E-07
6534,text-classification0,129,Experiments using a thesaurus for data augmentation are also conducted .,Deep Learning Methods,Deep Learning Methods,text-classification,0,8,1,0,,6.40E-05,0,negative,0.000241777,9.13E-06,3.94E-05,9.75E-08,1.31E-06,9.54E-06,3.01E-05,4.08E-05,2.59E-06,0.998141712,1.53E-06,0.00148183,1.58E-07
6535,text-classification0,130,LSTM LSTM LSTM ... : long - short term memory,Deep Learning Methods,Deep Learning Methods,text-classification,0,9,1,0,,0.005146814,0,negative,6.39E-05,2.91E-05,0.00893502,2.96E-06,1.10E-06,0.000511491,0.00069216,0.000832683,0.000495063,0.98694587,0.000797214,0.000660646,3.27E-05
6536,text-classification0,131,Long - short term memory .,Deep Learning Methods,Deep Learning Methods,text-classification,0,10,1,0,,1.97E-05,0,negative,1.36E-05,1.05E-06,0.000343881,5.05E-09,1.43E-08,3.82E-06,1.17E-05,1.52E-05,6.48E-06,0.999291103,3.82E-05,0.00027487,1.31E-07
6537,text-classification0,132,"We also offer a comparison with a recurrent neural network model , namely long - short term memory ( LSTM ) .",Deep Learning Methods,Deep Learning Methods,text-classification,0,11,1,0,,0.001575688,0,negative,3.74E-05,3.05E-05,0.000223636,4.60E-08,3.23E-07,1.04E-05,1.15E-05,4.02E-05,3.28E-05,0.99940181,4.12E-06,0.00020713,1.15E-07
6538,text-classification0,133,"The LSTM model used in our case is word - based , using pretrained word2vec embedding of size 300 as in previous models .",Deep Learning Methods,Deep Learning Methods,text-classification,0,12,1,0,,0.000877041,0,negative,2.64E-05,0.00014553,0.000155508,1.17E-05,2.23E-06,0.030270004,0.002114094,0.152329833,0.000285836,0.814543697,2.18E-05,7.53E-05,1.81E-05
6539,text-classification0,134,"The model is formed by taking mean of the outputs of all LSTM cells to form a feature vector , and then using multinomial logistic regression on this feature vector .",Deep Learning Methods,Deep Learning Methods,text-classification,0,13,1,0,,1.36E-05,0,negative,7.70E-06,6.19E-05,8.72E-05,1.49E-07,2.94E-07,0.000109361,1.12E-05,0.001154153,0.000938648,0.997622913,2.63E-06,3.46E-06,3.71E-07
6540,text-classification0,135,The output dimension is 512 .,Deep Learning Methods,Deep Learning Methods,text-classification,0,14,1,0,,0.005819088,0,negative,1.79E-05,8.67E-05,3.11E-05,2.39E-06,1.03E-06,0.016798391,0.00110019,0.116956681,0.000158856,0.864722777,2.55E-05,8.92E-05,9.25E-06
6541,text-classification0,136,"The variant of LSTM we used is the common "" vanilla "" architecture [ 8 ] .",Deep Learning Methods,Deep Learning Methods,text-classification,0,15,1,0,,2.17E-05,0,negative,2.14E-05,2.82E-05,0.003184824,3.98E-06,3.93E-06,0.001589345,0.000763662,0.001323094,0.000126594,0.99283201,1.85E-05,9.70E-05,7.43E-06
6542,text-classification0,137,We also used gradient clipping in which the gradient norm is limited to 5 . gives an illustration .,Deep Learning Methods,Deep Learning Methods,text-classification,0,16,1,0,,0.004064479,0,negative,0.000131419,1.36E-05,9.97E-05,3.51E-07,1.26E-06,0.000380455,0.000113327,0.001004137,1.45E-05,0.997971514,5.67E-07,0.000268668,5.09E-07
6543,text-classification0,138,Mean,Deep Learning Methods,,text-classification,0,17,1,0,,7.88E-07,0,negative,4.66E-06,1.02E-07,1.64E-06,9.53E-09,2.21E-08,4.42E-06,6.99E-06,1.21E-05,1.00E-06,0.999864874,2.66E-07,0.000103876,3.82E-08
6544,text-classification0,139,Choice of Alphabet,Deep Learning Methods,,text-classification,0,18,1,0,,2.14E-06,0,negative,2.25E-06,6.28E-06,9.79E-06,1.12E-07,7.98E-08,5.49E-05,3.00E-05,0.000398287,4.75E-05,0.999374492,2.94E-05,4.62E-05,6.75E-07
6545,text-classification0,140,"For the alphabet of English , one apparent choice is whether to distinguish between upper-case and lower - case letters .",Deep Learning Methods,Choice of Alphabet,text-classification,0,19,1,0,,1.50E-06,0,negative,1.53E-06,2.11E-06,2.19E-06,4.85E-07,6.00E-07,2.12E-05,2.26E-06,8.41E-05,2.05E-06,0.999799584,6.69E-05,1.67E-05,3.38E-07
6546,text-classification0,141,We report experiments on this choice and observed that it usually ( but not always ) gives worse results when such distinction is made .,Deep Learning Methods,Choice of Alphabet,text-classification,0,20,1,0,,3.08E-06,0,negative,9.80E-06,5.52E-06,4.17E-06,1.17E-06,1.72E-06,5.43E-05,4.92E-06,0.000183318,2.87E-06,0.999619958,9.15E-06,0.000102658,4.85E-07
6547,text-classification0,142,"One possible explanation might be that semantics do not change with different letter cases , therefore there is a benefit of regularization .",Deep Learning Methods,Choice of Alphabet,text-classification,0,21,1,0,,9.87E-06,0,negative,1.67E-05,3.35E-06,4.11E-06,1.07E-07,2.16E-07,7.79E-05,3.14E-06,0.000614404,8.98E-06,0.99923031,1.66E-06,3.88E-05,2.74E-07
6548,text-classification0,143,Large - scale Datasets and Results,Deep Learning Methods,,text-classification,0,22,1,0,,0.001076468,0,negative,1.23E-05,1.37E-06,1.59E-05,2.31E-08,1.30E-07,7.72E-06,0.000172251,2.87E-05,1.11E-06,0.990931391,2.63E-05,0.008802144,6.90E-07
6549,text-classification0,144,"Previous research on ConvNets in different are as has shown that they usually work well with largescale datasets , especially when the model takes in low - level raw features like characters in our case .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,23,1,0,,0.005952471,0,negative,7.45E-06,7.83E-07,2.29E-05,2.73E-07,4.58E-08,4.53E-05,0.000126089,0.000147157,1.45E-06,0.99849399,0.000247858,0.00090138,5.32E-06
6550,text-classification0,145,"However , most open datasets for text classification are quite small , and large - scale datasets are splitted with a significantly smaller training set than testing .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,24,1,0,,0.000357936,0,negative,4.75E-06,5.07E-07,4.72E-06,6.81E-07,1.49E-07,2.42E-05,7.97E-05,6.11E-05,3.45E-07,0.99895214,0.000259748,0.000606186,5.81E-06
6551,text-classification0,146,"Therefore , instead of confusing our community more by using them , we built several large - scale datasets for our experiments , ranging from hundreds of thousands to several millions of samples .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,25,1,0,,3.31E-05,0,negative,1.07E-05,8.17E-07,8.93E-06,1.80E-07,2.37E-06,1.59E-05,1.42E-05,2.51E-05,4.44E-07,0.999767874,8.10E-08,0.000153228,2.28E-07
6552,text-classification0,147,is a summary .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,26,1,0,,0.000666261,0,negative,4.95E-06,4.62E-08,2.29E-06,3.77E-08,4.64E-08,4.72E-06,4.13E-06,8.43E-06,3.29E-07,0.999880706,1.57E-07,9.40E-05,1.41E-07
6553,text-classification0,148,Sogou news corpus .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,27,1,0,,0.134085665,0,negative,0.000224628,1.51E-06,0.000523404,1.03E-06,4.83E-06,4.25E-05,0.005213353,5.98E-05,4.86E-07,0.645451031,2.84E-05,0.348303814,0.000145241
6554,text-classification0,149,"This dataset is a combination of the Sogo u CA and Sogo uCS news corpora , containing in total 2,909,551 news articles in various topic channels .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,28,1,0,,0.000445214,0,negative,4.04E-05,1.76E-06,0.000120268,9.82E-06,6.25E-05,0.000116189,0.00021392,6.64E-05,1.08E-06,0.998150494,3.29E-07,0.001204987,1.18E-05
6555,text-classification0,150,"We then labeled each piece of news using its URL , by manually classifying the their domain names .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,29,1,0,,1.20E-05,0,negative,3.36E-06,5.36E-07,8.48E-06,1.75E-07,1.73E-06,2.89E-05,8.35E-06,5.47E-05,1.24E-06,0.999850298,2.13E-08,4.18E-05,3.72E-07
6556,text-classification0,151,This gives us a large corpus of news articles labeled with their categories .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,30,1,0,,5.74E-05,0,negative,3.05E-05,7.88E-07,1.48E-05,7.78E-06,3.60E-05,5.25E-05,5.21E-05,3.07E-05,7.73E-07,0.999250082,4.84E-07,0.000517942,5.57E-06
6557,text-classification0,152,There are a large number categories but most of them contain only few articles .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,31,1,0,,0.000185927,0,negative,8.31E-06,1.50E-07,2.65E-06,2.93E-07,3.36E-07,1.07E-05,1.07E-05,1.96E-05,2.77E-07,0.999707888,7.85E-07,0.000237449,8.99E-07
6558,text-classification0,153,"We choose 5 categories - "" sports "" , "" finance "" , "" entertainment "" , "" automobile "" and "" technology "" .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,32,1,0,,0.001073049,0,negative,7.30E-06,5.08E-07,4.61E-06,2.23E-07,8.81E-07,9.44E-05,3.52E-05,0.000425622,6.75E-07,0.99927189,2.04E-08,0.000157649,9.68E-07
6559,text-classification0,154,"The number of training samples selected for each class is 90,000 and testing 12,000 .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,33,1,0,,0.00251755,0,negative,1.07E-05,3.54E-06,7.95E-06,3.37E-07,3.86E-07,0.000937303,0.000462281,0.014168012,4.67E-06,0.984155867,4.95E-07,0.000241016,7.41E-06
6560,text-classification0,155,"Although this is a dataset in Chinese , we used pypinyin package combined with jieba Chinese segmentation system to produce Pinyin - a phonetic romanization of Chinese .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,34,1,0,,3.50E-05,0,negative,0.000251183,1.94E-06,0.000272206,5.77E-06,6.32E-05,8.93E-05,0.000264264,4.11E-05,1.35E-06,0.994586131,1.89E-07,0.004413785,9.53E-06
6561,text-classification0,156,The models for English can then be applied to this dataset without change .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,35,1,0,,1.11E-05,0,negative,1.80E-05,6.90E-08,1.55E-05,3.44E-09,4.07E-08,2.27E-06,1.29E-05,6.51E-06,2.43E-07,0.998784322,1.48E-08,0.001160116,1.04E-07
6562,text-classification0,157,The fields used are title and content . :,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,36,1,0,,6.41E-05,0,negative,7.77E-06,6.22E-08,3.61E-06,5.39E-07,3.98E-07,3.74E-05,8.23E-06,2.55E-05,3.75E-07,0.999820421,2.61E-08,9.52E-05,5.12E-07
6563,text-classification0,158,Testing errors of all the models .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,37,1,0,,0.000475008,0,negative,2.59E-06,3.96E-08,2.78E-07,3.14E-09,7.59E-09,5.05E-06,1.30E-05,4.51E-05,1.08E-07,0.999584705,3.83E-08,0.000349024,1.13E-07
6564,text-classification0,159,Numbers are in percentage .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,38,1,0,,0.000136628,0,negative,1.44E-06,1.11E-07,1.24E-06,2.07E-08,1.87E-08,1.74E-05,2.04E-05,0.000103791,8.87E-07,0.999689154,1.13E-07,0.000165026,3.43E-07
6565,text-classification0,160,""" Lg "" stands for "" large "" and "" Sm "" stands for "" small "" .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,39,1,0,,0.000113548,0,negative,7.28E-07,1.90E-07,2.47E-06,1.36E-08,1.96E-08,2.41E-05,1.54E-05,0.000178947,1.80E-06,0.999720687,7.27E-08,5.52E-05,3.63E-07
6566,text-classification0,161,""" w2 v "" is an abbreviation for "" word2vec "" , and "" Lk "" for "" lookup DBPedia ontology dataset .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,40,1,0,,7.36E-05,0,negative,9.38E-07,2.62E-07,2.63E-06,1.43E-07,1.35E-07,3.80E-05,1.59E-05,0.000173444,1.21E-06,0.999709141,9.22E-08,5.71E-05,9.31E-07
6567,text-classification0,162,DBpedia is a crowd - sourced community effort to extract structured information from Wikipedia .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,41,1,0,,0.006875803,0,negative,0.000438168,1.20E-05,0.001518129,0.000636764,0.000159755,0.00097302,0.004979413,0.000253544,8.22E-06,0.968275278,7.83E-05,0.019570135,0.003097336
6568,text-classification0,163,The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014 .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,42,1,0,,0.000353652,0,negative,1.99E-05,1.56E-06,8.78E-05,1.06E-06,2.93E-05,4.09E-05,8.15E-05,5.99E-05,5.92E-07,0.999011924,2.49E-08,0.000661031,4.60E-06
6569,text-classification0,164,"From each of these 14 ontology classes , we randomly choose 40,000 training samples and 5,000 testing samples .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,43,1,0,,0.000135103,0,negative,7.12E-06,1.68E-06,3.50E-06,1.09E-07,7.58E-07,9.77E-05,8.62E-05,0.001242877,1.12E-06,0.998420587,1.82E-08,0.000136499,1.79E-06
6570,text-classification0,165,The fields we used for this dataset contain title and abstract of each Wikipedia article .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,44,1,0,,6.00E-05,0,negative,1.76E-06,2.54E-07,4.35E-06,9.38E-08,1.65E-06,2.92E-05,1.29E-05,7.52E-05,2.86E-07,0.999783212,9.16E-09,9.03E-05,7.73E-07
6571,text-classification0,166,Yelp reviews .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,45,1,0,,2.68E-05,0,negative,7.13E-05,5.38E-08,1.01E-05,4.46E-08,2.41E-07,4.12E-06,5.27E-05,8.63E-06,1.51E-07,0.992897608,5.83E-08,0.006953074,1.94E-06
6572,text-classification0,167,The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015 .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,46,1,0,,0.006785382,0,negative,2.06E-05,1.86E-06,0.000129613,5.86E-07,1.97E-05,3.55E-05,0.000251216,7.26E-05,4.33E-07,0.995084874,4.75E-08,0.004370085,1.28E-05
6573,text-classification0,168,"This dataset contains 1,569,264 samples that have review texts .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,47,1,0,,0.000307046,0,negative,1.36E-05,9.47E-07,2.54E-05,1.10E-06,3.68E-05,2.22E-05,2.73E-05,3.41E-05,3.95E-07,0.999483951,9.17E-09,0.000351762,2.48E-06
6574,text-classification0,169,"Two classification tasks are constructed from this dataset - one predicting full number of stars the user has given , and the other predicting a polarity label by considering stars 1 and 2 negative , and 3 and 4 positive .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,48,1,0,,1.44E-05,0,negative,7.17E-06,1.01E-06,3.01E-05,2.89E-08,1.64E-06,4.85E-06,1.57E-05,2.10E-05,6.27E-07,0.999555235,1.05E-08,0.000361924,6.53E-07
6575,text-classification0,170,"The full dataset has 130,000 training samples and 10,000 testing samples in each star , and the polarity dataset has 280,000 training samples and 19,000 test samples in each polarity .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,49,1,0,,0.000444695,0,negative,1.74E-05,7.62E-07,4.28E-05,1.50E-07,6.68E-06,2.00E-05,7.63E-05,7.30E-05,3.98E-07,0.998988947,3.56E-09,0.000771139,2.43E-06
6576,text-classification0,171,Yahoo!,Deep Learning Methods,,text-classification,0,50,1,0,,6.09E-07,0,negative,9.16E-07,2.96E-07,2.26E-06,1.76E-08,1.35E-07,3.93E-06,2.73E-06,2.03E-05,2.30E-06,0.99995556,2.73E-08,1.14E-05,1.53E-07
6577,text-classification0,172,Answers dataset .,Deep Learning Methods,Yahoo!,text-classification,0,51,1,0,,0.082534854,0,negative,6.10E-05,2.75E-06,8.19E-05,3.01E-07,3.77E-05,0.001294303,0.026803379,0.000165301,8.41E-07,0.922794783,1.57E-06,0.048689352,6.68E-05
6578,text-classification0,173,We obtained Yahoo!,Deep Learning Methods,,text-classification,0,52,1,0,,4.79E-07,0,negative,8.18E-06,2.89E-07,5.35E-06,2.73E-08,1.45E-06,2.66E-06,2.87E-06,6.38E-06,7.84E-07,0.999913379,8.15E-09,5.85E-05,1.25E-07
6579,text-classification0,174,Answers Comprehensive Questions and Answers version 1.0 dataset through the Yahoo !,Deep Learning Methods,We obtained Yahoo!,text-classification,0,53,1,0,,0.000424744,0,negative,0.000119004,2.73E-06,0.000241256,8.17E-06,0.000480304,0.004621044,0.025905721,0.000159111,1.74E-06,0.95789613,1.35E-06,0.004059249,0.006504196
6580,text-classification0,175,Webscope program .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,54,1,0,,0.001066196,0,negative,7.94E-05,1.79E-06,0.000181697,3.83E-06,7.16E-05,0.012770664,0.00525403,0.000468181,4.25E-06,0.980141358,1.72E-07,0.000327358,0.000695726
6581,text-classification0,176,"The corpus contains 4,483,032 questions and their answers .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,55,1,0,,0.000527334,0,negative,3.33E-05,4.27E-06,4.45E-05,2.36E-05,0.01132961,0.002804876,0.000597413,7.07E-05,9.13E-07,0.98476309,2.25E-08,7.21E-05,0.000255622
6582,text-classification0,177,We constructed a topic classification dataset from this corpus using 10 largest main categories .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,56,1,0,,1.14E-05,0,negative,5.08E-05,1.01E-05,0.000253572,2.53E-06,0.005783535,0.00090426,0.000715069,3.83E-05,1.94E-06,0.99199129,2.28E-08,0.000142333,0.000106155
6583,text-classification0,178,"Each class contains 140,000 training samples and 5,000 testing samples .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,57,1,0,,9.35E-05,0,negative,2.35E-05,5.09E-06,4.82E-05,6.15E-07,0.000130788,0.006368544,0.002584088,0.001447354,3.01E-06,0.989159723,8.19E-09,4.17E-05,0.000187374
6584,text-classification0,179,"The fields we used include question title , question content and best answer .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,58,1,0,,3.73E-05,0,negative,5.64E-06,4.29E-07,8.65E-06,3.42E-07,3.38E-05,0.002920077,0.000235406,0.000133288,6.37E-07,0.996628687,5.86E-09,1.31E-05,1.99E-05
6585,text-classification0,180,Amazon reviews .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,59,1,0,,9.88E-06,0,negative,0.000307094,8.33E-07,0.000176526,5.12E-07,9.29E-05,0.000800611,0.006123369,4.46E-05,6.68E-07,0.989579916,5.20E-08,0.002396244,0.000476687
6586,text-classification0,181,"We obtained an Amazon review dataset from the Stanford Network Analysis Project ( SNAP ) , which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,60,1,0,,7.16E-05,0,negative,9.76E-05,2.47E-05,0.000280662,0.000103897,0.053978444,0.003967675,0.001705615,8.07E-05,2.26E-06,0.938855867,6.31E-08,0.000199616,0.000702817
6587,text-classification0,182,"Similarly to the Yelp review dataset , we also constructed 2 datasets - one full score prediction and another polarity prediction .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,61,1,0,,7.08E-05,0,negative,0.000110056,5.85E-06,0.000700855,7.50E-07,0.001795243,0.000574931,0.001421736,1.89E-05,1.33E-06,0.994964803,6.82E-09,0.000339818,6.57E-05
6588,text-classification0,183,"The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,62,1,0,,0.000204271,0,negative,2.71E-05,2.79E-06,0.000109404,1.33E-06,0.00181816,0.001339409,0.000932655,7.15E-05,6.68E-07,0.995534663,2.17E-09,7.09E-05,9.14E-05
6589,text-classification0,184,The fields used are review title and review content .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,63,1,0,,5.26E-06,0,negative,1.23E-05,6.90E-07,1.35E-05,1.68E-06,9.82E-05,0.001639105,0.000117341,9.06E-05,8.96E-07,0.997977363,6.82E-09,1.52E-05,3.30E-05
6590,text-classification0,185,lists all the testing errors we obtained from these datasets for all the applicable models .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,64,1,0,,4.01E-06,0,negative,7.03E-06,6.48E-08,1.86E-06,7.51E-08,6.49E-06,0.000230191,8.58E-05,2.13E-05,1.02E-07,0.999617156,1.08E-09,2.46E-05,5.35E-06
6591,text-classification0,186,"Note that since we do not have a Chinese thesaurus , the Sogou News dataset does not have any results using thesaurus augmentation .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,65,1,0,,2.54E-05,0,negative,0.000253015,8.64E-08,1.28E-05,1.34E-07,4.03E-05,0.000156889,0.001369992,6.66E-06,1.75E-08,0.995135199,5.97E-09,0.002988979,3.59E-05
6592,text-classification0,187,We labeled the best result in blue and worse result in red .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,66,1,0,,2.00E-05,0,negative,5.84E-06,3.24E-07,1.11E-05,3.21E-08,3.08E-06,0.001036171,0.000426384,0.000120983,1.06E-06,0.998357648,1.98E-09,1.96E-05,1.78E-05
6593,text-classification0,188,Figure 3 : Relative errors with comparison models,Deep Learning Methods,We obtained Yahoo!,text-classification,0,67,1,0,,0.000166206,0,negative,7.30E-05,2.78E-07,2.83E-05,1.18E-07,2.70E-06,0.000461108,0.003552285,6.05E-05,1.98E-06,0.994723922,5.59E-08,0.000876997,0.000218785
6594,text-classification0,189,"To understand the results in table 4 further , we offer some empirical analysis in this section .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,68,1,0,,1.12E-05,0,negative,3.21E-05,3.75E-07,1.69E-06,4.19E-07,2.07E-05,0.000669867,0.000260759,7.89E-05,2.34E-07,0.998864274,1.73E-09,4.80E-05,2.27E-05
6595,text-classification0,190,"To facilitate our analysis , we present the relative errors in with respect to comparison models .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,69,1,0,,5.41E-06,0,negative,1.35E-05,4.54E-07,4.26E-06,7.96E-08,1.28E-05,0.000186925,8.37E-05,2.62E-05,4.08E-07,0.999642467,8.90E-10,2.54E-05,3.85E-06
6596,text-classification0,191,"Each of these plots is computed by taking the difference between errors on comparison model and our character - level ConvNet model , then divided by the comparison model error .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,70,1,0,,1.56E-06,0,negative,2.73E-05,2.87E-07,1.13E-05,3.14E-08,7.49E-06,0.000326542,0.000290858,3.31E-05,4.84E-07,0.999259093,7.31E-10,3.44E-05,9.07E-06
6597,text-classification0,192,All ConvNets in the figure are the large models with thesaurus augmentation respectively .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,71,1,0,,0.000169445,0,negative,7.07E-05,2.63E-06,0.000318792,4.83E-07,6.19E-05,0.005309055,0.006510754,0.000747629,4.75E-06,0.986266527,3.34E-09,9.31E-05,0.000613637
6598,text-classification0,193,Character - level,Deep Learning Methods,We obtained Yahoo!,text-classification,0,72,1,0,,0.00027842,0,negative,0.000809077,5.77E-06,0.002438349,7.23E-07,0.000115653,0.002681109,0.020864427,0.000132825,2.28E-05,0.967291619,4.56E-08,0.003266635,0.002370987
6599,text-classification0,194,ConvNet is an effective method .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,73,1,0,,0.000663534,0,negative,0.001450962,1.19E-06,0.000652848,2.31E-07,2.26E-05,0.000716634,0.014035492,7.24E-05,9.67E-07,0.97459254,5.69E-08,0.007335173,0.001118884
6600,text-classification0,195,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,74,1,1,results,0.003156897,0,negative,0.003737333,2.08E-06,4.89E-05,2.93E-07,2.83E-05,0.000689529,0.021982444,5.28E-05,6.33E-07,0.959639049,2.30E-08,0.013301634,0.000516998
6601,text-classification0,196,This is a strong indication that language could also bethought of as a signal no different from any other kind .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,75,1,0,,1.24E-06,0,negative,1.08E-05,6.20E-08,3.44E-06,1.55E-08,2.49E-06,6.46E-05,2.90E-05,6.41E-06,2.91E-07,0.999868137,5.93E-10,1.13E-05,3.48E-06
6602,text-classification0,197,shows 12 random first - layer patches learnt by one of our character - level ConvNets for DBPedia dataset .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,76,1,0,,3.10E-05,0,negative,7.76E-05,1.78E-06,6.58E-05,1.03E-06,9.31E-05,0.007125662,0.006234387,0.000839657,1.65E-06,0.984351064,2.46E-09,9.42E-05,0.001113999
6603,text-classification0,198,Dataset size forms a dichotomy between traditional and ConvNets models .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,77,1,0,,1.60E-05,0,negative,3.30E-05,2.58E-07,2.32E-05,5.98E-08,3.36E-05,0.000103481,0.000140421,7.65E-06,1.33E-07,0.999558939,8.67E-10,8.62E-05,1.30E-05
6604,text-classification0,199,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,78,1,1,results,0.004581106,0,negative,0.000996201,1.56E-06,2.55E-05,1.35E-06,4.20E-05,0.003350508,0.026881617,0.000515764,8.33E-07,0.963240346,1.63E-08,0.00358822,0.001356034
6605,text-classification0,200,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,79,1,0,,0.002027028,0,negative,0.000132986,3.18E-07,2.00E-05,1.89E-07,4.85E-06,0.001271515,0.011461798,0.000144039,3.19E-07,0.984180595,3.25E-08,0.001701068,0.00108226
6606,text-classification0,201,Conv Nets may work well for user - generated data .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,80,1,0,,0.000231359,0,negative,0.000114305,3.17E-07,1.32E-05,5.13E-07,6.84E-06,0.00263471,0.002046076,0.000300389,5.11E-07,0.994324331,8.36E-09,0.000237257,0.000321544
6607,text-classification0,202,User- generated data vary in the degree of how well the texts are curated .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,81,1,0,,1.23E-05,0,negative,5.21E-06,1.26E-07,6.21E-06,1.26E-07,2.61E-05,0.000136977,0.000154499,9.83E-06,9.04E-08,0.99952318,4.14E-09,6.72E-05,7.05E-05
6608,text-classification0,203,"For example , in our million scale datasets , Amazon reviews tend to be raw user-inputs , whereas users might be extra careful in their writings on Yahoo !",Deep Learning Methods,We obtained Yahoo!,text-classification,0,82,1,0,,3.03E-06,0,negative,3.55E-06,8.40E-08,4.49E-06,1.58E-07,3.68E-05,0.00021829,7.94E-05,8.16E-06,4.18E-08,0.999603141,1.06E-09,2.10E-05,2.49E-05
6609,text-classification0,204,Answers .,Deep Learning Methods,,text-classification,0,83,1,0,,1.83E-07,0,negative,7.41E-08,1.16E-08,1.05E-07,5.52E-10,7.42E-09,8.17E-07,6.35E-07,4.95E-06,1.12E-07,0.999989849,1.44E-10,3.42E-06,1.88E-08
6610,text-classification0,205,"Plots comparing word - based deep models ( figures 3 c , 3 d and 3 e ) show that character - level ConvNets work better for less curated user - generated texts .",Deep Learning Methods,Answers .,text-classification,0,84,1,0,,0.085337394,0,negative,0.00049492,4.24E-06,1.26E-05,6.07E-08,4.38E-06,0.000109121,0.002165922,0.000355326,9.76E-07,0.913750507,2.35E-08,0.0830849,1.70E-05
6611,text-classification0,206,This property suggests that ConvNets may have better applicability to real - world scenarios .,Deep Learning Methods,Answers .,text-classification,0,85,1,0,,7.16E-05,0,negative,3.75E-06,7.38E-07,1.32E-06,1.81E-08,1.96E-06,8.14E-06,2.06E-06,2.50E-05,7.49E-07,0.999925672,8.77E-10,3.03E-05,2.06E-07
6612,text-classification0,207,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",Deep Learning Methods,Answers .,text-classification,0,86,1,1,results,2.62E-05,0,negative,7.69E-06,1.09E-06,6.66E-07,9.89E-08,3.74E-06,0.000109007,2.96E-05,0.000344294,5.21E-07,0.999366188,4.03E-09,0.000135368,1.75E-06
6613,text-classification0,208,Choice of alphabet makes a difference .,Deep Learning Methods,Answers .,text-classification,0,87,1,0,,6.00E-06,0,negative,4.44E-05,1.71E-06,3.35E-06,8.08E-08,5.80E-06,3.34E-05,1.48E-05,6.96E-05,1.06E-06,0.999632948,1.49E-09,0.000191773,1.10E-06
6614,text-classification0,209,shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference .,Deep Learning Methods,Answers .,text-classification,0,88,1,0,,1.81E-05,0,negative,2.67E-05,3.12E-07,3.72E-06,3.70E-09,1.57E-06,5.48E-06,7.52E-06,8.32E-06,3.01E-07,0.999784136,3.61E-10,0.000161764,1.71E-07
6615,text-classification0,210,"For million - scale datasets , it seems that not making such distinction usually works better .",Deep Learning Methods,Answers .,text-classification,0,89,1,0,,0.00025566,0,negative,2.38E-05,1.20E-06,1.17E-06,2.38E-07,3.62E-06,0.00033254,0.000397377,0.001702519,3.42E-07,0.99473587,1.42E-08,0.002789165,1.22E-05
6616,text-classification0,211,"One possible explanation is that there is a regularization effect , but this is to be validated .",Deep Learning Methods,Answers .,text-classification,0,90,1,0,,1.16E-05,0,negative,2.11E-06,1.45E-06,1.36E-06,4.27E-08,1.34E-06,7.02E-05,6.64E-06,0.00024482,2.08E-06,0.99965944,1.53E-09,9.60E-06,9.19E-07
6617,text-classification0,212,Semantics of tasks may not matter .,Deep Learning Methods,Answers .,text-classification,0,91,1,0,,3.17E-06,0,negative,5.86E-07,2.36E-07,4.19E-07,1.75E-08,6.43E-07,1.24E-05,1.87E-06,2.54E-05,3.23E-07,0.999946894,1.18E-09,1.10E-05,2.18E-07
6618,text-classification0,213,Our datasets consist of two kinds of tasks : sentiment analysis ( Yelp and Amazon reviews ) and topic classification ( all others ) .,Deep Learning Methods,Answers .,text-classification,0,92,1,0,,0.000251491,0,negative,2.17E-06,2.56E-05,3.14E-05,1.51E-06,0.001837706,0.000147027,5.46E-05,0.000141535,8.31E-07,0.997676392,1.94E-09,7.49E-05,6.21E-06
6619,text-classification0,214,This dichotomy in task semantics does not seem to play a role in deciding which method is better .,Deep Learning Methods,Answers .,text-classification,0,93,1,0,,3.15E-06,0,negative,7.34E-06,7.89E-07,1.48E-06,2.83E-08,5.92E-06,2.25E-05,7.48E-06,4.78E-05,5.66E-07,0.99986701,3.69E-10,3.86E-05,5.02E-07
6620,text-classification0,215,Bag - of - means is a misuse of word2vec .,Deep Learning Methods,Answers .,text-classification,0,94,1,0,,3.44E-05,0,negative,2.40E-05,3.27E-05,0.003373917,3.85E-07,7.12E-05,0.000271588,0.000244829,0.000221278,1.59E-05,0.995474478,2.09E-08,0.000213709,5.60E-05
6621,text-classification0,216,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag - of - means model performs worse in every case .,Deep Learning Methods,Answers .,text-classification,0,95,1,0,,0.002104836,0,negative,0.000456637,6.91E-06,7.04E-06,1.84E-06,4.72E-05,0.001164203,0.003727305,0.003104687,6.32E-07,0.973646063,1.04E-08,0.017760131,7.74E-05
6622,text-classification0,217,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",Deep Learning Methods,Answers .,text-classification,0,96,1,1,results,3.34E-05,0,negative,4.59E-05,1.02E-06,4.32E-06,5.00E-09,2.31E-06,6.75E-06,3.68E-05,1.22E-05,2.63E-07,0.998426979,1.36E-09,0.00146285,6.16E-07
6623,text-classification0,218,"However , our experiments does not speak for any other language processing tasks or use of word2vec in any other way .",Deep Learning Methods,Answers .,text-classification,0,97,1,0,,2.27E-06,0,negative,7.76E-07,3.42E-07,1.59E-06,1.88E-08,1.07E-05,8.60E-06,2.25E-06,1.01E-05,9.08E-08,0.999949928,1.24E-10,1.54E-05,1.48E-07
6624,text-classification0,219,There is no free lunch .,Deep Learning Methods,Answers .,text-classification,0,98,1,0,,2.46E-06,0,negative,6.48E-07,2.13E-07,7.33E-07,4.87E-08,4.25E-06,3.01E-05,4.79E-06,4.51E-05,1.32E-07,0.999901869,2.21E-10,1.15E-05,6.25E-07
6625,text-classification0,220,Our experiments once again verifies that there is not a single machine learning model that can work for all kinds of datasets .,Deep Learning Methods,Answers .,text-classification,0,99,1,0,,1.94E-05,0,negative,5.40E-06,4.14E-07,5.52E-07,1.58E-08,2.03E-06,3.86E-05,4.78E-05,0.000105156,1.36E-07,0.999396727,5.44E-10,0.000401835,1.36E-06
6626,text-classification0,221,The factors discussed in this section could all play a role in deciding which method is the best for some specific application .,Deep Learning Methods,Answers .,text-classification,0,100,1,0,,4.73E-06,0,negative,1.65E-06,9.27E-07,7.19E-07,3.74E-08,1.86E-06,4.04E-05,4.47E-06,0.000159602,1.46E-06,0.999780454,1.74E-10,7.83E-06,5.45E-07
6627,text-classification0,222,Conclusion and Outlook,,,text-classification,0,0,1,0,,0.00034636,0,negative,8.44E-05,7.52E-05,4.61E-06,3.77E-06,1.33E-06,0.000120733,4.40E-05,0.000807863,0.000101511,0.996791355,0.001814641,0.000144601,5.92E-06
6628,natural_language_inference52,1,title,,,natural_language_inference,52,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
6629,natural_language_inference52,2,Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification,title,title,natural_language_inference,52,1,1,1,research-problem,0.937280597,1,research-problem,7.88E-09,2.47E-06,7.02E-09,1.23E-07,4.03E-08,1.43E-07,3.04E-07,1.14E-06,3.30E-07,0.006562382,0.993433018,1.59E-08,2.31E-08
6630,natural_language_inference52,3,abstract,,,natural_language_inference,52,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
6631,natural_language_inference52,4,"For many applications of question answering ( QA ) , being able to explain why a given model chose an answer is critical .",abstract,abstract,natural_language_inference,52,1,1,0,,0.977741469,1,research-problem,1.60E-08,2.53E-06,5.51E-09,1.51E-06,2.22E-07,2.36E-07,4.33E-07,9.07E-07,1.46E-07,0.007350704,0.992643227,1.24E-08,4.84E-08
6632,natural_language_inference52,5,"However , the lack of labeled data for answer justifications makes learning this difficult and expensive .",abstract,abstract,natural_language_inference,52,2,1,0,,0.008133883,0,research-problem,8.63E-08,1.83E-05,2.06E-08,1.71E-05,5.15E-06,2.32E-06,7.02E-07,5.75E-06,8.05E-07,0.145839051,0.854110661,2.63E-08,8.88E-08
6633,natural_language_inference52,6,"Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications , where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either .",abstract,abstract,natural_language_inference,52,3,1,0,,0.487845949,0,research-problem,1.64E-05,0.085005492,8.19E-05,3.20E-05,0.000312536,2.05E-05,2.25E-05,0.00018844,0.001889302,0.107640784,0.80477824,8.22E-06,3.67E-06
6634,natural_language_inference52,7,We propose a neural network architecture for QA that reranks answer justifications as an intermediate ( and human - interpretable ) step in answer selection .,abstract,abstract,natural_language_inference,52,4,1,0,,0.748642892,1,research-problem,6.07E-07,0.001568341,1.11E-05,8.77E-07,4.09E-06,1.14E-06,2.57E-06,1.20E-05,0.000362971,0.01172438,0.986310791,7.37E-07,4.69E-07
6635,natural_language_inference52,8,"Our approach is informed by a set of features designed to combine both learned representations and explicit features to capture the connection between questions , answers , and answer justifications .",abstract,abstract,natural_language_inference,52,5,1,0,,0.048786825,0,approach,1.94E-05,0.528464871,5.52E-05,6.46E-05,0.000918101,0.000113139,9.09E-06,0.001356178,0.096910432,0.352639499,0.019444005,2.70E-06,2.73E-06
6636,natural_language_inference52,9,We show that with this end - to - end approach we are able to significantly improve upon a strong IR baseline in both justification ranking ( + 9 % rated highly relevant ) and answer selection ( + 6 % P@1 ) .,abstract,abstract,natural_language_inference,52,6,1,0,,0.099192391,0,negative,0.000372975,0.028637545,3.79E-05,3.82E-05,0.00034644,3.66E-05,0.000131676,0.000693846,0.000876346,0.56580466,0.402243395,0.00076774,1.27E-05
6637,natural_language_inference52,10,Introduction,,,natural_language_inference,52,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
6638,natural_language_inference52,11,"Developing interpretable machine learning ( ML ) models , that is , models where a human user can understand what the model is learning , is considered by many to be crucial for ensuring usability and accelerating progress .",Introduction,Introduction,natural_language_inference,52,1,1,1,research-problem,0.958296161,1,research-problem,9.73E-07,0.000164578,2.36E-07,1.26E-05,1.85E-05,5.29E-06,8.17E-06,5.55E-06,3.58E-05,0.053814744,0.945931293,7.64E-07,1.45E-06
6639,natural_language_inference52,12,"For many applications of question answering ( QA ) , i.e. , finding short answers to natural language questions , simply providing an answer is not sufficient .",Introduction,Introduction,natural_language_inference,52,2,1,0,,0.965498585,1,research-problem,4.93E-07,6.44E-05,1.27E-07,1.04E-05,1.08E-05,4.81E-06,9.47E-06,4.31E-06,1.48E-05,0.031956012,0.96792222,5.29E-07,1.58E-06
6640,natural_language_inference52,13,A complete Question :,Introduction,Introduction,natural_language_inference,52,3,1,0,,0.004388622,0,negative,2.56E-06,0.0011898,8.11E-07,1.92E-06,9.13E-06,5.10E-05,1.47E-05,8.84E-05,0.007088922,0.701366458,0.290183412,1.88E-06,1.06E-06
6641,natural_language_inference52,14,Which of these is a response to an internal stimulus ?,Introduction,Introduction,natural_language_inference,52,4,1,0,,0.001860101,0,negative,3.85E-06,0.002468874,1.60E-06,1.36E-05,5.32E-05,0.000138504,1.60E-05,0.000164351,0.00891979,0.92215099,0.066065431,2.10E-06,1.72E-06
6642,natural_language_inference52,15,( A ) A sunflower turns to face the rising sun .,Introduction,Introduction,natural_language_inference,52,5,1,0,,0.020302733,0,negative,9.52E-05,0.040311303,7.87E-05,1.32E-05,0.000389398,0.000222153,0.00012188,0.000372886,0.125863055,0.761062633,0.071426878,3.37E-05,9.11E-06
6643,natural_language_inference52,16,( B ) A cucumber tendril wraps around a wire .,Introduction,Introduction,natural_language_inference,52,6,1,0,,0.011717568,0,negative,3.11E-05,0.023481754,5.51E-05,1.34E-05,0.000176657,0.00031426,0.000107175,0.000534975,0.164734404,0.69099697,0.119526233,1.68E-05,1.12E-05
6644,natural_language_inference52,17,( C ) A pine tree knocked sideways in a landslide grows upward in a bend .,Introduction,Introduction,natural_language_inference,52,7,1,0,,0.010951233,0,negative,2.71E-05,0.007860075,3.02E-05,7.43E-05,0.00269442,0.000268618,6.37E-05,0.000128964,0.004937717,0.958103199,0.025794997,1.00E-05,6.79E-06
6645,natural_language_inference52,18,( D ) Guard cells of a tomato plant leaf close when there is little water in the roots .,Introduction,Introduction,natural_language_inference,52,8,1,0,,0.008992178,0,negative,6.72E-05,0.027859659,6.87E-05,9.15E-06,0.000282361,0.000229208,0.000132377,0.000393919,0.114066179,0.776000871,0.080853046,2.90E-05,8.42E-06
6646,natural_language_inference52,19,Justification :,Introduction,Introduction,natural_language_inference,52,9,1,0,,0.001565767,0,negative,6.15E-06,0.004185336,1.58E-06,2.67E-06,6.23E-05,0.000113791,1.45E-05,0.000204235,0.011795983,0.971150619,0.012459876,2.15E-06,8.56E-07
6647,natural_language_inference52,20,Plants rely on hormones to send signals within the plant in order to respond to internal stimuli such as alack of water or nutrients . :,Introduction,Introduction,natural_language_inference,52,10,1,0,,0.019436044,0,negative,3.35E-05,0.005818731,3.27E-05,0.000123306,0.001689083,0.000247928,7.75E-05,8.09E-05,0.007172579,0.916943676,0.06776301,8.63E-06,8.41E-06
6648,natural_language_inference52,21,Example of an 8th grade science question with a justification for the correct answer .,Introduction,Introduction,natural_language_inference,52,11,1,0,,0.001829754,0,negative,4.52E-06,0.003278046,4.22E-06,3.57E-05,0.000957614,0.000203738,6.41E-05,0.000124387,0.004895727,0.941019844,0.049398949,7.00E-06,6.16E-06
6649,natural_language_inference52,22,"Note the lack of direct lexical overlap present between the justification and the correct answer , demonstrating the difficulty of the task of finding justifications using traditional distant supervision methods .",Introduction,Introduction,natural_language_inference,52,12,1,0,,0.005089465,0,negative,1.98E-05,0.000647622,2.11E-06,0.000182764,0.003154091,0.00019833,3.16E-05,4.11E-05,0.000274707,0.99321262,0.002228126,4.90E-06,2.12E-06
6650,natural_language_inference52,23,"approach must be interpretable , i.e. , able to explain why an answer is correct .",Introduction,Introduction,natural_language_inference,52,13,1,0,,0.004119553,0,negative,1.71E-05,0.00771298,9.01E-06,7.23E-05,0.003212211,0.000121522,2.20E-05,6.43E-05,0.005445003,0.971429952,0.011885398,5.51E-06,2.75E-06
6651,natural_language_inference52,24,"For example , in the medical domain , a QA approach that answers treatment questions would not be trusted if the treatment recommendation is not explained in terms that can be understood by the human user .",Introduction,Introduction,natural_language_inference,52,14,1,0,,0.035726343,0,research-problem,2.10E-06,0.000874579,5.92E-07,1.60E-05,4.97E-05,4.02E-05,2.66E-05,5.90E-05,0.000395228,0.465588739,0.532940481,3.69E-06,3.14E-06
6652,natural_language_inference52,25,One approach to interpreting complex models is to make use of human- interpretable information generated by the model to gain insight into what the model is learning .,Introduction,Introduction,natural_language_inference,52,15,1,0,,0.173396814,0,research-problem,3.88E-06,0.001147355,2.02E-06,6.78E-05,0.000159405,5.72E-05,4.71E-05,4.26E-05,0.000260985,0.241425194,0.756775939,3.46E-06,6.94E-06
6653,natural_language_inference52,26,"We follow the intuition of , whose two - component network first generates text spans from an input document , and then uses these text spans to make predictions .",Introduction,Introduction,natural_language_inference,52,16,1,0,,0.401622767,0,model,1.10E-05,0.064188874,5.69E-05,3.85E-06,0.000258376,7.66E-05,2.25E-05,0.000109977,0.810244574,0.122252128,0.002768194,4.39E-06,2.53E-06
6654,natural_language_inference52,27,these intermediate text spans to infer the model 's preferences .,Introduction,Introduction,natural_language_inference,52,17,1,0,,0.032750191,0,negative,4.97E-05,0.069390659,8.50E-05,6.13E-05,0.010556679,0.000389344,8.76E-05,0.000258587,0.210828973,0.706243178,0.002021962,1.82E-05,8.78E-06
6655,natural_language_inference52,28,"By learning these human - readable intermediate representations endto - end with a downstream task , the representations are optimized to correlate with what the model learns is discriminatory for the task , and they can be evaluated against what a human would consider to be important .",Introduction,Introduction,natural_language_inference,52,18,1,0,,0.781739933,1,model,8.96E-06,0.17917959,1.75E-05,6.26E-07,0.000212077,1.03E-05,7.28E-06,3.52E-05,0.791742388,0.028409303,0.000372911,3.24E-06,5.72E-07
6656,natural_language_inference52,29,Here we apply this general framework for model interpretability to QA .,Introduction,Introduction,natural_language_inference,52,19,1,0,,0.508686066,1,approach,2.79E-05,0.505938167,7.45E-05,1.92E-05,0.001839278,0.000103467,0.00012652,0.000248164,0.314202128,0.149805485,0.027560442,4.22E-05,1.25E-05
6657,natural_language_inference52,30,"In this work , we focus on answering multiplechoice science exam questions ; see example in ) .",Introduction,Introduction,natural_language_inference,52,20,1,0,,0.773565757,1,negative,4.52E-05,0.33389365,0.000128639,0.001217838,0.062633337,0.000880981,0.000906175,0.000944145,0.01462161,0.471131881,0.11340483,8.56E-05,0.000106086
6658,natural_language_inference52,31,"This domain is challenging as : ( a ) approximately 70 % of science exam ques- tion shave been shown to require complex forms of inference to solve , and ( b ) there are few structured knowledge bases to support this inference .",Introduction,Introduction,natural_language_inference,52,21,1,0,,0.143748158,0,negative,6.04E-06,0.001647109,3.94E-06,0.000629802,0.00172052,0.000209506,0.000197867,8.68E-05,0.000153973,0.598763605,0.396540996,1.06E-05,2.93E-05
6659,natural_language_inference52,32,"Within this domain , we propose an approach that learns to both select and explain answers , when the only supervision available is for which answer is correct ( but not how to explain it ) .",Introduction,Introduction,natural_language_inference,52,22,1,1,approach,0.858869629,1,approach,9.01E-05,0.405596678,0.000206444,0.00020657,0.005550173,0.000225764,0.000372869,0.000345048,0.087003445,0.311746009,0.188510546,9.66E-05,4.98E-05
6660,natural_language_inference52,33,"Intuitively , our approach chooses the justifications that provide the most help towards ranking the correct answers higher than incorrect ones .",Introduction,Introduction,natural_language_inference,52,23,1,1,approach,0.30542962,0,model,3.40E-05,0.314168908,3.13E-05,9.40E-07,0.000339481,1.95E-05,1.77E-05,6.92E-05,0.643165116,0.041892611,0.000252868,7.42E-06,1.03E-06
6661,natural_language_inference52,34,"More formally , our neural network approach alternates between using the current model with max - pooling to choose the highest scoring justifications for correct answers , and optimizing the answer ranking model given these justifications .",Introduction,Introduction,natural_language_inference,52,24,1,1,approach,0.884652831,1,model,8.96E-06,0.179225215,5.31E-05,5.57E-07,0.000145458,1.03E-05,1.11E-05,2.27E-05,0.809116268,0.011027896,0.000374985,2.68E-06,8.71E-07
6662,natural_language_inference52,35,"Crucially , these reranked texts serve as our human - readable answer justifications , and by examining them , we gain insight into what the model learned was useful for the QA task .",Introduction,Introduction,natural_language_inference,52,25,1,0,,0.311291736,0,dataset,6.10E-05,0.065839147,5.19E-05,0.000211563,0.490326356,0.000269563,0.000263189,0.000101425,0.003181046,0.439235844,0.000391907,5.48E-05,1.23E-05
6663,natural_language_inference52,36,The specific contributions of this work are :,Introduction,Introduction,natural_language_inference,52,26,1,0,,0.003438721,0,negative,2.22E-05,0.007957183,1.45E-05,0.000314617,0.001924293,0.000591804,9.61E-05,0.000303155,0.006905717,0.979431166,0.002421177,7.25E-06,1.09E-05
6664,natural_language_inference52,37,1 .,Introduction,Introduction,natural_language_inference,52,27,1,0,,0.001441301,0,negative,7.42E-06,0.002536696,2.04E-06,8.88E-06,0.000157322,0.000226104,2.77E-05,0.000283229,0.009539847,0.986814409,0.000392102,2.71E-06,1.58E-06
6665,natural_language_inference52,38,We propose an end - to - end neural method for learning to answer questions and select a high - quality justification for those answers .,Introduction,Introduction,natural_language_inference,52,28,1,0,,0.881184585,1,approach,5.14E-05,0.484104055,0.000288375,1.62E-05,0.001724379,9.23E-05,0.00021407,0.000166214,0.445774511,0.056181257,0.011327686,3.67E-05,2.29E-05
6666,natural_language_inference52,39,Our approach re-ranks free - text answer justifications without the need for structured knowledge bases .,Introduction,Introduction,natural_language_inference,52,29,1,0,,0.918428424,1,approach,0.0001828,0.807678779,0.000281774,7.37E-05,0.015955329,0.00021777,0.000832895,0.000317958,0.063637758,0.103000511,0.007546721,0.000221132,5.28E-05
6667,natural_language_inference52,40,"With supervision only for the correct answers , we learn this re-ranking through a form of distant supervision - i.e. , the answer ranking supervises the justification re-ranking .",Introduction,Introduction,natural_language_inference,52,30,1,0,,0.792498749,1,model,1.53E-05,0.162124798,8.64E-05,4.93E-07,0.000286533,1.19E-05,1.91E-05,2.26E-05,0.820820646,0.01652204,8.55E-05,3.57E-06,9.83E-07
6668,natural_language_inference52,41,2 .,Introduction,Introduction,natural_language_inference,52,31,1,0,,0.005087598,0,negative,1.08E-05,0.005569125,4.65E-06,4.83E-06,0.000146207,0.00032292,6.60E-05,0.000480692,0.030017501,0.963000096,0.000369766,4.63E-06,2.75E-06
6669,natural_language_inference52,42,"We investigate two distinct categories of features in this "" little data "" domain : explicit features , and learned representations .",Introduction,Introduction,natural_language_inference,52,32,1,0,,0.427940339,0,approach,6.37E-05,0.758853743,0.000212279,1.44E-05,0.006348982,0.000138304,0.000364763,0.000234846,0.146923683,0.085745157,0.001019139,6.62E-05,1.48E-05
6670,natural_language_inference52,43,"We show that , with limited training , explicit features perform far better despite their simplicity .",Introduction,Introduction,natural_language_inference,52,33,1,0,,0.105486964,0,negative,0.001524766,0.136170147,5.66E-05,0.000155829,0.003752666,0.001489317,0.005830778,0.004535089,0.02805046,0.815253508,0.00170263,0.001332858,0.000145331
6671,natural_language_inference52,44,3 .,Introduction,Introduction,natural_language_inference,52,34,1,0,,0.00968369,0,negative,1.80E-05,0.004853463,5.31E-06,5.62E-06,0.000206991,0.000316732,9.93E-05,0.000422726,0.023194313,0.970658362,0.000207859,8.19E-06,3.13E-06
6672,natural_language_inference52,45,"We demonstrate a large ( + 9 % ) improvement in generating high - quality justifications over a strong information retrieval ( IR ) baseline , while maintaining near state - of - the - art performance on the multiple - choice scienceexam QA task , demonstrating the success of the end - to - end strategy .",Introduction,Introduction,natural_language_inference,52,35,1,0,,0.248076056,0,negative,0.001795825,0.433843388,0.000272997,7.09E-05,0.011063569,0.000527448,0.019615447,0.001324476,0.046261806,0.475484865,0.003071264,0.0063093,0.000358684
6673,natural_language_inference52,46,Related work,,,natural_language_inference,52,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
6674,natural_language_inference52,70,Approach,,,natural_language_inference,52,0,1,0,,6.30E-05,0,negative,0.000256653,0.000405673,1.67E-05,0.000511976,2.16E-05,0.00097816,0.00011784,0.00317846,0.000653531,0.990596627,0.003176926,4.36E-05,4.23E-05
6675,natural_language_inference52,71,"One of the primary difficulties with the explainable QA task addressed here is that , while we have supervision for the correct answer , we do not have annotated answer justifications .",Approach,Approach,natural_language_inference,52,1,1,0,,0.053790966,0,research-problem,1.90E-06,0.001696344,5.49E-07,0.000117855,0.000375775,1.94E-05,6.80E-06,2.69E-05,3.55E-05,0.446709629,0.55100655,1.25E-06,1.54E-06
6676,natural_language_inference52,72,"Here we tackle this challenge by using the QA task performance as supervision for the justification reranking , allowing us to learn to choose both the correct answer and a compelling , human - readable justification for that answer .",Approach,Approach,natural_language_inference,52,2,1,0,,0.502224718,1,approach,2.28E-05,0.905411527,3.89E-05,9.02E-06,0.000976603,1.67E-05,6.05E-06,0.000194926,0.033834926,0.054127155,0.005351583,8.71E-06,1.14E-06
6677,natural_language_inference52,73,"Additionally , similar to the strategy applied to parsing , we combine representation - based features with explicit features that capture additional information that is difficult to model through embeddings , especially with limited training data .",Approach,Approach,natural_language_inference,52,3,1,0,,0.611243424,1,approach,5.69E-05,0.786586933,0.00011498,6.61E-06,0.000704913,4.87E-05,6.95E-06,0.000350068,0.154980757,0.056741975,0.000392826,7.38E-06,9.55E-07
6678,natural_language_inference52,74,The architecture of our approach is summarized in .,Approach,Approach,natural_language_inference,52,4,1,0,,0.018622115,0,negative,9.37E-06,0.037556362,1.28E-05,0.000312346,0.001217976,0.000253328,9.57E-06,0.000495369,0.025934082,0.925577264,0.008614708,2.00E-06,4.87E-06
6679,natural_language_inference52,75,"Given a question and a candidate answer , we first query an textual knowledge base ( KB ) to retrieve a pool of potential justifications for that answer candidate .",Approach,Approach,natural_language_inference,52,5,1,0,,0.095833954,0,negative,1.86E-05,0.460731662,8.68E-05,5.68E-06,0.002201545,2.08E-05,9.33E-06,0.000164192,0.033590661,0.470938904,0.032216971,1.34E-05,1.39E-06
6680,natural_language_inference52,76,"For each justification , we extract a set of features designed to model the relations between questions , answers , and answer justifications based on word embeddings , lexical overlap with the question and answer candidate , discourse , and information retrieval ( IR ) ( Section 4.2 ) .",Approach,Approach,natural_language_inference,52,6,1,0,,0.369361145,0,approach,8.72E-06,0.63082136,4.39E-05,3.82E-06,0.001131618,9.50E-05,4.78E-06,0.00077634,0.199720837,0.167173658,0.000217328,2.06E-06,5.62E-07
6681,natural_language_inference52,77,"These features are passed into a simple neural network to generate a score for each justification , given the current state of the model .",Approach,Approach,natural_language_inference,52,7,1,0,,0.026148359,0,model,6.27E-06,0.156916032,3.26E-05,6.83E-07,7.50E-05,6.58E-05,2.03E-06,0.000558424,0.539617833,0.302467607,0.000256509,1.00E-06,2.20E-07
6682,natural_language_inference52,78,A final max - pooling layer selects the top - scoring justification for the candidate answer and this max score is used also as the score for the answer candidate .,Approach,Approach,natural_language_inference,52,8,1,0,,0.566192495,1,model,2.47E-05,0.122272647,0.000169417,1.57E-06,8.50E-05,6.94E-05,5.87E-06,0.00043438,0.761589982,0.114967292,0.000376317,2.44E-06,9.71E-07
6683,natural_language_inference52,79,The system is trained using correct - incorrect answer pairs with a pairwise margin ranking loss objective function to enforce that the correct answer be ranked higher than any of the incorrect answers .,Approach,Approach,natural_language_inference,52,9,1,0,,0.161211472,0,model,3.04E-06,0.461414684,3.66E-05,4.28E-07,7.37E-05,3.00E-05,2.19E-06,0.000715933,0.461916602,0.075540715,0.000265127,5.96E-07,2.69E-07
6684,natural_language_inference52,80,"With this end - to - end approach , the model learns to select justifications that allow it to correctly answer questions .",Approach,Approach,natural_language_inference,52,10,1,0,,0.161228286,0,model,1.07E-05,0.375950114,9.34E-05,6.51E-07,0.00020437,1.15E-05,2.07E-06,9.86E-05,0.474085945,0.148998968,0.000540605,2.87E-06,2.80E-07
6685,natural_language_inference52,81,We hypothesize that this approach enables the model to indirectly learn to choose justifications that provide good explanations as to why the answer is correct .,Approach,Approach,natural_language_inference,52,11,1,0,,0.002422672,0,negative,7.84E-06,0.0921837,1.29E-05,1.12E-05,0.000341648,9.89E-05,2.53E-06,0.000554837,0.091039874,0.815262516,0.000481778,1.64E-06,6.07E-07
6686,natural_language_inference52,82,"We empirically test this hypothesis in Section 6 , where we show that indeed the model learns to correctly answer questions , as well as to select high - quality justifications for those answers .",Approach,Approach,natural_language_inference,52,12,1,0,,0.007597208,0,negative,4.05E-05,0.118368701,7.88E-06,1.37E-05,0.001138712,5.61E-05,4.63E-06,0.000442279,0.010087195,0.869049996,0.000776215,1.35E-05,6.14E-07
6687,natural_language_inference52,83,Model and Features,,,natural_language_inference,52,0,1,0,,0.001515202,0,negative,0.000113523,0.000658457,0.000113163,7.63E-05,2.31E-05,0.001086348,0.000249199,0.004766133,0.001531328,0.989274962,0.00196422,0.000106275,3.69E-05
6688,natural_language_inference52,84,Our approach consists of three main components : ( a ) the retrieval of a pool of candidate answer justifications ( Section 4.1 ) ; ( b ) the extraction of features for each ( Section 4.2 ) ; and ( c ) the scoring of the answer candidate itself based on this pool of justifications ( Section 4.3 ) .,Model and Features,Model and Features,natural_language_inference,52,1,1,0,,0.020718564,0,negative,0.000702763,0.145145592,0.012702319,5.04E-05,0.000587548,0.000972513,0.000308373,0.002973869,0.216603416,0.603818825,0.015841098,0.000227353,6.60E-05
6689,natural_language_inference52,85,The architecture of this latter scoring component is shown in .,Model and Features,Model and Features,natural_language_inference,52,2,1,0,,0.001152968,0,negative,0.000115536,0.002168181,0.001781008,5.45E-05,6.77E-05,0.001443718,0.000178961,0.002016915,0.11391997,0.877197028,0.001000005,1.22E-05,4.43E-05
6690,natural_language_inference52,86,Candidate Justification,Model and Features,,natural_language_inference,52,3,1,0,,0.000155625,0,negative,1.93E-05,0.000100033,3.48E-05,4.05E-06,1.69E-05,0.000421602,3.88E-05,0.000878411,0.000518018,0.997825386,0.000131393,9.56E-06,1.63E-06
6691,natural_language_inference52,87,Retrieval,Model and Features,,natural_language_inference,52,4,1,0,,0.026331873,0,negative,0.000179076,0.001605584,0.00159429,0.000182525,0.000952992,0.007672002,0.011868666,0.008391328,0.000975644,0.94613305,0.018252654,0.001808449,0.00038374
6692,natural_language_inference52,88,The first step in our process is to use standard information retrieval ( IR ) methods to retrieve a set of candidate justifications for each candidate answer to a given question .,Model and Features,Retrieval,natural_language_inference,52,5,1,0,,0.000326389,0,negative,2.84E-05,0.000148818,1.92E-05,3.49E-06,7.95E-06,9.53E-05,5.94E-05,0.000103378,8.97E-05,0.996145203,0.003237971,5.44E-05,6.80E-06
6693,natural_language_inference52,89,"To do this , we build a bag - of - words ( BOW ) query using the content lemmas for the question and answer candidate , boosting the answer lemmas to have four times more weight 1 .",Model and Features,Retrieval,natural_language_inference,52,6,1,0,,0.000600605,0,negative,7.84E-05,0.001046486,0.001609891,2.59E-07,8.68E-06,0.000303243,0.000161095,0.000635013,0.00194802,0.994161653,8.92E-06,3.68E-05,1.57E-06
6694,natural_language_inference52,90,We used Lucene 2 with a tf - idf based scoring function to return the top - scoring documents from the KB .,Model and Features,Retrieval,natural_language_inference,52,7,1,0,,0.004859111,0,negative,2.17E-05,0.000227201,0.000446081,2.22E-06,9.01E-06,0.016542013,0.001804069,0.010147971,0.000601055,0.970160345,7.89E-06,2.08E-05,9.67E-06
6695,natural_language_inference52,91,"Each of these indexed documents consists of a single sentence from our corpora , and serves as one potential justification .",Model and Features,Retrieval,natural_language_inference,52,8,1,0,,1.63E-06,0,negative,7.83E-06,4.09E-07,2.01E-06,3.00E-08,6.30E-07,1.23E-05,1.59E-06,4.86E-06,1.20E-06,0.999966399,9.14E-08,2.66E-06,1.25E-08
6696,natural_language_inference52,92,Feature Extraction,Model and Features,,natural_language_inference,52,9,1,0,,0.026631535,0,negative,5.05E-05,0.000534423,0.000326461,1.77E-05,8.46E-05,0.00393692,0.000461876,0.005483574,0.002978264,0.985526617,0.000522599,5.06E-05,2.58E-05
6697,natural_language_inference52,93,"For each retrieved candidate justification , we extract a set of features based on ( a ) distributed representations of the question , candidate answer , and justification terms ; ( b ) strict lexical overlap ; ( c ) discourse relations present in the justification ; and ( d ) the IR scores for the justification .",Model and Features,Feature Extraction,natural_language_inference,52,10,1,0,,8.78E-06,0,negative,0.000139404,0.005291341,0.000288602,4.19E-07,5.29E-05,0.000313119,2.67E-05,0.000209809,0.013852584,0.979796951,1.95E-05,8.04E-06,5.74E-07
6698,natural_language_inference52,94,Representation - based features ( Emb ) :,Model and Features,Feature Extraction,natural_language_inference,52,11,1,0,,0.027108113,0,negative,0.000270334,0.001231659,0.055978724,9.02E-08,2.04E-06,0.000230233,0.000226213,7.92E-05,0.008194638,0.932809727,0.000905865,6.90E-05,2.28E-06
6699,natural_language_inference52,95,"To model the similarity between the text of each question ( Q ) , candidate answer ( A ) , and candidate justification ( J ) , we include a set of features that utilize distributed representations of the words found in each .",Model and Features,Feature Extraction,natural_language_inference,52,12,1,0,,2.58E-06,0,negative,6.72E-05,0.001633699,0.000111771,7.00E-07,2.60E-05,0.000389936,1.55E-05,0.000290873,0.013475559,0.98397756,8.90E-06,1.82E-06,5.38E-07
6700,natural_language_inference52,96,First we encode each by summing the vectors for each of their words .,Model and Features,Feature Extraction,natural_language_inference,52,13,1,0,,6.80E-06,0,negative,0.000160394,0.000772507,0.000451166,8.19E-08,3.46E-06,0.000130086,1.40E-05,7.30E-05,0.021949404,0.976434975,7.57E-06,2.90E-06,4.13E-07
6701,natural_language_inference52,97,"3 . We then compute sim ( Q , A ) , sim ( Q , J ) , and sim ( A , J ) us - ing cosine similarity .",Model and Features,Feature Extraction,natural_language_inference,52,14,1,0,,9.01E-07,0,negative,1.51E-05,1.38E-05,3.60E-05,6.96E-09,3.21E-07,3.63E-05,2.32E-06,1.45E-05,0.000112918,0.999766741,7.27E-07,1.22E-06,2.15E-08
6702,natural_language_inference52,98,"Using another vector representation of only the unique words in the justification , i.e. , the words that do not occur in either the question or the candidate answer , we also compute sim ( Q , uniqueJ ) and sim ( A , unique J ) .",Model and Features,Feature Extraction,natural_language_inference,52,15,1,0,,1.23E-06,0,negative,0.0001542,0.000188869,0.000100119,9.90E-09,1.18E-06,2.79E-05,7.87E-06,1.89E-05,0.000913287,0.99856757,2.94E-06,1.72E-05,4.42E-08
6703,natural_language_inference52,99,"To create a feature which captures the relationship between the question , answer , and justification , we take inspiration from TransE , a popular relation extraction framework .",Model and Features,Feature Extraction,natural_language_inference,52,16,1,0,,5.43E-06,0,negative,5.68E-05,0.004627816,0.000228551,7.40E-07,7.14E-05,0.000328498,2.16E-05,0.000203727,0.008930905,0.985507127,1.77E-05,4.24E-06,8.94E-07
6704,natural_language_inference52,100,"TransE is based on the premise that if two entities , e 1 and e 2 are related by a relation r , then a mapping into k dimensions , m ( x ) ?",Model and Features,Feature Extraction,natural_language_inference,52,17,1,0,,1.01E-06,0,negative,5.88E-05,0.000406564,0.000801623,6.42E-08,3.56E-06,4.56E-05,1.56E-05,2.64E-05,0.000769361,0.997661237,0.000199891,1.08E-05,5.03E-07
6705,natural_language_inference52,101,"R k can be learned such that m ( e 1 ) + m ( r ) ? m ( e 2 ) . Here , we modify this intuition for QA by suggesting that given the vectorized representations of the question , answer candidate , and justification above , Q + J ? A , i.e. , a question combined with a strong justification will point towards an answer .",Model and Features,Feature Extraction,natural_language_inference,52,18,1,0,,4.84E-07,0,negative,0.000284048,5.58E-05,5.13E-05,5.07E-08,6.08E-06,1.90E-05,8.62E-06,7.73E-06,4.42E-05,0.999490478,4.05E-06,2.86E-05,7.31E-08
6706,natural_language_inference52,102,"Here we model this as an explicit feature , the euclidean distance between Q + J and A , and hypothesize that as a consequence the model will learn to select passages that maximize the quality of the justifications .",Model and Features,Feature Extraction,natural_language_inference,52,19,1,0,,2.30E-07,0,negative,4.20E-05,0.000228749,3.88E-05,1.02E-07,4.75E-06,3.97E-05,2.40E-06,3.11E-05,0.00316778,0.996441973,1.23E-06,1.17E-06,9.59E-08
6707,natural_language_inference52,103,This makes a total of six features based on distributed representations .,Model and Features,Feature Extraction,natural_language_inference,52,20,1,0,,1.75E-05,0,negative,0.000220447,0.000166666,8.49E-05,6.01E-06,0.001133365,0.001293701,0.00011692,0.00010214,0.000134636,0.996725046,9.26E-07,1.28E-05,2.41E-06
6708,natural_language_inference52,104,Lexical overlap features ( LO ) :,Model and Features,Feature Extraction,natural_language_inference,52,21,1,0,,0.004150434,0,negative,0.000422148,0.000353775,0.035140455,6.19E-08,3.28E-06,0.000213131,0.000185749,3.94E-05,0.003053319,0.960496557,3.60E-05,5.47E-05,1.38E-06
6709,natural_language_inference52,105,"We additionally characterize each justification in terms of a simple set of explicit features designed to capture the size of the justification , as well as the lexical overlap ( and difference ) between the justification and the question and answer candidate .",Model and Features,Feature Extraction,natural_language_inference,52,22,1,0,,1.02E-06,0,negative,0.00010331,0.001605666,0.00011135,2.93E-07,7.34E-05,0.00014629,1.59E-05,6.89E-05,0.00411734,0.993750183,1.22E-06,5.90E-06,2.81E-07
6710,natural_language_inference52,106,"We include these five features : the proportion of question words , of answer words , and of the combined set of question and answer words that also appear in the justification ; the proportion of justification words that do not appear in either the question or the answer ; and the length of the justification in words .",Model and Features,Feature Extraction,natural_language_inference,52,23,1,0,,8.51E-07,0,negative,1.96E-05,3.83E-05,6.70E-06,3.32E-07,3.85E-05,0.000290766,1.02E-05,5.64E-05,6.34E-05,0.99947376,1.58E-07,1.79E-06,9.65E-08
6711,natural_language_inference52,107,4,Model and Features,Feature Extraction,natural_language_inference,52,24,1,0,,9.93E-08,0,negative,2.86E-06,1.15E-06,3.53E-07,1.56E-08,4.11E-07,2.38E-05,1.37E-06,6.99E-06,9.71E-06,0.999952766,2.21E-07,3.71E-07,1.94E-08
6712,natural_language_inference52,108,Semi- Lexicalized,Model and Features,,natural_language_inference,52,25,1,0,,0.00585422,0,negative,0.000352916,0.002782958,0.054010843,2.49E-06,7.52E-05,0.000723073,0.002866945,0.001367891,0.024839311,0.909166586,0.00082402,0.002913677,7.41E-05
6713,natural_language_inference52,109,Discourse features ( lexDisc ) :,Model and Features,Semi- Lexicalized,natural_language_inference,52,26,1,0,,0.024824353,0,negative,5.26E-05,8.36E-06,0.037286486,4.22E-08,1.50E-07,3.55E-06,0.000121112,1.77E-05,0.000148,0.956864548,4.47E-06,0.005485338,7.60E-06
6714,natural_language_inference52,110,"These features use the discourse structure of the justification text , which has been shown to be useful for QA .",Model and Features,Semi- Lexicalized,natural_language_inference,52,27,1,0,,0.000182321,0,negative,5.23E-05,4.71E-06,0.000376913,2.84E-08,2.26E-07,9.43E-07,3.85E-06,7.87E-06,6.62E-05,0.998743195,2.13E-07,0.000742961,5.76E-07
6715,natural_language_inference52,111,We use the discourse parser of to fragment the text into elementary discourse units ( EDUs ) and then recursively connect neighboring EDUs with binary discourse relations .,Model and Features,Semi- Lexicalized,natural_language_inference,52,28,1,0,,0.000541158,0,negative,1.90E-05,4.66E-05,0.001100188,5.23E-08,3.03E-07,1.86E-06,7.01E-06,3.53E-05,0.003372152,0.995330065,6.25E-07,8.47E-05,2.13E-06
6716,natural_language_inference52,112,"For each of the 18 possible relation labels , we create a set of semi-lexicalized discourse features that indicate the presence of a given discourse relation as well as whether or not the head and modifier texts contain words from the question and / or the answer .",Model and Features,Semi- Lexicalized,natural_language_inference,52,29,1,0,,1.97E-05,0,negative,4.80E-06,8.51E-06,2.48E-05,5.55E-08,5.29E-07,3.14E-06,5.33E-06,8.45E-05,7.93E-05,0.999727676,2.53E-08,6.09E-05,4.63E-07
6717,natural_language_inference52,113,"For example , for the question Q :",Model and Features,Semi- Lexicalized,natural_language_inference,52,30,1,0,,9.64E-07,0,negative,5.19E-07,8.58E-08,3.13E-06,6.71E-10,5.14E-09,9.10E-08,5.13E-07,1.40E-06,1.62E-06,0.999913954,3.61E-08,7.86E-05,3.17E-08
6718,natural_language_inference52,114,What makes water a good solvent ...?,Model and Features,Semi- Lexicalized,natural_language_inference,52,31,1,0,,1.74E-05,0,negative,2.11E-06,6.24E-07,3.44E-06,6.76E-08,2.35E-08,1.80E-06,3.15E-06,3.66E-05,1.81E-05,0.999859405,4.17E-07,7.35E-05,8.27E-07
6719,natural_language_inference52,115,"A : strong polarity , with a discourse - parsed justification [ Water is an efficient solvent ] e 1 [ because of this polarity . ]",Model and Features,Semi- Lexicalized,natural_language_inference,52,32,1,0,,1.67E-05,0,negative,2.61E-06,6.86E-07,1.87E-05,5.52E-08,6.03E-08,3.39E-06,5.72E-06,4.35E-05,2.05E-05,0.99979659,1.14E-07,0.000106989,9.96E-07
6720,natural_language_inference52,116,"e 2 , we create the semi-lexicalized feature Q cause A , because there is a Cause relation between EDUs e 1 and e 2 , e 1 overlaps with the question , and e 2 overlaps with the answer .",Model and Features,Semi- Lexicalized,natural_language_inference,52,33,1,0,,3.35E-06,0,negative,6.95E-06,5.18E-07,1.15E-05,3.02E-08,1.60E-07,7.20E-07,1.25E-06,7.48E-06,9.28E-06,0.999860485,2.51E-08,0.00010132,2.36E-07
6721,natural_language_inference52,117,"Since there are 18 possible discourse relation labels , and the prefix and suffix can be any of Q , A , QA or None , this creates a set of 288 indicator features .",Model and Features,Semi- Lexicalized,natural_language_inference,52,34,1,0,,2.38E-05,0,negative,8.50E-06,3.44E-06,7.50E-06,2.47E-07,1.29E-06,6.67E-06,7.42E-06,0.000138141,2.72E-05,0.999728954,1.28E-08,6.96E-05,1.02E-06
6722,natural_language_inference52,118,IR - based features ( IR ++ ):,Model and Features,Semi- Lexicalized,natural_language_inference,52,35,1,0,,0.405949774,0,negative,3.62E-05,1.79E-05,0.039049339,3.26E-08,1.76E-07,3.85E-06,0.000130063,2.33E-05,0.000417159,0.956867706,4.02E-06,0.00343826,1.20E-05
6723,natural_language_inference52,119,"Finally , we also use a set of four IR - based features which are assigned at the level of the answer candidate ( i.e. , these features are identical for each of the candidate justifications for that answer choice ) .",Model and Features,Semi- Lexicalized,natural_language_inference,52,36,1,0,,8.82E-05,0,negative,2.43E-05,2.33E-05,0.000219899,8.48E-08,8.30E-07,4.09E-06,1.32E-05,9.14E-05,0.00032629,0.999136176,2.99E-08,0.000158799,1.54E-06
6724,natural_language_inference52,120,"Using the same query method as described in Section 4.1 , for each question and answer candidate we retrieve a set of indexed documents .",Model and Features,Semi- Lexicalized,natural_language_inference,52,37,1,0,,4.58E-05,0,negative,1.41E-05,9.69E-06,0.000295909,6.60E-09,1.80E-07,3.35E-07,4.57E-06,5.60E-06,0.000130826,0.998973372,7.01E-08,0.000564818,5.13E-07
6725,natural_language_inference52,121,"Using the tf - idf based retrieval scores of these returned documents , s ( d i ) ford i ?",Model and Features,Semi- Lexicalized,natural_language_inference,52,38,1,0,,2.37E-06,0,negative,2.50E-06,2.12E-07,1.12E-05,5.25E-10,5.75E-09,6.43E-08,4.28E-07,1.47E-06,3.65E-06,0.999833385,6.98E-09,0.000147051,2.64E-08
6726,natural_language_inference52,122,"D , we rank the answer candidates using two methods :",Model and Features,Semi- Lexicalized,natural_language_inference,52,39,1,0,,0.000406042,0,negative,4.76E-05,1.75E-05,0.003874475,2.06E-08,2.85E-07,9.02E-07,1.46E-05,1.03E-05,0.000152397,0.994703872,2.60E-07,0.001176324,1.46E-06
6727,natural_language_inference52,123,"by the maximum retrieved document score for each candidate , and",Model and Features,Semi- Lexicalized,natural_language_inference,52,40,1,0,,3.38E-06,0,negative,3.47E-06,4.60E-07,1.25E-05,1.45E-09,9.95E-09,1.04E-07,5.57E-07,3.59E-06,2.31E-05,0.999876143,1.57E-08,8.00E-05,7.79E-08
6728,natural_language_inference52,124,by the weighted sum of all retrieved document scores 5 :,Model and Features,Semi- Lexicalized,natural_language_inference,52,41,1,0,,5.18E-06,0,negative,5.20E-06,6.81E-07,5.77E-05,5.86E-10,8.11E-09,7.13E-08,7.88E-07,1.76E-06,3.57E-05,0.999756223,1.36E-08,0.000141767,7.53E-08
6729,natural_language_inference52,125,"We repeat this process using an unboosted query as well , for a total of four rankings of the answer candidates .",Model and Features,Semi- Lexicalized,natural_language_inference,52,42,1,0,,5.39E-05,0,negative,1.90E-05,5.18E-06,6.56E-05,6.36E-09,1.57E-07,6.76E-07,1.50E-05,1.97E-05,1.87E-05,0.999312279,1.22E-08,0.000543421,4.15E-07
6730,natural_language_inference52,126,"We then use these rankings to make a set of four reciprocal rank features , IR ++ 0 , ... , IR ++",Model and Features,Semi- Lexicalized,natural_language_inference,52,43,1,0,,6.69E-06,0,negative,8.12E-06,2.85E-06,9.52E-05,3.50E-09,7.32E-08,4.50E-07,3.45E-06,8.26E-06,5.09E-05,0.999690308,8.06E-09,0.000140297,1.71E-07
6731,natural_language_inference52,127,"3 , for each answer candidate ( i.e. , IR ++ 0 = 1.0 for the top - ranked candidate in the first ranking , IR ++",Model and Features,Semi- Lexicalized,natural_language_inference,52,44,1,0,,3.06E-06,0,negative,9.66E-06,1.11E-06,6.19E-05,2.02E-09,3.20E-08,4.73E-07,5.77E-06,1.41E-05,1.52E-05,0.999648306,8.23E-09,0.000243246,1.99E-07
6732,natural_language_inference52,128,"0 = 0.5 for the next candidate , etc . )",Model and Features,Semi- Lexicalized,natural_language_inference,52,45,1,0,,1.43E-05,0,negative,5.40E-06,4.43E-07,5.59E-06,5.32E-09,2.10E-08,1.07E-06,3.76E-06,7.02E-05,9.66E-06,0.999808184,4.55E-09,9.53E-05,3.24E-07
6733,natural_language_inference52,129,Neural Network,Model and Features,,natural_language_inference,52,46,1,0,,0.052971504,0,negative,3.34E-05,0.001176165,0.004973882,2.96E-06,5.30E-05,0.001078425,0.002351245,0.003557508,0.12095064,0.864973128,0.000287745,0.00023111,0.000330826
6734,natural_language_inference52,130,"As shown in , the extracted features for each candidate justification are concatenated and passed into a fully - connected feed - forward neural network ( NN ) .",Model and Features,Neural Network,natural_language_inference,52,47,1,0,,4.55E-05,0,negative,1.40E-05,5.48E-05,0.000239538,2.15E-08,7.91E-07,5.48E-06,6.94E-06,2.68E-05,0.003614822,0.996030804,6.06E-08,5.22E-06,6.62E-07
6735,natural_language_inference52,131,The output layer is a single node representing the justification score .,Model and Features,Neural Network,natural_language_inference,52,48,1,0,,1.94E-05,0,negative,7.47E-06,4.01E-05,4.35E-05,8.65E-08,7.22E-07,2.74E-05,1.54E-05,0.000239567,0.004916431,0.994704765,9.70E-08,2.43E-06,2.11E-06
6736,natural_language_inference52,132,"We then use max - pooling over these scores to select the current best justification for the answer candidate , and use it s score as the score for the answer candidate itself .",Model and Features,Neural Network,natural_language_inference,52,49,1,0,,1.88E-06,0,negative,6.17E-06,1.36E-05,7.35E-05,1.27E-09,1.57E-07,1.11E-06,2.77E-06,5.57E-06,0.000295276,0.999597057,5.67E-09,4.75E-06,4.65E-08
6737,natural_language_inference52,133,"For training , the correct answer for a given Weighted sum was based on the IR scores used in the winning Kaggle system from user Cardal ( https://github. com/Cardal/Kaggle_AllenAIscience ) question is paired with each of the incorrect answers , and each are scored as above .",Model and Features,Neural Network,natural_language_inference,52,50,1,0,,3.84E-07,0,negative,3.43E-07,8.30E-07,1.65E-06,3.53E-09,6.21E-07,6.28E-06,3.73E-06,2.30E-05,3.64E-06,0.999957982,7.42E-10,1.86E-06,3.05E-08
6738,natural_language_inference52,134,We compute the pair - wise margin ranking loss for each training pair :,Model and Features,Neural Network,natural_language_inference,52,51,1,0,,1.87E-06,0,negative,4.52E-06,8.18E-06,6.86E-06,1.56E-09,9.60E-08,1.49E-06,2.40E-06,2.13E-05,5.17E-05,0.999897643,6.64E-09,5.73E-06,4.48E-08
6739,natural_language_inference52,135,"where F ( a + ) and F ( a ? ) are the model scores for a correct and incorrect answer candidate and m is the margin , and backpropagate the gradients .",Model and Features,Neural Network,natural_language_inference,52,52,1,0,,3.95E-07,0,negative,2.98E-06,1.13E-06,8.54E-06,1.09E-09,5.51E-08,1.02E-06,1.13E-06,6.38E-06,2.03E-05,0.999956438,2.15E-09,2.01E-06,3.09E-08
6740,natural_language_inference52,136,"At testing time , we use the trained model to score each answer choice ( again using the maximum justification score ) and select the highest - scoring .",Model and Features,Neural Network,natural_language_inference,52,53,1,0,,1.22E-06,0,negative,3.66E-06,1.62E-05,3.28E-05,1.37E-09,1.62E-07,1.76E-06,7.06E-06,1.99E-05,0.000145875,0.999766356,4.34E-09,6.17E-06,7.32E-08
6741,natural_language_inference52,137,"As we are interested in not only correctly answering questions , but also selecting valid justification for those answers , we keep track of the scores of all justifications and use this information to return the top k justifications for each answer choice .",Model and Features,Neural Network,natural_language_inference,52,54,1,0,,1.40E-06,0,negative,1.53E-06,3.59E-05,1.37E-05,2.82E-09,5.19E-07,1.21E-06,1.73E-06,9.11E-06,0.000272189,0.999661088,1.02E-08,2.97E-06,6.56E-08
6742,natural_language_inference52,138,These are evaluated along with the answer selection performance in Section 6 .,Model and Features,Neural Network,natural_language_inference,52,55,1,0,,3.75E-07,0,negative,3.79E-06,7.14E-07,5.94E-07,3.88E-10,8.27E-08,2.50E-07,8.07E-07,1.75E-06,1.94E-06,0.999978526,4.33E-10,1.15E-05,4.96E-09
6743,natural_language_inference52,139,Experiments,,,natural_language_inference,52,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
6744,natural_language_inference52,140,Data and Setup,Experiments,,natural_language_inference,52,1,1,0,,0.000605367,0,negative,2.57E-05,0.000104537,0.000243429,9.57E-06,7.11E-06,0.009428007,0.001331551,0.048073065,7.54E-05,0.940096411,0.000307255,0.000219304,7.86E-05
6745,natural_language_inference52,141,We evaluated our model on the set of 8th grade science questions that was provided by the Allen Institute for Artificial Intelligence ( AI2 ) for a recent Kaggle challenge .,Experiments,Data and Setup,natural_language_inference,52,2,1,0,,0.045585975,0,negative,3.20E-05,6.70E-05,0.000288351,6.93E-06,0.000109794,0.058355418,0.001860244,0.00468245,5.40E-06,0.934362995,6.03E-05,0.00015921,9.85E-06
6746,natural_language_inference52,142,"The training set contained 2,500 question , each with 4 answer candidates .",Experiments,Data and Setup,natural_language_inference,52,3,1,0,,0.044111718,0,negative,1.01E-05,1.28E-05,6.25E-05,4.78E-06,0.000131553,0.128750532,0.00070731,0.00674916,2.35E-06,0.863544427,5.79E-06,1.40E-05,4.73E-06
6747,natural_language_inference52,143,"For our test set , we used the 800 publicly - released questions that were used as the validation set in the actual evaluation .",Experiments,Data and Setup,natural_language_inference,52,4,1,0,,0.019337428,0,negative,1.35E-05,5.35E-06,3.62E-05,1.58E-06,0.000118698,0.021479953,0.00032411,0.001202248,4.93E-07,0.976784588,1.93E-06,3.02E-05,1.11E-06
6748,natural_language_inference52,144,"We tuned our model architectures and hyper-parameters on the training data using five - fold cross - validation ( training on 4 folds , validating on 1 ) .",Experiments,Data and Setup,natural_language_inference,52,5,1,0,,0.715272763,1,experimental-setup,1.27E-05,6.01E-06,6.85E-06,3.04E-06,4.34E-06,0.798813066,0.000558317,0.056882596,2.01E-06,0.143698752,1.25E-06,6.98E-06,4.06E-06
6749,natural_language_inference52,145,"During testing , we froze the model architecture and all hyperparameters and re-trained on all the training data , setting aside a random 15 % of training questions to facilitate early stopping .",Experiments,Data and Setup,natural_language_inference,52,6,1,0,,0.086653996,0,experimental-setup,0.00011212,4.04E-05,7.22E-05,3.45E-06,1.50E-05,0.550223651,0.001259973,0.080856177,1.21E-05,0.367354884,1.92E-06,3.99E-05,8.25E-06
6750,natural_language_inference52,146,Baselines,,,natural_language_inference,52,0,1,0,,0.002699267,0,negative,1.90E-05,5.96E-05,8.38E-05,2.15E-07,4.25E-07,0.000133907,0.000174591,0.001395164,4.67E-05,0.996483638,0.000841338,0.000759529,2.11E-06
6751,natural_language_inference52,147,"In addition to previous work , we compare our model against two strong IR baselines :",Baselines,Baselines,natural_language_inference,52,1,1,0,,0.175814872,0,baselines,1.18E-05,3.68E-06,0.930450785,1.82E-07,1.48E-07,1.44E-05,0.00029719,0.000139505,8.84E-07,0.0686987,5.21E-06,0.000374203,3.36E-06
6752,natural_language_inference52,148,IR Baseline :,Baselines,Baselines,natural_language_inference,52,2,1,1,baselines,0.653776588,1,baselines,3.31E-06,1.13E-07,0.994000791,1.22E-08,4.56E-09,1.98E-06,5.10E-05,1.95E-05,1.42E-07,0.005811738,5.13E-07,0.000110102,7.83E-07
6753,natural_language_inference52,149,"For this baseline , we rank answer candidates by the maximum tf .idf document retrieval score using an unboosted query of question and answer terms ( see Section 4.1 for retrieval details ) .",Baselines,Baselines,natural_language_inference,52,3,1,1,baselines,0.556726168,1,baselines,8.58E-07,3.16E-07,0.992361223,3.53E-09,3.25E-09,1.08E-06,2.04E-05,1.47E-05,2.26E-07,0.007575691,3.65E-07,2.49E-05,2.15E-07
6754,natural_language_inference52,150,IR ++ :,Baselines,Baselines,natural_language_inference,52,4,1,1,baselines,0.061559686,0,baselines,5.94E-05,1.11E-06,0.699147827,1.50E-06,1.63E-07,6.60E-05,0.000475658,0.00045307,4.01E-06,0.298399507,2.51E-05,0.00131987,4.68E-05
6755,natural_language_inference52,151,"This baseline uses the same architecture as the full model , as described in Section 4.3 , but with only the IR ++ feature group .",Baselines,Baselines,natural_language_inference,52,5,1,1,baselines,0.575308156,1,baselines,2.85E-07,2.50E-08,0.998943286,1.44E-09,7.40E-10,3.70E-07,1.21E-05,2.52E-06,5.10E-08,0.001034603,6.48E-08,6.51E-06,1.58E-07
6756,natural_language_inference52,152,Corpora,Baselines,,natural_language_inference,52,6,1,0,,0.07318309,0,baselines,4.42E-05,1.90E-06,0.751771451,2.21E-06,1.31E-06,0.000104013,0.007956031,0.000685149,1.53E-06,0.231005736,5.01E-05,0.008195127,0.000181213
6757,natural_language_inference52,153,For our pool of candidate justifications ( as well as the scores for our IR baselines ) we used the corpora that were cited as being most helpful to the top - performing systems of the Kaggle challenge .,Baselines,Corpora,natural_language_inference,52,7,1,0,,0.012561229,0,negative,6.57E-05,1.78E-06,0.090903805,9.91E-07,3.33E-06,0.000129044,0.001242679,0.000706155,1.16E-06,0.905122399,1.12E-06,0.001803074,1.88E-05
6758,natural_language_inference52,154,"These consisted of short , flash - card style texts gathered from two online resources : about 700K sentences from StudyStack 7 and 25K sentences from Quizlet 8 .",Baselines,Corpora,natural_language_inference,52,8,1,0,,0.039059388,0,negative,0.000173376,1.84E-06,0.125467583,5.38E-05,4.06E-05,0.000323215,0.002127243,0.000292573,2.13E-06,0.870365494,4.32E-06,0.000979992,0.000167774
6759,natural_language_inference52,155,"From these corpora , we use the top 50 sentences retrieved by the IR model as our set of candidate justifications .",Baselines,Corpora,natural_language_inference,52,9,1,0,,0.035332842,0,negative,6.69E-05,3.31E-06,0.091417151,4.60E-06,1.29E-05,0.000189001,0.001356778,0.001154144,2.44E-06,0.904954557,9.12E-07,0.000779117,5.83E-05
6760,natural_language_inference52,156,"All of our corpora were annotated using using the Stanford CoreNLP toolkit , the dependency parser of , and the discourse parser of .",Baselines,Corpora,natural_language_inference,52,10,1,0,,0.496295069,0,negative,8.62E-05,3.81E-06,0.127987125,4.35E-05,2.08E-05,0.001410006,0.004504226,0.002588208,5.32E-06,0.862412084,3.35E-06,0.000735558,0.000199785
6761,natural_language_inference52,157,"While our model is able to learn a set of embeddings , we found performance was improved when using pre-trained embeddings , and in this low - data domain , fixing these embeddings to not update during training substantially reduced the amount of model over-fitting .",Baselines,Corpora,natural_language_inference,52,11,1,0,,0.336549621,0,negative,0.011350127,6.14E-06,0.112909682,4.14E-06,2.22E-06,0.000105475,0.002467864,0.000977958,4.39E-06,0.619621517,1.38E-05,0.252356557,0.000180107
6762,natural_language_inference52,158,"In order to pretrain domain - relevant embeddings for our vocabulary , we used the documents from the StudyStack and Quizlet corpora , supplemented by the newly released Aristo MINI corpus ( December 2016 release ) 9 , which contains 1.2M science - related sentences from various web sources .",Baselines,Corpora,natural_language_inference,52,12,1,0,,0.189041382,0,negative,0.000102944,7.63E-06,0.212023101,2.05E-06,8.63E-06,0.00012843,0.00158794,0.001060707,3.45E-06,0.783376468,1.23E-06,0.001658458,3.90E-05
6763,natural_language_inference52,159,"The training was done using the word2 vec algorithm as implemented by , such that the context for each word in a sentence is composed of all the other words in the same sentence .",Baselines,Corpora,natural_language_inference,52,13,1,0,,0.735455516,1,negative,5.14E-05,2.05E-05,0.280302076,4.99E-06,9.91E-07,0.004738648,0.006656491,0.099915526,5.61E-05,0.607793175,4.62E-06,0.000204184,0.000251246
6764,natural_language_inference52,160,We used embeddings of size 50 as we did not see a performance improvement with higher dimensionality .,Baselines,Corpora,natural_language_inference,52,14,1,0,,0.774653422,1,negative,0.000298548,1.44E-05,0.102048137,9.30E-06,1.34E-06,0.010947426,0.019910145,0.189008717,2.66E-05,0.674679406,8.86E-06,0.002592348,0.000454831
6765,natural_language_inference52,161,Model Tuning,,,natural_language_inference,52,0,1,0,,0.231523542,0,negative,0.000346373,0.000772697,5.09E-05,2.86E-05,9.77E-06,0.002147661,0.00033068,0.038699396,0.00067276,0.955486447,0.001062786,0.000357991,3.40E-05
6766,natural_language_inference52,162,"The neural model was implemented in Keras using the Theano ( Theano Development Team , 2016 ) backend .",Model Tuning,Model Tuning,natural_language_inference,52,1,1,0,,0.565158687,1,hyperparameters,0.000230341,0.00037209,0.001169731,0.003196943,6.71E-05,0.363661746,0.003217735,0.369806798,0.00041869,0.257280148,0.00030184,1.42E-05,0.000262617
6767,natural_language_inference52,163,"For our feedforward component , we use a shallow neural network that we lightly tuned to have a single fullyconnected layer containing 10 nodes , glorot uniform initialization , a tanh activation , and an L2regularization of 0.1 .",Model Tuning,Model Tuning,natural_language_inference,52,2,1,0,,0.377187214,0,hyperparameters,5.95E-05,0.00026103,0.000142694,2.21E-05,4.79E-06,0.067264739,0.000566526,0.890143693,0.000140755,0.041330142,1.86E-05,6.12E-06,3.93E-05
6768,natural_language_inference52,164,"We trained with the RM - SProp optimizer , a learning rate of 0.001 , 100 epochs , a batch size of 32 , and early stopping with a patience of 5 epochs .",Model Tuning,Model Tuning,natural_language_inference,52,3,1,0,,0.816950945,1,hyperparameters,3.88E-05,9.69E-05,5.29E-05,4.74E-05,5.17E-06,0.124480296,0.000768607,0.838141817,4.30E-05,0.036247957,1.88E-05,6.56E-06,5.17E-05
6769,natural_language_inference52,165,Our loss function used a margin of 1.0 .,Model Tuning,Model Tuning,natural_language_inference,52,4,1,0,,0.074143606,0,hyperparameters,3.32E-05,0.000117659,4.15E-05,3.15E-06,1.50E-06,0.037947503,0.000136436,0.855232192,0.000105237,0.106353987,1.22E-05,3.96E-06,1.15E-05
6770,natural_language_inference52,166,"We experimented with burn - in , i.e. , using the best justification chosen by the IR model for the first mini-batches , but found that models without burn - in performed better , indicating that the model benefited from being able to select its own justification .",Model Tuning,Model Tuning,natural_language_inference,52,5,1,0,,0.104516155,0,negative,0.005509353,0.000103416,0.000273893,1.82E-06,8.18E-06,0.001485438,7.58E-05,0.015504918,2.87E-05,0.976293735,3.97E-05,0.000669055,5.98E-06
6771,natural_language_inference52,167,Results,,,natural_language_inference,52,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
6772,natural_language_inference52,168,"Rather than seeking to outperform all other systems at selecting the correct answer to a question , here we aimed to construct a system system that can produce substantially better justifications for why the answer choice is correct to a human user , without unduly sacrificing accuracy on the answer selection task .",Results,Results,natural_language_inference,52,1,1,0,,0.025413915,0,negative,0.013132889,0.001102202,0.027127088,0.000101873,0.000143628,0.000359768,0.001861404,0.001279124,0.000992672,0.851668238,0.001478651,0.099153767,0.001598696
6773,natural_language_inference52,169,"Accordingly , we evaluate our system both in terms of it 's ability to correctly answer questions ( Section 6.1 ) , as well as provide highquality justifications for those answers ( 6.2 ) .",Results,Results,natural_language_inference,52,2,1,0,,0.005111357,0,negative,0.004258308,6.45E-05,0.000651708,9.11E-06,2.49E-05,7.03E-05,0.000505329,0.000369385,3.07E-05,0.959387373,2.07E-05,0.034513603,9.41E-05
6774,natural_language_inference52,170,"Additionally , we perform an error analysis ( Section 6.3 ) , taking advantage of the insight the reranked justifications provide into what the model is learning .",Results,Results,natural_language_inference,52,3,1,0,,0.106763324,0,negative,0.087871306,0.00011628,0.002075258,3.61E-05,4.95E-05,0.000105008,0.000462921,0.000422227,0.000136536,0.857278262,1.04E-05,0.051308431,0.000127772
6775,natural_language_inference52,171,QA Performance,Results,,natural_language_inference,52,4,1,1,results,0.713729978,1,results,0.002133838,3.09E-06,0.00014596,1.07E-06,9.54E-07,1.78E-05,0.003435183,0.000106819,1.42E-06,0.210891648,8.06E-05,0.78294987,0.000231792
6776,natural_language_inference52,172,We evaluated the accuracy of our system as well as the baselines on the held - out 800 set of test questions .,Results,QA Performance,natural_language_inference,52,5,1,0,,0.00311879,0,negative,0.000333178,3.03E-07,1.22E-05,8.54E-08,1.35E-07,2.48E-06,1.70E-05,7.95E-06,3.94E-07,0.962807736,8.71E-07,0.036800184,1.75E-05
6777,natural_language_inference52,173,"Performance , measured in precision at 1 ( P@1 ) , is shown in Table 2 for both the validation ( i.e. , cross validation on training ) and test partitions .",Results,QA Performance,natural_language_inference,52,6,1,0,,0.051264196,0,results,0.001422818,1.28E-07,1.98E-05,7.87E-08,4.32E-08,1.33E-06,7.74E-05,4.58E-06,1.18E-07,0.249566142,1.48E-06,0.748883038,2.30E-05
6778,natural_language_inference52,174,"Because NNs are sensitive to initialization , each experimental result shown is the average performance across five runs , each using different random seeds .",Results,QA Performance,natural_language_inference,52,7,1,0,,0.001930434,0,negative,0.000148775,1.85E-07,8.07E-06,1.32E-07,5.29E-08,4.51E-06,1.15E-05,2.27E-05,6.19E-07,0.988091362,6.35E-07,0.011693463,1.80E-05
6779,natural_language_inference52,175,"The best performing baseline on the validation data was a model using only IR ++ features ( line 3 ) , but its performance dropped substantially when evaluated on test due to the failure of several random seed initializations to learn .",Results,QA Performance,natural_language_inference,52,8,1,0,,0.842471009,1,results,0.002417817,3.33E-07,0.000301921,3.80E-07,1.49E-07,7.06E-06,0.000240027,1.35E-05,4.64E-07,0.188849967,4.62E-06,0.807994449,0.000169285
6780,natural_language_inference52,176,"For this reason , we assessed significance of our model combinations with respect to both the IR baseline as well as the IR ++ ( indicated by * and s , respectively ) .",Results,QA Performance,natural_language_inference,52,9,1,0,,0.00066067,0,negative,0.000667813,3.01E-07,2.98E-05,1.06E-07,8.23E-08,3.20E-06,4.50E-06,1.17E-05,1.43E-06,0.990301201,3.01E-07,0.008972718,6.85E-06
6781,natural_language_inference52,177,"Our full model that combines IR ++ , lexical overlap , discourse , and embeddings - based features , has a P@1 of 53.3 % ( line 7 ) , an absolute gain of 6.3 % over the strong IR baseline despite using the same background knowledge .",Results,QA Performance,natural_language_inference,52,10,1,1,results,0.927619587,1,results,0.003359678,1.02E-07,4.81E-05,2.05E-07,5.61E-08,1.27E-06,0.000168156,3.62E-06,1.50E-07,0.024963315,7.82E-07,0.971222114,0.000232485
6782,natural_language_inference52,178,Comparison to Previous Work :,Results,QA Performance,natural_language_inference,52,11,1,0,,0.089094409,0,negative,0.000979919,7.22E-07,0.000754002,1.01E-07,7.87E-08,3.15E-06,5.52E-05,8.45E-06,1.55E-06,0.83325665,5.04E-06,0.16489846,3.67E-05
6783,natural_language_inference52,179,"We compared our performance against another model that achieves state of the art performance on a different set of 8th grade science questions , TUPLE - INF ( T+T ' ) .",Results,QA Performance,natural_language_inference,52,12,1,0,,0.022571547,0,negative,0.000866696,2.14E-06,0.000381184,2.08E-07,3.43E-07,4.56E-06,4.78E-05,1.26E-05,2.38E-06,0.86044017,4.11E-06,0.13818605,5.18E-05
6784,natural_language_inference52,180,TUPLEINF ( T+T ' ) uses Integer Linear Programming to find support for questions via tuple representations of KB sentences 10 .,Results,QA Performance,natural_language_inference,52,13,1,0,,0.072050842,0,negative,0.006508297,8.46E-06,0.112390412,6.67E-06,2.14E-06,5.49E-05,0.000308575,5.54E-05,3.57E-05,0.661394604,0.000147019,0.214972747,0.004115112
6785,natural_language_inference52,181,"On our test data , TUPLEINF ( T+T ' ) achieves 46.17 % P@1 ( line 5 ) .",Results,QA Performance,natural_language_inference,52,14,1,0,,0.862669693,1,results,0.004852941,6.76E-08,1.43E-05,8.98E-08,3.39E-08,1.06E-06,0.000168282,4.16E-06,5.62E-08,0.03506971,3.42E-07,0.959778113,0.000110864
6786,natural_language_inference52,182,"As this model is independent of an IR component , we compare its performance against our full system without the IR - based features ( line 6 ) , whose performance is 48.66 % P@1 , an absolute improvement of 2.49 % P@1 ( 5.4 % relative ) despite our unstructured text inputs and the far smaller size of our knowledge base ( three orders of magnitude ) .",Results,QA Performance,natural_language_inference,52,15,1,0,,0.87917896,1,negative,0.005819121,3.92E-07,0.000115061,8.25E-08,1.16E-07,1.19E-06,5.54E-05,3.92E-06,6.70E-07,0.563786915,5.55E-07,0.430197077,1.95E-05
6787,natural_language_inference52,183,also tackle the AI2 Kaggle question set with an approach that learns alignments between questions and structured and semistructured KB data .,Results,QA Performance,natural_language_inference,52,16,1,1,results,0.002055324,0,negative,0.002065408,5.13E-07,0.001181907,3.37E-06,8.63E-07,1.55E-05,5.74E-05,9.94E-06,2.99E-06,0.881811772,1.46E-05,0.112723717,0.002112085
6788,natural_language_inference52,184,"They use only the training questions ( splitting them into training , validation , and testing partitions ) , supplemented by questions found in online study guides , and report an accuracy of 47.84 % .",Results,QA Performance,natural_language_inference,52,17,1,0,,0.004079361,0,negative,0.003999153,5.48E-07,0.00017663,6.09E-07,9.30E-07,4.26E-06,2.13E-05,8.40E-06,1.27E-06,0.907504551,8.66E-07,0.088190725,9.08E-05
6789,natural_language_inference52,185,"By way of a loose comparison ( since we are evaluating on different data partitions ) , our model has approximately 5 % higher performance despite our simpler set of features and unstructured KB .",Results,QA Performance,natural_language_inference,52,18,1,1,results,0.688322315,1,results,0.002249941,2.46E-08,3.79E-06,5.16E-08,1.69E-08,4.17E-07,9.16E-05,1.58E-06,3.01E-08,0.042281459,1.33E-07,0.955296324,7.46E-05
6790,natural_language_inference52,186,We also compare our model to our implementation of the basic Deep - Averaged Network ( DAN ) Architecture of .,Results,QA Performance,natural_language_inference,52,19,1,0,,0.178888008,0,negative,0.000597734,1.80E-06,0.000337725,4.68E-07,2.89E-07,1.14E-05,3.87E-05,3.97E-05,6.20E-06,0.984579492,1.23E-06,0.014328477,5.68E-05
6791,natural_language_inference52,187,"We used the same 50 - dimensional embeddings in both models , so with the reduced embedding dimension , we re-duced the size of each of the DAN dense layer to 50 as well .",Results,QA Performance,natural_language_inference,52,20,1,0,,0.297112055,0,negative,0.01429686,3.47E-05,0.000370065,5.68E-05,1.03E-05,0.001661555,0.001856733,0.010495696,0.000143187,0.920395107,9.55E-06,0.035244111,0.015425314
6792,natural_language_inference52,188,"For simplicity , we also did not implement their word - dropout , a feature that they reported as providing a performance boost .",Results,QA Performance,natural_language_inference,52,21,1,0,,0.000102152,0,negative,0.005006564,3.77E-07,6.54E-05,1.75E-06,3.75E-07,8.22E-06,4.54E-06,1.17E-05,3.98E-06,0.987004965,2.09E-07,0.007858624,3.33E-05
6793,natural_language_inference52,189,"Using this implementation , the performance on the test set was 31.50 % P@1 .",Results,QA Performance,natural_language_inference,52,22,1,0,,0.591678021,1,negative,0.005278669,1.25E-06,5.63E-05,1.56E-06,9.72E-07,3.29E-05,0.000377444,0.000104068,3.23E-06,0.662924681,1.28E-06,0.329885724,0.001331914
6794,natural_language_inference52,190,"To help with observed overfitting , we tried removing the dense layers and received a small boost to 32. 52 % P@1 ( line 4 ) .",Results,QA Performance,natural_language_inference,52,23,1,0,,0.342086643,0,negative,0.332869922,2.07E-06,9.95E-05,6.79E-06,1.53E-06,2.25E-05,0.000199734,8.41E-05,1.08E-05,0.375837516,8.30E-07,0.289975824,0.000888934
6795,natural_language_inference52,191,"The lower performance of their model , which relies exclusively on latent representations of the data , underscores the benefit of including explicit features alongside latent features in a deep - learning approach for this domain 11 .",Results,QA Performance,natural_language_inference,52,24,1,0,,0.002719408,0,negative,0.001516698,1.77E-07,6.77E-05,1.16E-07,5.40E-08,1.73E-06,9.30E-06,4.08E-06,6.15E-07,0.915846904,1.19E-06,0.082497941,5.35E-05
6796,natural_language_inference52,192,"In comparison to other systems that competed in the Kaggle challenge , our system comes in in 7th place out of 170 competitors ( top 4 % ) .",Results,QA Performance,natural_language_inference,52,25,1,1,results,0.812397309,1,results,0.006245048,1.06E-07,3.25E-05,1.17E-06,3.34E-07,2.16E-06,0.00014587,3.25E-06,2.46E-07,0.115607476,3.40E-07,0.876989773,0.000971724
6797,natural_language_inference52,193,"12 Compared with the systems which disclosed their methods , we use a subset of their corpora and substantially less hyperparameter tuning , and yet we achieve competitive results .",Results,QA Performance,natural_language_inference,52,26,1,0,,0.000602804,0,negative,0.00126595,5.71E-08,1.58E-05,3.80E-07,1.80E-07,2.88E-06,7.93E-05,6.87E-06,1.28E-07,0.61475182,2.20E-07,0.383567124,0.000309323
6798,natural_language_inference52,194,Feature Ablation :,Results,QA Performance,natural_language_inference,52,27,1,0,,0.475371557,0,negative,0.05189177,1.59E-06,0.004245025,2.93E-06,8.98E-07,7.76E-06,5.46E-05,8.78E-06,1.64E-05,0.667727643,3.80E-06,0.275466693,0.000572136
6799,natural_language_inference52,195,"To evaluate the contribution of the individual feature groups , we additionally performed an ablation experiment ( see ) .",Results,QA Performance,natural_language_inference,52,28,1,0,,0.006080026,0,negative,0.02076026,4.81E-07,7.72E-05,2.88E-07,4.93E-07,1.84E-06,9.12E-06,5.86E-06,2.06E-06,0.913845135,1.02E-07,0.065273792,2.34E-05
6800,natural_language_inference52,196,"Each of our ablated models performed significantly better than the IR baseline on the validation set , including our simplest model , IR ++ + LO .",Results,QA Performance,natural_language_inference,52,29,1,0,,0.551854013,1,results,0.003149466,4.82E-08,6.54E-06,4.08E-07,6.16E-08,2.57E-06,0.000232354,8.05E-06,9.69E-08,0.063307632,2.07E-07,0.932778348,0.000514216
6801,natural_language_inference52,197,Justification Performance,Results,,natural_language_inference,52,30,1,1,results,0.002887861,0,negative,0.002946541,2.92E-06,0.000280164,3.52E-06,8.33E-06,2.48E-05,0.000927467,0.000148226,5.60E-06,0.885045265,1.41E-06,0.110029176,0.000576598
6802,natural_language_inference52,198,"One of our key claims is that our approach addresses the related , but more challenging problem of performing explainable question answering , i.e. , providing a high - quality , compelling justification for the chosen answer .",Results,Justification Performance,natural_language_inference,52,31,1,0,,0.000186953,0,negative,0.001256134,7.44E-07,8.63E-05,2.52E-06,3.68E-07,6.84E-06,2.78E-05,1.15E-05,4.38E-06,0.906687944,6.53E-06,0.088930787,0.002978212
6803,natural_language_inference52,199,"To evaluate this claim , we evaluated a random set of 100 test questions that both the IR baseline and our full system answered correctly .",Results,Justification Performance,natural_language_inference,52,32,1,0,,2.00E-05,0,negative,0.000203325,8.50E-08,7.81E-06,3.14E-08,2.38E-07,7.78E-07,6.39E-06,3.67E-06,1.28E-07,0.992363559,1.54E-08,0.007391652,2.23E-05
6804,natural_language_inference52,200,"For each question , we assessed the quality of each of the top five justifications .",Results,Justification Performance,natural_language_inference,52,33,1,0,,8.48E-05,0,negative,0.000188572,1.22E-07,1.47E-05,7.90E-08,1.59E-07,1.91E-06,5.45E-06,9.04E-06,6.70E-07,0.9954127,2.72E-08,0.004305871,6.07E-05
6805,natural_language_inference52,201,"For IR , these were the highest - scoring retrieved documents , and for our system , these were 11 Another difference between our system and that of the DAN baseline is our usage of a text justification .",Results,Justification Performance,natural_language_inference,52,34,1,0,,6.70E-05,0,negative,0.000239952,8.73E-08,7.20E-05,3.01E-08,1.39E-07,1.04E-06,5.05E-06,3.11E-06,4.23E-07,0.992006467,2.69E-08,0.007640592,3.11E-05
6806,natural_language_inference52,202,"However , we suspect this difference is not the source of the performance difference : see , where a variant of the DAN baseline that included an averaged representation of a justification alongside the averaged representations of the question and answer failed to show a performance increase .",Results,Justification Performance,natural_language_inference,52,35,1,0,,2.42E-05,0,negative,0.001789769,1.90E-08,9.28E-06,1.15E-08,1.65E-08,3.34E-07,1.49E-05,1.56E-06,7.26E-08,0.86127407,2.54E-08,0.13687907,3.09E-05
6807,natural_language_inference52,203,Based on the public leaderboard ( https://www.kaggle.,Results,Justification Performance,natural_language_inference,52,36,1,0,,2.27E-05,0,negative,0.001042104,9.93E-08,0.000178058,4.44E-07,3.73E-07,1.45E-06,6.39E-06,3.11E-06,9.78E-07,0.980105124,2.93E-08,0.018547458,0.000114378
6808,natural_language_inference52,204,com/c/the-allen-ai-science-challenge/leaderboard ).,Results,Justification Performance,natural_language_inference,52,37,1,0,,6.29E-05,0,negative,0.001635487,1.29E-07,0.000144753,4.40E-05,1.40E-06,2.34E-05,1.98E-05,1.61E-05,3.14E-06,0.986910145,9.10E-08,0.007063983,0.004137567
6809,natural_language_inference52,205,The best scoring submission had an accuracy of 59.38 % .,Results,Justification Performance,natural_language_inference,52,38,1,0,,0.085978687,0,negative,0.001618586,3.98E-08,3.83E-05,5.35E-07,3.72E-07,3.21E-06,0.000147317,6.24E-06,1.82E-07,0.634455971,6.06E-08,0.360454374,0.003274789
6810,natural_language_inference52,206,"Note that for the systems that participated , this set served as validation while for us it was test , and thus it is likely that these scores are slightly overfitted to this dataset , but for us it was blind .",Results,Justification Performance,natural_language_inference,52,39,1,0,,2.57E-06,0,negative,6.95E-05,4.39E-09,2.51E-06,1.93E-08,2.81E-08,2.76E-07,9.74E-07,8.22E-07,2.55E-08,0.996953793,1.53E-09,0.002964548,7.49E-06
6811,natural_language_inference52,207,"As such this is a conservative comparison , and in reality the difference is likely to be smaller .",Results,Justification Performance,natural_language_inference,52,40,1,0,,2.41E-06,0,negative,9.73E-05,1.83E-08,1.11E-05,1.26E-08,1.82E-08,3.44E-07,1.61E-06,2.00E-06,2.00E-07,0.996986803,7.54E-09,0.002886126,1.44E-05
6812,natural_language_inference52,208,Question Q : Scientists use ice cores to help predict the impact of future atmospheric changes on climate .,Results,Justification Performance,natural_language_inference,52,41,1,0,,4.44E-05,0,negative,0.000188083,1.19E-07,1.48E-05,1.35E-06,6.81E-08,6.90E-06,1.42E-05,2.40E-05,1.76E-06,0.99327492,8.56E-07,0.004605524,0.001867428
6813,natural_language_inference52,209,Which property of ice cores do these scientists use ?,Results,Justification Performance,natural_language_inference,52,42,1,0,,6.30E-07,0,negative,9.47E-05,1.96E-08,4.50E-06,2.34E-07,1.99E-08,1.45E-06,1.62E-06,4.79E-06,3.49E-07,0.998641791,2.41E-08,0.001150767,9.97E-05
6814,natural_language_inference52,210,A : The composition of ancient materials trapped in air bubbles,Results,Justification Performance,natural_language_inference,52,43,1,0,,0.000319628,0,negative,0.00194046,5.94E-07,0.000145725,3.47E-06,2.73E-07,1.32E-05,0.000118455,4.30E-05,1.06E-05,0.934793202,3.78E-06,0.041854627,0.02107265
6815,natural_language_inference52,211,Rating,Results,,natural_language_inference,52,44,1,0,,0.010509526,0,negative,0.001287899,3.52E-06,0.000122207,2.00E-06,3.70E-06,6.07E-05,0.001112328,0.000990213,1.17E-05,0.948695179,7.03E-07,0.046232016,0.00147786
6816,natural_language_inference52,212,Example Justification,Results,,natural_language_inference,52,45,1,0,,7.18E-05,0,negative,0.000615268,1.95E-06,8.98E-05,1.56E-06,3.85E-06,2.10E-05,0.00032394,0.000253945,5.59E-06,0.977536151,5.40E-07,0.020429909,0.000716433
6817,natural_language_inference52,213,Good,Results,,natural_language_inference,52,46,1,0,,0.007043183,0,negative,0.004619286,1.40E-06,0.000308952,3.90E-06,3.83E-06,4.46E-05,0.00237723,0.000316768,4.40E-06,0.754847316,5.85E-07,0.235252325,0.002219388
6818,natural_language_inference52,214,Ice cores : cylinders of ice that scientist use to study trapped atmospheric gases and particles frozen within the ice in air bubbles,Results,Good,natural_language_inference,52,47,1,0,,0.003954193,0,negative,8.42E-05,7.14E-06,0.000328976,3.64E-06,9.47E-06,5.14E-05,6.45E-05,0.000104871,1.27E-05,0.998098061,3.10E-06,0.001174405,5.75E-05
6819,natural_language_inference52,215,Half,Results,,natural_language_inference,52,48,1,0,,0.001448047,0,negative,0.003212369,1.79E-06,0.000817802,1.58E-06,4.31E-06,3.10E-05,0.000773689,0.000146856,1.01E-05,0.957970946,2.32E-07,0.036024929,0.001004384
6820,natural_language_inference52,216,Ice core : sample from the accumulation of snow and ice over many years that have recrystallized and have trapped air bubbles from previous time periods Topical Vesicular texture formation [ has ] trapped air bubbles .,Results,Half,natural_language_inference,52,49,1,0,,2.27E-05,0,negative,0.000173738,8.98E-08,0.000193077,4.51E-07,2.17E-07,4.78E-06,7.90E-06,1.74E-05,1.20E-06,0.990937018,2.85E-08,0.004803197,0.003860914
6821,natural_language_inference52,217,Offtopic,Results,,natural_language_inference,52,50,1,0,,0.004763895,0,negative,0.061611805,3.63E-05,0.031998671,9.00E-05,4.51E-05,0.000284814,0.005955019,0.000988511,0.000351484,0.597410252,5.22E-06,0.262009819,0.039213047
6822,natural_language_inference52,218,Physical change : change during which some properties of material change but ...:,Results,Offtopic,natural_language_inference,52,51,1,0,,6.92E-06,0,negative,0.000488499,7.65E-08,0.00021164,1.15E-06,5.96E-08,2.45E-06,8.09E-06,1.15E-05,2.58E-06,0.978755099,4.36E-08,0.017823485,0.002695374
6823,natural_language_inference52,219,Example justifications from the our model and their associated ratings .,Results,Offtopic,natural_language_inference,52,52,1,0,,1.10E-06,0,negative,2.05E-05,7.48E-09,5.39E-06,1.97E-08,6.65E-09,4.56E-07,2.35E-06,5.48E-06,9.16E-08,0.991312226,3.55E-09,0.008390717,0.000262728
6824,natural_language_inference52,220,Model,,,natural_language_inference,52,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
6825,natural_language_inference52,221,"Good@1 Good@5 NDC G@5 IR Baseline 0.52 0.64 0.55 Our Approach 0.61 0.74 0.62 * *: Percentage of questions that have at least one good justification within the top 1 ( Good@1 ) and the top 5 ( Good@5 ) justifications , as well as the normalized discounted cumulative gain at 5 ( NDCG@5 ) of the ranked justifications .",Model,Model,natural_language_inference,52,1,1,0,,0.004248636,0,negative,0.000185098,0.00021971,0.000270454,1.28E-05,1.10E-05,0.000889267,0.001699665,0.006404304,0.002221455,0.980645416,0.00039061,0.006941522,0.00010873
6826,natural_language_inference52,222,Significance indicated as in the top - scoring justifications as re-ranked by our model .,Model,Model,natural_language_inference,52,2,1,0,,0.000690733,0,negative,8.68E-05,9.73E-05,7.92E-05,3.93E-06,4.77E-06,0.000295184,0.000167232,0.002217255,0.003710303,0.992862376,4.20E-05,0.000415472,1.81E-05
6827,natural_language_inference52,223,"Each of these justifications was composed of a single sentence from our corpus , though a future version could use multi-sentence passages , or aggregate several sentences together , as in .",Model,Model,natural_language_inference,52,3,1,0,,0.00044282,0,negative,0.003753329,5.48E-05,0.000826456,5.63E-05,0.000399499,0.000163081,0.00024918,0.000125668,0.000395721,0.991073905,7.29E-06,0.002884146,1.06E-05
6828,natural_language_inference52,224,"Following the methodology of , each justification received a rating of either Good ( if the connection between the question and correct answer was fully covered ) , Half ( if there was a missing link ) , Topical ( if the justification was simply of the right topic ) , or Off - Topic ( if the justification was completely unrelated to the question ) .",Model,Model,natural_language_inference,52,4,1,0,,0.001528696,0,negative,5.43E-05,6.07E-05,6.38E-05,2.04E-06,8.17E-06,0.000182877,6.47E-05,0.001051215,0.001773669,0.996474107,6.50E-06,0.000253117,4.86E-06
6829,natural_language_inference52,225,Examples of each rating are provided in .,Model,Model,natural_language_inference,52,5,1,0,,0.00026166,0,negative,3.97E-05,8.89E-06,1.19E-05,1.09E-06,4.08E-06,2.67E-05,1.63E-05,0.000134659,0.000166095,0.999385581,1.69E-06,0.000202118,1.07E-06
6830,natural_language_inference52,226,Results of this analysis are shown using three evaluation metrics in .,Model,Model,natural_language_inference,52,6,1,0,,0.00341146,0,negative,0.000298564,1.88E-05,3.89E-05,1.41E-07,1.21E-06,7.39E-06,2.91E-05,7.16E-05,0.000111139,0.993127327,3.80E-06,0.006291071,8.77E-07
6831,natural_language_inference52,227,"The first two columns show the percentage of questions which had a Good justification at position 1 ( Good@1 ) , and within the top 5 ( Good@5 ) .",Model,Model,natural_language_inference,52,7,1,0,,0.004008971,0,negative,5.46E-05,4.15E-05,5.61E-05,1.16E-06,4.27E-06,9.55E-05,0.000104897,0.000680291,0.000705532,0.997380659,8.62E-06,0.000861025,5.93E-06
6832,natural_language_inference52,228,"Note that 61 % of the top - ranked justifications from our system were rated as Good as compared to 52 % from the IR baseline ( a gain of 9 % ) , despite the systems using identical corpora .",Model,Model,natural_language_inference,52,8,1,1,results,0.114670954,0,negative,0.014609617,5.22E-05,0.000280681,1.00E-05,6.65E-05,6.87E-05,0.001919505,0.000363601,0.000100955,0.515444423,1.86E-05,0.466999862,6.54E-05
6833,natural_language_inference52,229,We also evaluated the justification ratings using normalized discounted cumulative gain at 5 ( NDCG@5 ) ( as formulated in Manning et al . .,Model,Model,natural_language_inference,52,9,1,0,,0.015841631,0,negative,0.000203564,0.000406958,0.000966946,6.74E-07,1.41E-05,0.000107259,0.000237513,0.000961595,0.001787774,0.993366566,5.65E-06,0.001937116,4.31E-06
6834,natural_language_inference52,230,Contribution of Learning to Rerank Justifications :,Model,Model,natural_language_inference,52,10,1,0,,0.029690646,0,negative,0.000914571,0.001066062,0.004317532,1.82E-05,2.13E-05,0.000159068,0.002634399,0.000817409,0.005683877,0.910159873,0.027222055,0.04644928,0.000536404
6835,natural_language_inference52,231,"The main assertion of this work is that through learning to rank answers and justifications for those answer candidates in an end - to - end manner , we both answer questions correctly and provide compelling justifications as to why the answer is correct .",Model,Model,natural_language_inference,52,11,1,0,,0.071124671,0,negative,0.001571363,0.02834922,0.003826229,0.000239193,0.000628417,0.000497112,0.001039432,0.003661703,0.068172859,0.857293753,0.011799115,0.022017586,0.00090402
6836,natural_language_inference52,232,"To confirm that this is the case , we also ran a version of our system that does not rerank justifications , but uses the top - ranked justification retrieved by IR .",Model,Model,natural_language_inference,52,12,1,0,,0.007274406,0,negative,0.000628473,0.000378297,0.003825895,2.17E-06,6.73E-05,9.06E-05,0.000557001,0.000366376,0.001849133,0.984100266,1.18E-05,0.00810916,1.35E-05
6837,natural_language_inference52,233,"This configuration dropped our performance on test to 48.7 % P@1 , a decrease of 4.6 % , and we additionally lose all justification improvements from our system ( see Section 6.2 ) , demonstrating that learning this reranking is key to our approach .",Model,Model,natural_language_inference,52,13,1,0,,0.333866888,0,negative,0.164148414,0.000315022,0.001456262,3.35E-05,0.000176003,0.000228758,0.00128539,0.000827601,0.00239101,0.717137495,1.72E-05,0.111863537,0.000119869
6838,natural_language_inference52,234,"Additionally , we tracked the number of times a new justification was chosen by the model as it trained .",Model,Model,natural_language_inference,52,14,1,0,,0.018275067,0,negative,0.000114092,0.000628769,0.000272538,3.97E-06,3.57E-05,0.000147175,9.05E-05,0.00140835,0.024268446,0.972723813,7.67E-06,0.000285557,1.34E-05
6839,natural_language_inference52,235,"We found that our system converges to a stable set of justifications during training , shown in .",Model,Model,natural_language_inference,52,15,1,0,,0.232081031,0,negative,0.007575561,0.000148899,0.000149021,3.91E-06,2.85E-05,0.000108494,0.000752747,0.001193681,0.000699327,0.87183779,1.71E-05,0.117437783,4.72E-05
6840,natural_language_inference52,236,Error Analysis,Model,,natural_language_inference,52,16,1,0,,0.002682682,0,negative,0.000337083,1.75E-05,5.33E-05,4.79E-05,3.47E-05,0.000222841,0.000292018,0.00044663,0.000414464,0.996039371,1.69E-05,0.00202587,5.14E-05
6841,natural_language_inference52,237,"To better understand the limitations of our current system , we performed an error analysis of 30 incorrectly answered questions .",Model,Error Analysis,natural_language_inference,52,17,1,0,,0.00023447,0,negative,0.0004196,4.29E-05,3.89E-05,1.15E-05,0.011786553,0.000123285,7.58E-05,4.15E-06,7.08E-06,0.987477579,3.54E-06,6.97E-06,2.16E-06
6842,natural_language_inference52,238,We examined the top 5 justifications returned for both the correct and chosen answers .,Model,Error Analysis,natural_language_inference,52,18,1,0,,0.000118205,0,negative,2.60E-05,5.75E-06,6.95E-06,7.82E-07,0.000242139,0.000403913,4.00E-05,1.39E-05,4.04E-06,0.999254721,3.55E-07,8.04E-07,5.24E-07
6843,natural_language_inference52,239,"Notably , 50 % of the questions analyzed had one or more good justifications in the top 5 returned by our system , but for a variety of reasons , summarized in , the system incorrectly ranked another justification higher .",Model,Error Analysis,natural_language_inference,52,19,1,0,,2.09E-05,0,negative,0.000331963,7.67E-07,3.90E-06,1.90E-05,0.001869835,0.00015284,2.35E-05,1.19E-06,3.43E-07,0.997592887,5.20E-07,2.17E-06,1.01E-06
6844,natural_language_inference52,240,"The table shows that the most common form of error was the system 's preference for short justifications with a large degree of lexical overlap with the question and answer choice itself , shown by the example in .",Model,Error Analysis,natural_language_inference,52,20,1,0,,4.20E-05,0,negative,0.000265553,1.46E-06,5.86E-06,4.23E-06,0.000801639,0.000116923,3.29E-05,1.60E-06,6.76E-07,0.998764533,5.76E-07,3.27E-06,7.47E-07
6845,natural_language_inference52,241,The effect was magnified when the correct answer required more explanation to connect the question to the answer .,Model,Error Analysis,natural_language_inference,52,21,1,0,,0.004214494,0,negative,0.031721449,0.000105656,7.29E-05,2.13E-05,0.00055201,0.001022761,0.000614809,9.86E-05,0.000221444,0.965473968,7.15E-06,6.55E-05,2.25E-05
6846,natural_language_inference52,242,This suggests that the system has learned that generally many unmatched words are indicative of an incorrect answer .,Model,Error Analysis,natural_language_inference,52,22,1,0,,2.03E-06,0,negative,9.61E-05,2.60E-06,7.53E-06,3.83E-08,4.21E-06,1.65E-05,6.78E-06,2.02E-06,1.62E-05,0.999846424,4.64E-07,1.11E-06,8.81E-08
6847,natural_language_inference52,243,"While this may typically be true , extending the system to be able to prefer the opposite with certain types of questions would potentially help with these errors .",Model,Error Analysis,natural_language_inference,52,23,1,0,,1.42E-06,0,negative,7.07E-05,9.26E-07,7.93E-07,2.84E-07,5.28E-06,4.31E-05,7.71E-06,3.42E-06,2.83E-06,0.999862376,1.52E-06,8.23E-07,2.20E-07
6848,natural_language_inference52,244,Type :,Model,Error Analysis,natural_language_inference,52,24,1,0,,1.58E-05,0,negative,2.38E-05,4.44E-06,1.60E-05,2.02E-07,3.95E-06,9.60E-05,2.84E-05,1.02E-05,7.59E-05,0.999735915,3.28E-06,9.97E-07,8.87E-07
6849,natural_language_inference52,245,Complex inference required Question : Mr. Harris mows his lawn twice each month .,Model,Error Analysis,natural_language_inference,52,25,1,0,,2.76E-06,0,negative,2.24E-05,3.15E-06,7.22E-06,1.52E-05,9.22E-05,0.000193986,7.75E-05,6.08E-06,7.02E-06,0.999414376,0.000150255,1.40E-06,9.23E-06
6850,natural_language_inference52,246,He claims that it is better to leave the clippings on the ground .,Model,Error Analysis,natural_language_inference,52,26,1,0,,1.84E-06,0,negative,1.95E-05,5.18E-07,1.97E-06,6.28E-08,2.35E-06,2.59E-05,6.34E-06,2.42E-06,2.42E-06,0.999936937,8.10E-07,5.55E-07,1.50E-07
6851,natural_language_inference52,247,Which long term effect will this most likely have on his lawn ?,Model,Error Analysis,natural_language_inference,52,27,1,0,,7.64E-07,0,negative,3.11E-05,1.63E-06,2.70E-06,1.42E-07,2.76E-06,3.47E-05,1.03E-05,5.06E-06,1.40E-05,0.999894843,1.39E-06,9.36E-07,3.80E-07
6852,natural_language_inference52,248,Correct : It will provide the lawn with needed nutrients .,Model,Error Analysis,natural_language_inference,52,28,1,0,,1.00E-06,0,negative,1.52E-05,1.49E-06,3.97E-06,6.22E-07,1.24E-05,0.000142151,1.52E-05,8.44E-06,9.51E-06,0.999789249,7.49E-07,4.69E-07,6.17E-07
6853,natural_language_inference52,249,The second largest source of errors came from questions requiring complex inference as with the question shown in .,Model,Error Analysis,natural_language_inference,52,29,1,0,,0.000118082,0,negative,0.000460987,1.18E-06,8.14E-06,1.69E-05,0.00188076,0.000307889,7.13E-05,2.74E-06,6.91E-07,0.997242192,2.48E-07,3.87E-06,3.12E-06
6854,natural_language_inference52,250,This demonstrates not only the difficulty of the ques - Type :,Model,Error Analysis,natural_language_inference,52,30,1,0,,1.66E-06,0,negative,4.33E-05,3.68E-07,1.80E-06,1.21E-08,1.18E-06,4.47E-06,3.25E-06,4.08E-07,2.01E-06,0.999942,2.38E-07,9.00E-07,3.82E-08
6855,natural_language_inference52,251,Knowledge base noise,Model,Error Analysis,natural_language_inference,52,31,1,0,,2.84E-05,0,negative,0.001247012,1.23E-05,0.000578394,3.05E-07,4.46E-05,0.000154613,0.00061742,1.42E-05,8.56E-05,0.997152661,3.70E-06,7.73E-05,1.19E-05
6856,natural_language_inference52,252,Question :,Model,Error Analysis,natural_language_inference,52,32,1,0,,8.30E-07,0,negative,6.48E-06,3.27E-06,2.23E-06,2.87E-07,3.03E-06,5.75E-05,8.81E-06,1.01E-05,2.71E-05,0.999876836,3.38E-06,2.90E-07,6.06E-07
6857,natural_language_inference52,253,If an object traveling to the right is acted upon by an unbalanced force from behind it the object will .,Model,Error Analysis,natural_language_inference,52,33,1,0,,1.45E-06,0,negative,5.12E-05,1.70E-06,8.37E-06,6.80E-08,5.46E-06,1.67E-05,5.66E-06,1.88E-06,1.65E-05,0.9998911,3.49E-07,8.19E-07,2.07E-07
6858,natural_language_inference52,254,Correct : speedup Chosen change direction,Model,Error Analysis,natural_language_inference,52,34,1,0,,1.12E-05,0,negative,5.99E-05,1.48E-06,1.48E-05,1.01E-07,5.17E-06,6.79E-05,3.77E-05,4.28E-06,1.99E-05,0.999786328,3.11E-07,1.49E-06,5.82E-07
6859,natural_language_inference52,255,Unbalanced force : force that acts on an object that will change its direction tion set but also the need for systems that can robustly handle a variety of question types and their corresponding information needs .,Model,Error Analysis,natural_language_inference,52,35,1,0,,2.92E-05,0,negative,0.000116724,6.46E-06,7.39E-05,9.69E-07,4.49E-05,9.13E-05,4.70E-05,3.22E-06,2.15E-05,0.999576654,8.78E-06,5.39E-06,3.18E-06
6860,natural_language_inference52,256,"Aside from these primary sources of error , there were some smaller trends :",Model,Error Analysis,natural_language_inference,52,36,1,0,,2.27E-05,0,negative,0.001703743,2.51E-06,4.14E-05,2.10E-07,8.93E-05,3.56E-05,6.73E-05,1.36E-06,3.22E-06,0.998040508,8.31E-08,1.42E-05,6.01E-07
6861,natural_language_inference52,257,"7 % of the incorrectly chosen answers actually had justifications which "" validated "" them due to noise in the knowledge base ( e.g. , the example shown in ) , 7 % required word - order to answer ( e.g. , mass divided by acceleration vs. acceleration divided by mass ) , another 7 % of questions suffered from lack of coverage of the question concept in the knowledge base , and 3 % failed to appropriately handle negation ( i.e. , questions of the format Which of the following are NOT ... ) .",Model,Error Analysis,natural_language_inference,52,37,1,0,,3.16E-05,0,negative,0.00018177,2.23E-06,5.77E-06,1.14E-06,0.000856574,9.27E-05,4.07E-05,1.76E-06,1.01E-06,0.998812098,8.29E-08,3.37E-06,8.38E-07
6862,natural_language_inference52,258,Conclusion,,,natural_language_inference,52,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
6863,natural_language_inference75,1,title,,,natural_language_inference,75,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
6864,natural_language_inference75,2,Key - Value Memory Networks for Directly Reading Documents,title,,natural_language_inference,75,1,1,0,,0.997060529,1,research-problem,2.72E-08,9.94E-06,1.47E-07,2.18E-08,2.02E-08,7.63E-08,4.35E-07,1.26E-06,4.87E-06,0.002157843,0.997825225,1.14E-07,2.62E-08
6865,natural_language_inference75,3,abstract,,,natural_language_inference,75,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
6866,natural_language_inference75,4,Directly reading documents and being able to answer questions from them is an unsolved challenge .,abstract,abstract,natural_language_inference,75,1,1,1,research-problem,0.775496742,1,research-problem,2.45E-08,3.88E-06,1.01E-08,3.06E-06,5.23E-07,3.60E-07,5.30E-07,1.19E-06,2.34E-07,0.012728639,0.987261477,1.70E-08,5.70E-08
6867,natural_language_inference75,5,"To avoid its inherent difficulty , question answering ( QA ) has been directed towards using Knowledge Bases ( KBs ) instead , which has proven effective .",abstract,abstract,natural_language_inference,75,2,1,1,research-problem,0.974668444,1,research-problem,1.33E-08,2.11E-06,3.93E-09,4.60E-07,9.44E-08,1.17E-07,2.57E-07,6.02E-07,1.15E-07,0.006222876,0.993773309,1.07E-08,2.75E-08
6868,natural_language_inference75,6,"Unfortunately KBs often suffer from being too restrictive , as the schema can not support certain types of answers , and too sparse , e.g. Wikipedia contains much more information than Freebase .",abstract,abstract,natural_language_inference,75,3,1,0,,0.007056023,0,research-problem,6.08E-08,1.50E-05,1.12E-08,4.97E-06,1.36E-06,1.69E-06,4.57E-07,5.78E-06,8.94E-07,0.121625155,0.878344548,2.66E-08,6.58E-08
6869,natural_language_inference75,7,"In this work we introduce a new method , Key - Value Memory Networks , that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation .",abstract,abstract,natural_language_inference,75,4,1,0,,0.735873065,1,research-problem,1.54E-05,0.035274409,0.000114945,4.00E-06,7.06E-05,7.74E-06,1.34E-05,7.42E-05,0.004965588,0.083024085,0.876425058,8.88E-06,1.67E-06
6870,natural_language_inference75,8,"To compare using KBs , information extraction or Wikipedia documents directly in a single framework we construct an analysis tool , WIKIMOVIES , a QA dataset that contains raw text alongside a preprocessed KB , in the domain of movies .",abstract,abstract,natural_language_inference,75,5,1,1,research-problem,0.180750181,0,negative,3.64E-05,0.036241388,8.91E-05,0.005372537,0.157644771,0.000282611,0.000210726,0.000290637,0.000242269,0.592347257,0.207211892,1.32E-05,1.73E-05
6871,natural_language_inference75,9,Our method reduces the gap between all three settings .,abstract,abstract,natural_language_inference,75,6,1,0,,0.035500697,0,negative,0.000315215,0.0195568,9.11E-06,3.17E-05,0.000121869,6.05E-05,4.69E-05,0.001618701,0.000804592,0.763602171,0.213667198,0.000159248,6.00E-06
6872,natural_language_inference75,10,It also achieves state - of - the - art results on the existing WIKIQA benchmark .,abstract,abstract,natural_language_inference,75,7,1,0,,0.009490764,0,research-problem,2.28E-05,0.001015594,8.94E-06,2.21E-05,8.45E-05,4.19E-05,0.000227233,0.000328597,3.05E-05,0.390885748,0.606844052,0.000479409,8.76E-06
6873,natural_language_inference75,11,Introduction,,,natural_language_inference,75,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
6874,natural_language_inference75,12,"Question answering ( QA ) has been along standing research problem in natural language processing , with the first systems attempting to answer questions by directly reading documents .",Introduction,Introduction,natural_language_inference,75,1,1,0,,0.972684136,1,research-problem,1.05E-06,0.000101927,3.78E-07,1.44E-05,2.35E-05,4.28E-06,2.46E-05,3.97E-06,1.76E-05,0.019065984,0.980737179,1.66E-06,3.46E-06
6875,natural_language_inference75,13,"The development of large - scale Knowledge Bases ( KBs ) such as Freebase helped organize information into structured forms , prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such data bases .",Introduction,Introduction,natural_language_inference,75,2,1,0,,0.412594441,0,research-problem,9.30E-07,0.000114358,2.35E-07,1.60E-05,2.32E-05,1.02E-05,1.01E-05,7.32E-06,4.02E-05,0.074846742,0.924927859,8.40E-07,2.07E-06
6876,natural_language_inference75,14,"Unfortunately , KBs have intrinsic limitations such as their inevitable incompleteness and fixed schemas that can not support all varieties of answers .",Introduction,Introduction,natural_language_inference,75,3,1,0,,0.037483602,0,research-problem,1.71E-06,0.000250205,2.86E-07,1.51E-05,3.08E-05,1.27E-05,9.45E-06,1.20E-05,6.56E-05,0.202478088,0.797121273,1.28E-06,1.50E-06
6877,natural_language_inference75,15,"Since information extraction ( IE ) , intended to fill in missing information in KBs , is neither accurate nor reliable enough , collections of raw textual resources and documents such as Wikipedia will always contain more information .",Introduction,Introduction,natural_language_inference,75,4,1,0,,0.903297508,1,research-problem,1.37E-06,0.000167502,2.72E-07,1.69E-05,2.99E-05,7.47E-06,1.56E-05,7.22E-06,2.90E-05,0.057736453,0.941984377,1.47E-06,2.42E-06
6878,natural_language_inference75,16,"As a result , even if KBs can be satisfactory for closed - domain problems , they are unlikely to scale up to answer general questions on any topic .",Introduction,Introduction,natural_language_inference,75,5,1,0,,0.050938981,0,research-problem,5.45E-06,0.001118084,6.44E-07,1.69E-05,9.02E-05,3.24E-05,1.95E-05,4.54E-05,0.000310207,0.480686759,0.517668419,3.67E-06,2.26E-06
6879,natural_language_inference75,17,"Starting from this observation , in this work we study the problem of answering by directly reading documents .",Introduction,Introduction,natural_language_inference,75,6,1,0,,0.938243356,1,research-problem,5.27E-06,0.008381378,4.97E-06,1.52E-05,0.000144169,1.15E-05,4.44E-05,2.67E-05,0.001112721,0.053291354,0.936949824,7.61E-06,4.90E-06
6880,natural_language_inference75,18,"Retrieving answers directly from text is harder than from KBs because information is far less structured , is indirectly and ambiguously expressed , and is usually scattered across multiple documents .",Introduction,Introduction,natural_language_inference,75,7,1,0,,0.682124823,1,research-problem,1.78E-06,0.000327276,5.67E-07,2.05E-05,6.99E-05,1.46E-05,2.37E-05,1.27E-05,4.75E-05,0.134748092,0.864727071,2.79E-06,3.50E-06
6881,natural_language_inference75,19,This explains why using a satisfactory KB - typically only available in closed domains - is preferred over raw text .,Introduction,Introduction,natural_language_inference,75,8,1,0,,0.023332124,0,negative,8.61E-06,0.00151861,1.26E-06,2.13E-05,0.000117373,6.38E-05,1.93E-05,8.43E-05,0.000791447,0.837669334,0.159698045,4.57E-06,2.12E-06
6882,natural_language_inference75,20,"We postulate that before trying to provide answers thatare not in KBs , document - based QA systems should first reach KB - based systems ' performance in such closed domains , where clear comparison and evaluation is possible .",Introduction,Introduction,natural_language_inference,75,9,1,0,,0.297803717,0,negative,1.86E-05,0.032200178,7.24E-06,1.18E-05,0.000228192,6.17E-05,3.37E-05,0.000161483,0.067938679,0.61459149,0.284725551,1.65E-05,4.75E-06
6883,natural_language_inference75,21,"To this end , this paper introduces WIKIMOVIES , a new analysis tool that allows for measuring the performance of QA systems when the knowledge source is switched from a KB to unstructured documents .",Introduction,Introduction,natural_language_inference,75,10,1,0,,0.972446035,1,approach,0.000581033,0.553121492,0.001098119,0.001982999,0.216731017,0.000513833,0.001001116,0.000232373,0.045473589,0.134029847,0.04493188,0.000199926,0.000102777
6884,natural_language_inference75,22,WIKIMOVIES contains ?,Introduction,Introduction,natural_language_inference,75,11,1,0,,0.021246348,0,negative,5.65E-05,0.00439158,1.36E-05,0.008517186,0.08189442,0.001904431,0.000454248,0.000456462,0.000891387,0.893777363,0.007558725,2.77E-05,5.64E-05
6885,natural_language_inference75,23,"100 k questions in the movie domain , and was designed to be answerable by using either a perfect KB ( based on OMDb 1 ) , Wikipedia pages or an imper- fect KB obtained through running an engineered IE pipeline on those pages .",Introduction,Introduction,natural_language_inference,75,12,1,0,,0.264087259,0,negative,3.03E-05,0.113502841,7.96E-05,0.000293956,0.149538723,0.001255421,0.000987086,0.000921147,0.009517426,0.696706645,0.027035876,6.52E-05,6.58E-05
6886,natural_language_inference75,24,"To bridge the gap between using a KB and reading documents directly , we still lack appropriate machine learning algorithms .",Introduction,Introduction,natural_language_inference,75,13,1,0,,0.199210585,0,research-problem,5.82E-06,0.000925944,9.83E-07,5.40E-05,0.000139918,5.25E-05,4.15E-05,5.19E-05,0.000171573,0.447923253,0.550621727,5.33E-06,5.50E-06
6887,natural_language_inference75,25,"In this work we propose the Key - Value Memory Network ( KV - MemNN ) , a new neural network architecture that generalizes the original Memory Network and can work with either knowledge source .",Introduction,Introduction,natural_language_inference,75,14,1,1,model,0.97909257,1,model,1.45E-05,0.099869497,0.000248329,7.85E-07,9.87E-05,1.66E-05,4.19E-05,3.13E-05,0.883375329,0.009901179,0.006391153,7.74E-06,3.04E-06
6888,natural_language_inference75,26,The KV - MemNN performs QA by first storing facts in a key - value structured memory before reasoning on them in order to predict an answer .,Introduction,Introduction,natural_language_inference,75,15,1,1,model,0.968442609,1,model,3.39E-05,0.233149325,0.001150225,9.13E-06,0.00053945,8.60E-05,0.000240969,0.00014777,0.493998357,0.120827669,0.149745475,4.47E-05,2.70E-05
6889,natural_language_inference75,27,"The memory is designed so that the model learns to use keys to address relevant memories with respect to the question , whose corresponding values are subsequently returned .",Introduction,Introduction,natural_language_inference,75,16,1,1,model,0.816194351,1,model,2.31E-06,0.038817696,9.26E-06,5.49E-07,4.52E-05,2.41E-05,6.97E-06,9.82E-05,0.947024935,0.013587104,0.000382361,5.44E-07,7.79E-07
6890,natural_language_inference75,28,"This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values , while still being trained using standard backpropagation via stochastic gradient descent .",Introduction,Introduction,natural_language_inference,75,17,1,1,model,0.750622753,1,model,1.06E-05,0.065860519,2.81E-05,3.02E-06,0.000250676,3.86E-05,1.08E-05,6.80E-05,0.897538889,0.035741061,0.000446124,2.15E-06,1.49E-06
6891,natural_language_inference75,29,"Our experiments on WIKIMOVIES indicate that , thanks to its key - value memory , the KV - Mem NN consistently outperforms the original Memory Network , and reduces the gap between answering from a human - annotated KB , from an automatically extracted KB or from directly reading Wikipedia .",Introduction,Introduction,natural_language_inference,75,18,1,0,,0.075515228,0,negative,0.007022456,0.105249425,0.000195693,4.94E-05,0.002234544,0.000504946,0.015959451,0.001313672,0.017482691,0.808972659,0.022938415,0.017879694,0.000196906
6892,natural_language_inference75,30,"We confirm our findings on WIKIQA , another Wikipedia - based QA benchmark where no KB is available , where we demonstrate that KV - Mem NN can reach state - of - the - art resultssurpassing the most recent attention - based neural network models .",Introduction,Introduction,natural_language_inference,75,19,1,0,,0.076300039,0,negative,0.000947869,0.247090097,0.000138028,1.81E-05,0.002679306,0.000191035,0.004794697,0.000548294,0.029662685,0.680170948,0.028654018,0.005033839,7.11E-05
6893,natural_language_inference75,31,Related Work,,,natural_language_inference,75,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
6894,natural_language_inference75,50,Model Description,,,natural_language_inference,75,0,1,0,,0.005147828,0,negative,4.39E-05,0.001071123,7.58E-05,6.83E-06,5.42E-06,0.000325056,0.000218788,0.008009065,0.001950553,0.970198676,0.017521335,0.000530572,4.29E-05
6895,natural_language_inference75,51,Our model is based on the end - to - end Memory Network architecture of .,Model Description,Model Description,natural_language_inference,75,1,1,0,,0.009934901,0,negative,0.00011212,0.001473985,0.001931048,2.07E-05,1.03E-05,0.001791372,0.000312335,0.009140495,0.036965948,0.94727498,0.0008226,9.91E-05,4.51E-05
6896,natural_language_inference75,52,"A high - level view of both models is as follows : one defines a memory , which is a possibly very large array of slots which can encode both long - term and short - term context .",Model Description,Model Description,natural_language_inference,75,2,1,0,,0.000163487,0,negative,0.000123171,0.000516062,0.000854965,1.35E-06,2.51E-06,0.000105525,2.71E-05,0.000550296,0.007437146,0.990080611,0.000143587,0.000153572,4.12E-06
6897,natural_language_inference75,53,"At test time one is given a query ( e.g. the question in QA tasks ) , which is used to iteratively address and read from the memory ( these iterations are also referred to as "" hops "" ) looking for relevant information to answer the question .",Model Description,Model Description,natural_language_inference,75,3,1,0,,0.000196549,0,negative,6.29E-05,0.000292512,0.000582299,7.64E-07,3.29E-06,3.00E-05,1.83E-05,0.000248127,0.000847847,0.997621812,9.65E-05,0.000193652,1.97E-06
6898,natural_language_inference75,54,"At each step , the collected information from the memory is cumulatively added to the original query to build context for the next round .",Model Description,Model Description,natural_language_inference,75,4,1,0,,0.005111677,0,negative,0.000141306,0.000320819,0.000754138,1.55E-07,1.24E-06,1.96E-05,8.73E-06,0.000219713,0.004514429,0.993838649,1.39E-05,0.000166531,7.84E-07
6899,natural_language_inference75,55,"At the last iteration , the final retrieved context and the most recent query are combined as features to predict a response from a list of candidates .",Model Description,Model Description,natural_language_inference,75,5,1,0,,0.008555545,0,negative,0.000136846,0.000628894,0.001331155,2.33E-07,1.15E-06,8.10E-05,2.01E-05,0.0013656,0.0122823,0.984031797,2.03E-05,9.92E-05,1.57E-06
6900,natural_language_inference75,56,illustrates the KV - Mem NN model architecture .,Model Description,Model Description,natural_language_inference,75,6,1,0,,0.000156244,0,negative,1.77E-05,4.98E-05,0.000139957,1.49E-06,6.10E-07,0.000293446,4.86E-05,0.001999105,0.00337475,0.993912014,0.000116974,3.72E-05,8.39E-06
6901,natural_language_inference75,57,"In KV - MemNNs we define the memory slots as pairs of vectors ( k 1 , v 1 ) . . . , ( k M , v M ) and denote the question x .",Model Description,Model Description,natural_language_inference,75,7,1,0,,7.92E-05,0,negative,1.53E-06,4.22E-05,1.08E-05,9.61E-08,1.06E-07,5.73E-05,5.63E-06,0.000919549,0.000401077,0.998500563,5.33E-05,7.45E-06,3.95E-07
6902,natural_language_inference75,58,The addressing and reading of the memory involves three steps :,Model Description,Model Description,natural_language_inference,75,8,1,0,,0.001255636,0,negative,0.0006392,0.000138528,0.003491601,1.67E-06,3.93E-06,0.00010703,7.61E-05,0.000404508,0.00159923,0.992556095,6.46E-05,0.000908907,8.62E-06
6903,natural_language_inference75,59,Key Hashing : the question can be used to preselect a small subset of the possibly large array .,Model Description,Model Description,natural_language_inference,75,9,1,0,,0.002205325,0,negative,3.10E-05,3.50E-05,0.000136853,4.46E-06,1.57E-06,0.000456942,0.000104743,0.001405437,0.000410004,0.99715077,9.47E-05,0.000159862,8.63E-06
6904,natural_language_inference75,60,"This is done using an inverted index that finds a subset ( k h 1 , v h 1 ) , . . . , ( k h N , v h N ) of memories of size N where the key shares at least one word with the question with frequency < F = 1000 ( to ignore stop words ) , following .",Model Description,Model Description,natural_language_inference,75,10,1,0,,0.000354014,0,negative,8.43E-05,0.000373744,0.000410588,3.14E-07,2.71E-06,0.000261091,6.14E-05,0.002762254,0.000988907,0.994886775,8.78E-06,0.000157237,1.92E-06
6905,natural_language_inference75,61,"More sophisticated retrieval schemes could be used here , see e.g. ,",Model Description,Model Description,natural_language_inference,75,11,1,0,,1.57E-05,0,negative,1.66E-05,2.21E-06,1.59E-05,7.14E-08,8.24E-08,3.75E-05,9.04E-06,0.000144707,2.96E-05,0.999684899,3.76E-06,5.54E-05,1.96E-07
6906,natural_language_inference75,62,Key,Model Description,,natural_language_inference,75,12,1,0,,2.07E-05,0,negative,2.00E-05,1.10E-05,3.90E-06,8.21E-06,7.39E-07,0.000356334,2.19E-05,0.002141724,0.000257841,0.997145129,1.04E-05,1.95E-05,3.35E-06
6907,natural_language_inference75,63,"Addressing : during addressing , each candidate memory is assigned a relevance probability by comparing the question to each key :",Model Description,Key,natural_language_inference,75,13,1,0,,0.005542657,0,negative,0.000588482,0.018918353,0.052811595,7.42E-06,0.000247654,0.000391934,0.000265917,0.000734843,0.062210177,0.857886213,0.005814164,0.000106134,1.71E-05
6908,natural_language_inference75,64,We discuss choices of feature map in Sec. 3.2 .,Model Description,Key,natural_language_inference,75,14,1,0,,4.45E-06,0,negative,2.18E-05,0.000542042,6.11E-06,2.61E-05,0.000136184,0.000400567,9.94E-06,0.000677015,0.000318124,0.997825244,3.37E-05,2.50E-06,6.51E-07
6909,natural_language_inference75,65,"Value Reading : in the final reading step , the values of the memories are read by taking their weighted sum using the addressing probabilities , and the vector o is returned :",Model Description,Key,natural_language_inference,75,15,1,0,,0.001412855,0,negative,0.000110099,0.005596191,0.004863032,8.25E-07,2.96E-05,0.000211618,8.66E-05,0.000600598,0.03582465,0.951094672,0.001549369,3.02E-05,2.57E-06
6910,natural_language_inference75,66,"The memory access process is conducted by the "" controller "" neural network using q = A ? X ( x ) as the query .",Model Description,Key,natural_language_inference,75,16,1,0,,0.000253442,0,negative,1.39E-05,0.00497439,0.000479469,1.03E-06,2.58E-05,0.000206578,2.23E-05,0.001014571,0.092429033,0.900415649,0.000413848,1.61E-06,1.78E-06
6911,natural_language_inference75,67,"After receiving the result o , the query is updated with",Model Description,Key,natural_language_inference,75,17,1,0,,6.45E-06,0,negative,4.65E-05,0.000686085,0.000240157,2.12E-07,1.77E-05,2.73E-05,7.05E-06,0.000109161,0.002485773,0.996254579,0.000114523,1.08E-05,2.34E-07
6912,natural_language_inference75,68,"The memory access is then repeated ( specifically , only the addressing and reading steps , but not the hashing ) , using a different matrix R j on each hop , j.",Model Description,Key,natural_language_inference,75,18,1,0,,2.50E-05,0,negative,5.46E-05,0.003785155,0.000725556,1.95E-07,1.62E-05,3.13E-05,1.01E-05,0.000142595,0.043529036,0.951572085,0.000128097,4.55E-06,4.32E-07
6913,natural_language_inference75,69,The key addressing equation is transformed accordingly to use the updated query :,Model Description,Key,natural_language_inference,75,19,1,0,,1.31E-05,0,negative,1.73E-05,0.000380665,9.94E-05,3.85E-07,1.27E-05,5.10E-05,4.03E-06,0.000133532,0.004850764,0.994408991,3.95E-05,1.55E-06,2.64E-07
6914,natural_language_inference75,70,The motivation for this is that new evidence can be combined into the query to focus on and retrieve more pertinent information in subsequent accesses .,Model Description,Key,natural_language_inference,75,20,1,0,,1.06E-05,0,negative,8.61E-06,0.001380347,2.93E-05,7.72E-06,0.000100443,5.64E-05,6.45E-06,0.000144885,0.001570111,0.995121776,0.001570454,2.40E-06,1.14E-06
6915,natural_language_inference75,71,"Finally , after a fixed number H hops , the resulting state of the controller is used to compute a final prediction over the possible outputs :",Model Description,Key,natural_language_inference,75,21,1,0,,1.00E-05,0,negative,5.32E-05,0.001380937,0.000563684,1.97E-07,1.87E-05,2.28E-05,8.14E-06,8.38E-05,0.00925405,0.988511801,9.08E-05,1.16E-05,2.95E-07
6916,natural_language_inference75,72,"where y i are the possible candidate outputs , e.g. all the entities in the KB , or all possible candidate answer sentences in the case of a dataset like WIKIQA ( see Sec. 5.2 ) .",Model Description,Key,natural_language_inference,75,22,1,0,,2.82E-06,0,negative,2.90E-06,8.02E-05,4.22E-06,2.34E-07,9.80E-06,1.61E-05,1.56E-06,5.65E-05,0.000158136,0.999644945,2.41E-05,1.31E-06,6.81E-08
6917,natural_language_inference75,73,The d D matrix B can also be constrained to be identical to A .,Model Description,Key,natural_language_inference,75,23,1,0,,1.42E-05,0,negative,1.28E-05,0.000806489,1.75E-05,1.09E-06,2.30E-05,0.000183667,7.88E-06,0.001007574,0.004749983,0.993158997,2.82E-05,2.29E-06,6.12E-07
6918,natural_language_inference75,74,"The whole network is trained end - to - end , and the model learns to perform the iterative accesses to output the desired target a by minimizing a standard cross - entropy loss between a and the correct answer a.",Model Description,Key,natural_language_inference,75,24,1,0,,0.000203347,0,negative,5.17E-05,0.025516257,0.001257293,8.20E-07,8.61E-05,5.40E-05,1.86E-05,0.000383505,0.215640193,0.75682393,0.000159917,5.80E-06,1.86E-06
6919,natural_language_inference75,75,"Backpropagation and stochastic gradient descent are thus used to learn the matrices A , B and R 1 , . . . , R H .",Model Description,Key,natural_language_inference,75,25,1,0,,0.000178714,0,negative,2.16E-05,0.003250484,8.09E-05,9.11E-06,9.04E-05,0.000989379,4.34E-05,0.004716277,0.016015582,0.974728698,4.92E-05,1.34E-06,3.75E-06
6920,natural_language_inference75,76,To obtain the standard End - To - End Memory Network of one can simply set the key and value to be the same for all memories .,Model Description,Key,natural_language_inference,75,26,1,0,,7.20E-06,0,negative,2.20E-05,0.001349863,0.000281565,4.61E-07,4.03E-05,5.07E-05,9.22E-06,0.000149869,0.010451156,0.987598313,4.08E-05,5.28E-06,4.79E-07
6921,natural_language_inference75,77,"Hashing was not used in that paper , but is important for computational efficiency for large memory sizes , as already shown in .",Model Description,Key,natural_language_inference,75,27,1,0,,2.19E-05,0,negative,1.06E-05,0.000224581,1.40E-05,1.18E-05,5.31E-05,0.00029207,2.54E-05,0.000369417,0.000242008,0.998143924,0.000607569,3.35E-06,2.25E-06
6922,natural_language_inference75,78,We will now goon to describe specific applications of key - value memories for the task of reading KBs or documents .,Model Description,Key,natural_language_inference,75,28,1,0,,3.74E-06,0,negative,1.50E-06,0.000141819,3.48E-06,3.87E-06,8.87E-05,5.44E-05,4.55E-06,8.40E-05,0.000189468,0.999347732,7.92E-05,8.40E-07,5.49E-07
6923,natural_language_inference75,79,Key - Value Memories,Model Description,,natural_language_inference,75,29,1,0,,0.012734897,0,negative,0.00010685,0.000184733,0.001655674,2.59E-06,3.24E-06,0.000451524,0.003495414,0.003602358,0.003071497,0.972437161,0.001903937,0.012831635,0.000253384
6924,natural_language_inference75,80,There are a variety of ways to employ key - value memories that can have important effects on over all performance .,Model Description,Key - Value Memories,natural_language_inference,75,30,1,0,,2.80E-06,0,negative,4.23E-06,2.96E-07,1.03E-06,5.75E-08,2.02E-08,3.23E-06,4.84E-06,1.43E-05,5.33E-06,0.999910067,1.74E-06,5.46E-05,2.58E-07
6925,natural_language_inference75,81,"The ability to encode prior knowledge in this way is an important component of KV - MemNNs , and we are free to define ?",Model Description,Key - Value Memories,natural_language_inference,75,31,1,0,,3.07E-07,0,negative,1.26E-06,3.69E-07,6.00E-07,7.45E-08,1.76E-08,3.88E-06,3.29E-06,1.89E-05,1.36E-05,0.999938124,1.92E-06,1.77E-05,3.00E-07
6926,natural_language_inference75,82,"X , ?",Model Description,Key - Value Memories,natural_language_inference,75,32,1,0,,1.58E-06,0,negative,1.45E-06,8.22E-08,3.49E-07,7.06E-09,9.66E-09,1.52E-06,1.64E-06,8.18E-06,2.54E-06,0.999969429,2.85E-08,1.47E-05,3.77E-08
6927,natural_language_inference75,83,"Y , ?",Model Description,Key - Value Memories,natural_language_inference,75,33,1,0,,1.41E-06,0,negative,1.71E-06,5.81E-08,3.36E-07,7.60E-09,9.83E-09,1.07E-06,1.41E-06,5.08E-06,1.84E-06,0.99996978,2.43E-08,1.86E-05,3.66E-08
6928,natural_language_inference75,84,K and ?,Model Description,Key - Value Memories,natural_language_inference,75,34,1,0,,7.08E-07,0,negative,4.94E-06,9.27E-08,5.27E-07,1.39E-08,2.70E-08,2.63E-06,4.16E-06,1.27E-05,2.39E-06,0.999929795,2.78E-08,4.26E-05,7.96E-08
6929,natural_language_inference75,85,"V for the query , answer , keys and values respectively .",Model Description,Key - Value Memories,natural_language_inference,75,35,1,0,,2.61E-06,0,negative,5.46E-07,1.31E-07,1.24E-06,8.16E-10,3.98E-09,2.96E-07,5.04E-07,2.42E-06,7.62E-06,0.999979963,1.99E-08,7.25E-06,1.56E-08
6930,natural_language_inference75,86,We now describe several possible variants of ?,Model Description,Key - Value Memories,natural_language_inference,75,36,1,0,,6.55E-07,0,negative,8.38E-07,4.88E-08,4.70E-07,6.64E-09,1.45E-08,6.92E-07,7.18E-07,2.57E-06,1.59E-06,0.999984287,1.07E-08,8.73E-06,2.23E-08
6931,natural_language_inference75,87,K and ?,Model Description,Key - Value Memories,natural_language_inference,75,37,1,0,,6.49E-07,0,negative,4.53E-06,8.47E-08,4.95E-07,1.23E-08,2.69E-08,2.40E-06,4.05E-06,1.20E-05,2.19E-06,0.999932637,1.97E-08,4.15E-05,8.05E-08
6932,natural_language_inference75,88,"V that we tried in our experiments , for simplicity we kept ?",Model Description,Key - Value Memories,natural_language_inference,75,38,1,0,,1.02E-06,0,negative,1.50E-06,1.41E-07,3.87E-07,1.72E-08,2.71E-08,4.86E-06,3.84E-06,3.13E-05,2.63E-06,0.99993841,1.90E-08,1.68E-05,6.56E-08
6933,natural_language_inference75,89,X and ?,Model Description,Key - Value Memories,natural_language_inference,75,39,1,0,,4.26E-07,0,negative,9.80E-07,3.54E-08,1.77E-07,4.23E-09,9.28E-09,9.63E-07,1.20E-06,5.18E-06,1.26E-06,0.999978312,9.56E-09,1.18E-05,3.25E-08
6934,natural_language_inference75,90,Y fixed as bag - of - words representations .,Model Description,Key - Value Memories,natural_language_inference,75,40,1,0,,1.88E-05,0,negative,3.49E-06,5.67E-07,2.52E-06,8.37E-09,2.44E-08,5.96E-06,8.05E-06,0.000107078,2.42E-05,0.999834331,8.15E-09,1.36E-05,1.26E-07
6935,natural_language_inference75,91,KB,Model Description,,natural_language_inference,75,41,1,0,,4.99E-05,0,negative,1.61E-05,4.21E-06,4.89E-06,5.36E-07,1.01E-06,8.24E-05,4.01E-05,0.000927539,9.27E-05,0.998684025,4.21E-07,0.000140908,5.13E-06
6936,natural_language_inference75,92,"Triple Knowledge base entries have a structure of triple "" subject relation object "" ( see for examples ) .",Model Description,KB,natural_language_inference,75,42,1,0,,9.64E-05,0,negative,3.15E-06,6.73E-06,1.63E-05,7.69E-07,6.20E-06,6.03E-05,2.58E-05,0.000210757,9.01E-05,0.999533476,5.47E-07,3.97E-05,6.17E-06
6937,natural_language_inference75,93,"The representation we consider is simple : the key is composed of the left - hand side entity ( subject ) and the relation , and the value is the right - hand side entity ( object ) .",Model Description,KB,natural_language_inference,75,43,1,0,,0.000143389,0,negative,8.96E-06,0.000310224,0.000170787,3.70E-07,7.46E-06,5.58E-05,3.48E-05,0.00039105,0.003928471,0.995035838,1.95E-06,4.65E-05,7.77E-06
6938,natural_language_inference75,94,"We double the KB and consider the reversed relation as well ( e.g. we now have two triples "" Blade Runner directed _ by Ridley Scott "" and "" Ridley Scott ! directed_by Blade Runner "" where ! di - rected_by is a different entry in the dictionary than directed_by ) .",Model Description,KB,natural_language_inference,75,44,1,0,,1.03E-05,0,negative,3.44E-06,1.74E-06,5.52E-06,2.83E-08,7.71E-07,8.42E-06,4.44E-06,3.87E-05,2.56E-05,0.999901154,1.46E-08,9.97E-06,2.89E-07
6939,natural_language_inference75,95,"Having the entry both ways round is important for answering different kinds of questions ( "" Who directed Blade Runner ? "" vs. "" What did Ridley Scott direct ? "" ) .",Model Description,KB,natural_language_inference,75,45,1,0,,0.001246206,0,negative,3.15E-05,4.75E-06,9.78E-06,3.03E-07,1.96E-06,1.46E-05,4.30E-05,9.54E-05,1.72E-05,0.998710393,1.38E-06,0.001065071,4.68E-06
6940,natural_language_inference75,96,For a standard Mem NN that does not have key - value pairs the whole triple has to be encoded into the same memory slot .,Model Description,KB,natural_language_inference,75,46,1,0,,6.08E-05,0,negative,2.97E-05,1.90E-05,5.69E-05,4.67E-08,1.12E-06,7.07E-06,8.29E-06,5.33E-05,0.000245643,0.999495496,2.75E-07,8.17E-05,1.39E-06
6941,natural_language_inference75,97,Sentence Level,Model Description,,natural_language_inference,75,47,1,0,,0.182821258,0,negative,0.001173134,8.53E-05,0.001787868,1.96E-05,0.000107847,0.000556891,0.015030587,0.003030877,0.000308657,0.603605123,1.62E-05,0.371945875,0.002331909
6942,natural_language_inference75,98,"For representing a document , one can split it up into sentences , with each memory slot encoding one sentence .",Model Description,Sentence Level,natural_language_inference,75,48,1,0,,7.19E-06,0,negative,1.13E-05,1.26E-06,8.37E-05,4.75E-09,1.23E-07,2.06E-06,0.00022454,1.08E-06,0.000154779,0.999448424,4.00E-08,6.79E-05,4.81E-06
6943,natural_language_inference75,99,Both the key and the value encode the entire sentence as a bag - of - words .,Model Description,Sentence Level,natural_language_inference,75,49,1,0,,0.00049795,0,negative,1.06E-05,4.73E-06,0.000127433,1.79E-08,2.48E-07,1.01E-05,0.000453881,1.19E-05,0.002194416,0.997148837,3.32E-08,1.96E-05,1.82E-05
6944,natural_language_inference75,100,"As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .",Model Description,Sentence Level,natural_language_inference,75,50,1,0,,1.04E-06,0,negative,4.79E-06,9.86E-07,0.000104257,1.79E-08,8.75E-08,4.91E-06,0.000305643,3.20E-06,0.000117362,0.999406832,1.12E-07,3.71E-05,1.47E-05
6945,natural_language_inference75,101,Window Level Documents are split up into windows of W words ; in our tasks we only include windows where the center word is an entity .,Model Description,Sentence Level,natural_language_inference,75,51,1,0,,0.000170421,0,negative,9.83E-06,1.78E-06,9.58E-05,3.17E-08,1.77E-06,4.76E-05,0.004699056,1.60E-05,2.44E-05,0.994949615,1.02E-08,9.65E-05,5.76E-05
6946,natural_language_inference75,102,Windows are represented using bag - of - words .,Model Description,Sentence Level,natural_language_inference,75,52,1,0,,0.001475918,0,negative,7.17E-06,4.58E-06,0.000314966,1.70E-08,2.45E-07,2.81E-05,0.002212698,2.70E-05,0.001509544,0.995807979,3.71E-08,2.92E-05,5.85E-05
6947,natural_language_inference75,103,Window representations for MemNNs have been shown to work well previously .,Model Description,Sentence Level,natural_language_inference,75,53,1,0,,0.000179582,0,negative,9.17E-06,3.35E-07,1.91E-05,5.61E-08,7.34E-08,3.91E-05,0.005939682,1.52E-05,9.11E-06,0.99349708,1.22E-06,0.000276298,0.000193587
6948,natural_language_inference75,104,"However , in Key - Value Mem",Model Description,,natural_language_inference,75,54,1,0,,0.001007536,0,negative,1.36E-05,2.31E-05,0.00011503,3.33E-07,1.24E-06,3.43E-05,0.000115725,0.000361851,0.000108084,0.997922087,1.29E-05,0.001272869,1.91E-05
6949,natural_language_inference75,105,"NNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture .",Model Description,"However , in Key - Value Mem",natural_language_inference,75,55,1,0,,0.049230884,0,negative,6.40E-05,2.04E-05,0.002412764,2.78E-07,5.52E-06,5.74E-05,0.000145382,8.17E-05,0.001034945,0.995909094,2.20E-07,0.000148458,0.000119842
6950,natural_language_inference75,106,"This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) .",Model Description,"However , in Key - Value Mem",natural_language_inference,75,56,1,0,,0.00012324,0,negative,9.07E-06,5.46E-07,2.38E-06,6.26E-09,1.78E-07,2.03E-06,2.29E-06,9.80E-06,1.77E-05,0.999935278,1.78E-09,2.02E-05,5.03E-07
6951,natural_language_inference75,107,We will compare these approaches in our experiments .,Model Description,"However , in Key - Value Mem",natural_language_inference,75,57,1,0,,6.50E-05,0,negative,8.55E-07,8.54E-08,7.13E-07,4.26E-09,1.07E-07,3.45E-06,5.56E-06,1.05E-05,7.80E-07,0.999955751,1.30E-09,2.15E-05,6.97E-07
6952,natural_language_inference75,108,"Window + Center Encoding Instead of representing the window as a pure bag - of - words , thus mixing the window center with the rest of the window , we can also encode them with different features .",Model Description,"However , in Key - Value Mem",natural_language_inference,75,58,1,0,,0.063334647,0,negative,4.87E-05,1.60E-05,0.000553254,9.86E-08,2.11E-06,2.42E-05,8.16E-05,4.33E-05,0.000627988,0.998340153,1.01E-07,0.000204377,5.82E-05
6953,natural_language_inference75,109,"Here , we double the size , D , of the dictionary and encode the center of the window and the value using the second dictionary .",Model Description,"However , in Key - Value Mem",natural_language_inference,75,59,1,0,,0.003022048,0,negative,0.000372939,2.81E-05,0.000153261,1.21E-07,7.79E-06,2.40E-05,6.86E-05,8.93E-05,0.000352245,0.998742306,1.11E-08,0.000141968,1.94E-05
6954,natural_language_inference75,110,This should help the model pick out the relevance of the window center ( more related to the answer ) as compared to the words either side of it ( more related to the question ) .,Model Description,"However , in Key - Value Mem",natural_language_inference,75,60,1,0,,0.000654208,0,negative,1.87E-05,7.63E-07,4.69E-06,1.06E-08,3.38E-07,2.48E-06,2.79E-06,9.01E-06,3.47E-05,0.9998947,2.06E-09,3.08E-05,1.08E-06
6955,natural_language_inference75,111,Window + Title,,,natural_language_inference,75,0,1,0,,0.025990861,0,negative,0.000247325,0.000337455,5.29E-05,8.94E-05,1.36E-05,0.001699592,0.000594819,0.007451937,0.000510018,0.984165586,0.004001316,0.000756568,7.96E-05
6956,natural_language_inference75,112,The title of a document is commonly the answer to a question that relates to the text it contains .,Window + Title,Window + Title,natural_language_inference,75,1,1,0,,3.24E-05,0,negative,5.98E-06,4.95E-05,8.25E-06,0.000341622,0.000226808,0.000261697,1.16E-05,4.32E-05,2.82E-05,0.997582419,0.001437916,8.16E-07,2.07E-06
6957,natural_language_inference75,113,"For example "" What did Harrison Ford star in ? "" can be ( partially ) answered by the Wikipedia document with the title "" Blade Runner "" .",Window + Title,Window + Title,natural_language_inference,75,2,1,0,,2.19E-05,0,negative,2.64E-06,1.10E-05,1.02E-06,3.24E-05,3.08E-05,0.000153163,6.70E-06,4.96E-05,7.84E-06,0.999075283,0.000628329,7.12E-07,5.12E-07
6958,natural_language_inference75,114,"For this reason , we also consider a representation where the key is the word window as before , but the value is the document title .",Window + Title,Window + Title,natural_language_inference,75,3,1,0,,0.000119521,0,negative,6.24E-05,0.009167085,0.004125217,4.69E-07,9.20E-06,0.000270602,3.19E-05,0.000610901,0.041619868,0.943285365,0.000798633,1.75E-05,8.30E-07
6959,natural_language_inference75,115,"We also keep all the standard ( window , center ) key - value pairs from the window - level representation as well , thus doubling the number of memory slots in comparison .",Window + Title,Window + Title,natural_language_inference,75,4,1,0,,0.00692329,0,negative,0.001839635,0.045552481,0.01571236,1.80E-05,0.000428112,0.002838258,0.00087366,0.005977663,0.051403456,0.874464444,0.00064837,0.000229113,1.45E-05
6960,natural_language_inference75,116,"To differentiate the two keys with different values we add an extra feature "" _window_ "" or "" _title_ "" to the key , depending on the value .",Window + Title,Window + Title,natural_language_inference,75,5,1,0,,0.001226903,0,negative,4.39E-05,0.002189149,7.43E-05,1.37E-05,4.10E-05,0.003728835,6.32E-05,0.008725394,0.006530948,0.978421952,0.000156323,8.40E-06,2.84E-06
6961,natural_language_inference75,117,"The "" _title_ "" version also includes the actual movie title in the key .",Window + Title,Window + Title,natural_language_inference,75,6,1,0,,0.000752713,0,negative,1.30E-05,9.09E-05,1.02E-05,0.000104575,0.00014272,0.001817765,3.61E-05,0.000789327,8.14E-05,0.99682654,8.15E-05,4.23E-06,1.72E-06
6962,natural_language_inference75,118,This representation can be combined with center encoding .,Window + Title,Window + Title,natural_language_inference,75,7,1,0,,0.001471769,0,negative,0.000156552,0.001503076,0.004222353,1.25E-06,1.14E-05,0.00017943,3.38E-05,0.000261548,0.012238629,0.980365016,0.000981493,4.37E-05,1.79E-06
6963,natural_language_inference75,119,Note that this representation is inherently specific to datasets in which there is an apparent or meaningful title for each document .,Window + Title,Window + Title,natural_language_inference,75,8,1,0,,2.64E-05,0,negative,7.15E-06,0.000397128,5.04E-05,4.83E-07,4.84E-06,8.25E-05,3.62E-06,0.00019632,0.000678862,0.998427782,0.000147879,2.94E-06,1.48E-07
6964,natural_language_inference75,120,The WikiMovies Benchmark,Window + Title,,natural_language_inference,75,9,1,0,,0.835293723,1,negative,0.000194535,0.001900053,0.000844945,0.000178038,0.000705146,0.004773147,0.042768906,0.00490107,0.000200189,0.854897745,0.075512158,0.012532054,0.000592015
6965,natural_language_inference75,121,The WIKIMOVIES benchmark consists of questionanswer pairs in the domain of movies .,Window + Title,The WikiMovies Benchmark,natural_language_inference,75,10,1,0,,0.100265214,0,negative,4.55E-05,2.94E-05,0.000220134,0.000339975,0.004548086,0.00196051,0.027784315,3.97E-05,5.81E-06,0.963437317,0.000832366,0.000651625,0.000105237
6966,natural_language_inference75,122,It was built with the following goals in mind : ( i ) machine learning techniques should have ample training examples for learning ; and ( ii ) one can analyze easily the performance of different representations of knowledge and breakdown the results by question type .,Window + Title,The WikiMovies Benchmark,natural_language_inference,75,11,1,0,,0.000126792,0,negative,3.71E-05,0.000343698,0.000343202,3.96E-06,0.000289886,0.00089209,0.003355341,0.000125253,0.000285439,0.994001346,0.000155926,0.000160148,6.63E-06
6967,natural_language_inference75,123,The dataset can be downloaded from http://fb.ai/babi.,Window + Title,The WikiMovies Benchmark,natural_language_inference,75,12,1,0,,7.29E-05,0,negative,1.19E-05,1.16E-06,1.07E-05,2.52E-05,0.000297997,0.00079927,0.000307951,9.65E-06,1.43E-06,0.998519649,6.78E-07,1.38E-05,5.80E-07
6968,natural_language_inference75,124,Knowledge Representations,Window + Title,,natural_language_inference,75,13,1,0,,9.07E-05,0,negative,2.56E-05,0.000293516,0.000195199,2.47E-06,1.81E-05,0.000400403,0.00050108,0.000694286,0.000622402,0.978652053,0.018370352,0.000214841,9.71E-06
6969,natural_language_inference75,125,We construct three forms of knowledge representation : ( i ) Doc : raw Wikipedia documents consisting of the pages of the movies mentioned ; ( ii ) KB : a classical graph - based KB consisting of entities and relations created from the Open Movie Data base ( OMDb ) and MovieLens ; and ( iii ) IE : information extraction performed on the Wikipedia pages to build a KB in a similar form as ( ii ) .,Window + Title,Knowledge Representations,natural_language_inference,75,14,1,0,,0.002244945,0,negative,0.000661036,0.013302615,0.00468097,8.59E-06,0.001720983,0.000530624,0.004236478,0.000154952,0.004504987,0.969294833,0.000446354,0.000445921,1.17E-05
6970,natural_language_inference75,126,We take care to construct QA pairs such that they are all potentially answerable from either the KB from ( ii ) or the original Wikipedia documents from ( i ) to eliminate data sparsity issues .,Window + Title,Knowledge Representations,natural_language_inference,75,15,1,0,,1.39E-05,0,negative,0.000156535,0.002001865,5.93E-05,3.51E-07,3.51E-05,5.16E-05,8.38E-05,6.06E-05,0.002175327,0.995328507,1.52E-05,3.17E-05,1.98E-07
6971,natural_language_inference75,127,"However , it should be noted that the advantage of working from raw documents in real applications is that data sparsity is less of a concern than for a KB , while on the other hand the KB has the information already parsed in a form amenable to manipulation by machines .",Window + Title,Knowledge Representations,natural_language_inference,75,16,1,0,,3.51E-06,0,negative,1.69E-05,4.70E-06,7.06E-07,2.07E-07,1.11E-06,7.67E-06,1.29E-05,2.36E-06,8.20E-06,0.99972231,0.000213261,9.61E-06,6.99E-08
6972,natural_language_inference75,128,"This dataset can help analyze what methods we need to close the gap between all three settings , and in particular what are the best methods for reading documents when a KB is not available .",Window + Title,Knowledge Representations,natural_language_inference,75,17,1,0,,1.02E-05,0,negative,2.20E-05,6.70E-06,5.19E-06,3.19E-06,0.000740572,3.56E-05,4.31E-05,1.43E-06,3.06E-06,0.999125411,1.67E-06,1.19E-05,1.21E-07
6973,natural_language_inference75,129,A sample of the dataset is shown in .,Window + Title,Knowledge Representations,natural_language_inference,75,18,1,0,,2.80E-06,0,negative,6.59E-06,6.49E-07,7.60E-07,4.57E-07,2.89E-05,1.67E-05,8.99E-06,9.00E-07,1.11E-06,0.999932031,5.43E-07,2.29E-06,2.70E-08
6974,natural_language_inference75,130,Doc,Window + Title,,natural_language_inference,75,19,1,0,,5.63E-05,0,negative,1.53E-05,0.000108629,3.59E-06,4.35E-05,4.14E-05,0.000813702,3.60E-05,0.00098668,0.000198287,0.997647509,9.68E-05,5.87E-06,2.84E-06
6975,natural_language_inference75,131,We selected a set of Wikipedia articles about movies by identifying a set of movies from OMDb 2 that had an associated article by title match .,Window + Title,Doc,natural_language_inference,75,20,1,0,,0.000735875,0,dataset,2.25E-05,0.003261138,4.86E-05,0.000431096,0.676304484,0.000942105,8.81E-05,7.64E-05,6.76E-05,0.31866609,8.37E-05,4.96E-06,3.20E-06
6976,natural_language_inference75,132,We keep the title and the first section ( before the contents box ) for each article .,Window + Title,Doc,natural_language_inference,75,21,1,0,,0.000284297,0,negative,5.43E-06,0.000456193,2.28E-06,0.000111867,0.000954156,0.001253448,2.01E-05,0.000321638,0.000258031,0.996461565,0.000152806,1.45E-06,1.08E-06
6977,natural_language_inference75,133,This gives ? 17 k documents ( movies ) which comprise the set of documents our models will read from in order to answer questions .,Window + Title,Doc,natural_language_inference,75,22,1,0,,0.000360834,0,negative,5.92E-06,0.000847765,1.02E-05,3.62E-05,0.016686626,0.001013472,4.72E-05,0.000164912,0.000138636,0.980991975,5.39E-05,1.83E-06,1.30E-06
6978,natural_language_inference75,134,KB,Window + Title,,natural_language_inference,75,23,1,0,,2.10E-05,0,negative,2.04E-05,8.11E-05,1.16E-05,4.71E-06,2.53E-05,0.000415497,5.22E-05,0.000609156,0.000276099,0.99843025,5.77E-05,1.32E-05,2.79E-06
6979,natural_language_inference75,135,Our set of movies were also matched to the MovieLens dataset 3 .,Window + Title,KB,natural_language_inference,75,24,1,0,,5.25E-05,0,negative,1.40E-05,0.00010083,4.77E-05,2.30E-05,0.03807239,0.000808274,0.000122911,2.70E-05,9.72E-06,0.960766599,2.01E-06,4.18E-06,1.37E-06
6980,natural_language_inference75,136,"We built a KB using OMDb and MovieLens metadata with entries for each movie and nine different relation types : director , writer , actor , release year , language , genre , tags , IMDb rating and IMDb votes , with ? 10 k related actors , ? 6 k directors and ?",Window + Title,KB,natural_language_inference,75,25,1,0,,0.000360198,0,negative,0.000103535,0.003351961,0.000414788,0.000146258,0.184683721,0.004417879,0.001114553,0.000280609,0.000196013,0.805240329,1.48E-05,1.73E-05,1.82E-05
6981,natural_language_inference75,137,43 k entities in total .,Window + Title,KB,natural_language_inference,75,26,1,0,,0.001534264,0,negative,0.000223072,0.000180547,7.20E-05,4.37E-05,0.028872924,0.000529666,0.000276164,3.04E-05,2.66E-05,0.96969557,9.70E-06,3.20E-05,7.62E-06
6982,natural_language_inference75,138,The KB is stored as triples ; see for examples .,Window + Title,KB,natural_language_inference,75,27,1,0,,0.000303616,0,negative,1.62E-05,0.000605144,0.000265778,5.27E-07,0.000110402,0.000177867,4.53E-05,4.86E-05,0.003313092,0.995375101,3.76E-05,2.93E-06,1.42E-06
6983,natural_language_inference75,139,"IMDb ratings and votes are originally real - valued but are binned and converted to text ( "" unheard of "" , "" unknown "" , "" well known "" , "" highly watched "" , "" famous "" ) .",Window + Title,KB,natural_language_inference,75,28,1,0,,5.56E-05,0,negative,7.63E-06,0.000258575,0.000116993,1.22E-05,0.007169338,0.000987082,0.00018378,6.24E-05,5.33E-05,0.99110383,3.59E-05,4.45E-06,4.51E-06
6984,natural_language_inference75,140,We finally only retain KB triples where the entities also appear in the Wikipedia articles 4 to try to guarantee that all QA pairs will be equally answerable by either the KB or Wikipedia document sources .,Window + Title,KB,natural_language_inference,75,29,1,0,,0.00048394,0,negative,0.000288504,0.002302577,0.000875261,8.48E-06,0.014299462,0.001242395,0.00061878,0.000186263,0.000452821,0.979691165,2.26E-06,2.82E-05,3.83E-06
6985,natural_language_inference75,141,IE,Window + Title,,natural_language_inference,75,30,1,0,,3.51E-05,0,negative,3.82E-06,2.38E-05,3.36E-06,3.96E-06,1.80E-05,0.000152658,1.27E-05,0.000140741,6.13E-05,0.999565399,9.57E-06,4.11E-06,6.54E-07
6986,natural_language_inference75,142,"As an alternative to directly reading documents , we explore leveraging information extraction techniques to transform documents into a KB format .",Window + Title,IE,natural_language_inference,75,31,1,0,,0.152765333,0,negative,0.003550102,0.235100247,0.008244207,0.000273943,0.070302618,0.003463658,0.003176749,0.000451832,0.036429697,0.636953894,0.001500148,0.000422828,0.000130077
6987,natural_language_inference75,143,An IE - KB representation has attractive properties such as more precise and compact expressions of facts and logical key - value pairings based on subjectverb - object groupings .,Window + Title,IE,natural_language_inference,75,32,1,0,,0.005111027,0,negative,0.000294966,0.006920791,0.000513702,4.71E-06,0.000376727,0.000269482,0.000703508,6.24E-05,0.003615227,0.958611495,0.028401022,0.000208048,1.79E-05
6988,natural_language_inference75,144,This can come at the cost of lower recall due to malformed or completely missing triplets .,Window + Title,IE,natural_language_inference,75,33,1,0,,0.000171524,0,negative,0.00010042,2.98E-05,4.64E-06,3.15E-07,2.84E-05,3.31E-05,1.20E-05,5.92E-06,5.07E-05,0.999716864,1.11E-05,6.68E-06,1.50E-07
6989,natural_language_inference75,145,For IE we use standard open - source software followed by some task - specific engineering to improve the results .,Window + Title,IE,natural_language_inference,75,34,1,0,,0.005960836,0,negative,0.001266919,0.001666977,0.001124095,1.15E-05,0.015191117,0.001123773,0.000739423,5.70E-05,0.000259614,0.978462264,3.25E-06,9.02E-05,3.83E-06
6990,natural_language_inference75,146,"We first employ coreference resolution via the Stanford NLP Toolkit to reduce ambiguity by replacing pronominal ( "" he "" , "" it "" ) and nominal ( "" the film "" ) references with their representative entities .",Window + Title,IE,natural_language_inference,75,35,1,0,,0.308809253,0,negative,0.003225911,0.201934836,0.013012029,0.000357495,0.360202916,0.00742891,0.012259756,0.000596873,0.009231963,0.390773925,0.000190317,0.000588741,0.000196328
6991,natural_language_inference75,147,Next we use the SENNA semantic role labeling tool to uncover the grammatical structure of each sentence and pair verbs with their arguments .,Window + Title,IE,natural_language_inference,75,36,1,0,,0.022577531,0,negative,0.000403543,0.004798433,0.001744198,4.38E-05,0.09064732,0.001392903,0.000620443,6.08E-05,0.001194783,0.899036524,6.30E-06,4.14E-05,9.55E-06
6992,natural_language_inference75,148,"Each triplet is cleaned of words thatare not recognized entities , and lemmatization is done to collapse different inflections of important task - specific verbs to one form ( e.g. stars , starring , star ? starred ) .",Window + Title,IE,natural_language_inference,75,37,1,0,,0.000653354,0,negative,0.000117621,0.000748909,0.000196856,2.00E-05,0.053947487,0.001761395,0.000293874,7.10E-05,0.000238198,0.942593344,8.87E-07,7.88E-06,2.62E-06
6993,natural_language_inference75,149,"Finally , we append the movie title to each triple similar to the "" Window + Title "" representation of Sec. 3.2 , which improved results .",Window + Title,IE,natural_language_inference,75,38,1,0,,0.017327022,0,negative,0.005560065,0.001610396,0.001046368,3.92E-06,0.002524069,0.001589369,0.001237216,0.00010143,0.002333169,0.983817095,2.67E-06,0.000167753,6.48E-06
6994,natural_language_inference75,150,Question - Answer Pairs,Window + Title,,natural_language_inference,75,39,1,0,,0.0004961,0,negative,2.65E-05,0.000399086,0.000218322,9.40E-06,0.002457679,0.000536203,0.002534628,0.000484728,4.88E-05,0.991818126,0.000104545,0.001323231,3.87E-05
6995,natural_language_inference75,151,"Within the dataset 's more than 100,000 questionanswer pairs , we distinguish 13 classes of question corresponding to different kinds of edges in our KB .",Window + Title,Question - Answer Pairs,natural_language_inference,75,40,1,0,,1.08E-05,0,negative,1.84E-05,2.91E-06,5.86E-06,2.50E-06,0.001085651,6.35E-05,0.000199489,3.28E-06,8.58E-07,0.998596551,3.90E-08,1.98E-05,1.15E-06
6996,natural_language_inference75,152,"They range in scope from specific - such as actor to movie : "" What movies did Harrison Ford star in ? "" and movie to actors : "" Who starred in Blade Runner ? "" - to more general , such as tag to movie : "" Which films can be described by dystopian ? "" ; see for the full list .",Window + Title,Question - Answer Pairs,natural_language_inference,75,41,1,0,,4.54E-07,0,negative,1.91E-06,2.81E-07,3.61E-07,3.09E-07,1.76E-06,2.79E-05,2.23E-05,3.90E-06,9.48E-07,0.999936527,1.34E-07,3.48E-06,1.70E-07
6997,natural_language_inference75,153,For some question there can be multiple correct answers .,Window + Title,Question - Answer Pairs,natural_language_inference,75,42,1,0,,3.46E-07,0,negative,1.60E-06,6.71E-08,3.61E-07,1.80E-08,4.51E-07,3.49E-06,7.51E-06,4.45E-07,5.59E-07,0.99998267,4.07E-08,2.74E-06,3.91E-08
6998,natural_language_inference75,154,"Using SimpleQuestions , an existing open - domain question answering dataset based on Freebase , we identified the subset of questions posed by human annotators that covered our question types .",Window + Title,Question - Answer Pairs,natural_language_inference,75,43,1,0,,3.24E-05,0,negative,5.64E-05,2.44E-05,2.19E-05,6.84E-06,0.003124083,8.98E-05,0.000425248,7.30E-06,3.47E-06,0.99615612,1.48E-07,8.12E-05,3.16E-06
6999,natural_language_inference75,155,We created our question set by substituting the entities in those questions with entities from all of our KB triples .,Window + Title,Question - Answer Pairs,natural_language_inference,75,44,1,0,,4.28E-06,0,negative,4.23E-06,2.96E-06,3.09E-06,4.65E-07,0.000103504,0.000171327,0.0002499,2.56E-05,3.22E-06,0.999430309,1.92E-08,4.52E-06,8.86E-07
7000,natural_language_inference75,156,"For example , if the original question written by an annotator was "" What movies did Harrison Ford star in ? "" , we created a pattern "" What movies did [ @actor ] star in ? "" , which we substitute for any other actors in our set , and repeat this for all annotations .",Window + Title,Question - Answer Pairs,natural_language_inference,75,45,1,0,,2.94E-07,0,negative,1.30E-06,3.35E-07,8.07E-07,1.33E-08,1.73E-06,1.11E-05,1.96E-05,2.20E-06,2.28E-06,0.999958821,1.00E-08,1.72E-06,6.01E-08
7001,natural_language_inference75,157,"We split the questions into disjoint training , development and test sets with ? 96 k , 10 k and 10 k examples , respectively .",Window + Title,Question - Answer Pairs,natural_language_inference,75,46,1,0,,6.02E-06,0,negative,7.80E-06,3.00E-06,2.02E-06,2.38E-07,2.75E-05,0.000280257,0.001006868,7.17E-05,2.54E-06,0.998586625,1.76E-08,9.63E-06,1.79E-06
7002,natural_language_inference75,158,The same question ( even worded differently ) can not appear in both train and test sets .,Window + Title,Question - Answer Pairs,natural_language_inference,75,47,1,0,,1.94E-07,0,negative,1.85E-06,2.68E-08,7.66E-08,3.35E-09,1.93E-07,1.67E-06,1.07E-05,3.85E-07,1.04E-07,0.999977424,1.16E-08,7.54E-06,1.95E-08
7003,natural_language_inference75,159,"Note that this is much larger than most existing datasets ; for example , the WIK - IQA dataset for which we also conduct experiments in Sec. 5.2 has only ? 1000 training pairs .",Window + Title,Question - Answer Pairs,natural_language_inference,75,48,1,0,,1.31E-06,0,negative,1.10E-05,1.24E-06,2.23E-06,6.86E-07,6.92E-05,0.000105461,0.000400007,1.06E-05,7.46E-07,0.999380615,3.40E-08,1.63E-05,2.00E-06
7004,natural_language_inference75,160,This section describes our experiments on WIKI - MOVIES and WIKIQA .,Window + Title,Question - Answer Pairs,natural_language_inference,75,49,1,0,,1.57E-05,0,negative,4.82E-05,8.76E-06,1.70E-05,4.25E-07,0.000210514,3.32E-05,0.000943355,4.21E-06,1.95E-06,0.998469499,6.54E-08,0.000260153,2.63E-06
7005,natural_language_inference75,161,WikiMovies,Window + Title,,natural_language_inference,75,50,1,1,results,9.65E-05,0,negative,0.000129704,0.000413077,0.000177641,0.001704341,0.028008378,0.011461925,0.003998789,0.00273818,8.27E-05,0.95009576,1.20E-05,0.000757436,0.000420025
7006,natural_language_inference75,162,We conducted experiments on the WIKI - MOVIES dataset described in Sec .,Window + Title,WikiMovies,natural_language_inference,75,51,1,0,,0.006594298,0,negative,0.000434883,0.000124872,0.000102429,5.32E-07,0.001014198,0.000335946,0.003828964,2.07E-05,1.27E-05,0.993080763,2.26E-07,0.001041716,2.14E-06
7007,natural_language_inference75,163,"4 . Our main goal is to compare the performance of KB , IE and Wikipedia ( Doc ) sources when trying varying learning methods .",Window + Title,WikiMovies,natural_language_inference,75,52,1,0,,0.000558214,0,negative,3.13E-05,0.000491629,0.000133743,5.50E-06,0.00051256,0.002169025,0.003279666,0.000165116,0.000220806,0.992786439,4.68E-05,0.000138794,1.87E-05
7008,natural_language_inference75,164,We compare four approaches : ( i ) the QA system of that performs well on existing datasets WebQuestions and SimpleQuestions that use KBs only ; ( ii ) supervised embeddings that do not make use of a KB at all but learn question - to - answer embeddings directly and hence act as a sanity check ; ( iii ) Memory Networks ; and ( iv ) Key - Value Memory Networks .,Window + Title,WikiMovies,natural_language_inference,75,53,1,0,,0.119701378,0,negative,0.000568646,0.006984243,0.004079155,2.53E-06,0.001933223,0.002696088,0.054046197,0.000275433,0.001042799,0.925626479,1.28E-05,0.002662889,6.95E-05
7009,natural_language_inference75,165,"Performance is reported using the accuracy of the top hit ( single answer ) over all possible answers ( all entities ) , i.e. the hits @ 1 metric measured in percent .",Window + Title,WikiMovies,natural_language_inference,75,54,1,0,,3.67E-05,0,negative,1.73E-06,5.71E-06,4.72E-06,9.91E-08,2.29E-05,0.000558087,0.000577513,3.21E-05,6.53E-06,0.998772334,3.34E-07,1.73E-05,5.88E-07
7010,natural_language_inference75,166,"In all cases hyperparameters are optimized on the development set , including the memory representations of Sec. 3.2 for MemNNs and KV - MemNNs .",Window + Title,WikiMovies,natural_language_inference,75,55,1,0,,0.008275792,0,negative,2.95E-05,0.000456183,1.66E-05,2.68E-06,0.000149054,0.05721957,0.01137048,0.01241561,0.000230422,0.918083653,2.77E-07,1.68E-05,9.12E-06
7011,natural_language_inference75,167,"As MemNNs do not support key - value pairs , we concatenate key and value together when they differ instead .",Window + Title,WikiMovies,natural_language_inference,75,56,1,0,,4.41E-05,0,negative,0.000109156,0.001320523,0.000742596,4.53E-07,0.000230068,0.00063275,0.000635539,7.91E-05,0.003353142,0.992865757,4.74E-07,2.77E-05,2.74E-06
7012,natural_language_inference75,168,The main results are given in .,Window + Title,WikiMovies,natural_language_inference,75,57,1,0,,4.04E-05,0,negative,3.36E-06,1.05E-05,4.92E-06,3.45E-07,2.19E-05,0.000515533,0.000255575,4.03E-05,3.38E-05,0.999103115,6.72E-07,8.82E-06,1.17E-06
7013,natural_language_inference75,169,"The QA system of outperforms Supervised Embeddings and Memory Networks for KB and IE - based KB representations , but is designed to work with a KB , not with documents ( hence the N / A in that column ) .",Window + Title,WikiMovies,natural_language_inference,75,58,1,0,,0.000472733,0,negative,7.81E-05,0.000143981,0.00133152,1.08E-05,0.005039426,0.002471272,0.035594827,5.13E-05,1.99E-05,0.95339835,3.20E-05,0.001675031,0.000153469
7014,natural_language_inference75,170,"However , Key - Value Memory Networks outperform all other methods on all three data source types .",Window + Title,WikiMovies,natural_language_inference,75,59,1,1,results,0.338887525,0,negative,0.005137405,7.52E-05,7.23E-05,1.72E-05,0.00049444,0.01354242,0.450864295,0.001234079,1.50E-05,0.485872337,3.64E-06,0.042225463,0.000446239
7015,natural_language_inference75,171,"Reading from Wikipedia documents directly ( Doc ) outperforms an IE - based KB ( IE ) , which is an encouraging result towards automated machine reading though a gap to a humanannotated KB still remains ( 93.9 vs. 76.2 ) .",Window + Title,WikiMovies,natural_language_inference,75,60,1,1,results,0.449470335,0,negative,0.00218493,6.43E-05,0.000313088,9.80E-07,0.000222141,0.000615578,0.371830015,6.21E-05,8.80E-06,0.507033791,9.68E-06,0.117402741,0.000251856
7016,natural_language_inference75,172,"The best memory representation for directly reading documents uses "" Window - level + Center Encoding + Title "" ( W = 7 and H = 2 ) ; see for a comparison of results for different representation types .",Window + Title,WikiMovies,natural_language_inference,75,61,1,0,,0.050187728,0,negative,6.96E-05,0.000115538,0.001607871,1.02E-06,0.000411933,0.004694304,0.034555873,0.000177741,0.000119126,0.957787064,2.26E-06,0.000408267,4.94E-05
7017,natural_language_inference75,173,"Both center encoding and title features help the windowlevel representation , while sentence - level is inferior .",Window + Title,WikiMovies,natural_language_inference,75,62,1,0,,0.048620462,0,negative,0.166188377,7.54E-05,0.000250028,8.02E-06,0.001133263,0.001465122,0.047159304,8.87E-05,2.73E-05,0.757777922,5.61E-07,0.025750206,7.57E-05
7018,natural_language_inference75,174,QA Breakdown,Window + Title,,natural_language_inference,75,63,1,0,,0.000921766,0,negative,0.000213078,7.93E-05,0.000368641,1.11E-05,0.001138605,0.000621709,0.000839249,0.000266456,0.000101914,0.995792531,1.81E-06,0.000514549,5.11E-05
7019,natural_language_inference75,175,A breakdown by question type comparing the different data sources for KV - Mem NNs is given in 0.6520 0.6652 AP- CNN 0.6886 0.6957 Attentive LSTM 0.6886 0.7069 Attentive CNN 0.6921 0.7108 L.D.C. 0.7058 0.7226 Memory Network 0.5170 0.5236,Window + Title,QA Breakdown,natural_language_inference,75,64,1,0,,1.91E-05,0,negative,7.55E-06,7.27E-07,2.26E-06,9.08E-08,2.14E-05,0.000522997,0.004925219,1.00E-05,1.29E-06,0.994492146,1.32E-08,1.41E-05,2.17E-06
7020,natural_language_inference75,176,"Key - Value Memory Network 0.7069 0.7265 ument based on the KB triples : for each relation type we have a set of template phrases ( 100 in total ) used to generate the fact , e.g .",Window + Title,QA Breakdown,natural_language_inference,75,65,1,0,,1.99E-05,0,negative,6.93E-05,1.49E-05,0.000310721,4.80E-08,0.000191997,0.000345913,0.016651843,5.41E-06,1.68E-05,0.982327421,1.50E-08,6.05E-05,5.17E-06
7021,natural_language_inference75,177,""" Blade Runner came out in 1982 "" for the entry BLADE RUNNER RELEASE_YEAR 1982 .",Window + Title,QA Breakdown,natural_language_inference,75,66,1,0,,7.60E-06,0,negative,9.83E-06,4.79E-07,7.49E-06,3.37E-07,0.00019578,0.000170001,0.001430338,9.53E-07,4.51E-07,0.99816908,2.58E-08,1.35E-05,1.77E-06
7022,natural_language_inference75,178,"We can then parameterize the complexity of our synthetic documents : ( i ) using one template , or all of them ; ( ii ) using conjunctions to combine facts into single sentences or not ; and ( iii ) using coreference between sentences where we replace the movie name with "" it "" .",Window + Title,QA Breakdown,natural_language_inference,75,67,1,0,,5.51E-05,0,negative,4.04E-05,3.21E-05,1.84E-05,3.08E-08,2.06E-05,0.000210093,0.003967312,1.33E-05,6.74E-05,0.995608152,1.46E-08,2.06E-05,1.66E-06
7023,natural_language_inference75,179,5,Window + Title,QA Breakdown,natural_language_inference,75,68,1,0,,1.53E-06,0,negative,3.06E-06,2.76E-07,5.52E-07,5.91E-08,7.63E-06,0.000118011,0.000340443,2.85E-06,1.72E-06,0.999523393,3.38E-09,1.60E-06,4.11E-07
7024,natural_language_inference75,180,The purpose of this experiment is to find which aspects are responsible for the gap in performance to a KB .,Window + Title,QA Breakdown,natural_language_inference,75,69,1,0,,1.91E-05,0,negative,1.10E-05,3.41E-05,2.82E-05,8.27E-07,0.000402973,0.000516827,0.004664061,1.35E-05,1.57E-05,0.994279584,3.96E-07,1.89E-05,1.39E-05
7025,natural_language_inference75,181,The results are given in .,Window + Title,QA Breakdown,natural_language_inference,75,70,1,0,,3.18E-05,0,negative,4.27E-06,2.88E-07,1.47E-06,1.61E-08,1.24E-05,5.84E-05,0.000949872,1.07E-06,5.51E-07,0.998961838,3.89E-09,9.31E-06,5.26E-07
7026,natural_language_inference75,182,"They indicate that some of the loss ( 93.9 % for KB to 82.9 % for One Template Sentence ) in performance is due directly to representing in sentence form , making the subject , relation and object harder to extract .",Window + Title,QA Breakdown,natural_language_inference,75,71,1,0,,0.000123362,0,negative,0.0001276,2.40E-06,2.88E-05,1.53E-08,1.50E-05,4.36E-05,0.003483682,1.08E-06,2.36E-06,0.996222557,1.07E-08,7.17E-05,1.20E-06
7027,natural_language_inference75,183,Moving to a larger number of templates does not deteriorate performance much ( 80 % ) .,Window + Title,QA Breakdown,natural_language_inference,75,72,1,0,,0.151845458,0,experiments,0.024342645,6.56E-06,1.60E-05,8.09E-07,9.85E-05,0.000781711,0.540014539,3.60E-05,1.97E-06,0.427937746,4.14E-08,0.006618987,0.000144476
7028,natural_language_inference75,184,The remaining performance drop seems to be split roughly equally between conjunctions ( 74 % ) and coreference ( 76 % ) .,Window + Title,QA Breakdown,natural_language_inference,75,73,1,0,,0.002843376,0,negative,0.012641212,5.53E-06,4.45E-05,8.74E-07,0.001059754,0.00023728,0.061758132,3.69E-06,1.76E-06,0.922868686,8.51E-09,0.001357111,2.15E-05
7029,natural_language_inference75,185,The hardest synthetic dataset combines these ( All Templates + Conj. + Coref. ) and is actually harder than using the real Wikipedia documents ( 72.5 % vs. 76.2 % ) .,Window + Title,QA Breakdown,natural_language_inference,75,74,1,0,,0.004571927,0,negative,4.85E-05,2.69E-06,2.61E-05,2.93E-07,0.001667116,0.000321077,0.029784044,2.99E-06,3.07E-07,0.967859768,4.44E-09,0.000277019,1.00E-05
7030,natural_language_inference75,186,"This is possibly because the amount of conjunctions and coreferences we make are artificially too high ( 50 % and 80 % of the time , respectively ) .",Window + Title,QA Breakdown,natural_language_inference,75,75,1,0,,7.35E-06,0,negative,1.38E-05,7.18E-07,1.08E-06,7.07E-08,9.51E-05,0.000122163,0.000899349,1.92E-06,2.79E-07,0.998859042,1.63E-09,5.81E-06,7.06E-07
7031,natural_language_inference75,187,WikiQA,Window + Title,,natural_language_inference,75,76,1,1,results,4.35E-06,0,negative,0.000135643,0.000132435,0.0008978,0.002390871,0.035638439,0.020273358,0.00665409,0.002066706,5.20E-05,0.929461656,5.53E-07,0.000515393,0.001781038
7032,natural_language_inference75,188,WIKIQA is an existing dataset for answer sentence selection using Wikipedia as the knowledge source .,Window + Title,WikiQA,natural_language_inference,75,77,1,0,,0.001121366,0,negative,2.79E-05,0.000111318,0.000462553,0.000219913,0.262746048,0.007955161,0.035695227,3.38E-05,3.34E-06,0.692148577,1.35E-06,0.00014394,0.00045082
7033,natural_language_inference75,189,"The task is , given a question , to select the sentence coming from a Wikipedia document that best answers the question , where performance is measured using mean average preci-sion ( MAP ) and mean reciprocal rank ( MRR ) of the ranked set of answers .",Window + Title,WikiQA,natural_language_inference,75,78,1,0,,6.79E-05,0,negative,6.75E-06,9.74E-05,6.39E-05,2.14E-05,0.002406521,0.002337136,0.003136224,4.74E-05,2.97E-05,0.991777282,4.60E-06,1.59E-05,5.59E-05
7034,natural_language_inference75,190,"The dataset uses a pre-built information retrieval step and hence provides a fixed set of candidate sentences per question , so systems do not have to consider ranking all of Wikipedia .",Window + Title,WikiQA,natural_language_inference,75,79,1,0,,6.53E-05,0,negative,3.60E-06,1.03E-05,2.01E-05,3.57E-06,0.037331238,0.000858707,0.000481878,4.92E-06,9.96E-07,0.961278839,3.38E-09,4.39E-06,1.46E-06
7035,natural_language_inference75,191,"In contrast to WIKIMOVIES , the training set size is small ( ? 1000 examples ) while the topic is much more broad ( all of Wikipedia , rather than just movies ) and the questions can only be answered by reading the documents , so no comparison to the use of KBs can be performed .",Window + Title,WikiQA,natural_language_inference,75,80,1,0,,0.000141748,0,negative,4.14E-05,0.000156108,0.000124978,1.10E-06,0.010678796,0.001280295,0.0085846,2.44E-05,7.64E-06,0.979001964,4.97E-08,9.01E-05,8.57E-06
7036,natural_language_inference75,192,"However , a wide range of methods have already been tried on WIKIQA , thus providing a useful benchmark to test if the same results found on WIKIMOVIES carry across to WIKIQA , in particular the performance of Key - Value Memory Networks .",Window + Title,WikiQA,natural_language_inference,75,81,1,0,,0.000152187,0,negative,1.05E-06,1.79E-06,2.62E-06,3.69E-08,2.17E-05,0.000102842,0.000824946,2.49E-06,6.79E-07,0.999031065,2.48E-07,9.44E-06,1.09E-06
7037,natural_language_inference75,193,"Due to the size of the training set , following many other works we pre-trained the word vectors ( matrices A and B which are constrained to be identical ) before training KV - MemNNs .",Window + Title,WikiQA,natural_language_inference,75,82,1,0,,2.82E-05,0,negative,2.89E-05,0.000183936,3.51E-05,2.64E-07,0.001138068,0.000650896,0.000951304,3.65E-05,4.82E-05,0.996918716,4.42E-09,6.58E-06,1.45E-06
7038,natural_language_inference75,194,"We employed Supervised Embeddings for that goal , training on all of Wikipedia while treating the input as a random sentence and the target as the subsequent sentence .",Window + Title,WikiQA,natural_language_inference,75,83,1,0,,7.16E-05,0,negative,1.64E-05,0.000766623,7.14E-05,1.41E-06,0.001295354,0.010920201,0.004638435,0.001086529,0.000892569,0.980292335,1.09E-08,2.11E-06,1.66E-05
7039,natural_language_inference75,195,"We then trained KV - MemNNs with dropout regularization : we sample words from the question , memory representations and the answers , choosing the dropout rate using the development set .",Window + Title,WikiQA,natural_language_inference,75,84,1,0,,0.025096647,0,negative,9.54E-05,0.001096338,0.000562226,1.24E-06,0.00289687,0.012537589,0.042376937,0.000774698,0.000976857,0.938597185,1.29E-08,1.35E-05,7.11E-05
7040,natural_language_inference75,196,"Finally , again following other successful methods , we combine our approach with exact matching word features between question and answers .",Window + Title,WikiQA,natural_language_inference,75,85,1,0,,0.044336942,0,negative,0.0011489,0.002440763,0.001269507,2.85E-06,0.009697375,0.002165731,0.036304064,8.45E-05,0.000494837,0.945775409,3.68E-08,0.00055006,6.60E-05
7041,natural_language_inference75,197,Key hashing was not used as candidates were already pre-selected .,Window + Title,WikiQA,natural_language_inference,75,86,1,0,,4.56E-05,0,negative,2.16E-05,3.21E-05,7.59E-05,7.59E-07,0.000860222,0.005566618,0.003021207,8.46E-05,3.36E-05,0.990292219,4.88E-09,3.69E-06,7.56E-06
7042,natural_language_inference75,198,"To represent the memories , we used the Window - Level representation ( the best choice on the dev set was W = 7 ) as the key and the whole sentence as the value , as the value should match the answer which in this case is a sentence .",Window + Title,WikiQA,natural_language_inference,75,87,1,0,,0.001153287,0,negative,1.92E-05,0.000263964,6.83E-05,5.34E-07,0.000357745,0.04399898,0.048459244,0.002993184,0.000756405,0.903013816,1.67E-08,4.99E-06,6.37E-05
7043,natural_language_inference75,199,"Additionally , in the representation all numbers in the text and the phrase "" how many "" in the question were replaced with the feature "" _number_ "" .",Window + Title,WikiQA,natural_language_inference,75,88,1,0,,3.36E-05,0,negative,6.89E-06,2.21E-05,9.02E-06,4.79E-06,0.001791274,0.011126856,0.001358644,0.000252528,2.86E-05,0.985391541,2.36E-09,1.06E-06,6.70E-06
7044,natural_language_inference75,200,The best choice of hops was also H = 2 for KV - MemNNs .,Window + Title,WikiQA,natural_language_inference,75,89,1,0,,0.007534949,0,negative,9.25E-05,0.000210557,3.02E-05,3.63E-06,0.001193252,0.085333636,0.240876103,0.006062984,7.87E-05,0.665771485,2.72E-08,5.03E-05,0.000296605
7045,natural_language_inference75,201,The results are given in .,Window + Title,WikiQA,natural_language_inference,75,90,1,0,,4.20E-05,0,negative,2.31E-06,7.47E-07,1.60E-06,1.96E-08,5.87E-05,7.81E-05,0.000320321,1.83E-06,7.27E-07,0.999531141,9.96E-10,4.09E-06,4.02E-07
7046,natural_language_inference75,202,"Key - Value Memory Networks outperform a large set of other methods , although the results of the L.D.C. method of are very similar .",Window + Title,WikiQA,natural_language_inference,75,91,1,1,results,0.152629321,0,negative,0.000370445,1.33E-05,7.26E-05,9.14E-08,7.22E-05,0.000677091,0.300623793,2.43E-05,3.13E-06,0.694321657,6.23E-08,0.003737678,8.36E-05
7047,natural_language_inference75,203,"Memory Networks , which can not easily pair windows to sentences , perform much worse , highlighting the importance of key - value memories .",Window + Title,WikiQA,natural_language_inference,75,92,1,0,,0.397294604,0,negative,0.003404514,2.20E-05,3.65E-05,4.69E-07,0.000249307,0.001505008,0.458402017,9.12E-05,3.47E-06,0.525927518,2.11E-08,0.010208979,0.00014903
7048,part-of-speech_tagging7,1,title,,,part-of-speech_tagging,7,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
7049,part-of-speech_tagging7,2,Multilingual Part - of - Speech Tagging with Bidirectional Long Short - Term Memory Models and Auxiliary Loss,title,title,part-of-speech_tagging,7,1,1,1,research-problem,0.998647702,1,research-problem,6.20E-08,2.11E-05,3.07E-07,6.05E-08,5.94E-08,1.19E-07,1.09E-06,2.11E-06,3.05E-06,0.001993898,0.997977728,3.12E-07,6.30E-08
7050,part-of-speech_tagging7,3,abstract,,,part-of-speech_tagging,7,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
7051,part-of-speech_tagging7,4,"Bidirectional long short - term memory ( bi - LSTM ) networks have recently proven successful for various NLP sequence modeling tasks , but little is known about their reliance to input representations , target languages , data set size , and label noise .",abstract,abstract,part-of-speech_tagging,7,1,1,0,,0.673703004,1,research-problem,4.61E-08,1.27E-05,1.60E-07,1.53E-07,1.02E-07,1.83E-07,4.04E-07,1.62E-06,1.92E-06,0.007593492,0.992389136,4.78E-08,3.57E-08
7052,part-of-speech_tagging7,5,"We address these issues and evaluate bi - LSTMs with word , character , and unicode byte embeddings for POS tagging .",abstract,abstract,part-of-speech_tagging,7,2,1,1,research-problem,0.322746704,0,research-problem,2.30E-05,0.221386385,0.000232524,2.72E-05,0.000700495,7.26E-05,6.49E-05,0.000956173,0.005539495,0.324959247,0.44600648,2.65E-05,4.96E-06
7053,part-of-speech_tagging7,6,We compare bi - LSTMs to traditional POS taggers across languages and data sizes .,abstract,abstract,part-of-speech_tagging,7,3,1,0,,0.02610013,0,negative,1.36E-05,0.110425069,0.000102847,2.40E-05,0.0007764,8.50E-05,7.90E-05,0.000971848,0.001801286,0.565260282,0.320422448,3.40E-05,4.22E-06
7054,part-of-speech_tagging7,7,"We also present a novel bi - LSTM model , which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words .",abstract,abstract,part-of-speech_tagging,7,4,1,0,,0.347689576,0,research-problem,6.47E-05,0.245596257,0.000949283,2.74E-05,0.000335669,7.83E-05,3.66E-05,0.000652966,0.106747867,0.286409729,0.359076269,1.79E-05,7.10E-06
7055,part-of-speech_tagging7,8,"The model obtains state - of - the - art performance across 22 languages , and works especially well for morphologically complex languages .",abstract,abstract,part-of-speech_tagging,7,5,1,0,,0.026146243,0,research-problem,2.02E-05,0.002434343,8.08E-06,2.46E-05,7.21E-05,4.15E-05,7.77E-05,0.000419419,0.000144168,0.266225252,0.730432129,9.39E-05,6.51E-06
7056,part-of-speech_tagging7,9,Our analysis suggests that bi - LSTMs are less sensitive to training data size and label corruptions ( at small noise levels ) than previously assumed .,abstract,abstract,part-of-speech_tagging,7,6,1,0,,0.016692376,0,negative,0.000178434,0.012393262,4.24E-06,7.36E-05,0.000322986,5.76E-05,1.69E-05,0.000635423,0.000404498,0.905536229,0.080346846,2.78E-05,2.18E-06
7057,part-of-speech_tagging7,10,Introduction,,,part-of-speech_tagging,7,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
7058,part-of-speech_tagging7,11,"Recently , bidirectional long short - term memory networks ( bi - LSTM ) have been used for language modelling , POS tagging , transition - based dependency parsing , fine - grained sentiment analysis , syntactic chunking , and semantic role labeling .",Introduction,Introduction,part-of-speech_tagging,7,1,1,0,,0.218417752,0,research-problem,2.44E-06,0.000522779,2.15E-06,2.39E-06,7.31E-06,1.53E-05,2.39E-05,2.15E-05,0.001118823,0.091283842,0.906994941,2.58E-06,2.18E-06
7059,part-of-speech_tagging7,12,LSTMs are recurrent neural networks ( RNNs ) in which layers are designed to prevent vanishing gradients .,Introduction,Introduction,part-of-speech_tagging,7,2,1,0,,0.881226564,1,model,1.95E-05,0.030028521,0.000248128,3.46E-05,0.000153107,0.000576999,0.000230919,0.000656484,0.33639072,0.309030715,0.322590025,1.15E-05,2.89E-05
7060,part-of-speech_tagging7,13,Bidirectional LSTMs make a backward and forward pass through the sequence before passing onto the next layer .,Introduction,Introduction,part-of-speech_tagging,7,3,1,0,,0.914931634,1,model,1.64E-05,0.028894578,0.000207372,1.06E-06,7.25E-05,1.92E-05,1.06E-05,2.15E-05,0.942232338,0.025798349,0.002721964,2.88E-06,1.21E-06
7061,part-of-speech_tagging7,14,"For further details , see .",Introduction,Introduction,part-of-speech_tagging,7,4,1,0,,0.003136713,0,negative,6.69E-06,0.000711424,1.16E-06,3.27E-05,0.000390317,7.65E-05,9.85E-06,4.26E-05,0.000589605,0.992998284,0.005138292,1.87E-06,7.30E-07
7062,part-of-speech_tagging7,15,We consider using bi - LSTMs for POS tagging .,Introduction,Introduction,part-of-speech_tagging,7,5,1,0,,0.751362572,1,negative,0.000131982,0.167782146,0.000136874,2.16E-05,0.000612667,0.000154092,0.000128424,0.000351516,0.264215445,0.346752125,0.219626728,7.12E-05,1.52E-05
7063,part-of-speech_tagging7,16,Previous work on using deep learning - based methods for POS tagging has focused either on a single language or a small set of languages .,Introduction,Introduction,part-of-speech_tagging,7,6,1,0,,0.769001344,1,research-problem,1.83E-06,0.000443815,5.91E-07,5.08E-06,1.76E-05,9.36E-06,2.13E-05,1.65E-05,7.30E-05,0.09112378,0.908282303,2.75E-06,2.18E-06
7064,part-of-speech_tagging7,17,Instead we evaluate our models across 22 languages .,Introduction,Introduction,part-of-speech_tagging,7,7,1,0,,0.12906146,0,negative,7.33E-05,0.113034745,0.000107553,6.58E-05,0.032638373,0.000397009,0.000224998,0.000291266,0.013796464,0.832548019,0.006760863,5.28E-05,8.79E-06
7065,part-of-speech_tagging7,18,"In addition , we compare performance with representations at different levels of granularity ( words , characters , and bytes ) .",Introduction,Introduction,part-of-speech_tagging,7,8,1,0,,0.215734006,0,approach,6.32E-05,0.656434822,8.21E-05,1.85E-05,0.004253432,0.000248969,0.000205919,0.000653786,0.146076571,0.187607132,0.004292332,5.46E-05,8.63E-06
7066,part-of-speech_tagging7,19,"These levels of representation were previously introduced in different efforts , but a comparative evaluation was missing .",Introduction,Introduction,part-of-speech_tagging,7,9,1,0,,0.012985427,0,negative,1.11E-05,0.004751499,5.08E-06,1.48E-05,0.000208471,0.000117426,3.08E-05,0.000105813,0.007151684,0.9171669,0.07042716,6.85E-06,2.34E-06
7067,part-of-speech_tagging7,20,"Moreover , deep networks are often said to require large volumes of training data .",Introduction,Introduction,part-of-speech_tagging,7,10,1,0,,0.020679838,0,negative,8.02E-06,0.00208248,1.15E-06,2.28E-05,0.000102941,6.94E-05,3.47E-05,0.000108996,0.000868331,0.675232628,0.321458002,6.13E-06,4.43E-06
7068,part-of-speech_tagging7,21,We investigate to what extent bi - LSTMs are more sensitive to the amount of training data and label noise than standard POS taggers .,Introduction,Introduction,part-of-speech_tagging,7,11,1,0,,0.261422684,0,approach,0.000132894,0.682251182,6.48E-05,2.64E-05,0.004783743,0.000125617,0.000202282,0.000371969,0.08397763,0.211119939,0.016838207,9.36E-05,1.18E-05
7069,part-of-speech_tagging7,22,"Finally , we introduce a novel model , a bi - LSTM trained with auxiliary loss .",Introduction,Introduction,part-of-speech_tagging,7,12,1,1,model,0.855583695,1,model,0.00016363,0.281442602,0.000301315,2.20E-05,0.002762609,6.94E-05,6.16E-05,8.62E-05,0.684442537,0.029608461,0.001005965,2.71E-05,6.54E-06
7070,part-of-speech_tagging7,23,The model jointly predicts the POS and the log frequency of the word .,Introduction,Introduction,part-of-speech_tagging,7,13,1,1,model,0.833135282,1,model,3.02E-06,0.013087794,1.72E-05,4.34E-07,2.05E-05,3.34E-05,9.68E-06,7.85E-05,0.969905534,0.016308245,0.000534256,5.62E-07,9.35E-07
7071,part-of-speech_tagging7,24,"The intuition behind this model is that the auxiliary loss , being predictive of word frequency , helps to differentiate the representations of rare and common words .",Introduction,Introduction,part-of-speech_tagging,7,14,1,0,,0.504902549,1,model,8.31E-06,0.040151529,4.03E-05,1.15E-06,5.37E-05,3.68E-05,1.15E-05,8.49E-05,0.930243426,0.028160724,0.001204315,1.98E-06,1.35E-06
7072,part-of-speech_tagging7,25,We indeed observe performance gains on rare and out - of - vocabulary words .,Introduction,Introduction,part-of-speech_tagging,7,15,1,0,,0.099907959,0,negative,0.005562406,0.045446775,5.31E-05,7.50E-05,0.002630433,0.000294435,0.002747269,0.000742842,0.011813993,0.918645235,0.008383746,0.003554998,4.97E-05
7073,part-of-speech_tagging7,26,These performance gains transfer into general improvements for morphologically rich languages .,Introduction,Introduction,part-of-speech_tagging,7,16,1,0,,0.016957902,0,negative,4.41E-05,0.002573044,4.35E-06,1.20E-05,0.000325172,6.39E-05,8.52E-05,6.25E-05,0.002273738,0.97732555,0.017177479,4.93E-05,3.66E-06
7074,part-of-speech_tagging7,27,Contributions,Introduction,,part-of-speech_tagging,7,17,1,0,,0.001049685,0,negative,9.61E-06,0.006100595,5.17E-06,6.34E-05,0.000408056,0.000356984,4.07E-05,0.000367874,0.022939863,0.96554715,0.004152764,2.82E-06,5.00E-06
7075,part-of-speech_tagging7,28,"In this paper , we a ) evaluate the effectiveness of different representations in bi - LSTMs , b ) compare these models across a large set of languages and under varying conditions ( data size , label noise ) and c) propose a novel bi - LSTM model with auxiliary loss ( LOGFREQ ) .",Introduction,Contributions,part-of-speech_tagging,7,18,1,0,,0.814780488,1,approach,2.49E-05,0.845809502,0.000107821,4.45E-06,0.001145039,1.95E-05,1.80E-05,0.000115398,0.096177453,0.044454691,0.012104481,1.72E-05,1.61E-06
7076,part-of-speech_tagging7,29,Tagging with bi - LSTMs,Introduction,Contributions,part-of-speech_tagging,7,19,1,0,,0.293644163,0,research-problem,7.12E-06,0.011558136,2.27E-05,2.68E-06,4.09E-05,2.94E-05,5.40E-05,0.000107299,0.016473693,0.187522915,0.78414931,2.76E-05,4.33E-06
7077,part-of-speech_tagging7,30,Recurrent neural networks ( RNNs ) allow the computation of fixed - size vector representations for word sequences of arbitrary length .,Introduction,Contributions,part-of-speech_tagging,7,20,1,0,,0.749236279,1,research-problem,7.44E-07,0.005247641,4.54E-06,5.60E-06,2.68E-05,3.58E-05,1.32E-05,0.00011025,0.007243024,0.255617164,0.731691905,1.34E-06,2.02E-06
7078,part-of-speech_tagging7,31,"An RNN is a function that reads inn vectors x 1 , ... , x n and produces an output vector h n , that depends on the entire sequence x 1 , ... , x n .",Introduction,Contributions,part-of-speech_tagging,7,21,1,0,,0.002125361,0,negative,1.26E-06,0.043152558,1.31E-05,2.03E-05,0.000475038,0.000102092,9.49E-06,0.000275201,0.092246439,0.807448597,0.05625277,1.14E-06,1.93E-06
7079,part-of-speech_tagging7,32,"The vector h n is then fed as an input to some classifier , or higher - level RNNs in stacked / hierarchical models .",Introduction,Contributions,part-of-speech_tagging,7,22,1,0,,0.029178076,0,negative,3.11E-06,0.029082369,6.65E-06,5.12E-06,0.000297492,0.000100405,4.27E-06,0.000469014,0.086060137,0.882117838,0.001852202,9.04E-07,4.84E-07
7080,part-of-speech_tagging7,33,The entire network is trained jointly such that the hidden representation captures the important information from the sequence for the prediction task .,Introduction,Contributions,part-of-speech_tagging,7,23,1,0,,0.782643671,1,model,4.57E-06,0.193687711,1.74E-05,9.01E-07,0.000208015,1.70E-05,1.90E-06,0.000145171,0.778544175,0.02725403,0.000118428,4.60E-07,2.82E-07
7081,part-of-speech_tagging7,34,"A bidirectional recurrent neural network ( bi - RNN ) is an extension of an RNN that reads the input sequence twice , from left to right and right to left , and the encodings are concatenated .",Introduction,Contributions,part-of-speech_tagging,7,24,1,0,,0.730994922,1,model,3.35E-06,0.082431323,0.000263356,2.86E-06,0.000191577,4.70E-05,2.15E-05,0.000120747,0.711046299,0.151743509,0.054121632,3.43E-06,3.39E-06
7082,part-of-speech_tagging7,35,"The literature uses the term bi - RNN to refer to two related architectures , which we refer to here as "" context bi - RNN "" and "" sequence bi - RNN "" .",Introduction,Contributions,part-of-speech_tagging,7,25,1,0,,0.007295695,0,negative,1.85E-06,0.022137551,2.99E-05,1.33E-05,0.00038076,0.000160514,2.45E-05,0.000279272,0.039156683,0.867883336,0.069927483,2.20E-06,2.67E-06
7083,part-of-speech_tagging7,36,"In a sequence bi - RNN ( b i - RNN seq ) , the input is a sequence of vectors x 1:n and the output is a concatenation ( ) of a forward ( f ) and reverse ( r ) RNN each reading the sequence in a different directions : v = bi - RNN seq ( x 1:n ) = RNN f ( x 1:n ) RNN r ( x n:1 )",Introduction,Contributions,part-of-speech_tagging,7,26,1,0,,0.040624383,0,negative,1.54E-06,0.045281948,3.24E-05,8.82E-07,0.000170633,5.67E-05,6.90E-06,0.000153719,0.28650176,0.658792238,0.008998733,1.96E-06,6.33E-07
7084,part-of-speech_tagging7,37,"In a context bi - RNN ( bi - RNN ctx ) , we get an additional input i indicating a sequence position , and the resulting vectors vi result from concatenating the RNN encodings up to i:",Introduction,Contributions,part-of-speech_tagging,7,27,1,0,,0.072937303,0,model,5.54E-06,0.176846233,6.77E-05,1.22E-06,0.00040001,2.44E-05,3.41E-06,9.52E-05,0.43765355,0.38286353,0.002036353,2.45E-06,4.21E-07
7085,part-of-speech_tagging7,38,"Thus , the state vector vi in this bi - RNN encodes information at position i and its entire sequential context .",Introduction,Contributions,part-of-speech_tagging,7,28,1,0,,0.079916591,0,model,1.58E-06,0.073070845,7.28E-06,1.04E-06,0.000127291,3.97E-05,3.06E-06,0.000295343,0.613173359,0.312210174,0.001069204,7.90E-07,3.79E-07
7086,part-of-speech_tagging7,39,Another view of the context bi - RNN is of taking a sequence x 1:n and returning the corresponding sequence of state vectors v 1 :n .,Introduction,Contributions,part-of-speech_tagging,7,29,1,0,,0.003390864,0,negative,2.17E-06,0.038075954,2.11E-05,2.41E-06,0.000132753,6.17E-05,1.14E-05,0.000168057,0.077111881,0.833453706,0.050954233,3.49E-06,1.06E-06
7087,part-of-speech_tagging7,40,LSTMs ) are a variant of RNNs that replace the cells of RNNs with LSTM cells that were designed to prevent vanishing gradients .,Introduction,Contributions,part-of-speech_tagging,7,30,1,0,,0.629732612,1,model,2.09E-05,0.183840216,0.001982096,1.17E-05,0.001426807,0.000187378,0.000120714,0.000294741,0.43258937,0.344842791,0.034650918,2.15E-05,1.09E-05
7088,part-of-speech_tagging7,41,Bidirectional LSTMs are the bi - RNN counterpart based on LSTMs .,Introduction,Contributions,part-of-speech_tagging,7,31,1,0,,0.531515932,1,model,7.61E-06,0.088584884,0.000642547,3.95E-06,0.000317657,0.000130455,5.66E-05,0.000216158,0.57519523,0.309039732,0.025792179,7.94E-06,5.09E-06
7089,part-of-speech_tagging7,42,Our basic bi - LSTM tagging model is a context bi - LSTM taking as input word embeddings w .,Introduction,Contributions,part-of-speech_tagging,7,32,1,0,,0.4493801,0,model,5.16E-06,0.177400777,0.000108178,3.98E-06,0.000611325,8.87E-05,1.51E-05,0.000345128,0.745825327,0.074839886,0.000751836,1.97E-06,2.62E-06
7090,part-of-speech_tagging7,43,We incorporate subtoken information using an hierarchical bi - LSTM architecture .,Introduction,Contributions,part-of-speech_tagging,7,33,1,0,,0.72128334,1,model,1.10E-05,0.12893276,0.000100821,2.29E-06,0.000475043,2.16E-05,7.11E-06,5.07E-05,0.852235388,0.018017471,0.000142671,2.02E-06,1.15E-06
7091,part-of-speech_tagging7,44,We compute subtokenlevel ( either characters cor unicode byte b) embeddings of words using a sequence bi - LSTM at the lower level .,Introduction,Contributions,part-of-speech_tagging,7,34,1,0,,0.827932366,1,model,8.87E-06,0.252198236,0.000204683,9.08E-07,0.000746083,2.63E-05,1.11E-05,5.35E-05,0.721089675,0.025541026,0.000116338,2.41E-06,9.63E-07
7092,part-of-speech_tagging7,45,This representation is then concatenated with the ( learned ) word embeddings vector w which forms the input to the context bi - LSTM at the next layer .,Introduction,Contributions,part-of-speech_tagging,7,35,1,0,,0.059074937,0,negative,8.43E-06,0.112141321,2.26E-05,6.91E-06,0.000918255,0.000125496,1.05E-05,0.000581185,0.308122458,0.577727705,0.000331062,2.92E-06,1.19E-06
7093,part-of-speech_tagging7,46,"This model , illustrated in ( lower part in left figure ) , is inspired by .",Introduction,Contributions,part-of-speech_tagging,7,36,1,0,,0.016377338,0,negative,2.27E-06,0.017845463,2.23E-05,3.07E-05,0.00188107,0.000171092,1.04E-05,0.000228779,0.064689526,0.913887786,0.001227922,1.02E-06,1.72E-06
7094,part-of-speech_tagging7,47,"We also test models in which we only keep sub-token information , e.g. , either both byte and character embeddings , right ) or a single ( sub - ) token representation alone .",Introduction,Contributions,part-of-speech_tagging,7,37,1,0,,0.398166339,0,approach,3.35E-05,0.751287136,0.000278944,1.20E-05,0.018846125,0.000267348,0.00010692,0.000649387,0.043907455,0.184454018,0.000124486,2.95E-05,3.15E-06
7095,part-of-speech_tagging7,48,"In our novel model , cf. left , we train the bi - LSTM tagger to predict both the tags of the sequence , as well as a label that represents the log frequency of the token as estimated from the training data .",Introduction,Contributions,part-of-speech_tagging,7,38,1,0,,0.251450848,0,model,7.96E-06,0.305912415,0.000129611,2.08E-06,0.001542707,2.62E-05,6.79E-06,7.35E-05,0.652380708,0.039826782,8.83E-05,2.07E-06,9.61E-07
7096,part-of-speech_tagging7,49,"Our combined cross - entropy loss is now : L ( ? t , y t ) + L ( ? a , ya ) , where t stands for a POS tag and a is the log frequency label , i.e. , a = int ( log ( f req train ( w ) ) .",Introduction,Contributions,part-of-speech_tagging,7,39,1,0,,0.02773236,0,negative,1.15E-05,0.187594754,4.85E-05,8.58E-06,0.001310175,0.000249271,3.06E-05,0.001279072,0.223433864,0.584912215,0.00110978,8.07E-06,3.61E-06
7097,part-of-speech_tagging7,50,Combining this log frequency objective with the tagging task can be seen as an instance of multi-task learning in which the labels are predicted jointly .,Introduction,Contributions,part-of-speech_tagging,7,40,1,0,,0.046348042,0,negative,9.28E-06,0.278908981,3.94E-05,9.07E-06,0.001321509,4.87E-05,1.32E-05,0.000202045,0.135638409,0.577980631,0.005817162,9.94E-06,1.70E-06
7098,part-of-speech_tagging7,51,"The idea behind this model is to make the representation predictive for frequency , which encourages the model to not share representations between common and rare words , thus benefiting the handling of rare tokens .",Introduction,Contributions,part-of-speech_tagging,7,41,1,0,,0.260606996,0,model,9.69E-06,0.165498866,0.000171267,2.49E-06,0.000874409,3.90E-05,7.30E-06,0.000108967,0.771843396,0.061385158,5.70E-05,1.50E-06,9.19E-07
7099,part-of-speech_tagging7,52,"epochs , default learning rate ( 0.1 ) , 128 dimensions for word embeddings , 100 for character and byte embeddings , 100 hidden states and Gaussian noise with ?= 0.2 .",Introduction,Contributions,part-of-speech_tagging,7,42,1,1,hyperparameters,0.801314248,1,hyperparameters,2.18E-05,0.151912931,2.76E-05,0.000998303,0.006715506,0.088401472,0.002636319,0.459888596,0.032317878,0.256430839,0.000403753,1.18E-05,0.000233207
7100,part-of-speech_tagging7,53,"As training is stochastic in nature , we use a fixed seed throughout .",Introduction,Contributions,part-of-speech_tagging,7,43,1,1,hyperparameters,0.639441959,1,approach,8.55E-06,0.525720612,1.73E-05,1.86E-05,0.00168031,0.003866405,0.000197649,0.052332334,0.1904383,0.22560269,0.000102795,3.70E-06,1.07E-05
7101,part-of-speech_tagging7,54,"Embeddings are not initialized with pre-trained embeddings , except when reported otherwise .",Introduction,Contributions,part-of-speech_tagging,7,44,1,0,,0.04749087,0,negative,4.98E-06,0.091065798,1.21E-05,0.000102794,0.00401134,0.005830994,0.000125186,0.027364217,0.035841395,0.83554603,8.18E-05,3.60E-06,9.75E-06
7102,part-of-speech_tagging7,55,In that case we use offthe - shelf polyglot embeddings .,Introduction,Contributions,part-of-speech_tagging,7,45,1,1,hyperparameters,0.221543719,0,negative,4.73E-05,0.260350949,9.20E-05,4.86E-05,0.008185315,0.000491236,4.05E-05,0.001458446,0.052940121,0.676217002,0.000112037,1.25E-05,3.87E-06
7103,part-of-speech_tagging7,56,2,Introduction,Contributions,part-of-speech_tagging,7,46,1,0,,0.000211196,0,negative,1.02E-06,0.001894015,9.79E-07,2.30E-06,0.000202956,4.88E-05,4.27E-06,0.000126467,0.002915699,0.994753021,4.95E-05,7.21E-07,2.43E-07
7104,part-of-speech_tagging7,57,No further unlabeled data is considered in this paper .,Introduction,Contributions,part-of-speech_tagging,7,47,1,0,,0.000398421,0,negative,3.31E-06,0.019977562,5.19E-06,8.84E-06,0.006314165,8.08E-05,7.12E-06,0.000404361,0.002428919,0.970730644,3.61E-05,2.41E-06,6.16E-07
7105,part-of-speech_tagging7,58,The code is released at : https : //github.com/bplank/bilstm-aux,Introduction,Contributions,part-of-speech_tagging,7,48,1,1,code,0.965354538,1,code,2.02E-07,5.05E-06,1.49E-07,0.993263643,0.005800154,0.000384757,9.96E-07,3.24E-06,3.76E-07,0.000540548,8.49E-08,4.66E-09,7.99E-07
7106,part-of-speech_tagging7,59,Taggers,Introduction,,part-of-speech_tagging,7,49,1,0,,0.116825252,0,negative,6.11E-05,0.020436819,0.000141772,0.000535841,0.021248813,0.00567336,0.006180083,0.00301715,0.011909051,0.929838935,0.00037061,0.000192346,0.000394127
7107,part-of-speech_tagging7,60,We want to compare POS taggers under varying conditions .,Introduction,Taggers,part-of-speech_tagging,7,50,1,0,,9.04E-06,0,negative,1.03E-06,8.32E-06,6.77E-07,2.30E-07,1.01E-06,6.53E-06,5.03E-06,1.63E-05,2.15E-05,0.999896937,2.99E-05,1.15E-05,1.09E-06
7108,part-of-speech_tagging7,61,We hence use three different types of taggers : our implementation of a bi -LSTM ; TNT - a second order HMM with suffix trie handling for OOVs .,Introduction,Taggers,part-of-speech_tagging,7,51,1,0,,0.003316814,0,negative,0.000160193,0.000700231,0.000696382,1.25E-06,0.000231177,5.96E-05,8.91E-05,3.72E-05,0.000652378,0.997256289,4.59E-07,0.000111041,4.76E-06
7109,part-of-speech_tagging7,62,We use TNT as it was among the best performing taggers evaluated in .,Introduction,Taggers,part-of-speech_tagging,7,52,1,0,,0.000117395,0,negative,2.68E-06,7.21E-06,4.59E-06,5.39E-08,1.19E-05,2.14E-05,1.49E-05,2.07E-05,7.76E-06,0.999897597,2.83E-08,1.10E-05,2.24E-07
7110,part-of-speech_tagging7,63,"We complement the NN - based and HMM - based tagger with a CRF tagger , using a freely available implementation ) based on crfsuite .",Introduction,Taggers,part-of-speech_tagging,7,53,1,0,,0.002620586,0,negative,0.000123287,0.000562673,0.000835991,3.47E-07,5.72E-05,2.44E-05,3.92E-05,2.40E-05,0.001880206,0.996411792,2.59E-07,3.79E-05,2.65E-06
7111,part-of-speech_tagging7,64,Datasets,Introduction,,part-of-speech_tagging,7,54,1,0,,0.001239528,0,negative,2.29E-06,0.002218975,5.17E-06,1.59E-06,0.000111889,0.000390521,0.000395055,0.000507909,0.004706956,0.991544179,9.80E-05,1.06E-05,6.85E-06
7112,part-of-speech_tagging7,65,"For the multilingual experiments , we use the data from the Universal Dependencies project v 1.2 ( 17 POS ) with the canonical data splits .",Introduction,Datasets,part-of-speech_tagging,7,55,1,0,,0.103015054,0,dataset,3.52E-06,0.06013161,0.000188768,2.66E-05,0.72689224,0.00070571,0.00017912,0.000402966,0.000891599,0.210560929,2.05E-06,1.12E-05,3.60E-06
7113,part-of-speech_tagging7,66,For languages with token segmentation ambiguity we use the provided gold segmentation .,Introduction,Datasets,part-of-speech_tagging,7,56,1,0,,0.04210136,0,negative,1.19E-05,0.069916714,0.00031842,3.91E-05,0.188808172,0.003719691,0.000420161,0.003158549,0.005478374,0.728096025,6.03E-06,1.71E-05,9.78E-06
7114,part-of-speech_tagging7,67,"If there is more than one treebank per language , we use the treebank that has the canonical language name ( e.g. , Finnish instead of Finnish - FTB ) .",Introduction,Datasets,part-of-speech_tagging,7,57,1,0,,0.001758213,0,negative,2.50E-06,0.057468196,8.02E-05,1.35E-06,0.005111983,0.000426806,4.24E-05,0.001104034,0.018248525,0.91750165,8.32E-06,2.95E-06,1.06E-06
7115,part-of-speech_tagging7,68,"We consider all languages that have at least 60 k tokens and are distributed with word forms , resulting in 22 languages .",Introduction,Datasets,part-of-speech_tagging,7,58,1,0,,0.14969789,0,dataset,2.61E-06,0.014515616,2.46E-05,0.000190412,0.785684039,0.001693506,0.000114564,0.000771289,0.000232052,0.196762606,1.24E-06,2.50E-06,4.93E-06
7116,part-of-speech_tagging7,69,We also report accuracies on WSJ ( 45 POS ) using the standard splits .,Introduction,Datasets,part-of-speech_tagging,7,59,1,0,,0.055451911,0,negative,2.79E-05,0.070743416,0.000332557,1.23E-05,0.181932934,0.000416361,0.000846644,0.000330332,0.001124442,0.743921251,1.61E-05,0.000290188,5.65E-06
7117,part-of-speech_tagging7,70,The overview of languages is provided in .,Introduction,Datasets,part-of-speech_tagging,7,60,1,0,,0.003543386,0,negative,7.23E-07,0.001273921,3.75E-06,2.19E-05,0.002170456,0.000238981,1.51E-05,0.000214696,0.000369866,0.995638181,5.02E-05,8.61E-07,1.28E-06
7118,part-of-speech_tagging7,71,Results,,,part-of-speech_tagging,7,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
7119,part-of-speech_tagging7,72,Our results are given in .,Results,Results,part-of-speech_tagging,7,1,1,0,,0.022307417,0,negative,0.008923951,1.37E-05,0.00120907,5.34E-05,1.79E-05,0.000251978,0.00095827,0.000389207,6.26E-05,0.939140022,9.49E-05,0.048037579,0.000847391
7120,part-of-speech_tagging7,73,"First of all , notice that TNT performs remarkably well across the 22 languages , closely followed by CRF .",Results,Results,part-of-speech_tagging,7,2,1,0,,0.975512388,1,results,0.004216793,9.38E-07,6.78E-05,4.06E-06,9.72E-07,2.02E-05,0.006432774,7.41E-05,4.34E-07,0.012795164,1.86E-05,0.975865365,0.000502816
7121,part-of-speech_tagging7,74,"The bi - LSTM tagger ( w ) without lower - level bi - LSTM for subtokens falls short , outperforms the traditional taggers only on 3 languages .",Results,Results,part-of-speech_tagging,7,3,1,0,,0.971114534,1,results,0.003765025,7.96E-07,0.000233491,9.83E-07,5.80E-07,8.57E-06,0.003543576,2.74E-05,2.63E-07,0.015973585,1.40E-05,0.976179218,0.000252524
7122,part-of-speech_tagging7,75,The bi-LSTM 2 https://sites.google.com/site/rmyeid/,Results,Results,part-of-speech_tagging,7,4,1,0,,0.81334299,1,baselines,0.005649643,0.000142212,0.775492541,1.51E-05,7.16E-06,0.000426862,0.001813399,0.001493937,0.000871472,0.204805115,0.000123092,0.00827354,0.000885876
7123,part-of-speech_tagging7,76,projects / polyglot,Results,Results,part-of-speech_tagging,7,5,1,0,,0.347603962,0,results,0.014484968,3.07E-05,0.00716165,0.000177136,3.09E-05,0.00062723,0.022900073,0.000864345,8.09E-05,0.28443582,0.001093068,0.644411801,0.023701445
7124,part-of-speech_tagging7,77,3,Results,Results,part-of-speech_tagging,7,6,1,0,,0.000879416,0,negative,0.00246613,5.40E-06,0.000165062,5.29E-06,2.47E-06,6.06E-05,0.000121893,0.000282386,3.81E-05,0.989955891,8.47E-06,0.00679003,9.84E-05
7125,part-of-speech_tagging7,78,"They found TreeTagger was closely followed by Hun - Pos , a re-implementation of TnT , and Stanford and ClearNLP were lower ranked .",Results,Results,part-of-speech_tagging,7,7,1,0,,0.086029265,0,negative,0.019039833,6.95E-06,0.001766666,2.36E-05,1.73E-05,0.000166724,0.004521457,0.000327259,1.07E-05,0.535782449,4.17E-05,0.437068791,0.001226617
7126,part-of-speech_tagging7,79,"In an initial investigation , we compared Tnt , HunPos and TreeTagger and found Tnt to be consistently better than Treetagger , Hunpos followed closely but crashed on some languages ( e.g. , Arabic ) .",Results,Results,part-of-speech_tagging,7,8,1,1,results,0.135498077,0,results,0.007315206,3.68E-06,0.00070093,3.72E-06,5.67E-06,5.11E-05,0.003274722,0.000129789,2.47E-06,0.325487982,2.43E-05,0.662735556,0.000264887
7127,part-of-speech_tagging7,80,"The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre-trained embeddings .",Results,Results,part-of-speech_tagging,7,9,1,1,results,0.979969652,1,results,0.008906401,1.08E-06,0.000116674,4.38E-06,1.05E-06,1.84E-05,0.007057296,7.57E-05,6.15E-07,0.010659067,8.09E-06,0.972328919,0.000822304
7128,part-of-speech_tagging7,81,This model ( w + c ) reaches the biggest improvement ( more than + 2 % accuracy ) on Hebrew and Slovene .,Results,Results,part-of-speech_tagging,7,10,1,1,results,0.916998212,1,results,0.009601591,1.33E-06,0.000898986,2.29E-06,1.15E-06,1.00E-05,0.00412366,2.76E-05,9.49E-07,0.016780496,1.12E-05,0.967859463,0.000681271
7129,part-of-speech_tagging7,82,Initializing the word embeddings ( + POLYGLOT ) with off - the - shelf languagespecific embeddings further improves accuracy .,Results,Results,part-of-speech_tagging,7,11,1,1,results,0.974608494,1,results,0.278892862,2.03E-05,0.00050055,1.95E-05,8.32E-06,8.90E-05,0.004971721,0.000520707,1.67E-05,0.061926915,1.18E-05,0.65180406,0.001217666
7130,part-of-speech_tagging7,83,The only system we are aware of that evaluates on UD is ( last column ) .,Results,Results,part-of-speech_tagging,7,12,1,0,,0.006630933,0,negative,0.007255488,3.09E-05,0.015498546,2.25E-05,2.46E-05,0.000150269,0.002547575,0.000314015,4.83E-05,0.800300267,0.000119175,0.172472321,0.001215979
7131,part-of-speech_tagging7,84,"However , note that these results are not strictly comparable as they use the earlier UD v 1.1 version .",Results,Results,part-of-speech_tagging,7,13,1,0,,0.001089356,0,negative,0.000928189,3.18E-06,0.000223269,3.63E-06,3.64E-06,4.77E-05,0.000342809,0.000175594,6.46E-06,0.973626016,7.21E-06,0.024527766,0.000104502
7132,part-of-speech_tagging7,85,The over all best system is the multi-task bi - LSTM FREQBIN ( it uses w + c and POLYGLOT initialization for w ) .,Results,Results,part-of-speech_tagging,7,14,1,1,results,0.932080683,1,negative,0.019702273,0.000134874,0.128163712,0.000218281,9.27E-05,0.001800021,0.048047401,0.003388093,0.000202141,0.482676572,0.000175422,0.297792754,0.01760571
7133,part-of-speech_tagging7,86,"While on macro average it is on par with bi - LSTM w + c , it obtains the best results on 12 / 22 languages , and it is successful in predicting POS for OOV tokens ( cf. OOV ACC columns ) , especially for languages like Arabic , Farsi , Hebrew , Finnish .",Results,Results,part-of-speech_tagging,7,15,1,0,,0.940469987,1,results,0.002076013,4.00E-07,7.27E-05,7.84E-07,5.39E-07,6.18E-06,0.003595596,2.32E-05,1.64E-07,0.019896388,4.54E-06,0.973977125,0.00034646
7134,part-of-speech_tagging7,87,We examined simple RNNs and confirm the finding of that they performed worse than their LSTM counterparts .,Results,Results,part-of-speech_tagging,7,16,1,0,,0.279269487,0,negative,0.010918307,6.85E-06,0.000260412,3.64E-06,8.95E-06,4.67E-05,0.001343826,0.000200578,5.80E-06,0.712641203,7.16E-06,0.274343529,0.000213016
7135,part-of-speech_tagging7,88,"Finally , the bi - LSTM tagger is competitive on WSJ , cf .",Results,Results,part-of-speech_tagging,7,17,1,0,,0.640702426,1,results,0.005779309,4.97E-07,8.67E-05,4.98E-07,5.10E-07,4.09E-06,0.002543521,1.73E-05,1.64E-07,0.026675371,2.13E-06,0.964745671,0.000144239
7136,part-of-speech_tagging7,89,Table 3 .,Results,Results,part-of-speech_tagging,7,18,1,0,,0.00287921,0,negative,0.00277212,2.12E-06,0.000220349,1.42E-06,1.90E-06,2.34E-05,0.0002006,0.000135078,9.05E-06,0.96248321,1.51E-06,0.034058464,9.08E-05
7137,part-of-speech_tagging7,90,Rare words,Results,,part-of-speech_tagging,7,19,1,0,,0.007115144,0,negative,0.009854808,7.61E-05,0.000913647,0.000356175,5.89E-05,0.001046312,0.002848653,0.003936932,0.00051162,0.934201518,0.000139775,0.03653048,0.009525128
7138,part-of-speech_tagging7,91,"In order to evaluate the effect of modeling sub - token information , we examine accuracy rates at different frequency rates .",Results,Rare words,part-of-speech_tagging,7,20,1,0,,0.000346639,0,negative,0.003157469,0.000118657,0.000204786,1.53E-07,2.65E-05,4.29E-05,0.000141604,2.41E-05,6.53E-05,0.99598912,1.74E-06,0.000226686,9.84E-07
7139,part-of-speech_tagging7,92,"shows absolute improvements in accuracy of bi - LSTM w + cover mean log frequency , for different language families .",Results,Rare words,part-of-speech_tagging,7,21,1,0,,0.000273341,0,negative,0.008934165,6.10E-06,0.00096535,2.23E-07,8.11E-06,7.94E-05,0.001447974,1.33E-05,8.09E-06,0.985469545,1.72E-05,0.003037609,1.29E-05
7140,part-of-speech_tagging7,93,"We see that especially for Slavic and non-Indoeuropean languages , having high morphologic complexity , most of the improvement is obtained in the Zipfian tail .",Results,Rare words,part-of-speech_tagging,7,22,1,0,,0.517652354,1,ablation-analysis,0.629045371,5.09E-05,0.000399481,1.14E-05,7.29E-05,0.000359617,0.015235134,0.000127055,2.51E-05,0.291839703,4.60E-05,0.062475834,0.000311518
7141,part-of-speech_tagging7,94,Rare tokens benefit from the sub-token representations .,Results,Rare words,part-of-speech_tagging,7,23,1,0,,0.487935538,0,ablation-analysis,0.632494852,6.45E-05,0.001373272,4.91E-06,8.41E-05,0.000152265,0.001503876,4.14E-05,0.000134948,0.35733864,7.21E-06,0.006736129,6.39E-05
7142,part-of-speech_tagging7,95,Data set size Prior work mostly used large data sets when applying neural network based approaches .,Results,Rare words,part-of-speech_tagging,7,24,1,0,,0.000104887,0,negative,8.82E-05,9.17E-06,3.95E-05,3.55E-07,9.11E-06,0.000117261,0.00011048,2.99E-05,6.54E-06,0.999503553,4.71E-05,3.24E-05,6.37E-06
7143,part-of-speech_tagging7,96,We evaluate how brittle such models are with respect to their more traditional counterparts by training bi - LSTM ( w + c without Polyglot embeddings ) for increas -,Results,Rare words,part-of-speech_tagging,7,25,1,0,,0.000211446,0,negative,0.006664277,0.000416534,0.001569045,7.51E-07,0.000121669,0.000149831,0.000711524,4.94E-05,0.000130944,0.989526887,5.32E-06,0.000645422,8.39E-06
7144,part-of-speech_tagging7,97,WSJ,Results,,part-of-speech_tagging,7,26,1,0,,0.003077095,0,negative,0.003148049,1.02E-05,0.000212671,1.44E-05,6.68E-06,9.76E-05,0.000502742,0.001069366,5.55E-05,0.968849417,6.71E-06,0.025181572,0.000845136
7145,part-of-speech_tagging7,98,Accuracy,Results,,part-of-speech_tagging,7,27,1,0,,0.80392012,1,negative,0.005574279,3.21E-05,0.000869345,8.37E-05,3.58E-05,0.000307131,0.009708703,0.001636368,7.18E-05,0.638605888,0.000119688,0.32675274,0.016202433
7146,part-of-speech_tagging7,99,"Convnet ( Santos and Zadrozny , 2014 ) 97.32 Convnet reimplementation 96.80 Bi-RNN 95.93 Bi-LSTM 97.36",Results,Accuracy,part-of-speech_tagging,7,28,1,0,,3.88E-05,0,negative,0.00298295,3.93E-07,8.66E-05,4.44E-06,6.26E-06,0.000207694,0.000229618,3.11E-05,2.80E-06,0.992260288,6.98E-07,0.001091804,0.003095306
7147,part-of-speech_tagging7,100,Our bi-LSTM w+ c 97.22 ing amounts of training instances ( number of sentences ) .,Results,Accuracy,part-of-speech_tagging,7,29,1,0,,0.000179118,0,negative,0.014483861,6.06E-07,4.59E-05,2.04E-07,2.62E-06,2.31E-05,0.000146974,1.33E-05,1.83E-06,0.979272494,3.20E-07,0.005462369,0.000546531
7148,part-of-speech_tagging7,101,The learning curves in show similar trends across language families .,Results,Accuracy,part-of-speech_tagging,7,30,1,0,,0.000629491,0,negative,0.038295341,6.18E-07,2.60E-05,1.25E-07,8.24E-07,6.87E-06,7.85E-05,5.59E-06,1.96E-06,0.953122837,4.99E-07,0.008362479,9.84E-05
7149,part-of-speech_tagging7,102,"4 TNT is better with little data , bi - LSTM is better with more data , and bi - LSTM always wins over CRF .",Results,Accuracy,part-of-speech_tagging,7,31,1,0,,0.238451734,0,negative,0.225211909,8.27E-07,3.77E-05,3.08E-06,3.46E-06,9.74E-05,0.002822407,5.01E-05,8.02E-07,0.662278742,1.07E-06,0.104816223,0.004676191
7150,part-of-speech_tagging7,103,The bi-LSTM model performs already surprisingly well after only 500 training sentences .,Results,Accuracy,part-of-speech_tagging,7,32,1,0,,0.394152925,0,negative,0.168019221,6.98E-07,4.07E-05,1.00E-06,1.46E-06,4.65E-05,0.004171485,2.96E-05,8.35E-07,0.494375848,1.30E-06,0.327763862,0.005547517
7151,part-of-speech_tagging7,104,For non-Indoeuropean languages it is on par and above the other taggers with even less data ( 100 sentences ) .,Results,Accuracy,part-of-speech_tagging,7,33,1,0,,0.010349891,0,negative,0.030993153,6.81E-07,8.14E-05,1.86E-06,1.04E-05,3.64E-05,0.00154634,1.33E-05,9.98E-07,0.823661854,1.08E-06,0.135515164,0.008137333
7152,part-of-speech_tagging7,105,"This shows that the bi - LSTMs often needs more data than the generative markovian model , but this is definitely less than what we expected .",Results,Accuracy,part-of-speech_tagging,7,34,1,0,,0.000129939,0,negative,0.006948717,5.48E-08,1.79E-06,1.11E-07,1.09E-06,6.22E-06,1.20E-05,1.85E-06,1.35E-07,0.992583692,4.35E-08,0.000396469,4.78E-05
7153,part-of-speech_tagging7,106,Label Noise,Results,,part-of-speech_tagging,7,35,1,0,,0.206311701,0,negative,0.015545207,7.68E-06,0.00060513,2.51E-05,1.32E-05,0.000100178,0.001562922,0.000663446,3.48E-05,0.838463227,4.08E-06,0.139088355,0.003886687
7154,part-of-speech_tagging7,107,"We investigated the susceptibility of the models to noise , by artificially corrupting training labels .",Results,Label Noise,part-of-speech_tagging,7,36,1,0,,4.15E-05,0,negative,0.000519985,7.28E-08,4.90E-06,5.08E-08,4.95E-08,1.23E-06,8.46E-07,1.17E-05,3.93E-07,0.998268443,6.31E-09,0.001155984,3.63E-05
7155,part-of-speech_tagging7,108,"Our initial results show that at low noise rates , bi - LSTMs and TNT are affected similarly , their accuracies drop to a similar degree .",Results,Label Noise,part-of-speech_tagging,7,37,1,0,,0.019833014,0,negative,0.046813031,3.65E-08,2.57E-05,7.26E-08,5.29E-08,6.98E-07,1.22E-05,2.55E-06,1.44E-07,0.739119036,2.54E-08,0.21388311,0.000143341
7156,part-of-speech_tagging7,109,"Only at higher noise levels ( more than 30 % corrupted labels ) , bi - LSTMs are less robust , showing higher drops in accuracy compared to TNT .",Results,Label Noise,part-of-speech_tagging,7,38,1,0,,0.157059082,0,results,0.025681418,1.93E-08,1.82E-05,1.18E-07,2.56E-08,7.60E-07,5.11E-05,3.00E-06,3.60E-08,0.185824264,3.92E-08,0.787850268,0.000570716
7157,part-of-speech_tagging7,110,This is the case for all investigated language families .,Results,Label Noise,part-of-speech_tagging,7,39,1,0,,0.005287556,0,negative,0.00067939,5.32E-08,6.67E-06,1.30E-07,3.17E-08,2.65E-06,7.75E-06,2.61E-05,3.47E-07,0.984669612,5.43E-08,0.014179187,0.00042801
7158,part-of-speech_tagging7,111,Related Work,,,part-of-speech_tagging,7,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
7159,part-of-speech_tagging7,120,Conclusions,,,part-of-speech_tagging,7,0,1,0,,0.000388901,0,negative,8.30E-05,6.93E-05,4.02E-06,8.07E-07,8.64E-07,3.36E-05,1.76E-05,0.00031599,6.31E-05,0.998796926,0.000458235,0.000155019,1.50E-06
7160,machine-translation8,1,title,,,machine-translation,8,1,1,0,,0.000701755,0,negative,8.67E-05,0.000310204,3.41E-06,0.00010693,6.41E-06,0.000664696,5.35E-05,0.005762623,0.000365664,0.991721448,0.000883565,2.01E-05,1.47E-05
7161,machine-translation8,2,Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,,,machine-translation,8,2,1,1,research-problem,0.971535422,1,research-problem,1.63E-05,7.96E-05,1.67E-05,8.57E-06,1.47E-06,2.23E-05,0.000833008,0.000142777,1.47E-05,0.024653338,0.973474739,0.000676945,5.95E-05
7162,machine-translation8,3,abstract,,,machine-translation,8,3,1,0,,0.004275135,0,negative,0.000172564,0.000566819,6.30E-06,0.000186221,1.07E-05,0.001072161,0.000114264,0.009973721,0.000790861,0.985126726,0.001897692,4.59E-05,3.61E-05
7163,machine-translation8,4,Neural machine translation is a recently proposed approach to machine translation .,,,machine-translation,8,4,1,0,,0.714401953,1,research-problem,3.40E-05,8.05E-05,0.000100038,3.72E-06,2.01E-06,1.90E-05,0.000661424,0.000104007,8.48E-06,0.062900603,0.935301918,0.000740438,4.39E-05
7164,machine-translation8,5,"Unlike the traditional statistical machine translation , the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance .",,,machine-translation,8,5,1,0,,0.677504492,1,research-problem,5.07E-05,0.000575873,7.73E-05,1.30E-05,5.72E-06,4.97E-05,0.000342865,0.000531086,6.56E-05,0.24617337,0.751679774,0.000390106,4.49E-05
7165,machine-translation8,6,The models proposed recently for neural machine translation often belong to a family of encoder - decoders and encode a source sentence into a fixed - length vector from which a decoder generates a translation .,,,machine-translation,8,6,1,0,,0.594359491,1,research-problem,1.51E-05,9.40E-05,1.18E-05,6.59E-06,1.30E-06,4.74E-05,0.000214557,0.000362414,2.22E-05,0.246337039,0.752730287,0.000135033,2.22E-05
7166,machine-translation8,7,"In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .",,,machine-translation,8,7,1,1,research-problem,0.080364111,0,negative,0.001406232,0.049909644,0.000771237,1.28E-05,9.31E-05,0.000133746,0.000136335,0.002407953,0.005746941,0.925191799,0.010026026,0.004134795,2.94E-05
7167,machine-translation8,8,"With this new approach , we achieve a translation performance comparable to the existing state - of - the - art phrase - based system on the task of English - to - French translation .",,,machine-translation,8,8,1,0,,0.700232255,1,results,0.002634251,0.001845546,0.000291768,9.13E-06,4.00E-05,8.19E-05,0.002478579,0.001221847,9.54E-05,0.483779824,0.01377486,0.493585058,0.000161954
7168,machine-translation8,9,"Furthermore , qualitative analysis reveals that the ( soft - ) alignments found by the model agree well with our intuition .",,,machine-translation,8,9,1,0,,0.397121678,0,negative,0.003708749,0.000297012,8.59E-06,1.10E-05,1.15E-05,0.000137141,0.000402033,0.002406695,4.67E-05,0.956655233,0.000807247,0.035478834,2.93E-05
7169,machine-translation8,10,INTRODUCTION,,,machine-translation,8,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
7170,machine-translation8,11,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .",INTRODUCTION,INTRODUCTION,machine-translation,8,1,1,1,research-problem,0.884257192,1,research-problem,2.06E-06,0.00057392,3.84E-06,7.14E-07,5.73E-06,2.98E-06,2.49E-05,4.96E-06,0.000437018,0.027501934,0.971435648,4.52E-06,1.76E-06
7171,machine-translation8,12,"Unlike the traditional phrase - based translation system ( see , e.g. , which consists of many small sub-components thatare tuned separately , neural machine translation attempts to build and train a single , large neural network that reads a sentence and outputs a correct translation .",INTRODUCTION,INTRODUCTION,machine-translation,8,2,1,0,,0.573166101,1,research-problem,1.44E-06,0.000633678,1.51E-06,1.11E-05,3.42E-05,1.01E-05,1.37E-05,1.12E-05,0.000218692,0.059944709,0.93911596,1.41E-06,2.36E-06
7172,machine-translation8,13,"Most of the proposed neural machine translation models belong to a family of encoderdecoders , with an encoder and a decoder for each language , or involve a language - specific encoder applied to each sentence whose outputs are then compared ) .",INTRODUCTION,INTRODUCTION,machine-translation,8,3,1,0,,0.121961086,0,research-problem,1.80E-06,0.001441056,2.41E-06,5.96E-06,1.88E-05,2.81E-05,2.17E-05,4.56E-05,0.001398917,0.137465387,0.859565623,2.08E-06,2.60E-06
7173,machine-translation8,14,An encoder neural network reads and encodes a source sentence into a fixed - length vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,4,1,0,,0.954947869,1,model,5.80E-06,0.024817976,8.34E-05,5.25E-07,3.53E-05,9.33E-06,6.12E-06,1.88E-05,0.962811957,0.009733043,0.002475445,1.36E-06,9.73E-07
7174,machine-translation8,15,A decoder then outputs a translation from the encoded vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,5,1,0,,0.81157731,1,model,1.72E-05,0.026408498,6.37E-05,5.00E-07,5.90E-05,1.44E-05,6.20E-06,2.59E-05,0.893001076,0.077285821,0.003114348,2.71E-06,5.86E-07
7175,machine-translation8,16,"The whole encoder - decoder system , which consists of the encoder and the decoder for a language pair , is jointly trained to maximize the probability of a correct translation given a source sentence .",INTRODUCTION,INTRODUCTION,machine-translation,8,6,1,0,,0.935040691,1,model,4.06E-06,0.037036945,2.44E-05,2.05E-07,2.97E-05,5.25E-06,2.79E-06,1.33E-05,0.957958644,0.004351816,0.000571608,8.46E-07,4.64E-07
7176,machine-translation8,17,A potential issue with this encoder - decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed - length vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,7,1,0,,0.071739127,0,negative,1.71E-05,0.008998339,5.74E-06,4.07E-05,0.00013938,0.000123249,2.91E-05,0.000168948,0.009886082,0.6722446,0.308333166,7.68E-06,5.94E-06
7177,machine-translation8,18,"This may make it difficult for the neural network to cope with long sentences , especially those thatare longer than the sentences in the training corpus .",INTRODUCTION,INTRODUCTION,machine-translation,8,8,1,0,,0.033089037,0,negative,4.55E-05,0.007834485,5.40E-06,1.00E-05,0.000351648,7.43E-05,2.82E-05,9.49E-05,0.007408302,0.957423117,0.026702206,1.96E-05,2.22E-06
7178,machine-translation8,19,showed that indeed the performance of a basic encoder - decoder deteriorates rapidly as the length of an input sentence increases .,INTRODUCTION,INTRODUCTION,machine-translation,8,9,1,0,,0.031769185,0,negative,1.84E-05,0.002526533,7.37E-06,1.83E-05,0.000171607,0.000103283,5.95E-05,7.44E-05,0.00427097,0.792208417,0.200522313,1.39E-05,4.97E-06
7179,machine-translation8,20,"In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .",INTRODUCTION,INTRODUCTION,machine-translation,8,10,1,1,model,0.964832566,1,model,1.57E-05,0.076256075,7.14E-05,6.69E-07,9.94E-05,9.72E-06,7.94E-06,1.61E-05,0.917301411,0.005424536,0.000793065,3.09E-06,9.43E-07
7180,machine-translation8,21,"Each time the proposed model generates a word in a translation , it ( soft - ) searches for a set of positions in a source sentence where the most relevant information is concentrated .",INTRODUCTION,INTRODUCTION,machine-translation,8,11,1,1,model,0.921434891,1,model,6.24E-06,0.053338173,3.92E-05,1.44E-07,4.56E-05,4.26E-06,3.25E-06,9.41E-06,0.937527072,0.008442261,0.000582835,1.32E-06,2.92E-07
7181,machine-translation8,22,The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .,INTRODUCTION,INTRODUCTION,machine-translation,8,12,1,1,model,0.843324941,1,model,2.00E-06,0.018099867,1.64E-05,5.75E-08,1.12E-05,4.99E-06,2.66E-06,1.26E-05,0.971569904,0.009880979,0.00039848,5.93E-07,2.13E-07
7182,machine-translation8,23,The most important distinguishing feature of this approach from the basic encoder - decoder is that it does not attempt to encode a whole input sentence into a single fixed - length vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,13,1,0,,0.100963068,0,negative,0.000189422,0.302576617,0.000136697,4.72E-05,0.002025201,0.000146594,9.01E-05,0.000253839,0.203003942,0.453284218,0.038173624,6.24E-05,1.02E-05
7183,machine-translation8,24,"Instead , it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation .",INTRODUCTION,INTRODUCTION,machine-translation,8,14,1,0,,0.957142606,1,model,2.50E-05,0.138222813,0.000182419,1.58E-06,0.000358682,2.15E-05,1.52E-05,3.19E-05,0.839214495,0.021316354,0.000604433,4.28E-06,1.34E-06
7184,machine-translation8,25,"This frees a neural translation model from having to squash all the information of a source sentence , regardless of its length , into a fixed - length vector .",INTRODUCTION,INTRODUCTION,machine-translation,8,15,1,0,,0.410027705,0,negative,0.000178616,0.158282419,9.47E-05,2.30E-05,0.001695217,0.000103577,4.13E-05,0.00017021,0.185153369,0.646216557,0.008004971,3.15E-05,4.54E-06
7185,machine-translation8,26,We show this allows a model to cope better with long sentences .,INTRODUCTION,INTRODUCTION,machine-translation,8,16,1,0,,0.541833421,1,negative,0.00045574,0.132231965,5.75E-05,1.16E-05,0.001046718,0.000108275,9.79E-05,0.000256326,0.314252831,0.54799036,0.003401532,8.43E-05,4.99E-06
7186,machine-translation8,27,"In this paper , we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder - decoder approach .",INTRODUCTION,INTRODUCTION,machine-translation,8,17,1,0,,0.917662705,1,approach,0.000245886,0.686943999,0.000151305,1.30E-05,0.001628655,5.88E-05,0.00026536,0.000181329,0.223916823,0.073833787,0.012583847,0.00016426,1.29E-05
7187,machine-translation8,28,"The improvement is more apparent with longer sentences , but can be observed with sentences of any length .",INTRODUCTION,INTRODUCTION,machine-translation,8,18,1,0,,0.172409677,0,negative,0.003300472,0.018281432,4.29E-05,0.000301447,0.003889799,0.000657769,0.002843784,0.001195631,0.003537304,0.956674623,0.007426707,0.001769032,7.91E-05
7188,machine-translation8,29,"On the task of English - to - French translation , the proposed approach achieves , with a single model , a translation performance comparable , or close , to the conventional phrase - based system .",INTRODUCTION,INTRODUCTION,machine-translation,8,19,1,0,,0.103202073,0,negative,0.000657986,0.10983064,0.000132775,3.26E-05,0.002821788,0.000227529,0.012094111,0.000678541,0.009982474,0.75714812,0.095638894,0.010570862,0.000183639
7189,machine-translation8,30,"Furthermore , qualitative analysis reveals that the proposed model finds a linguistically plausible ( soft - ) alignment between a source sentence and the corresponding target sentence .",INTRODUCTION,INTRODUCTION,machine-translation,8,20,1,0,,0.067816,0,negative,0.003631729,0.151020441,7.04E-05,4.79E-05,0.003444492,0.000238425,0.000865837,0.000529071,0.065279991,0.767842321,0.006333816,0.000672078,2.36E-05
7190,machine-translation8,31,BACKGROUND : NEURAL MACHINE TRANSLATION,INTRODUCTION,,machine-translation,8,21,1,0,,0.08063821,0,research-problem,1.18E-05,0.004006043,1.84E-05,1.25E-05,9.17E-05,8.87E-05,0.000762171,9.98E-05,0.003421387,0.251992623,0.739393586,6.53E-05,3.60E-05
7191,machine-translation8,32,"From a probabilistic perspective , translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x , i.e. , arg max y p ( y | x ) .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,22,1,0,,0.000428475,0,negative,1.69E-06,0.000355652,3.38E-06,3.37E-06,7.73E-06,1.62E-05,7.42E-06,3.90E-05,0.000632333,0.979382963,0.019542043,6.82E-06,1.34E-06
7192,machine-translation8,33,"In neural machine translation , we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,23,1,0,,0.00507601,0,negative,2.64E-05,0.004776502,5.69E-05,2.02E-05,6.98E-05,5.26E-05,6.79E-05,0.000114511,0.001707986,0.796021592,0.197006051,6.68E-05,1.28E-05
7193,machine-translation8,34,"Once the conditional distribution is learned by a translation model , given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,24,1,0,,0.000152331,0,negative,1.07E-05,0.001031721,2.38E-05,5.46E-08,7.96E-06,1.68E-06,9.21E-07,6.71E-06,0.006575789,0.992284328,4.33E-05,1.30E-05,6.02E-08
7194,machine-translation8,35,"Recently , a number of papers have proposed the use of neural networks to directly learn this conditional distribution ( see , e.g. , .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,25,1,0,,4.23E-05,0,negative,1.64E-06,3.21E-05,9.76E-07,6.37E-07,1.22E-06,1.12E-05,3.53E-06,1.73E-05,0.000125202,0.996527939,0.003275525,2.48E-06,2.83E-07
7195,machine-translation8,36,"This neural machine translation approach typically consists of two components , the first of which encodes a source sentence x and the second decodes to a target sentence y .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,26,1,0,,0.000155467,0,negative,4.68E-06,0.000283455,1.05E-05,5.00E-06,1.26E-05,3.81E-05,3.12E-05,3.66E-05,0.000505125,0.938816941,0.060235974,1.50E-05,4.81E-06
7196,machine-translation8,37,"For instance , two recurrent neural networks ( RNN ) were used by and to encode a variable - length source sentence into a fixed - length vector and to decode the vector into a variable - length target sentence .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,27,1,0,,7.28E-05,0,negative,2.02E-06,8.02E-05,1.07E-05,4.75E-07,4.52E-06,2.21E-05,6.16E-06,2.50E-05,0.001583884,0.997987272,0.000274558,2.62E-06,4.71E-07
7197,machine-translation8,38,"Despite being a quite new approach , neural machine translation has already shown promising results .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,28,1,0,,0.001839649,0,negative,7.16E-06,4.47E-05,2.23E-06,3.31E-06,8.78E-06,2.19E-05,3.41E-05,1.63E-05,3.99E-05,0.969615394,0.030175465,2.80E-05,2.74E-06
7198,machine-translation8,39,reported that the neural machine translation based on RNNs with long shortterm memory ( LSTM ) units achieves close to the state - of - the - art performance of the conventional phrase - based machine translation system on an English - to - French translation task .,INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,29,1,0,,0.003703053,0,negative,1.07E-05,7.22E-05,8.34E-06,2.16E-06,1.48E-05,1.84E-05,7.66E-05,1.86E-05,4.13E-05,0.975930267,0.02369455,0.000107852,4.24E-06
7199,machine-translation8,40,"1 Adding neural components to existing translation systems , for instance , to score the phrase pairs in the phrase table or to re-rank candidate translations , has allowed to surpass the previous state - of - the - art performance level .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,30,1,0,,0.002056809,0,negative,6.53E-06,6.32E-05,1.51E-06,5.22E-06,5.18E-06,4.75E-05,3.39E-05,5.57E-05,0.000119644,0.985613005,0.014018759,2.64E-05,3.45E-06
7200,machine-translation8,41,RNN ENCODER - DECODER,INTRODUCTION,,machine-translation,8,31,1,0,,0.324991483,0,model,3.46E-05,0.031179799,0.000257845,3.67E-05,0.00090642,0.000378012,0.000246977,0.000250156,0.82367099,0.141251304,0.001738562,1.51E-05,3.36E-05
7201,machine-translation8,42,"Here , we describe briefly the underlying framework , called RNN Encoder - Decoder , proposed by and upon which we build a novel architecture that learns to align and translate simultaneously .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,32,1,0,,0.00080559,0,negative,1.28E-05,0.000386641,1.06E-05,7.56E-07,5.51E-06,1.13E-05,2.42E-06,7.43E-05,0.005851937,0.993619671,5.84E-06,1.70E-05,1.12E-06
7202,machine-translation8,43,"In the Encoder - Decoder framework , an encoder reads the input sentence , a sequence of vectors x = ( x 1 , , x Tx ) , into a vector c. 2 The most common approach is to use an RNN such that",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,33,1,0,,0.000114891,0,negative,1.91E-06,5.45E-05,1.69E-05,3.00E-08,3.37E-07,2.27E-06,8.78E-07,1.92E-05,0.001940576,0.997944722,9.04E-06,9.33E-06,2.69E-07
7203,machine-translation8,44,"( 1 ) and c = q ( {h 1 , , h Tx } ) , where ht ?",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,34,1,0,,3.77E-05,0,negative,3.99E-06,2.28E-06,6.68E-07,1.50E-08,1.53E-07,1.48E-06,5.68E-07,9.87E-06,1.55E-05,0.999944294,1.75E-07,2.10E-05,3.31E-08
7204,machine-translation8,45,"Rn is a hidden state at time t , and c is a vector generated from the sequence of the hidden states .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,35,1,0,,2.46E-05,0,negative,8.63E-07,6.44E-06,7.56E-07,2.06E-08,1.66E-07,1.76E-06,3.32E-07,2.26E-05,0.00013295,0.999830945,3.18E-07,2.80E-06,4.93E-08
7205,machine-translation8,46,f and q are some nonlinear functions .,INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,36,1,0,,3.77E-05,0,negative,3.58E-06,8.97E-06,6.00E-07,1.33E-07,4.78E-07,6.99E-06,8.54E-07,0.000111783,0.000134475,0.999724562,3.07E-07,7.12E-06,1.44E-07
7206,machine-translation8,47,"used an LSTM as f and q ( {h 1 , , h T }) = h T , for instance .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,37,1,0,,1.46E-05,0,negative,1.24E-06,5.02E-07,6.32E-07,1.40E-07,5.39E-07,8.49E-06,7.47E-07,1.25E-05,5.63E-06,0.999965021,1.43E-07,4.36E-06,9.43E-08
7207,machine-translation8,48,"The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words {y 1 , , y t ?1 }.",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,38,1,0,,6.08E-05,0,negative,1.03E-06,7.75E-06,1.36E-06,3.71E-08,1.73E-07,2.63E-06,4.54E-07,2.46E-05,0.000396788,0.99956229,1.01E-06,1.76E-06,1.61E-07
7208,machine-translation8,49,"In other words , the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals :",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,39,1,0,,1.68E-05,0,negative,4.76E-06,5.72E-06,3.06E-06,8.19E-09,1.75E-07,5.72E-07,1.83E-07,4.20E-06,0.000154282,0.999820545,6.68E-08,6.41E-06,2.02E-08
7209,machine-translation8,50,"where y = y 1 , , y Ty .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,40,1,0,,5.15E-06,0,negative,6.58E-07,7.05E-07,1.69E-07,1.32E-08,7.51E-08,1.05E-06,1.77E-07,6.21E-06,1.28E-05,0.999975459,4.78E-08,2.63E-06,1.74E-08
7210,machine-translation8,51,"With an RNN , each conditional probability is modeled as",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,41,1,0,,3.08E-05,0,negative,2.51E-06,9.44E-06,2.90E-06,1.28E-08,1.93E-07,1.85E-06,4.71E-07,1.84E-05,0.000827705,0.99913192,2.26E-07,4.33E-06,8.30E-08
7211,machine-translation8,52,"where g is a nonlinear , potentially multi-layered , function that outputs the probability of y t , and st is the hidden state of the RNN .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,42,1,0,,9.51E-06,0,negative,1.80E-06,4.14E-06,6.36E-07,4.79E-08,2.69E-07,2.00E-06,3.44E-07,2.05E-05,7.87E-05,0.999888175,1.23E-07,3.21E-06,7.61E-08
7212,machine-translation8,53,It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used .,INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,43,1,0,,5.99E-05,0,negative,2.86E-06,1.68E-06,8.64E-07,3.51E-08,9.86E-08,5.40E-06,1.09E-06,2.67E-05,7.12E-05,0.999883567,1.82E-07,6.25E-06,1.07E-07
7213,machine-translation8,54,LEARNING TO ALIGN AND TRANSLATE,INTRODUCTION,,machine-translation,8,44,1,0,,0.082373908,0,negative,2.19E-05,0.015467804,1.92E-05,0.000109688,0.001087878,0.000723954,0.00374163,0.001239383,0.005128767,0.941853074,0.030278788,0.000179288,0.000148649
7214,machine-translation8,55,"In this section , we propose a novel architecture for neural machine translation .",INTRODUCTION,LEARNING TO ALIGN AND TRANSLATE,machine-translation,8,45,1,0,,0.010598358,0,negative,0.000594882,0.011206531,0.005650818,3.94E-06,0.000543959,0.000176529,0.001526995,3.32E-05,0.169458516,0.809798866,0.000563465,0.000328082,0.000114259
7215,machine-translation8,56,The new architecture consists of a bidirectional RNN as an encoder ( Sec. 3.2 ) and a decoder that emulates searching through a source sentence during decoding a translation ( Sec. 3.1 ) .,INTRODUCTION,LEARNING TO ALIGN AND TRANSLATE,machine-translation,8,46,1,0,,0.012276131,0,negative,0.000157083,0.003762403,0.001481076,1.53E-06,0.000230985,6.87E-05,0.000169999,2.40E-05,0.45170217,0.542340693,2.06E-05,1.82E-05,2.26E-05
7216,machine-translation8,57,DECODER : GENERAL DESCRIPTION,INTRODUCTION,,machine-translation,8,47,1,0,,0.562510655,1,model,2.20E-05,0.07224067,0.000235436,5.25E-06,0.000286621,0.000232442,0.000623343,0.000261697,0.798570149,0.12605297,0.001394056,3.53E-05,4.01E-05
7217,machine-translation8,58,x 1 x 2 x 3 x T :,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,48,1,0,,1.03E-06,0,negative,3.59E-06,3.92E-06,1.34E-06,9.44E-08,2.31E-06,1.98E-05,1.76E-06,2.44E-05,3.56E-05,0.999903153,1.93E-07,3.70E-06,8.89E-08
7218,machine-translation8,59,"The graphical illustration of the proposed model trying to generate the t-th target wordy t given a source sentence ( x 1 , x 2 , . . . , x T ) .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,49,1,0,,1.09E-06,0,negative,1.43E-06,9.91E-06,2.65E-06,3.19E-07,2.23E-06,4.88E-05,3.39E-06,7.87E-05,0.000203949,0.999644333,1.34E-06,2.35E-06,6.05E-07
7219,machine-translation8,60,"In a new model architecture , we define each conditional probability in Eq .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,50,1,0,,5.29E-06,0,negative,2.26E-05,4.72E-05,4.35E-05,1.57E-08,2.69E-06,2.91E-06,1.08E-06,6.70E-06,0.000783123,0.999081888,9.56E-08,8.22E-06,5.51E-08
7220,machine-translation8,61,( 2 ) as :,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,51,1,0,,3.09E-05,0,negative,3.89E-06,9.52E-06,6.49E-06,8.22E-09,5.20E-07,3.68E-06,8.31E-07,8.69E-06,0.000190868,0.999770357,2.37E-07,4.85E-06,5.40E-08
7221,machine-translation8,62,"where s i is an RNN hidden state for time i , computed by",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,52,1,0,,3.56E-07,0,negative,2.07E-06,3.59E-06,1.64E-06,2.19E-08,1.23E-06,4.55E-06,5.90E-07,7.82E-06,2.69E-05,0.999949465,1.00E-07,1.96E-06,3.61E-08
7222,machine-translation8,63,It should be noted that unlike the existing encoder - decoder approach ( see Eq.,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,53,1,0,,2.95E-06,0,negative,4.49E-05,0.000117148,0.000122004,4.39E-08,4.21E-06,5.25E-06,3.18E-06,9.27E-06,0.00063668,0.999022946,1.10E-06,3.30E-05,2.90E-07
7223,machine-translation8,64,"( 2 ) ) , here the probability is conditioned on a distinct context vector c i for each target wordy i .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,54,1,0,,1.74E-06,0,negative,4.61E-06,1.80E-05,7.04E-06,1.29E-08,1.20E-06,4.42E-06,7.98E-07,1.10E-05,0.000286685,0.999663617,9.93E-08,2.44E-06,6.03E-08
7224,machine-translation8,65,"The context vector c i depends on a sequence of annotations ( h 1 , , h Tx ) to which an encoder maps the input sentence .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,55,1,0,,2.87E-06,0,negative,1.47E-06,2.30E-05,1.95E-06,4.23E-08,2.25E-06,1.34E-05,1.08E-06,6.09E-05,0.000262465,0.999632223,5.27E-08,9.93E-07,1.09E-07
7225,machine-translation8,66,Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i - th word of the input sequence .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,56,1,0,,1.27E-05,0,negative,1.34E-05,5.74E-05,1.53E-05,4.24E-08,9.79E-06,5.14E-06,1.18E-06,1.23E-05,0.000534596,0.999345833,5.06E-08,4.85E-06,1.39E-07
7226,machine-translation8,67,We explain in detail how the annotations are computed in the next section .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,57,1,0,,5.46E-07,0,negative,7.28E-07,5.05E-06,2.90E-07,1.13E-07,3.22E-06,1.19E-05,3.77E-07,2.32E-05,2.24E-05,0.999932242,1.23E-08,4.57E-07,5.24E-08
7227,machine-translation8,68,"The context vector c i is , then , computed as a weighted sum of these annotations hi :",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,58,1,0,,1.27E-06,0,negative,2.56E-06,1.02E-05,3.58E-06,1.07E-08,1.01E-06,3.80E-06,5.80E-07,8.91E-06,0.000137535,0.99982999,3.50E-08,1.76E-06,5.26E-08
7228,machine-translation8,69,The weight ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,59,1,0,,0.000485301,0,negative,6.65E-07,6.26E-06,7.17E-07,5.33E-08,5.74E-07,4.28E-05,4.13E-06,0.000133067,9.72E-05,0.999712758,2.89E-07,1.25E-06,2.99E-07
7229,machine-translation8,70,ij of each annotation h j is computed by,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,60,1,0,,1.21E-06,0,negative,3.31E-06,6.81E-06,4.88E-06,1.70E-08,1.63E-06,4.41E-06,9.98E-07,8.71E-06,3.90E-05,0.999925925,5.88E-08,4.21E-06,8.05E-08
7230,machine-translation8,71,"where e ij = a (s i?1 , h j ) is an alignment model which scores how well the inputs around position j and the output at position i match .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,61,1,0,,6.15E-07,0,negative,1.06E-06,4.72E-06,2.52E-06,9.69E-09,8.86E-07,3.58E-06,5.94E-07,8.77E-06,4.98E-05,0.999926862,2.91E-08,1.07E-06,4.72E-08
7231,machine-translation8,72,"The score is based on the RNN hidden state s i ?1 ( just before emitting y i , Eq. ( 4 ) ) and the j - th annotation h j of the input sentence .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,62,1,0,,1.36E-06,0,negative,2.17E-06,2.05E-05,1.94E-06,2.59E-08,2.58E-06,1.25E-05,1.64E-06,5.89E-05,0.000101793,0.999795649,2.61E-08,2.20E-06,9.45E-08
7232,machine-translation8,73,We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,63,1,0,,2.45E-05,0,negative,1.41E-05,0.000716484,1.72E-05,6.08E-07,2.32E-05,0.000135311,1.58E-05,0.000963761,0.008279836,0.989827562,2.29E-07,3.10E-06,2.80E-06
7233,machine-translation8,74,"Note that unlike in traditional machine translation , the alignment is not considered to be a latent variable .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,64,1,0,,1.63E-06,0,negative,9.73E-07,3.42E-06,5.92E-07,4.29E-08,9.56E-07,5.74E-06,2.65E-07,1.18E-05,2.37E-05,0.999951924,1.73E-08,4.62E-07,3.84E-08
7234,machine-translation8,75,"Instead , the alignment model directly computes a soft alignment , which allows the gradient of the cost function to be backpropagated through .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,65,1,0,,0.000137675,0,negative,2.72E-05,0.000398483,0.000169314,2.84E-08,8.78E-06,6.24E-06,3.66E-06,1.46E-05,0.007356629,0.992007672,1.26E-07,6.77E-06,4.53E-07
7235,machine-translation8,76,This gradient can be used to train the alignment model as well as the whole translation model jointly .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,66,1,0,,0.000151297,0,negative,5.71E-06,5.82E-05,1.27E-05,1.82E-08,3.14E-06,7.10E-06,1.95E-06,3.02E-05,0.000729998,0.999147207,2.86E-08,3.57E-06,1.67E-07
7236,machine-translation8,77,"We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,67,1,0,,1.42E-06,0,negative,5.51E-07,9.82E-06,2.60E-06,1.41E-08,6.17E-07,4.81E-06,6.61E-07,1.08E-05,0.000148697,0.99982022,1.13E-07,9.93E-07,1.15E-07
7237,machine-translation8,78,Let ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,68,1,0,,4.19E-07,0,negative,2.49E-07,3.00E-07,1.43E-07,5.52E-09,2.37E-07,5.29E-06,4.49E-07,7.67E-06,2.94E-06,0.999982231,5.45E-09,4.60E-07,2.02E-08
7238,machine-translation8,79,"ij be a probability that the target wordy i is aligned to , or translated from , a source word x j .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,69,1,0,,3.60E-07,0,negative,5.98E-07,2.64E-06,3.90E-07,8.12E-09,5.18E-07,4.01E-06,3.79E-07,1.74E-05,2.78E-05,0.999945722,5.23E-09,5.06E-07,3.12E-08
7239,machine-translation8,80,"Then , the i - th context vector c i is the expected annotation over all the annotations with probabilities ? ij .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,70,1,0,,7.03E-07,0,negative,1.70E-07,1.84E-06,3.73E-07,1.33E-09,1.60E-07,1.90E-06,2.51E-07,7.67E-06,2.94E-05,0.999957933,3.43E-09,2.56E-07,1.50E-08
7240,machine-translation8,81,The probability ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,71,1,0,,2.23E-06,0,negative,2.99E-07,1.08E-06,2.67E-07,1.07E-08,2.48E-07,1.36E-05,1.55E-06,2.89E-05,1.77E-05,0.999935544,2.07E-08,6.70E-07,8.84E-08
7241,machine-translation8,82,"ij , or it s associated energy e ij , reflects the importance of the annotation h j with respect to the previous hidden state s i ?1 in deciding the next state s i and generating y i .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,72,1,0,,6.16E-07,0,negative,4.53E-06,8.04E-06,2.32E-06,4.70E-08,3.11E-06,6.30E-06,6.92E-07,1.32E-05,5.43E-05,0.99990602,7.22E-09,1.40E-06,8.24E-08
7242,machine-translation8,83,"Intuitively , this implements a mechanism of attention in the decoder .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,73,1,0,,4.98E-07,0,negative,9.87E-07,9.37E-06,4.35E-06,7.36E-09,7.61E-07,4.44E-06,7.60E-07,1.25E-05,0.000506778,0.999459498,8.55E-09,3.92E-07,1.08E-07
7243,machine-translation8,84,The decoder decides parts of the source sentence to pay attention to .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,74,1,0,,7.93E-06,0,negative,1.05E-05,0.000149944,0.000111847,5.72E-09,4.16E-06,2.31E-06,2.87E-06,5.54E-06,0.005316285,0.994390951,2.28E-08,5.31E-06,2.77E-07
7244,machine-translation8,85,"By letting the decoder have an attention mechanism , we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,75,1,0,,0.000170254,0,negative,0.000238652,0.000931558,3.68E-05,2.44E-07,4.31E-05,2.29E-05,1.73E-05,6.74E-05,0.003669393,0.994917296,7.27E-08,5.32E-05,2.09E-06
7245,machine-translation8,86,"With this new approach the information can be spread throughout the sequence of annotations , which can be selectively retrieved by the decoder accordingly .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,76,1,0,,0.000160933,0,negative,1.51E-05,0.000227419,1.70E-05,7.55E-08,1.26E-05,7.54E-06,3.51E-06,2.25E-05,0.001793467,0.997888421,5.33E-08,1.15E-05,7.33E-07
7246,machine-translation8,87,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,INTRODUCTION,,machine-translation,8,77,1,0,,0.654947675,1,model,1.59E-05,0.027384907,0.000515554,1.97E-06,0.00019487,0.000428493,0.00467669,0.000486224,0.493759557,0.471993357,0.000273949,6.41E-05,0.000204441
7247,machine-translation8,88,"The usual RNN , described in Eq. ( 1 ) , reads an input sequence x in order starting from the first symbol x 1 to the last one x Tx .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,78,1,0,,3.41E-07,0,negative,2.54E-07,2.34E-06,2.02E-06,1.12E-08,1.50E-07,1.76E-06,3.63E-06,7.73E-06,0.000160056,0.999818434,6.44E-08,2.83E-06,7.25E-07
7248,machine-translation8,89,"However , in the proposed scheme , we would like the annotation of each word to summarize not only the preceding words , but also the following words .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,79,1,0,,7.24E-07,0,negative,2.36E-06,6.91E-06,2.85E-06,1.01E-09,2.44E-07,1.85E-07,5.28E-07,1.02E-06,0.000229298,0.999752926,1.73E-09,3.63E-06,3.84E-08
7249,machine-translation8,90,"Hence , we propose to use a bidirectional RNN ( BiRNN , , which has been successfully used recently in speech recognition ( see , e.g. , .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,80,1,0,,4.34E-05,0,negative,2.32E-05,6.99E-05,0.000130199,5.49E-08,4.12E-06,3.01E-06,1.31E-05,6.55E-06,0.001061884,0.998659544,5.01E-08,2.63E-05,2.14E-06
7250,machine-translation8,91,A BiRNN consists of forward and backward RNN 's .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,81,1,0,,1.20E-06,0,negative,9.75E-07,5.15E-06,3.35E-05,8.33E-09,6.26E-07,1.02E-06,2.77E-06,2.84E-06,0.000625123,0.999324465,9.11E-09,2.48E-06,1.01E-06
7251,machine-translation8,92,The forward RNN ? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,82,1,0,,5.57E-05,0,negative,4.29E-07,4.98E-07,4.38E-06,5.09E-10,4.08E-08,3.94E-07,2.59E-06,9.65E-07,9.79E-05,0.999889989,3.74E-09,2.65E-06,1.59E-07
7252,machine-translation8,93,f reads the input sequence as it is ordered ( from x 1 to x Tx ) and calculates a sequence of forward hidden states (,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,83,1,0,,7.70E-07,0,negative,3.34E-06,1.37E-06,1.39E-05,6.39E-10,1.85E-07,1.67E-07,1.52E-06,4.38E-07,5.36E-05,0.999919065,8.24E-10,6.40E-06,5.06E-08
7253,machine-translation8,94,The backward RNN,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,84,1,0,,8.58E-06,0,negative,9.47E-07,3.66E-06,4.57E-05,7.91E-10,8.42E-08,6.95E-07,1.70E-05,1.73E-06,0.001024253,0.998888229,5.37E-08,1.66E-05,1.03E-06
7254,machine-translation8,95,? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,85,1,0,,1.46E-07,0,negative,1.08E-07,3.20E-08,5.82E-08,7.83E-10,2.28E-08,4.16E-07,6.57E-07,1.06E-06,9.19E-07,0.999995714,2.24E-10,9.95E-07,2.04E-08
7255,machine-translation8,96,"f reads the sequence in the reverse order ( from x Tx to x 1 ) , resulting in a sequence of backward hidden states (",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,86,1,0,,4.89E-07,0,negative,3.33E-06,5.95E-07,6.90E-06,3.23E-10,1.28E-07,1.01E-07,1.11E-06,2.37E-07,1.95E-05,0.999961904,2.64E-10,6.14E-06,2.68E-08
7256,machine-translation8,97,We obtain an annotation for each word x j by concatenating the forward hidden state ? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,87,1,0,,1.14E-07,0,negative,5.43E-07,1.83E-07,3.84E-07,6.37E-10,8.48E-08,1.24E-07,3.09E-07,5.19E-07,4.24E-06,0.999991937,9.87E-11,1.66E-06,1.87E-08
7257,machine-translation8,98,h j and the backward one,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,88,1,0,,2.93E-07,0,negative,2.35E-06,2.13E-07,1.39E-06,9.32E-10,1.43E-07,2.26E-07,9.49E-07,4.81E-07,3.60E-06,0.999985132,1.92E-10,5.48E-06,3.77E-08
7258,machine-translation8,99,"In this way , the annotation h j contains the summaries of both the preceding words and the following words .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,89,1,0,,5.22E-07,0,negative,2.23E-07,3.71E-07,2.66E-07,6.31E-10,8.97E-08,1.68E-07,2.44E-07,1.01E-06,2.00E-05,0.999976937,1.02E-10,6.30E-07,3.39E-08
7259,machine-translation8,100,"Due to the tendency of RNNs to better represent recent inputs , the annotation h j will be focused on the words around x j .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,90,1,0,,3.70E-07,0,negative,2.25E-07,5.13E-07,2.08E-07,1.78E-10,4.26E-08,1.03E-07,2.44E-07,9.17E-07,1.47E-05,0.999982541,5.63E-11,5.37E-07,1.22E-08
7260,machine-translation8,101,This sequence of annotations is used by the decoder and the alignment model later to compute the context vector ( Eqs. ( 5 ) - ) .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,91,1,0,,6.71E-07,0,negative,7.06E-07,1.49E-06,1.28E-06,7.56E-09,8.40E-07,7.12E-07,1.22E-06,3.72E-06,4.31E-05,0.999945084,1.70E-10,1.63E-06,2.05E-07
7261,machine-translation8,102,See for the graphical illustration of the proposed model .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,92,1,0,,1.15E-07,0,negative,7.71E-08,4.39E-08,8.75E-08,8.48E-10,2.20E-08,2.46E-07,3.20E-07,1.13E-06,2.26E-06,0.99999527,7.68E-11,5.11E-07,2.76E-08
7262,machine-translation8,103,EXPERIMENT SETTINGS,,,machine-translation,8,0,1,0,,0.002627569,0,negative,2.10E-05,0.000390606,6.58E-06,1.64E-05,8.80E-06,0.000520334,0.0001514,0.004909198,0.000115491,0.992556556,0.001165654,0.000131591,6.35E-06
7263,machine-translation8,104,We evaluate the proposed approach on the task of English - to - French translation .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,1,1,0,,0.049140658,0,negative,0.00010466,0.00020351,0.000602794,1.78E-05,3.87E-05,0.100156149,0.011554576,0.062170607,1.86E-05,0.822956626,0.000657464,0.001435278,8.32E-05
7264,machine-translation8,105,"We use the bilingual , parallel corpora provided by ACL WMT ' 14 .",EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,2,1,0,,0.151899722,0,experimental-setup,4.12E-05,4.72E-05,0.000553848,5.82E-05,0.000115646,0.592268613,0.008599549,0.059439008,1.55E-05,0.338686592,2.47E-05,0.000105968,4.39E-05
7265,machine-translation8,106,3,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,3,1,0,,0.00265071,0,negative,6.09E-05,4.38E-06,4.67E-05,2.13E-06,1.20E-06,0.068964119,0.000339611,0.020982997,1.29E-05,0.909482535,1.29E-05,8.45E-05,5.11E-06
7266,machine-translation8,107,"As a comparison , we also report the performance of an RNN Encoder - Decoder which was proposed recently by .",EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,4,1,0,,0.2180005,0,negative,0.000420948,9.84E-05,0.002939931,6.84E-06,1.52E-05,0.07466082,0.002630588,0.02472521,3.31E-05,0.893493155,3.67E-05,0.000918897,2.01E-05
7267,machine-translation8,108,We use the same training procedures and the same dataset for both models .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,5,1,0,,0.12081645,0,experimental-setup,3.60E-05,5.20E-05,0.000101623,8.98E-06,1.17E-05,0.452265753,0.003105444,0.181642794,1.39E-05,0.362628523,7.70E-06,0.000110333,1.52E-05
7268,machine-translation8,109,4,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,6,1,0,,0.002171849,0,negative,4.77E-05,3.91E-06,4.61E-05,1.64E-06,1.16E-06,0.081758032,0.000435459,0.02150576,1.19E-05,0.896092398,9.11E-06,8.16E-05,5.18E-06
7269,machine-translation8,110,DATASET,EXPERIMENT SETTINGS,,machine-translation,8,7,1,0,,0.006227333,0,negative,4.77E-05,5.36E-06,6.62E-05,2.00E-05,3.56E-06,0.234471586,0.002569733,0.048455374,1.37E-05,0.713963036,9.14E-05,0.000245964,4.63E-05
7270,machine-translation8,111,"WMT ' 14 contains the following English - French parallel corpora : Europarl ( 61 M words ) , news commentary ( 5.5 M ) , UN ( 421M ) and two crawled corpora of 90 M and 272.5 M words respectively , totaling 850M words .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,8,1,0,,0.020097686,0,negative,7.27E-05,5.44E-05,0.001313214,7.81E-05,0.003969682,0.021618333,0.007062804,0.000745481,3.95E-06,0.964464399,5.55E-05,0.000472367,8.90E-05
7271,machine-translation8,112,"Following the procedure described in , we reduce the size of the combined corpus to have 348M words using the data selection method by .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,9,1,0,,0.046158358,0,negative,0.001058582,0.000403577,0.000264343,2.87E-05,0.000554746,0.084852641,0.003119757,0.021678483,4.09E-05,0.887371825,1.58E-05,0.00055435,5.63E-05
7272,machine-translation8,113,"We do not use any monolingual data other than the mentioned parallel corpora , although it maybe possible to use a much larger monolingual corpus to pretrain an encoder .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,10,1,0,,0.000320067,0,negative,2.34E-05,1.01E-05,4.47E-05,2.08E-06,9.25E-05,0.003507413,0.000149967,0.000335054,1.00E-06,0.995784149,2.85E-06,4.55E-05,1.36E-06
7273,machine-translation8,114,"We concatenate news - test - After a usual tokenization 6 , we use a shortlist of 30,000 most frequent words in each language to train our models .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,11,1,0,,0.037427881,0,negative,4.53E-05,0.000108188,0.000410968,4.65E-05,0.000945542,0.206062886,0.003164899,0.016525194,1.25E-05,0.772594932,3.89E-06,4.05E-05,3.88E-05
7274,machine-translation8,115,Any word not included in the shortlist is mapped to a special token ( [ UNK ] ) .,EXPERIMENT SETTINGS,DATASET,machine-translation,8,12,1,0,,0.006391338,0,negative,1.46E-05,2.52E-05,0.000186374,2.04E-07,3.64E-06,0.007785099,9.66E-05,0.002785899,4.04E-05,0.989040862,2.62E-06,1.72E-05,1.35E-06
7275,machine-translation8,116,"We do not apply any other special preprocessing , such as lowercasing or stemming , to the data .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,13,1,0,,0.000459565,0,negative,2.92E-05,3.48E-05,7.48E-05,5.88E-06,0.000115297,0.004170717,5.74E-05,0.000466785,4.92E-06,0.995023514,1.67E-06,1.34E-05,1.61E-06
7276,machine-translation8,117,MODELS,,,machine-translation,8,0,1,0,,0.013378693,0,negative,0.001769058,0.000281584,0.001312991,0.00016118,0.000150821,0.001858418,0.032853537,0.00468419,0.00015464,0.882857056,0.025147519,0.048083361,0.000685644
7277,machine-translation8,118,We train two types of models .,MODELS,MODELS,machine-translation,8,1,1,1,hyperparameters,0.395654116,0,negative,0.000124328,4.09E-05,0.438906714,1.18E-06,1.83E-06,0.000962096,0.072295692,0.000905763,0.000902973,0.485636217,9.07E-06,5.87E-05,0.000154582
7278,machine-translation8,119,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .",MODELS,MODELS,machine-translation,8,2,1,1,hyperparameters,0.709008847,1,baselines,0.000213238,2.19E-05,0.9197896,3.58E-07,1.36E-06,6.41E-05,0.011902138,2.27E-05,0.001268539,0.066569568,5.97E-06,7.66E-05,6.40E-05
7279,machine-translation8,120,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .",MODELS,MODELS,machine-translation,8,3,1,1,hyperparameters,0.952097595,1,experiments,0.000154382,1.39E-05,0.009454218,1.26E-06,2.23E-06,0.004173474,0.83241148,0.002791953,6.26E-05,0.150394821,2.19E-06,0.000164149,0.00037337
7280,machine-translation8,121,The encoder and decoder of the RNNencdec have 1000 hidden units each .,MODELS,MODELS,machine-translation,8,4,1,1,hyperparameters,0.971102848,1,experiments,0.000156935,2.06E-05,0.00399289,1.75E-05,3.43E-06,0.020261261,0.913703445,0.0160093,0.000291935,0.042191306,9.48E-06,7.23E-05,0.003269558
7281,machine-translation8,122,The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .,MODELS,MODELS,machine-translation,8,5,1,1,hyperparameters,0.942986498,1,experiments,0.00021113,3.71E-05,0.02276601,2.35E-05,7.93E-06,0.017502146,0.843736986,0.011737967,0.00086059,0.099378498,1.22E-05,8.28E-05,0.0036433
7282,machine-translation8,123,It s decoder has 1000 hidden units .,MODELS,MODELS,machine-translation,8,6,1,1,hyperparameters,0.955463509,1,experiments,0.000201001,2.28E-05,0.010446567,9.52E-06,2.73E-06,0.019669157,0.818418684,0.018725998,0.000671238,0.128205085,1.39E-05,9.08E-05,0.003522479
7283,machine-translation8,124,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .",MODELS,MODELS,machine-translation,8,7,1,1,hyperparameters,0.482584583,0,negative,0.001225293,0.000135866,0.407344027,4.12E-06,4.20E-06,0.002118975,0.089422329,0.002031847,0.012247224,0.484819022,1.65E-05,0.00014644,0.000484175
7284,machine-translation8,125,We use a minibatch stochastic gradient descent ( SGD ) algorithm together with Adadelta to train each model .,MODELS,MODELS,machine-translation,8,8,1,0,,0.981135441,1,experiments,0.000489897,0.00016108,0.036096978,2.27E-05,9.62E-06,0.014907473,0.801809299,0.02215395,0.001603291,0.118966786,9.14E-06,0.000105404,0.003664435
7285,machine-translation8,126,Each SGD update direction is computed using a minibatch of 80 sentences .,MODELS,MODELS,machine-translation,8,9,1,0,,0.569827728,1,experiments,0.000140504,3.44E-05,0.00224711,2.36E-06,2.51E-06,0.010395036,0.795684365,0.020464305,0.000197434,0.169595785,4.00E-06,0.000104609,0.00112759
7286,machine-translation8,127,We trained each model for approximately 5 days .,MODELS,MODELS,machine-translation,8,10,1,0,,0.169959602,0,negative,0.000325716,1.92E-05,0.003229551,1.10E-05,2.34E-05,0.005646915,0.42707123,0.002845521,0.000116839,0.558731246,2.70E-06,0.000264627,0.001712081
7287,machine-translation8,128,"Once a model is trained , we use a beam search to find a translation that approximately maximizes the conditional probability ( see , e.g. , .",MODELS,MODELS,machine-translation,8,11,1,0,,0.053377513,0,negative,0.000546337,3.26E-05,0.212993264,6.89E-07,2.58E-06,0.000182692,0.0120519,0.000139432,0.001179478,0.772689799,3.55E-06,0.000101957,7.57E-05
7288,machine-translation8,129,used this approach to generate translations from their neural machine translation model .,MODELS,MODELS,machine-translation,8,12,1,0,,0.005772314,0,negative,0.000139364,4.84E-07,0.005140636,2.30E-06,5.77E-06,0.000222497,0.019737374,2.02E-05,4.46E-06,0.974299497,2.81E-06,0.000301481,0.000123088
7289,machine-translation8,130,"For more details on the architectures of the models and training procedure used in the experiments , see Appendices A and B.",MODELS,MODELS,machine-translation,8,13,1,0,,0.003990418,0,negative,3.44E-05,1.21E-06,0.000164341,2.41E-06,2.11E-06,0.00018545,0.004941045,0.000131335,1.24E-05,0.994415456,5.06E-07,8.75E-05,2.18E-05
7290,machine-translation8,131,RESULTS,,,machine-translation,8,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
7291,machine-translation8,132,QUANTITATIVE RESULTS,,,machine-translation,8,0,1,0,,0.352386936,0,negative,0.000138738,8.26E-05,1.61E-05,4.20E-07,5.65E-07,4.18E-05,0.000382204,0.000722484,2.22E-05,0.960882795,0.017153203,0.020550016,6.87E-06
7292,machine-translation8,133,In : Four sample alignments found by RNNsearch - 50 .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,1,1,0,,0.006922903,0,negative,0.004449084,2.80E-06,0.002131127,1.60E-05,1.68E-06,6.70E-05,0.000649668,0.000102838,7.49E-06,0.817801863,0.000138739,0.174373605,0.000258043
7293,machine-translation8,134,"The x - axis and y-axis of each plot correspond to the words in the source sentence ( English ) and the generated translation ( French ) , respectively .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,2,1,0,,0.000512711,0,negative,0.000150971,2.56E-06,4.80E-05,5.05E-07,8.65E-08,6.50E-05,3.58E-05,0.000970015,1.12E-05,0.996269472,6.91E-06,0.002431757,7.66E-06
7294,machine-translation8,135,Each pixel shows the weight ?,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,3,1,0,,0.000269317,0,negative,0.000203128,7.51E-07,0.00011353,1.58E-07,4.73E-08,8.20E-06,9.57E-06,8.60E-05,7.14E-06,0.997254756,1.68E-06,0.002312039,3.05E-06
7295,machine-translation8,136,"ij of the annotation of the j - th source word for the i - th target word ( see Eq. ) , in grayscale ( 0 : black , 1 : white ) .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,4,1,0,,6.45E-05,0,negative,0.000651554,8.69E-07,0.000239229,1.12E-07,6.85E-08,2.93E-06,6.87E-06,2.84E-05,3.33E-06,0.990761802,1.25E-06,0.008301915,1.71E-06
7296,machine-translation8,137,( a ) an arbitrary sentence .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,5,1,0,,0.00057489,0,negative,0.000101434,7.31E-07,0.000117304,8.98E-08,4.29E-08,4.61E-06,1.53E-05,4.85E-05,2.27E-06,0.99348707,1.02E-05,0.006206862,5.66E-06
7297,machine-translation8,138,( b - d ) three randomly selected samples among the sentences without any unknown words and of length between 10 and 20 words from the test set .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,6,1,0,,0.000805669,0,negative,0.000721567,1.05E-06,0.000504715,2.22E-07,1.94E-07,8.22E-06,0.000105232,7.47E-05,1.21E-06,0.942326426,3.34E-06,0.056244146,8.97E-06
7298,machine-translation8,139,One of the motivations behind the proposed approach was the use of a fixed - length context vector in the basic encoder - decoder approach .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,7,1,0,,0.000394128,0,negative,0.002134058,3.01E-05,0.000522384,1.09E-05,1.92E-06,4.96E-05,6.98E-05,0.000317142,6.01E-05,0.979495384,0.00010064,0.017134436,7.36E-05
7299,machine-translation8,140,We conjectured that this limitation may make the basic encoder - decoder approach to underperform with long sentences .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,8,1,0,,0.000167887,0,negative,0.000272903,9.84E-07,3.01E-05,5.79E-07,1.29E-07,9.26E-06,9.01E-06,6.50E-05,5.85E-06,0.996647986,3.41E-06,0.002950342,4.46E-06
7300,machine-translation8,141,"In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,9,1,1,results,0.868265463,1,results,0.017606805,7.75E-08,1.25E-05,1.15E-07,4.39E-08,1.11E-06,0.000196354,6.72E-06,4.77E-08,0.03519679,1.44E-06,0.946969646,8.33E-06
7301,machine-translation8,142,"On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,10,1,1,results,0.95825229,1,results,0.007830037,4.95E-08,7.23E-06,1.30E-07,3.18E-08,1.40E-06,0.000284856,1.09E-05,2.74E-08,0.017160982,3.78E-07,0.974694657,9.27E-06
7302,machine-translation8,143,"RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,11,1,1,results,0.935611024,1,results,0.000501244,2.42E-08,1.12E-05,3.26E-08,1.32E-08,7.89E-07,0.000256179,5.52E-06,1.17E-08,0.014988176,6.12E-07,0.984227195,9.00E-06
7303,machine-translation8,144,This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,12,1,1,results,0.518609536,1,results,0.00335797,6.82E-08,1.22E-05,1.19E-07,3.91E-08,1.65E-06,0.000271798,1.03E-05,5.09E-08,0.03191557,5.17E-07,0.964418678,1.10E-05
7304,machine-translation8,145,tokens when only the sentences having no unknown words were evaluated ( last column ) .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,13,1,0,,0.00053501,0,negative,0.000250302,2.38E-07,4.16E-05,1.22E-07,1.09E-07,5.95E-06,4.87E-05,4.96E-05,6.81E-07,0.974041275,7.14E-07,0.025554695,6.02E-06
7305,machine-translation8,146,QUALITATIVE ANALYSIS,QUANTITATIVE RESULTS,,machine-translation,8,14,1,0,,0.005487488,0,negative,0.002468571,4.73E-06,0.000242515,1.75E-05,1.43E-06,0.000118505,0.000731055,0.000657246,2.28E-05,0.897961528,8.42E-05,0.097210385,0.000479579
7306,machine-translation8,147,ALIGNMENT,QUANTITATIVE RESULTS,,machine-translation,8,15,1,0,,0.003871674,0,negative,0.000951153,2.88E-06,0.00028013,3.22E-06,1.26E-06,3.53E-05,0.00024039,0.00021818,1.27E-05,0.934578391,8.60E-06,0.063579437,8.83E-05
7307,machine-translation8,148,The proposed approach provides an intuitive way to inspect the ( soft - ) alignment between the words in a generated translation and those in a source sentence .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,16,1,0,,0.000186164,0,negative,0.00364671,6.92E-06,0.000713199,1.29E-06,4.89E-07,2.59E-06,7.38E-06,2.22E-05,3.48E-05,0.978857975,7.05E-06,0.016611932,8.75E-05
7308,machine-translation8,149,This is done by visualizing the annotation weights ?,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,17,1,0,,6.77E-06,0,negative,0.000192802,1.43E-07,2.77E-05,4.64E-07,2.21E-08,1.78E-06,1.38E-06,1.10E-05,1.79E-06,0.999152862,1.10E-06,0.000577539,3.14E-05
7309,machine-translation8,150,"ij from Eq. , as in .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,18,1,0,,2.80E-06,0,negative,0.000245715,4.00E-08,3.97E-05,3.82E-08,8.91E-09,3.04E-07,5.42E-07,3.00E-06,6.34E-07,0.99906392,7.00E-08,0.000642404,3.65E-06
7310,machine-translation8,151,Each row of a matrix in each plot indicates the weights associated with the annotations .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,19,1,0,,2.06E-06,0,negative,0.000122168,1.10E-07,8.36E-05,3.10E-08,1.45E-08,5.12E-07,6.44E-07,8.03E-06,3.11E-06,0.999591699,3.89E-08,0.000186473,3.54E-06
7311,machine-translation8,152,From this we see which positions in the source sentence were considered more important when generating the target word .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,20,1,0,,1.93E-06,0,negative,0.001742272,4.11E-08,2.76E-05,1.10E-08,1.16E-08,2.08E-07,1.45E-06,1.72E-06,2.57E-07,0.993589576,2.35E-08,0.004634889,1.94E-06
7312,machine-translation8,153,We can see from the alignments in that the alignment of words between English and French is largely monotonic .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,21,1,0,,0.001631069,0,results,0.006957802,5.43E-08,7.58E-05,5.26E-08,3.34E-08,6.43E-07,8.56E-05,4.30E-06,7.99E-08,0.421487585,4.17E-07,0.571336626,5.11E-05
7313,machine-translation8,154,We see strong weights along the diagonal of each matrix .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,22,1,0,,1.10E-05,0,negative,0.04229853,3.28E-07,0.000275223,6.09E-07,1.41E-07,2.71E-06,1.16E-05,2.22E-05,5.00E-06,0.943582393,1.19E-07,0.013753384,4.77E-05
7314,machine-translation8,155,"However , we also observe a number of non-trivial , non-monotonic alignments .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,23,1,0,,0.000105727,0,negative,0.003284744,6.14E-08,0.000118669,2.03E-08,3.73E-08,2.06E-07,2.73E-06,1.13E-06,1.60E-07,0.981520384,6.88E-08,0.015067592,4.19E-06
7315,machine-translation8,156,"Adjectives and nouns are typically ordered differently between French and English , and we see an example in The strength of the soft - alignment , opposed to a hard - alignment , is evident , for instance , from ].",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,24,1,0,,3.60E-06,0,negative,0.000163963,4.86E-08,4.58E-05,7.05E-07,7.08E-08,2.19E-06,4.52E-06,6.10E-06,4.50E-07,0.998346555,1.32E-06,0.001271349,0.000156973
7316,machine-translation8,157,We observe similar behaviors in all the presented cases in .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,25,1,0,,0.000261874,0,negative,0.004436727,4.27E-08,2.38E-05,2.30E-08,2.03E-08,3.10E-07,1.90E-05,2.99E-06,4.63E-08,0.870779404,7.16E-08,0.124729223,8.38E-06
7317,machine-translation8,158,"An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths , without requiring a counter - intuitive way of mapping some words to or from nowhere ( [ NULL ] ) ( see , e.g. , Chapters 4 and 5 of .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,26,1,0,,5.07E-06,0,negative,0.0010544,2.20E-07,0.000978565,1.04E-07,5.10E-08,5.39E-07,2.93E-06,1.69E-06,1.35E-06,0.988000833,8.84E-07,0.009929776,2.87E-05
7318,machine-translation8,159,LONG SENTENCES,QUANTITATIVE RESULTS,,machine-translation,8,27,1,0,,0.059833216,0,results,0.000369834,1.82E-07,3.63E-05,1.41E-07,1.39E-07,1.45E-06,0.000791983,9.42E-06,8.33E-08,0.076584498,3.34E-06,0.922099416,0.000103213
7319,machine-translation8,160,As clearly visible from the proposed model ( RNNsearch ) is much better than the conventional model ( RNNencdec ) at translating long sentences .,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,28,1,0,,0.626787595,1,results,7.99E-05,4.02E-10,1.11E-06,1.66E-09,2.35E-10,2.81E-08,6.02E-05,1.57E-07,5.09E-10,0.01547187,5.54E-09,0.984363562,2.32E-05
7320,machine-translation8,161,"This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed - length vector perfectly , but only accurately encoding the parts of the input sentence that surround a particular word .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,29,1,0,,1.22E-05,0,negative,4.55E-05,9.26E-09,1.09E-05,4.71E-09,1.77E-09,1.18E-07,1.74E-06,7.98E-07,6.95E-08,0.994362458,1.33E-08,0.005574842,3.52E-06
7321,machine-translation8,162,"As an example , consider this source sentence from the test set :",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,30,1,0,,4.99E-06,0,negative,5.40E-06,1.44E-09,6.91E-06,1.12E-09,1.03E-09,6.12E-08,1.90E-06,2.89E-07,1.36E-08,0.994214262,5.60E-09,0.005767948,3.20E-06
7322,machine-translation8,163,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,31,1,0,,0.000138744,0,negative,3.53E-05,4.22E-08,3.22E-05,3.59E-07,1.30E-08,1.34E-06,8.74E-06,6.98E-06,6.42E-07,0.990249908,2.90E-07,0.009357848,0.000306323
7323,machine-translation8,164,The RNNencdec - 50 translated this sentence into :,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,32,1,0,,1.26E-05,0,negative,4.00E-05,2.40E-09,1.13E-05,1.30E-07,8.09E-09,6.20E-07,6.48E-06,8.60E-07,6.17E-08,0.993223198,1.89E-08,0.006610893,0.000106478
7324,machine-translation8,165,Un privilge d'admission est le droit d'un mdecin de reconnatre un patient l'hpital ou un centre mdical d'un diagnostic ou de prendre un diagnostic en fonction de sontat de sant .,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,33,1,0,,0.000351476,0,negative,0.000401543,5.96E-08,0.000101867,1.49E-06,3.79E-08,1.64E-06,2.34E-05,4.54E-06,1.00E-06,0.956202395,2.66E-07,0.042185884,0.001075832
7325,machine-translation8,166,"On the other hand , the RNNsearch - 50 generated the following correct translation , preserving the whole meaning of the input sentence without omitting any details :",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,34,1,0,,0.001068919,0,negative,8.31E-05,4.55E-09,3.68E-05,3.36E-09,2.79E-09,1.40E-07,3.72E-05,5.16E-07,1.47E-08,0.78755426,2.00E-08,0.212265731,2.23E-05
7326,machine-translation8,167,"Un privilge d'admission est le droit d'un mdecin d'admettre un patient un hpital ou un centre mdical pour effectuer un diagnostic ou une procdure , selon son statut de travailleur des soins de sant l'hpital .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,35,1,0,,0.000229981,0,negative,0.000281072,6.61E-08,8.93E-05,1.37E-06,3.82E-08,1.67E-06,2.36E-05,5.35E-06,1.01E-06,0.96336064,2.83E-07,0.03493994,0.001295613
7327,machine-translation8,168,Let us consider another sentence from the test set :,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,36,1,0,,1.10E-05,0,negative,3.83E-06,7.52E-10,6.67E-06,7.11E-10,6.81E-10,3.89E-08,1.42E-06,2.03E-07,9.78E-09,0.997027112,1.63E-09,0.002957628,3.08E-06
7328,machine-translation8,169,"This kind of experience is part of Disney 's efforts to "" extend the lifetime of its series and build new relationships with audiences via digital platforms thatare becoming evermore important , "" he added .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,37,1,0,,1.43E-05,0,negative,6.19E-05,1.74E-08,1.07E-05,1.00E-06,3.12E-08,1.58E-06,1.07E-05,4.19E-06,3.16E-07,0.992742413,3.99E-08,0.006812095,0.000355055
7329,machine-translation8,170,The translation by the RNNencdec - 50 is,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,38,1,0,,2.08E-05,0,negative,6.86E-05,1.07E-08,6.69E-05,2.42E-08,9.66E-09,1.07E-06,2.98E-05,4.13E-06,1.39E-07,0.988874314,1.09E-08,0.010844053,0.000110936
7330,machine-translation8,171,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,39,1,0,,6.23E-05,0,negative,0.000289697,3.28E-08,0.000100689,1.17E-07,1.80E-08,4.50E-07,1.79E-05,1.33E-06,2.39E-07,0.938779585,1.31E-07,0.060527597,0.000282225
7331,machine-translation8,172,"As with the previous example , the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words ( see the underlined phrase ) .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,40,1,0,,3.66E-05,0,negative,0.000885394,6.31E-09,8.70E-06,4.23E-08,1.70E-08,3.31E-07,4.10E-05,1.23E-06,3.49E-08,0.874803137,7.45E-09,0.124139571,0.000120539
7332,machine-translation8,173,"After that point , the quality of the translation deteriorates , with basic mistakes such as the lack of a closing quotation mark .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,41,1,0,,0.000293293,0,negative,0.000483034,9.01E-09,1.06E-05,7.85E-08,8.58E-09,2.46E-07,1.03E-05,8.52E-07,9.41E-08,0.958942592,2.08E-08,0.040418389,0.000133749
7333,machine-translation8,174,"Again , the RNNsearch - 50 was able to translate this long sentence correctly :",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,42,1,0,,9.55E-05,0,negative,4.03E-05,1.55E-09,3.45E-06,4.70E-09,1.59E-09,2.29E-07,8.50E-05,1.11E-06,6.17E-09,0.697191973,1.73E-08,0.30250692,0.000170901
7334,machine-translation8,175,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,43,1,0,,6.11E-05,0,negative,0.00013539,2.65E-08,3.25E-05,3.14E-07,2.23E-08,7.44E-07,1.42E-05,2.34E-06,2.60E-07,0.978053291,1.17E-07,0.02118665,0.000574168
7335,machine-translation8,176,"In conjunction with the quantitative results presented already , these qualitative observations confirm our hypotheses that the RNNsearch architecture enables far more reliable translation of long sentences than the standard RNNencdec model .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,44,1,0,,0.003842491,0,results,0.000235952,1.42E-09,1.44E-06,4.12E-09,1.03E-09,7.28E-08,6.59E-05,4.77E-07,3.48E-09,0.318153046,5.30E-09,0.681500559,4.25E-05
7336,machine-translation8,177,"In Appendix C , we provide a few more sample translations of long source sentences generated by the RNNencdec - 50 , RNNsearch - 50 and Google Translate along with the reference translations .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,45,1,0,,0.000122194,0,negative,2.89E-05,2.32E-09,3.05E-06,5.01E-08,2.20E-08,1.93E-07,7.89E-06,9.67E-07,1.07E-08,0.990015624,7.32E-10,0.009919217,2.41E-05
7337,machine-translation8,178,6 RELATED WORK,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,46,1,0,,2.78E-05,0,negative,3.11E-05,7.73E-09,7.97E-06,4.72E-08,3.49E-09,8.78E-07,1.95E-05,3.39E-06,7.66E-08,0.986242171,3.94E-08,0.013545604,0.000149219
7338,machine-translation8,179,LEARNING TO ALIGN,QUANTITATIVE RESULTS,,machine-translation,8,47,1,0,,0.002382748,0,negative,0.000429288,1.52E-06,6.44E-05,1.35E-06,4.26E-07,7.93E-06,0.000114151,0.00012175,4.65E-06,0.952698552,2.62E-06,0.04636093,0.000192476
7339,machine-translation8,180,A similar approach of aligning an output symbol with an input symbol was proposed recently by in the context of handwriting synthesis .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,48,1,0,,2.70E-05,0,negative,8.08E-05,7.14E-08,0.000120452,1.77E-07,2.05E-08,1.39E-06,7.97E-06,5.29E-06,5.95E-07,0.997083285,2.01E-06,0.002125856,0.000572117
7340,machine-translation8,181,Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,49,1,0,,0.003302476,0,negative,0.000423196,2.33E-07,0.000240457,6.39E-06,1.49E-07,1.07E-05,9.99E-05,2.52E-05,1.25E-06,0.960333175,1.51E-05,0.019456996,0.019387245
7341,machine-translation8,182,"In his work , he used a mixture of Gaussian kernels to compute the weights of the annotations , where the location , width and mixture coefficient of each kernel was predicted from an alignment model .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,50,1,0,,5.17E-06,0,negative,0.000102142,5.06E-08,8.38E-05,6.95E-08,1.80E-08,6.85E-07,1.02E-06,5.36E-06,9.77E-07,0.999618796,1.68E-08,0.000139347,4.77E-05
7342,machine-translation8,183,"More specifically , his alignment was restricted to predict the location such that the location increases monotonically .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,51,1,0,,3.59E-06,0,negative,7.29E-05,2.39E-08,4.02E-05,8.41E-09,9.34E-09,1.61E-07,3.67E-07,1.51E-06,5.41E-07,0.999666697,7.53E-09,0.000210828,6.72E-06
7343,machine-translation8,184,"The main difference from our approach is that , in , the modes of the weights of the annotations only move in one direction .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,52,1,0,,7.32E-06,0,negative,0.000347505,1.08E-07,9.62E-05,2.84E-08,2.56E-08,1.64E-07,7.14E-07,1.41E-06,6.84E-07,0.998449548,9.46E-09,0.001092981,1.06E-05
7344,machine-translation8,185,"In the context of machine translation , this is a severe limitation , as ( long - distance ) reordering is often needed to generate a grammatically correct translation ( for instance , English - to - German ) .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,53,1,0,,2.29E-05,0,negative,4.88E-05,1.82E-08,8.34E-06,1.06E-07,9.36E-09,6.18E-07,3.53E-06,3.02E-06,7.53E-08,0.99766207,4.05E-07,0.002028946,0.000244112
7345,machine-translation8,186,"Our approach , on the other hand , requires computing the annotation weight of every word in the source sentence for each word in the translation .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,54,1,0,,2.06E-05,0,negative,0.000198619,2.36E-07,5.19E-05,2.29E-07,5.52E-08,1.23E-06,4.17E-06,1.42E-05,9.72E-07,0.997857226,8.52E-08,0.001567992,0.000303036
7346,machine-translation8,187,This drawback is not severe with the task of translation in which most of input and output sentences are only 15 - 40 words .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,55,1,0,,1.88E-05,0,negative,7.16E-05,8.56E-09,6.39E-06,4.02E-08,6.65E-09,3.65E-07,2.40E-06,1.78E-06,4.66E-08,0.997814956,7.52E-08,0.001975905,0.000126421
7347,machine-translation8,188,"However , this may limit the applicability of the proposed scheme to other tasks .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,56,1,0,,1.50E-06,0,negative,1.37E-05,1.20E-09,1.52E-06,6.73E-10,6.19E-10,2.27E-08,7.19E-08,2.12E-07,1.94E-08,0.999805247,4.86E-10,0.000178513,7.43E-07
7348,machine-translation8,189,NEURAL NETWORKS FOR MACHINE TRANSLATION,QUANTITATIVE RESULTS,,machine-translation,8,57,1,0,,0.365054872,0,results,0.0005988,1.46E-06,0.000285926,1.92E-06,5.57E-07,1.92E-05,0.004512564,0.000137714,2.30E-06,0.357185947,1.94E-05,0.631517734,0.005716497
7349,machine-translation8,190,"Since introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a fixed number of the preceding words , neural networks have widely been used in machine translation .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,58,1,0,,0.000257527,0,negative,1.92E-05,5.50E-07,2.62E-05,3.41E-07,2.01E-07,1.34E-05,0.000302704,1.85E-05,4.22E-06,0.998800967,3.53E-06,0.000733577,7.66E-05
7350,machine-translation8,191,"However , the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,59,1,0,,0.000106434,0,negative,9.47E-06,1.37E-07,3.13E-06,8.26E-08,8.27E-08,3.88E-06,7.62E-05,5.83E-06,4.06E-07,0.999382665,5.93E-07,0.000507363,1.01E-05
7351,machine-translation8,192,"For instance , proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase - based statistical machine translation system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,60,1,0,,4.55E-06,0,negative,6.20E-06,6.14E-08,7.38E-06,3.47E-08,7.23E-08,2.41E-06,4.69E-05,3.70E-06,4.07E-07,0.999701606,9.47E-08,0.000224277,6.85E-06
7352,machine-translation8,193,"More recently , and reported the successful use of the neural networks as a sub-component of the existing translation system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,61,1,0,,4.72E-06,0,negative,4.59E-06,3.40E-08,2.61E-06,3.16E-08,5.60E-08,2.25E-06,3.41E-05,3.13E-06,3.31E-07,0.999833875,5.42E-08,0.000115657,3.31E-06
7353,machine-translation8,194,"Traditionally , a neural network trained as a target - side language model has been used to rescore or rerank a list of candidate translations ( see , e.g. , .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,62,1,0,,2.41E-05,0,negative,7.33E-06,1.46E-07,6.39E-06,4.90E-08,5.00E-08,3.08E-06,7.28E-05,6.50E-06,6.66E-07,0.999581387,3.12E-07,0.000310922,1.04E-05
7354,machine-translation8,195,"Although the above approaches were shown to improve the translation performance over the stateof - the - art machine translation systems , we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,63,1,0,,4.07E-05,0,negative,6.09E-06,1.55E-07,4.28E-06,4.85E-09,2.71E-08,5.30E-07,2.91E-05,1.65E-06,5.55E-07,0.999257289,1.35E-07,0.000697607,2.57E-06
7355,machine-translation8,196,The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works .,QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,64,1,0,,1.87E-05,0,negative,6.12E-05,3.23E-06,4.61E-05,2.11E-07,8.01E-07,5.33E-06,9.24E-05,1.11E-05,1.13E-05,0.998893228,1.81E-07,0.000842183,3.28E-05
7356,machine-translation8,197,"Rather than using a neural network as apart of the existing system , our model works on its own and generates a translation from a source sentence directly .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,65,1,0,,6.08E-05,0,negative,0.000193947,3.59E-06,0.000103824,2.26E-08,4.17E-07,9.55E-07,4.44E-05,3.53E-06,2.72E-05,0.998891246,8.44E-09,0.000726791,4.03E-06
7357,machine-translation8,198,CONCLUSION,,,machine-translation,8,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
7358,machine-translation8,214,A MODEL ARCHITECTURE,,,machine-translation,8,0,1,0,,0.003618465,0,negative,0.001162909,0.002335179,0.000698774,0.000624802,6.98E-05,0.001338065,0.000834304,0.004714687,0.013152506,0.947989404,0.026035656,0.000653301,0.000390648
7359,machine-translation8,215,A.1 ARCHITECTURAL CHOICES,A MODEL ARCHITECTURE,,machine-translation,8,1,1,0,,0.023831326,0,negative,0.000559587,0.004980869,5.88E-05,1.19E-05,6.11E-05,7.24E-05,2.81E-05,0.000198533,0.015880805,0.970686464,0.007322209,0.000135324,3.95E-06
7360,machine-translation8,216,"The proposed scheme in Section 3 is a general framework where one can freely define , for instance , the activation functions f of recurrent neural networks ( RNN ) and the alignment model a .",A MODEL ARCHITECTURE,A.1 ARCHITECTURAL CHOICES,machine-translation,8,2,1,0,,1.32E-05,0,negative,1.89E-05,0.001254225,3.86E-05,2.37E-07,1.43E-06,3.17E-06,6.00E-07,5.98E-05,0.001959665,0.99623804,0.000394814,3.05E-05,9.81E-08
7361,machine-translation8,217,"Here , we describe the choices we made for the experiments in this paper .",A MODEL ARCHITECTURE,A.1 ARCHITECTURAL CHOICES,machine-translation,8,3,1,0,,1.16E-06,0,negative,2.50E-06,2.33E-05,3.35E-07,5.58E-07,1.43E-06,4.36E-06,2.66E-07,3.51E-05,1.78E-05,0.999908991,2.94E-06,2.45E-06,1.19E-08
7362,machine-translation8,218,A.1.1 RECURRENT NEURAL NETWORK,A MODEL ARCHITECTURE,,machine-translation,8,4,1,0,,0.00654157,0,negative,0.000166052,0.002087665,8.50E-05,4.28E-06,1.68E-05,6.73E-05,3.45E-05,0.000129518,0.040297139,0.943019528,0.014029348,5.76E-05,5.15E-06
7363,machine-translation8,219,"For the activation function f of an RNN , we use the gated hidden unit recently proposed by .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,5,1,0,,0.000118745,0,negative,7.65E-06,0.003055718,3.82E-05,4.85E-07,3.35E-06,0.000136818,7.33E-06,0.00229525,0.021706885,0.972439983,0.000305292,2.57E-06,4.43E-07
7364,machine-translation8,220,The gated hidden unit is an alternative to the conventional simple units such as an element - wise tanh .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,6,1,0,,0.001262159,0,negative,0.000341018,0.00858104,0.006542758,2.51E-06,6.15E-05,4.23E-05,2.07E-05,0.000139632,0.108380832,0.874351327,0.001428227,0.00010518,2.97E-06
7365,machine-translation8,221,"This gated unit is similar to along short - term memory ( LSTM ) unit proposed earlier by , sharing with it the ability to better model and learn long - term dependencies .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,7,1,0,,0.00039646,0,negative,5.23E-05,0.003565889,0.000665525,2.81E-06,3.62E-05,4.14E-05,7.03E-06,0.000154422,0.074162243,0.920984171,0.000315499,1.06E-05,1.88E-06
7366,machine-translation8,222,This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1 .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,8,1,0,,5.67E-05,0,negative,0.0002081,0.001030716,2.17E-05,2.81E-07,1.26E-05,6.82E-06,1.21E-06,6.27E-05,0.001584189,0.997027432,1.29E-05,3.12E-05,6.48E-08
7367,machine-translation8,223,These paths allow gradients to flow backward easily without suffering too much from the vanishing effect .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,9,1,0,,5.12E-05,0,negative,3.31E-05,0.000576823,2.40E-05,1.02E-07,4.17E-06,3.29E-06,4.28E-07,2.09E-05,0.00698095,0.992329857,1.64E-05,1.00E-05,3.81E-08
7368,machine-translation8,224,"It is therefore possible to use LSTM units instead of the gated hidden unit described here , as was done in a similar context by .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,10,1,0,,7.82E-06,0,negative,4.35E-05,0.000103548,7.64E-06,3.91E-07,2.06E-06,6.87E-06,7.48E-07,3.83E-05,0.000693253,0.999074797,2.19E-05,6.92E-06,5.70E-08
7369,machine-translation8,225,The new state s i of the RNN employing n gated hidden units 8 is computed by,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,11,1,0,,1.06E-06,0,negative,8.05E-06,9.43E-05,2.00E-06,1.13E-07,1.35E-06,1.76E-06,2.87E-07,2.01E-05,0.000442233,0.999395902,2.98E-05,4.08E-06,2.60E-08
7370,machine-translation8,226,"where is an element - wise multiplication , and z i is the output of the update gates ( see below ) .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,12,1,0,,4.94E-07,0,negative,2.11E-06,3.03E-05,4.40E-07,7.39E-08,5.62E-07,2.12E-06,1.31E-07,1.81E-05,0.000200128,0.999739432,5.76E-06,8.36E-07,1.12E-08
7371,machine-translation8,227,The proposed updated states i is computed b ?,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,13,1,0,,1.25E-06,0,negative,2.34E-05,0.000237783,1.03E-05,2.32E-08,1.07E-06,1.99E-06,5.22E-07,2.67E-05,0.00103713,0.998622241,2.21E-05,1.67E-05,2.22E-08
7372,machine-translation8,228,where e ( y,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,14,1,0,,1.55E-07,0,negative,4.20E-06,1.09E-05,4.38E-07,1.74E-08,2.35E-07,6.28E-07,1.44E-07,7.00E-06,5.29E-05,0.999910842,9.45E-06,3.17E-06,6.38E-09
7373,machine-translation8,229,i?1 ) ?,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,15,1,0,,2.87E-06,0,negative,1.68E-05,8.02E-05,1.76E-06,1.44E-07,1.75E-06,5.76E-06,1.09E-06,5.15E-05,0.000320268,0.999487929,1.78E-05,1.50E-05,5.05E-08
7374,machine-translation8,230,"R m is an m-dimensional embedding of a wordy i ?1 , and r i is the output of the reset gates ( see below ) .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,16,1,0,,3.06E-07,0,negative,1.16E-06,4.70E-05,7.72E-07,4.39E-08,6.91E-07,2.21E-06,1.84E-07,2.36E-05,0.00021679,0.999700772,5.81E-06,9.79E-07,1.20E-08
7375,machine-translation8,231,"When y i is represented as a 1 - of - K vector , e ( y i ) is simply a column of an embedding matrix E ?",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,17,1,0,,8.63E-07,0,negative,3.06E-06,3.79E-05,1.38E-06,8.12E-09,2.93E-07,4.47E-07,7.04E-08,6.08E-06,0.000160101,0.999783947,4.34E-06,2.34E-06,4.16E-09
7376,machine-translation8,232,R mK .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,18,1,0,,4.54E-07,0,negative,4.00E-06,9.79E-06,7.54E-07,2.00E-08,2.79E-07,1.00E-06,2.12E-07,7.03E-06,8.90E-05,0.999878322,6.01E-06,3.59E-06,1.17E-08
7377,machine-translation8,233,"Whenever possible , we omit bias terms to make the equations less cluttered .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,19,1,0,,2.75E-06,0,negative,6.25E-05,0.001305148,8.66E-06,3.23E-07,7.35E-06,1.01E-05,1.08E-06,0.000193619,0.004151443,0.994248666,2.13E-06,8.96E-06,6.05E-08
7378,machine-translation8,234,"The update gates z i allow each hidden unit to maintain its previous activation , and the reset gates r i control how much and what information from the previous state should be reset .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,20,1,0,,1.61E-06,0,negative,5.71E-06,0.000646755,4.49E-06,1.83E-07,2.01E-06,8.41E-06,5.45E-07,0.000180342,0.010410989,0.988731513,7.73E-06,1.27E-06,6.42E-08
7379,machine-translation8,235,We compute them by,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,21,1,0,,3.03E-07,0,negative,4.26E-06,2.69E-05,4.60E-07,5.23E-08,8.87E-07,1.64E-06,1.72E-07,1.28E-05,0.000230601,0.99971879,1.62E-06,1.86E-06,1.28E-08
7380,machine-translation8,236,where ? ( ) is a logistic sigmoid function .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,22,1,0,,9.88E-07,0,negative,3.18E-06,0.000128011,9.38E-07,2.02E-07,1.10E-06,1.33E-05,6.98E-07,0.000298571,0.000855647,0.998685479,1.15E-05,1.29E-06,7.28E-08
7381,machine-translation8,237,"At each step of the decoder , we compute the output probability ( Eq. ( 4 ) ) as a multi -layered function .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,23,1,0,,2.49E-05,0,negative,2.13E-05,0.003687369,4.80E-05,4.41E-08,4.45E-06,2.04E-06,6.32E-07,3.09E-05,0.071612206,0.92457158,1.43E-05,7.03E-06,6.47E-08
7382,machine-translation8,238,We use a single hidden layer of maxout units and normalize the output probabilities ( one for each word ) with a softmax function ( see Eq. ) .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,24,1,0,,0.000153542,0,negative,0.000127293,0.011878881,6.73E-05,2.62E-05,9.95E-05,0.001195983,9.24E-05,0.02057814,0.063142587,0.902725412,3.66E-05,1.68E-05,1.31E-05
7383,machine-translation8,239,A.1.2 ALIGNMENT MODEL,,,machine-translation,8,0,1,0,,0.016879781,0,negative,0.000435165,0.000149301,0.000152114,8.46E-06,3.62E-06,0.000164007,0.000205129,0.000790619,0.000189092,0.987617907,0.009195914,0.001065525,2.31E-05
7384,machine-translation8,240,The alignment model should be designed considering that the model needs to be evaluated T x Ty times for each sentence pair of lengths T x and Ty .,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,1,1,0,,0.000132417,0,negative,0.00010153,0.000339789,2.29E-05,6.05E-07,2.56E-06,1.42E-05,2.65E-06,0.000128127,0.002685652,0.996299313,0.000391537,1.05E-05,6.36E-07
7385,machine-translation8,241,"In order to reduce computation , we use a singlelayer multilayer perceptron such that",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,2,1,0,,0.000428486,0,negative,0.000449678,0.003123833,0.00018885,4.77E-06,1.95E-05,0.000289257,3.58E-05,0.002601187,0.033603095,0.95924805,0.000402185,2.73E-05,6.53E-06
7386,machine-translation8,242,where W a ?,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,3,1,0,,4.22E-06,0,negative,4.50E-05,1.17E-05,4.49E-06,7.17E-08,4.02E-07,2.73E-06,6.39E-07,1.22E-05,6.18E-05,0.999816141,3.73E-05,7.49E-06,6.59E-08
7387,machine-translation8,243,"R nn , U a ?",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,4,1,0,,8.76E-05,0,negative,0.000188857,1.01E-05,2.36E-05,9.58E-07,4.50E-06,8.01E-06,3.52E-06,1.13E-05,4.35E-05,0.999623466,5.65E-05,2.53E-05,5.23E-07
7388,machine-translation8,244,R n 2n and v a ?,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,5,1,0,,0.000159046,0,negative,0.000770898,0.000106047,2.32E-05,5.38E-06,1.91E-05,6.46E-05,3.24E-05,0.000211777,0.000212418,0.997683944,0.000709666,0.000154369,6.16E-06
7389,machine-translation8,245,Rn are the weight matrices .,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,6,1,0,,0.00014227,0,negative,4.56E-05,0.000194222,1.84E-05,2.71E-07,1.80E-06,1.22E-05,2.18E-06,0.000150578,0.001970242,0.997550786,4.58E-05,7.63E-06,3.25E-07
7390,machine-translation8,246,Since,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,7,1,0,,6.80E-06,0,negative,9.23E-06,6.13E-06,1.12E-06,4.13E-08,2.19E-07,1.47E-06,3.25E-07,7.28E-06,0.000106263,0.999855031,1.10E-05,1.85E-06,4.02E-08
7391,machine-translation8,247,"U ah j does not depend on i , we can pre-compute it in advance to minimize the computational cost .",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,8,1,0,,5.65E-05,0,negative,0.000739184,8.22E-05,4.00E-05,1.04E-07,2.69E-06,1.67E-06,1.49E-06,1.05E-05,0.000239545,0.99876448,1.89E-05,9.93E-05,1.10E-07
7392,machine-translation8,248,A.2 DETAILED DESCRIPTION OF THE MODEL,A.1.2 ALIGNMENT MODEL,,machine-translation,8,9,1,0,,0.000616231,0,negative,0.000332871,0.000135157,8.17E-05,1.55E-06,7.29E-06,1.21E-05,6.70E-06,3.21E-05,0.002211223,0.996920648,0.00020525,5.09E-05,2.48E-06
7393,machine-translation8,249,A.2.1 ENCODER,A.1.2 ALIGNMENT MODEL,,machine-translation,8,10,1,0,,0.001028726,0,negative,0.001999527,0.00035211,0.001071886,6.56E-07,6.53E-06,1.06E-05,8.15E-06,2.87E-05,0.004508619,0.991680204,0.00015135,0.000179567,2.03E-06
7394,machine-translation8,250,"In this section , we describe in detail the architecture of the proposed model ( RNNsearch ) used in the experiments ( see .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,11,1,0,,2.78E-06,0,negative,3.16E-05,2.08E-05,1.97E-05,2.16E-07,6.38E-07,1.18E-06,2.97E-07,1.70E-05,0.000293679,0.999593712,3.75E-06,1.67E-05,6.33E-07
7395,machine-translation8,251,"From hereon , we omit all bias terms in order to increase readability .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,12,1,0,,4.53E-06,0,negative,8.26E-05,2.41E-05,1.92E-05,1.70E-07,3.60E-07,1.42E-06,3.54E-07,4.28E-05,0.000357926,0.999444481,1.18E-06,2.49E-05,4.74E-07
7396,machine-translation8,252,The model takes a source sentence of 1 - of - K coded word vectors as input,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,13,1,0,,2.83E-05,0,negative,1.14E-05,7.35E-05,0.000142756,3.01E-07,4.56E-07,4.63E-06,1.34E-06,0.000173047,0.00593571,0.993580208,5.47E-05,1.49E-05,7.00E-06
7397,machine-translation8,253,"and outputs a translated sentence of 1 - of - K coded word vectors y = ( y 1 , . . . , y Ty ) , y i ? R Ky ,",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,14,1,0,,2.53E-06,0,negative,7.98E-05,5.64E-06,0.000127419,3.77E-08,2.96E-07,4.39E-07,3.82E-07,6.20E-06,7.56E-05,0.999599952,1.73E-06,0.000102095,3.51E-07
7398,machine-translation8,254,"where K x and Ky are the vocabulary sizes of source and target languages , respectively .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,15,1,0,,9.19E-07,0,negative,2.38E-05,5.39E-06,9.10E-06,7.67E-08,1.67E-07,6.95E-07,2.16E-07,2.16E-05,5.96E-05,0.999862219,2.09E-06,1.46E-05,3.50E-07
7399,machine-translation8,255,T x and Ty respectively denote the lengths of source and target sentences .,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,16,1,0,,1.23E-06,0,negative,3.09E-06,3.92E-06,3.46E-06,3.18E-08,6.93E-08,6.00E-07,1.45E-07,1.97E-05,9.82E-05,0.999864146,1.79E-06,4.62E-06,2.66E-07
7400,machine-translation8,256,"First , the forward states of the bidirectional recurrent neural network ( BiRNN ) are computed :",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,17,1,0,,4.94E-06,0,negative,6.81E-05,8.60E-06,0.000237555,1.75E-08,1.39E-07,2.88E-07,1.34E-07,3.88E-06,0.00044992,0.999194227,1.28E-06,3.57E-05,2.13E-07
7401,machine-translation8,257,are weight matrices .,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,18,1,0,,4.72E-05,0,negative,3.02E-05,1.56E-05,2.15E-05,1.84E-07,2.92E-07,1.96E-06,4.77E-07,9.33E-05,0.000483705,0.999332004,2.06E-06,1.76E-05,1.10E-06
7402,machine-translation8,258,"m and n are the word embedding dimensionality and the number of hidden units , respectively . ? ( ) is as usual a logistic sigmoid function .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,19,1,0,,1.49E-05,0,negative,4.53E-05,5.17E-05,2.45E-05,1.10E-06,1.03E-06,1.51E-05,2.99E-06,0.000809831,0.000953906,0.998064911,6.36E-06,1.70E-05,6.24E-06
7403,machine-translation8,259,"The backward states ( ? ? h 1 , , ? ? h Tx ) are computed similarly .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,20,1,0,,8.41E-07,0,negative,1.68E-05,1.95E-06,2.46E-05,1.00E-08,5.15E-08,4.32E-07,1.65E-07,6.73E-06,0.000111036,0.99982698,2.25E-07,1.09E-05,1.29E-07
7404,machine-translation8,260,"We share the word embedding matrix E between the forward and backward RNNs , unlike the weight matrices .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,21,1,0,,1.47E-06,0,negative,9.13E-05,6.61E-05,0.000110922,3.65E-07,1.16E-06,3.28E-06,1.05E-06,0.000138016,0.002274014,0.997292205,1.16E-06,1.81E-05,2.32E-06
7405,machine-translation8,261,"We concatenate the forward and backward states to to obtain the annotations ( h 1 , h 2 , , h Tx ) , where",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,22,1,0,,1.46E-06,0,negative,2.09E-05,2.46E-06,2.31E-05,1.57E-08,1.21E-07,2.33E-07,1.03E-07,4.43E-06,0.000102874,0.9998327,1.48E-07,1.27E-05,1.55E-07
7406,machine-translation8,262,A.,A.1.2 ALIGNMENT MODEL,,machine-translation,8,23,1,0,,5.13E-06,0,negative,9.65E-05,1.35E-05,1.25E-05,3.11E-07,2.34E-06,4.89E-06,1.26E-06,1.40E-05,0.000189574,0.999656149,2.43E-06,6.17E-06,3.30E-07
7407,machine-translation8,263,DECODER,A.1.2 ALIGNMENT MODEL,,machine-translation,8,24,1,0,,0.001429398,0,negative,0.000748668,0.001485013,0.00097638,1.77E-05,9.87E-05,8.68E-05,7.59E-05,0.000260292,0.091818911,0.903746847,0.000418448,0.000177619,8.88E-05
7408,machine-translation8,264,The hidden state s i of the decoder given the annotations from the encoder is computed by,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,25,1,0,,5.10E-07,0,negative,1.99E-05,4.32E-06,1.84E-05,2.99E-08,1.21E-07,2.78E-07,1.63E-07,6.53E-06,0.000146435,0.999784744,3.91E-07,1.85E-05,1.90E-07
7409,machine-translation8,265,E is the word embedding matrix for the target language .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,26,1,0,,4.08E-06,0,negative,8.37E-06,9.34E-06,6.54E-06,8.57E-08,1.85E-07,1.22E-06,3.92E-07,8.05E-05,0.000286028,0.999599695,3.73E-07,6.62E-06,5.91E-07
7410,machine-translation8,266,"W , W z , W r ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,27,1,0,,1.50E-06,0,negative,4.02E-06,8.84E-07,4.35E-06,1.33E-08,2.97E-08,3.30E-07,1.71E-07,7.06E-06,3.04E-05,0.99994311,2.11E-07,9.32E-06,1.20E-07
7411,machine-translation8,267,"R nm , U , U z , Ur ? R nn , and C , C z , Cr ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,28,1,0,,2.24E-06,0,negative,1.78E-05,1.79E-06,6.60E-06,6.98E-08,2.12E-07,9.13E-07,6.03E-07,1.97E-05,1.81E-05,0.999887701,2.71E-07,4.59E-05,3.45E-07
7412,machine-translation8,268,R n 2n are weights .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,29,1,0,,4.87E-06,0,negative,1.22E-05,3.03E-06,9.50E-06,3.17E-08,9.56E-08,7.38E-07,4.03E-07,2.45E-05,9.01E-05,0.999843256,1.84E-07,1.57E-05,2.80E-07
7413,machine-translation8,269,"Again , m and n are the word embedding dimensionality and the number of hidden units , respectively .",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,30,1,0,,4.58E-06,0,negative,1.35E-05,5.79E-05,8.73E-06,2.56E-07,5.51E-07,5.78E-06,2.11E-06,0.000515568,0.000975175,0.998403203,1.39E-06,1.37E-05,2.14E-06
7414,machine-translation8,270,The initial hidden state s 0 is computed by,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,31,1,0,,1.29E-06,0,negative,1.63E-05,6.90E-06,8.16E-06,5.26E-08,1.85E-07,1.34E-06,7.02E-07,7.01E-05,0.000186949,0.999689586,2.37E-07,1.88E-05,5.95E-07
7415,machine-translation8,271,The context vector c i are recomputed at each step by the alignment model : :,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,32,1,0,,7.89E-06,0,negative,3.71E-05,6.76E-06,0.000141669,9.70E-09,1.14E-07,2.24E-07,2.02E-07,4.89E-06,0.000448983,0.999337138,1.09E-07,2.26E-05,1.52E-07
7416,machine-translation8,272,Learning statistics and relevant information .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,33,1,0,,6.79E-06,0,negative,6.61E-05,2.54E-05,2.91E-05,1.61E-06,1.28E-06,4.07E-06,5.85E-06,7.66E-05,0.000176832,0.999161893,6.07E-05,0.00035687,3.37E-05
7417,machine-translation8,273,Each update corresponds to updating the parameters once using a single minibatch .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,34,1,0,,4.28E-05,0,negative,6.15E-05,0.000100344,7.95E-05,1.28E-07,9.56E-07,2.29E-06,2.01E-06,0.000146688,0.002779232,0.996806361,4.16E-07,1.84E-05,2.19E-06
7418,machine-translation8,274,One epoch is one pass through the training set .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,35,1,0,,1.86E-05,0,negative,2.06E-05,2.53E-05,4.88E-05,1.09E-07,4.81E-07,3.24E-06,1.65E-06,0.000207441,0.00124519,0.998434358,2.92E-07,9.56E-06,2.94E-06
7419,machine-translation8,275,NLL is the average conditional log-probabilities of the sentences in either the training set or the development set .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,36,1,0,,1.30E-06,0,negative,7.63E-06,1.63E-05,0.000128161,3.94E-07,7.85E-07,3.65E-06,4.05E-06,6.74E-05,0.000141381,0.999509012,3.67E-05,6.87E-05,1.59E-05
7420,machine-translation8,276,Note that the lengths of the sentences differ .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,37,1,0,,1.27E-06,0,negative,5.06E-06,4.60E-07,1.81E-06,2.93E-08,6.49E-08,3.64E-07,1.39E-07,7.03E-06,1.26E-05,0.999961447,3.56E-08,1.08E-05,1.49E-07
7421,machine-translation8,277,where,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,38,1,0,,9.77E-08,0,negative,2.12E-06,3.55E-07,1.18E-06,9.71E-09,2.07E-08,1.90E-07,8.09E-08,5.02E-06,1.91E-05,0.999968585,2.18E-08,3.19E-06,7.46E-08
7422,machine-translation8,278,and h j is the j - th annotation in the source sentence ( see Eq. ) .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,39,1,0,,3.04E-07,0,negative,1.68E-06,4.96E-07,2.93E-06,5.37E-09,3.66E-08,9.32E-08,4.27E-08,2.87E-06,2.26E-05,0.999966603,1.78E-08,2.52E-06,6.92E-08
7423,machine-translation8,279,v a ?,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,40,1,0,,3.73E-06,0,negative,1.18E-05,1.13E-06,8.40E-06,6.01E-08,1.53E-07,7.64E-07,4.85E-07,1.31E-05,3.10E-05,0.999909123,1.34E-07,2.32E-05,7.05E-07
7424,machine-translation8,280,"Rn , W a ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,41,1,0,,7.70E-06,0,negative,9.95E-06,1.32E-06,1.02E-05,3.70E-08,1.06E-07,5.20E-07,4.70E-07,1.43E-05,3.19E-05,0.999903934,1.60E-07,2.65E-05,6.14E-07
7425,machine-translation8,281,Rn n and U a ?,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,42,1,0,,4.03E-06,0,negative,2.75E-05,3.04E-06,1.14E-05,2.78E-07,6.68E-07,2.58E-06,1.98E-06,6.46E-05,3.76E-05,0.999769713,3.39E-07,7.65E-05,3.90E-06
7426,machine-translation8,282,Rn 2n are weight matrices .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,43,1,0,,1.04E-05,0,negative,9.62E-06,7.50E-06,1.50E-05,2.35E-08,1.79E-07,9.58E-07,7.50E-07,5.57E-05,0.000203658,0.999688088,8.43E-08,1.78E-05,6.18E-07
7427,machine-translation8,283,Note that the model becomes RNN Encoder - Decoder,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,44,1,0,,6.28E-06,0,negative,2.27E-05,1.11E-05,9.94E-05,1.98E-07,5.58E-07,1.06E-06,8.33E-07,2.48E-05,0.003450191,0.996372089,3.20E-07,1.19E-05,4.89E-06
7428,machine-translation8,284,"With the decoder state s i ?1 , the context c i and the last generated wordy i ? 1 , we define the probability of a target wordy i as",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,45,1,0,,2.57E-07,0,negative,5.01E-06,1.27E-06,4.44E-06,1.49E-08,8.79E-08,2.15E-07,1.41E-07,7.62E-06,4.87E-05,0.999924348,2.76E-08,7.93E-06,1.83E-07
7429,machine-translation8,285,"where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,46,1,0,,6.74E-07,0,negative,6.76E-06,2.10E-06,7.75E-06,1.22E-08,1.25E-07,3.44E-07,3.23E-07,9.41E-06,4.61E-05,0.999909492,5.22E-08,1.72E-05,3.78E-07
7430,machine-translation8,286,and Co ?,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,47,1,0,,1.76E-05,0,negative,5.03E-05,1.29E-06,1.05E-05,4.01E-07,1.74E-06,2.24E-06,1.16E-06,2.17E-05,2.10E-05,0.999836778,4.93E-08,5.03E-05,2.51E-06
7431,machine-translation8,287,R 2 l 2n are weight matrices .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,48,1,0,,1.89E-05,0,negative,1.73E-05,9.79E-06,1.15E-05,7.86E-08,5.06E-07,2.80E-06,2.11E-06,0.000158556,0.000189396,0.999574802,6.48E-08,3.08E-05,2.19E-06
7432,machine-translation8,288,This can be understood as having a deep output with a single maxout hidden layer .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,49,1,0,,7.75E-06,0,negative,4.07E-05,7.15E-06,0.00028211,8.83E-08,5.76E-07,5.95E-07,4.22E-07,7.52E-06,0.00101401,0.998630112,7.27E-08,1.50E-05,1.66E-06
7433,machine-translation8,289,A.2.3 MODEL SIZE,,,machine-translation,8,0,1,0,,0.017092254,0,negative,0.001691578,0.000167168,8.95E-05,8.61E-06,3.61E-06,0.000121241,0.000162857,0.000949701,8.34E-05,0.990916533,0.003467236,0.002323257,1.53E-05
7434,machine-translation8,290,"For all the models used in this paper , the size of a hidden layer n is 1000 , the word embedding dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500 .",A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,1,1,0,,0.1998433,0,negative,0.002001439,0.004123364,0.000404771,0.000253357,0.000130698,0.047722025,0.001778806,0.328259678,0.004504102,0.608222744,0.001589488,0.00023211,0.000777418
7435,machine-translation8,291,The number of hidden units in the alignment model n is 1000 .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,2,1,0,,0.145908204,0,negative,0.001526576,0.002871598,0.000363864,6.93E-05,4.41E-05,0.022534993,0.000905914,0.263371758,0.007551587,0.697878852,0.002191615,0.000162973,0.00052693
7436,machine-translation8,292,and ? ?,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,3,1,0,,1.69E-05,0,negative,0.000115406,1.23E-06,4.46E-06,4.37E-07,1.32E-06,6.63E-06,7.87E-07,7.90E-06,4.19E-06,0.999837142,1.32E-05,6.96E-06,3.63E-07
7437,machine-translation8,293,Ur as random orthogonal matrices .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,4,1,0,,0.00026457,0,negative,0.000238604,2.08E-05,0.000143454,2.78E-07,1.09E-06,1.50E-05,2.40E-06,6.45E-05,0.000154237,0.999293492,2.92E-05,3.61E-05,8.33E-07
7438,machine-translation8,294,"For W a and U a , we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.001 2 .",A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,5,1,0,,0.001481448,0,negative,0.001065279,0.000779178,0.000113856,2.13E-05,3.89E-05,0.009270663,0.000252346,0.054374268,0.001631738,0.932183863,0.000127154,7.98E-05,6.17E-05
7439,machine-translation8,295,All the elements of Va and all the bias vectors were initialized to zero .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,6,1,0,,0.02009936,0,negative,0.001155014,0.001565852,0.000196929,1.22E-05,2.11E-05,0.005476725,0.000190547,0.057319214,0.00635995,0.927283288,0.000275889,7.81E-05,6.51E-05
7440,machine-translation8,296,Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.01 2 .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,7,1,0,,0.011425618,0,negative,0.001277973,0.001947918,0.000262424,2.58E-05,3.25E-05,0.010216624,0.000355672,0.096487718,0.006465361,0.882427908,0.000304299,8.45E-05,0.000111328
7441,machine-translation8,297,B.2 TRAINING,,,machine-translation,8,0,1,0,,0.037400378,0,negative,0.000660774,0.002069719,0.001268101,5.49E-07,3.33E-06,8.76E-05,8.70E-05,0.002035628,0.00065432,0.988558391,0.002038141,0.002530396,6.03E-06
7442,machine-translation8,298,We used the stochastic gradient descent ( SGD ) algorithm .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,1,1,0,,0.010200541,0,negative,0.002572394,0.000176109,0.003034618,0.000202421,1.04E-05,0.014578882,0.000630039,0.077778636,0.001225054,0.888799497,6.93E-05,0.00061836,0.010304288
7443,machine-translation8,299,Adadelta was used to automatically adapt the learning rate of each parameter ( = 10 ?6 and ? = 0.95 ) .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,2,1,0,,0.009142106,0,negative,0.00294852,0.000183089,0.000386442,0.000253112,1.04E-05,0.029504086,0.000911207,0.358573976,0.001061113,0.593465315,6.06E-05,0.000671313,0.011970797
7444,machine-translation8,300,"We explicitly normalized the L 2 - norm of the gradient of the cost function each time to beat most a predefined threshold of 1 , when the norm was larger than the threshold .",B.2 TRAINING,B.2 TRAINING,machine-translation,8,3,1,0,,0.000329955,0,negative,0.002298196,6.58E-05,0.000169885,9.79E-06,3.33E-06,0.001621964,7.40E-05,0.028415066,0.000372741,0.966074212,6.06E-06,0.000386701,0.000502316
7445,machine-translation8,301,Each SGD update direction was computed with a minibatch of 80 sentences .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,4,1,0,,0.000640789,0,negative,0.001139709,0.000147784,0.000288224,2.66E-05,5.31E-06,0.008448979,0.000591051,0.145764725,0.000504093,0.836874232,3.33E-05,0.000643079,0.005532954
7446,machine-translation8,302,At each update our implementation requires time proportional to the length of the longest sentence in a minibatch .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,5,1,0,,4.86E-05,0,negative,0.004759079,9.21E-05,0.000930687,9.56E-06,5.33E-06,0.000304536,7.76E-05,0.002971953,0.000743422,0.984363787,5.94E-05,0.00324905,0.002433554
7447,machine-translation8,303,"Hence , to minimize the waste of computation , before every 20 - th update , we retrieved 1600 sentence pairs , sorted them according to the lengths and split them into 20 minibatches .",B.2 TRAINING,B.2 TRAINING,machine-translation,8,6,1,0,,0.000272689,0,negative,0.002376698,5.35E-05,0.000236961,3.33E-05,3.08E-05,0.00070116,0.00013934,0.003465334,9.91E-05,0.989335134,6.00E-06,0.00111031,0.002412395
7448,machine-translation8,304,The training data was shuffled once before training and was traversed sequentially in this manner .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,7,1,0,,1.34E-05,0,negative,0.000909581,1.09E-05,0.000253805,2.43E-06,1.94E-06,0.000111536,1.48E-05,0.000937867,7.47E-05,0.997314905,1.12E-06,0.000235674,0.000130807
7449,machine-translation8,305,In Tables 2 we present the statistics related to training all the models used in the experiments .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,8,1,0,,9.88E-06,0,negative,0.000146785,5.56E-07,4.65E-06,2.05E-07,2.29E-07,1.04E-05,3.15E-06,7.76E-05,1.55E-06,0.998902078,4.84E-07,0.00083615,1.62E-05
7450,machine-translation8,306,C TRANSLATIONS OF LONG SENTENCES,B.2 TRAINING,,machine-translation,8,9,1,0,,0.000307349,0,negative,0.001843246,2.22E-05,0.001168154,2.92E-06,7.93E-07,3.34E-05,0.000390116,0.000215194,2.63E-05,0.708706338,0.006097646,0.273765265,0.007728423
7451,machine-translation8,307,Source,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,10,1,0,,2.54E-07,0,negative,4.72E-05,1.72E-07,1.49E-05,5.79E-07,2.05E-08,6.52E-06,2.81E-06,5.63E-05,6.36E-06,0.999461723,5.06E-07,0.000314062,8.89E-05
7452,machine-translation8,308,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,11,1,0,,7.97E-06,0,negative,5.73E-05,3.31E-07,4.55E-05,2.30E-06,6.74E-08,7.12E-06,4.52E-06,3.35E-05,7.26E-06,0.997872896,4.98E-06,0.001414504,0.000549742
7453,machine-translation8,309,Reference,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,12,1,0,,1.24E-07,0,negative,1.18E-05,2.74E-08,4.86E-06,2.91E-08,2.46E-09,8.46E-07,7.78E-07,7.87E-06,2.03E-06,0.999782787,1.33E-07,0.000178278,1.05E-05
7454,machine-translation8,310,"Le privilge d'admission est le droit d'un mdecin , en vertu de son statut de membre soignant d'un hpital , d'admettre un patient dans un hpital ou un centre mdical afin d 'y dlivrer un diagnostic ou un traitement .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,13,1,0,,1.34E-05,0,negative,0.000594365,7.70E-07,0.00013229,2.82E-05,3.72E-07,1.82E-05,1.47E-05,3.46E-05,1.74E-05,0.991559642,6.39E-06,0.004208853,0.003384375
7455,machine-translation8,311,RNNenc - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,14,1,0,,8.81E-06,0,negative,0.00026079,9.87E-08,0.000242791,5.37E-07,6.98E-08,2.33E-05,9.44E-05,5.57E-05,1.86E-06,0.971665075,1.01E-06,0.025740908,0.001913416
7456,machine-translation8,312,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,15,1,0,,4.41E-06,0,negative,0.000728907,4.35E-07,0.000234155,1.02E-06,1.49E-07,4.19E-06,1.10E-05,9.54E-06,4.52E-06,0.981619045,4.36E-06,0.016794178,0.000588559
7457,machine-translation8,313,RNNsearch - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,16,1,0,,9.04E-06,0,negative,0.000326365,7.67E-08,0.000307412,2.91E-07,6.04E-08,1.19E-05,4.74E-05,3.17E-05,1.58E-06,0.985368498,3.00E-07,0.013263006,0.000641477
7458,machine-translation8,314,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,17,1,0,,4.50E-06,0,negative,0.000346679,3.46E-07,6.46E-05,3.04E-06,1.67E-07,6.27E-06,9.10E-06,1.46E-05,5.00E-06,0.993575209,4.71E-06,0.004754878,0.001215399
7459,machine-translation8,315,Google Translate,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,18,1,0,,0.077002718,0,negative,0.001974216,1.11E-06,0.000330973,0.000158451,2.19E-06,0.000328146,0.000762824,0.000516813,1.63E-05,0.870485537,3.70E-06,0.049139581,0.076280142
7460,machine-translation8,316,"Ce genre d'exprience fait partie des efforts de Disney "" tendre la dure de vie de sa srie et construire de nouvelles relations avec le public par le biais des plates - formes numriques qui deviennent de plus en plus important "" , at - il ajout .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,19,1,0,,3.51E-06,0,negative,0.000379739,2.88E-07,5.08E-05,5.57E-06,2.25E-07,6.68E-06,7.60E-06,1.35E-05,4.75E-06,0.994869235,2.81E-06,0.003247965,0.001410821
7461,machine-translation8,317,Source,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,20,1,0,,2.27E-07,0,negative,3.90E-05,1.16E-07,1.01E-05,3.64E-07,1.83E-08,4.30E-06,2.37E-06,4.62E-05,4.36E-06,0.99954283,1.49E-07,0.000257866,9.24E-05
7462,machine-translation8,318,"In a press conference on Thursday , Mr Blair stated that there was nothing in this video that might constitute a "" reasonable motive "" that could lead to criminal charges being brought against the mayor .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,21,1,0,,1.02E-06,0,negative,3.85E-05,2.60E-08,6.47E-06,5.75E-08,1.58E-08,8.36E-07,1.10E-06,3.82E-06,5.57E-07,0.999010939,1.36E-07,0.000886802,5.08E-05
7463,machine-translation8,319,Reference,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,22,1,0,,1.18E-07,0,negative,9.76E-06,1.82E-08,3.56E-06,1.79E-08,2.23E-09,5.47E-07,6.84E-07,6.12E-06,1.48E-06,0.999808443,4.05E-08,0.000158482,1.08E-05
7464,machine-translation8,320,"En confrence de presse , jeudi , M. Blair a affirm qu'il n'y avait rien dans cette vido qui puisse constituer des "" motifs raisonnables "" pouvant mener au dpt d'une accusation criminelle contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,23,1,0,,1.19E-06,0,negative,8.08E-05,8.71E-08,9.37E-06,4.09E-07,3.39E-08,1.67E-06,1.56E-06,7.63E-06,1.82E-06,0.998839458,3.50E-07,0.000883935,0.000172904
7465,machine-translation8,321,RNNenc - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,24,1,0,,7.02E-06,0,negative,0.000223144,6.54E-08,0.000148215,3.35E-07,6.50E-08,1.39E-05,7.44E-05,4.02E-05,1.41E-06,0.976938888,2.79E-07,0.020449341,0.002109697
7466,machine-translation8,322,"Lors de la confrence de presse de jeudi , M. Blair a dit qu'il n'y avait rien dans cette vido qui pourrait constituer une "" motivation raisonnable "" pouvant entraner des accusations criminelles portes contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,25,1,0,,8.56E-07,0,negative,9.84E-05,1.13E-07,1.79E-05,4.52E-07,4.65E-08,1.65E-06,1.67E-06,6.65E-06,2.29E-06,0.998586914,4.05E-07,0.001067178,0.00021637
7467,machine-translation8,323,RNNsearch - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,26,1,0,,7.09E-06,0,negative,0.000265319,4.92E-08,0.000176502,1.76E-07,5.36E-08,6.93E-06,3.66E-05,2.30E-05,1.16E-06,0.988893262,8.34E-08,0.009919374,0.000677532
7468,machine-translation8,324,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait conduire des accusations criminelles contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,27,1,0,,7.54E-07,0,negative,0.000110151,1.13E-07,2.27E-05,4.78E-07,5.55E-08,1.71E-06,1.96E-06,6.39E-06,2.18E-06,0.998331476,3.67E-07,0.001244033,0.000278412
7469,machine-translation8,325,Google Translate,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,28,1,0,,0.053815514,0,negative,0.001563037,7.30E-07,0.00022044,9.76E-05,2.01E-06,0.00022193,0.000619943,0.000472557,9.93E-06,0.870936375,1.01E-06,0.041120262,0.084734189
7470,machine-translation8,326,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait mener des accusations criminelles portes contre le maire . :",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,29,1,0,,4.55E-07,0,negative,0.000187267,6.51E-08,2.30E-05,7.68E-07,6.12E-08,2.38E-06,2.78E-06,6.11E-06,1.71E-06,0.997825662,2.04E-07,0.001532224,0.000417816
7471,machine-translation8,327,The translations generated by RNNenc - 50 and RNNsearch - 50 from long source sentences ( 30 words or more ) selected from the test set .,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,30,1,0,,6.51E-06,0,negative,1.40E-05,1.34E-08,1.11E-05,9.93E-09,2.39E-08,8.73E-07,4.35E-06,6.31E-06,8.36E-08,0.998328012,6.64E-09,0.001608203,2.71E-05
7472,machine-translation8,328,"For each source sentence , we also show the goldstandard translation .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,31,1,0,,9.61E-07,0,negative,1.18E-05,1.43E-08,3.02E-06,1.33E-08,1.13E-08,4.13E-07,9.37E-07,5.50E-06,2.28E-07,0.999420853,3.18E-09,0.000546329,1.08E-05
7473,machine-translation8,329,The translations by Google Translate were made on 27 August 2014 .,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,32,1,0,,8.88E-05,0,negative,0.000188485,4.46E-08,1.23E-05,1.45E-05,5.95E-07,1.77E-05,7.56E-06,1.83E-05,1.03E-06,0.998097354,3.30E-08,0.000676926,0.000965319
7474,machine-translation8,330,Reference,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,33,1,0,,1.08E-07,0,negative,7.65E-06,1.14E-08,2.63E-06,1.03E-08,1.89E-09,3.43E-07,5.92E-07,4.61E-06,1.00E-06,0.999830909,1.10E-08,0.000140744,1.15E-05
7475,machine-translation8,331,"Ce type d'exprience entre dans le cadre des efforts de Disney pour "" tendre la dure de vie de ses sries et construire de nouvelles relations avec son public grce des plateformes numriques qui so nt de plus en plus importantes "" , a-t - il ajout .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,34,1,0,,7.30E-07,0,negative,6.98E-05,1.22E-07,1.66E-05,5.57E-07,4.19E-08,2.18E-06,3.27E-06,1.18E-05,1.54E-06,0.997515264,8.21E-07,0.001709049,0.000668957
7476,machine-translation6,1,title,,,machine-translation,6,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
7477,machine-translation6,2,FRAGE : Frequency - Agnostic Word Representation,title,title,machine-translation,6,1,1,1,research-problem,0.930486759,1,research-problem,2.73E-08,1.43E-05,8.51E-08,3.53E-08,2.10E-08,2.01E-07,3.75E-07,3.17E-06,9.14E-06,0.006222097,0.993750402,6.88E-08,2.65E-08
7478,machine-translation6,3,abstract,,,machine-translation,6,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
7479,machine-translation6,4,Continuous word representation ( aka word embedding ) is a basic building block in many neural network - based models used in natural language processing tasks .,abstract,abstract,machine-translation,6,1,1,0,,0.362317548,0,research-problem,4.03E-08,1.69E-05,1.24E-07,5.11E-07,1.72E-07,2.92E-07,5.29E-07,2.09E-06,2.09E-06,0.006595611,0.993381541,3.72E-08,7.29E-08
7480,machine-translation6,5,"Although it is widely accepted that words with similar semantics should be close to each other in the embedding space , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of highfrequency and low - frequency words lie in different subregions of the embedding space , and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar .",abstract,abstract,machine-translation,6,2,1,1,research-problem,0.229561029,0,negative,5.49E-05,0.014700189,3.72E-06,5.57E-06,4.84E-05,1.54E-05,7.60E-06,0.000366105,0.000456173,0.691080965,0.293243813,1.65E-05,7.24E-07
7481,machine-translation6,6,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",abstract,abstract,machine-translation,6,3,1,0,,0.008619992,0,negative,3.85E-06,0.000763039,7.63E-07,5.64E-06,1.60E-05,5.28E-06,9.33E-07,3.78E-05,7.68E-05,0.879840749,0.11924839,5.89E-07,1.41E-07
7482,machine-translation6,7,"In this paper , we develop a neat , simple yet effective way to learn FRequency - AGnostic word Embedding ( FRAGE ) using adversarial training .",abstract,abstract,machine-translation,6,4,1,0,,0.781278922,1,research-problem,1.20E-05,0.088372009,3.87E-05,2.49E-05,0.000122866,1.84E-05,1.68E-05,0.000325211,0.002287806,0.076531165,0.832239053,7.22E-06,3.83E-06
7483,machine-translation6,8,"We conducted comprehensive studies on ten datasets across four natural language processing tasks , including word similarity , language modeling , machine translation and text classification .",abstract,abstract,machine-translation,6,5,1,0,,0.019597216,0,negative,1.32E-06,0.003664076,1.99E-06,0.000124041,0.003540226,4.35E-05,1.28E-05,0.000110625,4.02E-05,0.811458758,0.181000756,8.05E-07,1.02E-06
7484,machine-translation6,9,"Results show that with FRAGE , we achieve higher performance than the baselines in all tasks .",abstract,abstract,machine-translation,6,6,1,0,,0.051137921,0,negative,0.000301154,0.004832763,8.50E-06,1.61E-05,0.000136538,3.68E-05,0.000235715,0.000803962,0.000102432,0.812906561,0.17908931,0.001522767,7.37E-06
7485,machine-translation6,10,Introduction,,,machine-translation,6,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
7486,machine-translation6,11,"Word embeddings , which are distributed and continuous vector representations for word tokens , have been one of the basic building blocks for many neural network - based models used in natural language processing ( NLP ) tasks , such as language modeling , text classification and machine translation .",Introduction,Introduction,machine-translation,6,1,1,0,,0.24726673,0,research-problem,1.12E-06,0.000592785,8.64E-07,6.10E-06,7.84E-06,2.39E-05,1.53E-05,3.99E-05,0.000808794,0.076798415,0.921701416,1.21E-06,2.34E-06
7487,machine-translation6,12,"Different from classic one - hot representation , the learned word embeddings contain semantic information which can measure the semantic similarity between words , and can also be transferred into other learning tasks .",Introduction,Introduction,machine-translation,6,2,1,0,,0.903315552,1,model,0.000170606,0.348278567,7.73E-05,7.99E-06,0.000814075,4.05E-05,2.72E-05,0.00013339,0.470619973,0.16179576,0.017998912,3.25E-05,3.30E-06
7488,machine-translation6,13,"In deep learning approaches for NLP tasks , word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters .",Introduction,Introduction,machine-translation,6,3,1,0,,0.634403539,1,research-problem,3.64E-06,0.004555351,2.05E-06,2.38E-05,5.04E-05,7.16E-05,2.16E-05,0.000154434,0.003105204,0.249695046,0.742309695,2.62E-06,4.51E-06
7489,machine-translation6,14,"As the inputs of the neural network , word embeddings carryall the information of words that will be further processed by the network , and the quality of embeddings is critical and highly impacts the final performance of the learning task .",Introduction,Introduction,machine-translation,6,4,1,0,,0.915180656,1,negative,4.83E-05,0.130234729,2.29E-05,1.91E-05,0.000902914,0.000102763,2.33E-05,0.000283255,0.239857671,0.572285721,0.056202211,1.33E-05,3.90E-06
7490,machine-translation6,15,"Unfortunately , we find the word embeddings learned by many deep learning approaches are far from perfect .",Introduction,Introduction,machine-translation,6,5,1,0,,0.01318895,0,negative,3.99E-06,0.001461394,8.72E-07,4.16E-05,8.36E-05,9.82E-05,3.02E-05,0.000138458,0.000561245,0.542956053,0.454616292,3.85E-06,4.22E-06
7491,machine-translation6,16,"As shown in ( a ) and 1 ( b ) , in the embedding space learned by word2 vec model , the nearest neighbors of word "" Peking "" includes "" quickest "" , "" multicellular "" , and "" epigenetic "" , which are not semantically similar , while semantically related words such as "" Beijing "" and "" China "" are far from it .",Introduction,Introduction,machine-translation,6,6,1,0,,0.161582967,0,negative,0.000396869,0.059459938,4.50E-05,3.79E-06,0.000919602,9.51E-05,0.000151332,0.000228405,0.033058857,0.88958789,0.015863428,0.000186682,3.09E-06
7492,machine-translation6,17,Similar phenomena are observed from the word embeddings learned from translation tasks .,Introduction,Introduction,machine-translation,6,7,1,0,,0.00385553,0,negative,3.62E-06,0.001269974,8.46E-07,6.57E-06,2.03E-05,5.04E-05,2.69E-05,9.82E-05,0.001336946,0.573410266,0.423768989,4.96E-06,2.12E-06
7493,machine-translation6,18,"With a careful study , we find a more general problem which is rooted in low - frequency words in the text corpus .",Introduction,Introduction,machine-translation,6,8,1,0,,0.005028816,0,negative,4.76E-05,0.003916129,2.61E-06,0.000135181,0.001081233,0.000201933,3.74E-05,0.000116767,0.002052295,0.96760603,0.024790341,8.68E-06,3.84E-06
7494,machine-translation6,19,"Without any confusion , we also call high - frequency words as popular words and call low - frequency words as rare words .",Introduction,Introduction,machine-translation,6,9,1,0,,0.012677042,0,negative,4.11E-06,0.024611112,4.06E-06,3.51E-06,0.000186048,0.000352355,2.69E-05,0.000739705,0.108938121,0.861494371,0.003635391,2.94E-06,1.34E-06
7495,machine-translation6,20,"As is well known , the frequency distribution of words roughly follows a simple mathematical form known as Zipf 's law .",Introduction,Introduction,machine-translation,6,10,1,0,,0.050785861,0,negative,4.75E-06,0.010584274,3.08E-06,6.72E-06,6.73E-05,0.000136453,4.22E-05,0.000357055,0.028000744,0.717801536,0.242988038,4.48E-06,3.31E-06
7496,machine-translation6,21,"When the size of a text corpus grows , the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words .",Introduction,Introduction,machine-translation,6,11,1,0,,0.129047311,0,negative,4.48E-05,0.010043354,3.31E-06,5.03E-05,0.00036925,0.000192652,9.26E-05,0.000525202,0.005529296,0.797711921,0.185406016,2.04E-05,1.09E-05
7497,machine-translation6,22,"Interestingly , the learned embeddings of rare words and popular words behave differently .",Introduction,Introduction,machine-translation,6,12,1,1,research-problem,0.391698746,0,negative,0.000361209,0.015431851,6.88E-06,0.000338107,0.001593509,0.001206607,0.000679022,0.002548477,0.004545488,0.955772235,0.017315081,0.000167736,3.38E-05
7498,machine-translation6,23,"In the embedding space , a popular word usually has semantically related neighbors , while a rare word usually does not .",Introduction,Introduction,machine-translation,6,13,1,0,,0.05197159,0,model,5.77E-05,0.23336672,4.61E-05,1.60E-06,0.000439057,5.20E-05,2.66E-05,0.000217131,0.401844537,0.360386011,0.003543054,1.82E-05,1.26E-06
7499,machine-translation6,24,"Moreover , the nearest neighbors of more than 85 % rare words are rare words .",Introduction,Introduction,machine-translation,6,14,1,0,,0.026245742,0,negative,0.000215619,0.011898906,4.05E-05,0.000266276,0.069876735,0.000264148,0.000152483,9.78E-05,0.001207968,0.913350448,0.00256991,5.04E-05,8.76E-06
7500,machine-translation6,25,Word embeddings encode frequency information .,Introduction,Introduction,machine-translation,6,15,1,0,,0.298542424,0,model,1.22E-05,0.023956447,5.55E-05,9.80E-07,5.97E-05,5.15E-05,1.53E-05,9.09E-05,0.940238547,0.035010215,0.000505473,2.04E-06,1.30E-06
7501,machine-translation6,26,"As shown in ( a ) and 1 ( b ) , the embeddings of rare words and popular words actually lie in different subregions of the space .",Introduction,Introduction,machine-translation,6,16,1,1,research-problem,0.571803514,1,negative,0.000493411,0.118013002,2.67E-05,1.41E-05,0.001001494,0.000201618,0.000157439,0.000593466,0.075612387,0.800208508,0.003567321,0.000105901,4.66E-06
7502,machine-translation6,27,Such a phenomenon is also observed in .,Introduction,Introduction,machine-translation,6,17,1,0,,0.005206894,0,negative,2.54E-05,0.001522854,4.46E-06,1.25E-05,0.000395874,8.89E-05,3.32E-05,6.48E-05,0.001657292,0.993565681,0.002615887,1.19E-05,1.29E-06
7503,machine-translation6,28,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,Introduction,Introduction,machine-translation,6,18,1,1,research-problem,0.003507238,0,negative,1.55E-05,0.003895379,1.49E-06,3.52E-05,0.000227325,0.000215249,3.19E-05,0.000353926,0.004279467,0.981999365,0.008936586,5.76E-06,2.83E-06
7504,machine-translation6,29,"First , such embeddings will affect the semantic understanding of words .",Introduction,Introduction,machine-translation,6,19,1,0,,0.035451801,0,negative,0.000310307,0.014652137,3.01E-05,2.84E-05,0.00206108,8.91E-05,3.62E-05,7.68E-05,0.011724957,0.969928017,0.00103059,3.04E-05,1.88E-06
7505,machine-translation6,30,We observe more than half of the rare words are nouns or variants of popular words .,Introduction,Introduction,machine-translation,6,20,1,0,,0.063870956,0,negative,0.000101084,0.004553433,1.49E-05,0.011836557,0.422238284,0.001993261,0.000346196,0.000254916,0.000307324,0.5579011,0.000403364,1.49E-05,3.47E-05
7506,machine-translation6,31,Those rare words should have similar meanings or share the same topics with popular words .,Introduction,Introduction,machine-translation,6,21,1,0,,0.002050609,0,negative,5.18E-05,0.046225282,2.20E-05,5.57E-05,0.004343377,0.000406995,6.12E-05,0.000707186,0.048402764,0.898906007,0.000797752,1.42E-05,5.79E-06
7507,machine-translation6,32,"Second , the neighbors of a large number of rare words are semantically unrelated rare words .",Introduction,Introduction,machine-translation,6,22,1,0,,0.006398769,0,negative,0.000102041,0.020702038,3.38E-05,3.15E-05,0.003273047,0.000105994,5.99E-05,8.36E-05,0.004338991,0.965258382,0.005975897,3.12E-05,3.64E-06
7508,machine-translation6,33,"To some extent , those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding .",Introduction,Introduction,machine-translation,6,23,1,0,,0.009338019,0,negative,8.77E-05,0.004651329,9.14E-06,5.50E-06,0.000485343,4.77E-05,4.66E-05,4.74E-05,0.002064574,0.988842104,0.00367119,4.01E-05,1.38E-06
7509,machine-translation6,34,It will consequently limit the performance of down - stream tasks using the embeddings .,Introduction,Introduction,machine-translation,6,24,1,0,,0.060605318,0,negative,0.000156473,0.009045932,1.41E-05,1.04E-05,0.000556136,7.27E-05,3.67E-05,7.83E-05,0.009428125,0.979472439,0.001096477,3.06E-05,1.60E-06
7510,machine-translation6,35,"For example , in text classification , it can not be well guaranteed that the label of a sentence does not change when you replace one popular / rare word in the sentence by its rare / popular alternatives .",Introduction,Introduction,machine-translation,6,25,1,0,,0.003791843,0,negative,2.66E-06,0.0019241,1.28E-06,5.36E-05,0.000188838,0.00013148,6.20E-05,0.000162312,0.00067164,0.836933976,0.159855615,5.18E-06,7.29E-06
7511,machine-translation6,36,"To address this problem , in this paper , we propose an adversarial training method to learn FRequency - AGnostic word Embedding ( FRAGE ) .",Introduction,Introduction,machine-translation,6,26,1,1,approach,0.964600046,1,approach,8.23E-05,0.762016612,0.000223452,2.17E-05,0.002205766,8.51E-05,0.00018155,0.000233877,0.212777049,0.019676405,0.002443709,3.29E-05,1.95E-05
7512,machine-translation6,37,"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",Introduction,Introduction,machine-translation,6,27,1,1,approach,0.932427536,1,model,1.62E-05,0.188932513,3.95E-05,8.83E-07,0.00024958,1.46E-05,1.28E-05,4.12E-05,0.803456088,0.007175523,5.79E-05,2.17E-06,1.07E-06
7513,machine-translation6,38,"The discriminator optimizes its parameters to maximize its classification accuracy , while word embeddings are optimized towards a low task - dependent loss as well as fooling the discriminator to mis-classify the popular and rare words .",Introduction,Introduction,machine-translation,6,28,1,1,approach,0.888440769,1,model,8.36E-06,0.185355501,1.36E-05,1.17E-06,0.000207058,4.27E-05,1.88E-05,0.00027741,0.798935218,0.015076874,6.08E-05,1.22E-06,1.28E-06
7514,machine-translation6,39,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .",Introduction,Introduction,machine-translation,6,29,1,1,approach,0.066705132,0,negative,0.00055189,0.043649382,2.19E-05,1.27E-05,0.000817968,0.0001467,0.000313843,0.000537762,0.024360299,0.926979449,0.002440801,0.000158155,9.21E-06
7515,machine-translation6,40,"Consequently , rare words lie in the same region as and are mixed with popular words in the embedding space .",Introduction,Introduction,machine-translation,6,30,1,0,,0.076416349,0,model,7.21E-05,0.087888065,5.34E-05,1.10E-06,0.0004352,4.62E-05,2.57E-05,9.95E-05,0.533541076,0.377688194,0.000138683,9.85E-06,8.13E-07
7516,machine-translation6,41,Then FRAGE will catch better semantic information and help the task - specific model to perform better .,Introduction,Introduction,machine-translation,6,31,1,0,,0.008733849,0,negative,0.000127255,0.021151767,4.70E-05,2.17E-05,0.003073587,0.000170563,0.000135412,0.000145332,0.030222904,0.944607699,0.000243093,4.90E-05,4.66E-06
7517,machine-translation6,42,"We conduct experiments on four types of NLP tasks , including three word similarity tasks , two language modeling tasks , three sentiment classification tasks and two machine translation tasks to test our method .",Introduction,Introduction,machine-translation,6,32,1,0,,0.069082487,0,approach,4.96E-05,0.548007875,0.000112084,0.000107974,0.150803419,0.000655702,0.001364361,0.001045603,0.009121224,0.288258511,0.000317418,0.000111085,4.51E-05
7518,machine-translation6,43,"In all tasks , FRAGE outperforms the baselines .",Introduction,Introduction,machine-translation,6,33,1,0,,0.379029156,0,negative,0.003431022,0.047197307,0.000109617,0.000100608,0.003697892,0.001523269,0.095378863,0.00372038,0.00489608,0.813795452,0.003391624,0.021999968,0.000757918
7519,machine-translation6,44,"Specifically , in language modeling and machine translation , we achieve better performance than the state - of - the - art results on PTB , WT2 and WMT14 English - German datasets .",Introduction,Introduction,machine-translation,6,34,1,0,,0.021967329,0,negative,0.001666364,0.042765718,0.000101859,6.51E-05,0.003790101,0.000651805,0.064423812,0.00154422,0.003171666,0.836189907,0.004793639,0.040299932,0.00053585
7520,machine-translation6,45,Background,,,machine-translation,6,0,1,0,,6.12E-05,0,negative,4.94E-05,0.000177806,6.95E-06,6.36E-05,6.50E-06,0.000188754,4.78E-05,0.001033369,0.000170038,0.995496641,0.002736295,1.27E-05,1.01E-05
7521,machine-translation6,52,Adversarial Training,,,machine-translation,6,0,1,0,,0.753486682,1,negative,0.000229487,0.004887167,0.007723244,9.34E-06,9.76E-06,0.000503849,0.002913463,0.008149339,0.001219214,0.53480541,0.426326513,0.013046574,0.000176646
7522,machine-translation6,53,"The basic idea of our work to address the above problem is adversarial training , in which two or more models learn together by pursuing competing goals .",Adversarial Training,Adversarial Training,machine-translation,6,1,1,0,,0.009507828,0,negative,0.000173324,6.47E-05,0.004908841,3.53E-06,1.30E-06,6.03E-05,6.44E-05,0.000375335,9.07E-05,0.992700759,0.000130584,0.001375465,5.08E-05
7523,machine-translation6,54,"A representative example of adversarial training is Generative Adversarial Networks ( GANs ) for image generation , in which a discriminator and a generator compete with each other : the generator aims to generate images similar to the natural ones , and the discriminator aims to detect the generated ones from the natural ones .",Adversarial Training,Adversarial Training,machine-translation,6,2,1,0,,0.011477071,0,negative,6.65E-06,1.78E-06,0.000165212,1.13E-06,3.46E-08,5.21E-05,6.31E-05,0.00033519,7.37E-06,0.99764019,0.001341708,0.000307433,7.80E-05
7524,machine-translation6,55,"Recently , adversarial training has been successfully applied to NLP tasks .",Adversarial Training,Adversarial Training,machine-translation,6,3,1,0,,0.135421258,0,negative,1.37E-05,1.57E-06,5.60E-05,9.26E-07,3.40E-08,3.58E-05,6.29E-05,0.000273812,2.98E-06,0.996665926,0.002181633,0.000625444,7.92E-05
7525,machine-translation6,56,introduce an additional discriminator to differentiate the semantics learned from different languages in non-parallel bilingual data .,Adversarial Training,Adversarial Training,machine-translation,6,4,1,0,,0.00059255,0,negative,0.000119596,1.63E-06,0.000290003,3.21E-06,1.75E-07,4.40E-05,2.49E-05,0.000200548,2.15E-05,0.998580189,1.90E-05,0.000637558,5.76E-05
7526,machine-translation6,57,develops a discriminator to classify whether a sentence is created by human or generated by a model .,Adversarial Training,Adversarial Training,machine-translation,6,5,1,0,,0.000467864,0,negative,2.98E-05,8.40E-07,0.001599264,1.28E-06,1.99E-07,3.29E-05,2.77E-05,0.000112254,1.91E-05,0.997887925,1.67E-05,0.000201875,7.01E-05
7527,machine-translation6,58,Our proposed method is under the adversarial training framework but not exactly the conventional generator - discriminator approach since there is no generator in our scenario .,Adversarial Training,Adversarial Training,machine-translation,6,6,1,0,,0.003997755,0,negative,0.000134257,2.70E-05,0.001080767,5.53E-07,3.47E-07,2.00E-05,3.19E-05,0.00025981,5.19E-05,0.994419589,1.90E-05,0.003940911,1.39E-05
7528,machine-translation6,59,"For an NLP task and its neural network model ( including word embeddings ) , we introduce a discriminator to differentiate embeddings of popular words and rare words ; while the NN model aims to fool the discriminator and minimize the task - specific loss simultaneously .",Adversarial Training,Adversarial Training,machine-translation,6,7,1,0,,0.011437519,0,negative,0.000242658,0.000129007,0.00455507,8.73E-07,7.59E-07,4.01E-05,4.86E-05,0.000757065,0.000790725,0.992643038,1.54E-05,0.000754943,2.18E-05
7529,machine-translation6,60,Our work is also weakly related to adversarial domain adaptation which attempts to mitigate the negative effects of domain shift between training and testing .,Adversarial Training,Adversarial Training,machine-translation,6,8,1,0,,0.000929035,0,negative,3.11E-05,5.12E-06,0.000389962,2.80E-06,2.78E-07,6.54E-05,4.28E-05,0.000344509,1.85E-05,0.998500209,8.69E-05,0.000432286,8.02E-05
7530,machine-translation6,61,"The difference between this work and adversarial domain adaptation is that we do not target at the mismatch between training and testing ; instead , we aim to improve the effectiveness of word embeddings and consequently improve the performance of end - to - end NLP tasks .",Adversarial Training,Adversarial Training,machine-translation,6,9,1,0,,0.000980968,0,negative,5.59E-05,1.73E-05,0.000639401,7.41E-07,2.99E-07,2.06E-05,2.36E-05,0.000206905,3.25E-05,0.997852168,4.36E-05,0.001086023,2.10E-05
7531,machine-translation6,62,Empirical Study,Adversarial Training,,machine-translation,6,10,1,0,,0.000670804,0,negative,4.00E-06,6.92E-08,1.32E-05,8.93E-09,3.41E-09,1.55E-05,5.62E-05,0.000185072,5.82E-07,0.999008613,1.43E-06,0.000711023,4.28E-06
7532,machine-translation6,63,"In this section , we study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English - German translation task using Transformer .",Adversarial Training,Empirical Study,machine-translation,6,11,1,0,,0.033369328,0,negative,0.000425856,1.04E-05,0.000185352,3.71E-07,7.04E-07,3.15E-05,0.000259618,0.000174155,4.52E-06,0.98949855,1.46E-06,0.009400242,7.27E-06
7533,machine-translation6,64,The implementation details can be found in the supplementary material ( part A ) .,Adversarial Training,Empirical Study,machine-translation,6,12,1,0,,7.16E-05,0,negative,1.94E-05,3.12E-07,7.58E-06,3.16E-06,2.74E-07,4.31E-05,3.02E-05,8.06E-05,1.72E-06,0.999384779,4.78E-07,0.000422335,6.21E-06
7534,machine-translation6,65,Experimental Design,,,machine-translation,6,0,1,0,,0.001443768,0,negative,1.00E-05,0.000238562,7.74E-06,2.70E-06,2.64E-06,0.00028349,6.65E-05,0.003379208,0.000127737,0.994529038,0.001261853,8.71E-05,3.45E-06
7535,machine-translation6,66,"In both tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words ( roughly speaking , we set a word as a rare word if it s relative frequency is lower than 10 ? 6 in WMT14 dataset and 10 ? 7 in Google News dataset ) .",Experimental Design,Experimental Design,machine-translation,6,1,1,0,,0.173563195,0,hyperparameters,3.74E-05,4.87E-05,0.000349648,1.01E-06,1.57E-06,0.144493504,0.00118106,0.447799016,1.49E-05,0.406013116,7.19E-06,4.64E-05,6.44E-06
7536,machine-translation6,67,We have tried other thresholds such as 10 % or 25 % and found the observations are similar .,Experimental Design,Experimental Design,machine-translation,6,2,1,0,,0.087205093,0,negative,0.000280605,5.56E-06,0.000191045,2.85E-06,1.46E-06,0.036629208,0.001802171,0.044963106,2.84E-06,0.914840559,8.74E-05,0.001177978,1.52E-05
7537,machine-translation6,68,We study whether the semantic relationship between two words is reasonable .,Experimental Design,Experimental Design,machine-translation,6,3,1,0,,0.011066047,0,negative,0.000145825,0.000282018,0.001515697,2.12E-05,1.35E-05,0.008227385,0.002438434,0.02564412,3.97E-05,0.947480163,0.013695782,0.000413726,8.23E-05
7538,machine-translation6,69,"To achieve this , we randomly sampled some rare / popular words and checked the embeddings trained from different tasks .",Experimental Design,Experimental Design,machine-translation,6,4,1,0,,0.042383271,0,negative,0.000141414,9.12E-05,0.000184549,1.97E-05,2.33E-05,0.093797149,0.000741688,0.188146492,3.05E-05,0.716757393,4.58E-06,4.83E-05,1.37E-05
7539,machine-translation6,70,"For each sampled word , we determined its nearest neighbors based on the cosine similarity between its embeddings and others '.",Experimental Design,Experimental Design,machine-translation,6,5,1,0,,0.070999919,0,negative,0.000633918,0.0002451,0.023506957,3.54E-06,1.26E-05,0.042307584,0.001575362,0.077259608,0.000176237,0.853796114,1.37E-05,0.000443533,2.58E-05
7540,machine-translation6,71,We also manually chose words which are semantically similar to it .,Experimental Design,Experimental Design,machine-translation,6,6,1,0,,0.108100733,0,negative,0.000146033,5.05E-05,0.000752828,7.41E-06,3.24E-05,0.060174471,0.00109162,0.066103808,2.46E-05,0.871410859,7.66E-06,0.000182036,1.58E-05
7541,machine-translation6,72,"For simplicity , for each word , we call the nearest words predicted from the embeddings as model - predicted neighbors , and call our chosen words as semantic neighbors .",Experimental Design,Experimental Design,machine-translation,6,7,1,0,,0.006562814,0,negative,3.44E-05,5.07E-05,0.001121886,3.91E-07,1.03E-06,0.011502736,0.000189354,0.039342641,6.15E-05,0.947652567,3.91E-06,3.63E-05,2.65E-06
7542,machine-translation6,73,Observation,Experimental Design,,machine-translation,6,8,1,0,,0.003169755,0,negative,7.18E-05,9.42E-06,6.94E-05,3.31E-06,1.73E-06,0.005977023,0.000363009,0.016749267,1.68E-05,0.976439865,7.92E-05,0.000203045,1.61E-05
7543,machine-translation6,74,"To visualize word embeddings , we reduce their dimensionalities by SVD and plot two cases in .",Experimental Design,Observation,machine-translation,6,9,1,0,,0.001226659,0,negative,0.000359973,0.000111992,5.03E-05,2.77E-06,0.000214458,0.001183352,9.71E-05,7.70E-05,2.13E-05,0.99785274,9.15E-06,1.93E-05,4.58E-07
7544,machine-translation6,75,More cases and other studies without dimensionality reduction can be found in the supplementary material ( part C ) .,Experimental Design,Observation,machine-translation,6,10,1,0,,7.19E-06,0,negative,4.57E-06,1.64E-06,5.32E-07,1.11E-06,2.26E-05,0.000267541,5.87E-06,1.05E-05,8.95E-07,0.999682453,1.84E-06,3.89E-07,5.25E-08
7545,machine-translation6,76,We find that the embeddings trained from different tasks share some common patterns .,Experimental Design,Observation,machine-translation,6,11,1,0,,0.008529597,0,negative,0.000904563,0.000161003,7.27E-06,2.16E-06,7.01E-05,0.001112828,9.43E-05,0.000157194,5.46E-05,0.997360685,3.00E-05,4.44E-05,9.98E-07
7546,machine-translation6,77,"For both tasks , more than 90 % of model - predicted neighbors of rare words are rare words .",Experimental Design,Observation,machine-translation,6,12,1,0,,0.026806234,0,negative,0.0010571,6.07E-05,1.03E-05,1.20E-05,0.003225061,0.000966729,0.000364318,5.24E-05,2.96E-06,0.994094398,1.08E-05,0.000140494,2.70E-06
7547,machine-translation6,78,"For each rare word , the model - predicted neighbor is usually not semantically related to this word , and semantic neighbors we chose are faraway from it in the embedding space .",Experimental Design,Observation,machine-translation,6,13,1,0,,1.11E-05,0,negative,0.000236332,3.09E-05,1.40E-05,2.24E-07,3.89E-05,0.000191833,1.79E-05,1.74E-05,1.11E-05,0.999429557,1.70E-06,1.01E-05,6.91E-08
7548,machine-translation6,79,"In contrast , the model - predicted neighbors of popular words are very reasonable .",Experimental Design,Observation,machine-translation,6,14,1,0,,0.056448755,0,negative,0.004359902,2.32E-05,2.48E-06,3.64E-06,5.59E-05,0.001770562,0.000759415,0.000369019,4.07E-06,0.991924975,2.82E-05,0.000695352,3.32E-06
7549,machine-translation6,80,"As the patterns in rare words are different from that of popular words , we further check the whole embedding matrix to make a general understanding .",Experimental Design,Observation,machine-translation,6,15,1,0,,7.14E-05,0,negative,0.000603584,3.05E-05,5.31E-06,1.94E-07,2.50E-05,0.000139885,1.92E-05,2.30E-05,1.49E-05,0.999120007,6.45E-07,1.77E-05,7.11E-08
7550,machine-translation6,81,We also visualize the word embeddings using SVD by keeping the two directions with top - 2 largest eigenvalues as in and plot them in,Experimental Design,Observation,machine-translation,6,16,1,0,,0.000145723,0,negative,2.34E-05,5.02E-05,1.51E-05,1.46E-06,3.01E-05,0.004852075,0.000153129,0.000315972,5.57E-05,0.99447964,1.72E-05,5.18E-06,9.67E-07
7551,machine-translation6,82,Input Tokens Word Embeddings,Experimental Design,,machine-translation,6,17,1,0,,0.091250583,0,hyperparameters,4.62E-05,6.19E-05,0.001371415,2.83E-06,1.47E-06,0.129661805,0.002399418,0.542942612,0.000322885,0.322924328,2.50E-05,0.000148326,9.18E-05
7552,machine-translation6,83,Task - specific Outputs,Experimental Design,Input Tokens Word Embeddings,machine-translation,6,18,1,0,,0.000222657,0,negative,1.27E-05,4.05E-07,3.72E-05,1.41E-08,3.77E-08,1.76E-05,2.91E-05,6.19E-05,4.41E-06,0.997716488,1.75E-06,0.002117475,8.86E-07
7553,machine-translation6,84,Task - specific,Experimental Design,Input Tokens Word Embeddings,machine-translation,6,19,1,0,,0.000153285,0,negative,5.23E-05,7.05E-07,2.57E-05,2.15E-07,4.58E-07,4.56E-05,2.41E-05,0.000116643,4.62E-06,0.998828748,3.25E-07,0.000899169,1.51E-06
7554,machine-translation6,85,Model,,,machine-translation,6,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
7555,machine-translation6,86,Loss,Model,,machine-translation,6,1,1,0,,0.014869795,0,negative,0.001022065,0.000294912,0.000311191,4.36E-05,1.50E-05,0.000364814,0.000437282,0.002186917,0.017063242,0.974817151,0.000455131,0.002855289,0.00013344
7556,machine-translation6,87,Rare / Popular Labels Discriminator,Model,,machine-translation,6,2,1,0,,0.011282628,0,negative,0.000275833,0.000288762,0.001543655,7.43E-06,8.10E-06,0.000814609,0.000650925,0.004753446,0.018774361,0.970785742,8.89E-05,0.001943912,6.43E-05
7557,machine-translation6,88,Loss predict predict :,Model,Rare / Popular Labels Discriminator,machine-translation,6,3,1,0,,0.189269033,0,baselines,9.16E-05,8.22E-05,0.536862583,3.66E-06,4.14E-06,0.000712189,0.000940329,0.006613965,0.000375306,0.452546687,0.000145192,0.001570968,5.11E-05
7558,machine-translation6,89,"The proposed learning framework includes a task - specific predictor and a discriminator , whose function is to classify rare and popular words .",Model,Rare / Popular Labels Discriminator,machine-translation,6,4,1,0,,0.00216857,0,negative,2.42E-05,0.001701937,0.003178802,1.15E-06,2.41E-06,9.26E-05,1.33E-05,0.007863294,0.01754956,0.969464793,7.39E-05,2.86E-05,5.50E-06
7559,machine-translation6,90,Both modules use word embeddings as the input .,Model,Rare / Popular Labels Discriminator,machine-translation,6,5,1,0,,0.002724219,0,negative,3.69E-06,0.000213341,0.000641166,5.80E-07,3.18E-07,0.000842658,3.33E-05,0.106058039,0.003875574,0.888304249,1.79E-05,5.09E-06,4.04E-06
7560,machine-translation6,91,"a certain degree : the rare words and popular words lie in different regions after this linear projection , and thus they occupy different regions in the original embedding space .",Model,Rare / Popular Labels Discriminator,machine-translation,6,6,1,0,,0.000103194,0,negative,7.42E-05,2.61E-05,0.000270163,3.25E-07,5.70E-07,2.36E-05,5.89E-06,0.001304949,0.000116131,0.998018568,6.32E-06,0.000152182,1.02E-06
7561,machine-translation6,92,This strange phenomenon is also observed in other learned embeddings ( e.g. CBOW and GLOVE ) and mentioned in .,Model,Rare / Popular Labels Discriminator,machine-translation,6,7,1,0,,4.87E-05,0,negative,5.17E-06,9.40E-06,3.10E-05,5.56E-07,1.53E-07,4.83E-05,1.22E-05,0.002536344,1.92E-05,0.997118503,0.000160625,5.70E-05,1.52E-06
7562,machine-translation6,93,Explanation,Model,,machine-translation,6,8,1,0,,0.000122681,0,negative,0.000117602,7.65E-05,5.55E-05,1.39E-05,1.03E-05,0.000125871,6.40E-05,0.000705491,0.00514277,0.993383827,5.41E-05,0.000226163,2.40E-05
7563,machine-translation6,94,"From the empirical study above , we can see that the occupied spaces of popular words and rare words are different and here we intuitively explain a possible reason .",Model,Explanation,machine-translation,6,9,1,0,,1.57E-06,0,negative,1.23E-05,2.38E-07,9.88E-08,5.53E-09,3.76E-08,2.10E-06,5.00E-07,4.38E-06,3.90E-07,0.999972423,2.80E-07,7.21E-06,4.53E-09
7564,machine-translation6,95,We simply take word2vec as an example which is trained by stochastic gradient descent .,Model,Explanation,machine-translation,6,10,1,0,,7.59E-06,0,negative,2.24E-06,6.47E-06,3.70E-06,2.38E-08,1.70E-07,3.31E-05,2.29E-06,0.000118259,2.17E-05,0.999811025,1.82E-07,7.76E-07,3.10E-08
7565,machine-translation6,96,"During training , the sample rate of a popular word is high and the embedding of a popular word updates frequently .",Model,Explanation,machine-translation,6,11,1,0,,0.000341799,0,negative,8.25E-05,0.000128069,6.73E-06,2.36E-07,1.83E-06,6.21E-05,9.26E-06,0.000804591,0.000128375,0.998752468,1.84E-06,2.17E-05,3.25E-07
7566,machine-translation6,97,"For a rare word , the sample rate is low and its embedding rarely updates .",Model,Explanation,machine-translation,6,12,1,0,,3.14E-05,0,negative,0.000634949,4.54E-06,4.60E-06,1.85E-07,1.33E-06,1.55E-05,3.66E-06,3.45E-05,5.11E-06,0.999215594,5.61E-07,7.94E-05,9.42E-08
7567,machine-translation6,98,"According to our study , on average , the moving distance of the embedding for a popular word is twice longer than that of a rare word during training .",Model,Explanation,machine-translation,6,13,1,0,,7.31E-05,0,negative,0.000794964,1.47E-05,4.92E-06,1.07E-07,1.55E-06,3.37E-05,3.42E-05,0.000148739,1.24E-05,0.998538278,1.99E-06,0.000414027,4.31E-07
7568,machine-translation6,99,"As all word embeddings are usually initialized around the origin with a small variance , we observe in the final model , the embeddings of rare words are still around the origin and the popular words have moved faraway .",Model,Explanation,machine-translation,6,14,1,0,,0.000260343,0,negative,0.002857776,1.90E-05,6.43E-06,1.07E-07,9.04E-07,2.48E-05,1.89E-05,8.68E-05,4.45E-05,0.996534835,9.04E-07,0.000404826,2.45E-07
7569,machine-translation6,100,Discussion,Model,,machine-translation,6,15,1,0,,0.000618432,0,negative,0.000116125,5.79E-05,3.10E-05,4.63E-06,4.24E-06,8.61E-05,7.79E-05,0.000615006,0.002689764,0.995833561,3.35E-05,0.00043213,1.81E-05
7570,machine-translation6,101,We have strong evidence that the current phenomena are problematic .,Model,Discussion,machine-translation,6,16,1,0,,1.53E-06,0,negative,2.01E-05,1.34E-06,5.70E-07,8.23E-08,1.20E-06,3.25E-06,3.29E-07,4.63E-06,2.93E-06,0.999962152,1.42E-06,1.96E-06,2.35E-08
7571,machine-translation6,102,"First , according to our study , in both tasks , more than half of the rare words are nouns , e.g. , company names , city names .",Model,Discussion,machine-translation,6,17,1,0,,1.88E-05,0,negative,0.000163531,1.69E-06,2.29E-06,1.80E-06,4.44E-05,9.26E-06,1.19E-06,3.52E-06,6.39E-07,0.999761705,1.11E-06,8.75E-06,8.59E-08
7572,machine-translation6,103,"They may share some similar topics to popular entities , e.g. , big companies and cities ; around 10 % percent of rare words include a hyphen ( which is usually used to join popular words ) , and over 30 % rare words are different PoSs of popular words .",Model,Discussion,machine-translation,6,18,1,0,,9.56E-06,0,negative,4.19E-05,3.64E-06,9.73E-06,6.94E-07,3.81E-05,4.11E-06,4.71E-07,1.85E-06,1.51E-06,0.9998951,9.87E-07,1.76E-06,6.19E-08
7573,machine-translation6,104,These words should have mixed or similar semantics to some popular words .,Model,Discussion,machine-translation,6,19,1,0,,3.88E-07,0,negative,8.61E-06,1.73E-06,8.77E-07,8.75E-08,1.42E-06,3.78E-06,1.28E-07,6.67E-06,5.68E-06,0.99997007,1.54E-07,7.79E-07,1.72E-08
7574,machine-translation6,105,"These facts show that rare words and popular words should lie in the same region of the embedding space , which is different from what we observed .",Model,Discussion,machine-translation,6,20,1,0,,1.05E-05,0,negative,2.29E-05,2.86E-06,1.36E-06,6.72E-09,1.77E-07,8.68E-07,1.55E-07,3.28E-06,1.07E-05,0.999954069,8.69E-07,2.73E-06,7.38E-09
7575,machine-translation6,106,"Second , as we can see from the cases , for rare words , model - predicted neighbors are usually not semantically related words but frequency - related words ( rare words ) .",Model,Discussion,machine-translation,6,21,1,0,,7.30E-05,0,negative,0.001390537,3.84E-06,1.51E-05,7.31E-08,6.37E-06,2.33E-06,1.91E-06,2.77E-06,1.54E-06,0.998477629,7.20E-07,9.72E-05,3.80E-08
7576,machine-translation6,107,"This shows , for rare words , the embeddings encode more frequency information than semantic information .",Model,Discussion,machine-translation,6,22,1,0,,0.000544926,0,negative,0.002215831,2.71E-05,2.66E-05,9.29E-08,2.51E-06,5.07E-06,2.77E-06,1.44E-05,6.73E-05,0.99757597,1.29E-06,6.10E-05,1.32E-07
7577,machine-translation6,108,"It is not good to use such word embeddings into semantic understanding tasks , e.g. , text classification , language modeling , language understanding and translation .",Model,Discussion,machine-translation,6,23,1,0,,4.13E-05,0,negative,4.59E-06,5.38E-06,9.87E-07,3.59E-07,5.24E-07,6.44E-06,1.61E-06,1.57E-05,5.17E-06,0.998763661,0.001193036,2.07E-06,4.72E-07
7578,machine-translation6,109,Our Method,,,machine-translation,6,0,1,0,,0.007719084,0,negative,0.000122205,0.000817757,0.000212444,1.20E-05,5.49E-06,0.000321148,0.000437382,0.003881259,0.000932912,0.925387443,0.066737957,0.001045892,8.61E-05
7579,machine-translation6,110,"In this section , we present our method to improve word representations .",Our Method,Our Method,machine-translation,6,1,1,0,,0.003061581,0,negative,0.000390565,0.008828685,0.00045722,2.14E-06,2.75E-05,7.43E-05,1.40E-05,0.000411567,0.001432583,0.98775805,0.000238727,0.000363225,1.46E-06
7580,machine-translation6,111,"As we have a strong prior that many rare words should share the same region in the embedding space as popular words , the basic idea of our algorithm is to train the word embeddings in an adversarial framework :",Our Method,Our Method,machine-translation,6,2,1,0,,0.00095965,0,negative,9.82E-05,0.011386079,0.000455247,1.29E-06,9.69E-06,8.68E-05,6.28E-06,0.000980966,0.007740859,0.979136343,6.55E-05,3.19E-05,8.30E-07
7581,machine-translation6,112,We introduce a discriminator to categorize word embeddings into two classes : popular ones or rare ones .,Our Method,Our Method,machine-translation,6,3,1,0,,0.011690128,0,negative,0.000353118,0.045900951,0.000907406,3.61E-06,2.46E-05,0.000206485,2.24E-05,0.003090324,0.074342906,0.874733644,0.00031195,9.95E-05,3.09E-06
7582,machine-translation6,113,We hope the learned word embeddings not only minimize the task - specific training loss but also fool the discriminator .,Our Method,Our Method,machine-translation,6,4,1,0,,0.005013745,0,negative,4.86E-05,6.44E-05,9.96E-07,4.75E-07,9.94E-07,5.07E-05,1.42E-06,0.000327599,6.42E-05,0.999415303,3.73E-06,2.16E-05,1.36E-07
7583,machine-translation6,114,"By doing so , the frequency information is removed from the embedding and we call our method frequency - agnostic word embedding ( FRAGE ) .",Our Method,Our Method,machine-translation,6,5,1,0,,0.016267593,0,negative,0.001300118,0.013023461,0.003138646,1.66E-06,3.05E-05,5.90E-05,2.28E-05,0.000382842,0.004667931,0.976500603,0.000164747,0.000705572,2.13E-06
7584,machine-translation6,115,We first define some notations and then introduce our algorithm .,Our Method,Our Method,machine-translation,6,6,1,0,,4.50E-06,0,negative,3.57E-06,6.92E-05,6.00E-07,2.86E-07,6.58E-07,4.39E-05,8.01E-07,0.000322521,5.19E-05,0.999499869,2.72E-06,3.82E-06,5.94E-08
7585,machine-translation6,116,"We develop three types of notations : embeddings , task - specific parameters / loss , and discriminator parameters / loss .",Our Method,Our Method,machine-translation,6,7,1,0,,0.000158796,0,negative,2.54E-05,0.001038712,7.32E-06,2.73E-06,8.90E-06,0.00019397,3.89E-06,0.001558469,0.000550435,0.996597375,2.71E-06,9.76E-06,2.97E-07
7586,machine-translation6,117,Denote ? emb ?,Our Method,Our Method,machine-translation,6,8,1,0,,1.35E-05,0,negative,2.48E-06,4.46E-06,1.43E-06,7.79E-09,7.01E-08,4.03E-06,2.85E-07,2.31E-05,9.20E-06,0.999950034,7.98E-07,4.08E-06,8.83E-09
7587,machine-translation6,118,"R d|V | as the word embedding matrix to be learned , where d is the dimension of the embedding vectors and | V | is the vocabulary size .",Our Method,Our Method,machine-translation,6,9,1,0,,2.73E-05,0,negative,1.65E-05,7.90E-05,7.21E-06,6.57E-08,5.66E-07,1.61E-05,1.30E-06,0.000175978,3.97E-05,0.999645869,3.87E-06,1.37E-05,5.98E-08
7588,machine-translation6,119,Let V pop denote the set of popular words and V rare = V \ V pop denote the set of rare words .,Our Method,Our Method,machine-translation,6,10,1,0,,1.69E-05,0,negative,1.02E-06,2.40E-05,9.72E-07,5.37E-09,5.57E-08,9.73E-06,2.87E-07,0.000120539,3.76E-05,0.999803183,7.55E-07,1.83E-06,8.06E-09
7589,machine-translation6,120,Then the embedding matrix ?,Our Method,Our Method,machine-translation,6,11,1,0,,1.77E-05,0,negative,8.61E-06,6.77E-06,2.71E-06,8.02E-09,9.23E-08,5.92E-06,4.56E-07,4.37E-05,1.03E-05,0.999911976,6.41E-07,8.84E-06,1.06E-08
7590,machine-translation6,121,emb can be divided into two parts : ?,Our Method,Our Method,machine-translation,6,12,1,0,,9.22E-05,0,negative,1.41E-05,7.38E-05,0.00010168,7.25E-08,6.77E-07,9.59E-06,3.51E-06,4.07E-05,7.06E-05,0.999441832,0.000198946,4.42E-05,2.89E-07
7591,machine-translation6,122,emb pop for popular words and ?,Our Method,Our Method,machine-translation,6,13,1,0,,0.000472099,0,negative,1.51E-05,9.86E-06,4.90E-06,5.88E-07,5.45E-06,0.000101814,4.93E-06,0.000167672,9.57E-06,0.999661851,3.80E-07,1.77E-05,1.66E-07
7592,machine-translation6,123,emb rare for rare words .,Our Method,Our Method,machine-translation,6,14,1,0,,7.97E-05,0,negative,4.89E-06,6.10E-06,1.08E-06,5.68E-07,1.03E-06,1.83E-05,7.14E-07,4.32E-05,1.46E-05,0.999903903,1.67E-06,3.83E-06,7.18E-08
7593,machine-translation6,124,Let ?,Our Method,Our Method,machine-translation,6,15,1,0,,6.53E-06,0,negative,1.87E-06,2.01E-06,2.66E-07,1.39E-08,7.65E-08,8.31E-06,3.08E-07,3.75E-05,4.77E-06,0.999942492,2.88E-07,2.06E-06,9.66E-09
7594,machine-translation6,125,emb w denote the embedding of word w .,Our Method,Our Method,machine-translation,6,16,1,0,,2.80E-05,0,negative,1.11E-06,2.26E-05,1.84E-06,3.72E-08,2.42E-07,1.22E-05,4.25E-07,9.59E-05,8.10E-05,0.999782205,1.12E-06,1.20E-06,4.13E-08
7595,machine-translation6,126,Let ?,Our Method,Our Method,machine-translation,6,17,1,0,,6.38E-06,0,negative,1.81E-06,1.94E-06,2.62E-07,1.33E-08,8.12E-08,7.90E-06,3.09E-07,3.61E-05,4.76E-06,0.999944444,2.29E-07,2.12E-06,9.95E-09
7596,machine-translation6,127,model denote all the other task - specific parameters except word embeddings .,Our Method,Our Method,machine-translation,6,18,1,0,,6.44E-05,0,negative,4.71E-06,7.94E-05,8.14E-06,2.52E-07,7.14E-07,0.000154777,4.85E-06,0.001455518,0.000358218,0.997927393,1.38E-06,4.38E-06,2.91E-07
7597,machine-translation6,128,"For instance , for language modeling , ?",Our Method,Our Method,machine-translation,6,19,1,0,,2.64E-05,0,negative,2.04E-06,8.91E-06,1.18E-06,2.08E-08,1.25E-07,4.38E-06,1.27E-06,3.05E-05,3.69E-06,0.999884737,4.79E-05,1.52E-05,4.98E-08
7598,machine-translation6,129,"model is the parameters of the RNN or LSTM ; for neural machine translation , ?",Our Method,Our Method,machine-translation,6,20,1,0,,1.64E-05,0,negative,2.66E-06,3.11E-05,3.11E-06,3.08E-07,1.41E-06,6.20E-05,2.56E-06,0.000321217,4.95E-05,0.999519122,1.87E-06,4.85E-06,2.02E-07
7599,machine-translation6,130,"model is the parameters of the encoder , attention module and decoder .",Our Method,Our Method,machine-translation,6,21,1,0,,5.14E-05,0,negative,8.83E-06,0.000134662,1.22E-05,4.04E-07,1.62E-06,0.0001167,5.31E-06,0.001445784,0.001246297,0.997020966,1.84E-06,4.73E-06,6.38E-07
7600,machine-translation6,131,"Let L T ( S ; ? model , ? emb ) denote the task - specific loss over a dataset S. Taking language modeling as an example , the loss L T ( S ; ? model , ? emb ) is defined as the negative log likelihood of the data :",Our Method,Our Method,machine-translation,6,22,1,0,,1.01E-05,0,negative,1.06E-06,4.04E-05,2.89E-06,1.14E-08,1.29E-07,9.29E-06,6.50E-07,0.000107049,4.67E-05,0.999785113,2.88E-06,3.81E-06,3.71E-08
7601,machine-translation6,132,where y is a sentence .,Our Method,Our Method,machine-translation,6,23,1,0,,8.21E-06,0,negative,8.52E-07,2.77E-06,5.87E-07,5.48E-09,1.10E-07,1.73E-06,1.83E-07,1.24E-05,8.68E-06,0.999970363,2.78E-07,1.99E-06,1.08E-08
7602,machine-translation6,133,Let f ?,Our Method,Our Method,machine-translation,6,24,1,0,,1.04E-05,0,negative,4.90E-06,4.24E-06,6.73E-07,1.10E-08,1.93E-07,5.49E-06,4.81E-07,2.99E-05,5.21E-06,0.999938844,1.44E-07,9.94E-06,1.41E-08
7603,machine-translation6,134,D denote a discriminator with parameters ?,Our Method,Our Method,machine-translation,6,25,1,0,,2.37E-05,0,negative,3.53E-06,1.45E-05,1.11E-06,5.14E-08,3.79E-07,1.93E-05,9.74E-07,0.000173985,2.20E-05,0.999758265,2.36E-07,5.69E-06,4.31E-08
7604,machine-translation6,135,"D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .",Our Method,Our Method,machine-translation,6,26,1,0,,2.80E-05,0,negative,6.06E-06,6.85E-05,1.10E-05,2.48E-08,6.11E-07,1.72E-05,1.09E-06,0.000203381,0.000145892,0.999540827,2.04E-07,5.20E-06,6.49E-08
7605,machine-translation6,136,"Let L D ( V ; ? D , ? emb ) denote the loss of the discriminator :",Our Method,Our Method,machine-translation,6,27,1,0,,9.91E-06,0,negative,1.46E-05,1.52E-05,6.18E-06,6.50E-09,2.69E-07,2.45E-06,3.09E-07,1.88E-05,1.83E-05,0.999910228,9.04E-08,1.37E-05,1.15E-08
7606,machine-translation6,137,"Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ?",Our Method,Our Method,machine-translation,6,28,1,0,,0.000142804,0,negative,8.15E-06,0.00010154,7.56E-06,1.23E-08,6.09E-07,2.83E-06,4.12E-07,4.00E-05,9.65E-05,0.999729219,3.44E-07,1.28E-05,2.72E-08
7607,machine-translation6,138,model and ? emb ) and the discriminator ( ?,Our Method,Our Method,machine-translation,6,29,1,0,,0.000235523,0,negative,2.67E-05,5.43E-05,0.000175334,1.10E-07,4.41E-06,3.29E-05,9.89E-06,0.000127075,0.000124648,0.999351385,5.83E-07,9.21E-05,4.83E-07
7608,machine-translation6,139,D ) as below :,Our Method,Our Method,machine-translation,6,30,1,0,,0.000102242,0,negative,2.41E-05,2.65E-05,8.03E-05,1.06E-08,3.55E-07,5.07E-06,1.41E-06,2.62E-05,0.000170059,0.999627957,4.15E-07,3.75E-05,1.01E-07
7609,machine-translation6,140,where ?,Our Method,Our Method,machine-translation,6,31,1,0,,1.66E-06,0,negative,9.46E-07,4.58E-06,3.69E-07,3.38E-08,1.37E-07,8.21E-06,3.37E-07,4.97E-05,1.67E-05,0.999917393,1.88E-07,1.43E-06,2.53E-08
7610,machine-translation6,141,is a coefficient to trade off the two loss terms .,Our Method,Our Method,machine-translation,6,32,1,0,,0.000181106,0,negative,2.41E-05,0.000175975,6.14E-06,1.42E-06,6.61E-06,9.21E-05,4.06E-06,0.00172827,0.000320137,0.997629619,3.00E-07,1.07E-05,5.85E-07
7611,machine-translation6,142,We can see that when the model parameter ?,Our Method,Our Method,machine-translation,6,33,1,0,,2.18E-05,0,negative,4.27E-05,5.68E-05,1.19E-06,6.01E-07,2.63E-06,0.000211489,1.35E-05,0.003023083,5.17E-05,0.996522483,1.39E-06,7.17E-05,8.57E-07
7612,machine-translation6,143,model and the embedding ?,Our Method,Our Method,machine-translation,6,34,1,0,,0.002787891,0,negative,6.33E-06,0.000101213,3.09E-05,2.76E-07,1.99E-06,3.37E-05,5.49E-06,0.000255218,0.000946588,0.998596351,3.09E-06,1.78E-05,1.01E-06
7613,machine-translation6,144,"emb are fixed , the optimization of the discriminator ?",Our Method,Our Method,machine-translation,6,35,1,0,,5.54E-05,0,negative,5.00E-06,2.49E-05,6.40E-07,1.73E-07,7.53E-07,2.47E-05,1.31E-06,0.000273708,4.10E-05,0.999616058,3.26E-07,1.12E-05,1.25E-07
7614,machine-translation6,145,D becomes,Our Method,,machine-translation,6,36,1,0,,1.19E-05,0,negative,2.84E-05,1.39E-05,1.96E-06,1.88E-07,1.26E-06,1.55E-05,1.53E-06,8.16E-05,4.96E-05,0.999786576,2.06E-07,1.92E-05,1.69E-07
7615,machine-translation6,146,which is to minimize the classification error of popular and rare words .,Our Method,D becomes,machine-translation,6,37,1,0,,0.000204721,0,negative,0.000132727,6.29E-06,0.000229529,2.02E-06,1.40E-06,1.71E-05,2.84E-06,0.000182827,2.53E-05,0.998882103,5.74E-06,0.000204604,0.000307479
7616,machine-translation6,147,When the discriminator ?,Our Method,D becomes,machine-translation,6,38,1,0,,4.00E-05,0,negative,4.08E-05,5.39E-07,2.05E-05,1.01E-07,1.34E-07,9.06E-06,1.18E-06,0.000105223,5.90E-06,0.999750422,9.05E-08,4.95E-05,1.65E-05
7617,machine-translation6,148,"Dis fixed , the optimization of ?",Our Method,D becomes,machine-translation,6,39,1,0,,3.48E-05,0,negative,0.000260782,2.25E-06,2.66E-05,5.36E-07,8.65E-07,2.34E-05,2.29E-06,0.000583746,1.05E-05,0.998943012,6.39E-08,0.000113318,3.27E-05
7618,machine-translation6,149,model and ?,Our Method,D becomes,machine-translation,6,40,1,0,,6.42E-05,0,negative,0.000135834,6.19E-06,0.000610028,1.05E-06,1.26E-06,2.50E-05,5.48E-06,0.000255377,0.000197867,0.998178602,2.12E-06,0.000150365,0.00043085
7619,machine-translation6,150,emb becomes,Our Method,D becomes,machine-translation,6,41,1,0,,0.00010135,0,negative,0.000699314,1.53E-06,0.000101112,3.24E-07,8.12E-07,1.10E-05,2.99E-06,0.000125418,1.66E-05,0.998616123,1.18E-07,0.000350534,7.41E-05
7620,machine-translation6,151,"i.e. , to optimize the task performance as well as fooling the discriminator .",Our Method,D becomes,machine-translation,6,42,1,0,,1.14E-05,0,negative,8.69E-05,6.35E-07,1.77E-05,2.90E-07,4.18E-07,3.34E-06,2.09E-07,3.01E-05,2.69E-06,0.999813103,4.17E-08,3.53E-05,9.26E-06
7621,machine-translation6,152,We train ?,Our Method,D becomes,machine-translation,6,43,1,0,,5.39E-05,0,negative,6.17E-05,1.82E-06,1.50E-05,1.25E-06,7.83E-07,3.83E-05,2.63E-06,0.000562341,9.38E-06,0.999155368,1.00E-07,4.55E-05,0.000105729
7622,machine-translation6,153,"model , ?",Our Method,D becomes,machine-translation,6,44,1,0,,0.000123363,0,negative,0.000149144,2.49E-06,0.001970367,2.28E-07,7.98E-07,1.04E-05,5.52E-06,7.97E-05,9.01E-05,0.997198827,4.71E-07,0.000292942,0.000198961
7623,machine-translation6,154,emb and ?,Our Method,D becomes,machine-translation,6,45,1,0,,0.000112469,0,negative,0.000146283,2.90E-07,0.000134235,6.33E-08,5.62E-07,5.71E-06,1.87E-06,2.69E-05,2.38E-06,0.99945777,2.35E-08,0.00019754,2.64E-05
7624,machine-translation6,155,D iteratively by stochastic gradient descent or its variants .,Our Method,D becomes,machine-translation,6,46,1,0,,0.000143232,0,negative,0.000368608,1.17E-06,0.000245192,6.29E-08,5.24E-07,3.40E-06,1.17E-06,4.53E-05,8.76E-06,0.999184003,1.43E-08,0.000127515,1.42E-05
7625,machine-translation6,156,The general training process is shown in Algorithm 1 .,Our Method,D becomes,machine-translation,6,47,1,0,,0.000119723,0,negative,3.58E-05,8.79E-06,5.85E-05,2.57E-07,6.42E-07,1.18E-05,1.82E-06,0.000573528,9.46E-05,0.999087945,1.13E-07,3.62E-05,9.01E-05
7626,machine-translation6,157,Experiment,,,machine-translation,6,0,1,0,,0.02032449,0,negative,2.65E-05,7.60E-05,1.08E-06,1.79E-06,9.59E-07,0.000215726,5.34E-05,0.005112434,4.95E-05,0.993225469,0.001033374,0.000199963,3.87E-06
7627,machine-translation6,158,"We test our method on a wide range of tasks , including word similarity , language modeling , machine translation and text classification .",Experiment,Experiment,machine-translation,6,1,1,0,,0.074816469,0,negative,0.000242863,0.002068453,0.000350349,7.16E-05,0.000166177,0.004689413,0.004385036,0.061386931,6.71E-05,0.922241238,0.001017098,0.003079491,0.000234248
7628,machine-translation6,159,"For each task , we choose the state - of - the - art architecture together with the state - of - the - art training method as our baseline .",Experiment,Experiment,machine-translation,6,2,1,0,,0.410330794,0,hyperparameters,7.95E-05,0.00102372,0.000648546,6.83E-06,9.55E-06,0.025156053,0.001796065,0.679475607,0.000213801,0.291289713,3.68E-05,0.000191055,7.27E-05
7629,machine-translation6,160,Sample a minibatch ?,Experiment,Experiment,machine-translation,6,3,1,0,,0.002116849,0,negative,8.85E-05,9.10E-05,7.19E-05,4.35E-05,9.37E-06,0.008587872,0.001110798,0.110958363,5.23E-05,0.877858197,0.000552492,0.000412757,0.00016305
7630,machine-translation6,161,from S.,Experiment,Experiment,machine-translation,6,4,1,0,,0.005322272,0,negative,0.00023829,5.20E-05,6.09E-05,1.01E-05,4.65E-06,0.001014129,0.000189842,0.022471715,5.35E-05,0.975198685,0.000138908,0.00052844,3.88E-05
7631,machine-translation6,162,4:00,Experiment,Experiment,machine-translation,6,5,1,0,,0.002201482,0,negative,0.002030127,0.000149487,0.001091757,8.86E-05,2.81E-05,0.005977334,0.001389791,0.036053996,0.000233322,0.950857706,0.000206705,0.001620049,0.000273008
7632,machine-translation6,163,Sample a minibatchV = V pop ?,Experiment,Experiment,machine-translation,6,6,1,0,,0.018649265,0,negative,0.000209773,0.000254946,0.000279723,1.33E-05,1.32E-05,0.015305752,0.001728365,0.191040761,8.84E-05,0.789835208,0.000181547,0.000890251,0.000158673
7633,machine-translation6,164,V rare from V .,Experiment,Experiment,machine-translation,6,7,1,0,,0.013082603,0,negative,0.000889522,5.38E-05,0.000864843,2.91E-05,3.19E-05,0.001012044,0.00056612,0.007282381,3.21E-05,0.986886154,3.88E-05,0.00224269,7.06E-05
7634,machine-translation6,165,5:00,Experiment,Experiment,machine-translation,6,8,1,0,,0.001869124,0,negative,0.002736665,0.000124539,0.000772727,0.000104195,3.55E-05,0.004187627,0.000999659,0.026734555,0.000176004,0.96160468,8.84E-05,0.002190567,0.000244911
7635,machine-translation6,166,"Update ? model , ?",Experiment,Experiment,machine-translation,6,9,1,0,,0.020574697,0,negative,0.000512679,0.000678366,0.001537374,2.78E-05,2.19E-05,0.012049973,0.001220994,0.273140256,0.001549247,0.707835401,0.000135091,0.000887467,0.000403491
7636,machine-translation6,167,emb by gradient descent according to Eqn. ( 5 ) with data ?.,Experiment,Experiment,machine-translation,6,10,1,0,,0.031177358,0,negative,0.000181931,0.000354143,0.024996954,5.83E-06,6.15E-06,0.001495034,0.001701576,0.028687433,0.000299659,0.935621174,0.001785711,0.004567676,0.000296722
7637,machine-translation6,168,Update ?,Experiment,Experiment,machine-translation,6,11,1,0,,0.015588293,0,negative,0.000219795,0.000141997,0.000520796,1.65E-05,1.22E-05,0.007593683,0.000741717,0.092428365,0.000261399,0.897225366,5.98E-05,0.000617285,0.000161058
7638,machine-translation6,169,D by gradient ascent according to Eqn. ( 4 ) with vocabulary V .,Experiment,Experiment,machine-translation,6,12,1,0,,0.012276496,0,negative,0.00240711,0.000509837,0.006415902,9.21E-06,3.76E-05,0.001468416,0.000677631,0.022599975,0.000154628,0.959193283,4.46E-05,0.00635502,0.00012676
7639,machine-translation6,170,"7 : until Converge 8 : Output : ? model , ? emb , ?",Experiment,Experiment,machine-translation,6,13,1,0,,0.065266184,0,negative,0.002003468,0.000416519,0.00180082,3.65E-05,5.90E-05,0.009520511,0.003372512,0.208845444,0.000197353,0.758269182,7.78E-05,0.014672388,0.000728486
7640,machine-translation6,171,D .,Experiment,,machine-translation,6,14,1,0,,0.000658304,0,negative,0.000390977,4.64E-05,6.40E-05,2.24E-05,1.34E-05,0.001563943,0.000250218,0.018873089,5.94E-05,0.978205553,2.11E-05,0.000426133,6.34E-05
7641,machine-translation6,172,"For fair comparisons , for each task , our method shares the same model architecture as the baseline .",Experiment,D .,machine-translation,6,15,1,0,,0.000797066,0,negative,0.029184367,0.000238573,0.011834624,2.47E-05,0.000125173,0.001583325,0.001501695,0.000740348,0.00046074,0.935607883,6.30E-06,0.000767906,0.017924353
7642,machine-translation6,173,The only difference is that we use the original task - specific loss function with an additional adversarial loss as in Eqn..,Experiment,D .,machine-translation,6,16,1,0,,3.23E-05,0,negative,0.072303918,6.80E-05,0.022514563,2.22E-05,7.76E-05,0.000320645,0.000140784,0.000112295,0.000377462,0.898035658,4.85E-06,0.000694432,0.005327512
7643,machine-translation6,174,"Due to space limitations , we put dataset description , model description , hyperparameter configuration into supplementary material ( part A ) .",Experiment,D .,machine-translation,6,17,1,0,,0.000628984,0,negative,0.001507142,3.87E-06,7.89E-05,0.001578646,0.000274726,0.000889281,3.73E-05,0.000112272,1.18E-05,0.979731538,1.52E-06,6.15E-05,0.01571155
7644,machine-translation6,175,Settings,Experiment,,machine-translation,6,18,1,0,,0.002844271,0,negative,4.83E-05,0.00029218,6.27E-05,3.54E-05,3.16E-05,0.004299102,0.000474008,0.109085879,0.000115453,0.884933135,3.10E-05,0.000495723,9.54E-05
7645,machine-translation6,176,We conduct experiments on the following tasks .,Experiment,Settings,machine-translation,6,19,1,0,,0.001591598,0,negative,2.07E-05,2.78E-06,1.27E-05,2.51E-06,4.57E-05,0.27318176,0.000508865,0.002505222,3.02E-07,0.72368603,7.81E-07,2.96E-05,3.09E-06
7646,machine-translation6,177,"Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation .",Experiment,Settings,machine-translation,6,20,1,1,experiments,0.257938913,0,experimental-setup,7.03E-05,2.33E-05,0.001701508,3.24E-06,2.35E-05,0.713739037,0.002551608,0.012848815,1.79E-05,0.268887474,1.77E-05,8.21E-05,3.34E-05
7647,machine-translation6,178,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .",Experiment,Settings,machine-translation,6,21,1,1,experiments,0.978649737,1,experimental-setup,7.73E-07,2.85E-07,2.15E-06,1.28E-07,1.44E-07,0.980638844,0.000403783,0.017784784,1.30E-07,0.001167356,1.09E-08,3.03E-07,1.31E-06
7648,machine-translation6,179,"We test the baseline and our method on three datasets : RG65 , WS and RW .",Experiment,Settings,machine-translation,6,22,1,1,experiments,0.037765295,0,experimental-setup,7.79E-05,2.95E-05,0.000318585,1.34E-06,5.10E-05,0.55616369,0.005428245,0.009715043,2.20E-06,0.427917121,1.31E-06,0.000281068,1.30E-05
7649,machine-translation6,180,The RW dataset is a dataset for the evaluation of rare words .,Experiment,Settings,machine-translation,6,23,1,0,,0.301016543,0,negative,8.33E-05,1.33E-05,0.001862386,2.58E-05,0.002395473,0.418769381,0.010973072,0.001693578,9.16E-07,0.563638339,5.10E-06,0.000451624,8.78E-05
7650,machine-translation6,181,"Following common practice , we use cosine distance while computing the similarity between two word embeddings .",Experiment,Settings,machine-translation,6,24,1,0,,0.067750021,0,experimental-setup,5.67E-05,6.38E-05,0.00121256,7.29E-07,5.26E-06,0.914460251,0.001675599,0.024643436,2.66E-05,0.057826056,8.41E-07,1.73E-05,1.09E-05
7651,machine-translation6,182,Language Modeling is a basic task in natural language processing .,Experiment,Settings,machine-translation,6,25,1,1,experiments,0.819761501,1,negative,0.000199025,1.93E-05,0.000338836,0.000219644,0.000323469,0.296258969,0.091045862,0.004855913,2.71E-06,0.595797744,0.006621131,0.001862395,0.002454995
7652,machine-translation6,183,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,Experiment,Settings,machine-translation,6,26,1,1,experiments,0.062042019,0,experimental-setup,2.79E-05,3.88E-05,0.000227802,3.20E-06,1.68E-05,0.66738711,0.001581129,0.029118171,3.15E-05,0.301468729,2.22E-05,3.47E-05,4.19E-05
7653,machine-translation6,184,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .",Experiment,Settings,machine-translation,6,27,1,1,experiments,0.102780522,0,experimental-setup,0.000282016,0.000164982,0.000845106,1.16E-05,0.000706812,0.532678523,0.009833357,0.006829908,6.61E-06,0.44776703,2.55E-06,0.000815361,5.62E-05
7654,machine-translation6,185,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .",Experiment,Settings,machine-translation,6,28,1,1,experiments,0.966878903,1,experimental-setup,0.000157218,6.32E-05,0.004758631,3.41E-06,0.000132641,0.820329103,0.021323357,0.01006466,7.07E-06,0.142665283,8.88E-07,0.000437762,5.67E-05
7655,machine-translation6,186,Machine Translation is a popular task in both deep learning and natural language processing .,Experiment,Settings,machine-translation,6,29,1,1,experiments,0.823339141,1,negative,0.000131878,1.42E-05,0.000205189,0.000146497,0.000232996,0.381774104,0.073386499,0.005196189,1.96E-06,0.532858058,0.002850779,0.001311927,0.001889725
7656,machine-translation6,187,"We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .",Experiment,Settings,machine-translation,6,30,1,1,experiments,0.186231333,0,experimental-setup,5.83E-05,3.26E-05,0.000460445,2.23E-06,0.000213327,0.509465051,0.008260808,0.006827941,2.23E-06,0.47430834,8.25E-07,0.000339959,2.80E-05
7657,machine-translation6,188,"We use Transformer as the baseline model , which achieves state - of - the - art accuracy on multiple translation datasets .",Experiment,Settings,machine-translation,6,31,1,1,experiments,0.95524003,1,experimental-setup,0.000103991,3.46E-05,0.002073872,5.53E-07,9.44E-06,0.930545574,0.016547932,0.019536635,7.76E-06,0.03101167,1.92E-07,9.55E-05,3.23E-05
7658,machine-translation6,189,We use transformer_base and transformer_big configurations following tensor2 tensor .,Experiment,Settings,machine-translation,6,32,1,0,,0.873604196,1,experimental-setup,1.15E-05,1.70E-06,6.71E-05,3.16E-07,2.36E-06,0.979130192,0.001269277,0.009600329,1.47E-06,0.009907729,1.25E-08,3.55E-06,4.49E-06
7659,machine-translation6,190,Text Classification is a conventional machine learning task and is evaluated by accuracy .,Experiment,Settings,machine-translation,6,33,1,1,experiments,0.135533682,0,negative,4.17E-05,6.29E-06,8.41E-05,7.96E-05,0.000121303,0.469913192,0.019624841,0.004849331,1.07E-06,0.50338731,0.000832395,0.000339702,0.000719212
7660,machine-translation6,191,"Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) .",Experiment,Settings,machine-translation,6,34,1,1,experiments,0.732582206,1,experimental-setup,0.000420898,0.000554778,0.002807389,1.29E-05,0.0002932,0.83965349,0.030027905,0.025094487,4.09E-05,0.099976452,2.63E-06,0.00085055,0.000264419
7661,machine-translation6,192,"In all tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words , which is the same as our empirical study .",Experiment,Settings,machine-translation,6,35,1,0,,0.061690381,0,experimental-setup,8.56E-06,3.99E-06,1.77E-05,1.64E-07,6.83E-06,0.884444441,0.001486674,0.016817685,7.76E-07,0.097203567,1.84E-08,6.82E-06,2.74E-06
7662,machine-translation6,193,"For all the tasks except word embedding , we use full - batch gradient descent to update the discriminator .",Experiment,Settings,machine-translation,6,36,1,0,,0.932252162,1,experimental-setup,8.87E-06,7.60E-06,1.06E-05,4.24E-07,1.65E-06,0.954222301,0.001285656,0.041366379,2.47E-06,0.003081377,2.26E-08,2.01E-06,1.06E-05
7663,machine-translation6,194,"For word embedding , mini- batch stochastic gradient descent is used to update the discriminator with a batch size 3000 , since the vocabulary size is large .",Experiment,Settings,machine-translation,6,37,1,0,,0.971986501,1,experimental-setup,1.03E-06,6.11E-07,7.48E-07,1.40E-07,3.04E-07,0.960973203,0.000646173,0.037415854,1.61E-07,0.000957263,6.88E-09,4.69E-07,4.03E-06
7664,machine-translation6,195,"For language modeling and machine translation tasks , we use logistic regression as the discriminator .",Experiment,Settings,machine-translation,6,38,1,0,,0.90860485,1,experimental-setup,1.99E-06,7.21E-07,6.25E-06,5.56E-08,4.57E-07,0.978177149,0.001325909,0.016681943,2.10E-07,0.003800765,6.42E-09,1.42E-06,3.12E-06
7665,machine-translation6,196,"For other tasks , we find using a shallow neural network with 5 https://github.com/tensorflow/models/blob/master/tutorials/embedding",Experiment,Settings,machine-translation,6,39,1,0,,0.25154621,0,experimental-setup,0.000165214,1.82E-05,0.002744813,2.37E-06,2.94E-05,0.909990249,0.004453612,0.008955364,9.46E-06,0.073484714,3.90E-07,9.69E-05,4.94E-05
7666,machine-translation6,197,6 http://mattmahoney.net/dc/textdata.html,Experiment,Settings,machine-translation,6,40,1,0,,0.017510858,0,experimental-setup,1.56E-05,3.62E-07,6.37E-06,0.000224186,0.000365084,0.694688662,0.000221688,0.000617173,1.74E-07,0.303842191,7.56E-08,6.21E-06,1.23E-05
7667,machine-translation6,198,7 https://github.com/salesforce/awd-lstm-lm,Experiment,Settings,machine-translation,6,41,1,0,,0.89908668,1,experimental-setup,1.20E-05,2.76E-07,7.33E-06,0.024278661,0.000422029,0.951811207,0.00023773,0.000246893,1.16E-07,0.022926926,6.44E-08,1.48E-06,5.53E-05
7668,machine-translation6,199,8 https://github.com/zihangdai/mos,Experiment,Settings,machine-translation,6,42,1,0,,0.870869683,1,experimental-setup,1.44E-05,3.59E-07,6.21E-06,0.007445318,0.00047196,0.905128186,0.000167099,0.000334695,1.40E-07,0.086403028,5.20E-08,1.94E-06,2.66E-05
7669,machine-translation6,200,9 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl,Experiment,Settings,machine-translation,6,43,1,0,,0.938603163,1,experimental-setup,4.53E-06,1.54E-07,2.43E-06,0.02682732,0.000231019,0.964025044,0.000149437,0.000208431,4.70E-08,0.008516861,2.45E-08,5.80E-07,3.41E-05
7670,machine-translation6,201,"To improve the training for imbalanced labeled data , a common method is to adjust loss function by reweighting the training samples ; To regularize the parameter space , a common method is to use l 2 regularization .",Experiment,Settings,machine-translation,6,44,1,0,,0.037404799,0,experimental-setup,3.56E-05,6.31E-06,2.86E-05,1.11E-05,3.12E-05,0.757342868,0.001482884,0.013357369,1.31E-06,0.227557944,5.27E-06,5.70E-05,8.26E-05
7671,machine-translation6,202,We tested these methods in machine translation and found the performance is not good .,Experiment,Settings,machine-translation,6,45,1,0,,0.04321548,0,experimental-setup,1.25E-05,6.76E-07,1.53E-05,3.24E-06,5.83E-05,0.524481955,0.004397738,0.00215542,1.06E-07,0.468681181,1.16E-06,0.000133033,5.94E-05
7672,machine-translation6,203,Detailed analysis is provided in the supplementary material ( part B ) .,Experiment,Settings,machine-translation,6,46,1,0,,0.006350757,0,negative,2.42E-05,1.98E-06,1.13E-05,5.50E-06,0.000156424,0.291185252,0.000302873,0.002014024,8.59E-07,0.706269253,4.03E-08,2.28E-05,5.57E-06
7673,machine-translation6,204,https://github.com/brightmart/text_classification one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size .,Experiment,Settings,machine-translation,6,47,1,0,,0.957264388,1,experimental-setup,3.68E-06,2.68E-07,1.92E-06,0.003698385,0.000111623,0.99244897,0.000190655,0.000546268,3.64E-08,0.002966805,6.07E-09,5.60E-07,3.08E-05
7674,machine-translation6,205,"In all tasks , we set the hyper - parameter ? to 0.1 .",Experiment,Settings,machine-translation,6,48,1,0,,0.931492364,1,experimental-setup,1.80E-06,6.82E-07,3.29E-07,8.33E-08,3.97E-07,0.951103201,0.001064458,0.045403249,1.32E-07,0.002418978,4.20E-09,1.51E-06,5.18E-06
7675,machine-translation6,206,We list other hyper - parameters related to different task - specific models in the supplementary material ( part A ) .,Experiment,Settings,machine-translation,6,49,1,0,,0.014079206,0,experimental-setup,1.22E-05,1.42E-06,4.81E-06,1.04E-06,1.08E-05,0.611563681,0.000383726,0.009626317,1.00E-06,0.378381234,2.03E-08,9.68E-06,4.05E-06
7676,machine-translation6,207,"In this subsection , we provide the experimental results of all tasks .",Experiment,Settings,machine-translation,6,50,1,0,,0.018805848,0,negative,1.74E-05,1.82E-06,4.02E-06,2.78E-07,2.31E-05,0.135312958,0.00121936,0.002589482,2.74E-07,0.860692305,2.62E-08,0.000135099,3.93E-06
7677,machine-translation6,208,"For simplicity , we use "" with FRAGE "" as our proposed method in the tables .",Experiment,Settings,machine-translation,6,51,1,0,,0.024616273,0,experimental-setup,1.84E-06,1.09E-06,2.89E-06,1.17E-07,2.81E-06,0.824500462,0.000824331,0.014317609,4.79E-07,0.160341671,9.74E-09,3.75E-06,2.93E-06
7678,machine-translation6,209,Results,,,machine-translation,6,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
7679,machine-translation6,210,RG65,Results,,machine-translation,6,1,1,0,,0.022128287,0,negative,0.067814926,7.57E-05,0.007895294,0.001674131,7.03E-05,0.002111305,0.004212772,0.002836952,0.000762112,0.83623264,0.00035825,0.064276765,0.011678878
7680,machine-translation6,211,Word Similarity,Results,,machine-translation,6,2,1,0,,0.946993633,1,results,0.00650443,3.37E-05,0.019021541,3.11E-05,1.20E-05,0.000621606,0.01842175,0.001772434,6.27E-05,0.385054208,0.000943727,0.560487177,0.007033559
7681,machine-translation6,212,The results on three word similarity tasks are listed in .,Results,Word Similarity,machine-translation,6,3,1,0,,0.010857637,0,results,0.000228911,1.28E-08,2.07E-05,9.73E-09,3.98E-09,7.01E-07,7.52E-05,1.94E-06,1.47E-08,0.378212658,1.19E-06,0.621434817,2.38E-05
7682,machine-translation6,213,""" Paras "" denotes the number of model parameters .",Results,Word Similarity,machine-translation,6,4,1,0,,0.000184604,0,negative,5.90E-05,3.60E-07,2.41E-05,2.12E-07,2.10E-08,9.70E-06,5.61E-06,8.32E-05,5.72E-06,0.99910013,1.03E-06,0.000686792,2.40E-05
7683,machine-translation6,214,Language Modeling,,,machine-translation,6,0,1,1,results,0.920300461,1,experiments,0.005180313,0.000515319,0.00106481,0.004971848,0.001161773,0.001865126,0.369166679,0.002520426,4.00E-05,0.220467323,0.270090172,0.115815941,0.00714026
7684,machine-translation6,215,The results of language modeling on PTB and WT2 datasets are presented in .,Language Modeling,Language Modeling,machine-translation,6,1,1,0,,0.616146081,1,experiments,0.000346589,2.01E-07,9.14E-06,1.92E-08,9.68E-08,1.41E-05,0.970695712,6.17E-07,1.24E-07,0.026540501,4.36E-06,0.002260848,0.000127663
7685,machine-translation6,216,"We test our model and the baselines at several checkpoints used in the baseline papers : without finetune , with finetune , with post -process ( continuous cache pointer or dynamic evaluation ) .",Language Modeling,Language Modeling,machine-translation,6,2,1,0,,0.284642958,0,experiments,0.000153066,1.27E-06,9.80E-06,2.20E-07,1.06E-06,0.000452793,0.950729337,1.18E-05,1.83E-06,0.048478453,4.19E-07,4.27E-05,0.000117343
7686,machine-translation6,217,"In all these settings , our method outperforms the two baselines .",Language Modeling,Language Modeling,machine-translation,6,3,1,1,results,0.743899484,1,experiments,0.000495768,6.74E-08,5.18E-07,1.47E-07,1.40E-07,3.03E-05,0.992707992,1.92E-06,4.32E-08,0.004953608,4.01E-07,0.001526479,0.000282598
7687,machine-translation6,218,"On PTB dataset , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .",Language Modeling,Language Modeling,machine-translation,6,4,1,1,results,0.902256303,1,experiments,0.001029059,9.73E-08,1.85E-06,5.31E-08,8.89E-08,1.56E-05,0.99382472,9.55E-07,5.47E-08,0.002957681,2.12E-07,0.001892801,0.00027681
7688,machine-translation6,219,"On WT2 dataset , which contains more rare words , our method achieves larger improvements .",Language Modeling,Language Modeling,machine-translation,6,5,1,1,results,0.775385909,1,experiments,0.001969187,7.03E-08,8.95E-07,7.44E-08,1.18E-07,1.27E-05,0.989905843,1.11E-06,2.56E-08,0.003900544,3.12E-07,0.003896816,0.000312318
7689,machine-translation6,220,"We improve the results of AWD - LSTM and AWD - LSTM - MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity , respectively .",Language Modeling,Language Modeling,machine-translation,6,6,1,1,results,0.943168823,1,experiments,0.000977045,1.12E-07,1.60E-06,4.87E-08,1.43E-07,1.20E-05,0.994324259,6.99E-07,7.40E-08,0.002831657,1.30E-07,0.00156184,0.000290358
7690,machine-translation6,221,Machine Translation,Language Modeling,,machine-translation,6,7,1,1,results,0.981613437,1,experiments,7.94E-05,5.99E-08,1.76E-06,1.59E-06,2.09E-06,3.53E-05,0.989940909,2.95E-07,2.62E-08,0.003649391,3.74E-06,0.000272526,0.00601292
7691,machine-translation6,222,The results of neural machine translation on WMT14 English - German and IWSLT14 German - English tasks are shown in .,Language Modeling,Machine Translation,machine-translation,6,8,1,0,,0.6710799,1,experiments,0.000376328,5.32E-07,5.75E-05,2.30E-07,5.11E-07,6.81E-05,0.731070482,4.56E-06,5.26E-07,0.24667465,0.000103151,0.020486837,0.001156625
7692,machine-translation6,223,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,Language Modeling,Machine Translation,machine-translation,6,9,1,1,results,0.796853547,1,experiments,0.002247397,3.56E-07,1.15E-05,3.03E-07,3.77E-07,7.66E-05,0.892906096,1.04E-05,2.74E-07,0.056837744,2.50E-06,0.047352468,0.000553932
7693,machine-translation6,224,"task , respectively .",Language Modeling,Machine Translation,machine-translation,6,10,1,0,,0.62533517,1,negative,7.64E-05,1.83E-07,8.43E-06,5.41E-08,5.43E-07,3.52E-05,0.006832191,2.40E-06,1.25E-06,0.992731739,7.98E-07,0.000304821,5.94E-06
7694,machine-translation6,225,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,Language Modeling,Machine Translation,machine-translation,6,11,1,1,results,0.95536019,1,experiments,0.002005429,2.70E-07,6.14E-06,3.57E-07,3.55E-07,8.90E-05,0.930283511,1.24E-05,2.23E-07,0.038096641,1.54E-06,0.02882905,0.00067515
7695,machine-translation6,226,These results show improving word embeddings can achieve better results in more complicated tasks and larger datasets .,Language Modeling,Machine Translation,machine-translation,6,12,1,0,,0.313204589,0,negative,0.003889966,3.05E-07,7.57E-06,1.73E-07,4.46E-07,7.99E-05,0.227837289,8.47E-06,4.56E-07,0.746802177,2.70E-06,0.021296615,7.39E-05
7696,machine-translation6,227,Text Classification,Language Modeling,,machine-translation,6,13,1,1,results,0.863846602,1,experiments,6.44E-05,2.54E-07,4.94E-06,9.94E-07,2.55E-06,0.00021909,0.949516071,2.74E-06,3.79E-07,0.046312437,8.41E-06,0.00020959,0.00365811
7697,machine-translation6,228,The results are listed in .,Language Modeling,Text Classification,machine-translation,6,14,1,0,,0.002963156,0,negative,3.20E-05,6.48E-08,2.77E-06,2.76E-08,4.35E-07,1.30E-05,0.000684016,1.17E-06,3.22E-07,0.999113576,2.91E-07,0.000151767,5.69E-07
7698,machine-translation6,229,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,Language Modeling,Text Classification,machine-translation,6,15,1,1,results,0.884517263,1,experiments,0.006424992,3.91E-06,3.14E-05,3.16E-06,4.74E-06,0.000311152,0.65188976,7.76E-05,1.60E-06,0.208625602,1.13E-05,0.131655744,0.000958974
7699,machine-translation6,230,"As a summary , our experiments on four different tasks with 10 datasets verify the effectiveness of our method .",Language Modeling,Text Classification,machine-translation,6,16,1,0,,0.303890468,0,negative,0.000966176,7.93E-07,2.17E-06,9.81E-07,2.24E-06,0.000211339,0.065308217,2.84E-05,8.21E-07,0.924491266,3.66E-06,0.008931795,5.21E-05
7700,machine-translation6,231,"We provide some case studies and visualizations of our method in the supplementary material ( part C ) , which show that the semantic similarities are reasonable and the popular / rare words are better mixed together in the embedding space .",Language Modeling,Text Classification,machine-translation,6,17,1,0,,0.047768548,0,negative,1.20E-05,9.56E-08,6.49E-07,8.75E-07,2.48E-06,5.86E-05,0.000259978,2.21E-06,2.00E-07,0.999637651,1.89E-07,2.42E-05,8.92E-07
7701,machine-translation6,232,Conclusion,,,machine-translation,6,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
7702,machine-translation6,269,A.3 Models Description,,,machine-translation,6,0,1,0,,0.040655104,0,negative,8.95E-05,8.89E-05,1.70E-05,9.86E-07,6.02E-07,5.93E-05,6.74E-05,0.000795212,8.83E-05,0.993604707,0.004526117,0.00065625,5.79E-06
7703,machine-translation6,270,We use task - specific baseline models .,A.3 Models Description,A.3 Models Description,machine-translation,6,1,1,0,,0.00036045,0,negative,7.29E-05,0.000173085,0.000508431,4.39E-07,1.90E-06,5.30E-05,2.22E-05,0.000507126,0.000175299,0.9983516,4.94E-05,8.40E-05,5.92E-07
7704,machine-translation6,271,"In language modeling , AWD - LSTM is a weight - dropped LSTM which uses Drop Connect on hidden - to - hidden weights as a means of recurrent regularization .",A.3 Models Description,A.3 Models Description,machine-translation,6,2,1,0,,0.037310322,0,negative,0.000206231,0.001308275,0.009705424,4.64E-05,1.18E-05,0.000202567,0.000461244,0.000992836,0.001024276,0.541904694,0.443539802,0.000442346,0.000154121
7705,machine-translation6,272,"The model is trained by NT - ASGD , which is a variant of the averaged stochastic gradient method .",A.3 Models Description,A.3 Models Description,machine-translation,6,3,1,0,,0.01635168,0,negative,0.00020936,0.009702768,0.005050898,3.09E-06,7.70E-06,0.000172525,4.48E-05,0.006334453,0.031196714,0.946085074,0.001130302,5.43E-05,8.06E-06
7706,machine-translation6,273,"The training process has two steps , in the second step , the model is finetuned using another configuration of NT - ASGD .",A.3 Models Description,A.3 Models Description,machine-translation,6,4,1,0,,0.004434296,0,negative,0.00178374,0.004572575,0.002830979,2.19E-06,2.35E-05,2.39E-05,1.98E-05,0.000521354,0.005841959,0.983848487,0.000208645,0.000319529,3.39E-06
7707,machine-translation6,274,AWD - LSTM - MoS uses the Mixture of Softmaxes structure to the vanilla AWD - LSTM and achieves the state - of - the - art result on PTB and WT2 .,A.3 Models Description,A.3 Models Description,machine-translation,6,5,1,0,,0.265166193,0,negative,0.00740294,0.001846656,0.089736833,5.27E-06,1.94E-05,0.000132945,0.001516134,0.000895162,0.000599965,0.772627854,0.008324631,0.116812434,7.98E-05
7708,machine-translation6,275,"For machine translation , Transformer is a recently developed architecture in which the selfattention network is used during encoding and decoding step .",A.3 Models Description,A.3 Models Description,machine-translation,6,6,1,0,,0.002060643,0,negative,0.00032738,0.000536564,0.006349884,1.15E-05,6.94E-06,6.41E-05,0.000138199,0.000274156,0.001618197,0.842900183,0.147421821,0.000300838,5.02E-05
7709,machine-translation6,276,"It achieves the best performances on several machine translation tasks , e.g.",A.3 Models Description,A.3 Models Description,machine-translation,6,7,1,0,,0.025583279,0,negative,0.001380092,3.89E-05,0.000361226,1.01E-06,3.92E-06,1.34E-05,0.000430051,8.31E-05,5.32E-06,0.875292761,0.003085193,0.119291157,1.40E-05
7710,machine-translation6,277,"WMT14 English - German , WMT14 English - French datasets .",A.3 Models Description,A.3 Models Description,machine-translation,6,8,1,0,,0.006111271,0,negative,0.000580332,0.00010754,0.003855281,2.53E-06,1.94E-05,4.65E-05,0.000855658,0.000221499,2.76E-05,0.95414037,0.001206462,0.038910072,2.69E-05
7711,machine-translation6,278,Word2vec is one of the pioneer works on using deep learning to NLP tasks .,A.3 Models Description,A.3 Models Description,machine-translation,6,9,1,0,,0.002641491,0,negative,0.000136868,0.000126104,0.000881919,1.59E-05,6.19E-06,0.000111004,0.000330842,0.000446021,9.92E-05,0.900018346,0.09695058,0.00078104,9.60E-05
7712,machine-translation6,279,"Based on the co-occurrence of words , it produces distributed representations of words ( word embeddings ) .",A.3 Models Description,A.3 Models Description,machine-translation,6,10,1,0,,0.004306667,0,negative,0.000243019,0.001497961,0.009854856,2.11E-07,1.89E-06,7.97E-06,6.34E-06,9.44E-05,0.020090467,0.967755395,0.000293473,0.000152207,1.75E-06
7713,machine-translation6,280,"RCNN contains both recurrent and convolutional layers to catch the key components in texts , and is widely used in text classification tasks .",A.3 Models Description,A.3 Models Description,machine-translation,6,11,1,0,,0.012128174,0,negative,9.78E-05,0.000367283,0.002304351,7.23E-06,3.93E-06,6.61E-05,0.000134622,0.000487104,0.00059557,0.90222937,0.093359988,0.000292183,5.45E-05
7714,machine-translation6,281,B Additional Comparisons,A.3 Models Description,,machine-translation,6,12,1,0,,4.74E-05,0,negative,0.000135222,3.17E-06,3.51E-05,1.41E-08,6.33E-08,1.36E-06,4.71E-06,1.36E-05,9.19E-06,0.998937272,3.64E-05,0.000823662,1.87E-07
7715,machine-translation6,282,"We compare some other simple methods with ours on machine translation tasks , which include reweighting method and l 2 regularization ( weight decay ) .",A.3 Models Description,B Additional Comparisons,machine-translation,6,13,1,0,,0.041519481,0,negative,0.00094726,0.003293499,0.028842176,4.38E-06,0.000101421,0.000196665,0.000696278,0.000681479,0.000144628,0.961464006,0.000608531,0.003001459,1.82E-05
7716,machine-translation6,283,Results are listed in .,A.3 Models Description,B Additional Comparisons,machine-translation,6,14,1,0,,4.91E-06,0,negative,0.000180143,7.06E-07,7.48E-05,3.69E-08,1.02E-06,3.72E-06,8.87E-06,6.57E-06,4.55E-07,0.999228529,1.55E-06,0.000493466,1.49E-07
7717,machine-translation6,284,"We notice that those simple methods do notwork for the tasks , even have negative effects . : BLEU scores on test set of the WMT14 English - German task and IWSLT14 German - English task .",A.3 Models Description,B Additional Comparisons,machine-translation,6,15,1,0,,2.72E-05,0,negative,0.000937871,2.24E-06,3.61E-05,4.54E-07,9.41E-07,3.33E-05,0.000212973,0.000109299,1.43E-06,0.990542796,0.000200164,0.007915797,6.57E-06
7718,machine-translation6,285,WMT,A.3 Models Description,,machine-translation,6,16,1,0,,0.007885444,0,negative,0.001655014,5.05E-05,0.000677013,8.41E-06,1.52E-05,6.33E-05,0.000277202,0.00022717,0.000128036,0.99105409,0.000434682,0.005375664,3.38E-05
7719,machine-translation6,286,"Our method is denoted as "" FRAGE "" , "" Reweighting "" denotes reweighting the loss of each word by reciprocal of its frequency , and "" Weight Decay "" denotes putting weight decay rate ( 0.2 ) on embeddings .",A.3 Models Description,WMT,machine-translation,6,17,1,0,,1.91E-05,0,negative,4.10E-06,6.22E-05,1.81E-05,2.56E-07,9.98E-07,1.64E-05,3.40E-06,0.000244796,0.0002895,0.99935403,3.32E-06,2.52E-06,3.54E-07
7720,machine-translation6,287,C Case Study on Original Models and Qualitative Analysis of Our Method,A.3 Models Description,WMT,machine-translation,6,18,1,0,,9.31E-05,0,negative,3.61E-05,9.86E-06,1.09E-05,3.29E-07,3.99E-07,4.50E-06,1.90E-05,2.70E-05,3.86E-05,0.998897547,0.000640318,0.000312487,2.95E-06
7721,machine-translation6,288,We provide more word similarity cases in to justify our statement in Section 3 .,A.3 Models Description,WMT,machine-translation,6,19,1,0,,3.55E-06,0,negative,0.000202383,1.35E-06,2.72E-06,3.87E-08,8.89E-07,1.16E-06,1.14E-06,5.21E-06,2.40E-06,0.999711645,2.59E-07,7.08E-05,3.52E-08
7722,machine-translation6,289,We also present the effectiveness of our method by showcase and embedding visualizations .,A.3 Models Description,WMT,machine-translation,6,20,1,0,,2.19E-05,0,negative,0.000223834,1.97E-05,1.26E-05,7.90E-08,3.65E-06,1.10E-06,3.10E-06,5.62E-06,9.30E-06,0.999300809,6.42E-07,0.000419582,7.16E-08
7723,machine-translation6,290,"From the cases and visualizations in and , we find the word similarities are improved and popular / rare words are better mixed together .",A.3 Models Description,WMT,machine-translation,6,21,1,0,,0.023020579,0,negative,0.021885236,7.51E-06,2.99E-05,5.47E-07,2.46E-06,6.95E-06,0.000122536,3.96E-05,7.15E-06,0.900672328,1.08E-05,0.077211616,3.35E-06
7724,machine-translation6,291,"( a ) ( b ) : These figures show that , in different tasks , the embeddings of rare and popular words are better mixed together after applying our method .",A.3 Models Description,WMT,machine-translation,6,22,1,0,,0.015167354,0,negative,0.011724308,1.04E-05,1.00E-04,1.93E-07,1.29E-06,5.47E-06,0.000124451,2.90E-05,1.27E-05,0.938753629,2.73E-05,0.049209257,2.10E-06
7725,machine-translation9,1,title,,,machine-translation,9,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
7726,machine-translation9,2,Under review as a conference paper at ICLR 2018 COMPRESSING WORD EMBEDDINGS VIA DEEP COMPOSITIONAL CODE LEARNING,title,title,machine-translation,9,1,1,1,research-problem,0.997781974,1,research-problem,2.78E-08,1.46E-05,3.08E-08,7.37E-07,1.17E-07,7.11E-07,1.05E-06,5.88E-06,2.98E-06,0.007960908,0.992012822,5.13E-08,9.70E-08
7727,machine-translation9,3,abstract,,,machine-translation,9,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
7728,machine-translation9,4,"Natural language processing ( NLP ) models often require a massive number of parameters for word embeddings , resulting in a large storage or memory footprint .",abstract,abstract,machine-translation,9,1,1,0,,0.973330374,1,research-problem,1.57E-08,3.48E-06,6.90E-09,1.10E-06,1.58E-07,2.90E-07,4.16E-07,1.32E-06,1.90E-07,0.007665498,0.99232746,1.43E-08,4.67E-08
7729,machine-translation9,5,Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance .,abstract,abstract,machine-translation,9,2,1,1,research-problem,0.760113244,1,research-problem,7.23E-08,1.52E-05,1.78E-08,7.37E-06,9.04E-07,1.95E-06,7.44E-07,7.22E-06,8.18E-07,0.045532703,0.954432848,3.57E-08,1.20E-07
7730,machine-translation9,6,"For this purpose , we propose to construct the embeddings with few basis vectors .",abstract,abstract,machine-translation,9,3,1,0,,0.190506754,0,negative,4.31E-05,0.251719721,1.60E-05,0.000114582,0.000684347,0.000293926,1.49E-05,0.007781523,0.008037457,0.684645599,0.046641337,3.74E-06,3.70E-06
7731,machine-translation9,7,"For each word , the composition of basis vectors is determined by a hash code .",abstract,abstract,machine-translation,9,4,1,0,,0.08201726,0,negative,7.33E-06,0.132192212,6.67E-05,3.37E-06,9.11E-05,6.84E-05,5.11E-06,0.001182826,0.103537884,0.730257575,0.032585681,1.22E-06,6.22E-07
7732,machine-translation9,8,"To maximize the compression rate , we adopt the multi-codebook quantization approach instead of binary coding scheme .",abstract,abstract,machine-translation,9,5,1,0,,0.202650028,0,approach,0.000120501,0.673019804,0.000204636,7.35E-05,0.000904197,0.000115476,2.39E-05,0.001629604,0.056945066,0.235690472,0.031250304,1.65E-05,6.04E-06
7733,machine-translation9,9,"Each code is composed of multiple discrete numbers , such as ( 3 , 2 , 1 , 8 ) , where the value of each component is limited to a fixed range .",abstract,abstract,machine-translation,9,6,1,0,,0.086613996,0,negative,1.99E-06,0.016662222,5.56E-06,1.20E-05,0.000100949,0.000923053,1.31E-05,0.011981033,0.006411784,0.95129493,0.012591796,6.54E-07,8.67E-07
7734,machine-translation9,10,We propose to directly learn the discrete codes in an end - to - end neural network by applying the Gumbel - softmax trick .,abstract,abstract,machine-translation,9,7,1,0,,0.26096214,0,approach,9.92E-05,0.523044052,0.000113872,9.80E-05,0.001053711,8.16E-05,2.18E-05,0.001134607,0.026217962,0.366016164,0.082100642,1.29E-05,5.50E-06
7735,machine-translation9,11,Experiments show the compression rate achieves 98 % in a sentiment analysis task and 94 % ? 99 % in machine translation tasks without performance loss .,abstract,abstract,machine-translation,9,8,1,0,,0.01465108,0,negative,0.000137745,0.004446819,1.07E-05,3.08E-05,0.000203643,3.84E-05,9.34E-05,0.000514616,0.000140019,0.732669704,0.261416569,0.00029047,7.13E-06
7736,machine-translation9,12,"In both tasks , the proposed method can improve the model performance by slightly lowering the compression rate .",abstract,abstract,machine-translation,9,9,1,0,,0.043844572,0,negative,0.002282022,0.003545056,6.29E-06,0.000167129,0.000258492,0.000100371,0.000212173,0.001573093,9.67E-05,0.904875515,0.085958594,0.000909674,1.49E-05
7737,machine-translation9,13,"Compared to other approaches such as character - level segmentation , the proposed method is language - independent and does not require modifications to the network architecture .",abstract,abstract,machine-translation,9,10,1,0,,0.035557248,0,negative,6.62E-05,0.245028354,3.95E-05,3.50E-05,0.000488488,5.15E-05,4.21E-05,0.001311913,0.010591225,0.516442721,0.225840191,5.69E-05,6.06E-06
7738,machine-translation9,14,INTRODUCTION,,,machine-translation,9,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
7739,machine-translation9,15,Word embeddings play an important role in neural - based natural language processing ( NLP ) models .,INTRODUCTION,INTRODUCTION,machine-translation,9,1,1,0,,0.842610687,1,research-problem,7.35E-07,0.000235497,3.79E-07,1.61E-06,2.64E-06,5.41E-06,7.74E-06,1.18E-05,0.000204763,0.040611512,0.958915845,9.25E-07,1.11E-06
7740,machine-translation9,16,Neural word embeddings encapsulate the linguistic information of words in continuous vectors .,INTRODUCTION,INTRODUCTION,machine-translation,9,2,1,0,,0.937374023,1,model,2.87E-06,0.024475772,2.70E-05,4.83E-07,1.57E-05,2.95E-05,9.10E-06,9.80E-05,0.939974966,0.023594812,0.011769489,1.19E-06,1.13E-06
7741,machine-translation9,17,"However , as each word is assigned an independent embedding vector , the number of parameters in the embedding matrix can be huge .",INTRODUCTION,INTRODUCTION,machine-translation,9,3,1,0,,0.118511278,0,negative,4.59E-05,0.018314695,3.35E-06,2.27E-05,0.000176584,0.000144247,3.56E-05,0.000430246,0.02352185,0.810247506,0.147041067,1.15E-05,4.72E-06
7742,machine-translation9,18,"For example , when each embedding has 500 dimensions , the network has to hold 100M embedding parameters to represent 200K words .",INTRODUCTION,INTRODUCTION,machine-translation,9,4,1,0,,0.117440171,0,negative,7.98E-05,0.146215572,2.35E-05,7.43E-05,0.001828573,0.005911646,0.000737663,0.014627862,0.134160924,0.666300306,0.029963365,3.14E-05,4.50E-05
7743,machine-translation9,19,"In practice , for a simple sentiment analysis model , the word embedding parameters account for 98.8 % of the total parameters .",INTRODUCTION,INTRODUCTION,machine-translation,9,5,1,0,,0.77565578,1,negative,0.000569494,0.160133156,2.50E-05,0.000380145,0.018532752,0.002579248,0.001048653,0.005500374,0.020162932,0.7789392,0.01194314,0.000134744,5.12E-05
7744,machine-translation9,20,"As only a small portion of the word embeddings is selected in the forward pass , the giant embedding matrix usually does not cause a speed issue .",INTRODUCTION,INTRODUCTION,machine-translation,9,6,1,0,,0.076035865,0,negative,0.004360683,0.020644413,2.33E-05,6.60E-05,0.001367515,0.000141642,0.000155608,0.000207998,0.005299171,0.952361518,0.015178865,0.000185952,7.27E-06
7745,machine-translation9,21,"However , the massive number of parameters in the neural network results in a large storage or memory footprint .",INTRODUCTION,INTRODUCTION,machine-translation,9,7,1,0,,0.029960591,0,negative,4.80E-05,0.007552301,4.49E-06,2.47E-05,0.000235814,9.99E-05,4.87E-05,0.000169493,0.005066286,0.830546311,0.156180464,1.82E-05,5.38E-06
7746,machine-translation9,22,"When other components of the neural network are also large , the model may fail to fit into GPU memory during training .",INTRODUCTION,INTRODUCTION,machine-translation,9,8,1,0,,0.079546239,0,negative,0.00016894,0.027755556,1.29E-05,2.56E-05,0.000428756,0.000206454,7.18E-05,0.000331597,0.035230194,0.903308951,0.03241204,4.13E-05,5.81E-06
7747,machine-translation9,23,"Moreover , as the demand for low - latency neural computation for mobile platforms increases , some neural - based models are expected to run on mobile devices .",INTRODUCTION,INTRODUCTION,machine-translation,9,9,1,0,,0.104463103,0,negative,9.65E-06,0.005602315,4.51E-06,5.17E-05,0.000162036,0.000223989,7.50E-05,0.000333601,0.004740417,0.564718508,0.424057634,1.03E-05,1.03E-05
7748,machine-translation9,24,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .",INTRODUCTION,INTRODUCTION,machine-translation,9,10,1,1,research-problem,0.678379532,1,negative,9.62E-06,0.002307532,1.65E-06,5.93E-05,0.00011337,0.000118979,4.38E-05,0.000157559,0.001263501,0.539565191,0.456345738,6.75E-06,6.99E-06
7749,machine-translation9,25,"In this study , we attempt to reduce the number of parameters used in word embeddings without hurting the model performance .",INTRODUCTION,INTRODUCTION,machine-translation,9,11,1,0,,0.902726214,1,approach,0.000117216,0.835023583,5.84E-05,4.45E-05,0.003305875,0.000100507,6.90E-05,0.000341609,0.08653473,0.068322567,0.006043942,2.90E-05,9.06E-06
7750,machine-translation9,26,Neural networks are known for the significant redundancy in the connections .,INTRODUCTION,INTRODUCTION,machine-translation,9,12,1,0,,0.005125629,0,negative,4.74E-06,0.003412325,7.17E-06,9.67E-06,4.44E-05,0.000151647,5.90E-05,0.000171886,0.017835446,0.581271438,0.397022032,4.56E-06,5.74E-06
7751,machine-translation9,27,"In this work , we further hypothesize that learning independent embeddings causes more redundancy in the embedding vectors , as the inter-similarity among words is ignored .",INTRODUCTION,INTRODUCTION,machine-translation,9,13,1,0,,0.079625983,0,approach,0.000160856,0.441127514,6.68E-05,3.79E-05,0.002709019,0.00019874,5.47E-05,0.000498428,0.282830855,0.267183428,0.005098212,2.63E-05,7.22E-06
7752,machine-translation9,28,Some words are very similar regarding the semantics .,INTRODUCTION,INTRODUCTION,machine-translation,9,14,1,0,,0.00103112,0,negative,4.43E-05,0.002067848,8.56E-06,6.37E-05,0.008490984,0.000239246,0.000110867,8.61E-05,0.000557582,0.987352229,0.000949016,2.60E-05,3.53E-06
7753,machine-translation9,29,"For example , "" dog "" and "" dogs "" have almost the same meaning , except one is plural .",INTRODUCTION,INTRODUCTION,machine-translation,9,15,1,0,,0.000912649,0,negative,3.31E-06,0.001137811,1.50E-06,6.49E-05,0.000826423,0.000312895,3.72E-05,0.000196386,0.000835813,0.991280079,0.005296939,3.43E-06,3.29E-06
7754,machine-translation9,30,"To efficiently represent these two words , it is desirable to share information between the two embeddings .",INTRODUCTION,INTRODUCTION,machine-translation,9,16,1,0,,0.178785919,0,negative,0.000119475,0.171994512,3.11E-05,2.85E-05,0.000804114,0.00011359,6.38E-05,0.00033981,0.214600558,0.561588704,0.050258954,4.86E-05,8.31E-06
7755,machine-translation9,31,"However , a small portion in both vectors still has to be trained independently to capture the syntactic difference .",INTRODUCTION,INTRODUCTION,machine-translation,9,17,1,0,,0.367657663,0,negative,0.000553427,0.025215368,2.48E-05,2.78E-05,0.00121433,8.74E-05,8.63E-05,0.000148309,0.012837622,0.947987167,0.011726612,8.57E-05,5.04E-06
7756,machine-translation9,32,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .",INTRODUCTION,INTRODUCTION,machine-translation,9,18,1,1,approach,0.17159598,0,model,7.48E-06,0.104902714,2.11E-05,2.30E-06,0.000235674,9.08E-05,1.86E-05,0.00022851,0.795296214,0.098432717,0.000759401,2.70E-06,1.75E-06
7757,machine-translation9,33,Each component,INTRODUCTION,,machine-translation,9,19,1,0,,0.019074209,0,negative,6.02E-05,0.022548438,6.17E-05,2.37E-05,0.000648262,0.00024701,7.97E-05,0.000182334,0.335170414,0.636936405,0.004018616,1.62E-05,7.02E-06
7758,machine-translation9,34,Ci w is an integer number in .,INTRODUCTION,Each component,machine-translation,9,20,1,0,,4.08E-05,0,negative,1.09E-05,0.000822739,1.80E-06,8.31E-07,1.90E-05,1.47E-05,5.28E-07,0.000130055,0.003918209,0.994986253,9.29E-05,1.79E-06,2.87E-07
7759,machine-translation9,35,"Ideally , similar words should have similar codes .",INTRODUCTION,Each component,machine-translation,9,21,1,0,,6.07E-05,0,negative,4.84E-06,0.000919312,2.78E-06,1.53E-07,1.88E-05,4.31E-06,1.91E-07,3.13E-05,0.003449756,0.995524072,4.32E-05,1.25E-06,9.30E-08
7760,machine-translation9,36,"For example , we may desire C dog = ( 3 , 2 , 4 , 1 ) and C dogs = ( 3 , 2 , 4 , 2 ) .",INTRODUCTION,Each component,machine-translation,9,22,1,0,,2.02E-05,0,negative,5.46E-06,0.000398109,1.82E-06,2.99E-07,3.26E-05,1.37E-05,5.02E-07,4.44E-05,0.001478201,0.998004826,1.81E-05,1.90E-06,1.56E-07
7761,machine-translation9,37,"Once we have obtained such compact codes for all words in the vocabulary , we use embedding vectors to represent the codes rather than the unique words .",INTRODUCTION,Each component,machine-translation,9,23,1,1,approach,0.001776263,0,negative,4.03E-05,0.054683851,8.57E-05,1.94E-07,4.91E-05,1.03E-05,1.10E-06,0.000111769,0.462624326,0.482291615,9.88E-05,2.65E-06,3.92E-07
7762,machine-translation9,38,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .",INTRODUCTION,Each component,machine-translation,9,24,1,1,approach,3.45E-05,0,negative,4.19E-05,0.02966399,2.96E-05,9.85E-07,0.00025349,1.56E-05,1.64E-06,0.000197907,0.080995566,0.888735114,5.83E-05,5.14E-06,8.16E-07
7763,machine-translation9,39,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,INTRODUCTION,Each component,machine-translation,9,25,1,1,approach,4.01E-05,0,negative,1.06E-05,0.003451558,7.78E-06,1.12E-07,1.23E-05,5.03E-06,2.96E-07,4.77E-05,0.041638177,0.954726246,9.86E-05,1.38E-06,1.89E-07
7764,machine-translation9,40,( where E i ( C i w ) is the Ci w - th codeword in the codebook E i .,INTRODUCTION,Each component,machine-translation,9,26,1,0,,3.38E-06,0,negative,1.53E-05,0.000232328,4.88E-06,8.92E-08,1.38E-05,1.50E-06,1.22E-07,5.27E-06,0.000671608,0.999042458,1.11E-05,1.49E-06,4.50E-08
7765,machine-translation9,41,"In this way , the number of vectors in the embedding matrix will be M K , which is usually much smaller than the vocabulary size .",INTRODUCTION,Each component,machine-translation,9,27,1,0,,3.43E-05,0,negative,6.08E-05,0.003258656,3.40E-06,6.22E-07,5.64E-05,2.53E-05,1.97E-06,0.000284871,0.005864534,0.990368338,6.81E-05,6.22E-06,8.07E-07
7766,machine-translation9,42,gives an intuitive comparison between the compositional approach and the conventional approach ( assigning unique IDs ) .,INTRODUCTION,Each component,machine-translation,9,28,1,0,,1.41E-05,0,negative,8.73E-06,0.000127213,3.92E-06,4.83E-08,8.02E-06,1.57E-06,1.42E-07,5.29E-06,0.000491179,0.999341169,1.08E-05,1.92E-06,3.22E-08
7767,machine-translation9,43,"The codes of all the words can be stored in an integer matrix , denoted by C. Thus , the storage footprint of the embedding layer now depends on the total size of the combined codebook E and the code matrix C.",INTRODUCTION,Each component,machine-translation,9,29,1,0,,0.000355776,0,negative,2.75E-05,0.005510966,7.14E-06,3.94E-07,3.50E-05,1.51E-05,1.08E-06,0.000141552,0.044215352,0.949944784,9.69E-05,3.52E-06,6.88E-07
7768,machine-translation9,44,"Although the number of embedding vectors can be greatly reduced by using such coding approach , we want to prevent any serious degradation in performance compared to the models using normal embeddings .",INTRODUCTION,Each component,machine-translation,9,30,1,0,,0.000107684,0,negative,0.000136275,0.000555032,1.41E-06,5.57E-07,5.37E-05,2.99E-06,2.61E-07,1.42E-05,0.000486062,0.998726645,1.50E-05,7.72E-06,1.02E-07
7769,machine-translation9,45,"In other words , given a set of baseline word embeddings ? ( w ) , we wish to find a set of codes ?",INTRODUCTION,Each component,machine-translation,9,31,1,0,,9.11E-06,0,negative,7.89E-06,0.000932718,2.77E-06,2.51E-07,2.72E-05,3.07E-06,2.08E-07,2.29E-05,0.002042858,0.99694354,1.50E-05,1.44E-06,1.06E-07
7770,machine-translation9,46,and combined codebook that can produce the embeddings with the same effectiveness as ? ( w ) .,INTRODUCTION,Each component,machine-translation,9,32,1,0,,0.000162938,0,negative,8.82E-05,0.004423869,0.000320842,1.36E-07,5.29E-05,5.40E-06,1.21E-06,2.15E-05,0.050505709,0.94454234,2.36E-05,1.39E-05,3.53E-07
7771,machine-translation9,47,A safe and straight - forward way is to minimize the squared distance between the baseline embeddings and the composed embeddings as,INTRODUCTION,Each component,machine-translation,9,33,1,0,,2.90E-05,0,negative,0.000104434,0.006134165,1.07E-05,3.89E-07,5.12E-05,8.29E-06,8.73E-07,9.90E-05,0.011817569,0.981743124,2.30E-05,6.97E-06,3.29E-07
7772,machine-translation9,48,where | V | is the vocabulary size .,INTRODUCTION,Each component,machine-translation,9,34,1,0,,1.67E-05,0,negative,7.91E-06,0.000819679,1.73E-06,3.03E-07,1.75E-05,1.26E-05,6.80E-07,0.000168717,0.002826917,0.996124091,1.83E-05,1.09E-06,4.02E-07
7773,machine-translation9,49,The baseline embeddings can be a set of pre-trained vectors such as word2vec or GloVe embeddings .,INTRODUCTION,Each component,machine-translation,9,35,1,0,,0.001887641,0,negative,2.32E-05,0.007808607,1.97E-05,5.18E-06,0.000116065,0.00013801,5.42E-06,0.001632463,0.018816681,0.971402966,2.49E-05,2.68E-06,4.20E-06
7774,machine-translation9,50,"In Eq. 3 , the baseline embedding matrix ?",INTRODUCTION,Each component,machine-translation,9,36,1,0,,4.93E-05,0,negative,1.07E-05,0.000993646,2.64E-06,3.94E-07,2.61E-05,1.35E-05,6.52E-07,0.000155254,0.002479856,0.996309923,5.69E-06,1.42E-06,2.54E-07
7775,machine-translation9,51,is approximated by M codewords selected from M codebooks .,INTRODUCTION,Each component,machine-translation,9,37,1,0,,0.000368403,0,negative,4.80E-05,0.007181372,5.57E-05,2.98E-07,9.11E-05,8.19E-06,1.58E-06,7.04E-05,0.037477347,0.955019094,3.69E-05,9.31E-06,7.51E-07
7776,machine-translation9,52,The selection of codewords is controlled by the code C w .,INTRODUCTION,Each component,machine-translation,9,38,1,0,,0.000330246,0,negative,2.16E-05,0.010065054,1.08E-05,3.83E-07,5.43E-05,9.33E-06,8.28E-07,0.000155585,0.061132073,0.928532879,1.46E-05,2.15E-06,4.02E-07
7777,machine-translation9,53,"Such problem of learning compact codes with multiple codebooks is formalized and discussed in the research field of compressionbased source coding , known as product quantization and additive quantization .",INTRODUCTION,Each component,machine-translation,9,39,1,0,,0.000778513,0,negative,9.17E-06,0.001580772,4.18E-06,3.56E-05,0.000116594,5.16E-05,6.50E-06,0.000158873,0.000650651,0.983867119,0.013501429,4.85E-06,1.27E-05
7778,machine-translation9,54,Previous works learn compositional codes so as to enable an efficient similarity search of vectors .,INTRODUCTION,Each component,machine-translation,9,40,1,0,,0.000123369,0,negative,1.00E-05,0.001130479,6.11E-06,3.87E-06,6.09E-05,2.82E-05,2.90E-06,8.84E-05,0.001003851,0.996532333,0.001126546,3.37E-06,2.98E-06
7779,machine-translation9,55,"In this work , we utilize such codes for a different purpose , that is , constructing word embeddings with drastically fewer parameters .",INTRODUCTION,Each component,machine-translation,9,41,1,1,research-problem,0.000792349,0,negative,0.000135378,0.259142308,0.000272571,3.90E-06,0.002112928,4.03E-05,8.97E-06,0.000234172,0.07428468,0.663694104,4.32E-05,2.48E-05,2.65E-06
7780,machine-translation9,56,"Due to the discreteness in the hash codes , it is usually difficult to directly optimize the objective function in Eq.",INTRODUCTION,Each component,machine-translation,9,42,1,0,,5.07E-05,0,negative,1.26E-05,0.002754175,2.52E-06,7.44E-07,2.68E-05,6.24E-06,1.01E-06,0.000111742,0.003893937,0.993046074,0.000138809,4.54E-06,7.44E-07
7781,machine-translation9,57,3 .,INTRODUCTION,Each component,machine-translation,9,43,1,0,,1.48E-05,0,negative,8.78E-06,9.73E-05,7.67E-07,1.42E-07,9.10E-06,5.82E-06,3.30E-07,2.62E-05,0.000749511,0.99909988,1.22E-06,8.03E-07,1.17E-07
7782,machine-translation9,58,"In this paper , we propose a simple and straight - forward method to learn the codes in an end - to - end neural network .",INTRODUCTION,Each component,machine-translation,9,44,1,0,,0.021215562,0,approach,0.000377521,0.545322934,0.000281651,2.03E-05,0.003150299,7.03E-05,2.68E-05,0.000507519,0.156200019,0.293522359,0.000438557,6.64E-05,1.54E-05
7783,machine-translation9,59,We utilize the Gumbel - softmax trick to find the best discrete codes that minimize the loss .,INTRODUCTION,Each component,machine-translation,9,45,1,1,approach,0.004805883,0,negative,0.000232703,0.117302037,0.000146983,8.21E-07,0.000543945,1.59E-05,4.26E-06,0.000157449,0.292957962,0.58861962,7.26E-06,9.65E-06,1.38E-06
7784,machine-translation9,60,"Besides the simplicity , this approach also allows one to use any arbitrary differentiable loss function , such as cosine similarity .",INTRODUCTION,Each component,machine-translation,9,46,1,0,,0.00020538,0,negative,5.81E-05,0.009432316,9.77E-06,2.11E-06,0.00018307,8.02E-06,9.78E-07,7.40E-05,0.016470724,0.973746028,7.54E-06,6.70E-06,6.34E-07
7785,machine-translation9,61,The contribution of this work can be summarized as follows :,INTRODUCTION,Each component,machine-translation,9,47,1,0,,1.10E-06,0,negative,5.68E-06,0.000149105,1.04E-06,7.63E-06,0.000100341,2.08E-05,5.65E-07,3.95E-05,0.000268788,0.999398703,6.52E-06,4.57E-07,7.84E-07
7786,machine-translation9,62,We propose to utilize the compositional coding approach for constructing the word embeddings with significantly fewer parameters .,INTRODUCTION,Each component,machine-translation,9,48,1,0,,0.001738088,0,negative,0.000710838,0.124033723,0.000121828,6.06E-06,0.001483146,3.19E-05,6.56E-06,0.00024621,0.118204994,0.755119178,8.99E-06,2.31E-05,3.44E-06
7787,machine-translation9,63,"In the experiments , we show that over 98 % of the embedding parameters can be eliminated in sentiment analysis task without affecting performance .",INTRODUCTION,Each component,machine-translation,9,49,1,0,,0.012975856,0,negative,0.010321036,0.021411587,1.90E-05,2.11E-05,0.003738946,8.84E-05,4.13E-05,0.000614693,0.003681622,0.95968817,6.96E-06,0.000356431,1.07E-05
7788,machine-translation9,64,"In machine translation tasks , the loss - free compression rate reaches 94 % ? 99 % . We propose a direct learning approach for the codes in an end - to - end neural network , with a Gumbel - softmax layer to encourage the discreteness .",INTRODUCTION,Each component,machine-translation,9,50,1,0,,0.031049576,0,negative,0.00050249,0.323146896,0.000361807,3.99E-05,0.00377214,0.00018888,7.30E-05,0.00132722,0.071845765,0.597841201,0.000700221,0.000145493,5.50E-05
7789,machine-translation9,65,The neural network for learning codes will be packaged into a tool .,INTRODUCTION,Each component,machine-translation,9,51,1,0,,0.00015733,0,negative,1.17E-05,0.00018375,3.29E-06,3.94E-06,0.000172154,3.72E-05,1.34E-06,6.98E-05,0.001156879,0.998355843,1.51E-06,1.15E-06,1.49E-06
7790,machine-translation9,66,"With the learned codes and basis vectors , the computation graph for composing embeddings is fairly easy to implement , and does not require modifications to other parts in the neural network .",INTRODUCTION,Each component,machine-translation,9,52,1,0,,0.000311981,0,negative,0.000185803,0.041413457,3.01E-05,7.63E-06,0.001022708,4.55E-05,1.11E-05,0.000437264,0.057680683,0.899110252,1.58E-05,3.37E-05,5.93E-06
7791,machine-translation9,67,RELATED WORK,INTRODUCTION,Each component,machine-translation,9,53,1,0,,1.23E-06,0,negative,2.10E-06,5.59E-05,8.84E-07,1.84E-07,2.03E-05,7.39E-06,8.46E-07,1.77E-05,7.57E-05,0.999813527,4.16E-06,8.85E-07,3.73E-07
7792,machine-translation9,68,"Existing works for compressing neural networks include low - precision computation , quantization and knowledge distillation .",INTRODUCTION,Each component,machine-translation,9,54,1,0,,0.000236248,0,negative,1.16E-05,0.000422845,7.56E-06,1.94E-05,0.000172488,9.65E-05,7.91E-06,0.000120841,0.000190776,0.998667195,0.0002681,3.88E-06,1.09E-05
7793,machine-translation9,69,"Network quantization such as HashedNet forces the weight matrix to have few real weights , with a hash function to determine the weight assignment .",INTRODUCTION,Each component,machine-translation,9,55,1,0,,1.71E-05,0,negative,1.30E-05,0.000566871,1.49E-05,3.76E-06,0.000114996,4.63E-05,3.04E-06,0.000113057,0.001462729,0.997637123,1.78E-05,1.85E-06,4.57E-06
7794,machine-translation9,70,"To capture the non-uniform nature of the networks , DeepCompression groups weight values into clusters based on pre-trained weight matrices .",INTRODUCTION,Each component,machine-translation,9,56,1,0,,0.006185974,0,negative,0.000165405,0.077719735,0.000861217,1.05E-06,0.001135277,3.21E-05,9.16E-06,0.000148109,0.204137323,0.715778352,2.16E-06,6.73E-06,3.39E-06
7795,machine-translation9,71,The weight assignment for each value is stored in the form of Huffman codes .,INTRODUCTION,Each component,machine-translation,9,57,1,0,,0.000265798,0,negative,8.30E-06,0.007337308,2.01E-05,1.24E-07,4.87E-05,1.04E-05,1.41E-06,0.000134995,0.161147146,0.831288375,1.41E-06,8.36E-07,8.59E-07
7796,machine-translation9,72,"However , as the embedding matrix is tremendously big , the number of hash codes a model need to maintain is still large even with Huffman coding .",INTRODUCTION,Each component,machine-translation,9,58,1,0,,0.000123299,0,negative,1.84E-05,0.00025281,9.13E-07,8.95E-07,4.70E-05,1.55E-05,2.07E-06,8.20E-05,0.000292836,0.999272996,9.77E-06,3.46E-06,1.44E-06
7797,machine-translation9,73,Network pruning works in a different way that makes a network sparse .,INTRODUCTION,Each component,machine-translation,9,59,1,0,,0.000135738,0,negative,7.13E-05,0.000641212,6.17E-05,2.33E-07,7.35E-05,4.86E-06,9.13E-07,1.11E-05,0.001882013,0.997247017,1.10E-06,4.68E-06,4.50E-07
7798,machine-translation9,74,Iterative pruning prunes a weight value if its absolute value is smaller than a threshold .,INTRODUCTION,Each component,machine-translation,9,60,1,0,,0.002631285,0,negative,8.67E-05,0.012942187,0.000327676,4.26E-07,0.00022512,1.42E-05,3.89E-06,8.58E-05,0.078679224,0.907625905,2.42E-06,4.20E-06,2.24E-06
7799,machine-translation9,75,The remaining network weights are retrained after pruning .,INTRODUCTION,Each component,machine-translation,9,61,1,0,,0.002025871,0,negative,6.15E-05,0.009180859,3.04E-05,1.41E-07,7.88E-05,1.38E-05,2.89E-06,0.000265731,0.082194725,0.908167789,5.37E-07,1.79E-06,1.13E-06
7800,machine-translation9,76,Some recent works also apply iterative pruning to prune 80 % of the connections for neural machine translation models .,INTRODUCTION,Each component,machine-translation,9,62,1,0,,0.000148379,0,negative,1.29E-05,0.000501293,7.66E-06,4.61E-06,0.000116697,9.22E-05,6.56E-06,0.000328386,0.000425092,0.998481838,1.14E-05,2.25E-06,9.12E-06
7801,machine-translation9,77,"In this paper , we compare the proposed method with iterative pruning .",INTRODUCTION,Each component,machine-translation,9,63,1,0,,0.000253613,0,negative,5.51E-05,0.03533189,0.000150439,3.21E-07,0.000343967,1.55E-05,8.88E-06,0.000107898,0.012883784,0.95107784,4.22E-06,1.85E-05,1.56E-06
7802,machine-translation9,78,"The problem of learning compact codes considered in this paper is closely related to learning to hash , which aims to learn the hash codes for vectors to facilitate the approximate nearest neighbor search .",INTRODUCTION,Each component,machine-translation,9,64,1,0,,0.000317706,0,negative,7.96E-06,0.001897717,1.14E-05,2.78E-05,0.000477978,8.14E-05,8.05E-06,0.000201904,0.000516026,0.996652479,9.84E-05,4.92E-06,1.40E-05
7803,machine-translation9,79,"Initiated byproduct quantization , subsequent works such as additive quantization explore the use of multiple codebooks for source coding , resulting in compositional codes .",INTRODUCTION,Each component,machine-translation,9,65,1,0,,5.11E-05,0,negative,6.75E-06,0.000599415,1.63E-05,9.67E-07,7.64E-05,2.15E-05,2.95E-06,5.48E-05,0.001232334,0.997977609,6.17E-06,2.16E-06,2.79E-06
7804,machine-translation9,80,We also adopt the coding scheme of additive quantization for its storage efficiency .,INTRODUCTION,Each component,machine-translation,9,66,1,0,,0.01061412,0,negative,0.000298516,0.120915143,0.000493261,1.22E-05,0.004217106,9.24E-05,2.70E-05,0.000682318,0.16710264,0.70612012,2.09E-06,1.98E-05,1.73E-05
7805,machine-translation9,81,Previous works mainly focus on performing efficient similarity search of image descriptors .,INTRODUCTION,Each component,machine-translation,9,67,1,0,,0.000184734,0,negative,8.24E-06,0.000324384,5.25E-06,3.41E-06,0.00010443,4.36E-05,7.06E-06,8.87E-05,0.00010621,0.999237449,6.01E-05,5.06E-06,6.12E-06
7806,machine-translation9,82,"In this work , we put more focus on reducing the codebook sizes and learning efficient codes to avoid performance loss .",INTRODUCTION,Each component,machine-translation,9,68,1,0,,0.002187776,0,negative,0.000298769,0.025922374,4.02E-05,1.50E-06,0.000732387,1.94E-05,6.04E-06,0.000131399,0.008716267,0.964103066,2.20E-06,2.37E-05,2.75E-06
7807,machine-translation9,83,utilizes an improved version of product quantization to compress text classification models .,INTRODUCTION,Each component,machine-translation,9,69,1,0,,0.00026377,0,negative,0.000241292,0.004076768,0.00179308,1.14E-05,0.002476203,0.000127674,6.12E-05,0.000103791,0.004707106,0.986273239,1.81E-05,4.82E-05,6.20E-05
7808,machine-translation9,84,"However , to match the baseline performance , much longer hash codes are required byproduct quantization .",INTRODUCTION,Each component,machine-translation,9,70,1,0,,5.05E-05,0,negative,9.02E-05,0.000104088,3.80E-06,2.88E-07,7.52E-05,5.72E-06,1.33E-06,1.13E-05,9.42E-05,0.999605503,5.17E-07,7.16E-06,6.65E-07
7809,machine-translation9,85,This will be detailed in Section 5.2 .,INTRODUCTION,Each component,machine-translation,9,71,1,0,,6.69E-07,0,negative,6.75E-06,2.10E-05,5.39E-07,9.56E-08,2.24E-05,1.91E-06,9.45E-08,3.57E-06,8.83E-05,0.999855031,7.47E-09,2.23E-07,4.19E-08
7810,machine-translation9,86,"To learn the codebooks and code assignment , additive quantization alternatively optimizes the codebooks and the discrete codes .",INTRODUCTION,Each component,machine-translation,9,72,1,0,,0.001680648,0,negative,2.56E-05,0.004266422,5.12E-05,2.91E-07,0.000154237,6.02E-06,1.26E-06,4.78E-05,0.010908251,0.984535517,2.46E-07,2.27E-06,8.90E-07
7811,machine-translation9,87,The learning of code assignment is performed by Beam Search algorithm when the codebooks are fixed .,INTRODUCTION,Each component,machine-translation,9,73,1,0,,0.004135551,0,negative,6.49E-05,0.062469798,0.000171764,3.11E-07,0.000461287,1.21E-05,6.07E-06,0.00013663,0.205583884,0.731083066,1.08E-06,6.06E-06,3.07E-06
7812,machine-translation9,88,"In this work , we propose a straight - forward method to directly learn the code assignment and codebooks simutaneously in an end - to - end neural network .",INTRODUCTION,Each component,machine-translation,9,74,1,0,,0.003343381,0,negative,0.000530978,0.240675841,0.000336912,2.89E-05,0.005893016,0.000175839,6.49E-05,0.001080418,0.063059821,0.688035708,1.05E-05,6.01E-05,4.71E-05
7813,machine-translation9,89,"Some recent works in learning to hash also utilize neural networks to produce binary codes by applying binary constrains ( e.g. , sigmoid function ) .",INTRODUCTION,Each component,machine-translation,9,75,1,0,,3.50E-05,0,negative,3.55E-06,0.000136925,4.50E-06,8.48E-07,3.66E-05,3.03E-05,3.42E-06,8.36E-05,0.000173613,0.999515514,5.59E-06,1.62E-06,3.95E-06
7814,machine-translation9,90,"In this work , we encourage the discreteness with the Gumbel - Softmax trick for producing compositional codes .",INTRODUCTION,Each component,machine-translation,9,76,1,0,,0.000319748,0,negative,0.000603567,0.181126637,0.000355252,8.51E-06,0.004233169,8.55E-05,4.93E-05,0.000590528,0.05220961,0.760667714,2.81E-06,4.86E-05,1.89E-05
7815,machine-translation9,91,"As an alternative to our approach , one can also reduce the number of unique word types by forcing a character - level segmentation .",INTRODUCTION,Each component,machine-translation,9,77,1,0,,5.08E-05,0,negative,2.68E-05,0.00050823,5.98E-06,1.47E-07,6.44E-05,2.82E-06,5.85E-07,1.16E-05,0.000969272,0.998406521,7.63E-08,3.11E-06,3.36E-07
7816,machine-translation9,92,"proposed a character - based neural language model , which applies a convolutional layer after the character embeddings .",INTRODUCTION,Each component,machine-translation,9,78,1,0,,1.30E-05,0,negative,1.70E-05,0.000125881,7.84E-05,1.64E-06,0.000284453,5.60E-05,8.08E-06,5.13E-05,0.000344027,0.999017862,1.03E-06,2.20E-06,1.21E-05
7817,machine-translation9,93,"propose to use char-gram as input features , which are further hashed to save space .",INTRODUCTION,Each component,machine-translation,9,79,1,0,,5.88E-06,0,negative,7.05E-06,2.44E-05,5.57E-06,2.27E-06,9.94E-05,5.56E-05,1.49E-06,3.58E-05,4.41E-05,0.999721239,7.33E-08,5.45E-07,2.37E-06
7818,machine-translation9,94,"Generally , using characterlevel inputs requires modifications to the model architecture .",INTRODUCTION,Each component,machine-translation,9,80,1,0,,7.66E-06,0,negative,7.69E-06,3.56E-05,1.15E-06,5.46E-07,3.41E-05,9.56E-06,5.87E-07,2.07E-05,3.84E-05,0.999850148,2.03E-07,7.59E-07,7.14E-07
7819,machine-translation9,95,"Moreover , some Asian languages such as Japanese and Chinese retain a large vocabulary at the character level , which makes the character - based approach difficult to be applied .",INTRODUCTION,Each component,machine-translation,9,81,1,0,,1.46E-05,0,negative,3.29E-06,3.72E-05,1.31E-06,2.47E-07,5.35E-05,6.31E-06,1.04E-06,1.66E-05,1.59E-05,0.999861672,6.58E-07,1.49E-06,8.57E-07
7820,machine-translation9,96,"In contrast , our approach does not suffer from these limitations .",INTRODUCTION,Each component,machine-translation,9,82,1,0,,5.36E-06,0,negative,3.62E-05,6.92E-05,2.38E-06,5.45E-07,0.000135305,4.02E-06,4.85E-07,6.96E-06,4.92E-05,0.999692977,3.09E-08,2.50E-06,2.46E-07
7821,machine-translation9,97,ADVANTAGE OF COMPOSITIONAL CODES,INTRODUCTION,Each component,machine-translation,9,83,1,0,,0.000121829,0,negative,5.88E-05,9.41E-05,3.43E-05,2.18E-07,3.56E-05,1.20E-05,1.56E-05,1.54E-05,0.000446353,0.999253815,1.65E-06,2.75E-05,4.57E-06
7822,machine-translation9,98,"In this section , we formally describe the compositional coding approach and analyze its merits for compressing word embeddings .",INTRODUCTION,Each component,machine-translation,9,84,1,0,,1.27E-05,0,negative,2.61E-05,0.009980842,4.42E-05,2.43E-07,0.000264306,7.74E-06,3.80E-06,4.60E-05,0.01441155,0.975208917,2.14E-07,4.27E-06,1.81E-06
7823,machine-translation9,99,The coding approach follows the scheme in additive quantization .,INTRODUCTION,Each component,machine-translation,9,85,1,0,,7.15E-05,0,negative,1.85E-05,0.003087915,0.000115182,2.44E-07,0.000204871,9.93E-06,2.65E-06,3.77E-05,0.026175118,0.970344447,6.37E-08,1.25E-06,2.21E-06
7824,machine-translation9,100,"We represent each word w with a compact code C w that is composed of M components such that , which also indicates that M log 2 K bits are required to store each code .",INTRODUCTION,Each component,machine-translation,9,86,1,0,,6.21E-06,0,negative,3.96E-06,0.000819183,5.09E-06,5.74E-08,7.28E-05,6.49E-06,1.03E-06,5.89E-05,0.003063812,0.9959675,1.68E-08,4.82E-07,6.38E-07
7825,machine-translation9,101,"For convenience , K is selected to be a number of a multiple of 2 , so that the codes can be efficiently stored .",INTRODUCTION,Each component,machine-translation,9,87,1,0,,1.67E-05,0,negative,2.29E-05,0.001498427,2.64E-06,2.21E-06,0.000464818,0.000296151,2.96E-05,0.006752376,0.000858747,0.990057506,2.49E-08,2.79E-06,1.18E-05
7826,machine-translation9,102,If we restrict each component,INTRODUCTION,Each component,machine-translation,9,88,1,0,,1.15E-06,0,negative,1.31E-05,7.02E-05,2.88E-06,3.02E-08,1.49E-05,1.95E-06,3.52E-07,9.65E-06,0.000568944,0.999317262,3.45E-09,5.99E-07,1.34E-07
7827,machine-translation9,103,"Ci w to values of 0 or 1 , the code for each word C w will be a binary code .",INTRODUCTION,Each component,machine-translation,9,89,1,0,,2.24E-06,0,negative,1.89E-06,0.000155178,3.63E-06,3.83E-09,1.04E-05,1.35E-06,3.68E-07,9.06E-06,0.001483805,0.99833381,3.81E-09,3.90E-07,9.30E-08
7828,machine-translation9,104,"In this case , the code learning problem is equivalent to a matrix factorization problem with binary components .",INTRODUCTION,Each component,machine-translation,9,90,1,0,,7.05E-06,0,negative,3.06E-06,0.000889806,9.08E-06,2.04E-08,2.34E-05,1.37E-06,4.92E-07,1.37E-05,0.004585125,0.994472918,3.39E-08,6.45E-07,3.42E-07
7829,machine-translation9,105,"Forcing the compact codes to be binary numbers can be beneficial , as the learning problem is usually easier to solve in the binary case , and some existing optimization algorithms in learning to hash can be reused .",INTRODUCTION,Each component,machine-translation,9,91,1,0,,0.000311056,0,negative,1.39E-05,2.54E-05,5.79E-07,3.88E-08,1.17E-05,3.49E-06,7.68E-07,1.89E-05,3.08E-05,0.999892799,4.41E-09,1.49E-06,2.68E-07
7830,machine-translation9,106,"However , the compositional coding approach produces shorter codes and is thus more storage efficient .",INTRODUCTION,Each component,machine-translation,9,92,1,0,,5.17E-06,0,negative,6.68E-06,3.73E-05,3.05E-06,5.22E-08,1.27E-05,3.09E-06,9.61E-07,8.75E-06,5.35E-05,0.999871627,7.45E-08,1.72E-06,5.51E-07
7831,machine-translation9,107,"As the number of basis vectors is M K regardless of the vocabulary size , the only uncertain factor contributing to the model size is the size of the hash codes , which is proportional to the vocabulary size .",INTRODUCTION,Each component,machine-translation,9,93,1,0,,9.88E-06,0,negative,0.000115504,0.000275026,6.90E-06,7.02E-08,8.12E-05,6.81E-06,3.98E-06,4.01E-05,0.000535937,0.998929468,8.79E-09,4.22E-06,8.16E-07
7832,machine-translation9,108,"Therefore , maintaining short codes is cruicial in our work .",INTRODUCTION,Each component,machine-translation,9,94,1,0,,6.03E-06,0,negative,5.87E-06,2.98E-05,9.52E-07,2.43E-07,5.22E-05,6.06E-06,5.47E-07,1.44E-05,1.96E-05,0.999869019,2.85E-08,7.03E-07,6.08E-07
7833,machine-translation9,109,Suppose we wish the model to have a set of N basis vectors .,INTRODUCTION,Each component,machine-translation,9,95,1,0,,5.02E-07,0,negative,3.59E-07,2.27E-05,3.29E-07,1.25E-08,6.87E-06,2.43E-06,2.37E-07,2.50E-05,9.78E-05,0.9998439,2.62E-09,1.19E-07,1.91E-07
7834,machine-translation9,110,"Then in the binary case , each code will have N / 2 bits .",INTRODUCTION,Each component,machine-translation,9,96,1,0,,3.97E-06,0,negative,1.52E-06,4.70E-05,1.41E-06,5.49E-09,8.24E-06,3.21E-06,8.92E-07,1.81E-05,0.000479266,0.999439744,3.35E-09,2.44E-07,3.97E-07
7835,machine-translation9,111,"For the compositional coding approach , if we can find a M K decomposition such that M K = N , then each code will have M log 2 K bits .",INTRODUCTION,Each component,machine-translation,9,97,1,0,,5.86E-06,0,negative,6.09E-06,0.000233586,1.22E-05,8.45E-09,1.99E-05,1.66E-06,1.13E-06,7.32E-06,0.001239485,0.998476812,9.75E-09,1.29E-06,4.72E-07
7836,machine-translation9,112,"For example , a binary code will have a length of 256 bits to support 512 basis vectors .",INTRODUCTION,Each component,machine-translation,9,98,1,0,,4.25E-06,0,negative,4.04E-06,0.000132828,5.23E-06,9.20E-08,7.06E-05,7.17E-05,9.88E-06,0.000317046,0.000663233,0.998716625,7.32E-09,7.50E-07,7.89E-06
7837,machine-translation9,113,"In contrast , a 32 16 compositional coding scheme will produce codes of only 128 bits . :",INTRODUCTION,Each component,machine-translation,9,99,1,0,,1.01E-05,0,negative,0.000281183,9.69E-05,6.45E-05,9.01E-08,0.000184538,1.93E-05,2.43E-05,2.34E-05,0.000281899,0.998996269,4.50E-09,2.36E-05,3.95E-06
7838,machine-translation9,114,Comparison of different coding approaches .,INTRODUCTION,Each component,machine-translation,9,100,1,0,,2.30E-06,0,negative,2.68E-06,1.49E-05,2.71E-06,1.68E-09,1.67E-06,1.74E-06,2.42E-06,8.16E-06,6.98E-05,0.999893946,6.71E-09,1.71E-06,3.32E-07
7839,machine-translation9,115,"To support N basis vectors , a binary code will have N / 2 bits and the embedding computation is a summation over N / 2 vectors .",INTRODUCTION,Each component,machine-translation,9,101,1,0,,1.62E-05,0,negative,3.47E-06,0.000315431,6.50E-06,2.99E-08,4.05E-05,1.57E-05,4.57E-06,0.000108206,0.002143463,0.997358018,7.04E-09,4.90E-07,3.68E-06
7840,machine-translation9,116,"For the compositional approach with M codebooks and K codewords in each codebook , each code has M log 2 K bits , and the computation is a summation over M vectors .",INTRODUCTION,Each component,machine-translation,9,102,1,0,,3.32E-05,0,negative,2.03E-05,0.002752788,6.52E-05,3.61E-08,0.000113775,8.54E-06,7.06E-06,5.33E-05,0.010586355,0.986387347,1.05E-08,2.08E-06,3.13E-06
7841,machine-translation9,117,A comparison of different coding approaches is summarized in .,INTRODUCTION,Each component,machine-translation,9,103,1,0,,3.49E-06,0,negative,8.42E-07,6.17E-06,5.51E-07,8.99E-08,1.84E-05,6.41E-06,5.24E-07,1.57E-05,1.13E-05,0.999939394,2.52E-09,2.36E-07,4.17E-07
7842,machine-translation9,118,We also report the number of basis vectors required to compute an embedding as a measure of computational cost .,INTRODUCTION,Each component,machine-translation,9,104,1,0,,4.78E-06,0,negative,8.34E-07,3.46E-05,5.61E-07,4.39E-08,8.56E-05,4.57E-06,5.91E-07,2.25E-05,1.75E-05,0.999832689,1.87E-10,2.74E-07,1.33E-07
7843,machine-translation9,119,"For the conventional approach , the number of vectors is identical to the vocabulary size and the computation is basically a single indexing operation .",INTRODUCTION,Each component,machine-translation,9,105,1,0,,1.89E-05,0,negative,6.99E-06,0.000602085,1.20E-05,5.74E-08,5.84E-05,5.65E-06,2.35E-06,4.04E-05,0.000965316,0.998303994,1.06E-08,1.03E-06,1.74E-06
7844,machine-translation9,120,"In the case of binary codes , the computation for constructing an embedding involves a summation over N / 2 basis vectors .",INTRODUCTION,Each component,machine-translation,9,106,1,0,,5.59E-06,0,negative,3.15E-06,0.00014937,2.35E-06,3.44E-08,2.47E-05,6.01E-06,1.61E-06,4.92E-05,0.000470486,0.999291298,4.84E-09,4.32E-07,1.37E-06
7845,machine-translation9,121,"For the compositional approach , the number of vectors required to construct an embedding vector is M .",INTRODUCTION,Each component,machine-translation,9,107,1,0,,5.52E-05,0,negative,2.72E-06,0.000300878,2.10E-06,5.11E-08,5.42E-05,3.32E-05,7.13E-06,0.000536014,0.000460991,0.998598841,2.18E-09,4.34E-07,3.38E-06
7846,machine-translation9,122,Both the binary and compositional approaches have significantly fewer vectors in the embedding matrix .,INTRODUCTION,Each component,machine-translation,9,108,1,0,,9.07E-05,0,negative,0.000138429,0.000107066,1.16E-05,1.77E-07,0.000319588,4.31E-05,2.20E-05,0.000129761,0.000122544,0.999092921,8.47E-10,6.70E-06,6.12E-06
7847,machine-translation9,123,The compositional coding approach provides a better balance with shorter codes and lower computational cost .,INTRODUCTION,Each component,machine-translation,9,109,1,0,,0.000639617,0,negative,0.000211988,0.000125065,1.64E-05,1.14E-07,0.000120926,7.60E-06,8.11E-06,2.03E-05,0.000201094,0.99926776,1.37E-09,1.82E-05,2.45E-06
7848,machine-translation9,124,CODE LEARNING WITH GUMBEL - SOFTMAX,INTRODUCTION,Each component,machine-translation,9,110,1,0,,0.004038413,0,negative,1.48E-05,0.00090566,0.000153867,2.60E-07,6.59E-05,5.16E-05,0.000184384,0.000266512,0.00250685,0.995706455,3.96E-07,2.80E-05,0.000115404
7849,machine-translation9,125,Let ? ?,INTRODUCTION,Each component,machine-translation,9,111,1,0,,3.60E-07,0,negative,2.07E-07,1.19E-06,1.07E-07,3.51E-09,2.73E-06,1.81E-06,1.98E-07,6.83E-06,4.31E-06,0.999982492,9.05E-11,5.58E-08,7.17E-08
7850,machine-translation9,126,"R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",INTRODUCTION,Each component,machine-translation,9,112,1,0,,3.22E-06,0,negative,4.27E-06,5.27E-05,1.90E-06,2.44E-08,4.23E-05,4.75E-06,1.70E-06,3.70E-05,5.79E-05,0.999796397,3.15E-10,5.39E-07,5.73E-07
7851,machine-translation9,127,"By using the reconstruction loss as the objective function in Eq. 3 , we are actually finding an approximate matrix factorization ?",INTRODUCTION,Each component,machine-translation,9,113,1,0,,3.64E-06,0,negative,3.78E-07,5.25E-06,2.38E-07,1.94E-08,6.21E-06,3.58E-06,3.89E-07,1.54E-05,1.11E-05,0.999957091,3.94E-10,1.01E-07,2.55E-07
7852,machine-translation9,128,is a basis matrix for the i - th component .,INTRODUCTION,Each component,machine-translation,9,114,1,0,,9.49E-06,0,negative,1.04E-06,2.31E-05,7.64E-07,2.13E-08,1.84E-05,4.51E-06,4.75E-07,4.64E-05,8.44E-05,0.999820599,1.18E-10,8.30E-08,3.08E-07
7853,machine-translation9,129,Di is a | V | K code matrix in which each row is an K-dimensional one - hot vector .,INTRODUCTION,Each component,machine-translation,9,115,1,0,,3.66E-06,0,negative,1.37E-06,0.000225967,3.99E-06,5.06E-08,6.38E-05,1.93E-05,3.16E-06,0.000225696,0.00082517,0.99862873,4.23E-10,1.77E-07,2.62E-06
7854,machine-translation9,130,If we let d i w be the one - hot vector corresponding to the code component,INTRODUCTION,Each component,machine-translation,9,116,1,0,,3.17E-07,0,negative,9.40E-07,1.29E-05,8.34E-07,5.56E-10,2.88E-06,2.69E-07,1.47E-07,2.15E-06,6.74E-05,0.999912271,5.50E-11,1.59E-07,3.08E-08
7855,machine-translation9,131,"Ci w for word w , the computation of the word embeddings can be reformulated as",INTRODUCTION,Each component,machine-translation,9,117,1,0,,8.88E-07,0,negative,6.03E-07,1.45E-05,1.19E-06,2.46E-08,1.87E-05,5.33E-06,6.72E-07,2.97E-05,7.05E-05,0.999857722,3.28E-10,1.65E-07,7.49E-07
7856,machine-translation9,132,"Therefore , the problem of learning discrete codes C w can be converted to a problem of finding a set of optimal one - hot vectors d 1 w , ... , d M wand source dictionaries A 1 , ... , AM , that minimize the reconstruction loss .",INTRODUCTION,Each component,machine-translation,9,118,1,0,,2.54E-06,0,negative,3.07E-07,2.81E-05,8.61E-07,4.96E-08,1.03E-05,6.04E-06,9.45E-07,4.72E-05,5.42E-05,0.999850657,3.30E-09,1.90E-07,1.22E-06
7857,machine-translation9,133,The Gumbel - softmax reparameterization trick is useful for parameterizing a discrete distribution such as the K-dimensional one - hot vectors d i win Eq.,INTRODUCTION,Each component,machine-translation,9,119,1,0,,1.93E-06,0,negative,5.41E-05,0.000139324,2.02E-05,2.42E-08,0.000108699,3.76E-06,3.87E-06,1.14E-05,0.000127577,0.999527003,2.58E-10,3.34E-06,7.08E-07
7858,machine-translation9,134,"5 . By applying the Gumbel - softmax trick , the k - th elemement ind i w is computed as",INTRODUCTION,Each component,machine-translation,9,120,1,0,,9.66E-07,0,negative,2.80E-06,2.33E-05,4.64E-06,3.95E-09,1.60E-05,2.12E-06,7.87E-07,9.82E-06,7.83E-05,0.999861741,5.84E-11,3.28E-07,2.50E-07
7859,machine-translation9,135,where,INTRODUCTION,Each component,machine-translation,9,121,1,0,,9.78E-08,0,negative,1.32E-07,8.18E-07,8.87E-08,2.51E-09,1.37E-06,1.15E-06,1.23E-07,5.03E-06,4.83E-06,0.999986367,1.51E-11,2.83E-08,5.19E-08
7860,machine-translation9,136,Gk is a noise term that is sampled from the Gumbel distribution ? log ( ?,INTRODUCTION,Each component,machine-translation,9,122,1,0,,2.47E-06,0,negative,1.68E-06,2.67E-05,1.64E-06,1.43E-08,2.18E-05,8.76E-06,1.87E-06,7.08E-05,6.21E-05,0.999803713,5.33E-11,1.63E-07,6.96E-07
7861,machine-translation9,137,"log ( Uniform [ 0 , 1 ] ) ) , whereas ?",INTRODUCTION,Each component,machine-translation,9,123,1,0,,4.24E-06,0,negative,3.72E-06,5.72E-05,5.03E-06,4.07E-09,2.11E-05,6.38E-06,3.97E-06,3.65E-05,0.000105793,0.999759102,9.96E-11,3.93E-07,7.98E-07
7862,machine-translation9,138,is the temperature of the softmax .,INTRODUCTION,Each component,machine-translation,9,124,1,0,,1.75E-05,0,negative,1.78E-06,4.60E-05,1.24E-06,8.98E-08,4.19E-05,4.52E-05,6.93E-06,0.000791629,9.35E-05,0.998966506,1.32E-10,2.09E-07,4.91E-06
7863,machine-translation9,139,"In our model , the vector ?",INTRODUCTION,Each component,machine-translation,9,125,1,0,,1.02E-06,0,negative,3.87E-07,1.07E-05,4.70E-07,8.36E-09,1.04E-05,6.68E-06,8.63E-07,6.58E-05,4.59E-05,0.999858351,2.81E-11,6.49E-08,3.95E-07
7864,machine-translation9,140,i w is computed by a simple neural network with a single hidden layer as,INTRODUCTION,Each component,machine-translation,9,126,1,0,,4.56E-07,0,negative,6.56E-07,7.42E-06,8.39E-07,4.08E-09,5.96E-06,1.75E-06,3.70E-07,1.26E-05,5.41E-05,0.999915955,3.90E-11,8.32E-08,2.50E-07
7865,machine-translation9,141,"In our experiments , the hidden layer h w always has a size of M K /2 .",INTRODUCTION,Each component,machine-translation,9,127,1,0,,4.97E-05,0,negative,5.00E-06,0.000275441,3.21E-06,6.68E-07,0.000393346,0.001239321,0.000170417,0.015840558,0.000128285,0.981869661,1.72E-10,8.79E-07,7.32E-05
7866,machine-translation9,142,We found that a fixed temperature of ? = 1 just works well .,INTRODUCTION,Each component,machine-translation,9,128,1,0,,0.000355038,0,negative,0.000100896,5.08E-05,3.91E-06,1.23E-05,0.00044132,0.002390331,0.000465272,0.007929578,3.12E-05,0.988278269,5.09E-10,1.64E-05,0.000279797
7867,machine-translation9,143,The Gumbel - softmax trick is applied to ?,INTRODUCTION,Each component,machine-translation,9,129,1,0,,2.83E-06,0,negative,3.15E-07,1.77E-06,2.54E-07,1.77E-09,5.94E-06,2.04E-06,4.74E-07,1.26E-05,4.09E-06,0.999972295,3.86E-12,6.91E-08,1.32E-07
7868,machine-translation9,144,i w to obtain d i w .,INTRODUCTION,Each component,machine-translation,9,130,1,0,,2.50E-07,0,negative,1.47E-06,4.16E-06,8.10E-07,3.61E-09,1.03E-05,1.11E-06,3.93E-07,4.35E-06,1.11E-05,0.99996591,1.44E-11,1.95E-07,1.75E-07
7869,machine-translation9,145,"Then , the model reconstructs the embedding E(C w ) with Eq. 5 and computes the reconstruction loss with Eq.",INTRODUCTION,Each component,machine-translation,9,131,1,0,,4.52E-06,0,negative,1.47E-05,3.46E-05,1.82E-05,1.87E-09,3.09E-05,7.36E-07,1.73E-06,2.92E-06,8.80E-05,0.999806772,1.50E-11,1.22E-06,2.64E-07
7870,machine-translation9,146,3 .,INTRODUCTION,Each component,machine-translation,9,132,1,0,,1.06E-06,0,negative,5.89E-07,1.54E-06,2.27E-07,2.42E-09,2.62E-06,2.00E-06,4.28E-07,9.07E-06,8.80E-06,0.999974482,6.47E-12,5.50E-08,1.92E-07
7871,machine-translation9,147,"The model architecture of the end - to - end neural network is illustrated in , which is effectively an auto - encoder with a Gumbel - softmax middle layer .",INTRODUCTION,Each component,machine-translation,9,133,1,0,,3.78E-05,0,negative,5.13E-06,9.19E-05,3.42E-05,5.85E-07,0.000147889,4.34E-05,5.22E-06,0.000128271,0.001469533,0.998054909,1.19E-10,1.81E-07,1.88E-05
7872,machine-translation9,148,"The whole neural network for coding learning has five parameters ( ? , b , ? , b , A ) .",INTRODUCTION,Each component,machine-translation,9,134,1,0,,6.64E-05,0,negative,5.84E-06,0.000134162,6.62E-06,3.04E-07,0.000178081,0.000223817,3.55E-05,0.003345491,0.000607269,0.995413634,5.41E-11,2.42E-07,4.90E-05
7873,machine-translation9,149,"Once the coding learning model is trained , the code C w for each word can be easily obtained by applying argmax to the one - hot vectors d 1 w , ... , d M w .",INTRODUCTION,Each component,machine-translation,9,135,1,0,,1.13E-06,0,negative,4.29E-07,1.01E-05,3.25E-07,3.18E-09,5.98E-06,1.97E-06,5.44E-07,2.36E-05,2.78E-05,0.999928801,1.11E-11,8.62E-08,2.90E-07
7874,machine-translation9,150,The basis vectors ( codewords ) for composing the embeddings can be found as the row vectors in the weight matrix A.,INTRODUCTION,Each component,machine-translation,9,136,1,0,,5.70E-06,0,negative,4.62E-07,2.92E-05,8.93E-07,1.41E-09,7.43E-06,1.29E-06,4.30E-07,1.71E-05,0.000184322,0.999758519,7.32E-12,5.60E-08,2.34E-07
7875,machine-translation9,151,"For general NLP tasks , one can learn the compositional codes from publicly available word vectors such as GloVe vectors .",INTRODUCTION,Each component,machine-translation,9,137,1,0,,3.73E-05,0,negative,2.99E-07,9.99E-06,8.77E-07,3.43E-08,1.49E-05,8.69E-06,1.85E-06,4.71E-05,7.92E-06,0.99990601,1.15E-10,2.12E-07,2.03E-06
7876,machine-translation9,152,"However , for some tasks such as machine translation , the word embeddings are usually jointly learned with other parts of the neural network .",INTRODUCTION,Each component,machine-translation,9,138,1,0,,1.07E-05,0,negative,5.74E-07,6.10E-06,1.59E-06,1.18E-07,1.59E-05,1.82E-05,4.07E-06,4.80E-05,3.91E-06,0.999894158,6.95E-10,3.56E-07,6.96E-06
7877,machine-translation9,153,"For such tasks , one has to first train a normal model to obtain the baseline embeddings .",INTRODUCTION,Each component,machine-translation,9,139,1,0,,9.21E-06,0,negative,6.90E-07,4.69E-06,1.11E-06,9.87E-08,1.77E-05,1.15E-05,2.31E-06,3.73E-05,4.89E-06,0.999915761,1.05E-10,3.34E-07,3.55E-06
7878,machine-translation9,154,"Then , based on the trained embedding matrix , one can learn a set of task - specific codes .",INTRODUCTION,Each component,machine-translation,9,140,1,0,,3.15E-06,0,negative,8.74E-07,2.65E-05,1.67E-06,1.53E-09,1.10E-05,4.74E-07,2.66E-07,5.54E-06,0.000139248,0.99981424,3.54E-12,7.72E-08,1.77E-07
7879,machine-translation9,155,"As the reconstructed embeddings E( C w ) are not identical to the original embeddings ? ( w ) , the model parameters other than the embedding matrix have to be retrained again .",INTRODUCTION,Each component,machine-translation,9,141,1,0,,3.43E-06,0,negative,3.23E-06,2.19E-06,1.73E-07,4.45E-09,6.02E-06,2.62E-06,9.06E-07,1.54E-05,4.26E-06,0.999964588,4.62E-12,1.63E-07,4.48E-07
7880,machine-translation9,156,The code learning model can not be jointly trained with the machine translation model as it takes far more iterations for the coding layer to converge to one - hot vectors .,INTRODUCTION,Each component,machine-translation,9,142,1,0,,9.29E-06,0,negative,4.47E-05,1.26E-05,4.45E-06,2.43E-08,3.41E-05,5.96E-06,5.48E-06,1.69E-05,1.75E-05,0.999854271,1.13E-11,1.70E-06,2.33E-06
7881,machine-translation9,157,EXPERIMENTS,,,machine-translation,9,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
7882,machine-translation9,158,"In our experiments , we focus on evaluating the maximum loss - free compression rate of word embeddings on two typical NLP tasks : sentiment analysis and machine translation .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,1,1,0,,0.082645017,0,negative,0.001971979,0.025533956,0.004572251,0.000196696,0.000407059,0.015906962,0.016438818,0.219800141,0.000812751,0.691425597,0.006526618,0.014639137,0.001768037
7883,machine-translation9,159,We compare the model performance and the size of embedding layer with the baseline model and the iterative pruning method .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,2,1,0,,0.053235755,0,negative,0.000326558,0.000203202,0.000108963,1.16E-05,1.12E-05,0.015782007,0.001921759,0.215680137,4.16E-05,0.764542014,4.28E-05,0.001243162,8.50E-05
7884,machine-translation9,160,Please note that the sizes of other parts in the neural networks are not included in our results .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,3,1,0,,0.001528995,0,negative,0.000242511,2.50E-05,3.80E-05,1.86E-05,1.14E-05,0.003380537,0.000295624,0.017388561,2.17E-05,0.978277363,1.24E-05,0.000259569,2.86E-05
7885,machine-translation9,161,"For dense matrices , we report the size of dumped numpy arrays .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,4,1,0,,0.01094918,0,negative,0.000194126,5.22E-05,0.000161211,1.18E-05,1.58E-05,0.013902982,0.002083626,0.058468352,2.03E-05,0.923944513,2.87E-05,0.001035861,8.05E-05
7886,machine-translation9,162,"For the sparse matrices , we report the size of dumped compressed sparse column matrices ( csc matrix ) in scipy .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,5,1,0,,0.10655579,0,negative,0.000413311,0.000171803,0.000650543,0.000305088,0.000142155,0.151395691,0.018010398,0.302110089,7.01E-05,0.524562611,6.26E-05,0.001206567,0.000899035
7887,machine-translation9,163,All float numbers take 32 bits storage .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,6,1,0,,0.500443197,1,hyperparameters,0.000114926,0.000127352,0.000375799,5.38E-05,1.39E-05,0.241845226,0.017819565,0.660019445,8.69E-05,0.078279427,5.44E-05,0.000303073,0.000906168
7888,machine-translation9,164,"We enable the "" compressed "" option when dumping the matrices , without this option , the file size is about 1.1 times bigger .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,7,1,0,,0.195933275,0,negative,0.004061865,0.000352052,0.001313405,0.001208352,0.000218813,0.167725312,0.012863367,0.352323709,0.00023819,0.455332307,8.76E-05,0.002561476,0.001713568
7889,machine-translation9,165,CODE LEARNING,EXPERIMENTS,EXPERIMENTS,machine-translation,9,8,1,1,experiments,0.864951485,1,experiments,0.000929477,0.000125516,0.002696065,0.000490944,0.000125563,0.005742302,0.700873241,0.018107166,2.45E-05,0.081527668,0.015189922,0.143013756,0.031153857
7890,machine-translation9,166,"To learn efficient compact codes for each word , our proposed method requires a set of baseline embedding vectors .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,9,1,0,,0.111508654,0,hyperparameters,0.004401758,0.026079495,0.003684466,0.000416294,0.000618551,0.022540478,0.003778209,0.575845477,0.006768522,0.349179846,0.000573306,0.003291273,0.002822326
7891,machine-translation9,167,"For the sentiment analysis task , we learn the codes based on the publicly available GloVe vectors .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,10,1,0,,0.685134178,1,negative,0.003332503,0.013551926,0.009832771,0.000210639,0.001270074,0.030236847,0.03488547,0.308806251,0.000529566,0.570649087,0.000192549,0.023250118,0.003252201
7892,machine-translation9,168,"For the machine translation task , we first train a normal neural machine translation ( NMT ) model to obtain task - specific word embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,11,1,0,,0.806648787,1,negative,0.006899698,0.045435559,0.172217268,0.000189602,0.000895709,0.010219136,0.024767639,0.133068998,0.003639667,0.570494519,0.001863798,0.023259833,0.007048573
7893,machine-translation9,169,Then we learn the codes using the pre-trained embeddings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,12,1,0,,0.118981544,0,negative,0.00071034,0.000752289,0.002949131,1.53E-05,2.08E-05,0.008708255,0.000761685,0.160072978,0.001393028,0.823858989,1.70E-05,0.000343813,0.000396365
7894,machine-translation9,170,We train the end - to - end network described in Section 4 to learn the codes automatically .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,13,1,0,,0.039003765,0,hyperparameters,0.00028501,0.000914918,0.000706625,1.24E-05,2.37E-05,0.028770882,0.002035696,0.703340491,0.000458937,0.262833176,9.05E-06,0.000219979,0.000389208
7895,machine-translation9,171,"In each iteration , a small batch of the embeddings is sampled uniformly from the baseline embedding matrix .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,14,1,1,experiments,0.183346281,0,hyperparameters,0.000424202,0.001003243,0.000163384,1.88E-05,2.09E-05,0.025024713,0.00192105,0.88318851,0.000286763,0.087307948,1.10E-05,0.000225467,0.000404075
7896,machine-translation9,172,The network parameters are optimized to minimize the reconstruction loss of the sampled embeddings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,15,1,0,,0.499577341,0,hyperparameters,0.000143563,0.001004852,7.81E-05,1.25E-05,1.01E-05,0.016445345,0.000576885,0.919230858,0.000552192,0.061681847,7.84E-06,6.35E-05,0.000192298
7897,machine-translation9,173,"In our experiments , the batch size is set to 128 .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,16,1,1,experiments,0.960359119,1,hyperparameters,2.41E-05,6.41E-05,1.08E-05,2.36E-05,4.61E-06,0.051162788,0.002227151,0.937342687,1.80E-05,0.008748695,3.33E-06,4.30E-05,0.000327235
7898,machine-translation9,174,We use Adam optimizer with a fixed learning rate of 0.0001 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,17,1,1,experiments,0.980054592,1,hyperparameters,2.00E-05,5.59E-05,1.13E-05,1.87E-05,2.27E-06,0.046415777,0.002064401,0.947818937,2.28E-05,0.00323783,2.40E-06,2.01E-05,0.000309518
7899,machine-translation9,175,The training is run for 200K iterations .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,18,1,1,experiments,0.783920666,1,hyperparameters,7.32E-05,9.71E-05,2.98E-05,9.09E-06,4.35E-06,0.044644271,0.003018999,0.904714329,4.46E-05,0.046762706,8.05E-06,0.00012116,0.000472409
7900,machine-translation9,176,"Every 1,000 iterations , we examine the loss on a fixed validation set and save the parameters if the loss decreases .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,19,1,0,,0.061811463,0,hyperparameters,0.001196677,0.000971,0.000360219,4.23E-05,7.12E-05,0.035242923,0.005479913,0.695535103,0.000278781,0.258943662,1.13E-05,0.000841096,0.001025842
7901,machine-translation9,177,"We evenly distribute the model training to 4 GPUs using the nccl package , so that one round of code learning takes around 15 minutes to complete .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,20,1,1,experiments,0.864559284,1,hyperparameters,0.000148192,0.000194846,9.13E-05,0.000384646,5.66E-05,0.165453689,0.016209807,0.789049779,6.09E-05,0.025081198,6.57E-06,0.000238234,0.003024254
7902,machine-translation9,178,SENTIMENT ANALYSIS,EXPERIMENTS,EXPERIMENTS,machine-translation,9,21,1,1,experiments,0.984074392,1,experiments,0.00163384,9.84E-05,0.000826829,0.002609429,0.000783366,0.004212993,0.715173609,0.00871267,1.58E-05,0.034313153,0.001639276,0.150490826,0.07948988
7903,machine-translation9,179,"Dataset : For sentiment analysis , we use a standard separation of IMDB movie review dataset , which contains 25 k reviews for training and 25 K reviews for testing purpose .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,22,1,0,,0.107848639,0,negative,0.001684393,0.000964767,0.009484321,0.006471612,0.019755299,0.019443045,0.154667649,0.034065186,9.05E-05,0.672816677,0.000418922,0.041956306,0.038181363
7904,machine-translation9,180,We lowercase and tokenize all texts with the nltk package .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,23,1,0,,0.909735795,1,hyperparameters,0.000673445,0.000267613,0.000452341,0.001234841,0.000415519,0.185312275,0.015529922,0.600572273,0.000166961,0.1916289,4.52E-06,0.000523695,0.003217693
7905,machine-translation9,181,We choose the 300 - dimensional uncased Glo Ve word vectors ( trained on 42B tokens of Common Crawl data ) as our baseline embeddings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,24,1,0,,0.959095987,1,hyperparameters,2.03E-05,9.20E-05,3.14E-05,8.38E-06,4.40E-06,0.049066856,0.004208352,0.926601189,2.81E-05,0.019473936,1.66E-06,6.37E-05,0.00039978
7906,machine-translation9,182,"The vocabulary for the model training contains all words appears both in the IMDB dataset and the GloVe vocabulary , which results in around 75 K words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,25,1,0,,0.217811734,0,hyperparameters,0.000255607,0.000276518,0.000215826,0.000122755,0.000260767,0.087067025,0.018091477,0.707548741,6.05E-05,0.182660595,3.38E-06,0.000801722,0.002635058
7907,machine-translation9,183,We truncate the texts of reviews to assure they are not longer than 400 words .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,26,1,0,,0.075074855,0,hyperparameters,0.000624257,0.000290451,0.000242088,7.04E-05,0.000201063,0.056524346,0.006056995,0.649689198,9.88E-05,0.284266556,1.95E-06,0.000461453,0.001472468
7908,machine-translation9,184,Model architecture :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,27,1,0,,0.325633626,0,negative,0.0114662,0.003509622,0.038334238,0.00540186,0.000882219,0.04082055,0.040574089,0.195375501,0.029623463,0.506397721,0.001014989,0.017715317,0.108884233
7909,machine-translation9,185,Both the baseline model and the compressed models have the same computational graph except the embedding layer .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,28,1,0,,0.366043652,0,negative,0.002637328,0.004126435,0.012085546,0.00013639,0.000278483,0.029449615,0.014825593,0.451816108,0.002635832,0.474844074,1.65E-05,0.002017995,0.005130099
7910,machine-translation9,186,The model is composed of a single LSTM layer with 150 hidden units and a softmax layer for predicting the binary label .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,29,1,0,,0.586118483,1,hyperparameters,0.000198448,0.000307191,0.000212621,0.000141601,2.80E-05,0.059903192,0.008706544,0.910368476,0.000295647,0.015188178,4.39E-06,0.000111521,0.004534207
7911,machine-translation9,187,"For the baseline model , the embedding layer contains a large 75 K 300 embedding matrix initialized by GloVe embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,30,1,0,,0.939937717,1,hyperparameters,0.000123507,0.000218234,6.31E-05,3.13E-05,1.29E-05,0.032653265,0.004044986,0.949068766,0.000107216,0.012209104,1.20E-06,8.71E-05,0.001379411
7912,machine-translation9,188,"For the compressed models based on the compositional coding , the embedding layer maintains a matrix of basis vectors .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,31,1,0,,0.252402023,0,negative,0.004455053,0.005400145,0.045547196,2.98E-05,0.000195447,0.004756925,0.003225493,0.108169104,0.006161329,0.815533503,1.85E-05,0.004292948,0.002214534
7913,machine-translation9,189,"Suppose we use a 32 16 coding scheme , the basis matrix will then have a shape of 512 300 , which is initialized by the concatenated weight matrices [ A 1 ; A 2 ; ... ; AM ] in the code learning model .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,32,1,0,,0.014554089,0,hyperparameters,0.000260379,0.000324579,0.000205811,1.96E-05,3.81E-05,0.041484306,0.006865602,0.764113666,0.0001835,0.184319534,4.30E-06,0.000696495,0.00148409
7914,machine-translation9,190,The embedding parameters for both models remain fixed during the training .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,33,1,0,,0.712526834,1,hyperparameters,8.51E-05,0.000260901,3.44E-05,9.50E-06,7.92E-06,0.016307273,0.001946563,0.925900551,0.000123036,0.054649783,1.64E-06,0.000152261,0.000521153
7915,machine-translation9,191,"For the models with network pruning , the sparse embedding matrix is finetuned during training .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,34,1,0,,0.628653535,1,hyperparameters,0.003803375,0.002548487,0.000878431,3.27E-05,0.000129825,0.010205345,0.00302922,0.692309584,0.000589623,0.282776959,2.42E-06,0.002256728,0.001437296
7916,machine-translation9,192,Training details :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,35,1,0,,0.013661859,0,negative,0.000210573,0.000344109,0.000462913,0.000538131,0.000934493,0.011356354,0.006860778,0.098725115,7.90E-05,0.874772652,8.10E-06,0.002861716,0.002846045
7917,machine-translation9,193,The models are trained with Adam optimizer for 15 epochs with a fixed learning rate of 0.0001 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,36,1,1,experiments,0.985199755,1,hyperparameters,2.43E-05,4.84E-05,7.61E-06,1.89E-05,4.94E-06,0.024370756,0.002820889,0.968518362,1.62E-05,0.003294247,3.54E-07,3.08E-05,0.000844388
7918,machine-translation9,194,"At the end of each epoch , we evaluate the loss on a small validation set .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,37,1,0,,0.066874741,0,negative,0.003693916,0.002989895,0.002046851,1.43E-05,0.000133247,0.005595905,0.00693866,0.272791824,0.000741678,0.696517781,2.97E-06,0.006544022,0.001988979
7919,machine-translation9,195,The parameters with lowest validation loss are saved .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,38,1,0,,0.012642002,0,negative,0.000413509,9.70E-05,0.000210018,3.25E-06,2.11E-05,0.002303957,0.000855687,0.066328037,7.01E-05,0.928354165,4.85E-07,0.001113445,0.000229227
7920,machine-translation9,196,"Results : For different settings of the number of components M and the number of codewords K , we train the code learning network .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,39,1,0,,0.558963228,1,results,0.005824666,0.000170474,0.000984116,6.71E-06,5.69E-05,0.00055736,0.034090695,0.005678481,2.17E-05,0.311378432,1.67E-05,0.639359478,0.001854336
7921,machine-translation9,197,The average reconstruction loss on a fixed validation set is summarized in the left of .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,40,1,0,,0.001689042,0,negative,0.000548783,2.42E-05,6.11E-05,3.48E-06,3.00E-05,0.000663038,0.001407231,0.010356615,7.33E-06,0.974496147,8.40E-07,0.012257305,0.000143884
7922,machine-translation9,198,"For reference , we also report the total size ( MB ) of the embedding layer in the right table , which includes the sizes of the basis matrix and the hash table .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,41,1,0,,0.0222348,0,negative,8.84E-05,2.97E-05,4.56E-05,4.35E-06,3.00E-05,0.001395067,0.000897806,0.024741995,1.06E-05,0.97066566,3.22E-07,0.001975146,0.000115256
7923,machine-translation9,199,We can see that increasing either M or K can effectively decrease the reconstruction loss .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,42,1,0,,0.166480126,0,negative,0.039157447,0.000129855,0.000249144,1.25E-05,5.67E-05,0.00078791,0.01162483,0.023407718,3.48E-05,0.549910585,5.33E-06,0.373360651,0.001262525
7924,machine-translation9,200,"However , setting M to a large number will result in longer hash codes , thus significantly increase the size of the embedding layer .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,43,1,0,,0.005834114,0,negative,0.003754318,9.07E-05,0.000132516,1.55E-05,5.47E-05,0.001832245,0.002920711,0.032860017,4.00E-05,0.93807943,5.31E-06,0.019135703,0.001078821
7925,machine-translation9,201,"Hence , it is important to choose correct numbers for M and K to balance the performance and model size .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,44,1,0,,0.00950881,0,negative,0.00098835,6.52E-05,4.90E-05,7.45E-05,5.79E-05,0.001988434,0.001007781,0.036908117,3.29E-05,0.952511486,7.60E-06,0.005652634,0.000656097
7926,machine-translation9,202,"To see how the reconstructed loss translates to the classification accuracy , we train the sentiment analysis model for different settings of code schemes and report the results in : Trade - off between the model performance and the size of embedding layer on IMDB sentiment analysis task",EXPERIMENTS,EXPERIMENTS,machine-translation,9,45,1,0,,0.130472543,0,results,0.00190111,0.000248009,0.000372457,8.81E-06,3.53E-05,0.00078675,0.04678093,0.019152164,3.14E-05,0.434238096,6.07E-05,0.492002968,0.004381278
7927,machine-translation9,203,We also show the results using normalized product quantization ( NPQ ) .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,46,1,0,,0.723351272,1,negative,0.001784584,0.000167148,0.00268787,2.17E-06,5.68E-05,0.000222151,0.006642557,0.002617599,2.60E-05,0.873305455,1.78E-06,0.111969536,0.000516301
7928,machine-translation9,204,"We quantize the filtered Glo Ve embeddings with the codes provided by the authors , and train the models based on the quantized embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,47,1,0,,0.875724349,1,hyperparameters,9.46E-05,0.000349706,7.38E-05,3.97E-05,4.55E-05,0.022135317,0.004215646,0.928689483,9.19E-05,0.041963284,1.33E-07,0.000108816,0.002192211
7929,machine-translation9,205,"To make the results comparable , we report the codebook size in numpy format .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,48,1,0,,0.009327502,0,negative,0.000135367,9.52E-05,0.000106943,2.25E-05,7.39E-05,0.014518287,0.004922303,0.207714497,2.46E-05,0.769989518,2.93E-07,0.001658224,0.000738436
7930,machine-translation9,206,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,49,1,1,experiments,0.834340241,1,hyperparameters,0.000358985,0.000813867,0.000353976,4.05E-05,7.43E-05,0.016699733,0.0147096,0.839991688,0.000222925,0.116766858,2.02E-06,0.00258338,0.007382177
7931,machine-translation9,207,"In this case , the total size of the embedding layer is 1.23 MB , which is equivalent to a compression rate of 98.4 % .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,50,1,0,,0.903286053,1,hyperparameters,0.000463572,0.000361677,0.000170868,4.42E-05,0.000113333,0.026679033,0.030056715,0.823753148,9.45E-05,0.105105548,7.64E-07,0.002216098,0.010940599
7932,machine-translation9,208,We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,51,1,1,experiments,0.376761473,0,results,0.008745332,4.09E-05,0.000130361,4.82E-05,7.71E-05,0.001919115,0.033358673,0.037663661,1.81E-05,0.343798454,4.62E-06,0.563579158,0.010616382
7933,machine-translation9,209,The improved model performance maybe a byproduct of the strong regularization .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,52,1,0,,0.007986436,0,negative,0.01173335,4.48E-05,0.000249609,1.04E-05,8.95E-05,0.000467607,0.001310946,0.007970732,2.20E-05,0.952958361,2.69E-07,0.024684607,0.000457807
7934,machine-translation9,210,MACHINE TRANSLATION,EXPERIMENTS,EXPERIMENTS,machine-translation,9,53,1,1,experiments,0.988940825,1,experiments,0.00083382,3.04E-05,0.000356443,0.000397589,0.000402953,0.001627395,0.527610803,0.007516438,2.52E-06,0.027594176,2.46E-05,0.299643239,0.133959615
7935,machine-translation9,211,"Dataset : For machine translation tasks , we experiment on IWSLT 2014 German - to - English translation task and ASPEC English - to - Japanese translation task .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,54,1,0,,0.603631452,1,negative,0.001031551,0.000246015,0.003680214,0.00383043,0.015868258,0.008212942,0.236940713,0.024947009,1.68E-05,0.372654632,2.43E-05,0.14909901,0.183448113
7936,machine-translation9,212,"The IWSLT14 training data contains 178K sentence pairs , which is a small dataset for machine translation .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,55,1,0,,0.1454795,0,negative,0.00046248,0.000248203,0.001634901,0.000588483,0.021229118,0.006035262,0.036590341,0.029643836,1.53E-05,0.855571173,1.22E-06,0.028444234,0.019535412
7937,machine-translation9,213,We utilize moses toolkit to tokenize and lowercase both sides of the texts .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,56,1,0,,0.929629424,1,hyperparameters,0.000279308,0.000938697,0.001540761,0.000129739,0.000251727,0.032083797,0.020349923,0.810926827,0.000408565,0.106173581,3.68E-07,0.000684666,0.026232041
7938,machine-translation9,214,Then we concatenate all five TED / TEDx development and test corpus to form a test set containing 6750 sentence pairs .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,57,1,0,,0.153644818,0,negative,0.000704309,0.000618162,0.001540349,8.27E-05,0.010874976,0.001913323,0.008703442,0.028762175,4.64E-05,0.932099447,2.31E-07,0.010567658,0.00408686
7939,machine-translation9,215,We apply byte - pair encoding to transform the texts to subword level so that the vocabulary has a size of 20 K for each language .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,58,1,0,,0.870441019,1,hyperparameters,9.94E-05,0.000243974,0.000129669,1.56E-05,2.79E-05,0.0223313,0.014635341,0.927805625,8.22E-05,0.028455848,8.60E-08,0.000195903,0.005977054
7940,machine-translation9,216,"For evaluation , we report tokenized BLEU using "" multi -bleu.perl "" .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,59,1,0,,0.008514712,0,negative,0.000141345,4.48E-05,0.000439557,4.86E-06,0.000315994,0.000540492,0.009904318,0.007857163,2.96E-06,0.941196741,1.58E-07,0.03841803,0.001133597
7941,machine-translation9,217,The ASPEC dataset contains 300M bilingual pairs in the training data with the automatically estimated quality scores provided for each pair .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,60,1,0,,0.070168481,0,negative,0.000764122,0.000731093,0.00359387,0.000550404,0.042159276,0.003433202,0.018643458,0.027582584,3.02E-05,0.868886865,4.52E-07,0.020006534,0.013617969
7942,machine-translation9,218,We only use the first 150M pairs for training the models .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,61,1,0,,0.265233683,0,hyperparameters,0.000507997,0.000277082,0.000386953,6.70E-05,0.000849806,0.017783763,0.031029383,0.546981725,3.29E-05,0.383853162,1.05E-07,0.004097502,0.014132592
7943,machine-translation9,219,The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,62,1,0,,0.49518321,0,negative,0.000149021,0.000128255,0.000490478,1.22E-05,0.00056437,0.005593641,0.011108035,0.131756778,2.83E-05,0.844503045,6.32E-08,0.002747581,0.002918242
7944,machine-translation9,220,The vocabulary size for each language is reduced to 40K using byte - pair encoding .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,63,1,0,,0.928308172,1,hyperparameters,0.000146448,0.000117139,5.33E-05,1.03E-05,2.36E-05,0.012204876,0.02306552,0.927059473,3.43E-05,0.026038008,8.88E-08,0.000706515,0.010540441
7945,machine-translation9,221,The evaluation is performed using a standard kytea - based post -processing script for this dataset .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,64,1,0,,0.016482877,0,negative,8.40E-05,3.81E-05,0.000192811,1.12E-06,9.07E-05,0.000298745,0.00130587,0.005685538,4.46E-06,0.988979518,1.73E-08,0.003151516,0.000167668
7946,machine-translation9,222,Model architecture :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,65,1,0,,0.433322463,0,negative,0.004410006,0.000962417,0.020015513,0.001137782,0.000750559,0.013500588,0.031770734,0.130494833,0.006450383,0.537974646,1.21E-05,0.016267108,0.236253283
7947,machine-translation9,223,"In our preliminary experiments , we found a 32 16 coding works well for a vanilla NMT model .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,66,1,0,,0.388317908,0,results,0.00210139,2.75E-05,9.12E-05,2.38E-05,6.12E-05,0.00738828,0.107992152,0.183662826,7.97E-06,0.336979899,6.09E-07,0.340581228,0.021081931
7948,machine-translation9,224,"As it is more meaningful to test on a high - performance model , we applied several techniques to improve the performance .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,67,1,0,,0.009613973,0,negative,0.009998955,0.000100431,0.000796723,7.95E-06,0.000271126,0.000505192,0.004296579,0.005736052,1.80E-05,0.921261187,7.70E-08,0.055832729,0.001174981
7949,machine-translation9,225,The model has a standard bi-directional encoder composed of two LSTM layers similar to .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,68,1,0,,0.017492714,0,hyperparameters,0.001049996,0.000633115,0.005927178,0.000389593,0.000337093,0.025901623,0.024533324,0.572740357,0.0019393,0.281982115,7.45E-07,0.001266044,0.083299518
7950,machine-translation9,226,The decoder contains two LSTM layers .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,69,1,0,,0.763186813,1,hyperparameters,0.00282998,0.001631696,0.009137387,0.000370789,0.000548862,0.01432532,0.020982406,0.465372503,0.004807894,0.318844625,1.14E-06,0.002074161,0.159073238
7951,machine-translation9,227,Residual connection with a scaling factor of 1 / 2 is applied to the two decoder states to compute the outputs .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,70,1,0,,0.759604681,1,hyperparameters,0.001530716,0.000815405,0.002096919,7.09E-05,0.000168375,0.015429785,0.02131121,0.797240992,0.001236898,0.126041163,1.81E-07,0.001263615,0.032793809
7952,machine-translation9,228,All LSTMs and embeddings have 256 hidden units in the IWSLT14 task and 1000 hidden units in ASPEC task .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,71,1,0,,0.963181399,1,hyperparameters,4.63E-05,5.47E-05,7.63E-05,3.97E-05,4.09E-05,0.033458895,0.042368907,0.879303675,1.56E-05,0.021877159,4.15E-08,0.000464597,0.02225327
7953,machine-translation9,229,The decoder states are firstly linearly transformed to 600 - dimensional vectors before computing the final softmax .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,72,1,0,,0.87711309,1,hyperparameters,0.000170191,0.000191049,0.000127888,4.57E-06,3.61E-05,0.009552967,0.01100649,0.84860017,0.000135066,0.123956473,2.89E-08,0.000389286,0.005829767
7954,machine-translation9,230,Dropout with a rate of 0.2 is applied everywhere except the recurrent computation .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,73,1,0,,0.97983266,1,hyperparameters,7.44E-05,5.67E-05,2.96E-05,3.85E-05,2.13E-05,0.013608287,0.010501905,0.957532818,2.24E-05,0.006245905,1.55E-08,0.000139982,0.011728114
7955,machine-translation9,231,"We apply Key - Value Attention to the first decoder , where the query is the sum of the feedback embedding and the previous decoder state and the keys are computed by linear transformation of encoder states .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,74,1,0,,0.390159564,0,negative,0.007437471,0.008249578,0.046675654,1.42E-05,0.00057458,0.002686982,0.0244371,0.090712368,0.007403835,0.773040864,3.88E-07,0.012539029,0.026227974
7956,machine-translation9,232,Training details :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,75,1,0,,0.015928407,0,negative,7.10E-05,9.22E-05,0.000274562,0.000141639,0.000860179,0.004347827,0.006552672,0.083637777,1.60E-05,0.895087709,6.52E-08,0.002526619,0.006391681
7957,machine-translation9,233,All models are trained by Nesterov 's accelerated gradient with an initial learning rate of 0.25 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,76,1,1,experiments,0.987638616,1,hyperparameters,1.96E-05,3.14E-05,1.31E-05,1.91E-05,1.29E-05,0.013242731,0.008824698,0.966793297,7.56E-06,0.004243362,5.61E-09,6.58E-05,0.006726393
7958,machine-translation9,234,"We evaluate the smoothed BLEU ) on a validation set composed of 50 batches every 7,000 iterations .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,77,1,0,,0.019613173,0,negative,0.000194745,6.79E-05,6.69E-05,3.15E-06,5.31E-05,0.002501399,0.008648399,0.159743461,9.67E-06,0.821471189,1.51E-08,0.005160165,0.002079826
7959,machine-translation9,235,The learning rate is reduced by a factor of 10 if no improvement is observed in 3 validation runs .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,78,1,0,,0.8938073,1,hyperparameters,0.000243483,7.90E-05,2.58E-05,1.23E-05,3.17E-05,0.006658554,0.010579783,0.944784661,1.92E-05,0.026335203,1.11E-08,0.000458632,0.010771733
7960,machine-translation9,236,The training ends after the learning rate is reduced three times .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,79,1,0,,0.369191496,0,hyperparameters,0.000554423,0.000195248,0.000309494,4.39E-06,5.67E-05,0.004187899,0.020497866,0.531125427,8.59E-05,0.425507982,3.74E-08,0.003695513,0.013779117
7961,machine-translation9,237,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,80,1,1,experiments,0.854464377,1,hyperparameters,0.000192556,0.000252867,0.00042674,0.000306132,0.000525068,0.045397491,0.045206661,0.726989369,5.58E-05,0.099971385,3.75E-08,0.000982912,0.079693002
7962,machine-translation9,238,We firstly train a baseline NMT model to obtain the task - specific embeddings for all in - vocabulary words in both languages .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,81,1,0,,0.565258509,1,negative,0.001198485,0.002202248,0.016610045,7.10E-06,0.000306171,0.002685244,0.014496958,0.118679612,0.001450113,0.824098189,6.51E-08,0.002795324,0.01547045
7963,machine-translation9,239,"Then based on these baseline embeddings , we obtain the hash codes and basis vectors by training the code learning model .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,82,1,0,,0.042719653,0,negative,0.00018389,0.000180139,0.0013676,1.11E-06,5.72E-05,0.000165537,0.000745623,0.010320234,0.000224715,0.984183964,8.61E-09,0.00116084,0.001409098
7964,machine-translation9,240,"Finally , the NMT models using compositional coding are retrained by plugging in the reconstructed embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,83,1,0,,0.71921227,1,negative,0.006188272,0.003876766,0.02207161,1.13E-05,0.000518586,0.001325306,0.011463233,0.059232457,0.003121594,0.870241747,8.16E-08,0.005730085,0.016218989
7965,machine-translation9,241,"Note that the embedding layer is fixed in this phase , other parameters are retrained from random initial values .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,84,1,0,,0.110571697,0,negative,0.000783324,0.000214993,0.000371878,3.17E-06,6.58E-05,0.00154566,0.002613549,0.130298933,0.000134894,0.860858797,5.87E-09,0.001241366,0.001867581
7966,machine-translation9,242,Results :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,85,1,0,,0.089694074,0,negative,0.000377274,2.03E-05,0.000234689,6.72E-07,3.07E-05,0.000130847,0.005584502,0.002744964,5.95E-06,0.953018756,2.12E-08,0.036826864,0.001024465
7967,machine-translation9,243,The experimental results are summarized in .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,86,1,0,,0.002297305,0,negative,4.70E-05,1.44E-06,4.32E-05,7.40E-07,2.74E-05,7.36E-05,0.000413892,0.00090365,7.78E-07,0.995353401,2.58E-09,0.002908309,0.000226662
7968,machine-translation9,244,All translations are decoded by the beam search with a beam size of 5 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,87,1,0,,0.961158684,1,hyperparameters,9.30E-05,9.40E-05,0.000107578,5.82E-06,3.62E-05,0.011855315,0.02411097,0.887226041,3.98E-05,0.061121988,4.63E-09,0.000381192,0.014928099
7969,machine-translation9,245,The performance of iterative pruning varies between tasks .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,88,1,0,,0.013035184,0,negative,0.000126827,1.03E-05,9.18E-05,8.54E-07,6.40E-06,0.000153843,0.003074374,0.006094709,3.53E-06,0.971577579,1.17E-07,0.017419343,0.001440341
7970,machine-translation9,246,The loss - free compression rate reaches 92 % on ASPEC dataset by pruning 90 % of the connections .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,89,1,1,experiments,0.95856159,1,results,0.003321156,1.95E-05,0.000135802,1.81E-06,2.48E-05,0.000305046,0.151177002,0.012793869,2.65E-06,0.119110298,2.03E-08,0.677903507,0.035204538
7971,machine-translation9,247,"However , with the same pruning ratio , a modest performance loss is observed in IWSLT14 dataset .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,90,1,1,experiments,0.326377079,0,results,0.016572948,6.67E-06,7.19E-05,1.28E-06,2.21E-05,0.000103202,0.057628946,0.003911558,5.12E-07,0.113188733,1.03E-08,0.801902825,0.006589315
7972,machine-translation9,248,"For the models using compositional coding , the loss - free compression rate is 94 % for the IWSLT14 dataset and 99 % for the ASPEC dataset .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,91,1,1,experiments,0.983755874,1,experiments,0.00221701,0.000103797,0.000385249,4.89E-06,0.000102165,0.00239852,0.318138549,0.16461927,9.34E-06,0.231943965,1.21E-08,0.219275813,0.060801412
7973,machine-translation9,249,"Similar to the sentiment analysis task , a significant performance improvement can be observed by slightly lowering the compression rate .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,92,1,0,,0.779456489,1,results,0.004894707,3.20E-06,3.30E-05,8.53E-07,1.18E-05,5.69E-05,0.044222831,0.002217677,2.92E-07,0.091763906,6.74E-09,0.851482844,0.005312017
7974,machine-translation9,250,"Note that the sizes of NMT models are still quite large due to the big softmax layer and the recurrent layers , which are not reported in the , we show some examples of learned codes based on the 300 - dimensional uncased GloVe embeddings used in the sentiment analysis task .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,93,1,0,,0.059953875,0,negative,0.000134026,1.19E-05,9.82E-05,2.04E-06,0.00012005,0.000361064,0.003371576,0.008298324,1.53E-06,0.980089448,2.06E-09,0.006435416,0.001076415
7975,machine-translation9,251,We can see that the model learned to assign similar codes to the words with similar meanings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,94,1,0,,0.003900861,0,negative,0.001159584,7.45E-06,8.99E-05,5.18E-07,1.41E-05,0.000201821,0.003616426,0.009246595,5.97E-06,0.971681353,2.25E-09,0.012497969,0.001478301
7976,machine-translation9,252,"Such a code - sharing mechanism can significantly reduce the redundancy of the word embeddings , thus helping to achieve a high compression rate .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,95,1,0,,0.253396708,0,negative,0.00291302,0.000105173,0.000858652,1.95E-06,5.72E-05,0.000125788,0.003192569,0.004401945,8.07E-05,0.945984381,1.36E-08,0.038520435,0.00375811
7977,machine-translation9,253,ANALYSIS OF CODE EFFICIENCY,EXPERIMENTS,EXPERIMENTS,machine-translation,9,96,1,0,,0.002160008,0,negative,0.0002666,1.88E-05,0.000377022,4.35E-06,2.59E-05,0.00057288,0.029184329,0.011507404,1.27E-05,0.9080993,3.59E-07,0.032026953,0.017903381
7978,machine-translation9,254,"Besides the performance , we also care about the storage efficiency of the codes .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,97,1,0,,0.003392184,0,negative,0.000105916,2.53E-05,7.37E-05,3.73E-07,1.12E-05,0.000128402,0.000606174,0.010895223,2.55E-05,0.985918005,2.47E-09,0.001640389,0.00056977
7979,machine-translation9,255,"In the ideal situation , all codewords shall be fully utilized to convey a fraction of meaning .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,98,1,0,,0.022978192,0,negative,0.000685012,6.75E-05,0.000707692,1.77E-06,7.77E-05,0.00040301,0.002511253,0.016263163,7.05E-05,0.970346047,3.00E-09,0.004721089,0.00414524
7980,machine-translation9,256,"However , as the codes are category word 8 8 code 16 16 code dog 0 7 0 1 7 3 7 0 7 7 0 8 3 5 8 5 B 2 E E 0 B 0 A animal cat 7 7 0 1 7 3 7 0 7 7 2 8 B 5 8 CB 2 E E 4 B 0 A penguin 0 7 0 1 7 3 6 0 7 7 E 8 7 6 4 CF DE 3 D 8 0 A go 7 7 0 6 4 3 3 0 2 C C 8 2 C 1 1 B D 0 E 0 B 5 8 verb went 4 0 7 6 4 3 2 0 BC C 6 BC 7 5 B 8 6 E 0 D 0 4 gone 7 7 0 6 4 3 3 0 2 C C 8 0 B 1 5 B D 6 E 0 2 5 A : Examples of learned compositional codes based on Glo Ve embedding vectors automatically learned , it is possible that some codewords are abandoned during the training .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,99,1,0,,0.000920278,0,negative,3.89E-05,3.56E-06,0.000203053,9.58E-08,1.72E-05,0.000107591,0.000874858,0.001254506,2.39E-06,0.99554607,5.36E-10,0.001715357,0.000236384
7981,machine-translation9,257,"In extreme cases , some "" dead "" codewords can be used by none of the words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,100,1,0,,0.004980494,0,negative,0.000256752,4.38E-06,7.72E-05,2.81E-07,1.49E-05,6.40E-05,0.000423827,0.002082334,3.19E-06,0.994302617,3.79E-10,0.002504111,0.000266379
7982,machine-translation9,258,"To analyze the code efficiency , we count the number of words that contain a specific subcode in each component .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,101,1,0,,0.013466663,0,negative,8.00E-05,1.20E-05,8.67E-05,9.50E-08,2.13E-05,9.07E-05,0.000561039,0.00415764,4.74E-06,0.993595495,2.21E-10,0.001165096,0.000225208
7983,machine-translation9,259,gives a visualization of the code balance for three coding schemes .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,102,1,0,,0.00547291,0,negative,9.48E-05,4.14E-06,0.000275252,2.23E-06,3.26E-05,0.000232381,0.002667813,0.004211987,3.38E-06,0.985446319,2.84E-09,0.003226511,0.003802535
7984,machine-translation9,260,Each column shows the counts of the subcodes of a specific component .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,103,1,0,,0.003952535,0,negative,1.51E-05,2.20E-06,2.31E-05,3.76E-07,1.82E-05,0.000294471,0.000781261,0.011655135,2.07E-06,0.986137628,1.70E-10,0.000566095,0.000504351
7985,machine-translation9,261,"In our experiments , when using a 8 8 coding scheme , we found 31 % of the words have a subcode "" 0 "" for the first component , while the subcode "" 1 "" is only used by 5 % of the words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,104,1,0,,0.12361458,0,negative,0.002610749,1.84E-05,0.000168628,1.27E-06,0.000242575,0.000424695,0.034012344,0.011504964,1.63E-06,0.84828336,1.08E-09,0.095467948,0.007263457
7986,machine-translation9,262,The assignment of codes is more balanced for larger coding schemes .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,105,1,0,,0.557104951,1,negative,0.005625208,5.91E-05,0.00021937,5.52E-06,7.56E-05,0.000593503,0.022839987,0.048581621,1.98E-05,0.820676184,3.92E-09,0.085731012,0.015573113
7987,machine-translation9,263,"In any coding scheme , even the most unpopular codeword is used by about 1000 words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,106,1,0,,0.002507135,0,negative,0.000285896,3.31E-05,0.00024659,3.03E-06,0.000130245,0.001212157,0.007747957,0.058801405,1.33E-05,0.914430406,1.70E-09,0.002563311,0.014532598
7988,machine-translation9,264,This result indicates that the code learning model is capable of assigning codes efficiently without wasting a codeword .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,107,1,0,,0.049071773,0,negative,0.000622217,9.84E-06,8.12E-05,2.06E-07,9.25E-06,7.95E-05,0.003330821,0.003346844,6.88E-06,0.976560186,1.01E-09,0.014741393,0.001211663
7989,machine-translation9,265,The results show that any codeword is assigned to more than 1000 words without wasting .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,108,1,0,,0.575841519,1,negative,0.001574233,2.90E-05,9.88E-05,1.42E-06,9.07E-05,0.000620979,0.043481397,0.037545581,4.13E-06,0.831853504,7.93E-10,0.065980649,0.018719647
7990,machine-translation9,266,CONCLUSION,,,machine-translation,9,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
7991,natural_language_inference21,1,title,,,natural_language_inference,21,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
7992,natural_language_inference21,2,DiSAN : Directional Self - Attention Network for RNN / CNN - Free Language Understanding,title,title,natural_language_inference,21,1,1,1,research-problem,0.999099045,1,research-problem,1.05E-07,2.29E-05,4.23E-07,1.03E-06,5.81E-07,7.08E-07,7.98E-06,4.98E-06,2.06E-06,0.003055414,0.996902697,6.47E-07,4.29E-07
7993,natural_language_inference21,3,abstract,,,natural_language_inference,21,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
7994,natural_language_inference21,4,"Recurrent neural nets ( RNN ) and convolutional neural nets ( CNN ) are widely used on NLP tasks to capture the long - term and local dependencies , respectively .",abstract,abstract,natural_language_inference,21,1,1,0,,0.444789976,0,research-problem,3.87E-08,2.60E-05,9.87E-08,3.56E-07,1.36E-07,1.21E-06,6.17E-07,1.12E-05,8.23E-06,0.023853684,0.97609829,3.02E-08,6.22E-08
7995,natural_language_inference21,5,"Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation , significantly less training time , and flexibility in modeling dependencies .",abstract,abstract,natural_language_inference,21,2,1,0,,0.061660157,0,research-problem,2.99E-08,1.82E-05,2.09E-08,4.62E-07,1.62E-07,5.44E-07,4.11E-07,5.07E-06,4.20E-06,0.02085796,0.97911288,2.67E-08,4.10E-08
7996,natural_language_inference21,6,"We propose a novel attention mechanism in which the attention between elements from input sequence ( s ) is directional and multi-dimensional ( i.e. , feature - wise ) .",abstract,abstract,natural_language_inference,21,3,1,0,,0.3334096,0,research-problem,2.49E-05,0.198202521,0.00013447,1.27E-05,0.000117633,3.95E-05,1.50E-05,0.000573456,0.097883875,0.211239567,0.491745797,6.99E-06,3.52E-06
7997,natural_language_inference21,7,"A light - weight neural net , "" Directional Self - Attention Network ( DiSAN ) "" , is then proposed to learn sentence embedding , based solely on the proposed attention without any RNN / CNN structure .",abstract,abstract,natural_language_inference,21,4,1,0,,0.470102124,0,research-problem,3.90E-05,0.227143271,0.000311837,2.90E-05,0.000348248,5.53E-05,2.56E-05,0.000684483,0.084984195,0.262791467,0.423571167,9.92E-06,6.46E-06
7998,natural_language_inference21,8,"DiSAN is only composed of a directional self - attention with temporal order encoded , followed by a multi-dimensional attention that compresses the sequence into a vector representation .",abstract,abstract,natural_language_inference,21,5,1,0,,0.174481971,0,research-problem,9.20E-05,0.064361566,0.002445079,3.38E-05,0.000671778,7.09E-05,5.25E-05,0.000257586,0.013639082,0.458080037,0.460267793,2.13E-05,6.47E-06
7999,natural_language_inference21,9,"Despite it s simple form , DiSAN outperforms complicated RNN models on both prediction quality and time efficiency .",abstract,abstract,natural_language_inference,21,6,1,0,,0.048942462,0,negative,0.001655253,0.002609189,2.92E-05,0.000499461,0.00075923,0.000156007,0.000650914,0.000898187,4.35E-05,0.767437315,0.223070114,0.002157175,3.44E-05
8000,natural_language_inference21,10,"It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02 % on the Stanford Natural Language Inference ( SNLI ) dataset , and shows stateof - the - art test accuracy on the Stanford Sentiment Treebank ( SST ) , Multi - Genre natural language inference ( MultiNLI ) , Sentences Involving Compositional Knowledge ( SICK ) , Customer Review , MPQA , TREC question - type classification and Subjectivity ( SUBJ ) datasets .",abstract,abstract,natural_language_inference,21,7,1,0,,0.026284318,0,research-problem,5.58E-05,0.004947395,7.50E-05,1.78E-05,0.000116842,3.41E-05,0.000361783,0.000431415,0.000135688,0.237454434,0.755416001,0.000936896,1.69E-05
8001,natural_language_inference21,11,narrative,abstract,abstract,natural_language_inference,21,8,1,0,,0.007580665,0,research-problem,1.25E-05,0.002042138,2.68E-05,6.22E-05,0.000142521,7.68E-05,4.52E-05,0.000245254,0.001382517,0.388025155,0.607923222,1.08E-05,4.88E-06
8002,natural_language_inference21,12,"performance on a large number of NLP tasks , including neural machine translation , natural language inference , conversation generation , question answering , machine reading comprehension , and sentiment analysis .",abstract,abstract,natural_language_inference,21,9,1,0,,0.028682658,0,research-problem,2.91E-08,1.55E-05,4.21E-08,2.04E-07,4.27E-07,2.68E-07,6.66E-07,2.31E-06,1.11E-06,0.033589337,0.966389981,1.12E-07,4.26E-08
8003,natural_language_inference21,13,The attention uses a hidden layer to compute a categorical distribution over elements from the input sequence to reflect their importance weights .,abstract,abstract,natural_language_inference,21,10,1,0,,0.223196889,0,model,2.91E-05,0.137788091,0.000832957,1.17E-05,0.000174362,0.000107487,1.72E-05,0.00090984,0.445142815,0.380662328,0.034316837,3.50E-06,3.80E-06
8004,natural_language_inference21,14,"It allows RNN / CNN to maintain a variable - length memory , so that elements from the input sequence can be selected by their importance / relevance and merged into the output .",abstract,abstract,natural_language_inference,21,11,1,0,,0.049041621,0,negative,5.53E-05,0.119107526,0.000829893,1.72E-05,0.000409947,7.40E-05,1.54E-05,0.000513742,0.116743558,0.714801093,0.047420226,9.02E-06,3.06E-06
8005,natural_language_inference21,15,"In contrast to RNN and CNN , the attention mechanism is trained to capture the dependencies that make significant contributions to the task , regardless of the distance between the elements in the sequence .",abstract,abstract,natural_language_inference,21,12,1,0,,0.197943891,0,negative,8.19E-05,0.377019157,0.000386198,1.28E-05,0.000510754,5.74E-05,1.55E-05,0.000684673,0.135842144,0.45961686,0.025757426,1.27E-05,2.50E-06
8006,natural_language_inference21,16,It can thus provide complementary information to the distance - aware dependencies modeled by RNN / CNN .,abstract,abstract,natural_language_inference,21,13,1,0,,0.013394029,0,negative,9.82E-05,0.023134035,6.97E-05,1.37E-05,0.000442193,2.70E-05,6.56E-06,0.000139525,0.010774733,0.946646935,0.018627601,1.88E-05,1.02E-06
8007,natural_language_inference21,17,"In addition , computing attention only requires matrix multiplication , which is highly parallelizable compared to the sequential computation of RNN .",abstract,abstract,natural_language_inference,21,14,1,0,,0.11141997,0,negative,9.93E-05,0.056230308,5.14E-05,5.36E-05,0.000976486,4.24E-05,2.11E-05,0.00030478,0.003394774,0.833409114,0.105390766,2.26E-05,3.44E-06
8008,natural_language_inference21,18,"In a very recent work , an attention mechanism is solely used to construct a sequence to sequence ( seq2seq ) model that achieves a state - of - the - art quality score on the neural machine translation ( NMT ) task .",abstract,abstract,natural_language_inference,21,15,1,0,,0.756503131,1,research-problem,2.95E-07,0.000268328,5.88E-07,2.40E-06,2.36E-06,3.56E-06,2.93E-06,3.28E-05,6.45E-05,0.081879991,0.917741642,2.93E-07,3.34E-07
8009,natural_language_inference21,19,"The seq2seq model , "" Transformer "" , has an encoder - decoder structure that is only composed of stacked attention networks , without using either recurrence or convolution .",abstract,abstract,natural_language_inference,21,16,1,0,,0.548955035,1,negative,0.000156031,0.160011409,0.014775233,0.000165921,0.003432352,0.001010343,0.000283527,0.002342131,0.164599852,0.614303758,0.038847094,3.36E-05,3.87E-05
8010,natural_language_inference21,20,"The proposed attention , "" multi- head attention "" , projects the input sequence to multiple subspaces , then applies scaled dotproduct attention to its representation in each subspace , and lastly concatenates their output .",abstract,abstract,natural_language_inference,21,17,1,0,,0.386601853,0,approach,2.85E-05,0.333477535,0.000975325,1.42E-05,0.000470497,7.82E-05,3.29E-05,0.000581164,0.326501936,0.267040767,0.070784204,8.59E-06,6.23E-06
8011,natural_language_inference21,21,"By doing this , it can combine different attentions from multiple subspaces .",abstract,abstract,natural_language_inference,21,18,1,0,,0.041239566,0,negative,8.39E-06,0.02879171,3.16E-05,4.02E-06,0.000125237,1.93E-05,1.91E-06,0.000176626,0.034792584,0.931629683,0.004416695,1.90E-06,3.40E-07
8012,natural_language_inference21,22,This mechanism is used in Transformer to compute both the context - aware features inside the encoder / decoder and the bottleneck features between them .,abstract,abstract,natural_language_inference,21,19,1,0,,0.156244307,0,negative,2.69E-05,0.239303793,0.000249788,1.10E-05,0.000434437,5.24E-05,8.01E-06,0.000523775,0.357353597,0.397292876,0.00473712,4.11E-06,2.16E-06
8013,natural_language_inference21,23,"The attention mechanism has more flexibility in sequence length than RNN / CNN , and is more task / data - driven when modeling dependencies .",abstract,abstract,natural_language_inference,21,20,1,0,,0.109660537,0,negative,0.000262252,0.160494503,0.000248553,9.87E-05,0.001733952,0.000133876,2.79E-05,0.000971902,0.038359806,0.787034438,0.01059728,3.11E-05,5.79E-06
8014,natural_language_inference21,24,"Unlike sequential models , its computation can be easily and significantly accelerated by existing distributed / parallel computing schemes .",abstract,abstract,natural_language_inference,21,21,1,0,,0.090623008,0,negative,7.94E-06,0.005757099,6.05E-06,1.82E-05,0.000117043,1.43E-05,7.18E-06,8.34E-05,0.000590024,0.785141411,0.208249894,6.42E-06,1.07E-06
8015,natural_language_inference21,25,"However , to the best of our knowledge , a neural net entirely based on attention has not been designed for other NLP tasks except NMT , especially those that can not be cast into a seq2seq problem .",abstract,abstract,natural_language_inference,21,22,1,0,,0.054046778,0,research-problem,3.09E-07,0.000884486,6.83E-07,4.36E-06,9.06E-06,1.03E-05,4.88E-06,0.000105068,0.000103602,0.311743744,0.68713253,5.04E-07,4.51E-07
8016,natural_language_inference21,26,"Compared to RNN , a dis advantage of most attention mechanisms is that the temporal order information is lost , which however might be important to the task .",abstract,abstract,natural_language_inference,21,23,1,0,,0.042979914,0,negative,2.08E-05,0.002038496,6.54E-06,4.92E-06,3.16E-05,6.90E-06,7.53E-06,6.71E-05,0.000184289,0.84130713,0.156312325,1.18E-05,5.89E-07
8017,natural_language_inference21,27,This explains why positional encoding is applied to the sequence before being processed by the attention in Transformer .,abstract,abstract,natural_language_inference,21,24,1,0,,0.000747719,0,negative,5.20E-06,0.003250175,4.97E-06,3.62E-06,6.39E-05,1.87E-05,1.38E-06,0.000130957,0.003001359,0.991478881,0.002039957,7.36E-07,1.77E-07
8018,natural_language_inference21,28,How to model order information within an attention is still an open problem .,abstract,abstract,natural_language_inference,21,25,1,0,,0.230145199,0,research-problem,2.12E-07,0.000153301,1.02E-07,1.37E-05,4.80E-06,1.17E-05,3.21E-06,7.58E-05,1.72E-05,0.335624324,0.664094927,2.01E-07,5.06E-07
8019,natural_language_inference21,29,"The goal of this paper is to develop a unified and RNN / CNN - free attention network that can be generally utilized to learn the sentence encoding model for different NLP tasks , such as natural language inference , sentiment analysis , sentence classification and semantic relatedness .",abstract,abstract,natural_language_inference,21,26,1,0,,0.654586641,1,approach,2.56E-05,0.427602882,0.000124052,3.91E-05,0.000675146,5.01E-05,5.48E-05,0.000878569,0.038302092,0.245928228,0.286283195,2.59E-05,1.03E-05
8020,natural_language_inference21,30,We focus on the sentence encoding model because it is a basic module of most DNNs used in the NLP literature .,abstract,abstract,natural_language_inference,21,27,1,0,,0.081736359,0,approach,1.65E-05,0.523087689,4.57E-05,3.28E-05,0.000951706,0.000142865,3.04E-05,0.002664724,0.041118701,0.420524464,0.011371207,8.43E-06,4.70E-06
8021,natural_language_inference21,31,"We propose a novel attention mechanism that differs from previous ones in that it is 1 ) multi-dimensional : the attention w.r.t. each pair of elements from the source ( s ) is a vector , where each entry is the attention computed on each feature ; and 2 ) directional : it uses one or multiple positional masks to model the asymmetric attention between two elements .",abstract,abstract,natural_language_inference,21,28,1,1,model,0.349856264,0,approach,2.25E-05,0.516805437,0.000151499,1.22E-05,0.000401586,4.83E-05,2.15E-05,0.000638716,0.347969651,0.120208025,0.013705267,1.00E-05,5.32E-06
8022,natural_language_inference21,32,"We compute feature - wise attention since each element in a sequence is usually represented by a vector , e.g. , word / character embedding , and attention on different features can contain different information about dependency , thus to handle the variation of contexts around the same word .",abstract,abstract,natural_language_inference,21,29,1,1,model,0.138410278,0,approach,3.47E-05,0.432709362,0.000199969,6.15E-06,0.000595885,6.71E-05,1.03E-05,0.000993244,0.226578774,0.337449937,0.001345557,7.00E-06,1.93E-06
8023,natural_language_inference21,33,We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing .,abstract,abstract,natural_language_inference,21,30,1,1,model,0.216043595,0,approach,4.67E-05,0.60067545,0.000162759,2.15E-05,0.001317566,0.00011181,2.31E-05,0.001444285,0.192129263,0.202708926,0.001342456,1.17E-05,4.49E-06
8024,natural_language_inference21,34,"This design mitigates the weakness of attention in modeling order information , and takes full advantage of parallel computing .",abstract,abstract,natural_language_inference,21,31,1,0,,0.074947229,0,approach,4.82E-05,0.393486669,0.000302353,3.98E-05,0.002488281,8.99E-05,3.12E-05,0.000540392,0.208487804,0.389996206,0.004463247,1.94E-05,6.69E-06
8025,natural_language_inference21,35,"We then build a light - weight and RNN / CNN - free neural network , "" Directional Self - Attention Network ( DiSAN ) "" , for sentence encoding .",abstract,abstract,natural_language_inference,21,32,1,1,model,0.588064628,1,approach,5.99E-05,0.46521669,0.001082286,3.44E-05,0.003043413,0.00014607,7.20E-05,0.000940772,0.368730375,0.156825782,0.003804896,2.48E-05,1.86E-05
8026,natural_language_inference21,36,This network relies entirely on the proposed attentions and does not use any RNN / CNN structure .,abstract,abstract,natural_language_inference,21,33,1,0,,0.022762409,0,negative,4.10E-05,0.087874295,0.001125217,3.47E-05,0.002767219,0.000122202,2.70E-05,0.000484196,0.05721702,0.846956145,0.00333484,1.16E-05,4.48E-06
8027,natural_language_inference21,37,"In DiSAN , the input sequence is processed by directional ( forward and backward ) self - attentions to model context dependency and produce context - aware representations for all tokens .",abstract,abstract,natural_language_inference,21,34,1,1,model,0.537286374,1,negative,3.30E-05,0.394852075,0.000922787,2.88E-05,0.001271845,9.60E-05,5.37E-05,0.000683572,0.113166372,0.448993996,0.039866956,2.02E-05,1.06E-05
8028,natural_language_inference21,38,"Then , a multi-dimensional attention computes a vector representation of the entire sequence , which can be passed into a classification / regression module to compute the final prediction for a particular task .",abstract,abstract,natural_language_inference,21,35,1,1,model,0.338398541,0,model,1.52E-05,0.123397199,0.000303659,1.71E-06,0.000219403,2.58E-05,7.78E-06,0.000218688,0.650853009,0.223952852,0.000999147,3.96E-06,1.55E-06
8029,natural_language_inference21,39,"Unlike Transformer , neither stacking of attention blocks nor an encoderdecoder structure is required .",abstract,abstract,natural_language_inference,21,36,1,0,,0.043595216,0,negative,0.000331079,0.278518015,0.000294375,0.000141858,0.00609386,0.000151733,3.59E-05,0.001076646,0.034207623,0.67802238,0.001082466,3.81E-05,6.05E-06
8030,natural_language_inference21,40,"The simple architecture of DiSAN leads to fewer parameters , less computation and easier parallelization .",abstract,abstract,natural_language_inference,21,37,1,0,,0.009179421,0,negative,0.000467181,0.01766658,5.52E-05,0.001153977,0.009966461,0.000501235,0.000153656,0.000957876,0.001746826,0.963288366,0.0039246,0.000101314,1.67E-05
8031,natural_language_inference21,41,"In experiments 1 , we compare DiSAN with the currently popular methods on various NLP tasks , e.g. , natural language inference , sentiment analysis , sentence classification , etc .",abstract,abstract,natural_language_inference,21,38,1,0,,0.085081236,0,negative,3.22E-05,0.28881562,0.00016002,4.57E-05,0.014275093,0.000232633,0.000326372,0.001759358,0.002254989,0.688725021,0.00315152,0.000209886,1.16E-05
8032,natural_language_inference21,42,DiSAN achieves the highest test accuracy on the Stanford Natural Language Inference ( SNLI ) dataset among sentence - encoding models and improves the currently best result by 1.02 % .,abstract,abstract,natural_language_inference,21,39,1,0,,0.040403126,0,negative,0.000564386,0.027854463,0.000342387,0.000209039,0.005771254,0.000378329,0.008065942,0.003055733,0.00041028,0.901263234,0.030101222,0.021748674,0.000235056
8033,natural_language_inference21,43,"It also shows the state - of - the - art performance on the Stanford Sentiment Treebank ( SST ) , Multi - Genre natural language inference ( MultiNLI ) , SICK , Customer Review , MPQA , SUBJ and TREC question - type classification datasets .",abstract,abstract,natural_language_inference,21,40,1,0,,0.004390406,0,negative,1.82E-05,0.006287186,6.21E-05,6.25E-05,0.005566106,0.000102951,0.000393793,0.000345565,0.000136874,0.977833183,0.008755107,0.000424897,1.16E-05
8034,natural_language_inference21,44,"Meanwhile , it has fewer parameters and exhibits much higher computation efficiency than the mod-els it outperforms , e.g. , LSTM and tree - based models .",abstract,abstract,natural_language_inference,21,41,1,0,,0.00664583,0,negative,0.000661022,0.022074068,8.10E-05,0.000144299,0.004153646,0.000167565,0.000225644,0.000788537,0.000637162,0.968559856,0.002000433,0.000496331,1.05E-05
8035,natural_language_inference21,45,Annotation :,abstract,abstract,natural_language_inference,21,42,1,0,,0.000833684,0,negative,5.46E-06,0.004291517,8.69E-06,6.76E-05,0.001620474,5.34E-05,8.96E-06,0.000243682,0.000485953,0.991727969,0.001481371,3.29E-06,1.58E-06
8036,natural_language_inference21,46,1 ) Lowercase denotes a vector ; 2 ) bold lowercase denotes a sequence of vectors ( stored as a matrix ) ; and 3 ) uppercase denotes a matrix or a tensor .,abstract,abstract,natural_language_inference,21,43,1,0,,0.000323358,0,negative,4.38E-07,0.00420117,1.93E-06,1.07E-05,0.00013036,0.000140403,6.96E-06,0.001430827,0.002082295,0.991413863,0.000579322,9.13E-07,7.89E-07
8037,natural_language_inference21,47,Background,,,natural_language_inference,21,0,1,0,,6.12E-05,0,negative,4.94E-05,0.000177806,6.95E-06,6.36E-05,6.50E-06,0.000188754,4.78E-05,0.001033369,0.000170038,0.995496641,0.002736295,1.27E-05,1.01E-05
8038,natural_language_inference21,175,Experiments,,,natural_language_inference,21,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
8039,natural_language_inference21,176,"In this section , we first apply DiSAN to natural language inference and sentiment analysis tasks .",Experiments,Experiments,natural_language_inference,21,1,1,0,,0.112818713,0,negative,0.002835011,0.013645823,0.013923088,0.00045562,0.001266988,0.006783703,0.032523097,0.044889617,0.000439676,0.849571219,0.006186542,0.024439165,0.003040449
8040,natural_language_inference21,177,DiSAN achieves the state - of - the - art performance and significantly better efficiency than other baseline methods on benchmark datasets for both tasks .,Experiments,Experiments,natural_language_inference,21,2,1,0,,0.725941093,1,results,0.00174967,4.31E-05,0.000378801,1.26E-05,1.51E-05,0.000643481,0.044444489,0.006726427,3.03E-06,0.063620941,0.000345061,0.880242467,0.001774917
8041,natural_language_inference21,178,We also conduct experiments on other NLP tasks and DiSAN also achieves state - of - the - art performance .,Experiments,Experiments,natural_language_inference,21,3,1,0,,0.370431361,0,negative,0.004072592,0.000496358,0.001440015,3.92E-05,0.000132889,0.00322249,0.035713728,0.018613327,2.39E-05,0.661185539,0.000225469,0.273759193,0.001075245
8042,natural_language_inference21,179,Training Setup :,Experiments,Experiments,natural_language_inference,21,4,1,0,,0.03755676,0,hyperparameters,5.23E-05,0.000443661,0.000163777,1.04E-05,3.55E-06,0.029851586,0.001359401,0.849669971,0.000346245,0.117693838,9.26E-05,0.000117486,0.000195197
8043,natural_language_inference21,180,We use cross-entropy loss plus L2 regularization penalty as optimization objective .,Experiments,Experiments,natural_language_inference,21,5,1,1,experimental-setup,0.55571158,1,hyperparameters,0.000106026,0.000782077,6.66E-05,1.24E-05,5.83E-06,0.018550964,0.000765335,0.94201766,0.000291251,0.037143433,2.47E-05,6.93E-05,0.000164368
8044,natural_language_inference21,181,We minimize it by Adadelta ) ( an optimizer of mini - batch SGD ) with batch size of 64 .,Experiments,Experiments,natural_language_inference,21,6,1,1,experimental-setup,0.83114749,1,hyperparameters,9.08E-05,9.62E-05,0.000124595,5.61E-05,1.00E-05,0.19283695,0.008294707,0.769916891,3.47E-05,0.027992767,1.20E-05,7.84E-05,0.000455847
8045,natural_language_inference21,182,"We use Adadelta rather than Adam because in our experiments , DiSAN optimized by Adadelta can achieve more stable performance than Adam optimized one .",Experiments,Experiments,natural_language_inference,21,7,1,0,,0.353223929,0,hyperparameters,0.000234545,0.000317533,0.000370304,1.27E-05,1.10E-05,0.073573271,0.005908172,0.814356077,7.49E-05,0.10441041,2.45E-05,0.000368933,0.000337707
8046,natural_language_inference21,183,Initial learning rate is set to 0.5 .,Experiments,Experiments,natural_language_inference,21,8,1,1,experimental-setup,0.954273362,1,hyperparameters,2.34E-05,4.72E-05,9.70E-06,6.86E-06,1.22E-06,0.030766169,0.00097421,0.958978199,2.94E-05,0.008966631,1.14E-05,3.33E-05,0.000152323
8047,natural_language_inference21,184,"All weight matrices are initialized by Glorot Initialization , and the biases are initialized with 0 .",Experiments,Experiments,natural_language_inference,21,9,1,1,experimental-setup,0.957348857,1,hyperparameters,1.90E-05,6.59E-05,1.09E-05,3.41E-06,1.03E-06,0.030742161,0.000735002,0.956944029,3.38E-05,0.011342079,5.33E-06,1.90E-05,7.84E-05
8048,natural_language_inference21,185,We initialize the word embedding in x by 300D Glo Ve 6B pre-trained vectors .,Experiments,Experiments,natural_language_inference,21,10,1,1,experimental-setup,0.96260763,1,hyperparameters,2.70E-05,5.81E-05,1.43E-05,8.91E-06,1.99E-06,0.063993177,0.002238589,0.922916193,2.55E-05,0.01044538,7.00E-06,4.73E-05,0.000216618
8049,natural_language_inference21,186,"The Out - of - Vocabulary words in training set are randomly initialized by uniform distribution between ( ? 0.05 , 0.05 ) .",Experiments,Experiments,natural_language_inference,21,11,1,1,experimental-setup,0.942924394,1,hyperparameters,1.76E-05,5.71E-05,9.67E-06,2.76E-06,9.92E-07,0.032408737,0.000884199,0.951180726,2.57E-05,0.015297278,5.09E-06,2.61E-05,8.41E-05
8050,natural_language_inference21,187,The word embeddings are fine - tuned during the training phrase .,Experiments,Experiments,natural_language_inference,21,12,1,0,,0.398768442,0,hyperparameters,0.000927685,0.002826581,0.00091839,2.51E-05,3.45E-05,0.016611496,0.001155325,0.800306339,0.001570002,0.174804387,1.97E-05,0.000353197,0.000447357
8051,natural_language_inference21,188,We use Dropout ) with keep probability 0.75 for language inference and 0.8 for sentiment analysis .,Experiments,Experiments,natural_language_inference,21,13,1,1,experimental-setup,0.972033292,1,hyperparameters,8.80E-05,0.000122883,5.26E-05,4.64E-05,5.69E-06,0.078370224,0.003699333,0.911934158,4.59E-05,0.005111424,3.42E-06,3.91E-05,0.000480801
8052,natural_language_inference21,189,"The L2 regularization decay factors ? are 5 10 ?5 and 10 ? 4 for language inference and sentiment analysis , respectively .",Experiments,Experiments,natural_language_inference,21,14,1,1,experimental-setup,0.946348929,1,hyperparameters,3.68E-05,8.04E-05,1.20E-05,1.77E-05,3.78E-06,0.051384908,0.002359294,0.936895799,2.59E-05,0.008826968,6.58E-06,5.80E-05,0.000291829
8053,natural_language_inference21,190,Note that the dropout keep probability and ?,Experiments,Experiments,natural_language_inference,21,15,1,0,,0.000991397,0,negative,0.001151679,0.000271765,0.00028375,1.39E-05,1.84E-05,0.010264762,0.001097591,0.16084606,0.000151657,0.824153851,1.98E-05,0.001587083,0.000139653
8054,natural_language_inference21,191,varies with the scale of corresponding dataset .,Experiments,Experiments,natural_language_inference,21,16,1,0,,0.004217988,0,negative,0.000477897,6.54E-05,0.00022219,1.47E-05,1.40E-05,0.002045999,0.000636035,0.024159158,5.58E-05,0.970217527,4.85E-05,0.001853751,0.00018907
8055,natural_language_inference21,192,Hidden units number d h is set to 300 .,Experiments,Experiments,natural_language_inference,21,17,1,1,experimental-setup,0.949538754,1,hyperparameters,1.95E-05,3.69E-05,9.50E-06,7.06E-06,1.33E-06,0.029483201,0.001328506,0.962088911,2.81E-05,0.006741115,4.69E-06,3.44E-05,0.000216793
8056,natural_language_inference21,193,"Activation functions ? ( ) are ELU ( exponential linear unit ) ( Clevert , Unterthiner , and Hochreiter 2016 ) if not specified .",Experiments,Experiments,natural_language_inference,21,18,1,1,experimental-setup,0.140376868,0,negative,0.000118073,6.99E-05,0.000340581,2.84E-05,7.84E-06,0.02143082,0.001575199,0.125422742,0.000143946,0.849988599,4.81E-05,0.00045545,0.000370326
8057,natural_language_inference21,194,All models are implemented with TensorFlow 2 and run on sin - 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3 gle Nvidia GTX 1080 Ti graphic card .,Experiments,Experiments,natural_language_inference,21,19,1,1,experimental-setup,0.177025868,0,hyperparameters,0.000153655,0.000110465,0.000362325,0.000301017,3.29E-05,0.31139823,0.028656283,0.608815723,8.31E-05,0.046546053,1.28E-05,0.00038686,0.003140591
8058,natural_language_inference21,195,Natural Language Inference,Experiments,,natural_language_inference,21,20,1,1,results,0.947743065,1,experiments,0.001198636,7.55E-05,0.000783895,0.00099282,0.000404303,0.002599584,0.734473564,0.005978118,1.25E-05,0.04913563,0.005020158,0.14915751,0.050167729
8059,natural_language_inference21,196,The goal of Natural Language Inference ( NLI ) is to reason the semantic relationship between a premise sentence and a corresponding hypothesis sentence .,Experiments,Natural Language Inference,natural_language_inference,21,21,1,0,,0.854673319,1,experiments,0.000411561,2.48E-06,2.12E-05,4.13E-05,2.84E-05,0.000830601,0.444102501,6.84E-06,2.88E-06,0.438117436,0.001170955,0.000202582,0.115061124
8060,natural_language_inference21,197,"The possible relationship could be entailment , neutral or contradiction .",Experiments,Natural Language Inference,natural_language_inference,21,22,1,0,,0.003848286,0,negative,8.37E-05,8.23E-07,6.82E-06,1.08E-06,7.94E-06,0.000346973,0.015607168,4.00E-06,1.23E-05,0.983684327,1.27E-06,7.56E-06,0.000235974
8061,natural_language_inference21,198,"We compare different models on a widely used benchmark , Stanford Natural Language Inference ( SNLI ) 3 dataset , which consists of 549,367/9,842/9,824 ( train / dev/test ) premise -hypothesis pairs with labels .",Experiments,Natural Language Inference,natural_language_inference,21,23,1,0,,0.311750833,0,experiments,0.000348409,2.24E-05,9.97E-05,1.19E-06,0.000126484,0.000316582,0.802062353,7.07E-06,4.71E-06,0.19480731,1.04E-06,0.000298498,0.001904235
8062,natural_language_inference21,199,"Following the standard procedure in , we launch two sentence encoding models ( e.g. , DiSAN ) with tied parameters for the premise sentence and hypothesis sentence , respectively .",Experiments,Natural Language Inference,natural_language_inference,21,24,1,0,,0.264440991,0,experiments,0.001024363,0.00013106,0.001654155,1.21E-06,1.97E-05,0.00140208,0.8449073,3.85E-05,0.001871743,0.145843973,2.09E-06,3.41E-05,0.003069708
8063,natural_language_inference21,200,"Given the output encoding s p for the premise and sh for the hypothesis , the representation of relationship is the concatenation of s p , sh , s p ? sh and s p sh , which is fed into a 300 D fully connected layer and then a 3 - unit output layer with softmax to compute a probability distribution over the three types of relationship .",Experiments,Natural Language Inference,natural_language_inference,21,25,1,0,,0.012755065,0,negative,0.000252051,2.20E-05,0.000216146,4.01E-07,6.88E-06,0.000813523,0.126103542,2.48E-05,0.001467598,0.870060718,2.98E-06,1.22E-05,0.001017175
8064,natural_language_inference21,201,"For thorough comparison , besides the neural nets proposed in previous works of NLI , we implement five extra neural net baselines to compare with DiSAN .",Experiments,Natural Language Inference,natural_language_inference,21,26,1,0,,0.761919841,1,experiments,0.000632311,1.37E-05,0.000109265,2.21E-07,1.88E-05,0.000216269,0.930085996,4.48E-06,8.87E-06,0.06834304,1.48E-07,8.90E-05,0.000477963
8065,natural_language_inference21,202,They help us to analyze the improvement contributed by each part of DiSAN and to verify that the two attention mechanisms proposed in Section 3 can improve other networks .,Experiments,Natural Language Inference,natural_language_inference,21,27,1,0,,0.025480699,0,negative,0.002359934,7.54E-07,7.30E-06,5.64E-08,1.86E-06,4.14E-05,0.099987087,7.35E-07,2.89E-06,0.89734928,1.61E-07,0.000211471,3.71E-05
8066,natural_language_inference21,203,Word Embedding with additive attention .,Experiments,Natural Language Inference,natural_language_inference,21,28,1,0,,0.282644245,0,experiments,0.00043078,5.33E-06,0.000330515,1.48E-07,6.15E-07,0.000133636,0.895441397,2.31E-06,5.53E-05,0.0994319,1.64E-05,0.000288596,0.003863109
8067,natural_language_inference21,204,Word,Experiments,,natural_language_inference,21,29,1,0,,0.003392811,0,negative,0.000187351,6.64E-05,0.00042825,1.30E-05,1.27E-05,0.025897071,0.019806052,0.234624462,0.000152324,0.709327483,4.25E-05,0.007223875,0.002218405
8068,natural_language_inference21,205,Embedding with s 2t self - attention :,Experiments,Word,natural_language_inference,21,30,1,0,,0.168424538,0,negative,0.001924251,4.69E-06,0.065593183,5.16E-07,7.07E-07,6.68E-05,0.022278782,2.39E-05,4.98E-05,0.812149914,6.39E-06,0.024285177,0.073615941
8069,natural_language_inference21,206,DiSAN with,Experiments,Word,natural_language_inference,21,31,1,0,,0.000722336,0,negative,0.001302741,3.16E-07,0.000939082,8.86E-07,8.21E-07,2.79E-05,0.001225316,1.04E-05,5.17E-06,0.956050491,2.33E-07,0.006814805,0.033621803
8070,natural_language_inference21,207,DiSA blocks removed .,Experiments,Word,natural_language_inference,21,32,1,0,,0.00015776,0,negative,0.000961716,1.92E-07,0.000401112,1.72E-06,9.56E-07,6.13E-05,0.000799695,1.38E-05,2.61E-06,0.954684558,1.17E-07,0.00162513,0.041447147
8071,natural_language_inference21,208,"Multi - head with s 2t self - attention : Multi-head attention ) ( 8 heads , each has 75 hidden units ) with source2token self - attention .",Experiments,Word,natural_language_inference,21,33,1,0,,0.241509556,0,tasks,0.000838464,2.52E-06,0.028508382,2.66E-06,2.24E-06,0.00049043,0.113783404,8.40E-05,1.20E-05,0.306284633,6.85E-07,0.008545906,0.541444643
8072,natural_language_inference21,209,The positional encoding 3 https://nlp.stanford.edu/projects/snli/,Experiments,Word,natural_language_inference,21,34,1,0,,0.003481664,0,negative,0.000166255,8.11E-07,0.009285229,6.29E-07,4.02E-07,7.21E-05,0.003235615,2.72E-05,2.72E-05,0.906309886,2.36E-06,0.001386966,0.079485368
8073,natural_language_inference21,210,method used in is applied to the input sequence to encode temporal information .,Experiments,Word,natural_language_inference,21,35,1,0,,0.000250327,0,negative,0.000353835,1.63E-06,0.001202519,4.36E-06,1.85E-06,0.000107189,0.000776339,0.000100715,4.43E-05,0.879233587,6.17E-07,0.000490839,0.117682169
8074,natural_language_inference21,211,"We find our experiments show that multi-head attention is sensitive to hyperparameters , so we adjust keep probability of dropout from 0.7 to 0.9 with step 0.05 and report the best result .",Experiments,Word,natural_language_inference,21,36,1,0,,0.0287983,0,negative,0.020146876,1.09E-06,3.11E-05,1.44E-05,3.80E-06,0.000391918,0.032665058,0.000431623,1.59E-06,0.572157256,2.01E-07,0.037402837,0.336752213
8075,natural_language_inference21,212,Bi - LSTM with s 2t self - attention : a multi-dimensional source2token self - attention block is applied to the output of Bi - LSTM ( 300D forward + 300D backward LSTMs ) .,Experiments,Word,natural_language_inference,21,37,1,0,,0.25018375,0,negative,0.002472461,4.64E-06,0.054577031,1.52E-06,2.14E-06,0.000265335,0.087195821,8.35E-05,4.37E-05,0.444187294,6.48E-07,0.008898629,0.402267235
8076,natural_language_inference21,213,DiSAN without directions :,Experiments,Word,natural_language_inference,21,38,1,0,,0.012442852,0,negative,0.001534494,1.40E-06,0.059642486,1.09E-07,3.38E-07,3.07E-05,0.025035697,7.39E-06,1.44E-05,0.853392354,1.02E-06,0.020539823,0.039799817
8077,natural_language_inference21,214,"DiSAN with the forward / backward masks M f wand M bw replaced with two diag - dis abled masks M diag , i.e. , DiSAN without forward / backward order information .",Experiments,Word,natural_language_inference,21,39,1,0,,0.000261167,0,negative,0.001453056,7.61E-07,0.002897634,1.01E-07,6.29E-07,2.33E-05,0.003537717,1.30E-05,6.11E-06,0.981919222,4.56E-08,0.003224047,0.006924369
8078,natural_language_inference21,215,"Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .",Experiments,Word,natural_language_inference,21,40,1,1,results,0.914934111,1,results,0.002675247,9.14E-08,3.67E-05,5.60E-07,3.70E-07,1.48E-05,0.067810755,5.60E-06,7.57E-08,0.098065667,9.12E-08,0.558591783,0.2727983
8079,natural_language_inference21,216,"DiSAN surpasses the RNN / CNN based models with more complicated architecture and more parameters by large margins , e.g. , + 2.32 % to Bi - LSTM , + 1.42 % to Bi - LSTM with additive attention .",Experiments,Word,natural_language_inference,21,41,1,1,results,0.902176869,1,tasks,0.009252033,1.11E-07,5.53E-05,1.38E-06,5.37E-07,2.57E-05,0.061811646,9.96E-06,1.23E-07,0.075881526,6.40E-08,0.397485939,0.455475689
8080,natural_language_inference21,217,"It even outperforms models with the assistance of a semantic parsing tree , e.g. , + 3.52 % to Tree - based CNN , + 2.42 % to SPINN - PI .",Experiments,Word,natural_language_inference,21,42,1,1,results,0.713283816,1,results,0.007095498,1.26E-07,7.19E-05,1.40E-06,6.62E-07,2.77E-05,0.051852317,1.16E-05,1.45E-07,0.131164877,8.38E-08,0.421447729,0.388326014
8081,natural_language_inference21,218,"In the results of the five baseline methods and DiSAN at the bottom of , we demonstrate that making attention multi-dimensional ( feature - wise ) or directional brings substantial improvement to different neural nets .",Experiments,Word,natural_language_inference,21,43,1,0,,0.460853519,0,results,0.017950115,2.58E-07,4.35E-05,3.28E-07,4.81E-07,1.77E-05,0.072467345,1.23E-05,2.64E-07,0.382288381,7.31E-08,0.468538064,0.058681199
8082,natural_language_inference21,219,"First , a comparison between the first two models shows that changing token - wise attention to multi-dimensional / feature - wise attention leads to 3.31 % improvement on a word embedding based model .",Experiments,Word,natural_language_inference,21,44,1,1,results,0.937077577,1,results,0.047300843,3.15E-07,5.66E-05,1.44E-06,1.04E-06,2.61E-05,0.075896953,1.57E-05,2.43E-07,0.171147907,6.36E-08,0.491070001,0.214482783
8083,natural_language_inference21,220,"Also , a comparison between the third baseline and DiSAN shows that DiSAN can substantially outperform multi-head attention by 1.45 % .",Experiments,Word,natural_language_inference,21,45,1,1,results,0.892967988,1,results,0.013326517,1.11E-07,3.87E-05,5.31E-07,4.43E-07,1.67E-05,0.068870879,9.04E-06,9.20E-08,0.142870485,4.65E-08,0.567094934,0.207771574
8084,natural_language_inference21,221,"Moreover , a comparison between the forth baseline and DiSAN shows that the DiSA block can even outperform Bi - LSTM layer in context encoding , improving test accuracy by 0.64 % .",Experiments,Word,natural_language_inference,21,46,1,1,results,0.879320958,1,results,0.021861641,1.13E-07,3.40E-05,5.51E-07,4.39E-07,1.69E-05,0.064404291,9.75E-06,1.19E-07,0.143996617,3.77E-08,0.582221334,0.187454216
8085,natural_language_inference21,222,A comparison between the fifth baseline and DiSAN shows that directional self - attention with forward and backward masks ( with temporal order encoded ) can bring 0.96 % improvement .,Experiments,Word,natural_language_inference,21,47,1,1,results,0.870298342,1,results,0.019240001,1.19E-07,4.95E-05,2.30E-07,3.41E-07,1.13E-05,0.066351982,7.03E-06,1.24E-07,0.228742049,3.79E-08,0.568128324,0.117468974
8086,natural_language_inference21,223,Additional advantages of DiSAN shown in are its fewer parameters and compelling time efficiency .,Experiments,Word,natural_language_inference,21,48,1,0,,0.020517106,0,negative,0.007172316,1.19E-07,0.000196507,6.68E-07,1.42E-06,1.29E-05,0.002253316,3.37E-06,6.86E-07,0.918928775,3.00E-08,0.039888227,0.031541661
8087,natural_language_inference21,224,It is 3 faster than widely used Bi - LSTM model .,Experiments,Word,natural_language_inference,21,49,1,0,,0.010797257,0,tasks,0.000575439,1.89E-07,0.000362975,1.09E-06,9.34E-07,8.28E-05,0.018800877,2.72E-05,1.12E-06,0.090181685,3.00E-08,0.004344715,0.885620961
8088,natural_language_inference21,225,"Compared to other models with competitive performance , e.g. , 600D Bi - LSTM encoders with intra-attention ( 2.8M ) , 300D NSE encoders ( 3.0 M ) and 600D Bi - LSTM encoders with multidimensional attention ( 2.88 M ) , DiSAN only has 2.35 M parameters .",Experiments,Word,natural_language_inference,21,50,1,0,,0.02256958,0,negative,0.003233326,7.60E-07,0.000514942,1.45E-06,4.27E-06,0.000116843,0.040871359,5.12E-05,1.63E-06,0.523284373,3.46E-08,0.017957383,0.413962472
8089,natural_language_inference21,226,Sentiment Analysis,Experiments,,natural_language_inference,21,51,1,1,results,0.992028931,1,experiments,0.000995811,6.29E-05,0.000565779,0.001338737,0.000842185,0.0031461,0.540120976,0.013293763,5.93E-06,0.031685374,3.53E-05,0.196419417,0.211487688
8090,natural_language_inference21,227,Model,,,natural_language_inference,21,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
8091,natural_language_inference21,228,Test Accu MV- RNN 44.4 RNTN 45.7 Bi-LSTM 49.8 Tree- LSTM 51.0 CNN - non-static 48.0 CNN - Tensor 51.2 NCSL 51.1 LR-Bi-LSTM,Model,Model,natural_language_inference,21,1,1,0,,0.002927442,0,negative,0.00044582,6.94E-05,0.000357129,2.63E-05,4.02E-05,0.001385604,0.003921359,0.005049619,0.000567549,0.969145559,5.84E-05,0.0187732,0.000159874
8092,natural_language_inference21,229,"50 Sentiment analysis aims to analyze the sentiment of a sentence or a paragraph , e.g. , a movie or a product review .",Model,Model,natural_language_inference,21,2,1,0,,0.134066801,0,negative,0.000420348,0.000502372,0.000302533,0.00114475,9.93E-05,0.000788152,0.002317408,0.001598821,0.002768508,0.75252626,0.231092394,0.003685543,0.002753614
8093,natural_language_inference21,230,"We use Stanford Sentiment Treebank ( SST ) 4 for the experiments , and only focus on the fine - grained movie review sentiment classification over five classes , i.e. , very negative , negative , neutral , positive and very positive .",Model,Model,natural_language_inference,21,3,1,0,,0.110888076,0,negative,0.000325643,0.010279351,0.005958974,2.85E-05,0.000438466,0.00040658,0.00112185,0.005522369,0.007872257,0.964544741,0.000200478,0.003236372,6.44E-05
8094,natural_language_inference21,231,"We use the standard train / dev / test sets split with 8,544/1,101/2,210 samples .",Model,Model,natural_language_inference,21,4,1,0,,0.051105902,0,negative,0.000238053,0.001694277,0.000562,0.000121461,0.00034037,0.010301377,0.006158813,0.061534777,0.004490698,0.913133804,2.81E-05,0.001195282,0.000200976
8095,natural_language_inference21,232,"Similar to Section 5.1 , we employ a single sentence encoding model to obtain a sentence representation s of a movie review , then pass it into a 300D fully connected layer .",Model,Model,natural_language_inference,21,5,1,0,,0.049983221,0,negative,0.001236444,0.009947438,0.025617387,9.51E-06,5.03E-05,0.00027549,0.000510954,0.002991042,0.348311914,0.609504471,0.000218811,0.00123367,9.26E-05
8096,natural_language_inference21,233,"Finally , a 5 - unit output layer with softmax is used to calculate a probability distribution over the five classes .",Model,Model,natural_language_inference,21,6,1,0,,0.297915048,0,model,0.001013989,0.004030975,0.00482039,1.94E-05,3.30E-05,0.001579733,0.001080841,0.021712386,0.504684369,0.460266935,0.000116269,0.000429341,0.000212307
8097,natural_language_inference21,234,"In , we compare previous works with DiSAN on test accuracy .",Model,Model,natural_language_inference,21,7,1,0,,0.021997894,0,negative,0.000545163,0.001095967,0.003618186,4.51E-06,9.79E-05,6.94E-05,0.000375935,0.000571066,0.001737663,0.982363213,3.74E-05,0.009467133,1.65E-05
8098,natural_language_inference21,235,"To the best of our knowledge , DiSAN improves the last best accuracy ( given by CNN - Tensor ) by 0.52 % .",Model,Model,natural_language_inference,21,8,1,1,results,0.326023739,0,results,0.007429508,0.000148965,0.001341083,1.60E-05,5.05E-05,9.02E-05,0.005519576,0.00043338,0.000219492,0.082528053,7.71E-05,0.901668579,0.000477639
8099,natural_language_inference21,236,"Compared to tree - based models with heavy use of the prior structure , e.g. , MV - RNN , RNTN and Tree - LSTM , DiSAN outperforms them by 7.32 % , 6.02 % and 0.72 % , respectively .",Model,Model,natural_language_inference,21,9,1,1,results,0.83933272,1,results,0.012250962,3.55E-05,0.000273729,9.38E-06,1.73E-05,5.77E-05,0.004838085,0.000378629,4.77E-05,0.023682054,1.93E-05,0.958003306,0.00038642
8100,natural_language_inference21,237,"Additionally , DiSAN achieves better performance than CNN - based models .",Model,Model,natural_language_inference,21,10,1,1,results,0.437990778,0,results,0.006232165,1.24E-05,9.95E-05,5.61E-06,6.74E-06,4.03E-05,0.003194017,0.000295463,2.07E-05,0.018046678,1.05E-05,0.971782199,0.000253674
8101,natural_language_inference21,238,"More recent works tend to focus on lexicon - based sentiment analysis , by exploring sentiment lexicons , negation words and intensity words .",Model,Model,natural_language_inference,21,11,1,0,,0.112022911,0,negative,0.000203524,0.000322374,0.000159667,1.48E-05,1.42E-05,0.000129978,0.00032145,0.000912856,0.001146866,0.988861156,0.004443951,0.003370312,9.88E-05
8102,natural_language_inference21,239,4 https://nlp.stanford.edu/sentiment/,Model,Model,natural_language_inference,21,12,1,0,,0.101244045,0,negative,0.002832037,5.62E-05,0.000567301,0.055292506,0.00236961,0.004019881,0.000816961,0.000901778,0.001145334,0.930643453,7.66E-06,0.001074334,0.000272947
8103,natural_language_inference21,240,"Nonetheless , DiSAN still outperforms these fancy models , such as NCSL ( + 0.62 % ) and LR- Bi- LSTM ( + 1.12 % ) . :",Model,Model,natural_language_inference,21,13,1,1,results,0.825567406,1,results,0.015783274,2.25E-05,0.000257688,8.66E-06,1.81E-05,4.22E-05,0.004298024,0.000235261,3.23E-05,0.02382707,1.10E-05,0.955134676,0.000329115
8104,natural_language_inference21,241,Fine - grained sentiment analysis accuracy vs. sentence length .,Model,Model,natural_language_inference,21,14,1,0,,0.06535983,0,results,0.001233649,0.000411138,0.001198966,1.45E-05,2.59E-05,0.000103372,0.008693805,0.000656895,0.000645435,0.452925902,0.0064419,0.526598504,0.001050046
8105,natural_language_inference21,242,"The results of LSTM , Bi - LSTM and Tree - LSTM are from and the result of DiSAN is the average over five random trials .",Model,Model,natural_language_inference,21,15,1,0,,0.002155227,0,negative,5.36E-05,2.13E-05,4.90E-05,8.31E-07,5.91E-06,8.65E-05,0.000197655,0.000692227,0.000200138,0.997723781,1.60E-06,0.000962702,4.81E-06
8106,natural_language_inference21,243,It is also interesting to see the performance of different models on the sentences with different lengths .,Model,Model,natural_language_inference,21,16,1,0,,0.008786402,0,negative,0.000247049,1.57E-05,1.47E-05,1.79E-07,6.43E-07,2.16E-05,0.000118815,0.00031207,0.000162874,0.989202628,6.03E-06,0.009894163,3.58E-06
8107,natural_language_inference21,244,"In , we compare LSTM , Bi - LSTM , Tree - LSTM and DiSAN on different sentence lengths .",Model,Model,natural_language_inference,21,17,1,0,,0.544239086,1,negative,0.001169296,0.006985563,0.033783525,6.04E-06,8.90E-05,0.000255813,0.004495581,0.002884116,0.017928956,0.876668905,0.000180446,0.055361225,0.000191501
8108,natural_language_inference21,245,"In the range of ( 5 , 12 ) , the length range for most movie review sentences , DiSAN significantly outperforms others .",Model,Model,natural_language_inference,21,18,1,0,,0.759680143,1,results,0.004111928,6.83E-05,0.000137875,1.20E-05,2.33E-05,0.000178973,0.009057507,0.00223957,0.000107525,0.057232668,2.12E-05,0.926105722,0.000703458
8109,natural_language_inference21,246,"Meanwhile , DiSAN also shows impressive performance for slightly longer sentences or paragraphs in the range of ( 25 , 38 ) .",Model,Model,natural_language_inference,21,19,1,0,,0.236684598,0,results,0.001548943,7.49E-06,8.19E-05,1.49E-06,4.29E-06,2.20E-05,0.002971797,0.000168862,9.75E-06,0.037161714,8.14E-06,0.957874792,0.000138845
8110,natural_language_inference21,247,"DiSAN performs poorly when the sentence length ? 38 , in which however only 3.21 % of total movie review sentences lie .",Model,Model,natural_language_inference,21,20,1,0,,0.173205445,0,results,0.004715152,2.22E-05,0.000223475,4.80E-06,1.38E-05,5.44E-05,0.004075792,0.000346303,2.63E-05,0.049350476,1.85E-05,0.940826306,0.000322509
8111,natural_language_inference21,248,Experiments on Other NLP Tasks,Model,,natural_language_inference,21,21,1,0,,0.06483639,0,results,0.000868961,0.000130363,0.000486911,7.74E-06,9.40E-05,8.54E-05,0.008382175,0.000406446,0.000126948,0.477815498,9.16E-05,0.511205182,0.000298714
8112,natural_language_inference21,249,Multi - Genre Natural Language Inference Multi - Genre Natural Language Inference ( MultiNLI ) 5 dataset consists of 433 k sentence pairs annotated with textual entailment information .,Model,Experiments on Other NLP Tasks,natural_language_inference,21,22,1,0,,0.515433117,1,negative,0.000866752,2.42E-06,0.002442667,8.28E-06,0.000130015,0.000127747,0.079403158,6.37E-06,1.46E-06,0.898028759,6.02E-06,0.016802426,0.002173933
8113,natural_language_inference21,250,"This dataset is similar to SNLI , but it covers more genres of spoken and written text , and supports a distinctive cross - genre generalization evaluation .",Model,Experiments on Other NLP Tasks,natural_language_inference,21,23,1,0,,0.000293812,0,negative,0.000110323,3.20E-07,0.000218329,3.13E-07,2.53E-05,1.71E-05,0.001778477,1.40E-06,3.47E-07,0.997460262,4.30E-08,0.000378227,9.61E-06
8114,natural_language_inference21,251,"However , MultiNLI is a quite new dataset , and its leaderboard does not include a session for the sentence - encoding only model .",Model,Experiments on Other NLP Tasks,natural_language_inference,21,24,1,0,,9.82E-05,0,negative,3.63E-05,8.47E-08,0.000101622,8.49E-08,3.17E-06,7.31E-06,0.001180585,6.55E-07,9.32E-08,0.998243612,1.50E-07,0.000417768,8.60E-06
8115,natural_language_inference21,252,"Hence , we only compare DiSAN with the baselines provided at the official website .",Model,Experiments on Other NLP Tasks,natural_language_inference,21,25,1,0,,7.16E-05,0,negative,1.55E-05,1.32E-07,3.54E-05,4.50E-09,1.03E-07,4.21E-06,0.000337642,1.43E-06,5.37E-07,0.999555138,1.14E-08,4.95E-05,3.68E-07
8116,natural_language_inference21,253,The results of DiSAN and two sentence - encoding models on the leaderboard are shown in .,Model,Experiments on Other NLP Tasks,natural_language_inference,21,26,1,0,,0.549486267,1,negative,0.001590733,1.53E-07,0.000112649,7.19E-09,8.71E-08,6.61E-06,0.060705683,2.07E-06,2.04E-07,0.847007782,2.12E-07,0.090553739,2.01E-05
8117,natural_language_inference21,254,Sentence Classifications,Model,,natural_language_inference,21,27,1,0,,0.333871193,0,results,0.000646466,0.000513163,0.002406514,1.94E-05,7.00E-05,0.000197439,0.035599643,0.001165178,0.001028498,0.191235076,0.001336171,0.761676196,0.004106244
8118,natural_language_inference21,255,The goal of sentence classification is to correctly predict the class label of a given sentence in various scenarios .,Model,Sentence Classifications,natural_language_inference,21,28,1,0,,0.000962558,0,negative,5.70E-05,1.63E-06,3.85E-05,1.47E-05,2.45E-07,3.03E-05,0.000502197,0.000113445,1.89E-05,0.981314445,0.000379366,0.015389751,0.002139521
8119,natural_language_inference21,256,"We evaluate the models on four sentence classification benchmarks of various NLP tasks , such as sentiment analysis and question - type classification .",Model,Sentence Classifications,natural_language_inference,21,29,1,0,,4.33E-05,0,negative,2.75E-05,3.35E-06,5.26E-05,9.55E-08,2.07E-07,3.29E-06,0.000141159,4.28E-05,6.45E-06,0.993541659,2.10E-07,0.006170186,1.05E-05
8120,natural_language_inference21,257,They are listed as follows .,Model,Sentence Classifications,natural_language_inference,21,30,1,0,,2.40E-08,0,negative,2.72E-06,1.30E-08,1.03E-06,1.03E-08,4.78E-09,2.36E-07,7.80E-07,1.58E-06,6.27E-07,0.999896554,1.27E-08,9.63E-05,1.67E-07
8121,natural_language_inference21,258,"1 ) CR : Customer review of various products ( cameras , etc. ) , which is to predict whether the review is positive or negative ; 2 ) MPQA : Opinion polarity detection subtask of the MPQA dataset ; 3 ) SUBJ : Subjectivity dataset ( Pang and Lee 2004 ) whose labels indicate whether each sentence is subjective or objective ; 4 ) TREC : TREC question - type classification dataset .",Model,Sentence Classifications,natural_language_inference,21,31,1,0,,0.000161598,0,negative,6.40E-05,7.26E-07,0.000342511,1.68E-06,1.51E-06,1.63E-05,0.001167021,3.05E-05,4.21E-06,0.953675497,1.71E-06,0.044082148,0.000612149
8122,natural_language_inference21,259,The experimental results of DiSAN and existing methods are shown in .,Model,Sentence Classifications,natural_language_inference,21,32,1,0,,0.000235241,0,negative,0.000103598,2.53E-08,7.84E-06,2.40E-09,6.38E-09,1.82E-07,0.000123386,1.45E-06,7.91E-08,0.764117658,2.65E-08,0.235644084,1.67E-06
8123,natural_language_inference21,260,"particular , we will focus primarily on the probability in forward / backward DiSA blocks ( ) , forward / backward fusion gates F in Eq. ( 19 ) , and the probability in multi-dimensional source2 token self - attention block .",Model,Sentence Classifications,natural_language_inference,21,33,1,0,,1.74E-06,0,negative,1.89E-05,8.60E-07,1.09E-05,2.74E-08,2.47E-08,1.10E-06,1.30E-05,2.56E-05,1.36E-05,0.999431428,7.72E-08,0.000482934,1.62E-06
8124,natural_language_inference21,261,"For the first two , we desire to demonstrate the dependency at token level , but attention probability in DiSAN is defined on each feature , so we average the probabilities along the feature dimension .",Model,Sentence Classifications,natural_language_inference,21,34,1,0,,3.96E-07,0,negative,0.000162542,4.42E-07,9.15E-05,1.36E-08,6.10E-08,5.26E-07,1.18E-05,3.96E-06,5.17E-06,0.997908307,1.54E-08,0.001814869,7.15E-07
8125,natural_language_inference21,262,We select two sentences from SNLI test set as examples for this case study .,Model,Sentence Classifications,natural_language_inference,21,35,1,0,,2.28E-06,0,negative,8.54E-06,2.20E-07,8.19E-06,1.91E-08,2.18E-07,5.80E-07,1.87E-05,9.65E-06,7.05E-07,0.998603307,1.09E-08,0.00134794,1.92E-06
8126,natural_language_inference21,263,Sentence 1 is Families have some dogs in front of a carousel and sentence 2 is volleyball match is in progress between ladies .,Model,Sentence Classifications,natural_language_inference,21,36,1,0,,2.82E-07,0,negative,2.34E-06,2.39E-08,1.24E-06,1.62E-08,1.27E-08,5.66E-07,3.17E-06,5.89E-06,9.41E-07,0.999759199,1.27E-08,0.000225672,9.19E-07
8127,natural_language_inference21,264,"shows that1 ) semantically important words such as nouns and verbs usually get large attention , but stop words ( am , is , are , etc. ) do not ; 2 ) globally important words , e.g. , volleyball , match , ladies in sentence 1 and dog , front , carousel in sentence 2 , get large attention from all other words ; 3 ) if a word is important to only some of the other words ( e.g. to constitute a phrase or sense - group ) , it gets large attention only from these words , e.g. , attention between progress , between in sentence1 , and attention between families , have in sentence 2 .",Model,Sentence Classifications,natural_language_inference,21,37,1,0,,4.78E-05,0,negative,4.48E-05,3.82E-08,6.60E-06,9.21E-09,2.54E-08,3.17E-07,1.43E-05,2.44E-06,6.66E-07,0.994042258,1.54E-08,0.005887071,1.53E-06
8128,natural_language_inference21,265,Case Study,Model,,natural_language_inference,21,38,1,0,,0.008365745,0,negative,9.99E-05,0.0001362,0.000206173,6.59E-06,3.02E-05,0.00011809,0.000496381,0.001791294,0.003653539,0.989287702,9.96E-06,0.003961237,0.000202714
8129,natural_language_inference21,266,This also shows that directional information can help to generate context - aware word representation with temporal order encoded .,Model,Case Study,natural_language_inference,21,39,1,0,,0.00019271,0,negative,0.001003737,3.47E-05,2.38E-05,9.38E-08,7.76E-06,1.41E-05,6.21E-05,3.48E-05,4.36E-05,0.996603742,3.78E-07,0.002170629,6.18E-07
8130,natural_language_inference21,267,"For instance , for word match in sentence 1 , its forward DiSA focuses more on word volleyball , while its backward attention focuses more on progress and ladies , so the representation of word match contains the essential information of the entire sentence , and simultaneously includes the positional order information .",Model,Case Study,natural_language_inference,21,40,1,0,,4.36E-06,0,negative,1.14E-05,1.55E-05,2.53E-05,7.00E-09,1.04E-06,5.06E-06,7.45E-06,1.47E-05,4.69E-05,0.999799908,2.45E-07,7.23E-05,1.07E-07
8131,natural_language_inference21,268,"In addition , forward and backward DiSAs can focus on different parts of a sentence .",Model,Case Study,natural_language_inference,21,41,1,0,,4.38E-05,0,negative,1.84E-05,4.20E-05,2.37E-05,2.37E-08,2.08E-06,5.16E-06,6.80E-06,1.89E-05,0.000109696,0.999692682,8.90E-07,7.95E-05,2.49E-07
8132,natural_language_inference21,269,"For example , the forward one in sentence 2 pays attention to the word families , whereas the backward one focuses on the word carousel .",Model,Case Study,natural_language_inference,21,42,1,0,,6.91E-07,0,negative,8.42E-07,2.48E-06,1.10E-06,2.88E-08,8.33E-07,1.70E-05,5.68E-06,3.63E-05,1.48E-05,0.999912552,1.98E-07,8.00E-06,1.45E-07
8133,natural_language_inference21,270,"Since forward and backward attentions are computed separately , it avoids normalization over multiple significant words to weaken their weights .",Model,Case Study,natural_language_inference,21,43,1,0,,7.13E-06,0,negative,0.000324695,0.001537548,0.000181013,4.79E-07,5.31E-05,3.71E-05,3.23E-05,0.000148174,0.002120175,0.995417069,6.56E-07,0.000146012,1.61E-06
8134,natural_language_inference21,271,"Note that this is a weakness of traditional attention compared to RNN , especially for long sentences .",Model,Case Study,natural_language_inference,21,44,1,0,,1.34E-06,0,negative,4.99E-06,1.38E-06,6.77E-07,1.12E-07,2.09E-06,9.83E-06,2.99E-06,1.25E-05,1.61E-06,0.99994798,1.74E-07,1.55E-05,1.22E-07
8135,natural_language_inference21,272,"In , we show that the gate value F in Eq. ( 19 ) .",Model,Case Study,natural_language_inference,21,45,1,0,,1.81E-06,0,negative,2.03E-05,1.81E-05,2.13E-06,3.60E-07,1.02E-05,2.04E-05,5.35E-06,9.05E-05,3.48E-05,0.999768505,9.59E-08,2.91E-05,2.26E-07
8136,natural_language_inference21,273,The gate combines the input and output of masked self - attention .,Model,Case Study,natural_language_inference,21,46,1,0,,4.53E-05,0,negative,1.84E-05,0.000559413,0.000329811,4.63E-07,1.69E-05,6.39E-05,4.06E-05,0.000227417,0.024374364,0.974346692,1.58E-06,1.36E-05,6.94E-06
8137,natural_language_inference21,274,It tends to selects the input representation h instead of the output s if the corresponding weight in F is large .,Model,Case Study,natural_language_inference,21,47,1,0,,2.76E-05,0,negative,3.66E-05,0.000122749,6.47E-05,7.67E-07,2.25E-05,8.41E-05,2.98E-05,0.00024025,0.000110893,0.999240425,6.33E-07,4.48E-05,1.83E-06
8138,natural_language_inference21,275,"This shows that the gate values for meaningless words , especially stop words is small .",Model,Case Study,natural_language_inference,21,48,1,0,,1.98E-05,0,negative,0.000966357,1.56E-05,3.84E-06,1.77E-07,1.45E-05,2.68E-05,5.48E-05,0.000140301,1.20E-05,0.997675207,5.58E-08,0.001089503,7.80E-07
8139,natural_language_inference21,276,"The stop words themselves can not contribute important information , so only their semantic relations to other words might help to understand the sentence .",Model,Case Study,natural_language_inference,21,49,1,0,,2.33E-06,0,negative,1.62E-05,1.96E-06,8.25E-07,2.70E-08,2.36E-06,6.17E-06,2.44E-06,1.18E-05,4.60E-06,0.999923189,1.55E-08,3.04E-05,5.33E-08
8140,natural_language_inference21,277,"Hence , the gate tends to use their context features given by masked self - attention .",Model,Case Study,natural_language_inference,21,50,1,0,,4.49E-05,0,negative,4.53E-05,8.65E-05,2.09E-05,3.19E-08,4.06E-06,6.62E-06,4.38E-06,4.10E-05,0.000282681,0.999478063,4.30E-08,3.03E-05,1.86E-07
8141,natural_language_inference21,278,"In , we show the two multi-dimensional source2token self - attention score vectors of the same word in the two sentences , by their heatmaps .",Model,Case Study,natural_language_inference,21,51,1,0,,4.17E-06,0,negative,2.55E-06,1.45E-05,9.47E-06,3.99E-08,3.26E-06,2.68E-05,1.15E-05,8.06E-05,0.00011067,0.999729677,1.32E-08,1.06E-05,2.62E-07
8142,natural_language_inference21,279,"The first pair has two sentences : one is The glass bottle is big , and another is A man is pouring a glass of tea .",Model,Case Study,natural_language_inference,21,52,1,0,,1.92E-06,0,negative,1.39E-06,3.60E-06,2.22E-06,7.67E-07,0.000329128,5.30E-05,1.63E-05,3.86E-05,1.15E-06,0.999538492,7.51E-09,1.49E-05,4.55E-07
8143,natural_language_inference21,280,They share the same word is glass with different meanings .,Model,Case Study,natural_language_inference,21,53,1,0,,1.86E-07,0,negative,5.03E-07,1.09E-06,5.92E-07,4.69E-08,3.92E-06,1.08E-05,2.09E-06,1.46E-05,3.48E-06,0.999959382,9.73E-09,3.44E-06,7.37E-08
8144,natural_language_inference21,281,The second pair has two sentences : one is The restaurant is about to close and another is A biker is close to the fountain .,Model,Case Study,natural_language_inference,21,54,1,0,,2.09E-06,0,negative,1.32E-06,4.46E-06,3.25E-06,4.52E-07,0.00029516,4.51E-05,1.69E-05,3.78E-05,1.74E-06,0.999576933,5.80E-09,1.65E-05,4.37E-07
8145,natural_language_inference21,282,It can be seen that the two attention vectors for the same words are very different due to their different meanings in different contexts .,Model,Case Study,natural_language_inference,21,55,1,0,,8.40E-06,0,negative,7.62E-05,1.39E-05,1.95E-06,1.40E-08,1.31E-06,6.18E-06,1.40E-05,5.11E-05,2.28E-05,0.99959775,3.03E-08,0.000214617,1.57E-07
8146,natural_language_inference21,283,This indicates that the multi-dimensional attention vector is not redundant because it can encode more information than one single score used in traditional attention and it is able to capture subtle difference of the same word in different contexts or sentences .,Model,Case Study,natural_language_inference,21,56,1,0,,4.41E-05,0,negative,0.000302798,2.04E-05,5.44E-06,2.92E-08,7.31E-06,7.57E-06,2.50E-05,2.72E-05,2.06E-05,0.999188769,1.21E-08,0.000394718,2.25E-07
8147,natural_language_inference21,284,"Additionally , it can also alleviate the weakness of the attention overlong sequence , which can avoid normalization over entire sequence in traditional attention only once .",Model,Case Study,natural_language_inference,21,57,1,0,,0.000104484,0,negative,0.000222248,0.000226811,3.74E-05,4.15E-07,3.18E-05,2.91E-05,6.69E-05,0.000122251,0.000313942,0.997906872,1.70E-07,0.001038561,3.55E-06
8148,natural_language_inference21,285,Conclusion,,,natural_language_inference,21,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
8149,sentiment_analysis38,1,title,,,sentiment_analysis,38,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
8150,sentiment_analysis38,2,Learning to Generate Reviews and Discovering Sentiment,title,,sentiment_analysis,38,1,1,0,,0.995426532,1,research-problem,1.54E-08,7.63E-06,1.84E-08,1.30E-07,7.52E-08,1.05E-07,6.47E-07,1.30E-06,5.31E-07,0.003408247,0.996581189,6.39E-08,4.42E-08
8151,sentiment_analysis38,3,abstract,,,sentiment_analysis,38,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
8152,sentiment_analysis38,4,We explore the properties of byte - level recurrent language models .,abstract,abstract,sentiment_analysis,38,1,1,0,,0.230831946,0,research-problem,3.40E-07,0.00221709,1.16E-06,1.31E-06,3.35E-06,2.91E-06,2.44E-06,5.23E-05,0.000153154,0.043892335,0.953672722,6.09E-07,2.63E-07
8153,sentiment_analysis38,5,"When given sufficient amounts of capacity , training data , and compute time , the representations learned by these models include disentangled features corresponding to high - level concepts .",abstract,abstract,sentiment_analysis,38,2,1,0,,0.017714082,0,negative,3.17E-06,0.005226658,1.60E-06,3.24E-06,2.16E-05,6.25E-06,9.25E-07,9.62E-05,0.000556956,0.804772354,0.189309994,9.05E-07,1.51E-07
8154,sentiment_analysis38,6,"Specifically , we find a single unit which performs sentiment analysis .",abstract,abstract,sentiment_analysis,38,3,1,0,,0.007668868,0,negative,5.50E-05,0.219043397,0.000218508,4.90E-05,0.001534556,7.04E-05,1.52E-05,0.000511736,0.040630134,0.676262032,0.061596954,9.56E-06,3.54E-06
8155,sentiment_analysis38,7,"These representations , learned in an unsupervised manner , achieve state of the art on the binary subset of the Stanford Sentiment Treebank .",abstract,abstract,sentiment_analysis,38,4,1,0,,0.036464566,0,research-problem,8.44E-07,0.001352644,4.00E-06,2.47E-06,6.90E-06,1.18E-05,6.39E-06,0.000132173,0.000131448,0.120332505,0.878015777,2.55E-06,4.93E-07
8156,sentiment_analysis38,8,They are also very data efficient .,abstract,abstract,sentiment_analysis,38,5,1,0,,0.001294297,0,negative,3.64E-06,0.000809854,3.58E-06,1.01E-05,2.69E-05,3.09E-05,7.46E-06,9.11E-05,0.000228481,0.706503262,0.292281953,2.23E-06,5.98E-07
8157,sentiment_analysis38,9,"When using only a handful of labeled examples , our approach matches the performance of strong baselines trained on full datasets .",abstract,abstract,sentiment_analysis,38,6,1,0,,0.015551979,0,negative,4.08E-05,0.00195568,1.79E-06,7.27E-05,8.04E-05,9.39E-05,8.06E-05,0.001031344,9.29E-05,0.562289049,0.434106718,0.000146218,7.85E-06
8158,sentiment_analysis38,10,We also demonstrate the sentiment unit has a direct influence on the generative process of the model .,abstract,abstract,sentiment_analysis,38,7,1,0,,0.087935029,0,negative,0.000879681,0.026205724,1.66E-05,7.15E-05,0.000524323,6.26E-05,1.42E-05,0.00060558,0.003772275,0.950403169,0.017402297,3.97E-05,2.39E-06
8159,sentiment_analysis38,11,Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment .,abstract,abstract,sentiment_analysis,38,8,1,0,,0.008959666,0,negative,8.25E-06,0.00927978,5.75E-06,7.25E-07,2.63E-05,2.64E-05,1.34E-06,0.000609272,0.002718846,0.982807769,0.004514187,1.38E-06,7.64E-08
8160,sentiment_analysis38,12,Introduction and Motivating Work,,,sentiment_analysis,38,0,1,0,,5.68E-05,0,negative,7.91E-05,0.000130041,6.09E-06,0.000126545,6.32E-06,0.000244113,7.86E-05,0.000724819,0.000121461,0.982571834,0.015846419,4.36E-05,2.10E-05
8161,sentiment_analysis38,13,Representation learning ) plays a critical role in many modern machine learning systems .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,1,1,1,research-problem,0.220528034,0,research-problem,1.22E-07,5.32E-05,1.79E-07,2.07E-07,5.13E-07,6.14E-07,1.02E-06,1.50E-06,1.43E-05,0.016539159,0.983388814,2.18E-07,1.03E-07
8162,sentiment_analysis38,14,Representations map raw data to more useful forms and the choice of representation is an important component of any application .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,2,1,0,,0.062444923,0,research-problem,5.73E-07,0.00025664,1.94E-07,4.51E-06,8.40E-06,1.20E-05,1.60E-06,1.46E-05,0.000176153,0.388613355,0.610911273,3.45E-07,2.97E-07
8163,sentiment_analysis38,15,"Broadly speaking , there are two are as of research emphasizing different details of how to learn useful representations .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,3,1,0,,0.001730997,0,negative,1.50E-06,0.000367122,5.79E-07,4.20E-06,2.56E-05,2.96E-05,2.59E-06,2.58E-05,0.000206756,0.870531187,0.128804099,6.61E-07,2.79E-07
8164,sentiment_analysis38,16,"The supervised training of high - capacity models on large labeled datasets is critical to the recent success of deep learning techniques for a wide range of applications such as image classification , speech recognition , and machine translation ) .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,4,1,0,,0.713932103,1,research-problem,3.15E-07,0.000104172,9.84E-08,7.88E-06,8.76E-06,5.46E-06,1.94E-06,5.55E-06,1.87E-05,0.094751025,0.90509548,2.26E-07,4.20E-07
8165,sentiment_analysis38,17,Analysis of the task specific representations learned by these models reveals many fascinating properties .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,5,1,0,,0.003979587,0,negative,1.05E-06,0.000397858,2.02E-07,9.17E-07,8.82E-06,7.90E-06,9.60E-07,1.18E-05,0.000332266,0.861510153,0.137727363,5.79E-07,9.19E-08
8166,sentiment_analysis38,18,"Image classifiers learn a broadly useful hierarchy of feature detectors rerepresenting raw pixels as edges , textures , and objects .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,6,1,0,,0.076232466,0,research-problem,1.57E-06,0.00196572,2.83E-06,1.09E-05,8.23E-05,2.55E-05,4.61E-06,2.66E-05,0.001243141,0.429551585,0.567083273,9.14E-07,1.01E-06
8167,sentiment_analysis38,19,"In the field of computer vision , it is now commonplace to reuse these representations on a broad suite of related tasks - one of the most successful examples of transfer learning to date .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,7,1,0,,0.070698826,0,research-problem,6.14E-07,0.00029199,3.38E-07,5.53E-06,1.53E-05,9.41E-06,3.63E-06,1.03E-05,6.77E-05,0.215228976,0.784364904,6.43E-07,5.66E-07
8168,sentiment_analysis38,20,There is also along history of unsupervised representation learning .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,8,1,0,,0.0013141,0,negative,1.69E-06,0.000286302,8.37E-07,5.50E-06,4.97E-05,2.88E-05,3.57E-06,1.74E-05,0.000218615,0.905521137,0.093865401,7.80E-07,3.23E-07
8169,sentiment_analysis38,21,Much of the early research into modern deep learning was developed and validated via this approach ) .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,9,1,0,,0.002033018,0,negative,1.32E-06,0.00031671,5.61E-07,1.71E-05,5.72E-05,4.74E-05,3.20E-06,2.64E-05,0.000171467,0.86480376,0.13455393,4.37E-07,4.54E-07
8170,sentiment_analysis38,22,"Unsupervised learning is promising due to its ability to scale beyond only the subsets and domains of data that can be cleaned and labeled given resource , privacy , or other constraints .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,10,1,0,,0.094369042,0,research-problem,1.78E-06,0.000470441,8.37E-07,1.39E-05,3.30E-05,2.37E-05,7.10E-06,2.18E-05,0.000140678,0.362407284,0.636876756,1.68E-06,1.14E-06
8171,sentiment_analysis38,23,This advantage is also its difficulty .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,11,1,0,,0.000292279,0,negative,7.69E-06,0.00020259,8.53E-07,1.90E-05,0.000177621,2.79E-05,2.02E-06,1.25E-05,0.000228839,0.993532502,0.005787201,1.06E-06,2.25E-07
8172,sentiment_analysis38,24,"While supervised approaches have clear objectives that can be directly optimized , unsupervised approaches rely on proxy tasks such as reconstruction , density estimation , or generation , which do not directly encourage useful representations for specific tasks .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,12,1,0,,0.008180734,0,negative,2.35E-06,0.000882878,3.80E-07,2.37E-05,0.0001119,2.65E-05,2.01E-06,3.24E-05,0.000172191,0.951650173,0.047094331,8.11E-07,3.76E-07
8173,sentiment_analysis38,25,"As a result , much work has gone into designing objectives , priors , and architectures meant to encourage the learning of useful representations .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,13,1,0,,0.014591667,0,negative,2.21E-06,0.000635036,4.43E-07,1.11E-05,5.43E-05,4.22E-05,3.93E-06,4.57E-05,0.000273397,0.871163145,0.127767059,8.30E-07,6.21E-07
8174,sentiment_analysis38,26,We refer readers to for a detailed review .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,14,1,0,,0.000788795,0,negative,1.21E-06,0.000369827,2.59E-07,1.24E-05,0.000120692,4.41E-05,1.06E-06,3.54E-05,0.000447969,0.998398271,0.000568352,3.29E-07,1.06E-07
8175,sentiment_analysis38,27,"Despite these difficulties , there are notable applications of unsupervised learning .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,15,1,0,,0.002305273,0,negative,9.10E-07,0.000243645,3.04E-07,6.67E-06,2.22E-05,2.30E-05,3.05E-06,2.03E-05,0.000110379,0.785194999,0.214373445,6.26E-07,3.97E-07
8176,sentiment_analysis38,28,Pre-trained word vectors are a vital part of many modern NLP systems .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,16,1,0,,0.059201346,0,research-problem,1.46E-06,0.000419906,4.72E-07,1.78E-05,3.88E-05,5.10E-05,9.61E-06,5.32E-05,0.000131769,0.446973772,0.552298564,1.58E-06,1.98E-06
8177,sentiment_analysis38,29,"These representations , learned by modeling word co-occurrences , increase the data efficiency and generalization capability of NLP systems .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,17,1,0,,0.255967225,0,negative,7.29E-06,0.006770145,6.24E-06,1.11E-05,0.000196293,5.26E-05,1.25E-05,7.47E-05,0.007650121,0.803671925,0.181539544,5.71E-06,1.80E-06
8178,sentiment_analysis38,30,Topic modelling can also discover factors within a corpus of text which align to human interpretable concepts such as art or education .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,18,1,0,,0.116445317,0,negative,3.99E-06,0.00138003,3.48E-06,2.37E-05,0.000226853,9.19E-05,2.12E-05,5.36E-05,0.000820362,0.730744892,0.266621845,4.83E-06,3.32E-06
8179,sentiment_analysis38,31,"How to learn representations of phrases , sentences , and documents is an open are a of research .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,19,1,0,,0.082794644,0,research-problem,7.41E-07,0.000415048,4.34E-07,4.03E-06,1.63E-05,1.72E-05,4.80E-06,2.58E-05,0.000167449,0.441783184,0.55756319,9.39E-07,7.78E-07
8180,sentiment_analysis38,32,"Inspired by the success of word vectors , propose skipthought vectors , a method of training a sentence encoder by predicting the preceding and following sentence .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,20,1,0,,0.149288716,0,approach,9.74E-05,0.404093813,0.001874369,1.59E-05,0.002762743,0.000125728,7.32E-05,0.000199198,0.243341057,0.326005631,0.021354878,4.90E-05,7.10E-06
8181,sentiment_analysis38,33,The representation learned by this objective performs competitively on a broad suite of evaluated tasks .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,21,1,0,,0.027228508,0,negative,0.000818801,0.015241315,1.33E-05,2.14E-06,0.000377171,3.51E-05,7.48E-05,0.000149012,0.002125499,0.978208841,0.001502085,0.001450686,1.27E-06
8182,sentiment_analysis38,34,More advanced training techniques such as layer normalization further improve results .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,22,1,0,,0.103504608,0,negative,0.001517196,0.00076147,5.28E-06,0.000131249,0.001675303,0.000307278,6.14E-05,0.000182464,0.000173988,0.994662956,0.000405145,0.000113074,3.21E-06
8183,sentiment_analysis38,35,"However , skip - thought vectors are still outperformed by supervised models which directly optimize the desired performance metric on a specific dataset .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,23,1,0,,0.059469441,0,negative,0.0006857,0.002026307,1.24E-05,5.18E-06,0.000242877,4.56E-05,0.000122682,8.86E-05,0.000183309,0.989318453,0.006274553,0.000992001,2.25E-06
8184,sentiment_analysis38,36,"This is the case for both text classification ar Xiv:1704.01444v2LG ] 6 Apr 2017 tasks , which measure whether a specific concept is well encoded in a representation , and more general semantic similarity tasks .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,24,1,0,,0.06617644,0,negative,6.67E-07,0.000546369,3.71E-07,8.64E-06,2.80E-05,3.84E-05,7.61E-06,4.93E-05,0.000178003,0.754409589,0.244730645,1.44E-06,1.01E-06
8185,sentiment_analysis38,37,"This occurs even when the datasets are relatively small by modern standards , often consisting of only a few thousand labeled examples .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,25,1,0,,0.004475654,0,negative,1.72E-06,0.00051778,2.40E-07,9.25E-06,6.62E-05,2.89E-05,2.68E-06,4.53E-05,0.000148424,0.982293937,0.016884055,1.14E-06,4.12E-07
8186,sentiment_analysis38,38,"In contrast to learning a generic representation on one large dataset and then evaluating on other tasks / datasets , proposed using similar unsupervised objectives such as sequence autoencoding and language modeling to first pretrain a model on a dataset and then finetune it for a given task .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,26,1,0,,0.003680628,0,negative,1.94E-06,0.001840312,2.70E-06,1.15E-05,0.00019523,6.21E-05,6.50E-06,6.71E-05,0.000562815,0.980794379,0.01645272,1.82E-06,8.61E-07
8187,sentiment_analysis38,39,This approach outperformed training the same model from random initialization and achieved state of the art on several text classification datasets .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,27,1,0,,0.082634533,0,negative,0.000141986,0.043400868,4.47E-05,3.24E-05,0.002669355,0.000334074,0.000247361,0.000504786,0.005716014,0.940094625,0.006154487,0.000644177,1.51E-05
8188,sentiment_analysis38,40,Combining language modelling with topic modelling and fitting a small supervised feature extractor on top has also achieved strong results on in - domain document level sentiment analysis .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,28,1,0,,0.110471125,0,negative,1.19E-05,0.00093837,4.42E-06,1.35E-05,0.000204872,7.45E-05,9.09E-05,5.24E-05,0.000132454,0.841555836,0.156874726,4.09E-05,5.25E-06
8189,sentiment_analysis38,41,"Considering this , we hypothesize two effects maybe combining to result in the weaker performance of purely unsupervised approaches .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,29,1,0,,0.001895539,0,negative,9.14E-06,0.001884341,2.50E-06,1.25E-05,0.001115132,5.25E-05,1.72E-06,3.61E-05,0.003171629,0.993671434,4.18E-05,1.05E-06,2.00E-07
8190,sentiment_analysis38,42,Skip - thought vectors were trained on a corpus of books .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,30,1,0,,0.267396741,0,negative,2.32E-05,0.256562364,4.60E-05,1.89E-05,0.005738102,0.00567305,0.000230285,0.018661147,0.096766426,0.616097845,0.000161964,1.21E-05,8.70E-06
8191,sentiment_analysis38,43,"But some of the classification tasks they are evaluated on , such as sentiment analysis of reviews of consumer goods , do not have much overlap with the text of novels .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,31,1,0,,0.003729257,0,negative,3.79E-07,0.000110203,3.36E-07,9.49E-06,0.00031094,3.78E-05,6.06E-06,1.49E-05,1.27E-05,0.991449099,0.00804677,8.54E-07,4.62E-07
8192,sentiment_analysis38,44,"We propose this distributional issue , combined with the limited capacity of current models , results in representational underfitting .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,32,1,0,,0.004521891,0,negative,9.13E-05,0.029042164,1.43E-05,1.26E-05,0.001906707,6.69E-05,8.31E-06,0.000109081,0.017453784,0.950857507,0.00042236,1.39E-05,9.86E-07
8193,sentiment_analysis38,45,"Current generic distributed sentence representations maybe very lossy - good at capturing the gist , but poor with the precise semantic or syntactic details which are critical for applications .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,33,1,0,,0.010479111,0,negative,3.05E-06,0.000642349,1.95E-06,1.24E-05,0.000130869,5.98E-05,2.53E-05,3.67E-05,0.000139272,0.926386681,0.072553535,6.09E-06,2.11E-06
8194,sentiment_analysis38,46,The experimental and evaluation protocols maybe underestimating the quality of unsupervised representation learning for sentences and documents due to certain seemingly insignificant design decisions .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,34,1,0,,0.002675104,0,negative,9.82E-07,6.34E-05,2.16E-07,9.88E-07,7.39E-05,8.15E-06,1.98E-06,5.83E-06,1.61E-05,0.99885346,0.000973416,1.54E-06,8.94E-08
8195,sentiment_analysis38,47,also raises concern about current evaluation tasks in their recent work which provides a thorough survey of architectures and objectives for learning unsupervised sentence representations - including the above mentioned skip - thoughts .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,35,1,0,,0.000637739,0,negative,8.26E-07,7.28E-05,5.55E-07,5.81E-06,0.000163656,2.43E-05,2.39E-06,8.90E-06,3.30E-05,0.998681988,0.001004911,6.20E-07,1.98E-07
8196,sentiment_analysis38,48,"In this work , we test whether this is the case .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,36,1,0,,0.000827411,0,negative,1.99E-06,0.004772998,2.56E-06,3.14E-06,0.000597557,6.08E-05,3.60E-06,0.000114576,0.003101177,0.991213258,0.000126794,1.22E-06,3.62E-07
8197,sentiment_analysis38,49,We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,37,1,1,approach,0.067950091,0,negative,2.55E-05,0.161891292,0.000112963,0.000194534,0.027318454,0.000343732,0.000170833,0.000308603,0.004471117,0.783349744,0.02174452,4.89E-05,1.98E-05
8198,sentiment_analysis38,50,showed that word - level recurrent language modelling supports the learning of useful word vectors and we are interested in pushing this line of work .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,38,1,0,,0.00294782,0,negative,2.04E-06,0.000502528,1.93E-06,6.40E-06,0.000104824,9.40E-05,2.33E-05,7.14E-05,0.000498376,0.987048996,0.011639327,4.64E-06,2.14E-06
8199,sentiment_analysis38,51,"As an approach , we consider the popular research benchmark of byte ( character ) level language modelling due to its further simplicity and generality .",Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,39,1,1,approach,0.030503995,0,negative,8.85E-06,0.055272739,5.98E-05,0.000110914,0.073460354,0.00029699,6.02E-05,0.000163943,0.002557606,0.866848764,0.001139794,1.38E-05,6.22E-06
8200,sentiment_analysis38,52,We are also interested in evaluating this approach as it is not immediately clear whether such a low - level training objective supports the learning of high - level representations .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,40,1,0,,0.001576392,0,negative,3.14E-06,0.003748527,2.50E-06,1.41E-06,0.000156215,5.16E-05,7.56E-06,0.000151301,0.002027372,0.993636361,0.000208385,5.17E-06,4.34E-07
8201,sentiment_analysis38,53,We train on a very large corpus picked to have a similar distribution as our task of interest .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,41,1,1,approach,0.000955787,0,negative,1.39E-05,0.023274663,2.77E-05,6.07E-05,0.302661968,0.000558426,4.89E-05,0.00027891,0.000808166,0.672246446,6.08E-06,1.18E-05,2.35E-06
8202,sentiment_analysis38,54,We also benchmark on a wider range of tasks to quantify the sensitivity of the learned represen -tation to various degrees of out - of - domain data and tasks .,Introduction and Motivating Work,Introduction and Motivating Work,sentiment_analysis,38,42,1,0,,0.022854827,0,negative,2.69E-05,0.046135865,1.61E-05,3.07E-06,0.004706563,7.97E-05,3.65E-05,0.000192179,0.003108144,0.94562241,1.75E-05,5.43E-05,6.90E-07
8203,sentiment_analysis38,55,Dataset,Introduction and Motivating Work,,sentiment_analysis,38,43,1,0,,0.000421051,0,negative,1.36E-06,0.000169178,9.86E-07,1.65E-05,0.000264122,0.000216365,1.58E-05,0.000123439,0.000227112,0.998843638,0.000117831,2.30E-06,1.45E-06
8204,sentiment_analysis38,56,Much previous work on language modeling has evaluated on relatively small but competitive datasets such as Penn Treebank and Hutter Prize Wikipedia .,Introduction and Motivating Work,Dataset,sentiment_analysis,38,44,1,0,,0.041907213,0,negative,1.82E-06,0.000433294,2.91E-06,1.77E-05,0.00074534,0.00019224,7.73E-05,5.62E-05,2.86E-05,0.988665658,0.009770047,4.59E-06,4.31E-06
8205,sentiment_analysis38,57,As discussed in performance on these datasets is primarily dominated by regularization .,Introduction and Motivating Work,Dataset,sentiment_analysis,38,45,1,0,,0.018079919,0,negative,0.000137365,0.000605406,5.90E-06,8.07E-06,0.003461707,9.49E-05,2.01E-05,3.60E-05,5.37E-05,0.995549613,6.99E-06,1.99E-05,3.72E-07
8206,sentiment_analysis38,58,"Since we are interested in high - quality sentiment representations , we chose the Amazon product review dataset introduced in as a training corpus .",Introduction and Motivating Work,Dataset,sentiment_analysis,38,46,1,0,,0.045799112,0,dataset,3.79E-06,0.007415124,3.05E-05,4.34E-05,0.580293005,0.000511639,5.80E-05,0.000107325,0.000124627,0.411403229,3.70E-06,3.88E-06,1.81E-06
8207,sentiment_analysis38,59,"In de-duplicated form , this dataset contains over 82 million product reviews from May 1996 to July 2014 amounting to over 38 billion training bytes .",Introduction and Motivating Work,Dataset,sentiment_analysis,38,47,1,0,,0.076821841,0,dataset,1.06E-06,0.000445715,8.32E-06,0.000128123,0.946684545,0.000142414,8.96E-06,6.31E-06,9.57E-06,0.052563473,2.78E-07,5.97E-07,6.34E-07
8208,sentiment_analysis38,60,"Due to the size of the dataset , we first split it into 1000 shards containing equal numbers of reviews and set aside 1 shard for validation and 1 shard for test .",Introduction and Motivating Work,Dataset,sentiment_analysis,38,48,1,0,,0.050071136,0,negative,1.67E-05,0.020006634,4.51E-05,2.42E-05,0.166817459,0.002471605,0.000260134,0.000966849,0.001381946,0.807993715,3.10E-06,6.18E-06,6.37E-06
8209,sentiment_analysis38,61,Model and Training Details,,,sentiment_analysis,38,0,1,0,,0.008282172,0,negative,2.79E-05,0.001206993,3.04E-05,3.17E-05,1.55E-05,0.000664561,0.000143346,0.008059807,0.000602508,0.987764957,0.001368502,7.03E-05,1.36E-05
8210,sentiment_analysis38,62,Many potential recurrent architectures and hyperparameter settings were considered in preliminary experiments on the dataset .,Model and Training Details,Model and Training Details,sentiment_analysis,38,1,1,0,,0.088351068,0,experimental-setup,4.77E-05,3.67E-05,3.99E-05,1.85E-05,6.06E-06,0.609017779,0.001407618,0.256091662,4.20E-05,0.133205178,1.60E-05,4.94E-05,2.14E-05
8211,sentiment_analysis38,63,"Given the size of the dataset , searching the wide space of possible configurations is quite costly .",Model and Training Details,Model and Training Details,sentiment_analysis,38,2,1,0,,0.05807796,0,negative,0.000157786,0.000106579,7.51E-05,0.00013172,2.03E-05,0.190124134,0.004360254,0.128065523,0.000112248,0.672172715,0.004193666,0.00026318,0.000216733
8212,sentiment_analysis38,64,"To help alleviate this , we evaluated the generative performance of smaller candidate models after a single pass through the dataset .",Model and Training Details,Model and Training Details,sentiment_analysis,38,3,1,0,,0.02279081,0,negative,0.002293749,0.00112697,0.004017656,1.49E-05,8.93E-05,0.032596741,0.001484191,0.03413182,0.000590664,0.922097993,4.50E-05,0.001476175,3.49E-05
8213,sentiment_analysis38,65,The model chosen for the large scale experiment is a single layer multiplicative LSTM with 4096 units .,Model and Training Details,Model and Training Details,sentiment_analysis,38,4,1,0,,0.968480444,1,experimental-setup,8.78E-06,2.13E-05,6.60E-05,1.02E-05,1.74E-06,0.677024903,0.003725739,0.30970988,3.55E-05,0.009346359,5.77E-06,8.74E-06,3.51E-05
8214,sentiment_analysis38,66,We observed multiplicative LSTMs to converge faster than normal LSTMs for the hyperparam - eter settings that were explored both in terms of data and wall - clock time .,Model and Training Details,Model and Training Details,sentiment_analysis,38,5,1,0,,0.887336259,1,experimental-setup,0.000690126,8.36E-05,0.000173685,3.08E-05,1.80E-05,0.374502699,0.013506525,0.340853416,6.46E-05,0.26355556,0.000193677,0.006078961,0.000248283
8215,sentiment_analysis38,67,The model was trained for a single epoch on mini-batches of 128 subsequences of length 256 for a total of 1 million weight updates .,Model and Training Details,Model and Training Details,sentiment_analysis,38,6,1,0,,0.960110952,1,experimental-setup,1.27E-05,4.82E-05,3.44E-05,9.73E-06,3.15E-06,0.559063389,0.002933793,0.422941945,4.68E-05,0.01484245,6.85E-06,1.13E-05,4.53E-05
8216,sentiment_analysis38,68,States were initialized to zero at the beginning of each shard and persisted across updates to simulate full - backpropagation and allow for the forward propagation of information outside of a given subsequence .,Model and Training Details,Model and Training Details,sentiment_analysis,38,7,1,0,,0.524747947,1,hyperparameters,1.24E-05,7.93E-05,0.000132033,1.28E-06,9.25E-07,0.304599838,0.000798386,0.626521843,0.000440704,0.067381631,9.48E-06,9.28E-06,1.30E-05
8217,sentiment_analysis38,69,Adam ) was used to accelerate learning with an initial 5e - 4 learning rate that was decayed linearly to zero over the course of training .,Model and Training Details,Model and Training Details,sentiment_analysis,38,8,1,0,,0.974727116,1,hyperparameters,1.11E-05,4.14E-05,2.73E-05,4.43E-06,1.01E-06,0.330380718,0.001118487,0.661844925,5.19E-05,0.00648924,2.30E-06,4.52E-06,2.27E-05
8218,sentiment_analysis38,70,Weight normalization was applied to the LSTM parameters .,Model and Training Details,Model and Training Details,sentiment_analysis,38,9,1,0,,0.908298558,1,hyperparameters,2.84E-05,0.000117827,8.38E-05,5.38E-06,2.42E-06,0.304865722,0.000994712,0.657590264,0.000293559,0.035976016,4.61E-06,1.05E-05,2.69E-05
8219,sentiment_analysis38,71,Data - parallelism was used across 4 Pascal Titan,Model and Training Details,Model and Training Details,sentiment_analysis,38,10,1,0,,0.531757635,1,experimental-setup,1.04E-05,1.55E-05,0.000247988,3.95E-05,6.88E-06,0.876651914,0.012908087,0.095982021,2.60E-05,0.013985251,7.68E-06,3.67E-05,8.22E-05
8220,sentiment_analysis38,72,X gpus to speedup training and increase effective memory size .,Model and Training Details,Model and Training Details,sentiment_analysis,38,11,1,0,,0.275122942,0,experimental-setup,7.04E-05,5.77E-05,0.00059629,3.77E-05,1.59E-05,0.547063612,0.006306848,0.193166648,0.000173629,0.252150671,4.99E-05,0.000149681,0.000161016
8221,sentiment_analysis38,73,Training took approximately one month .,Model and Training Details,Model and Training Details,sentiment_analysis,38,12,1,0,,0.025480098,0,experimental-setup,0.000176513,0.000280157,0.000364935,0.000215139,0.000215073,0.380203915,0.00815799,0.246359447,0.000364379,0.362807209,7.94E-05,0.000322295,0.000453569
8222,sentiment_analysis38,74,"The model is compact , containing approximately as many parameters as there are reviews in the training dataset .",Model and Training Details,Model and Training Details,sentiment_analysis,38,13,1,0,,0.593429363,1,hyperparameters,0.000435938,0.00272046,0.001419181,4.09E-05,5.73E-05,0.225923363,0.003859153,0.666808211,0.002265559,0.095687679,6.48E-05,0.00041339,0.000304015
8223,sentiment_analysis38,75,It also has a high ratio of compute to total parameters compared to other large scale language models due to operating at a byte level .,Model and Training Details,Model and Training Details,sentiment_analysis,38,14,1,0,,0.025636569,0,negative,0.003039578,0.001471473,0.271531334,0.000200968,0.00044753,0.09896482,0.036047663,0.030921361,0.002003938,0.541157281,0.000907067,0.011865865,0.001441123
8224,sentiment_analysis38,76,The selected model reaches 1.12 bits per byte .,Model and Training Details,Model and Training Details,sentiment_analysis,38,15,1,0,,0.928439315,1,hyperparameters,0.000320316,0.000166375,0.00045955,1.53E-05,1.70E-05,0.334385758,0.013096173,0.569967577,0.00028818,0.07943443,2.96E-05,0.001455392,0.0003644
8225,sentiment_analysis38,77,Experimental Setup and Results,,,sentiment_analysis,38,0,1,0,,0.000883244,0,negative,7.40E-06,4.90E-05,3.11E-06,4.51E-07,6.57E-07,5.05E-05,3.63E-05,0.000575577,1.89E-05,0.99834407,0.000747503,0.000165495,1.07E-06
8226,sentiment_analysis38,78,Our model processes text as a sequence of UTF - 8 encoded bytes .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,1,1,0,,0.770038283,1,baselines,0.000502224,0.000380244,0.552808771,3.43E-05,1.12E-05,0.026440767,0.006557893,0.039276777,0.001782937,0.370067434,0.000675804,0.000965316,0.000496302
8227,sentiment_analysis38,79,"For each byte , the model updates its hidden state and predicts a probability distribution over the next possible byte .",Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,2,1,0,,0.598211933,1,negative,0.000793203,0.00027347,0.289766539,2.42E-06,1.64E-06,0.009336665,0.001292865,0.039221831,0.001850763,0.656480983,0.000140292,0.000781197,5.81E-05
8228,sentiment_analysis38,80,The hidden state of the model serves as an online summary of the sequence which encodes all information the model has learned to preserve that is relevant to predicting the future bytes of the sequence .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,3,1,0,,0.388758378,0,negative,0.000908596,0.000247939,0.10278147,9.09E-06,5.12E-06,0.010867342,0.00105911,0.033017033,0.001689733,0.8483316,9.32E-05,0.000899904,8.99E-05
8229,sentiment_analysis38,81,We are interested in understanding the properties of the learned encoding .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,4,1,0,,0.010252551,0,negative,7.23E-05,1.88E-05,0.000585737,4.22E-06,1.04E-06,0.002888904,0.000298276,0.0068791,2.36E-05,0.988436937,0.000371616,0.00040138,1.81E-05
8230,sentiment_analysis38,82,The process of extracting a feature representation is outlined as follows :,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,5,1,0,,0.050158601,0,negative,0.000441791,4.04E-05,0.027107063,6.00E-06,2.84E-06,0.004538365,0.000602127,0.006556421,0.000146225,0.959716079,7.47E-05,0.000710255,5.77E-05
8231,sentiment_analysis38,83,"Since newlines are used as review delimiters in the training dataset , all newline characters are replaced with spaces to avoid the model resetting state .",Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,6,1,0,,0.311651523,0,negative,0.001638304,8.13E-05,0.014446881,1.95E-06,3.09E-06,0.019760426,0.001098757,0.079235565,7.88E-05,0.882577302,3.52E-06,0.001058751,1.54E-05
8232,sentiment_analysis38,84,Any leading whitespace is removed and replaced with a newline + space to simulate a start token .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,7,1,0,,0.174676704,0,negative,0.00432324,9.46E-05,0.220532084,2.33E-06,3.52E-06,0.008667729,0.002033341,0.019171723,0.000149598,0.74063864,1.10E-05,0.004335666,3.65E-05
8233,sentiment_analysis38,85,Any trailing whitespace is removed and replaced with a space to simulate an end token .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,8,1,0,,0.153603983,0,negative,0.005579942,7.44E-05,0.144265128,2.44E-06,3.96E-06,0.004989243,0.001475107,0.010730112,0.000124673,0.827290094,1.05E-05,0.005420346,3.40E-05
8234,sentiment_analysis38,86,The text is encoded as a UTF - 8 byte sequence .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,9,1,0,,0.907170329,1,negative,0.000304673,0.000167791,0.016442473,2.37E-05,1.56E-05,0.185443868,0.016954961,0.388288825,0.00015968,0.390658921,4.12E-05,0.001153406,0.000344924
8235,sentiment_analysis38,87,Model states are initialized to zeros .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,10,1,0,,0.961804554,1,hyperparameters,6.44E-05,3.60E-05,0.000747082,1.61E-06,4.80E-07,0.095274826,0.00171328,0.728130229,9.79E-05,0.173765355,1.02E-05,0.00012924,2.94E-05
8236,sentiment_analysis38,88,The model processes the sequence and the final cell states of the mL - STM are used as a feature representation .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,11,1,0,,0.346215843,0,negative,0.000334103,0.000163971,0.120797025,5.17E-06,3.25E-06,0.009660999,0.001247081,0.035931738,0.002140166,0.829034473,8.51E-05,0.000471466,0.000125475
8237,sentiment_analysis38,89,Tanh is applied to bound values between - 1 and 1 .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,12,1,0,,0.735473392,1,hyperparameters,8.22E-05,2.89E-05,0.000964542,2.64E-06,5.75E-07,0.161053886,0.002493082,0.734112431,4.38E-05,0.101087806,4.34E-06,8.60E-05,3.98E-05
8238,sentiment_analysis38,90,"We follow the methodology established in by training a logistic regression classifier on top of our model 's representation on datasets for tasks including semantic relatedness , text classification , and paraphrase detection .",Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,13,1,0,,0.103569005,0,negative,0.001573174,0.000904463,0.031329797,2.80E-05,7.06E-05,0.007402667,0.003231249,0.015052442,0.000156112,0.936479811,2.34E-05,0.003651328,9.69E-05
8239,sentiment_analysis38,91,"For the details on these comparison experiments , we refer the reader to their work .",Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,14,1,0,,0.00629062,0,negative,4.53E-05,2.54E-06,0.000197309,2.61E-06,1.32E-06,0.002050443,0.000245087,0.002548624,2.89E-06,0.994453955,5.43E-06,0.000438525,5.95E-06
8240,sentiment_analysis38,92,One exception is that we use an L1 penalty for text classification results instead of L2 as we found this performed better in the very low data regime .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,15,1,0,,0.34884323,0,negative,0.008309967,1.11E-05,0.007613531,3.15E-06,3.73E-06,0.005057747,0.009830822,0.007690073,3.05E-06,0.683070342,2.72E-05,0.278296809,8.24E-05
8241,sentiment_analysis38,93,shows the results of our model on 4 standard text classification datasets .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,16,1,0,,0.485167235,0,negative,0.000380959,2.01E-06,0.002990386,4.15E-07,1.29E-06,0.000876547,0.007741265,0.001528186,4.40E-07,0.804362056,1.51E-05,0.182066978,3.43E-05
8242,sentiment_analysis38,94,The performance of our model is noticeably lopsided .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,17,1,0,,0.848772959,1,results,0.004881196,7.76E-07,0.000645259,7.12E-07,8.41E-07,0.000171284,0.004934725,0.000422042,2.03E-07,0.036665453,3.30E-06,0.952210447,6.38E-05
8243,sentiment_analysis38,95,On the MR and CR ) sentiment analysis datasets we improve the state of the art by a significant margin .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,18,1,0,,0.977984241,1,results,0.000676752,7.64E-07,0.000389255,3.04E-07,5.53E-07,4.09E-05,0.004998882,0.000117051,8.02E-08,0.009700515,5.66E-06,0.984008759,6.06E-05
8244,sentiment_analysis38,96,"The MR and CR datasets are sentences extracted from Rotten Tomatoes , a movie review website , and Amazon product reviews ( which almost certainly overlaps with our training corpus ) .",Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,19,1,0,,0.097106761,0,negative,0.000194789,3.23E-05,0.009472719,1.66E-05,0.00021095,0.003152804,0.005165252,0.002632823,5.88E-06,0.973714912,5.01E-06,0.005321814,7.41E-05
8245,sentiment_analysis38,97,This suggests that our model has learned a rich representation of text from a similar domain .,Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,20,1,0,,0.020789094,0,negative,0.001246325,1.06E-05,0.005851219,1.41E-06,5.55E-06,0.001200183,0.000483153,0.001498445,1.27E-05,0.983390347,2.31E-06,0.006277768,2.01E-05
8246,sentiment_analysis38,98,"On the other two datasets , SUBJ 's subjectivity / objectivity detection ) and MPQA 's opinion polarity our model has no noticeable advantage over other unsupervised representation learning approaches and is still outperformed by a supervised approach .",Experimental Setup and Results,Experimental Setup and Results,sentiment_analysis,38,21,1,0,,0.700421756,1,results,0.000876092,3.10E-07,0.000175103,1.81E-07,3.21E-07,5.61E-05,0.00370832,0.000160909,5.30E-08,0.015005757,1.62E-06,0.979979879,3.54E-05
8247,sentiment_analysis38,99,Review Sentiment Analysis,Experimental Setup and Results,,sentiment_analysis,38,22,1,1,results,0.985023456,1,experiments,0.003736967,5.67E-05,0.007770103,0.001240034,0.000416057,0.006504517,0.474916526,0.003742778,1.26E-05,0.07894165,0.001278809,0.402084524,0.019298743
8248,sentiment_analysis38,100,"To better quantify the learned representation , we also test on a wider set of sentiment analysis datasets with different properties .",Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,23,1,0,,0.235499563,0,negative,0.000392006,3.62E-05,2.35E-05,2.59E-07,2.02E-05,0.001361187,0.284976591,3.82E-05,1.19E-05,0.712141929,4.93E-07,0.000975816,2.17E-05
8249,sentiment_analysis38,101,The Stanford Sentiment Treebank ( SST ) was created specifically to evaluate more complex compositional models of language .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,24,1,0,,0.442161384,0,negative,9.27E-05,1.49E-05,0.00040779,1.13E-05,0.002201485,0.005307109,0.393891765,1.91E-05,6.07E-06,0.597296793,3.58E-06,0.000290248,0.000457233
8250,sentiment_analysis38,102,It is derived from the same base dataset as MR but was relabeled via Amazon Mechanical and includes dense labeling of the phrases of parse trees computed for all sentences .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,25,1,0,,0.031636336,0,negative,5.68E-05,5.25E-06,0.000133912,3.78E-06,0.001130303,0.003007909,0.129402705,1.07E-05,2.18E-06,0.86606364,4.42E-07,0.000123018,5.94E-05
8251,sentiment_analysis38,103,"For the binary subtask , this amounts to 76961 total labels compared to the 6920 sentence level labels .",Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,26,1,0,,0.482990245,0,negative,0.000123796,7.67E-06,1.99E-05,8.77E-06,0.001145275,0.011558061,0.376239761,5.93E-05,3.19E-06,0.610433056,4.19E-07,0.000132073,0.000268723
8252,sentiment_analysis38,104,"As a demonstration of the capability of unsupervised representation learning to simplify data collection and remove preprocessing steps , our reported results ignore these dense labels and computed parse trees , using only the raw text and sentence level labels .",Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,27,1,0,,0.021742435,0,negative,0.000174215,2.93E-06,1.03E-05,1.31E-06,9.35E-05,0.000513886,0.013888978,3.02E-06,1.89E-06,0.985163521,4.42E-07,0.000137758,8.32E-06
8253,sentiment_analysis38,105,The representation learned by our model achieves 91.8 % significantly outperforming the state of the art of 90.2 % by a 30 model ensemble .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,28,1,1,results,0.967322545,1,experiments,0.000315537,3.75E-07,3.15E-06,8.66E-08,9.12E-07,0.000119331,0.987014619,3.19E-06,1.32E-07,0.00740092,1.09E-07,0.004977175,0.000164455
8254,sentiment_analysis38,106,"As visualized in , our model is very data efficient .",Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,29,1,0,,0.87347113,1,experiments,0.000261073,4.67E-07,3.56E-06,5.78E-08,5.92E-07,0.000216924,0.968340309,5.06E-06,3.64E-07,0.026193229,2.76E-07,0.00488893,8.92E-05
8255,sentiment_analysis38,107,It matches the performance of baselines using as few as a dozen labeled examples and outperforms all previous results with only a few hundred labeled examples .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,30,1,1,results,0.879327498,1,experiments,8.42E-05,2.77E-07,1.81E-06,4.50E-07,1.64E-06,0.000769614,0.955385104,1.36E-05,1.61E-07,0.041163258,4.85E-07,0.00225015,0.000329261
8256,sentiment_analysis38,108,This is under 10 % of the total sentences in the dataset .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,31,1,0,,0.131351914,0,negative,5.51E-05,7.09E-07,4.85E-06,1.06E-05,0.000580634,0.001902789,0.024689528,5.30E-06,5.06E-07,0.972644088,2.16E-07,3.84E-05,6.73E-05
8257,sentiment_analysis38,109,"Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of 53.6 % on the fine - grained subtask , achieving 52.9 % .",Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,32,1,1,results,0.75294195,1,experiments,0.001321184,4.37E-07,6.85E-06,1.46E-07,3.02E-06,0.00015996,0.943045929,3.28E-06,1.15E-07,0.045974009,2.47E-07,0.009359301,0.000125519
8258,sentiment_analysis38,110,We conducted further analysis to understand what repre - sentations our model learned and how they achieve the observed data efficiency .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,33,1,0,,0.010264612,0,negative,8.75E-05,7.53E-07,3.26E-06,2.48E-07,4.14E-05,0.000640162,0.011683399,2.64E-06,6.80E-07,0.987496332,2.91E-08,4.10E-05,2.51E-06
8259,sentiment_analysis38,111,The benefit of an L1 penalty in the low data regime ( see ) is a clue .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,34,1,0,,0.026031004,0,negative,0.000161263,4.68E-07,2.95E-06,8.73E-08,1.10E-06,0.000742903,0.060910909,7.95E-06,1.63E-06,0.93800506,3.44E-07,0.000156161,9.18E-06
8260,sentiment_analysis38,112,L1 regularization is known to reduce sample complexity when there are many irrelevant features .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,35,1,1,results,0.51240749,1,negative,0.000137155,8.66E-06,3.05E-05,3.20E-06,1.28E-05,0.005870927,0.158084117,0.000103433,1.63E-05,0.835336748,9.16E-06,0.000148387,0.000238517
8261,sentiment_analysis38,113,This is likely to be the case for our model since it is trained as a language model and not as a supervised feature extractor .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,36,1,0,,0.022405366,0,negative,3.13E-05,9.09E-07,2.42E-06,1.43E-07,2.10E-06,0.001167274,0.019480297,1.31E-05,3.11E-06,0.979265874,2.60E-07,2.57E-05,7.48E-06
8262,sentiment_analysis38,114,"By inspecting the relative contributions of features on various datasets , we discovered a single unit within the mLSTM that directly corresponds to sentiment .",Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,37,1,0,,0.677518843,1,negative,0.000900663,2.29E-05,3.43E-05,2.82E-06,0.000122632,0.003653733,0.292509662,4.91E-05,8.48E-05,0.70192444,6.59E-07,0.000480959,0.000213322
8263,sentiment_analysis38,115,In we show the histogram of the final activations of this unit after processing IMDB reviews which shows a bimodal distribution with a clear separation between positive and negative reviews .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,38,1,0,,0.185671015,0,negative,0.000427528,3.04E-06,4.34E-05,1.87E-07,2.11E-05,0.00074607,0.354909404,7.14E-06,9.83E-06,0.642549436,3.25E-07,0.001221548,6.09E-05
8264,sentiment_analysis38,116,In we visualize the activations of this unit on 6 randomly selected reviews from a set of 100 high contrast reviews which shows it acts as an online estimate of the local sentiment of the review .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,39,1,0,,0.165042207,0,negative,0.000290938,1.76E-05,9.23E-05,7.06E-07,3.18E-05,0.002537629,0.485042059,4.03E-05,0.000100894,0.511134303,9.92E-07,0.000451371,0.000259184
8265,sentiment_analysis38,117,"Fitting a threshold to this single unit achieves a test accuracy of 92.30 % which outperforms a strong supervised results on the dataset , the 91.87 % of NB - SVM trigram , but is still below the semi-supervised state of the art of 94.09 % .",Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,40,1,1,results,0.907440203,1,experiments,0.001024234,4.53E-07,6.67E-06,7.82E-08,4.23E-06,0.000124682,0.958639386,2.06E-06,1.20E-07,0.031772866,4.13E-08,0.008275974,0.000149196
8266,sentiment_analysis38,118,Using the full 4096 unit representation achieves 92.88 % .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,41,1,1,results,0.967542778,1,experiments,0.000185709,1.12E-07,1.62E-06,1.65E-08,2.52E-07,8.57E-05,0.984294697,2.16E-06,6.54E-08,0.012587078,1.89E-08,0.00275788,8.47E-05
8267,sentiment_analysis38,119,This is an improvement of only 0.58 % over the sentiment unit suggesting that almost all information the model retains that is relevant to sentiment analysis is represented in the very compact form of a single scalar .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,42,1,0,,0.96812416,1,experiments,0.001316901,4.10E-07,3.97E-06,8.34E-08,2.69E-06,0.000116605,0.976772003,2.17E-06,1.65E-07,0.017067639,1.98E-08,0.004541369,0.000175974
8268,sentiment_analysis38,120,has a full list of results on the IMDB dataset .,Experimental Setup and Results,Review Sentiment Analysis,sentiment_analysis,38,43,1,0,,0.033901726,0,negative,1.86E-05,2.07E-07,1.39E-05,3.52E-07,2.32E-05,0.000791635,0.08513609,2.57E-06,3.22E-07,0.913838289,1.44E-07,0.000124817,4.99E-05
8269,sentiment_analysis38,121,Sentiment Unit,Experimental Setup and Results,,sentiment_analysis,38,44,1,0,,0.745960964,1,negative,0.001830988,4.89E-05,0.176454981,2.38E-05,4.94E-05,0.009202008,0.121111205,0.015926718,0.000112792,0.452008067,1.82E-05,0.211835704,0.011377159
8270,sentiment_analysis38,122,Capacity Ceiling,Experimental Setup and Results,,sentiment_analysis,38,45,1,1,results,0.320570803,0,negative,0.001743239,3.82E-05,0.013084991,3.79E-05,4.23E-05,0.009173594,0.015194373,0.024304416,0.000143881,0.878898359,4.57E-06,0.055131175,0.002203013
8271,sentiment_analysis38,123,"Encouraged by these results , we were curious how well the model 's representation scales to larger datasets .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,46,1,0,,0.007661538,0,negative,0.000126673,1.04E-07,9.34E-06,2.61E-07,1.84E-06,0.00235908,0.003056753,6.36E-05,1.67E-07,0.993257475,3.98E-08,0.001040697,8.39E-05
8272,sentiment_analysis38,124,We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in .,Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,47,1,1,results,0.569965491,1,negative,0.000304105,1.39E-05,0.001412133,1.51E-06,4.07E-05,0.01115539,0.12214884,0.000362988,2.77E-06,0.85520912,1.82E-07,0.007331627,0.002016782
8273,sentiment_analysis38,125,"This dataset contains 598,000 examples which is an order of magnitude larger than any other datasets we tested on .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,48,1,0,,0.034990956,0,negative,2.20E-05,2.70E-07,6.33E-05,9.58E-06,0.000598772,0.007100293,0.002035125,2.55E-05,1.31E-07,0.989560741,4.75E-09,0.000117584,0.000466697
8274,sentiment_analysis38,126,"When visualizing performance as a function of number of training examples in , we observe a "" capacity ceiling "" where the test accuracy of our approach only improves by a little over 1 % across a four order of magnitude increase in training data .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,49,1,0,,0.688308415,1,negative,0.051813617,7.08E-06,0.000697407,3.42E-06,5.62E-05,0.00307983,0.162042938,0.000207322,3.79E-06,0.580444332,3.29E-07,0.194160688,0.007483022
8275,sentiment_analysis38,127,"Using the full dataset , we achieve 95 . 22 % test accuracy .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,50,1,1,results,0.955997711,1,experiments,0.001729219,5.41E-07,8.94E-05,3.25E-07,6.52E-06,0.000936616,0.416614907,8.58E-05,1.52E-07,0.191282136,3.64E-08,0.381939263,0.00731516
8276,sentiment_analysis38,128,"This better than a BoW TFIDF baseline at 93 . 66 % but slightly worse than the 95.64 % of a linear classifier on top of the 500,000 most frequent n- grams up to length 5 .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,51,1,0,,0.02860368,0,experiments,0.001685289,3.65E-07,0.00023426,8.27E-07,9.30E-06,0.004356846,0.509628699,0.00017337,2.16E-07,0.310418831,7.00E-08,0.163903252,0.009588672
8277,sentiment_analysis38,129,The observed capacity ceiling is an interesting phenomena and stumbling point for scaling our unsupervised representations .,Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,52,1,1,results,0.028275881,0,negative,0.000160881,2.54E-07,3.27E-05,5.69E-07,5.55E-06,0.000964029,0.000866827,3.11E-05,7.84E-07,0.997403874,2.81E-08,0.00040902,0.000124352
8278,sentiment_analysis38,130,We think a variety of factors are contributing to cause this .,Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,53,1,0,,0.002008141,0,negative,9.24E-06,5.80E-08,8.05E-06,9.51E-08,8.48E-07,0.000737116,0.000334409,2.53E-05,2.74E-07,0.998818511,6.19E-09,3.63E-05,2.98E-05
8279,sentiment_analysis38,131,"Since our model is trained only on Amazon reviews , it is does not appear to be sensitive to concepts specific to other domains .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,54,1,0,,0.001943635,0,negative,0.000843187,1.92E-07,8.50E-05,2.23E-07,1.69E-05,0.000461627,0.002512519,1.03E-05,1.46E-07,0.991830275,5.88E-09,0.00414391,9.58E-05
8280,sentiment_analysis38,132,"For instance , Yelp reviews are of businesses , where details like hospitality , location , and atmosphere are important .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,55,1,0,,0.006313961,0,negative,9.47E-06,1.37E-07,2.24E-05,2.25E-06,1.51E-05,0.003471005,0.002594288,4.59E-05,1.49E-07,0.992199116,1.38E-07,0.000247186,0.00139284
8281,sentiment_analysis38,133,But these ideas are not present in reviews of products .,Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,56,1,0,,0.000439317,0,negative,3.02E-06,1.40E-08,4.16E-06,4.28E-08,5.96E-07,0.000351321,0.000152116,5.91E-06,5.74E-08,0.999440308,7.73E-09,2.76E-05,1.48E-05
8282,sentiment_analysis38,134,"Additionally , there is a notable drop in the relative performance of our approach transitioning from sentence to document datasets .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,57,1,1,results,0.426863289,0,negative,0.016013665,1.05E-06,0.000491375,1.10E-06,2.43E-05,0.001016711,0.123700785,4.92E-05,3.83E-07,0.59009659,9.38E-08,0.263553954,0.005050817
8283,sentiment_analysis38,135,This is likely due to our model working on the byte level which leads to it focusing on the content of the last few sentences instead of the whole document .,Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,58,1,0,,0.004448809,0,negative,0.000107575,1.38E-07,4.03E-05,4.54E-08,1.50E-06,0.00052918,0.001299463,1.80E-05,2.79E-07,0.997540857,3.52E-09,0.000412033,5.06E-05
8284,sentiment_analysis38,136,"Finally , as the amount of labeled data increases , the performance of the simple linear model we train on top of our static representation will eventually saturate .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,59,1,1,results,0.308797886,0,negative,0.005083706,1.16E-06,0.000145959,2.77E-06,1.80E-05,0.006218493,0.036835761,0.000328468,1.75E-06,0.937873474,6.37E-08,0.007542882,0.005947483
8285,sentiment_analysis38,137,Complex models explicitly trained for a task can continue to improve and eventually outperform our approach with enough labeled data .,Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,60,1,0,,0.17315451,0,negative,0.00012149,7.55E-08,1.35E-05,1.88E-07,7.15E-07,0.001812037,0.021722848,8.07E-05,1.91E-07,0.971488095,4.78E-08,0.003524375,0.001235718
8286,sentiment_analysis38,138,"With this context , the observed results make a lot of sense .",Experimental Setup and Results,Capacity Ceiling,sentiment_analysis,38,61,1,0,,0.000835051,0,negative,4.31E-06,1.08E-08,1.78E-06,4.94E-09,1.04E-07,1.00E-04,0.000233013,5.62E-06,4.12E-08,0.999565114,1.56E-09,8.27E-05,7.35E-06
8287,sentiment_analysis38,139,Other Tasks,Experimental Setup and Results,,sentiment_analysis,38,62,1,0,,0.066176492,0,negative,5.58E-05,8.62E-07,0.000225821,4.19E-07,5.88E-06,0.000264161,0.001901599,0.000718807,4.74E-07,0.972384731,1.55E-07,0.024353839,8.74E-05
8288,sentiment_analysis38,140,"Besides classification , we also evaluate on two other standard tasks : semantic relatedness and paraphrase detection .",Experimental Setup and Results,Other Tasks,sentiment_analysis,38,63,1,0,,0.019765647,0,negative,6.89E-05,5.93E-06,0.000435139,9.89E-08,2.37E-05,0.003064723,0.049534577,7.41E-05,6.40E-07,0.943538462,7.92E-09,0.003161431,9.23E-05
8289,sentiment_analysis38,141,"While our model performs competitively on Microsoft Research Paraphrase Corpus in , it performs poorly on the SICK semantic relatedness task in .",Experimental Setup and Results,Other Tasks,sentiment_analysis,38,64,1,0,,0.689974321,1,experiments,0.000693122,1.40E-07,3.44E-05,1.29E-07,4.23E-06,0.001816416,0.455597026,6.98E-05,1.34E-08,0.194528529,1.69E-08,0.346431734,0.000824422
8290,sentiment_analysis38,142,"It is likely that the form and content of the semantic relatedness task , which is built on top of descriptions of images and videos and contains sentences such as "" A sea turtle is hunting for fish "" is effectively out - of - domain for our model which has only been trained on the text of product reviews .",Experimental Setup and Results,Other Tasks,sentiment_analysis,38,65,1,0,,0.001524187,0,negative,1.65E-05,1.39E-07,1.86E-05,9.73E-08,7.98E-06,0.002678646,0.004539506,4.32E-05,8.09E-08,0.99219683,4.83E-09,0.000465601,3.28E-05
8291,sentiment_analysis38,143,Generative Analysis,Experimental Setup and Results,,sentiment_analysis,38,66,1,0,,0.082660708,0,negative,0.000428965,8.89E-06,0.003170205,5.28E-06,1.16E-05,0.001097609,0.041604448,0.002266731,9.44E-06,0.80069357,1.20E-05,0.147414251,0.003276968
8292,sentiment_analysis38,144,"Although the focus of our analysis has been on the properties of our model 's representation , it is trained as a generative model and we are also interested in its generative capabilities .",Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,67,1,0,,9.96E-06,0,negative,2.47E-06,3.07E-07,1.57E-06,1.51E-08,5.56E-07,1.27E-05,2.39E-05,2.16E-06,8.86E-07,0.999950444,1.77E-09,4.69E-06,3.41E-07
8293,sentiment_analysis38,145,and both designed conditional generative models to disentangle the content of text from various attributes like sentiment or tense .,Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,68,1,0,,6.77E-06,0,negative,9.49E-07,2.50E-08,1.60E-06,8.00E-09,1.31E-07,2.18E-05,4.64E-05,1.25E-06,2.32E-07,0.999923856,3.66E-09,2.95E-06,7.07E-07
8294,sentiment_analysis38,146,We were curious whether a similar result could be achieved using the sentiment unit .,Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,69,1,0,,7.43E-05,0,negative,1.83E-06,4.63E-09,2.19E-07,3.94E-10,2.46E-08,4.49E-06,0.000127932,3.67E-07,1.92E-08,0.999825311,7.10E-10,3.96E-05,2.13E-07
8295,sentiment_analysis38,147,"In we show that by simply setting the sentiment unit to be positive or negative , the model generates corresponding positive or negative reviews .",Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,70,1,0,,0.000260698,0,negative,0.000121036,6.84E-07,7.12E-06,1.51E-09,3.83E-07,1.90E-06,0.000216855,6.65E-07,1.76E-06,0.999350848,1.15E-09,0.000298014,7.31E-07
8296,sentiment_analysis38,148,"While all sampled negative reviews contain sentences with negative sentiment , they sometimes contain sentences with positive sentiment as well .",Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,71,1,0,,0.000584002,0,negative,1.72E-05,2.65E-08,1.03E-06,4.81E-09,6.31E-06,4.30E-06,0.000108087,2.56E-07,5.33E-09,0.999775092,8.43E-11,8.74E-05,3.14E-07
8297,sentiment_analysis38,149,This might be reflective of the bias of the training corpus which contains over 5 x as many five star reviews as one star reviews .,Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,72,1,0,,1.18E-05,0,negative,1.45E-06,2.07E-08,2.90E-07,1.91E-09,1.61E-07,1.38E-05,7.21E-05,2.05E-06,5.63E-08,0.99990378,2.16E-10,5.82E-06,4.24E-07
8298,sentiment_analysis38,150,"Nevertheless , it is interesting to see that such a simple manipulation of the model 's representation has a noticeable effect on its behavior .",Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,73,1,0,,0.000174623,0,negative,0.000583468,6.24E-08,6.00E-07,5.60E-09,4.95E-07,1.25E-05,0.000975505,2.02E-06,5.46E-08,0.997814612,5.47E-10,0.000609456,1.25E-06
8299,sentiment_analysis38,151,The samples are also high quality for a byte level language model and often include valid sentences .,Experimental Setup and Results,Generative Analysis,sentiment_analysis,38,74,1,0,,0.000443468,0,negative,1.40E-06,1.23E-08,3.07E-07,9.98E-09,3.44E-06,1.04E-05,6.75E-05,6.49E-07,5.31E-09,0.999897383,7.52E-11,1.84E-05,5.94E-07
8300,sentiment_analysis38,152,Discussion and Future Work,Experimental Setup and Results,,sentiment_analysis,38,75,1,0,,0.009775714,0,negative,0.000107468,1.02E-06,0.000383789,1.46E-07,1.50E-06,0.000348913,0.001301908,0.000923079,2.13E-06,0.9871231,4.67E-08,0.009721398,8.55E-05
8301,sentiment_analysis38,153,"It is an open question why our model recovers the concept of sentiment in such a precise , disentangled , interpretable , and manipulable way .",Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,76,1,0,,6.11E-05,0,negative,2.48E-05,2.11E-06,5.19E-06,1.75E-07,4.56E-06,2.74E-05,3.42E-06,2.32E-05,2.46E-06,0.999883335,3.13E-08,2.17E-05,1.60E-06
8302,sentiment_analysis38,154,It is possible that sentiment as a conditioning feature has strong predictive capability for language modelling .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,77,1,0,,4.22E-05,0,negative,2.35E-05,5.30E-07,5.39E-06,3.72E-08,2.07E-06,2.50E-05,4.70E-06,1.88E-05,1.19E-06,0.999898148,5.84E-09,1.94E-05,1.20E-06
8303,sentiment_analysis38,155,This is likely since sentiment is such an important component of a review .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,78,1,0,,6.91E-06,0,negative,2.30E-05,5.99E-07,2.71E-06,1.75E-08,2.41E-06,8.32E-06,1.69E-06,1.03E-05,5.14E-07,0.999934675,1.66E-09,1.54E-05,3.52E-07
8304,sentiment_analysis38,156,Previous work analysing LSTM language models showed the existence of interpretable units that indicate position within a line or presence inside a quotation .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,79,1,0,,0.000471357,0,negative,1.61E-05,2.43E-06,2.61E-05,4.04E-07,5.72E-06,9.82E-05,5.19E-05,5.32E-05,1.60E-06,0.999647154,1.73E-06,7.10E-05,2.44E-05
8305,sentiment_analysis38,157,"In many ways , the sentiment unit in this model is just a scaled up example of the same phenomena .",Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,80,1,0,,6.51E-06,0,negative,8.94E-06,1.70E-06,3.62E-05,1.05E-08,7.19E-06,7.94E-06,2.67E-06,6.21E-06,4.55E-06,0.999914653,7.20E-10,9.45E-06,4.77E-07
8306,sentiment_analysis38,158,The update equation of an LSTM could play a role .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,81,1,0,,8.24E-06,0,negative,1.08E-05,1.34E-06,3.70E-06,3.86E-08,1.18E-06,2.79E-05,2.42E-06,5.40E-05,4.68E-06,0.999889218,2.11E-09,3.82E-06,9.67E-07
8307,sentiment_analysis38,159,The element - wise operation of its gates may encourage axis - aligned representations .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,82,1,0,,1.87E-05,0,negative,0.000211063,1.89E-06,3.54E-05,8.88E-09,1.67E-06,6.70E-06,3.47E-06,8.02E-06,7.94E-06,0.999678461,1.02E-09,4.49E-05,4.63E-07
8308,sentiment_analysis38,160,Models such as word2vec have also been observed to have small subsets of dimensions strongly associated with specific tasks .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,83,1,0,,7.09E-05,0,negative,2.01E-05,8.41E-07,1.09E-05,6.92E-08,2.06E-06,6.15E-05,2.61E-05,5.83E-05,7.17E-07,0.999757531,3.27E-08,5.36E-05,8.18E-06
8309,sentiment_analysis38,161,Our work highlights the sensitivity of learned representations to the data distribution they are trained on .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,84,1,0,,0.000191169,0,negative,0.000407337,3.48E-05,5.59E-05,5.07E-07,4.11E-05,3.05E-05,1.71E-05,3.87E-05,2.52E-05,0.999225809,1.31E-08,0.000115825,7.29E-06
8310,sentiment_analysis38,162,"The results make clear that it is unrealistic to expect a model trained on a corpus of books , where the two most common genres are Romance and Fantasy , to learn an encoding which preserves the exact sentiment of a review .",Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,85,1,0,,0.000810513,0,negative,0.001231669,3.99E-06,1.25E-05,1.23E-08,4.32E-06,9.43E-06,4.80E-05,1.19E-05,1.28E-06,0.997020779,5.64E-09,0.001653749,2.38E-06
8311,sentiment_analysis38,163,"Likewise , it is unrealistic to expect a model trained on Amazon product reviews to represent the precise semantic content of a caption of an image or a video .",Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,86,1,0,,2.05E-05,0,negative,5.70E-06,7.48E-07,6.53E-06,1.03E-07,2.58E-06,1.87E-05,6.16E-06,1.71E-05,4.40E-07,0.999904806,6.78E-08,3.36E-05,3.52E-06
8312,sentiment_analysis38,164,There are several promising directions for future work highlighted by our results .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,87,1,0,,0.000103617,0,negative,3.16E-05,1.20E-06,7.40E-06,1.74E-07,4.58E-06,0.000151947,2.81E-05,0.000100916,2.07E-06,0.999628585,6.81E-09,3.62E-05,7.20E-06
8313,sentiment_analysis38,165,"The observed performance plateau , even on relatively similar domains , suggests improving the representation model both in terms of architecture and size .",Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,88,1,0,,0.004196452,0,negative,0.003173613,3.02E-06,4.46E-05,1.58E-07,1.34E-05,3.26E-05,8.03E-05,2.19E-05,2.30E-06,0.994834116,4.52E-09,0.001781774,1.23E-05
8314,sentiment_analysis38,166,"Since our model operates at the byte - level , hierarchical / multi-timescale extensions could improve the quality of representations for longer documents .",Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,89,1,0,,0.000576293,0,negative,0.000230362,2.48E-06,1.58E-05,1.38E-07,4.25E-06,8.27E-05,5.02E-05,6.55E-05,3.69E-06,0.999303479,7.28E-09,0.000222322,1.91E-05
8315,sentiment_analysis38,167,The sensitivity of learned representations to their training domain could be addressed by training on a wider mix of datasets with better coverage of target tasks .,Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,90,1,0,,7.57E-05,0,negative,2.25E-05,2.39E-06,2.92E-06,5.79E-08,1.95E-06,5.69E-05,1.51E-05,0.000119477,3.06E-06,0.999751277,2.99E-09,2.01E-05,4.26E-06
8316,sentiment_analysis38,168,"Finally , our work encourages further research into language modelling as it demonstrates that the standard language modelling objective with no modifications is sufficient to learn high - quality representations .",Experimental Setup and Results,Discussion and Future Work,sentiment_analysis,38,91,1,0,,0.003979267,0,negative,0.00016619,2.33E-06,7.83E-06,3.18E-07,8.81E-06,0.000130355,0.000118747,0.0001069,1.16E-06,0.999094315,1.50E-08,0.000336,2.70E-05
8317,natural_language_inference57,1,title,,,natural_language_inference,57,1,1,0,,0.000701755,0,negative,8.67E-05,0.000310204,3.41E-06,0.00010693,6.41E-06,0.000664696,5.35E-05,0.005762623,0.000365664,0.991721448,0.000883565,2.01E-05,1.47E-05
8318,natural_language_inference57,2,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,,,natural_language_inference,57,2,1,1,research-problem,0.036335699,0,research-problem,1.41E-05,0.000183458,3.34E-05,2.08E-06,6.06E-07,6.72E-05,0.000418316,0.000961372,0.000105684,0.301238482,0.69590857,0.001035814,3.09E-05
8319,natural_language_inference57,3,abstract,,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,natural_language_inference,57,3,1,0,,0.004275135,0,negative,0.000172564,0.000566819,6.30E-06,0.000186221,1.07E-05,0.001072161,0.000114264,0.009973721,0.000790861,0.985126726,0.001897692,4.59E-05,3.61E-05
8320,natural_language_inference57,4,"Hypernymy , textual entailment , and image captioning can be seen as special cases of a single visual - semantic hierarchy over words , sentences , and images .",,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,natural_language_inference,57,4,1,1,research-problem,0.012102575,0,negative,3.52E-05,0.00017502,1.50E-05,2.33E-05,9.48E-06,4.42E-05,0.000258292,0.000260594,2.37E-05,0.510341594,0.488539403,0.000241016,3.33E-05
8321,natural_language_inference57,5,In this paper we advocate for explicitly modeling the partial order structure of this hierarchy .,,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,natural_language_inference,57,5,1,0,,0.055303521,0,negative,0.000733302,0.036367432,0.00111966,4.85E-06,3.43E-05,0.000132982,5.86E-05,0.002401321,0.01432459,0.937453482,0.006302115,0.001052402,1.50E-05
8322,natural_language_inference57,6,"Towards this goal , we introduce a general method for learning ordered representations , and show how it can be applied to a variety of tasks involving images and language .",,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,natural_language_inference,57,6,1,0,,0.087929789,0,negative,0.000430108,0.043766735,0.000202357,5.28E-05,0.000131854,0.000316355,0.000247198,0.005935615,0.005453091,0.923459151,0.018190835,0.001749158,6.48E-05
8323,natural_language_inference57,7,We show that the resulting representations improve performance over current approaches for hypernym prediction and image - caption retrieval .,,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,natural_language_inference,57,7,1,0,,0.090029084,0,negative,0.002036019,0.002284611,0.000118383,1.80E-06,1.28E-05,5.40E-05,0.000234535,0.001158408,0.000188969,0.91835616,0.001724109,0.073812611,1.76E-05
8324,natural_language_inference57,8,INTRODUCTION,,,natural_language_inference,57,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
8325,natural_language_inference57,9,Computer vision and natural language processing are becoming increasingly intertwined .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,1,1,0,,0.126555908,0,research-problem,7.32E-07,0.000192214,3.14E-07,3.65E-06,8.16E-06,3.79E-06,8.51E-06,5.53E-06,9.37E-05,0.040211828,0.959468981,1.29E-06,1.31E-06
8326,natural_language_inference57,10,"Recent work in vision has moved beyond discriminating between a fixed set of object classes , to automatically generating open - ended lingual descriptions of images .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,2,1,0,,0.64717644,1,research-problem,7.14E-07,0.000139758,2.12E-07,5.20E-06,8.28E-06,5.41E-06,6.23E-06,5.83E-06,5.70E-05,0.071192407,0.928577199,7.63E-07,9.63E-07
8327,natural_language_inference57,11,Recent methods for natural language processing such as learn the semantics of language by grounding it in the visual world .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,3,1,0,,0.27430983,0,research-problem,3.94E-07,0.000152924,2.64E-07,7.62E-07,2.66E-06,2.34E-06,5.85E-06,4.19E-06,7.55E-05,0.044080282,0.95567336,8.16E-07,6.20E-07
8328,natural_language_inference57,12,"Looking to the future , autonomous artificial agents will need to jointly model vision and language in order to parse the visual world and communicate with people .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,4,1,0,,0.075224512,0,research-problem,4.12E-06,0.001423276,1.41E-06,3.77E-05,9.00E-05,8.51E-05,2.99E-05,8.21E-05,0.002515894,0.464374532,0.531346629,4.73E-06,4.73E-06
8329,natural_language_inference57,13,"But what , precisely , is the relationship between images and the words or captions we use to describe them ?",INTRODUCTION,INTRODUCTION,natural_language_inference,57,5,1,0,,0.003368546,0,negative,2.38E-06,0.002891896,1.04E-06,1.96E-05,5.33E-05,0.000113814,1.65E-05,0.0001705,0.005257452,0.74434023,0.247129486,1.68E-06,2.17E-06
8330,natural_language_inference57,14,"It is akin to the hypernym relation between words , and textual entailment among phrases : captions are simply abstractions of images .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,6,1,0,,0.004286055,0,negative,1.36E-05,0.001815976,7.98E-06,0.00060751,0.004807013,0.000239453,3.38E-05,3.48E-05,0.001109811,0.943411318,0.047910583,3.05E-06,5.14E-06
8331,natural_language_inference57,15,"In fact , all three relations can be seen as special cases of a partial order over images and language , illustrated in , which we refer to as the visualsemantic hierarchy .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,7,1,1,research-problem,0.148633686,0,negative,2.06E-05,0.047959188,4.43E-05,7.33E-06,0.00041355,6.54E-05,2.10E-05,7.50E-05,0.445367329,0.472023921,0.033991681,8.72E-06,1.87E-06
8332,natural_language_inference57,16,"As a partial order , this relation is transitive : "" woman walking her dog "" , "" woman walking "" , "" person walking "" , "" person "" , and "" entity "" are all valid abstractions of the rightmost image .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,8,1,0,,0.129710647,0,negative,0.000139145,0.063328984,6.17E-05,4.06E-05,0.001995715,0.000147103,5.81E-05,0.000196022,0.139814455,0.772533143,0.021654997,2.53E-05,4.68E-06
8333,natural_language_inference57,17,Our goal in this work is to learn representations that respect this partial order structure .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,9,1,0,,0.317322456,0,research-problem,9.85E-06,0.078666581,1.10E-05,2.67E-05,0.0003083,7.41E-05,3.81E-05,0.000204216,0.046591153,0.296122234,0.577931377,9.92E-06,6.45E-06
8334,natural_language_inference57,18,"Most recent approaches to modeling the hypernym , entailment , and image - caption relations involve learning distributed representations or embeddings .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,10,1,0,,0.784380645,1,research-problem,2.13E-06,0.001433854,1.20E-06,1.81E-05,3.48E-05,3.07E-05,2.67E-05,4.82E-05,0.000379457,0.153881681,0.84413659,2.62E-06,3.94E-06
8335,natural_language_inference57,19,"This is a very powerful and general approach which maps the objects of interest - words , phrases , images - to points in a high - dimensional vector space .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,11,1,0,,0.90932557,1,model,9.00E-05,0.272441382,0.000644209,3.19E-05,0.001683884,0.000130084,0.000126366,0.000131346,0.471256418,0.196097096,0.057311466,4.14E-05,1.44E-05
8336,natural_language_inference57,20,"One line of work , exemplified by and first applied to the caption - image relationship by , requires the mapping to be distance - preserving : semantically 1 ar Xiv : 1511.06361v6LG ] 1 Mar 2016",INTRODUCTION,INTRODUCTION,natural_language_inference,57,12,1,0,,0.128068639,0,negative,1.45E-05,0.011815796,9.48E-06,2.78E-05,0.000137582,9.72E-05,5.35E-05,0.000152946,0.010371605,0.532336445,0.444964909,1.23E-05,5.96E-06
8337,natural_language_inference57,21,Published as a conference paper at ICLR 2016 similar objects are mapped to points thatare nearby in the embedding space .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,13,1,0,,0.596963552,1,negative,4.09E-05,0.033106724,7.05E-05,0.00032623,0.001920306,0.00027047,0.000182411,0.000192154,0.025594001,0.562277275,0.375958757,2.93E-05,3.10E-05
8338,natural_language_inference57,22,A symmetric distance measure such as Euclidean or cosine distance is typically used .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,14,1,0,,0.001652255,0,negative,1.29E-05,0.01433022,1.27E-05,2.28E-05,0.000229583,0.000405358,7.87E-05,0.00075315,0.018630625,0.918237141,0.047267483,1.16E-05,7.85E-06
8339,natural_language_inference57,23,"Since the visual - semantic hierarchy is an antisymmetric relation , we expect this approach to introduce systematic model error .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,15,1,0,,0.020487878,0,negative,0.000592018,0.019592278,1.66E-05,3.29E-05,0.001982712,0.000131251,7.01E-05,0.000164447,0.008785113,0.964953984,0.003614476,6.03E-05,3.68E-06
8340,natural_language_inference57,24,"Other approaches do not have such explicit constraints , learning a more - or - less general binary relation between the objects of interest , e.g. ; ; .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,16,1,0,,0.002930569,0,negative,9.36E-06,0.009077384,9.86E-06,3.81E-05,0.000217346,0.000176644,6.44E-05,0.000202856,0.007999444,0.78541746,0.196768969,1.00E-05,8.13E-06
8341,natural_language_inference57,25,"Notably , no existing approach directly imposes the transitivity and antisymmetry of the partial order , leaving the model to induce these properties from data .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,17,1,0,,0.013671594,0,negative,9.97E-06,0.009265531,4.15E-06,0.000104224,0.000602501,0.000128299,3.07E-05,0.000160114,0.004930723,0.918080913,0.066672606,5.88E-06,4.41E-06
8342,natural_language_inference57,26,"In contrast , we propose to exploit the partial order structure of the visual - semantic hierarchy by learning a mapping which is not distance - preserving but order - preserving between the visualsemantic hierarchy and a partial order over the embedding space .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,18,1,1,model,0.930475779,1,model,4.63E-05,0.360321327,7.68E-05,2.68E-06,0.000356497,2.75E-05,2.32E-05,7.61E-05,0.615053049,0.022710194,0.001293904,1.02E-05,2.34E-06
8343,natural_language_inference57,27,We call embeddings learned in this way order- embeddings .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,19,1,1,model,0.399834944,0,model,8.81E-06,0.131153832,4.52E-05,5.11E-06,0.000478237,0.000164156,3.22E-05,0.000374038,0.703778879,0.162980074,0.000972354,3.82E-06,3.20E-06
8344,natural_language_inference57,28,This idea can be integrated into existing relational learning methods simply by replacing their comparison operation with ours .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,20,1,0,,0.041333163,0,negative,1.84E-05,0.056187031,1.72E-05,7.05E-06,0.000234819,0.000140787,3.13E-05,0.000280141,0.368077723,0.567810228,0.007183227,8.90E-06,3.11E-06
8345,natural_language_inference57,29,"By modifying existing methods in this way , we find that order - embeddings provide a marked improvement over the state - of - art for hypernymy prediction and caption - image retrieval , and near state - of - the - art performance for natural language inference .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,21,1,0,,0.104158965,0,negative,0.008583203,0.366185681,0.00020426,0.000260544,0.009038333,0.001080761,0.011683896,0.003167156,0.048127359,0.537751039,0.007180656,0.006392256,0.000344856
8346,natural_language_inference57,30,This paper is structured as follows .,INTRODUCTION,INTRODUCTION,natural_language_inference,57,22,1,0,,0.000685365,0,negative,9.54E-06,0.006386752,6.01E-06,0.000866179,0.003077111,0.001082074,8.64E-05,0.000770681,0.004814089,0.979298255,0.003586087,3.37E-06,1.35E-05
8347,natural_language_inference57,31,"We begin , in Section 2 , by giving a unified mathematical treatment of our tasks , and describing the general approach of learning order- embeddings .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,23,1,0,,0.010532912,0,negative,2.26E-05,0.057549613,1.42E-05,0.000720703,0.015185062,0.000899426,0.000113181,0.000821849,0.018348368,0.904129242,0.002167323,1.19E-05,1.65E-05
8348,natural_language_inference57,32,"In the next three sections we describe in detail the tasks we tackle , how we apply the order - embeddings idea to each of them , and the results we obtain .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,24,1,0,,0.015873844,0,negative,1.12E-05,0.02458999,5.71E-06,7.01E-05,0.007961492,0.000257928,7.06E-05,0.000213489,0.00804398,0.957624146,0.00113499,1.10E-05,5.41E-06
8349,natural_language_inference57,33,"The tasks are hypernym prediction ( Section 3 ) , captionimage retrieval ( Section 4 ) , and textual entailment ( Section 5 ) .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,25,1,0,,0.048411421,0,negative,3.11E-05,0.022530693,7.84E-05,0.006794924,0.220257401,0.001796725,0.001546472,0.000364712,0.001029472,0.722560996,0.022794827,4.72E-05,0.000167024
8350,natural_language_inference57,34,"In the supplementary material , we visualize novel vector regularities that emerge in our learned embeddings of images and language .",INTRODUCTION,INTRODUCTION,natural_language_inference,57,26,1,0,,0.319260171,0,negative,9.26E-05,0.022991067,1.84E-05,0.00190786,0.034523864,0.001851496,0.00024683,0.00088622,0.006296328,0.930798344,0.000344887,2.22E-05,1.99E-05
8351,natural_language_inference57,35,LEARNING ORDER - EMBEDDINGS,INTRODUCTION,,natural_language_inference,57,27,1,0,,0.078730738,0,negative,2.34E-05,0.076186207,3.65E-05,5.07E-06,7.00E-05,0.000262478,0.000630089,0.000872258,0.385563709,0.424731489,0.111494493,8.97E-05,3.46E-05
8352,natural_language_inference57,36,"To unify our treatment of various tasks , we introduce the problem of partial order completion .",INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,28,1,0,,0.000298233,0,negative,5.79E-05,0.00225608,0.000122723,7.53E-06,5.65E-05,8.90E-05,0.000129327,0.000172991,0.003310307,0.991000718,0.001911986,0.000863307,2.16E-05
8353,natural_language_inference57,37,"In partial order completion , we are given a set of positive examples P = { ( u , v ) } of ordered pairs drawn from a partially ordered set ( X , X ) , and a set of negative examples N which we know to be unordered .",INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,29,1,0,,2.56E-05,0,negative,5.28E-06,0.000363205,2.43E-05,2.17E-06,9.77E-06,4.25E-05,3.24E-05,0.000151087,0.000940982,0.997550266,0.000711615,0.000159268,7.22E-06
8354,natural_language_inference57,38,"Our goal is to predict whether an unseen pair ( u , v ) is ordered .",INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,30,1,0,,2.63E-05,0,negative,3.43E-06,0.000154324,7.53E-06,1.43E-06,4.23E-06,3.20E-05,1.45E-05,0.000133416,0.000689938,0.998485601,0.0004093,6.00E-05,4.30E-06
8355,natural_language_inference57,39,"Note that hypernym prediction , caption - image retrieval , and textual entailment are all special cases of this task , since they all involve classifying pairs of concepts in the ( partially ordered ) visual - semantic hierarchy .",INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,31,1,0,,6.09E-05,0,negative,2.18E-06,1.04E-05,2.58E-06,5.64E-06,4.04E-06,3.17E-05,1.78E-05,2.69E-05,2.44E-05,0.998742561,0.001084541,4.26E-05,4.58E-06
8356,natural_language_inference57,40,"We tackle this problem by learning a mapping from X into a partially ordered embedding space ( Y , Y ) .",INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,32,1,0,,0.000141149,0,negative,8.05E-05,0.003260777,0.000136026,1.29E-06,2.86E-05,2.68E-05,2.67E-05,0.00011173,0.009078467,0.986758922,9.19E-05,0.000394209,4.10E-06
8357,natural_language_inference57,41,The idea is to predict the ordering of an unseen pair in X based on its ordering in the embedding space .,INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,33,1,0,,7.95E-05,0,negative,1.65E-05,0.000510902,0.000112376,1.55E-07,5.75E-06,8.63E-06,4.45E-06,2.28E-05,0.009351672,0.989921736,1.11E-05,3.31E-05,8.66E-07
8358,natural_language_inference57,42,This is possible only if the mapping satisfies the following crucial property :,INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,34,1,0,,2.14E-05,0,negative,1.12E-05,2.72E-05,8.43E-06,5.70E-08,9.53E-07,3.59E-06,3.39E-06,1.21E-05,0.000276982,0.999588365,4.23E-06,6.33E-05,2.02E-07
8359,natural_language_inference57,43,"This definition implies that each combination of embedding space Y , order Y , and orderembedding f determines a unique completion of our data as a partial order X .",INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,35,1,0,,9.14E-06,0,negative,7.65E-06,1.40E-05,7.23E-06,3.20E-08,1.15E-06,2.61E-06,1.30E-06,5.58E-06,0.000168105,0.999768506,3.13E-07,2.35E-05,5.89E-08
8360,natural_language_inference57,44,"In the following , we first consider the choice of Y and Y , and then discuss how to find an appropriate f .",INTRODUCTION,LEARNING ORDER - EMBEDDINGS,natural_language_inference,57,36,1,0,,6.87E-06,0,negative,6.54E-06,3.01E-05,1.61E-06,1.17E-07,1.72E-06,6.31E-06,2.09E-06,2.43E-05,0.000132238,0.999765964,6.97E-07,2.82E-05,1.06E-07
8361,natural_language_inference57,45,THE REVERSED PRODUCT ORDER,INTRODUCTION,,natural_language_inference,57,37,1,0,,0.15463353,0,model,6.69E-06,0.027387626,6.79E-05,2.68E-07,1.77E-05,4.67E-05,0.000181862,8.76E-05,0.816107444,0.148099778,0.007972271,1.91E-05,5.03E-06
8362,natural_language_inference57,46,The choice of Y and Y is somewhat application - dependent .,INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,38,1,0,,2.64E-06,0,negative,1.28E-05,2.11E-06,1.28E-06,2.17E-07,2.13E-07,1.48E-06,1.01E-06,2.35E-05,1.27E-05,0.999836469,3.69E-07,0.000107474,3.53E-07
8363,natural_language_inference57,47,"For the purpose of modeling the semantic hierarchy , our choices are narrowed by the following considerations .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,39,1,0,,9.47E-06,0,negative,0.000106495,7.26E-06,9.34E-06,2.39E-07,9.66E-07,1.14E-06,9.65E-07,1.14E-05,3.35E-05,0.999616979,1.06E-07,0.000211324,2.76E-07
8364,natural_language_inference57,48,Much of the expressive power of human language comes from abstraction and composition .,INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,40,1,0,,0.000101501,0,negative,3.57E-05,7.20E-06,1.44E-05,1.41E-05,2.31E-06,2.10E-05,2.40E-05,5.45E-05,5.83E-05,0.999134173,8.39E-05,0.00050278,4.76E-05
8365,natural_language_inference57,49,"For any two concepts , say "" dog "" and "" cat "" , we can name a concept that is an abstraction of the two , such as "" mammal "" , as well as a concept that composes the two , such as "" dog chasing cat "" .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,41,1,0,,3.05E-06,0,negative,6.37E-07,1.00E-06,8.93E-07,2.93E-08,9.62E-08,1.00E-06,3.89E-07,1.80E-05,2.37E-05,0.99994301,6.70E-08,1.09E-05,2.16E-07
8366,natural_language_inference57,50,"So , in order to represent the visual - semantic hierarchy , we need to choose an order Y that is rich enough to embed these two relations .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,42,1,0,,5.36E-05,0,negative,5.29E-05,7.12E-06,4.68E-06,5.20E-08,3.61E-07,6.45E-07,1.23E-06,1.37E-05,5.39E-05,0.999519445,1.82E-07,0.000345452,3.05E-07
8367,natural_language_inference57,51,"We also restrict ourselves to orders Y with atop element , which is above every other element in the order .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,43,1,0,,1.39E-05,0,negative,9.85E-06,2.67E-05,1.25E-05,2.62E-08,3.25E-07,5.82E-07,9.33E-07,1.52E-05,0.000247386,0.999627472,1.73E-07,5.86E-05,2.62E-07
8368,natural_language_inference57,52,"In the visual - semantic hierarchy , this element represents the most general possible concept ; practically , it provides an anchor for the embedding .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,44,1,0,,2.43E-05,0,negative,4.65E-06,2.61E-06,7.46E-06,9.91E-09,1.24E-07,2.92E-07,2.86E-07,6.04E-06,0.000189636,0.999772096,3.68E-08,1.66E-05,1.88E-07
8369,natural_language_inference57,53,"Finally , we choose the embedding space Y to be continuous in order to allow optimization with gradient - based methods .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,45,1,0,,0.002955741,0,negative,0.000213316,0.000204397,2.14E-05,1.09E-06,4.88E-06,2.44E-05,2.72E-05,0.001531514,0.000663906,0.997018196,1.68E-07,0.000284254,5.18E-06
8370,natural_language_inference57,54,"A natural choice that satisfies all three properties is the reversed product order on RN + , defined by the conjunction of total orders on each coordinate :",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,46,1,0,,2.08E-05,0,negative,1.27E-05,6.89E-06,5.53E-05,9.41E-09,1.77E-07,3.25E-07,1.06E-06,3.25E-06,9.14E-05,0.999687661,1.58E-07,0.000140892,1.77E-07
8371,natural_language_inference57,55,"for all vectors x , y with nonnegative coordinates .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,47,1,0,,8.52E-07,0,negative,1.25E-06,5.74E-07,6.99E-07,4.28E-09,2.35E-08,1.69E-07,1.91E-07,5.18E-06,1.09E-05,0.999961928,2.65E-08,1.90E-05,6.55E-08
8372,natural_language_inference57,56,Note the reversal of direction : smaller coordinates imply higher position in the partial order .,INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,48,1,0,,8.77E-06,0,negative,4.28E-06,5.07E-07,2.15E-06,3.34E-08,6.36E-08,6.90E-07,7.36E-07,1.02E-05,2.27E-05,0.999933611,3.07E-08,2.47E-05,2.91E-07
8373,natural_language_inference57,57,"The origin is then the top element of the order , representing the most general concept .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,49,1,0,,5.16E-06,0,negative,1.79E-06,9.41E-07,2.35E-06,1.19E-08,8.91E-08,3.78E-07,3.43E-07,8.30E-06,6.95E-05,0.999908022,1.66E-08,8.08E-06,1.66E-07
8374,natural_language_inference57,58,Instead of viewing our embeddings as single points x ?,INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,50,1,0,,1.98E-06,0,negative,1.08E-06,1.36E-06,1.00E-06,1.06E-08,3.93E-08,2.47E-07,2.36E-07,6.25E-06,2.67E-05,0.999948103,6.95E-08,1.48E-05,1.35E-07
8375,natural_language_inference57,59,"RN + , we can also view them as sets {y : x y}.",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,51,1,0,,4.52E-06,0,negative,7.66E-06,3.32E-06,1.43E-05,1.07E-08,1.35E-07,4.45E-07,1.07E-06,6.38E-06,3.46E-05,0.999746364,1.61E-07,0.000185248,3.01E-07
8376,natural_language_inference57,60,"The meaning of a word is then the union of all concepts of which it is a hypernym , and the meaning of a sentence is the union of all sentences that entail it .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,52,1,0,,3.53E-06,0,negative,9.10E-07,1.56E-06,2.26E-06,2.02E-08,1.29E-07,4.86E-07,3.27E-07,8.99E-06,5.34E-05,0.999922934,7.66E-08,8.43E-06,4.43E-07
8377,natural_language_inference57,61,"The visual - semantic hierarchy can then be seen as a special case of the subset relation , a connection also used by .",INTRODUCTION,THE REVERSED PRODUCT ORDER,natural_language_inference,57,53,1,0,,2.17E-06,0,negative,2.15E-06,1.21E-06,7.40E-06,1.49E-08,9.86E-08,3.33E-07,4.18E-07,3.43E-06,0.000105494,0.99986343,1.22E-07,1.56E-05,3.39E-07
8378,natural_language_inference57,62,PENALIZING ORDER VIOLATIONS,INTRODUCTION,,natural_language_inference,57,54,1,0,,0.127816341,0,negative,4.12E-05,0.032040426,1.73E-05,4.50E-06,0.000154699,0.000206368,0.001678193,0.000903851,0.048529626,0.914168535,0.002044161,0.000168452,4.26E-05
8379,natural_language_inference57,63,"Having fixed the embedding space and order , we now consider the problem of finding an orderembedding into this space .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,55,1,0,,5.27E-05,0,negative,3.54E-06,0.000138263,3.56E-06,7.11E-08,3.07E-06,3.98E-06,6.17E-06,1.49E-05,0.001804563,0.997996841,3.12E-06,2.14E-05,5.08E-07
8380,natural_language_inference57,64,"In practice , the order embedding condition ( Definition 1 ) is too restrictive to impose as a hard constraint .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,56,1,0,,6.38E-05,0,negative,1.22E-05,1.16E-05,3.69E-07,1.35E-07,3.15E-06,3.71E-06,3.83E-06,1.35E-05,4.14E-05,0.999887385,3.29E-07,2.21E-05,2.54E-07
8381,natural_language_inference57,65,"Instead , we aim to find an approximate order - embedding : a mapping which violates the order - embedding condition , imposed as a soft constraint , as little as possible .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,57,1,0,,0.004657502,0,negative,8.77E-05,0.004765465,6.10E-05,4.48E-07,7.01E-05,1.17E-05,2.48E-05,4.72E-05,0.019719047,0.975114821,2.06E-06,9.36E-05,2.10E-06
8382,natural_language_inference57,66,"More precisely , we define a penalty that measures the degree to which a pair of points violates the product order .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,58,1,0,,0.000432172,0,negative,6.34E-05,0.002033822,4.36E-05,8.82E-08,1.84E-05,5.85E-06,1.46E-05,2.98E-05,0.068236521,0.929512217,6.71E-07,4.01E-05,9.22E-07
8383,natural_language_inference57,67,"In particular , we define the penalty for an ordered pair ( x , y) of points in RN + as",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,59,1,0,,1.69E-05,0,negative,1.05E-05,7.16E-05,5.26E-06,2.94E-08,3.85E-06,2.17E-06,3.32E-06,8.84E-06,0.000812316,0.999066505,1.03E-07,1.54E-05,1.50E-07
8384,natural_language_inference57,68,"Crucially , E ( x , y ) = 0 ??",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,60,1,0,,2.68E-06,0,negative,8.30E-06,6.16E-06,1.61E-06,1.07E-08,1.98E-06,1.20E-06,2.98E-06,2.27E-06,2.90E-05,0.999918874,3.63E-08,2.75E-05,5.33E-08
8385,natural_language_inference57,69,"x y according to the reversed product order ; if the order is not satisfied , E ( x , y) is positive .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,61,1,0,,8.51E-06,0,negative,3.66E-06,6.49E-06,2.08E-06,3.45E-09,9.20E-07,4.07E-07,1.00E-06,9.80E-07,0.000102644,0.99987133,2.67E-08,1.04E-05,2.88E-08
8386,natural_language_inference57,70,"This effectively imposes a strong prior on the space of relations , encouraging our learned relation to satisfy the partial order properties of transitivity and antisymmetry .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,62,1,0,,0.00012023,0,negative,7.54E-05,0.000220526,2.53E-05,9.33E-08,2.02E-05,4.19E-06,7.76E-06,1.35E-05,0.00396817,0.995639669,3.32E-08,2.48E-05,4.20E-07
8387,natural_language_inference57,71,This penalty is key to our method .,INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,63,1,0,,0.000199749,0,negative,5.27E-05,0.000219455,6.29E-06,6.88E-07,2.64E-05,2.19E-05,1.58E-05,0.00011314,0.001941198,0.997576824,1.04E-07,2.39E-05,1.66E-06
8388,natural_language_inference57,72,"Throughout the remainder of the paper , we will use it where previous work has used symmetric distances or learned comparison operators .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,64,1,0,,9.52E-06,0,negative,8.37E-07,2.82E-05,1.62E-06,3.07E-07,8.97E-06,2.63E-05,1.06E-05,5.63E-05,0.000108611,0.99975316,9.40E-08,4.26E-06,6.78E-07
8389,natural_language_inference57,73,"Recall that P and N are our positive and negative examples , respectively .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,65,1,0,,4.50E-06,0,negative,2.10E-07,2.35E-06,1.58E-07,3.26E-09,4.38E-07,1.09E-06,7.48E-07,5.16E-06,4.05E-05,0.999948327,6.59E-09,9.57E-07,2.63E-08
8390,natural_language_inference57,74,"Then , to learn an approximate order - embedding f , we could use a max - margin loss which encourages positive examples to have zero penalty , and negative examples to have penalty greater than a margin :",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,66,1,0,,2.91E-05,0,negative,1.33E-05,5.01E-05,4.60E-06,1.99E-08,4.67E-06,2.58E-06,4.90E-06,1.12E-05,0.000304401,0.999590161,1.96E-08,1.38E-05,1.71E-07
8391,natural_language_inference57,75,"In practice we are often not given negative examples , in which case this loss admits the trivial solution of mapping all objects to the same point .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,67,1,0,,1.81E-05,0,negative,1.97E-06,4.90E-06,6.14E-07,6.06E-08,1.77E-06,1.72E-06,2.35E-06,3.82E-06,1.87E-05,0.999951237,2.78E-07,1.23E-05,2.23E-07
8392,natural_language_inference57,76,"The best way of dealing with this problem depends on the application , so we will describe task - specific variations of the loss in the next several sections .",INTRODUCTION,PENALIZING ORDER VIOLATIONS,natural_language_inference,57,68,1,0,,5.09E-06,0,negative,1.39E-06,6.53E-06,7.38E-07,2.50E-08,1.21E-06,1.57E-06,1.67E-06,3.44E-06,3.92E-05,0.999937792,7.67E-08,6.25E-06,1.10E-07
8393,natural_language_inference57,77,HYPERNYM PREDICTION,INTRODUCTION,,natural_language_inference,57,69,1,1,experiments,0.183454008,0,negative,0.000113458,0.021426711,0.000437075,0.000108781,0.00579977,0.003931131,0.414181433,0.002573799,0.003150661,0.537144506,0.001136543,0.004613153,0.00538298
8394,natural_language_inference57,78,"To test the ability of our model to learn partial orders from incomplete data , our first task is to predict withheld hypernym pairs in Word Net .",INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,70,1,0,,0.000285946,0,negative,3.84E-05,1.73E-05,2.85E-05,9.30E-07,4.49E-05,1.07E-05,0.00027069,8.05E-06,8.92E-06,0.999155115,9.50E-08,0.000369904,4.66E-05
8395,natural_language_inference57,79,"A hypernym pair is a pair of concepts where the first concept is a specialization or an instance of the second , e.g. , ( woman , person ) or ( New York , city ) .",INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,71,1,0,,1.35E-05,0,negative,5.86E-07,8.82E-07,2.55E-06,2.20E-08,4.10E-07,2.34E-06,7.73E-06,4.79E-06,2.85E-05,0.999943133,1.43E-08,3.54E-06,5.50E-06
8396,natural_language_inference57,80,Our setup differs significantly from previous work in that we use only the WordNet hierarchy as training data .,INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,72,1,0,,3.92E-05,0,negative,2.68E-05,1.72E-05,1.98E-05,4.91E-07,1.61E-05,7.00E-06,3.63E-05,7.23E-06,2.50E-05,0.999794743,6.61E-09,4.26E-05,6.78E-06
8397,natural_language_inference57,81,"The most similar evaluation has been that of , who use external linguistic data in the form of distributional semantic vectors .",INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,73,1,0,,1.61E-05,0,negative,1.01E-06,5.61E-08,1.08E-06,5.25E-09,1.25E-07,1.17E-06,1.88E-05,7.08E-07,3.94E-07,0.999955592,1.02E-08,1.92E-05,1.85E-06
8398,natural_language_inference57,82,"and also evaluate on the WordNet hierarchy , but they use other relations in WordNet as training data ( and external linguistic data , in Socher 's case ) .",INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,74,1,0,,2.65E-06,0,negative,2.90E-06,1.98E-07,4.17E-06,1.41E-08,2.97E-07,2.14E-06,2.13E-05,1.50E-06,1.08E-06,0.999947834,7.13E-09,1.60E-05,2.58E-06
8399,natural_language_inference57,83,"Additionally , the latter two consider only direct hypernyms , rather than the full , transitive hypernymy relation .",INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,75,1,0,,4.06E-05,0,negative,3.34E-06,3.95E-07,2.57E-06,2.49E-08,3.21E-07,1.89E-06,1.02E-05,1.90E-06,3.90E-06,0.999961432,1.16E-08,1.10E-05,3.07E-06
8400,natural_language_inference57,84,But predicting the transitive hypernym relation is a better - defined problem because individual hypernym edges in WordNet vary dramatically in the degree of abstraction they require .,INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,76,1,0,,0.000113795,0,negative,1.51E-06,1.84E-07,1.02E-06,2.33E-07,2.73E-07,6.75E-06,3.67E-05,3.19E-06,1.04E-06,0.999897572,5.08E-07,2.27E-05,2.83E-05
8401,natural_language_inference57,85,"For instance , ( person , organism ) is a direct hypernym pair , but it takes eight hypernym edges to get from cat to organism .",INTRODUCTION,HYPERNYM PREDICTION,natural_language_inference,57,77,1,0,,6.06E-06,0,negative,1.10E-06,9.34E-08,1.35E-06,6.20E-09,5.98E-07,8.80E-07,3.89E-06,7.97E-07,1.46E-06,0.999985223,4.36E-10,3.66E-06,9.38E-07
8402,natural_language_inference57,86,LOSS FUNCTION,INTRODUCTION,,natural_language_inference,57,78,1,0,,0.008035489,0,negative,9.49E-05,0.005029448,0.000106605,0.000146427,0.007540181,0.003488222,0.005369035,0.001755447,0.00424573,0.972044753,3.47E-06,4.93E-05,0.000126494
8403,natural_language_inference57,87,"To apply order- embeddings to this task , we again view it as partial order completion - we can infer a hypothesis from a premise exactly when the hypothesis is above the premise in the visual - semantic hierarchy .",INTRODUCTION,LOSS FUNCTION,natural_language_inference,57,79,1,0,,2.21E-05,0,negative,3.86E-06,9.90E-06,5.45E-05,9.88E-09,5.93E-07,6.13E-07,1.68E-06,4.63E-06,3.95E-05,0.999805039,3.48E-08,7.88E-05,7.46E-07
8404,natural_language_inference57,88,"Unlike our other tasks , for which we had to generate contrastive negatives , datasets for natural language inference include labeled negative examples .",INTRODUCTION,LOSS FUNCTION,natural_language_inference,57,80,1,0,,8.06E-06,0,negative,5.90E-07,2.82E-07,1.14E-06,2.54E-07,7.70E-06,3.20E-06,1.67E-06,1.03E-05,1.15E-07,0.999944923,3.73E-09,2.91E-05,7.35E-07
8405,natural_language_inference57,89,"So , we can simply use a max - margin loss :",INTRODUCTION,LOSS FUNCTION,natural_language_inference,57,81,1,0,,6.50E-06,0,negative,4.55E-06,9.09E-07,5.39E-06,5.34E-10,6.48E-08,1.55E-07,4.05E-07,2.67E-06,6.42E-06,0.99996111,3.04E-10,1.83E-05,3.88E-08
8406,natural_language_inference57,90,"where ( p , h ) are positive and ( p , h ) negative pairs of premise and hypothesis .",INTRODUCTION,LOSS FUNCTION,natural_language_inference,57,82,1,0,,6.87E-07,0,negative,1.68E-07,1.24E-07,5.72E-07,6.43E-10,3.09E-08,1.20E-07,6.46E-08,2.52E-06,1.84E-06,0.999993283,1.42E-10,1.25E-06,2.63E-08
8407,natural_language_inference57,91,"To embed sentences , we use the same GRU encoder as in the caption - image retrieval task .",INTRODUCTION,LOSS FUNCTION,natural_language_inference,57,83,1,0,,0.000205759,0,negative,3.54E-05,5.92E-05,0.0002666,3.00E-08,3.49E-06,1.06E-05,3.72E-05,0.000175553,0.000229021,0.999115152,1.45E-09,6.16E-05,6.07E-06
8408,natural_language_inference57,92,DATASET,INTRODUCTION,,natural_language_inference,57,84,1,0,,0.001211512,0,negative,2.92E-06,0.000352771,5.42E-06,2.79E-05,0.000737366,0.002265254,0.001896282,0.000966904,0.000295862,0.993383195,1.29E-06,1.14E-05,5.35E-05
8409,natural_language_inference57,93,"To evaluate order- embeddings on the natural language inference task , we use the recently proposed SNLI corpus , which contains 570,000 pairs of sentences , each labeled with "" entailment "" if the inference is valid , "" contradiction "" if the two sentences contradict , or "" neutral "" if the inference is invalid but there is no contradiction .",INTRODUCTION,DATASET,natural_language_inference,57,85,1,0,,0.715314143,1,dataset,3.30E-06,0.156975074,0.000232276,1.59E-05,0.773119992,0.000906903,0.001116067,0.000239816,0.001027856,0.066347732,3.32E-07,5.77E-06,8.95E-06
8410,natural_language_inference57,94,"Our method only allows us to discriminate between entailment and non-entailment , so we merge the "" contradiction "" and "" neutral "" classes together to serve as our negative examples .",INTRODUCTION,DATASET,natural_language_inference,57,86,1,0,,0.070458094,0,dataset,2.95E-05,0.092922983,0.000248116,6.58E-06,0.528380264,0.000575585,0.00033916,0.000137193,0.004615183,0.372736433,1.61E-07,6.32E-06,2.53E-06
8411,natural_language_inference57,95,DETAILS OF TRAINING,,,natural_language_inference,57,0,1,0,,0.001258626,0,negative,2.68E-05,0.000366579,9.41E-06,1.18E-05,1.13E-05,0.00040927,0.000129767,0.004530016,0.000138197,0.993492284,0.000740259,0.000126992,7.34E-06
8412,natural_language_inference57,96,"To train the model , we use the standard pairwise ranking objective from Eq. ( 5 ) .",DETAILS OF TRAINING,DETAILS OF TRAINING,natural_language_inference,57,1,1,1,experiments,0.075305549,0,hyperparameters,3.01E-05,0.00052853,0.000701773,1.68E-06,1.62E-06,0.195023298,0.001924507,0.595334395,0.000543309,0.205828854,4.16E-05,2.68E-05,1.36E-05
8413,natural_language_inference57,97,"We sample minibatches of 128 random image - caption pairs , and draw all contrastive terms from the minibatch , giving us 127 contrastive images for each caption and captions for each image .",DETAILS OF TRAINING,DETAILS OF TRAINING,natural_language_inference,57,2,1,1,experiments,0.239090614,0,experimental-setup,1.26E-05,0.000113632,5.31E-05,8.88E-06,1.05E-05,0.484840994,0.005840432,0.38159326,4.86E-05,0.127412703,1.46E-05,2.20E-05,2.87E-05
8414,natural_language_inference57,98,"We train for 15 - 30 epochs using the Adam optimizer with learning rate 0.001 , and early stopping on the validation set .",DETAILS OF TRAINING,DETAILS OF TRAINING,natural_language_inference,57,3,1,1,experiments,0.99481672,1,experimental-setup,6.88E-06,1.60E-05,8.61E-06,8.34E-06,1.04E-06,0.594202696,0.002831161,0.395733931,1.08E-05,0.007152512,2.98E-06,4.94E-06,2.01E-05
8415,natural_language_inference57,99,"We set the dimension of the embedding space and the GRU hidden state N to 1024 , the dimension of the learned word embeddings to 300 , and the margin ? to 0.05 .",DETAILS OF TRAINING,DETAILS OF TRAINING,natural_language_inference,57,4,1,1,experiments,0.993242718,1,hyperparameters,4.81E-06,1.38E-05,5.80E-06,1.90E-06,4.44E-07,0.454337283,0.002278817,0.535113772,9.85E-06,0.008208626,4.50E-06,7.09E-06,1.33E-05
8416,natural_language_inference57,100,"All these hyperparameters , as well as the learning rate and batchsize , were selected using the validation set .",DETAILS OF TRAINING,DETAILS OF TRAINING,natural_language_inference,57,5,1,0,,0.908680513,1,hyperparameters,1.03E-05,3.55E-05,7.23E-06,2.41E-06,1.42E-06,0.384271978,0.001087644,0.555831647,2.25E-05,0.058708542,3.48E-06,1.03E-05,7.00E-06
8417,natural_language_inference57,101,"For consistency with and to mitigate overfitting , we constrain the caption and image embeddings to have unit L2 norm .",DETAILS OF TRAINING,DETAILS OF TRAINING,natural_language_inference,57,6,1,1,experiments,0.86208163,1,hyperparameters,0.000106504,0.000395517,0.000130941,8.47E-06,6.96E-06,0.342891905,0.002431841,0.534130737,0.000256978,0.119559309,6.80E-06,4.95E-05,2.45E-05
8418,natural_language_inference57,102,"This constraint implies that no two points can be exactly ordered with zero order- violation penalty , but since we use a ranking loss , only the relative size of the penalties matters .",DETAILS OF TRAINING,DETAILS OF TRAINING,natural_language_inference,57,7,1,0,,0.008762549,0,negative,0.000749589,0.000224031,0.001848314,4.85E-06,1.49E-05,0.023605244,0.000848614,0.019307389,0.00042851,0.952482972,1.75E-05,0.000449777,1.83E-05
8419,natural_language_inference57,103,RESULTS,,,natural_language_inference,57,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
8420,natural_language_inference57,104,The state - of - the - art method for 3 - class classification on SNLI is that of .,RESULTS,RESULTS,natural_language_inference,57,1,1,0,,0.594659004,1,negative,0.003405781,0.00011994,0.00564587,0.000223192,2.94E-05,0.000782894,0.012603805,0.001719483,8.29E-05,0.790572621,0.090613202,0.082219676,0.011981195
8421,natural_language_inference57,105,"Unfortunately , they do not compute 2 - class accuracy , so we can not compare to them directly .",RESULTS,RESULTS,natural_language_inference,57,2,1,0,,0.003825802,0,negative,0.001648744,2.90E-05,0.000834471,5.30E-05,1.01E-05,0.000660647,0.00174915,0.00171704,6.11E-05,0.958881099,0.000911789,0.03234384,0.001100073
8422,natural_language_inference57,106,"As abridge to facilitate comparison , we use a challenging baseline which can be evaluated on both the 2 - class and 3 - class problems .",RESULTS,RESULTS,natural_language_inference,57,3,1,0,,0.010711959,0,negative,0.003361521,0.000153788,0.012546429,9.97E-06,1.31E-05,0.000194688,0.001495183,0.001010104,0.000101404,0.945660459,5.50E-05,0.035210008,0.000188361
8423,natural_language_inference57,107,"The baseline , referred to as skip - thoughts , involves a feedforward neural network on top of skip - thought vectors , a state - of - the - art semantic representation of sentences .",RESULTS,RESULTS,natural_language_inference,57,4,1,0,,0.961948404,1,baselines,0.003496291,0.000166746,0.852981326,2.05E-05,1.00E-05,0.000215437,0.002361228,0.000499119,0.000313165,0.125020378,0.000468936,0.013413671,0.001033243
8424,natural_language_inference57,108,"Given pairs of sentence vectors u and v , the input to the network is the concatenation of u , v and the absolute difference |u ? v|.",RESULTS,RESULTS,natural_language_inference,57,5,1,0,,0.018871433,0,negative,0.001386462,0.000280192,0.004247353,3.29E-05,1.71E-05,0.0005864,0.000411682,0.005950937,0.001727077,0.980561753,0.000307622,0.003636245,0.000854298
8425,natural_language_inference57,109,"We tuned the number of layers , layer dimensionality and dropout rates to optimize performance on the development set , using the Adam optimizer .",RESULTS,RESULTS,natural_language_inference,57,6,1,0,,0.912716837,1,hyperparameters,0.021795028,0.000684279,0.001073211,0.001207816,0.00010975,0.066052233,0.022464243,0.50630837,0.001116703,0.357656358,0.000134979,0.010480538,0.010916493
8426,natural_language_inference57,110,Batch normalization and PReLU units were used .,RESULTS,RESULTS,natural_language_inference,57,7,1,0,,0.739231046,1,negative,0.097288726,0.000715482,0.007877493,0.003572108,0.00019025,0.036828611,0.023250405,0.148331358,0.002537909,0.647051775,0.000140815,0.014847421,0.017367646
8427,natural_language_inference57,111,"Our best network used 2 hidden layers of 1000 units each , with dropout rate of 0.5 across both the input and hidden layers .",RESULTS,RESULTS,natural_language_inference,57,8,1,0,,0.962630527,1,hyperparameters,0.041049745,0.001235246,0.005710306,0.010776885,0.000350553,0.120423429,0.074539798,0.415992024,0.003468206,0.19629401,0.000522611,0.01503122,0.114605967
8428,natural_language_inference57,112,We did not backpropagate through the skip - thought encoder .,RESULTS,RESULTS,natural_language_inference,57,9,1,0,,0.001754041,0,negative,0.054855234,2.55E-05,0.003311416,5.17E-05,2.59E-05,0.000278056,0.000392981,0.000546119,0.000147742,0.925430375,1.00E-05,0.014662859,0.000262085
8429,natural_language_inference57,113,"We also evaluate against EOP classifier , a 2 - class baseline introduced by , and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance , order - embeddings ( symmetric ) .",RESULTS,RESULTS,natural_language_inference,57,10,1,0,,0.394276061,0,negative,0.010972221,0.000186138,0.011297254,1.09E-05,2.69E-05,0.000138744,0.003213139,0.000626463,7.27E-05,0.793440246,4.13E-05,0.179593589,0.000380343
8430,natural_language_inference57,114,The results for all models are shown in .,RESULTS,RESULTS,natural_language_inference,57,11,1,0,,0.101826753,0,results,0.006171185,1.29E-06,7.30E-05,5.70E-07,7.63E-07,1.19E-05,0.002003346,5.79E-05,5.71E-07,0.243868919,6.85E-06,0.747732726,7.11E-05
8431,natural_language_inference57,115,We see that order- embeddings outperform the skipthought baseline despite not using external text corpora .,RESULTS,RESULTS,natural_language_inference,57,12,1,1,experiments,0.958517592,1,results,0.014362393,3.15E-06,6.57E-05,7.81E-06,2.41E-06,4.85E-05,0.00901789,0.00027355,1.85E-06,0.030810052,1.22E-05,0.944222526,0.001171903
8432,natural_language_inference57,116,"While our method is almost certainly worse than the state - of - the - art method of , which uses a word - by - word attention mechanism , it is also much simpler .",RESULTS,RESULTS,natural_language_inference,57,13,1,0,,0.372380259,0,results,0.012598491,3.27E-06,0.000173756,2.34E-06,2.36E-06,1.68E-05,0.002775559,6.90E-05,1.41E-06,0.089468953,1.23E-05,0.894609566,0.000266157
8433,natural_language_inference57,117,CAPTION - IMAGE RETRIEVAL,RESULTS,,natural_language_inference,57,14,1,0,,0.841943836,1,results,0.003531792,7.72E-06,0.000741573,3.18E-05,1.06E-05,5.64E-05,0.03138669,0.000100341,3.37E-06,0.032752336,0.000352077,0.916256307,0.014768922
8434,natural_language_inference57,118,The caption - image retrieval task has become a standard evaluation of joint models of vision and language .,RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,15,1,0,,0.238532175,0,negative,0.000552119,4.42E-07,5.78E-05,2.54E-05,6.39E-07,6.11E-05,0.001341877,9.74E-06,2.35E-06,0.835478733,0.000557676,0.037930788,0.123981334
8435,natural_language_inference57,119,"The task involves ranking a large dataset of images by relevance for a query caption ( Image Retrieval ) , and ranking captions by relevance for a query image ( Caption Retrieval ) .",RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,16,1,0,,0.068173196,0,negative,0.000571647,8.51E-07,8.65E-05,9.39E-05,1.49E-06,9.00E-05,0.000907575,1.54E-05,5.96E-06,0.819923978,0.000452129,0.025759102,0.152091424
8436,natural_language_inference57,120,"Given a set of aligned image - caption pairs as training data , the goal is then to learn a caption - image compatibility score S ( c , i ) to be used at test time .",RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,17,1,0,,0.000411742,0,negative,9.09E-05,5.23E-07,4.18E-05,1.21E-07,5.52E-08,2.15E-06,1.30E-05,4.56E-06,7.21E-06,0.997664647,1.77E-06,0.002089789,8.35E-05
8437,natural_language_inference57,121,"Many modern approaches model the caption - image relationship symmetrically , either by embedding into a common "" visual - semantic "" space with inner-product similarity , or by using Canonical Correlations Analysis between distributed representations of images and captions .",RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,18,1,0,,0.00679787,0,negative,0.000339246,1.00E-06,0.000148966,2.02E-06,1.88E-07,2.52E-05,0.000134902,1.12E-05,1.06E-05,0.989932669,4.09E-05,0.005825606,0.003527491
8438,natural_language_inference57,122,"While and model a finer - grained alignment between regions in the image and segments of the caption , the similarity they use is still symmetric .",RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,19,1,0,,0.001463755,0,negative,0.00035496,4.16E-07,9.29E-05,3.92E-07,1.20E-07,6.68E-06,8.33E-05,5.67E-06,2.19E-06,0.983766595,2.79E-06,0.015188617,0.000495309
8439,natural_language_inference57,123,"An alternative is to learn an unconstrained binary relation , either with a neural language model conditioned on the image or using a multimodal CNN .",RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,20,1,0,,0.000942631,0,negative,0.000214107,3.44E-07,7.26E-05,2.60E-06,1.43E-07,2.05E-05,8.13E-05,1.46E-05,5.16E-06,0.992621801,7.31E-06,0.004534639,0.002424978
8440,natural_language_inference57,124,"In contrast to these lines of work , we propose to treat the caption - image pairs as a two - level partial order with captions above the images they describe , and let",RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,21,1,0,,0.000717757,0,negative,0.001297213,4.61E-06,0.001073263,2.46E-07,2.80E-07,5.86E-06,9.53E-05,7.28E-06,7.90E-05,0.99136049,2.06E-06,0.00592392,0.000150489
8441,natural_language_inference57,125,"with E our order-violation penalty defined in Eq , and f c , f i are embedding functions from captions and images into RN + .",RESULTS,CAPTION - IMAGE RETRIEVAL,natural_language_inference,57,22,1,0,,0.000107388,0,negative,6.22E-05,7.05E-08,4.32E-06,4.33E-08,1.99E-08,1.87E-06,1.13E-05,6.02E-06,1.01E-06,0.999398194,5.46E-08,0.00049612,1.88E-05
8442,natural_language_inference57,126,IMAGE AND CAPTION EMBEDDINGS,RESULTS,,natural_language_inference,57,23,1,0,,0.508929038,1,negative,0.019585668,8.13E-05,0.011756443,0.000666708,8.91E-05,0.001349475,0.014931567,0.003336966,0.000413945,0.449896406,0.000263152,0.391518461,0.106110742
8443,natural_language_inference57,127,"To learn f c and f i , we use the approach of except , since we are embedding into RN + , we constrain the embedding vectors to have nonnegative entries by taking their absolute value .",RESULTS,IMAGE AND CAPTION EMBEDDINGS,natural_language_inference,57,24,1,0,,0.003538166,0,negative,0.001153113,1.99E-06,0.002355674,2.30E-07,1.11E-07,4.81E-06,1.65E-05,4.06E-05,2.53E-05,0.989437342,5.40E-07,0.006816425,0.000147369
8444,natural_language_inference57,128,"Thus , to embed images , we use f i ( i ) = | W i CNN ( i ) | ( 6 ) where W i is a learned N 4096 matrix , N being the dimensionality of the embedding space .",RESULTS,IMAGE AND CAPTION EMBEDDINGS,natural_language_inference,57,25,1,0,,0.000479218,0,negative,7.08E-05,1.42E-07,6.97E-05,4.97E-08,1.57E-08,3.53E-06,4.32E-06,3.45E-05,3.08E-06,0.99907785,6.15E-08,0.000698836,3.71E-05
8445,natural_language_inference57,129,"CNN ( i ) is the same image feature used by : we rescale images to have smallest side 256 pixels , we take 224 224 crops from the corners , center , and their horizontal reflections , run the 10 crops through the 19 - layer VGG network of ( weights pre-trained on ImageNet and fixed during training ) , and average their fc7 features ..",RESULTS,IMAGE AND CAPTION EMBEDDINGS,natural_language_inference,57,26,1,0,,0.001043021,0,negative,0.000197335,7.62E-08,0.000335323,2.06E-07,6.42E-08,1.03E-05,4.87E-05,2.48E-05,8.70E-07,0.992541058,8.06E-08,0.006478141,0.000363004
8446,natural_language_inference57,130,Best results over all are in bold ; best results using 1 - crop VGG features are underlined .,RESULTS,IMAGE AND CAPTION EMBEDDINGS,natural_language_inference,57,27,1,0,,0.002039927,0,negative,9.04E-05,2.48E-08,3.03E-05,5.88E-08,8.55E-09,3.44E-06,3.25E-05,1.31E-05,2.11E-07,0.975369744,1.02E-07,0.02425411,0.000206038
8447,natural_language_inference57,131,"To embed the captions , we use a recurrent neural net encoder with GRU activations , so f c ( c ) = | GRU ( c ) | , the absolute value of hidden state after processing the last word .",RESULTS,IMAGE AND CAPTION EMBEDDINGS,natural_language_inference,57,28,1,0,,0.005843713,0,negative,0.000332912,5.08E-07,0.000585995,2.39E-07,8.37E-08,1.08E-05,2.52E-05,0.000112894,1.42E-05,0.996728095,1.05E-07,0.001830479,0.000358452
8448,natural_language_inference57,132,EXPLORATION,RESULTS,,natural_language_inference,57,29,1,0,,0.009739259,0,negative,0.016492475,0.000141168,0.0041748,0.000125071,7.89E-05,0.000817933,0.004910674,0.005954409,0.000411892,0.849581107,4.71E-05,0.104572536,0.012691907
8449,natural_language_inference57,133,Why would order- embeddings do well on such a shallow partial order ?,RESULTS,EXPLORATION,natural_language_inference,57,30,1,0,,2.64E-06,0,negative,4.64E-05,7.29E-07,4.98E-06,1.85E-07,9.18E-08,1.54E-05,2.35E-06,7.69E-05,2.37E-06,0.999714862,5.13E-06,0.000117698,1.28E-05
8450,natural_language_inference57,134,Why are they much more helpful for image retrieval than for caption retrieval ?,RESULTS,EXPLORATION,natural_language_inference,57,31,1,0,,5.58E-07,0,negative,2.24E-05,2.19E-07,1.59E-06,2.38E-08,4.08E-08,2.91E-06,5.89E-07,1.76E-05,4.13E-07,0.999898327,7.11E-07,5.35E-05,1.69E-06
8451,natural_language_inference57,135,"Intuitively , symmetric similarity should fail when an image has captions with very different levels of detail , because the captions are so dissimilar that it is impossible to map both their embeddings close to the same image embedding .",RESULTS,EXPLORATION,natural_language_inference,57,32,1,0,,0.000309564,0,negative,0.000742041,1.45E-06,3.76E-05,3.99E-08,1.68E-07,2.54E-06,1.18E-06,1.47E-05,2.07E-06,0.997632769,8.11E-07,0.001562507,2.11E-06
8452,natural_language_inference57,136,Order- embeddings do n't have this problem : the less detailed caption can be embedded very faraway from the image while remaining above it in the partial order .,RESULTS,EXPLORATION,natural_language_inference,57,33,1,0,,4.00E-06,0,negative,0.000261806,8.46E-07,2.68E-05,9.78E-07,4.48E-07,2.26E-05,3.11E-06,4.33E-05,3.40E-06,0.999275225,3.05E-06,0.000322665,3.58E-05
8453,natural_language_inference57,137,"To evaluate this intuition , we use caption length as a proxy for level of detail and select , among pairs of co-referring captions in our validation set , the 100 pairs with the biggest length difference .",RESULTS,EXPLORATION,natural_language_inference,57,34,1,0,,6.18E-07,0,negative,0.000227296,2.82E-06,2.37E-05,2.95E-08,1.12E-06,2.86E-06,9.91E-07,1.81E-05,9.34E-07,0.999496534,1.71E-08,0.000225253,3.83E-07
8454,natural_language_inference57,138,"For image retrieval with 1000 target images , the mean rank over captions in this set is 6.4 for orderembeddings and 9.7 for cosine similarity , a much bigger difference than over the entire dataset .",RESULTS,EXPLORATION,natural_language_inference,57,35,1,0,,0.00084955,0,negative,0.007109138,3.19E-06,8.97E-05,2.98E-07,4.01E-06,1.44E-05,0.000112074,5.57E-05,8.45E-07,0.832978054,5.07E-07,0.159579151,5.29E-05
8455,natural_language_inference57,139,Some particularly dramatic examples of this are shown in .,RESULTS,EXPLORATION,natural_language_inference,57,36,1,0,,1.41E-07,0,negative,3.75E-05,4.09E-08,2.16E-06,6.01E-08,2.08E-07,1.42E-06,1.16E-07,1.93E-06,1.79E-07,0.999940745,1.07E-08,1.53E-05,3.53E-07
8456,natural_language_inference57,140,"Moreover , if we use the shorter caption as a query , and retrieve captions in order of increasing error , the mean rank of the longer caption is 34.0 for order - embeddings and 47.6 for cosine similarity , showing that order - embeddings are able to capture the relatedness of co-referring captions with very different lengths .",RESULTS,EXPLORATION,natural_language_inference,57,37,1,0,,0.011448202,0,negative,0.092273858,4.26E-06,8.85E-05,3.95E-07,2.82E-06,1.22E-05,0.000120423,5.94E-05,2.13E-06,0.667545411,5.42E-07,0.239841029,4.91E-05
8457,natural_language_inference57,141,"This also explains why order - embeddings provide a much smaller improvement for caption retrieval than for image retrieval : all the caption retrieval metrics are based on the position of the first ground truth caption in the retrieval order , so the embeddings need only learn to retrieve one of each image 's five captions well , which symmetric similarity is well suited for .",RESULTS,EXPLORATION,natural_language_inference,57,38,1,0,,0.000370799,0,negative,0.027792248,1.94E-06,9.89E-05,1.45E-07,7.69E-07,6.07E-06,4.15E-05,2.34E-05,1.40E-06,0.85368055,4.55E-07,0.118330257,2.23E-05
8458,natural_language_inference57,142,Captions,RESULTS,EXPLORATION,natural_language_inference,57,39,1,0,,5.30E-07,0,negative,2.17E-05,3.73E-07,3.16E-06,4.50E-07,4.11E-07,7.12E-06,5.81E-07,3.67E-05,2.30E-06,0.999899209,4.07E-08,2.34E-05,4.57E-06
8459,natural_language_inference57,143,Image rank cosine order - emb a sitting are a with furniture and flowers makes a backdrop for a boy with headphones sitting in the foreground atone of the chairs at a dining table that holds glasses and a handbag working at a laptop,RESULTS,EXPLORATION,natural_language_inference,57,40,1,0,,4.27E-05,0,negative,0.00022224,1.61E-05,0.001600337,5.17E-07,1.05E-06,2.77E-05,7.56E-05,8.00E-05,4.59E-05,0.991926085,9.16E-05,0.005456289,0.000456478
8460,natural_language_inference57,144,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,RESULTS,,natural_language_inference,57,41,1,1,experiments,0.867353534,1,results,0.010426419,2.57E-05,0.001243692,0.00018093,5.08E-05,0.000198647,0.056974713,0.000393051,1.24E-05,0.111652981,0.000191388,0.676006327,0.142642967
8461,natural_language_inference57,145,Natural language inference can be seen as a generalization of hypernymy from words to sentences .,RESULTS,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,natural_language_inference,57,42,1,0,,0.000390708,0,negative,0.000182377,9.29E-07,3.51E-05,3.86E-06,8.86E-07,4.11E-05,0.001522489,8.62E-06,4.66E-06,0.974774302,6.29E-05,0.001246298,0.022116516
8462,natural_language_inference57,146,"For example , from "" woman walking her dog in a park "" we can infer both "" woman walking her dog "" and "" dog in a park "" , but not "" old woman "" or "" black dog "" .",RESULTS,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,natural_language_inference,57,43,1,0,,1.59E-05,0,negative,1.97E-05,2.25E-08,1.54E-06,4.53E-08,1.57E-07,1.81E-06,1.06E-05,6.28E-07,6.69E-07,0.999915594,1.39E-08,2.49E-05,2.43E-05
8463,natural_language_inference57,147,"Given a pair of sentences , our task is to predict whether we can infer the second sentence ( the hypothesis ) from the first ( the premise ) .",RESULTS,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,natural_language_inference,57,44,1,0,,0.000315457,0,negative,0.000175836,2.27E-06,6.84E-05,2.68E-06,1.86E-06,2.28E-05,0.000642281,9.89E-06,1.72E-05,0.989722948,1.34E-05,0.001113917,0.008206439
8464,natural_language_inference57,148,IMPLEMENTATION DETAILS,,,natural_language_inference,57,0,1,0,,0.045796543,0,negative,4.47E-05,0.001220595,9.47E-06,3.20E-05,1.55E-05,0.000465852,0.000169332,0.007430528,0.000344669,0.986360185,0.003666023,0.000225373,1.58E-05
8465,natural_language_inference57,149,"Just as for caption - image ranking , we set the dimensions of the embedding space and GRU hidden state to be 1024 , the dimension of the word embeddings to be 300 , and constrain the embeddings to have unit L2 norm .",IMPLEMENTATION DETAILS,IMPLEMENTATION DETAILS,natural_language_inference,57,1,1,1,experiments,0.990376508,1,hyperparameters,1.30E-05,3.36E-05,8.99E-06,2.11E-06,7.47E-07,0.474105392,0.000648488,0.517983802,1.55E-05,0.007165956,4.86E-06,6.88E-06,1.06E-05
8466,natural_language_inference57,150,We train for 10 epochs with batches of 128 sentence pairs .,IMPLEMENTATION DETAILS,IMPLEMENTATION DETAILS,natural_language_inference,57,2,1,1,experiments,0.98933932,1,experimental-setup,1.60E-05,3.12E-05,1.07E-05,8.84E-06,1.65E-06,0.579644411,0.001448159,0.413155177,1.58E-05,0.005617773,6.26E-06,9.27E-06,3.48E-05
8467,natural_language_inference57,151,We use the Adam optimizer with learning rate 0.001 and early stopping on the validation set .,IMPLEMENTATION DETAILS,IMPLEMENTATION DETAILS,natural_language_inference,57,3,1,1,experiments,0.994600308,1,hyperparameters,1.07E-05,2.17E-05,4.32E-06,6.23E-06,8.19E-07,0.485088391,0.000626752,0.511374021,1.05E-05,0.002831523,3.12E-06,4.66E-06,1.73E-05
8468,natural_language_inference57,152,"During evaluation , we find the optimal classification threshold on validation , then use the threshold to classify the test set .",IMPLEMENTATION DETAILS,IMPLEMENTATION DETAILS,natural_language_inference,57,4,1,0,,0.042906825,0,negative,0.000660195,0.000431667,0.000323369,5.06E-06,1.96E-05,0.119325994,0.000747848,0.180809113,0.00014542,0.696967748,2.09E-05,0.000523951,1.91E-05
8469,natural_language_inference57,153,Method 2 - class 3 - class,IMPLEMENTATION DETAILS,IMPLEMENTATION DETAILS,natural_language_inference,57,5,1,0,,0.117451372,0,negative,0.001300215,0.000107533,0.016764907,2.31E-05,2.16E-05,0.296577139,0.030016175,0.148708647,0.000164638,0.491212761,0.000486864,0.01417907,0.000437401
8470,natural_language_inference57,154,Neural Attention * 83.5 EOP classifier 75.0 * skip - thoughts 87.7 81.5 order - embeddings ( symmetric ) 79.3 * order - embeddings,IMPLEMENTATION DETAILS,IMPLEMENTATION DETAILS,natural_language_inference,57,6,1,0,,0.011069936,0,experimental-setup,0.000732498,4.48E-05,0.000504155,0.000209864,9.14E-05,0.561835606,0.021286048,0.124511449,4.43E-05,0.287803388,0.000156195,0.002397044,0.000383351
8471,natural_language_inference57,155,88.6 *: Test accuracy ( % ) on SNLI .,IMPLEMENTATION DETAILS,IMPLEMENTATION DETAILS,natural_language_inference,57,7,1,0,,0.05875684,0,negative,0.003344677,2.78E-05,0.001030418,2.33E-05,3.44E-05,0.071176721,0.032235897,0.045734518,1.37E-05,0.736857987,0.000331792,0.108767745,0.000421042
8472,natural_language_inference57,156,CONCLUSION AND FUTURE WORK,,,natural_language_inference,57,0,1,0,,0.000993755,0,negative,6.55E-05,5.57E-05,6.74E-06,8.33E-07,5.66E-07,8.35E-05,7.13E-05,0.000591015,4.69E-05,0.993684537,0.004983168,0.000405376,4.84E-06
8473,natural_language_inference12,1,title,,,natural_language_inference,12,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
8474,natural_language_inference12,2,Shortcut - Stacked Sentence Encoders for Multi- Domain Inference,title,,natural_language_inference,12,1,1,1,research-problem,0.998750165,1,research-problem,4.04E-08,3.55E-05,1.81E-07,7.28E-08,5.45E-08,1.64E-07,7.85E-07,3.42E-06,9.21E-06,0.002806934,0.997143371,2.05E-07,5.25E-08
8475,natural_language_inference12,3,abstract,,,natural_language_inference,12,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
8476,natural_language_inference12,4,We present a simple sequential sentence encoder for multi-domain natural language inference .,abstract,abstract,natural_language_inference,12,1,1,1,research-problem,0.92569266,1,research-problem,1.31E-06,0.003867654,1.66E-05,9.49E-07,6.30E-06,1.44E-06,3.17E-06,2.07E-05,0.000450523,0.018345606,0.977283663,1.54E-06,4.76E-07
8477,natural_language_inference12,5,Our encoder is based on stacked bidirectional LSTM - RNNs with shortcut connections and fine - tuning of word embeddings .,abstract,abstract,natural_language_inference,12,2,1,0,,0.46398195,0,negative,2.83E-05,0.387240612,0.000274988,0.00065432,0.00154187,0.001327494,6.42E-05,0.008814519,0.068335595,0.483274598,0.048425938,2.45E-06,1.51E-05
8478,natural_language_inference12,6,"The over all supervised model uses the above encoder to encode two input sentences into two vectors , and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural .",abstract,abstract,natural_language_inference,12,3,1,0,,0.137790447,0,negative,3.08E-05,0.093127864,0.003040674,2.14E-05,0.000211494,0.000151132,3.44E-05,0.001085174,0.064860035,0.625749439,0.211677449,5.53E-06,4.59E-06
8479,natural_language_inference12,7,"Our Shortcut - Stacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference ( top non-ensemble single - model result in the EMNLP RepEval 2017 Shared Task ( Nangia et al. , 2017 ) ) .",abstract,abstract,natural_language_inference,12,4,1,0,,0.009903301,0,research-problem,5.80E-05,0.005612467,2.12E-05,1.32E-05,5.90E-05,2.47E-05,0.000119199,0.000365527,0.000173653,0.245107469,0.748103437,0.000334814,7.32E-06
8480,natural_language_inference12,8,"Moreover , they achieve the new state - of - theart encoding result on the original SNLI dataset ( Bowman et al. , 2015 ) .",abstract,abstract,natural_language_inference,12,5,1,0,,0.004134429,0,research-problem,3.37E-06,0.001690235,3.34E-06,5.88E-06,1.73E-05,1.20E-05,8.51E-06,0.000154532,0.000103782,0.378335109,0.619657312,7.91E-06,7.33E-07
8481,natural_language_inference12,9,Introduction and Background,,,natural_language_inference,12,0,1,0,,4.80E-05,0,negative,0.000183286,0.000204494,1.47E-05,0.000799599,2.21E-05,0.000666118,0.000137236,0.001295591,0.000238865,0.991928113,0.004449998,2.41E-05,3.58E-05
8482,natural_language_inference12,58,Results and Analysis,,,natural_language_inference,12,0,1,0,,0.004153659,0,negative,4.92E-05,4.74E-05,7.12E-06,1.41E-07,3.63E-07,1.90E-05,8.00E-05,0.000295611,1.21E-05,0.995144313,0.001609051,0.002734443,1.25E-06
8483,natural_language_inference12,59,Ablation Analysis Results,,,natural_language_inference,12,0,1,0,,0.040979843,0,negative,0.039014443,0.000119161,0.001039071,0.000216458,0.000101227,0.000236134,0.000966627,0.00023596,7.35E-05,0.948818266,0.001235909,0.007902221,4.11E-05
8484,natural_language_inference12,60,We now investigate the effectiveness of each of the enhancement components in our over all model .,Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,1,1,0,,0.007532866,0,ablation-analysis,0.718611516,2.21E-05,0.000365914,4.88E-07,4.42E-06,3.39E-05,5.68E-05,4.97E-06,0.00022006,0.280617168,1.54E-05,9.89E-06,3.74E-05
8485,natural_language_inference12,61,"These ablation results are shown in and 4 , all based on the Multi - NLI development sets .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,2,1,1,ablation-analysis,0.083205002,0,ablation-analysis,0.922880217,1.06E-06,0.00020709,1.44E-07,7.80E-06,3.33E-06,5.45E-05,1.54E-07,2.06E-06,0.076790374,2.70E-06,3.66E-05,1.39E-05
8486,natural_language_inference12,62,"Finally , shows results for different encoders on SNLI and Multi - NLI test sets .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,3,1,0,,0.202942489,0,ablation-analysis,0.870615113,5.27E-06,0.000124885,7.87E-08,1.44E-06,5.79E-06,0.000141132,8.76E-07,5.37E-06,0.128967833,1.54E-05,9.78E-05,1.90E-05
8487,natural_language_inference12,63,"First , shows the performance changes for different number of biLSTM layers and their varying dimension size .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,4,1,0,,0.43318524,0,ablation-analysis,0.851045618,3.93E-06,0.000798013,5.38E-07,1.42E-05,9.47E-06,2.95E-05,4.49E-07,1.27E-05,0.148049295,3.67E-06,1.29E-05,1.97E-05
8488,natural_language_inference12,64,The dimension size of a biLSTM layer is referring to the dimension of the hidden state for both the forward and backward LSTM - RNNs .,Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,5,1,0,,0.003819648,0,negative,0.100137325,0.000148684,0.002749085,9.22E-06,3.81E-05,0.003103655,0.000489064,0.000590089,0.003108965,0.887796619,8.78E-05,2.38E-06,0.001739036
8489,natural_language_inference12,65,"As shown , each added layer model improves the accuracy and we achieve a substantial improvement in accuracy ( around 2 % ) on both matched and mismatched settings , compared to the single - layer biLSTM in .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,6,1,1,ablation-analysis,0.964940888,1,ablation-analysis,0.999143167,1.84E-07,7.65E-06,7.66E-08,3.96E-07,7.79E-07,3.25E-05,7.32E-08,3.00E-07,0.000757536,3.21E-07,1.84E-05,3.86E-05
8490,natural_language_inference12,66,"We only experimented with up to 3 layers with 512 , 1024 , 2048 dimensions each , so the model still has potential to improve the result further with a larger dimension and more layers .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,7,1,0,,0.634449214,1,ablation-analysis,0.717969552,2.25E-05,0.000953846,0.000103628,0.000455666,0.002155092,0.001296671,4.03E-05,6.71E-05,0.264461906,4.44E-05,2.27E-05,0.012406557
8491,natural_language_inference12,67,"Next , in , we show that the shortcut connections among the biLSTM layers is also an important contributor to accuracy improvement ( around 1.5 % on top of the full 3 - layered stacked - RNN model ) .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,8,1,1,ablation-analysis,0.962730223,1,ablation-analysis,0.999160163,4.20E-07,1.52E-05,1.47E-07,1.16E-06,7.14E-07,7.23E-06,4.53E-08,1.25E-06,0.000802551,1.39E-07,2.26E-06,8.74E-06
8492,natural_language_inference12,68,This demonstrates that simply stacking the biLSTM layers is not sufficient to handle a complex task like Multi - NLI and it is significantly better to have the higher layer connected to both the output and the original input of all the previous layers ( note that results are based on multi-layered models with shortcut connections ) .,Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,9,1,0,,0.003844912,0,ablation-analysis,0.928846476,9.24E-07,5.30E-05,6.92E-08,9.78E-07,4.42E-06,2.15E-05,2.90E-07,3.02E-06,0.071042352,3.48E-06,1.02E-05,1.33E-05
8493,natural_language_inference12,69,"Next , in , we show that fine - tuning the word embeddings also improves results , again for both the in - domain task and cross - domain tasks ( the ablation results are based on a smaller model with a 128 +256 2 - layer biLSTM ) .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,10,1,1,ablation-analysis,0.96869334,1,ablation-analysis,0.998416623,3.74E-07,9.17E-06,7.05E-08,5.34E-07,1.18E-06,5.12E-05,1.21E-07,4.39E-07,0.001470635,4.43E-07,2.13E-05,2.80E-05
8494,natural_language_inference12,70,"Hence , all our models were trained with word embeddings being fine - tuned .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,11,1,0,,0.018125457,0,negative,0.200373491,0.000370504,0.005573556,2.07E-05,0.000317818,0.002278884,0.000653371,0.000244367,0.001727124,0.78728576,9.26E-06,3.65E-06,0.001141521
8495,natural_language_inference12,71,The last ablation in shows that a classifier with two layers of relu is preferable than other options .,Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,12,1,1,ablation-analysis,0.941014064,1,ablation-analysis,0.991043267,1.57E-07,1.97E-05,6.35E-08,1.01E-06,1.56E-06,3.17E-05,8.76E-08,2.54E-07,0.008870437,3.79E-07,1.51E-05,1.63E-05
8496,natural_language_inference12,72,"Thus , we use that setting for our strongest encoder .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,13,1,0,,0.000795589,0,negative,0.053026071,3.05E-05,0.001076901,4.41E-06,9.28E-05,0.000157721,4.36E-05,1.57E-05,0.000225606,0.945213752,2.00E-06,2.07E-06,0.000108921
8497,natural_language_inference12,73,Multi - NLI and SNLI Test Results,Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,14,1,0,,0.776640165,1,ablation-analysis,0.899458517,8.76E-06,0.000346565,1.37E-07,1.18E-06,1.76E-05,0.002050454,2.22E-06,1.51E-05,0.096753155,0.00012524,0.000535144,0.000685865
8498,natural_language_inference12,74,"Finally , in , we report the test results for MNLI and SNLI .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,15,1,0,,0.093876322,0,negative,0.395058976,2.71E-06,0.000210591,3.03E-06,0.000139756,2.73E-05,4.82E-05,8.12E-07,5.39E-06,0.604416576,2.33E-06,1.86E-05,6.57E-05
8499,natural_language_inference12,75,"First for Multi - NLI , we improve substantially over the CBOW and biL - STM Encoder baselines reported in the dataset paper .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,16,1,1,results,0.895773207,1,ablation-analysis,0.993413739,2.61E-06,9.56E-05,4.09E-07,2.54E-06,6.56E-06,0.000370568,6.14E-07,3.14E-06,0.005619693,2.41E-06,0.000101995,0.000380092
8500,natural_language_inference12,76,We also show that our final shortcut - based stacked encoder achieves around 3 % improvement as compared to the 1 layer biLSTM - Max Encoder in the second last row ( using the exact same classifier and optimizer settings ) .,Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,17,1,1,results,0.939862055,1,ablation-analysis,0.996374366,1.34E-06,3.24E-05,3.29E-07,1.90E-06,4.54E-06,0.000274469,4.90E-07,1.78E-06,0.002864935,7.30E-07,5.56E-05,0.000387114
8501,natural_language_inference12,77,Our shortcut - encoder was also the top singe - model ( non-ensemble ) result on the EMNLP RepEval Shared Task leaderboard .,Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,18,1,1,results,0.540193387,1,ablation-analysis,0.90628663,7.81E-06,0.003182515,1.19E-06,1.32E-05,6.14E-05,0.002079631,2.85E-06,2.61E-05,0.08477423,1.75E-05,0.000243053,0.003303912
8502,natural_language_inference12,78,"Next , for SNLI , we compare our shortcutstacked encoder with the current state - of - the - art encoders from the SNLI leaderboard ( https :// nlp.stanford.edu/projects/snli/ ) .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,19,1,1,results,0.399239441,0,ablation-analysis,0.53811554,0.0003103,0.013821399,2.13E-06,0.000254275,0.000141887,0.000964174,7.79E-06,0.000285113,0.44580107,6.68E-06,4.37E-05,0.000245902
8503,natural_language_inference12,79,"We also compare to the recent biLSTM - Max Encoder of , which served as our model 's 1 - layer starting point .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,20,1,1,results,0.005873527,0,negative,0.230739566,0.000286749,0.04675786,3.55E-06,0.000230078,0.00026313,0.000695733,1.27E-05,0.000802129,0.719740887,1.22E-05,1.75E-05,0.000437954
8504,natural_language_inference12,80,"The results indicate that ' Our Shortcut - Stacked Encoder ' sur-passes all the previous state - of - the - art encoders , and achieves the new best encoding - based result on SNLI , suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders .",Ablation Analysis Results,Ablation Analysis Results,natural_language_inference,12,21,1,1,results,0.947298859,1,ablation-analysis,0.986797487,3.57E-06,0.00013996,7.59E-08,1.50E-06,3.51E-06,0.000207397,2.46E-07,6.47E-06,0.012669814,1.71E-06,6.38E-05,0.000104469
8505,natural_language_inference12,81,Conclusion,,,natural_language_inference,12,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
8506,text_generation3,1,title,,,text_generation,3,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
8507,text_generation3,2,An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue Generation,title,title,text_generation,3,1,1,1,research-problem,0.998893938,1,research-problem,2.77E-08,9.76E-06,9.46E-08,3.87E-08,3.00E-08,6.96E-08,6.18E-07,1.23E-06,2.29E-06,0.001532854,0.998452809,1.48E-07,4.03E-08
8508,text_generation3,3,abstract,,,text_generation,3,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
8509,text_generation3,4,Generating semantically coherent responses is still a major challenge in dialogue generation .,abstract,abstract,text_generation,3,1,1,0,,0.919243109,1,research-problem,1.95E-08,2.08E-06,6.29E-09,9.49E-07,2.33E-07,1.24E-07,3.69E-07,4.81E-07,8.29E-08,0.006658505,0.993337096,1.64E-08,3.63E-08
8510,text_generation3,5,"Different from conventional text generation tasks , the mapping between inputs and responses in conversations is more complicated , which highly demands the understanding of utterance - level semantic dependency , a relation between the whole meanings of inputs and outputs .",abstract,abstract,text_generation,3,2,1,0,,0.776146214,1,research-problem,4.20E-08,9.48E-06,1.36E-08,2.49E-06,1.35E-06,3.11E-07,5.36E-07,1.20E-06,2.96E-07,0.024305459,0.975678727,2.78E-08,5.98E-08
8511,text_generation3,6,"To address this problem , we propose an Auto - Encoder Matching ( AEM ) model to learn such dependency .",abstract,abstract,text_generation,3,3,1,0,,0.528388644,1,research-problem,1.64E-05,0.073136361,0.000183939,7.89E-06,8.28E-05,1.99E-05,1.16E-05,0.000223781,0.019378218,0.141898823,0.765033543,4.32E-06,2.45E-06
8512,text_generation3,7,The model contains two auto - encoders and one mapping module .,abstract,abstract,text_generation,3,4,1,0,,0.617019609,1,negative,2.52E-05,0.0764538,0.000227414,6.13E-05,0.000214555,0.000351485,3.15E-05,0.002359102,0.184908093,0.584967788,0.150388078,2.71E-06,8.96E-06
8513,text_generation3,8,"The auto - encoders learn the semantic representations of inputs and responses , and the mapping module learns to connect the utterance - level representations .",abstract,abstract,text_generation,3,5,1,0,,0.084367926,0,negative,2.23E-05,0.089282216,0.000349453,4.08E-06,5.96E-05,8.89E-05,8.70E-06,0.000978989,0.356422936,0.514150562,0.03862869,1.95E-06,1.57E-06
8514,text_generation3,9,Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models .,abstract,abstract,text_generation,3,6,1,0,,0.003907868,0,negative,8.51E-05,0.001518291,2.21E-06,6.19E-06,6.44E-05,9.30E-06,2.85E-05,0.000134829,3.51E-05,0.911550142,0.086289099,0.000275499,1.27E-06
8515,text_generation3,10,1,abstract,abstract,text_generation,3,7,1,0,,4.00E-05,0,negative,5.81E-07,0.00024644,1.20E-07,4.87E-06,6.69E-06,1.37E-05,4.65E-07,7.69E-05,0.000215726,0.991868842,0.007565494,9.12E-08,6.51E-08
8516,text_generation3,11,Introduction,,,text_generation,3,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
8517,text_generation3,12,"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .",Introduction,Introduction,text_generation,3,1,1,1,research-problem,0.74967337,1,research-problem,7.48E-07,7.42E-05,1.95E-07,1.83E-05,1.74E-05,6.39E-06,1.24E-05,5.17E-06,1.81E-05,0.039663243,0.960180637,9.26E-07,2.32E-06
8518,text_generation3,13,"Recently there is an increasing amount of studies about purely datadriven dialogue models , which learn from large corpora of human conversations without handcrafted rules or templates .",Introduction,Introduction,text_generation,3,2,1,0,,0.621875424,1,research-problem,7.51E-07,0.000307134,2.05E-07,9.79E-07,3.35E-06,4.53E-06,6.87E-06,1.13E-05,0.00014551,0.069791286,0.929726428,9.80E-07,6.37E-07
8519,text_generation3,14,Most of them are based on the sequence - to - sequence ( Seq2Seq ) framework that maximizes the probability of gold responses given the previous dialogue turn .,Introduction,Introduction,text_generation,3,3,1,0,,0.04164713,0,negative,1.53E-05,0.00631332,1.42E-05,0.000129115,0.000327344,0.000326421,8.64E-05,0.000290499,0.004670117,0.551905223,0.435899641,8.19E-06,1.43E-05
8520,text_generation3,15,Although such methods offer great * Equal Contribution 1 The code is available at https://github.com/lancopku/AMM,Introduction,Introduction,text_generation,3,4,1,0,,0.014554378,0,negative,1.56E-05,0.000950518,2.19E-06,5.32E-05,0.000117542,6.21E-05,2.87E-05,4.93E-05,0.00046999,0.698345274,0.299896175,5.81E-06,3.64E-06
8521,text_generation3,16,"promise for generating fluent responses , they still suffer from the poor semantic relevance between inputs and responses .",Introduction,Introduction,text_generation,3,5,1,0,,0.026980109,0,research-problem,1.82E-06,0.000185474,3.69E-07,5.60E-06,1.42E-05,1.03E-05,1.71E-05,1.53E-05,8.06E-05,0.180158116,0.81950678,2.81E-06,1.61E-06
8522,text_generation3,17,"For example , given "" What 's your name "" as the input , the models generate "" I like it "" as the output .",Introduction,Introduction,text_generation,3,6,1,0,,0.020767025,0,negative,5.24E-06,0.00733487,4.47E-06,3.70E-06,0.000138118,0.0001214,2.08E-05,0.000154883,0.042760073,0.930941754,0.018509542,3.88E-06,1.29E-06
8523,text_generation3,18,"Recently , the neural attention mechanism has been proved successful in many tasks including neural machine translation and abstractive summarization , for its ability of capturing word - level dependency by associating a generated word with relevant words in the source - side context .",Introduction,Introduction,text_generation,3,7,1,0,,0.285411761,0,research-problem,3.01E-06,0.001145592,2.02E-06,4.51E-06,1.29E-05,2.85E-05,3.07E-05,4.29E-05,0.002618129,0.171381162,0.824724162,3.46E-06,2.92E-06
8524,text_generation3,19,Recent studies have applied the attention mechanism to dialogue generation to improve the dialogue coherence .,Introduction,Introduction,text_generation,3,8,1,0,,0.496238013,0,research-problem,3.29E-06,0.001018045,9.16E-07,4.24E-06,1.53E-05,1.96E-05,2.87E-05,4.31E-05,0.000439829,0.158915369,0.839505389,3.89E-06,2.34E-06
8525,text_generation3,20,"However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses .",Introduction,Introduction,text_generation,3,9,1,1,research-problem,0.189645234,0,research-problem,1.47E-06,0.000286696,4.93E-07,3.13E-05,6.24E-05,1.58E-05,2.43E-05,1.36E-05,4.63E-05,0.143450424,0.856061429,2.19E-06,3.59E-06
8526,text_generation3,21,"For example , given "" Try not to take on more than you can handle "" as the input and "" You are right "" as the response , each response word can not find any aligned words from the input .",Introduction,Introduction,text_generation,3,10,1,0,,0.003103207,0,negative,3.49E-06,0.002263023,1.88E-06,6.52E-06,0.000185483,8.87E-05,1.46E-05,6.71E-05,0.006965849,0.97635082,0.014048283,3.09E-06,1.08E-06
8527,text_generation3,22,"In fact , this task requires the model to understand the utterance - level dependency , a relation between the whole meanings of inputs and outputs .",Introduction,Introduction,text_generation,3,11,1,0,,0.077825857,0,research-problem,3.29E-06,0.001581271,1.07E-06,3.25E-05,8.75E-05,4.30E-05,1.74E-05,5.07E-05,0.001353385,0.474615783,0.522207842,3.22E-06,3.00E-06
8528,text_generation3,23,"Due to the lack of utterance - level semantic dependency , the conventional attention - based methods that simply capture the word - level dependency achieve less satisfying performance in generating high - quality responses .",Introduction,Introduction,text_generation,3,12,1,0,,0.016687778,0,negative,8.02E-05,0.005286633,3.88E-06,1.05E-05,0.0001578,2.48E-05,0.000113583,6.88E-05,0.000645705,0.721524631,0.271962252,0.000116089,5.10E-06
8529,text_generation3,24,"To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .",Introduction,Introduction,text_generation,3,13,1,1,model,0.961464485,1,model,1.70E-05,0.148893178,8.73E-05,9.89E-07,0.000126819,1.35E-05,1.53E-05,2.96E-05,0.841928306,0.006992407,0.00188927,4.60E-06,1.75E-06
8530,text_generation3,25,"First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .",Introduction,Introduction,text_generation,3,14,1,1,model,0.93060015,1,approach,0.000124769,0.623986812,0.000286264,2.45E-05,0.003648939,9.13E-05,8.36E-05,0.000168362,0.344297184,0.02652086,0.000734817,2.52E-05,7.37E-06
8531,text_generation3,26,"Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .",Introduction,Introduction,text_generation,3,15,1,1,model,0.727848361,1,model,4.78E-06,0.037430989,3.76E-05,8.21E-08,2.84E-05,4.07E-06,3.08E-06,1.06E-05,0.955887602,0.006481993,0.000110009,5.68E-07,2.04E-07
8532,text_generation3,27,"The advantage is that by explicitly sep - arating representation learning and dependency learning , the model has a stronger modeling ability compared to traditional Seq2Seq models .",Introduction,Introduction,text_generation,3,16,1,0,,0.595638856,1,model,0.000798727,0.37837499,0.000327046,3.73E-05,0.004678508,0.000127197,0.000202688,0.00020621,0.385474483,0.225791284,0.003771296,0.000197137,1.32E-05
8533,text_generation3,28,Experimental results show that our model substantially outperforms baseline methods in generating highquality responses .,Introduction,Introduction,text_generation,3,17,1,0,,0.017256495,0,negative,0.001793138,0.037121573,1.92E-05,2.69E-05,0.001659466,0.000186823,0.004839006,0.000715518,0.002919547,0.935686148,0.006552966,0.008420654,5.90E-05
8534,text_generation3,29,Our contributions are listed as follows :,Introduction,Introduction,text_generation,3,18,1,0,,0.00329647,0,negative,1.80E-05,0.004176643,8.17E-06,0.000434042,0.00308972,0.000383546,5.56E-05,0.00014052,0.00271062,0.981034884,0.007935007,5.33E-06,7.94E-06
8535,text_generation3,30,"To promote coherence in dialogue generation , we propose a novel Auto - Encoder Matching model to learn the utterance - level dependency .",Introduction,Introduction,text_generation,3,19,1,0,,0.950589313,1,model,4.15E-05,0.32522043,0.000206542,3.99E-06,0.000445983,3.60E-05,8.07E-05,7.36E-05,0.652354221,0.016284746,0.00522708,1.81E-05,7.00E-06
8536,text_generation3,31,"In our proposed model , we explicitly separate utterance representation learning and dependency learning for a better expressive ability .",Introduction,Introduction,text_generation,3,20,1,0,,0.896573792,1,model,4.35E-05,0.161239828,6.46E-05,1.94E-06,0.000296768,2.83E-05,1.74E-05,5.89E-05,0.827325953,0.010713439,0.000203086,4.34E-06,1.93E-06
8537,text_generation3,32,Experimental results on automatic evaluation and human evaluation show that our model can generate much more coherent text compared to baseline models .,Introduction,Introduction,text_generation,3,21,1,0,,0.00968844,0,negative,0.001237102,0.033277177,2.42E-05,9.15E-06,0.001562493,0.000108258,0.004271942,0.000380488,0.00270319,0.944849473,0.003561964,0.007977495,3.71E-05
8538,text_generation3,33,Approach,,,text_generation,3,0,1,0,,6.30E-05,0,negative,0.000256653,0.000405673,1.67E-05,0.000511976,2.16E-05,0.00097816,0.00011784,0.00317846,0.000653531,0.990596627,0.003176926,4.36E-05,4.23E-05
8539,text_generation3,34,"In this section , we introduce our proposed model .",Approach,Approach,text_generation,3,1,1,0,,0.002782905,0,negative,5.20E-06,0.111724524,4.80E-06,1.66E-05,0.000322915,0.000125936,3.11E-06,0.001002711,0.038224945,0.845115658,0.003451431,1.23E-06,9.52E-07
8540,text_generation3,35,An overview is presented in Section 2.1 .,Approach,Approach,text_generation,3,2,1,0,,0.000501754,0,negative,4.22E-06,0.006405011,1.11E-06,3.71E-05,0.000446884,4.99E-05,1.18E-06,0.000118482,0.000961585,0.990847382,0.001126264,6.49E-07,2.72E-07
8541,text_generation3,36,"The details of the modules are shown in Sections 2.2 , 2.3 and 2.4 .",Approach,Approach,text_generation,3,3,1,0,,0.001848466,0,negative,2.20E-06,0.008049538,8.44E-07,5.94E-06,0.00014821,3.71E-05,8.33E-07,0.000153043,0.002588044,0.988658965,0.000354625,4.91E-07,1.20E-07
8542,text_generation3,37,The training method is introduced in Section 2.5 .,Approach,Approach,text_generation,3,4,1,0,,0.002310628,0,negative,3.51E-06,0.070518822,3.69E-06,1.53E-05,0.000411106,9.53E-05,2.38E-06,0.000955033,0.011441304,0.915326324,0.001226007,7.15E-07,5.22E-07
8543,text_generation3,38,Overview,Approach,,text_generation,3,5,1,0,,0.000591507,0,negative,1.90E-06,0.006064485,1.11E-06,7.24E-06,2.68E-05,8.54E-05,2.54E-06,0.000377458,0.008099238,0.979595796,0.005736852,6.27E-07,5.87E-07
8544,text_generation3,39,"The proposed model contains three modules : an encoder , a decoder , and a mapping module , as shown in .",Approach,Overview,text_generation,3,6,1,0,,0.018886611,0,negative,2.38E-05,0.080488561,0.000142637,5.05E-06,9.57E-05,7.77E-05,4.31E-06,0.000391462,0.334278486,0.576577324,0.007909216,3.89E-06,1.83E-06
8545,text_generation3,40,"In general , our model is different from the conventional sequence - to - sequence models .",Approach,Overview,text_generation,3,7,1,0,,0.003555619,0,negative,5.60E-05,0.039131941,6.64E-05,7.00E-07,4.83E-05,1.76E-05,1.55E-06,0.000100902,0.009547738,0.947615518,0.003390942,2.22E-05,1.56E-07
8546,text_generation3,41,The encoder and decoder are both implemented as autoencoders .,Approach,Overview,text_generation,3,8,1,0,,0.084184578,0,negative,2.79E-05,0.140162969,8.73E-05,2.57E-05,0.000220693,0.000532963,8.93E-06,0.003721279,0.251728437,0.602683838,0.000795691,1.94E-06,2.37E-06
8547,text_generation3,42,"They learn the internal representations of inputs and target responses , respectively .",Approach,Overview,text_generation,3,9,1,0,,3.48E-05,0,negative,6.98E-06,0.012015384,6.56E-05,2.44E-07,1.08E-05,1.89E-05,5.88E-07,0.000115832,0.06483135,0.922308354,0.000624827,1.02E-06,8.26E-08
8548,text_generation3,43,"In addition , a mapping module is built to map the internal representations of the input and the response .",Approach,Overview,text_generation,3,10,1,0,,0.012508006,0,model,3.35E-05,0.076816203,0.000193089,1.10E-06,6.09E-05,4.31E-05,2.49E-06,0.000235795,0.666396692,0.255481413,0.000732092,2.85E-06,7.45E-07
8549,text_generation3,44,Encoder,Approach,,text_generation,3,11,1,0,,0.151336233,0,model,2.24E-05,0.140812588,7.46E-05,3.90E-05,0.000448117,0.000268511,4.11E-05,0.001743181,0.531532157,0.313189885,0.011795772,1.78E-05,1.49E-05
8550,text_generation3,45,The encoder E ? is an unsupervised auto - encoder based on Long Short Term Memory Networks ( LSTM ) .,Approach,Encoder,text_generation,3,12,1,0,,0.002997758,0,negative,1.79E-05,0.000859354,0.000369136,6.95E-07,2.06E-06,5.90E-05,7.76E-06,0.001348093,0.022514332,0.974783248,2.13E-05,1.29E-05,4.19E-06
8551,text_generation3,46,"As it is essentially a LSTM - based Seq2Seq model , we name the encoder and decoder of the autoencoder "" source - encoder "" and "" source - decoder "" .",Approach,Encoder,text_generation,3,13,1,0,,8.14E-05,0,negative,3.31E-05,0.000638115,0.000408676,1.04E-06,7.86E-06,2.47E-05,4.37E-06,0.000177099,0.006087209,0.992570664,1.00E-05,3.35E-05,3.57E-06
8552,text_generation3,47,"To be specific , the encoder E ?",Approach,Encoder,text_generation,3,14,1,0,,5.55E-06,0,negative,3.43E-06,1.27E-05,3.83E-06,2.54E-08,9.87E-08,1.96E-06,2.61E-07,2.94E-05,0.000123504,0.999815837,8.32E-07,8.07E-06,6.90E-08
8553,text_generation3,48,"receives the source text x = {x 1 , x 2 , ... , x n } , and encodes it to an internal representation h , and then decodes h to a new sequencex = {x 1 ,x 2 , ... ,x n } for the reconstruction of the input .",Approach,Encoder,text_generation,3,15,1,0,,7.31E-06,0,negative,1.74E-05,8.52E-05,0.000106315,8.71E-08,9.81E-07,2.20E-06,7.88E-07,1.90E-05,0.001166778,0.998556102,5.90E-06,3.87E-05,5.50E-07
8554,text_generation3,49,We extract the hidden state h as the semantic representation .,Approach,Encoder,text_generation,3,16,1,0,,1.98E-05,0,negative,4.60E-06,8.46E-05,1.34E-05,6.19E-08,4.06E-07,3.49E-06,4.03E-07,9.55E-05,0.002615452,0.997173816,2.19E-06,5.68E-06,4.19E-07
8555,text_generation3,50,The encoder E ?,Approach,Encoder,text_generation,3,17,1,0,,2.40E-05,0,negative,3.01E-06,1.78E-05,5.28E-06,3.00E-08,1.10E-07,4.68E-06,6.96E-07,7.99E-05,0.00047508,0.999402977,2.05E-06,8.22E-06,2.34E-07
8556,text_generation3,51,"is trained to reduce the reconstruction loss , whose loss function is defined as follows :",Approach,Encoder,text_generation,3,18,1,0,,1.44E-05,0,negative,1.34E-05,0.000115123,2.43E-05,6.46E-08,5.31E-07,2.75E-06,4.68E-07,9.36E-05,0.000785264,0.998945061,2.18E-06,1.71E-05,2.60E-07
8557,text_generation3,52,where ?,Approach,Encoder,text_generation,3,19,1,0,,1.90E-07,0,negative,6.95E-07,3.73E-06,3.16E-07,7.32E-08,5.88E-08,2.93E-06,1.85E-07,3.72E-05,6.16E-05,0.999889845,1.39E-06,1.97E-06,8.29E-08
8558,text_generation3,53,refers to the parameters of the encoder E ? .,Approach,Encoder,text_generation,3,20,1,0,,8.37E-07,0,negative,1.76E-06,1.20E-05,5.68E-07,5.68E-08,1.43E-07,3.38E-06,2.58E-07,0.000101567,9.21E-05,0.999784011,3.50E-07,3.73E-06,8.58E-08
8559,text_generation3,54,Decoder,Approach,,text_generation,3,21,1,0,,0.025314573,0,model,3.43E-05,0.106377803,9.42E-05,7.59E-05,0.001165632,0.000385439,7.30E-05,0.00160363,0.447993937,0.436654416,0.005489584,2.91E-05,2.31E-05
8560,text_generation3,55,"Similar to the encoder , our decoder D ? is also a LSTM - based auto - encoder .",Approach,Decoder,text_generation,3,22,1,0,,3.91E-05,0,negative,3.85E-05,0.000437091,0.000291267,2.13E-07,2.63E-06,1.19E-05,2.57E-06,0.000144293,0.004033307,0.995007902,1.77E-06,2.75E-05,1.04E-06
8561,text_generation3,56,"However , as there is no target text provided in the testing stage , we propose the customized implementation , which is illustrated in Section 2.5 .",Approach,Decoder,text_generation,3,23,1,0,,4.28E-05,0,negative,9.56E-05,0.000444595,4.19E-05,7.56E-07,1.25E-05,7.41E-06,1.60E-06,7.10E-05,0.000538152,0.998675319,1.35E-06,0.000109343,5.24E-07
8562,text_generation3,57,"Here in the introduction of the decoder , we do not provide the testing details .",Approach,Decoder,text_generation,3,24,1,0,,3.09E-07,0,negative,1.21E-06,1.56E-06,8.98E-08,4.31E-07,4.89E-07,3.15E-06,6.52E-08,1.39E-05,8.27E-06,0.999969752,5.33E-08,9.56E-07,2.29E-08
8563,text_generation3,58,"Similarly , we name the encoder and decoder of the auto - encoder "" target - encoder "" and "" targetdecoder "" .",Approach,Decoder,text_generation,3,25,1,0,,9.60E-07,0,negative,5.62E-07,1.38E-05,1.64E-06,8.10E-08,5.02E-07,4.73E-06,2.26E-07,4.50E-05,0.000139293,0.999792445,1.49E-07,1.41E-06,1.06E-07
8564,text_generation3,59,"The target - encoder receives the target y = {y 1 , y 2 , ... , y n } and encodes it to a utterancelevel semantic representation s , and then decodes s to a new sequence to approximate the target text .",Approach,Decoder,text_generation,3,26,1,0,,3.87E-05,0,negative,6.15E-06,0.000269658,8.52E-05,4.16E-08,9.31E-07,2.76E-06,7.60E-07,4.34E-05,0.005820131,0.993754108,2.13E-06,1.42E-05,5.40E-07
8565,text_generation3,60,The loss function is identical to that of the encoder :,Approach,Decoder,text_generation,3,27,1,0,,7.46E-05,0,negative,3.93E-06,0.000127517,1.17E-05,5.26E-08,3.04E-07,1.24E-05,1.34E-06,0.00048606,0.001443229,0.99790677,8.12E-07,5.51E-06,3.76E-07
8566,text_generation3,61,Mapping Module,Approach,,text_generation,3,28,1,0,,0.006021913,0,negative,3.71E-05,0.09821462,0.000246894,9.52E-06,0.000812445,0.000134477,5.25E-05,0.000701602,0.356721035,0.540587442,0.002429749,4.32E-05,9.45E-06
8567,text_generation3,62,"As our model is constructed for dialogue generation , we design the mapping module to ensure that the generated response is semantically consistent with the source .",Approach,Mapping Module,text_generation,3,29,1,0,,1.96E-06,0,negative,4.81E-05,4.23E-05,9.17E-06,2.98E-08,4.75E-07,1.14E-06,2.63E-07,1.04E-05,0.000709237,0.999165377,1.83E-07,1.31E-05,1.90E-07
8568,text_generation3,63,There are many matching models that can be used to learn such dependency relations .,Approach,Mapping Module,text_generation,3,30,1,0,,5.74E-07,0,negative,1.83E-06,3.46E-06,7.64E-07,2.61E-08,4.31E-08,1.52E-06,4.82E-07,1.13E-05,1.69E-05,0.999932787,2.22E-05,8.25E-06,4.05E-07
8569,text_generation3,64,"For simplicity , we only use a simple feedforward network for implementation .",Approach,Mapping Module,text_generation,3,31,1,0,,4.43E-05,0,negative,3.97E-05,5.36E-05,4.58E-06,6.55E-08,6.74E-07,1.74E-06,4.08E-07,2.56E-05,0.000319007,0.999539266,1.53E-07,1.49E-05,2.13E-07
8570,text_generation3,65,The mapping module M ?,Approach,Mapping Module,text_generation,3,32,1,0,,6.17E-06,0,negative,2.86E-06,1.83E-06,1.35E-06,2.37E-09,2.47E-08,5.44E-07,2.17E-07,5.02E-06,4.95E-05,0.999933119,1.37E-07,5.35E-06,6.23E-08
8571,text_generation3,66,transforms the source semantic representation h to a new representation t.,Approach,Mapping Module,text_generation,3,33,1,0,,2.03E-06,0,negative,2.67E-05,5.56E-06,1.08E-05,1.22E-08,1.94E-07,3.37E-07,1.85E-07,2.35E-06,5.08E-05,0.999887363,1.10E-07,1.55E-05,8.89E-08
8572,text_generation3,67,"To be specific , we implement a multi - layer perceptron ( MLP ) g ( ) for M ? and train it by minimizing the L2 - norm loss J 3 ( ? ) of the transformed representation t and the semantic representation of target response s:",Approach,Mapping Module,text_generation,3,34,1,0,,2.47E-06,0,negative,2.71E-05,3.14E-05,9.11E-06,2.57E-08,4.48E-07,2.06E-06,7.48E-07,2.61E-05,0.000145902,0.999739642,1.23E-07,1.72E-05,1.87E-07
8573,text_generation3,68,Training and Testing,,,text_generation,3,0,1,0,,0.026655859,0,negative,3.30E-05,0.000368934,1.44E-05,1.02E-05,1.99E-05,0.000346232,0.000227248,0.003785691,7.43E-05,0.994010902,0.000930828,0.000170483,7.84E-06
8574,text_generation3,69,"In the testing stage , given an input utterance , the encoder E ? , the decoder D ? , and the matching module M ?",Training and Testing,Training and Testing,text_generation,3,1,1,0,,0.001404171,0,negative,0.000193202,1.38E-05,0.002796059,1.04E-06,1.23E-06,0.016247775,0.003421946,0.010909292,2.39E-05,0.966096243,2.52E-05,0.000251845,1.85E-05
8575,text_generation3,70,work together to produce a dialogue response .,Training and Testing,Training and Testing,text_generation,3,2,1,0,,0.000809238,0,negative,0.000136951,2.56E-06,0.001082477,9.39E-06,4.03E-06,0.032196074,0.005374857,0.003840144,8.99E-06,0.957009668,2.00E-05,0.000254148,6.07E-05
8576,text_generation3,71,The source - encoder first receives the input x and encodes it to a semantic representation h of the source utterance .,Training and Testing,Training and Testing,text_generation,3,3,1,0,,0.008852841,0,negative,0.00125635,0.000107418,0.11522627,3.02E-06,3.68E-06,0.021980591,0.008580859,0.012483066,0.000546804,0.839045605,5.18E-05,0.000562941,0.000151553
8577,text_generation3,72,"Then , the mapping module transforms h tot , a target response representation .",Training and Testing,Training and Testing,text_generation,3,4,1,0,,0.015803121,0,negative,0.000867181,4.19E-05,0.076151799,1.18E-06,1.41E-06,0.023582685,0.004044326,0.012151101,0.000628503,0.882253022,1.63E-05,0.000200907,5.97E-05
8578,text_generation3,73,"Finally , t is sent to the target - decoder for response generation .",Training and Testing,Training and Testing,text_generation,3,5,1,0,,0.010928071,0,negative,0.00023965,1.94E-05,0.008936675,2.29E-07,4.30E-07,0.021154289,0.004100779,0.016168774,9.84E-05,0.949031298,8.62E-06,0.000223049,1.83E-05
8579,text_generation3,74,"In the training stage , besides the auto - encoder loss and the mapping loss , we also use an end - toend loss J 4 ( ? , ? , ? ) :",Training and Testing,Training and Testing,text_generation,3,6,1,0,,0.112950853,0,negative,0.000359315,8.92E-05,0.002167066,3.54E-06,2.68E-06,0.254810162,0.009230379,0.308666738,0.000184249,0.424267562,7.83E-06,0.000134804,7.65E-05
8580,text_generation3,75,"where x is the source input , y is the target response , and T is the length of response sequence .",Training and Testing,Training and Testing,text_generation,3,7,1,0,,0.000107363,0,negative,4.84E-05,4.24E-06,0.000106732,1.03E-06,6.35E-07,0.027094244,0.001476783,0.021115372,1.88E-05,0.950061085,9.38E-06,4.74E-05,1.58E-05
8581,text_generation3,76,The model learns to generate ?,Training and Testing,Training and Testing,text_generation,3,8,1,0,,0.008134492,0,negative,3.33E-05,4.48E-06,7.67E-05,1.67E-06,5.99E-07,0.110058776,0.002020902,0.059869827,2.50E-05,0.827856771,3.31E-06,3.08E-05,1.79E-05
8582,text_generation3,77,"to approximate y by minimizing the reconstruction losses J 1 ( ? ) and J 2 ( ? ) , the mapping loss J 3 ( ? ) , and the end - to - end loss J 4 ( ? , ? , ? ) .",Training and Testing,Training and Testing,text_generation,3,9,1,0,,0.002792731,0,negative,4.37E-05,7.25E-06,0.000122224,1.90E-06,1.10E-06,0.053239734,0.001767767,0.035844996,1.60E-05,0.908864179,9.73E-06,5.69E-05,2.45E-05
8583,text_generation3,78,The details are illustrated below :,Training and Testing,Training and Testing,text_generation,3,10,1,0,,0.000106043,0,negative,0.000130184,1.17E-06,0.000265832,1.72E-06,4.23E-06,0.003590998,0.000778536,0.000509523,3.62E-06,0.994590308,6.29E-07,0.000115975,7.27E-06
8584,text_generation3,79,"where J refers to the total loss , and ? 1 , ? 2 , and ?",Training and Testing,Training and Testing,text_generation,3,11,1,0,,0.000210155,0,negative,0.000199609,3.85E-06,0.000214367,6.10E-07,1.05E-06,0.02397868,0.00180329,0.009819121,7.99E-06,0.963717025,2.07E-06,0.000243667,8.67E-06
8585,text_generation3,80,3 are hyperparameters .,Training and Testing,Training and Testing,text_generation,3,12,1,0,,0.048593223,0,hyperparameters,2.04E-05,8.55E-06,2.45E-05,9.55E-07,3.07E-07,0.352052998,0.003277482,0.584829041,2.08E-05,0.059719126,1.69E-06,1.29E-05,3.12E-05
8586,text_generation3,81,Experiment,,,text_generation,3,0,1,0,,0.02032449,0,negative,2.65E-05,7.60E-05,1.08E-06,1.79E-06,9.59E-07,0.000215726,5.34E-05,0.005112434,4.95E-05,0.993225469,0.001033374,0.000199963,3.87E-06
8587,text_generation3,82,We conduct experiments on a high - quality dialogue dataset called DailyDialog built by .,Experiment,Experiment,text_generation,3,1,1,0,,0.052958223,0,negative,0.002499986,0.01068311,0.00485881,0.000178252,0.001991308,0.003171701,0.010507024,0.041607899,0.000208117,0.903445225,0.000705996,0.019566066,0.000576505
8588,text_generation3,83,The dialogues in the dataset reflect our daily communication and cover various topics about our daily life .,Experiment,Experiment,text_generation,3,2,1,0,,0.023361361,0,negative,0.000173866,0.000228415,0.000218276,0.000548117,0.002223599,0.003073054,0.001453937,0.008186064,2.42E-05,0.983314002,8.64E-05,0.000368045,0.00010205
8589,text_generation3,84,"We split the dataset into three parts with 36.3 K pairs for training , 11.1 K pairs for validation , and 11.1 K pairs for testing .",Experiment,Experiment,text_generation,3,3,1,0,,0.141808921,0,negative,0.000539765,0.001230382,0.000258409,0.000446401,0.000882383,0.046164204,0.007401176,0.43351627,0.00016399,0.508296362,6.96E-05,0.000547455,0.000483637
8590,text_generation3,85,Experimental Details,,,text_generation,3,0,1,0,,0.000926096,0,negative,3.88E-05,0.000666525,7.54E-06,4.78E-05,1.78E-05,0.000423732,0.000128389,0.003714383,0.000185199,0.993368916,0.001272079,0.000120585,8.23E-06
8591,text_generation3,86,"For dialogue generation , we set the maximum length to 15 words for each generated sentence .",Experimental Details,Experimental Details,text_generation,3,1,1,1,hyperparameters,0.98461191,1,experimental-setup,4.87E-06,9.62E-06,5.72E-06,6.81E-07,3.99E-07,0.664322226,0.000783544,0.327132145,5.67E-06,0.007722982,2.53E-06,4.75E-06,4.86E-06
8592,text_generation3,87,"Based on the performance on the validation set , we set the hidden size to 512 , embedding size to 64 and vocabulary size to 40 K for baseline models and the proposed model .",Experimental Details,Experimental Details,text_generation,3,2,1,1,hyperparameters,0.987653074,1,experimental-setup,9.37E-06,1.06E-05,4.48E-06,1.79E-06,6.67E-07,0.676276089,0.000756281,0.316914351,5.81E-06,0.006002815,2.65E-06,6.77E-06,8.36E-06
8593,text_generation3,88,"The parameters are updated by the Adam algorithm ( Kingma and Ba , 2014 ) and initialized by sampling from the uniform distribution ( [? 0.1 , 0.1 ] ) .",Experimental Details,Experimental Details,text_generation,3,3,1,1,hyperparameters,0.991119343,1,experimental-setup,8.11E-06,1.39E-05,5.00E-06,1.22E-06,4.78E-07,0.609320915,0.000438254,0.382971482,1.18E-05,0.007217375,2.36E-06,3.37E-06,5.72E-06
8594,text_generation3,89,The initial learning rate is 0.002 and the model is trained in minibatches with a batch size of 256 . ? 1 and ?,Experimental Details,Experimental Details,text_generation,3,4,1,1,hyperparameters,0.992928353,1,experimental-setup,5.86E-06,5.98E-06,3.79E-06,1.24E-06,4.02E-07,0.700072964,0.000752318,0.29318776,4.31E-06,0.005950143,3.01E-06,5.30E-06,6.93E-06
8595,text_generation3,90,3 are set to 1 and ?,Experimental Details,Experimental Details,text_generation,3,5,1,0,,0.535694622,1,experimental-setup,1.64E-05,1.03E-05,5.10E-06,2.01E-06,7.57E-07,0.627881169,0.000644685,0.357714796,1.05E-05,0.013685696,5.62E-06,1.36E-05,9.45E-06
8596,text_generation3,91,2 is set to 0.01 in Equation .,Experimental Details,Experimental Details,text_generation,3,6,1,0,,0.713332079,1,experimental-setup,1.88E-05,2.11E-05,5.37E-06,3.37E-06,1.36E-06,0.536817114,0.000626149,0.452047703,1.56E-05,0.010412484,4.67E-06,1.20E-05,1.42E-05
8597,text_generation3,92,"It is important to note that for a fair comparison , we reimplement the baseline models with the best settings on the validation set .",Experimental Details,Experimental Details,text_generation,3,7,1,0,,0.013011605,0,negative,0.000395875,0.000120975,0.000465479,5.10E-06,2.27E-05,0.239810449,0.002047277,0.080900164,5.97E-05,0.675768296,5.16E-06,0.000387592,1.13E-05
8598,text_generation3,93,"After fixing the hyperparameters , we combine the training and validation sets together as a larger training set to produce the final model .",Experimental Details,Experimental Details,text_generation,3,8,1,0,,0.511149484,1,negative,0.001198346,0.001015812,0.001653983,1.20E-05,3.28E-05,0.290299194,0.002005471,0.256868863,0.001764368,0.444641638,3.54E-05,0.000396124,7.61E-05
8599,text_generation3,94,Results,,,text_generation,3,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
8600,text_generation3,95,"We use BLEU , to compare the performance of different models , and use the widely - used BLEU - 4 as our main BLEU score .",Results,Results,text_generation,3,1,1,0,,0.008650737,0,negative,0.002201586,5.91E-05,0.001356229,2.52E-05,3.08E-05,0.000524156,0.002640556,0.002059106,4.35E-05,0.964224012,5.24E-05,0.026224092,0.000559288
8601,text_generation3,96,The results are shown in .,Results,Results,text_generation,3,2,1,0,,0.091387577,0,negative,0.006370056,9.27E-07,0.000203698,2.53E-07,4.87E-07,9.53E-06,0.000667907,3.22E-05,6.94E-07,0.540871796,5.40E-06,0.451813386,2.36E-05
8602,text_generation3,97,The proposed AEM model significantly outperforms the Seq2Seq model .,Results,Results,text_generation,3,3,1,1,results,0.982630347,1,results,0.008146529,8.25E-07,6.73E-05,1.32E-06,5.49E-07,6.95E-06,0.003114938,3.60E-05,2.93E-07,0.010759692,8.24E-06,0.977602667,0.000254772
8603,text_generation3,98,It demonstrates the effectiveness of utterance - level dependency on improving the quality of generated text .,Results,Results,text_generation,3,4,1,1,results,0.883385639,1,results,0.032567846,4.35E-06,0.000394382,1.48E-06,2.55E-06,1.03E-05,0.001047743,3.67E-05,2.74E-06,0.283222441,1.49E-05,0.682601562,9.29E-05
8604,text_generation3,99,"Furthermore , we find that the utterance - level dependency also benefits the learning of word - level dependency .",Results,Results,text_generation,3,5,1,0,,0.979362357,1,results,0.224121172,5.33E-05,0.000372647,7.87E-05,2.81E-05,0.000256715,0.010528181,0.001548173,6.15E-05,0.090865321,5.71E-05,0.670003039,0.002025998
8605,text_generation3,100,The improvement from the AEM model to the AEM + Attention model 2 is 0.68 BLEU - 4 point .,Results,Results,text_generation,3,6,1,1,results,0.946192959,1,results,0.027273344,2.47E-06,0.000237375,2.94E-06,1.98E-06,1.49E-05,0.005431826,7.89E-05,1.23E-06,0.028921895,7.07E-06,0.937468767,0.000557278
8606,text_generation3,101,"It is much more obvious than the improvement from the Seq2Seq model to the Seq2Seq + Attention , which is 0.29 BLEU - 4 point .",Results,Results,text_generation,3,7,1,0,,0.154054479,0,results,0.112068405,2.64E-06,0.000205376,1.10E-05,3.85E-06,3.40E-05,0.006535947,0.000135386,2.56E-06,0.053476802,9.00E-06,0.826781466,0.000733573
8607,text_generation3,102,"We also report the diversity of the generated responses by calculating the number of distinct unigrams , bigrams , and trigrams .",Results,Results,text_generation,3,8,1,0,,0.020424193,0,negative,0.005820708,1.74E-05,0.000300709,2.46E-06,7.05E-06,2.99E-05,0.000411867,0.000179254,1.24E-05,0.899654829,6.10E-06,0.093504299,5.30E-05
8608,text_generation3,103,The results are shown in .,Results,Results,text_generation,3,9,1,0,,0.056754242,0,negative,0.005830499,7.93E-07,0.000147944,2.08E-07,5.19E-07,7.19E-06,0.000611842,2.74E-05,6.34E-07,0.556694104,2.56E-06,0.436650323,2.60E-05
8609,text_generation3,104,We find that the AEM model achieves significant improvement on the diversity of generated text .,Results,Results,text_generation,3,10,1,1,results,0.959019005,1,results,0.025872603,1.59E-06,5.66E-05,3.47E-06,1.13E-06,1.50E-05,0.003738921,0.000100449,9.45E-07,0.01651184,5.81E-06,0.953229069,0.000462527
8610,text_generation3,105,The number of unique trigram of the AEM model is almost six times more than that of the Seq2Seq model .,Results,Results,text_generation,3,11,1,0,,0.79558443,1,results,0.138993069,1.86E-05,0.000480812,2.18E-05,2.32E-05,0.000114792,0.008574907,0.000615492,1.88E-05,0.13719252,1.55E-05,0.711602401,0.002328116
8611,text_generation3,106,"Also , it should be noticed that the attention mechanism performs almost the same compared to the AEM model ( 31.2 K vs. 34.6 K in terms of Dist - 3 ) , which indicates that the utterance - level dependency and the word - level dependency are both indispensable for dialogue generation .",Results,Results,text_generation,3,12,1,1,results,0.969440303,1,results,0.043379004,1.18E-06,5.53E-05,1.57E-06,8.12E-07,8.66E-06,0.002798248,5.32E-05,5.95E-07,0.025914863,3.55E-06,0.927595895,0.000187185
8612,text_generation3,107,"Therefore , by combining the two dependencies together , the AEM + Attention model achieves the best results .",Results,Results,text_generation,3,13,1,1,results,0.940026383,1,results,0.022118702,6.01E-07,4.40E-05,8.42E-07,5.44E-07,6.14E-06,0.00222335,3.69E-05,2.76E-07,0.022523153,1.46E-06,0.952901445,0.000142578
8613,text_generation3,108,Such improvements are expected .,Results,Results,text_generation,3,14,1,0,,0.002270078,0,negative,0.007585654,3.85E-06,0.000210363,4.96E-06,3.90E-06,3.55E-05,0.000254036,0.000183779,2.09E-05,0.951515877,6.71E-06,0.040005278,0.000169165
8614,text_generation3,109,"With the increase of the relevance of the generated text , it gets harder for the model to generate repeated responses .",Results,Results,text_generation,3,15,1,0,,0.080686804,0,negative,0.11629119,1.14E-05,0.000519495,6.50E-06,7.11E-06,3.84E-05,0.000868313,0.000180506,1.95E-05,0.62633828,1.77E-05,0.2554383,0.000263303
8615,text_generation3,110,"In our experimental results , the number of repetitive "" I do n't know "" in the AEM + Attention model is reduced by 50 % compared to the Seq2Seq model .",Results,Results,text_generation,3,16,1,0,,0.919273971,1,results,0.067072929,5.02E-06,0.00013296,2.77E-06,3.97E-06,1.52E-05,0.00361521,0.00010174,2.16E-06,0.062744026,3.76E-06,0.865704492,0.000595747
8616,text_generation3,111,"For dialogue generation , human evaluation is more convincing , so we also report human evaluation results on the test set .",Results,Results,text_generation,3,17,1,0,,0.181181239,0,results,0.002898932,1.76E-06,0.000140266,5.04E-07,2.02E-06,8.60E-06,0.001522644,4.01E-05,5.05E-07,0.44129706,7.99E-06,0.553981577,9.80E-05
8617,text_generation3,112,"We randomly choose 100 utterances in daily communication style for the human evaluation , each of which is sent to different models to generate responses .",Results,Results,text_generation,3,18,1,0,,0.064929997,0,negative,0.00428383,9.55E-05,0.000286891,3.43E-05,7.86E-05,0.0009771,0.003501066,0.012853422,5.39E-05,0.959511096,9.06E-06,0.016460305,0.001854907
8618,text_generation3,113,The results are distributed to the annotators who have no knowledge about which model the sentence is from .,Results,Results,text_generation,3,19,1,0,,0.048655655,0,negative,0.003984558,3.33E-05,0.001211752,5.58E-06,1.28E-05,3.90E-05,0.000308495,0.000285989,9.31E-05,0.963578581,5.81E-06,0.030087574,0.000353449
8619,text_generation3,114,All annotators have linguistic background .,Results,Results,text_generation,3,20,1,0,,0.161206757,0,negative,0.002021085,4.28E-05,0.000759289,4.13E-05,3.98E-05,0.000312702,0.000818466,0.002271466,8.48E-05,0.980397126,1.18E-05,0.011962876,0.001236469
8620,text_generation3,115,They are asked to score the generated responses in terms of fluency and coherence .,Results,Results,text_generation,3,21,1,0,,0.001142663,0,negative,0.00102787,1.42E-05,0.000292545,2.65E-06,5.18E-06,2.87E-05,9.87E-05,0.000286213,3.90E-05,0.990858893,4.12E-06,0.007215369,0.000126526
8621,text_generation3,116,Fluency rep - resents whether each sentence is incorrect grammar .,Results,Results,text_generation,3,22,1,0,,0.150567405,0,negative,0.02116819,2.63E-05,0.02559316,2.44E-05,2.74E-05,8.60E-05,0.001390575,0.000250888,6.18E-05,0.787690387,4.87E-05,0.161907852,0.001724378
8622,text_generation3,117,Coherence evaluates whether the generated response is relevant to the input .,Results,Results,text_generation,3,23,1,0,,0.172389879,0,negative,0.004682794,7.12E-05,0.014883048,2.18E-05,2.14E-05,0.000154495,0.000423344,0.000901387,0.000442388,0.962942139,4.09E-05,0.013953024,0.001462083
8623,text_generation3,118,The score ranges from 1 to 10 ( 1 is very bad and 10 is very good ) .,Results,Results,text_generation,3,24,1,0,,0.022763441,0,negative,0.002230064,3.84E-05,0.000330281,1.29E-05,1.16E-05,0.000807151,0.001303339,0.010961857,0.000102189,0.969412695,9.81E-06,0.013265246,0.001514467
8624,text_generation3,119,"To evaluate the over all performance , we use the geometric mean of fluency and coherence as the final evaluation metric .",Results,Results,text_generation,3,25,1,0,,0.004536948,0,negative,0.003573582,2.62E-05,0.000345564,2.30E-06,7.39E-06,7.00E-05,0.000387031,0.00101004,2.25E-05,0.975317868,1.06E-06,0.019100695,0.000135699
8625,text_generation3,120,shows the results of human evaluation .,Results,Results,text_generation,3,26,1,1,results,0.007041139,0,negative,0.00146565,1.57E-06,0.00024096,1.06E-06,3.04E-06,1.78E-05,0.000180542,0.000100116,4.83E-06,0.980267147,8.18E-07,0.01758152,0.000134903
8626,text_generation3,121,The inter-annotator agreement is satisfactory considering the difficulty of human evaluation .,Results,Results,text_generation,3,27,1,1,results,0.84985744,1,results,0.006278717,1.77E-06,3.96E-05,5.79E-06,3.49E-06,3.14E-05,0.005697849,0.000227118,1.42E-06,0.110637195,3.59E-06,0.875308273,0.001763754
8627,text_generation3,122,"The Pearson 's correlation coefficient is 0.69 on coherence and 0.57 on fluency , with p < 0.0001 .",Results,Results,text_generation,3,28,1,1,results,0.791609724,1,results,0.16674173,6.05E-05,0.000515802,0.000551341,0.000126038,0.000689894,0.015054454,0.004909846,9.11E-05,0.356487429,1.86E-05,0.434467469,0.020285844
8628,text_generation3,123,"First , it is clear that the AEM model outperforms the Seq2Seq model with a large margin , which proves the effectiveness of the AEM model on generating high quality responses .",Results,Results,text_generation,3,29,1,1,results,0.954270116,1,results,0.068979255,2.10E-06,4.83E-05,4.48E-06,2.91E-06,1.17E-05,0.005917851,7.59E-05,7.05E-07,0.045609888,1.10E-06,0.878655541,0.000690224
8629,text_generation3,124,"Second , it is interesting to note that with the attention mechanism , the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model .",Results,Results,text_generation,3,30,1,1,results,0.822514168,1,ablation-analysis,0.740479033,2.83E-06,7.08E-05,8.47E-06,3.88E-06,1.40E-05,0.001530328,8.53E-05,2.37E-06,0.044549182,6.51E-07,0.21298052,0.000272541
8630,text_generation3,125,It suggests that the utterance - level dependency greatly benefits the learning of wordlevel dependency .,Results,Results,text_generation,3,31,1,0,,0.259309949,0,negative,0.074609014,1.00E-05,0.000422457,4.83E-06,7.96E-06,3.11E-05,0.000633505,0.000227889,1.77E-05,0.798045306,1.54E-06,0.125685698,0.000302927
8631,text_generation3,126,"Therefore , it is expected that the AEM + Attention model achieves the best G-score .",Results,Results,text_generation,3,32,1,1,results,0.221836953,0,negative,0.012498949,2.44E-06,9.38E-05,8.64E-07,2.14E-06,8.86E-06,0.000487818,9.51E-05,2.43E-06,0.667280246,7.10E-07,0.319408231,0.000118373
8632,text_generation3,127,shows the examples generated by the AEM model and the Seq2Seq model .,Results,Results,text_generation,3,33,1,0,,0.006983767,0,negative,0.001490638,1.78E-06,0.000165796,8.66E-07,2.63E-06,2.51E-05,0.000443511,0.000186625,3.21E-06,0.95799514,5.75E-07,0.03936655,0.000317613
8633,text_generation3,128,"For easy questions ( ex. 4 and ex. 5 ) , they both perform well .",Results,Results,text_generation,3,34,1,0,,0.306567547,0,results,0.012503432,2.34E-06,4.15E-05,1.92E-05,4.92E-06,0.000129924,0.010528601,0.001067493,2.30E-06,0.220782402,3.34E-06,0.75130005,0.003614549
8634,text_generation3,129,"For hard questions ( ex. 1 and ex. 2 ) , the proposed model obviously outperforms the Seq2Seq model .",Results,Results,text_generation,3,35,1,0,,0.929168626,1,results,0.010192479,5.13E-07,1.50E-05,1.29E-06,6.88E-07,8.98E-06,0.005103219,7.51E-05,2.24E-07,0.028246119,5.73E-07,0.955730719,0.000625087
8635,text_generation3,130,It shows that the utterance - level dependency learned by the proposed model is useful for handling complex inputs .,Results,Results,text_generation,3,36,1,0,,0.551605089,1,results,0.072988411,3.90E-06,9.25E-05,1.41E-06,3.03E-06,9.67E-06,0.001273949,7.50E-05,3.41E-06,0.361782344,7.59E-07,0.563521882,0.000243813
8636,text_generation3,131,Error Analysis,Results,,text_generation,3,37,1,0,,0.003812523,0,negative,0.007496266,2.50E-06,0.000147022,6.41E-05,1.59E-05,0.000146683,0.001312783,0.00039149,9.96E-06,0.952325048,2.56E-06,0.035478748,0.002606937
8637,text_generation3,132,"Although our model achieves the best performance , there are still several failure cases .",Results,Error Analysis,text_generation,3,38,1,0,,0.001025438,0,negative,0.145893545,6.26E-06,1.45E-05,1.14E-06,0.000241077,0.000144715,0.000766065,4.61E-06,1.09E-06,0.852621136,1.29E-06,0.000287696,1.68E-05
8638,text_generation3,133,We find that the model performs badly for the inputs with unseen words .,Results,Error Analysis,text_generation,3,39,1,0,,0.037642175,0,negative,0.047303039,1.66E-05,6.47E-06,4.79E-06,0.000224193,0.001061896,0.001662522,4.89E-05,3.65E-06,0.949411535,2.65E-06,0.000192886,6.08E-05
8639,text_generation3,134,"For instance , given "" Bonjour "" as the input , it generates "" Stay out of here "" as the output .",Results,Error Analysis,text_generation,3,40,1,0,,7.63E-07,0,negative,0.000119354,2.07E-06,4.46E-05,3.32E-08,1.35E-05,2.92E-05,6.21E-06,6.97E-07,1.33E-05,0.999769965,6.88E-08,7.30E-07,2.94E-07
8640,text_generation3,135,It shows that the proposed model is sensitive to the unseen utterance representations .,Results,Error Analysis,text_generation,3,41,1,0,,0.001336504,0,negative,0.15150035,1.43E-05,3.41E-05,1.06E-07,2.64E-05,3.42E-05,0.000148209,1.79E-06,1.27E-05,0.848150199,3.76E-07,7.58E-05,1.47E-06
8641,text_generation3,136,"Therefore , we would like to explore more approaches to address this problem in the future work .",Results,Error Analysis,text_generation,3,42,1,0,,6.24E-06,0,negative,0.000545095,6.87E-06,4.70E-06,6.65E-07,3.58E-05,0.000375959,5.59E-05,1.46E-05,9.77E-06,0.998944259,5.30E-07,2.10E-06,3.70E-06
8642,text_generation3,137,"For example , the auto - encoders can be replaced by variational auto-encoders to ensure that the distribution of utterance representations is normal , which has a better generalization ability .",Results,Error Analysis,text_generation,3,43,1,0,,1.92E-05,0,negative,0.000912176,1.12E-05,1.94E-05,8.57E-08,6.01E-06,6.48E-05,1.83E-05,5.96E-06,5.29E-05,0.998906404,2.05E-07,1.69E-06,9.27E-07
8643,text_generation3,138,Conclusion,,,text_generation,3,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
8644,sarcasm_detection1,1,title,,,sarcasm_detection,1,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
8645,sarcasm_detection1,2,CASCADE : Contextual Sarcasm Detection in Online Discussion Forums,title,,sarcasm_detection,1,1,1,1,research-problem,0.998681037,1,research-problem,4.60E-07,5.21E-05,7.92E-07,1.40E-05,9.67E-06,2.78E-06,2.89E-05,9.10E-06,1.38E-06,0.007171756,0.992705563,1.75E-06,1.76E-06
8646,sarcasm_detection1,3,abstract,,,sarcasm_detection,1,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
8647,sarcasm_detection1,4,"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .",abstract,abstract,sarcasm_detection,1,1,1,1,research-problem,0.711295213,1,research-problem,4.48E-08,4.26E-06,1.23E-08,1.78E-06,7.98E-07,3.48E-07,4.57E-07,1.06E-06,1.65E-07,0.02005568,0.979935322,2.19E-08,5.24E-08
8648,sarcasm_detection1,5,"However , a sarcastic sentence can be expressed with contextual presumptions , background and commonsense knowledge .",abstract,abstract,sarcasm_detection,1,2,1,0,,0.000583463,0,negative,2.60E-07,4.88E-05,5.48E-08,1.95E-05,2.78E-05,4.52E-06,5.82E-07,9.62E-06,3.68E-06,0.787219969,0.212665064,5.68E-08,7.44E-08
8649,sarcasm_detection1,6,"In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .",abstract,abstract,sarcasm_detection,1,3,1,1,research-problem,0.876488744,1,research-problem,2.56E-05,0.075475069,0.000226694,5.80E-05,0.000496465,3.63E-05,6.00E-05,0.000256215,0.002479116,0.065187918,0.855669248,1.82E-05,1.13E-05
8650,sarcasm_detection1,7,"For the latter , CASCADE aims at extracting contextual information from the discourse of a discussion thread .",abstract,abstract,sarcasm_detection,1,4,1,0,,0.550362588,1,research-problem,1.73E-06,0.00245119,2.43E-05,8.26E-06,3.40E-05,8.42E-06,5.23E-06,3.69E-05,0.000257989,0.121755082,0.875415184,9.01E-07,8.15E-07
8651,sarcasm_detection1,8,"Also , since the sarcastic nature and form of expression can vary from person to person , CASCADE utilizes user embeddings that encode stylometric and personality features of the users .",abstract,abstract,sarcasm_detection,1,5,1,0,,0.441317011,0,negative,5.40E-05,0.360437709,0.000558957,1.22E-05,0.000455916,7.95E-05,1.45E-05,0.000796115,0.08481674,0.510501141,0.042262125,9.04E-06,2.07E-06
8652,sarcasm_detection1,9,"When used along with content - based feature extractors such as Convolutional Neural Networks ( CNNs ) , we see a significant boost in the classification performance on a large Reddit corpus .",abstract,abstract,sarcasm_detection,1,6,1,0,,0.053419386,0,negative,0.000293975,0.005487719,1.83E-05,3.75E-05,0.000230609,3.17E-05,0.000201324,0.000421158,0.000120518,0.552376384,0.439829372,0.000937473,1.40E-05
8653,sarcasm_detection1,10,Introduction,,,sarcasm_detection,1,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
8654,sarcasm_detection1,11,Sarcasm is a linguistic tool that uses irony to express contempt .,Introduction,Introduction,sarcasm_detection,1,1,1,0,,0.006979476,0,research-problem,2.55E-05,0.002042763,2.01E-05,0.002373972,0.010487255,0.000227055,0.000167343,3.56E-05,0.000352587,0.308129881,0.676094467,7.59E-06,3.60E-05
8655,sarcasm_detection1,12,Its figurative nature poses a great challenge for affective systems performing sentiment analysis .,Introduction,Introduction,sarcasm_detection,1,2,1,0,,0.348843176,0,research-problem,5.97E-06,0.000544481,1.33E-06,3.51E-05,0.000132558,1.17E-05,2.36E-05,9.67E-06,9.82E-05,0.11338272,0.885747242,3.99E-06,3.55E-06
8656,sarcasm_detection1,13,"Previous research in automated sarcasm detection has primarily focused on lexical , pragmatic cues found in sentences .",Introduction,Introduction,sarcasm_detection,1,3,1,0,,0.611396046,1,research-problem,1.83E-06,0.000209678,4.03E-07,9.41E-06,3.54E-05,6.50E-06,1.36E-05,6.91E-06,2.89E-05,0.076440585,0.923243524,1.51E-06,1.77E-06
8657,sarcasm_detection1,14,"Interjections , punctuations , sentimental shifts , etc. , have been considered as major indicators of sarcasm .",Introduction,Introduction,sarcasm_detection,1,4,1,0,,0.009844026,0,negative,1.39E-05,0.000813925,1.38E-06,8.97E-05,0.001041162,8.87E-05,4.02E-05,5.36E-05,0.000176484,0.8612412,0.136429711,5.28E-06,4.74E-06
8658,sarcasm_detection1,15,"When such lexical cues are present in sentences , sarcasm detection can achieve high accuracy .",Introduction,Introduction,sarcasm_detection,1,5,1,0,,0.037273674,0,research-problem,8.62E-06,0.000969185,1.03E-06,1.66E-05,9.26E-05,9.73E-06,2.61E-05,1.65E-05,0.000216352,0.141398042,0.857230009,1.09E-05,4.29E-06
8659,sarcasm_detection1,16,"However , sarcasm is also expressed implicitly , i.e. , without the use of any explicit lexical cues .",Introduction,Introduction,sarcasm_detection,1,6,1,0,,0.023850569,0,negative,1.41E-05,0.005637743,3.79E-06,5.54E-05,0.000934378,6.72E-05,2.67E-05,6.87E-05,0.001232148,0.787666922,0.204283719,5.80E-06,3.31E-06
8660,sarcasm_detection1,17,Such use of sarcasm also relies on the context which involves the presumption of commonsense and background knowledge of an event .,Introduction,Introduction,sarcasm_detection,1,7,1,0,,0.030816695,0,negative,1.17E-05,0.001151112,1.99E-06,0.000284402,0.002393202,0.000131387,3.16E-05,5.57E-05,0.000240387,0.942378255,0.053311732,4.66E-06,3.86E-06
8661,sarcasm_detection1,18,"When it comes to detecting sarcasm in a discussion forum , it may not only require understanding the context of the previous comments but also need necessary external background knowledge about the topic of discussion .",Introduction,Introduction,sarcasm_detection,1,8,1,0,,0.101696535,0,research-problem,7.30E-06,0.000873353,1.17E-06,0.000328089,0.000694756,8.07E-05,3.80E-05,4.38E-05,0.000156602,0.440037431,0.55772392,5.08E-06,9.87E-06
8662,sarcasm_detection1,19,The usage of slangs and informal language also diminishes the reliance on lexical cues .,Introduction,Introduction,sarcasm_detection,1,9,1,0,,0.02897536,0,negative,0.000111209,0.001977642,5.96E-06,7.06E-05,0.003739103,6.40E-05,4.76E-05,2.97E-05,0.000383196,0.982065118,0.011479181,2.44E-05,2.32E-06
8663,sarcasm_detection1,20,This particular type of sarcasm is tough to detect .,Introduction,Introduction,sarcasm_detection,1,10,1,0,,0.004756523,0,negative,3.61E-05,0.002787382,7.02E-06,5.56E-05,0.001971353,0.000104251,3.13E-05,5.64E-05,0.001431696,0.98352817,0.009976711,1.14E-05,2.68E-06
8664,sarcasm_detection1,21,Contextual dependencies for sarcasm can take many forms .,Introduction,Introduction,sarcasm_detection,1,11,1,0,,0.036416384,0,research-problem,5.17E-06,0.001071825,3.27E-06,1.45E-05,0.00016331,2.35E-05,4.38E-05,1.87E-05,0.00028888,0.303579915,0.694776768,6.22E-06,4.17E-06
8665,sarcasm_detection1,22,"As an example , a sarcastic post from Reddit 1 , "" I 'm sure Hillary would 've done that , lmao. "" requires background knowledge about the event , i.e. , Hillary Clinton 's action at the time the post was made .",Introduction,Introduction,sarcasm_detection,1,12,1,0,,0.003585329,0,negative,6.46E-06,0.002560356,2.55E-06,0.000115056,0.002373619,0.000223828,4.57E-05,0.000129161,0.001801713,0.968269807,0.024460375,5.49E-06,5.87E-06
8666,sarcasm_detection1,23,"Similarly , sarcastic posts like "" But atheism , yeah * that 's * a religion ! "" requires the knowledge that topics like atheism often contain argumentative discussions and are more prone towards sarcasm .",Introduction,Introduction,sarcasm_detection,1,13,1,0,,0.008681247,0,negative,7.41E-06,0.000590046,1.12E-06,0.00018549,0.001894259,0.000102623,2.13E-05,3.86E-05,0.000168557,0.975349956,0.021635255,2.84E-06,2.54E-06
8667,sarcasm_detection1,24,"In this work , we attempt the task of sarcasm detection in online discussion forums .",Introduction,Introduction,sarcasm_detection,1,14,1,0,,0.912736322,1,research-problem,0.000100991,0.07610576,9.25E-05,0.00115485,0.021435532,0.00029471,0.001459702,0.000260708,0.002342175,0.231569262,0.664899498,0.000144702,0.000139647
8668,sarcasm_detection1,25,"Particularly , we propose a hybrid network , named CASCADE , that utilizes both content and contextual - information required for sarcasm detection .",Introduction,Introduction,sarcasm_detection,1,15,1,1,model,0.96643548,1,model,1.29E-05,0.068031582,0.000120883,7.20E-07,0.000131367,1.33E-05,1.34E-05,1.99E-05,0.926710838,0.004555005,0.000385243,3.23E-06,1.54E-06
8669,sarcasm_detection1,26,It starts by processing contextual information in two ways .,Introduction,Introduction,sarcasm_detection,1,16,1,0,,0.907213236,1,model,0.000128082,0.194836933,0.000697236,1.26E-05,0.001263081,0.000115988,0.000130961,0.000110963,0.655009794,0.139067439,0.00857728,4.00E-05,9.63E-06
8670,sarcasm_detection1,27,"First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .",Introduction,Introduction,sarcasm_detection,1,17,1,1,model,0.933378889,1,approach,0.000352376,0.669224429,0.000839871,0.000166262,0.083466303,0.000298538,0.000275007,0.000173832,0.163530752,0.080886637,0.00069048,7.31E-05,2.24E-05
8671,sarcasm_detection1,28,"Recent findings suggest that such modeling of the user and their preferences , is highly effective for the given task .",Introduction,Introduction,sarcasm_detection,1,18,1,0,,0.047434054,0,negative,6.09E-06,0.001916923,1.28E-06,3.46E-05,6.98E-05,9.78E-05,5.48E-05,0.000162408,0.0011474,0.599625276,0.396870105,6.86E-06,6.66E-06
8672,sarcasm_detection1,29,"It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .",Introduction,Introduction,sarcasm_detection,1,19,1,1,model,0.936335621,1,model,7.57E-05,0.330967099,0.000746543,7.19E-06,0.002472004,6.26E-05,0.000116465,5.52E-05,0.638276489,0.026089013,0.001095552,2.83E-05,7.88E-06
8673,sarcasm_detection1,30,"Second , it extracts contextual information from the discourse of comments in the discussion forums .",Introduction,Introduction,sarcasm_detection,1,20,1,1,model,0.958049071,1,model,3.60E-05,0.132531903,0.00026489,1.25E-06,0.000685317,2.59E-05,3.82E-05,2.61E-05,0.844329861,0.021567586,0.000478398,1.24E-05,2.26E-06
8674,sarcasm_detection1,31,This is done by document modeling of these consolidated comments belonging to the same forum .,Introduction,Introduction,sarcasm_detection,1,21,1,1,model,0.869143254,1,model,7.93E-05,0.216858154,0.000254825,5.53E-06,0.001756357,4.48E-05,5.13E-05,5.08E-05,0.686730213,0.092203415,0.001938744,2.21E-05,4.36E-06
8675,sarcasm_detection1,32,"We hypothesize that these discourse features would give the important contextual information , background cues along with topical information required for detecting sarcasm .",Introduction,Introduction,sarcasm_detection,1,22,1,0,,0.030028342,0,negative,3.33E-05,0.071301359,2.69E-05,6.68E-05,0.005824145,0.00034185,4.31E-05,0.000409882,0.171590923,0.749624535,0.000721668,1.02E-05,5.28E-06
8676,sarcasm_detection1,33,"After the contextual modeling phase , CASCADE is provided with a comment for sarcasm detection .",Introduction,Introduction,sarcasm_detection,1,23,1,1,model,0.630104574,1,negative,0.000445444,0.287821487,0.000731861,4.90E-05,0.058661323,0.000313285,0.000467531,0.000207455,0.192489576,0.457716973,0.000903808,0.000170435,2.18E-05
8677,sarcasm_detection1,34,It performs content - modeling using a Convolutional Neural Network ( CNN ) to extract its syntactic features .,Introduction,Introduction,sarcasm_detection,1,24,1,1,model,0.920255516,1,model,6.23E-05,0.177120024,0.001099095,5.47E-06,0.00204595,8.91E-05,0.000152932,6.30E-05,0.79497094,0.024008229,0.000354788,1.81E-05,1.01E-05
8678,sarcasm_detection1,35,This CNN representation is then concatenated with the relevant user embedding and discourse features to get the final representation which is used for classification .,Introduction,Introduction,sarcasm_detection,1,25,1,1,model,0.872838326,1,model,4.20E-06,0.033678936,2.38E-05,5.40E-07,7.29E-05,3.50E-05,1.60E-05,8.34E-05,0.934266241,0.031577128,0.00023924,1.43E-06,1.29E-06
8679,sarcasm_detection1,36,The over all contribution of this work can be summarized as :,Introduction,Introduction,sarcasm_detection,1,26,1,0,,0.008615198,0,negative,2.33E-05,0.003634874,1.39E-05,0.000497068,0.003012519,0.000581735,8.62E-05,0.000160391,0.002756896,0.98682912,0.002387502,6.00E-06,1.05E-05
8680,sarcasm_detection1,37,"We propose a novel hybrid sarcasm detector , CASCADE that models content and contextual information .",Introduction,Introduction,sarcasm_detection,1,27,1,0,,0.91681772,1,model,4.49E-05,0.179053904,0.000571456,4.53E-06,0.000737641,6.62E-05,0.00016404,6.63E-05,0.803464013,0.014173192,0.001617505,2.21E-05,1.43E-05
8681,sarcasm_detection1,38,We model stylometric and personality details of users along with discourse features of discussion forums to learn informative contextual representations .,Introduction,Introduction,sarcasm_detection,1,28,1,0,,0.830811084,1,model,1.62E-05,0.309585679,8.04E-05,4.98E-06,0.001332123,6.09E-05,5.43E-05,0.000146922,0.665381234,0.023005998,0.000317771,7.58E-06,5.86E-06
8682,sarcasm_detection1,39,"Experiments on a large Reddit corpus , SARC , demonstrate significant performance improvement over state - of - the - art automated sarcasm detectors .",Introduction,Introduction,sarcasm_detection,1,29,1,0,,0.021321546,0,negative,0.001082266,0.069105017,0.00013341,6.13E-05,0.016321883,0.000264537,0.010462116,0.000313808,0.003481408,0.883399007,0.007157603,0.008075391,0.000142258
8683,sarcasm_detection1,40,"In the remaining paper , Section 2 compares our model to related works ; Section 3 provides the task description and proposed approach ; here , Section 3.3 explains the process of learning contextual features comprising user embeddings and discourse features ; Section 3.6 presents the hybrid prediction model followed by experimentation details and result analysis in Section 4 ; finally , Section 5 draws conclusion .",Introduction,Introduction,sarcasm_detection,1,30,1,0,,0.014126119,0,negative,2.94E-05,0.070133898,2.35E-05,0.000211415,0.014334796,0.000529027,0.00014091,0.000648425,0.020725504,0.892136275,0.001050354,1.94E-05,1.71E-05
8684,sarcasm_detection1,41,Related Work,,,sarcasm_detection,1,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
8685,sarcasm_detection1,221,Experimental Results,,,sarcasm_detection,1,0,1,0,,0.010987357,0,negative,0.000353352,0.00011777,2.16E-05,3.98E-07,1.02E-06,3.15E-05,0.000609269,0.000558707,1.70E-05,0.933411653,0.009870816,0.054999552,7.39E-06
8686,sarcasm_detection1,222,Dataset,Experimental Results,,sarcasm_detection,1,1,1,0,,0.004922675,0,negative,0.000564547,7.28E-07,6.74E-05,9.44E-06,2.34E-07,0.000109129,0.00021136,0.000212516,8.19E-06,0.963854051,2.41E-05,0.034523794,0.000414559
8687,sarcasm_detection1,223,"We perform our experiments on a large - scale self - annotated corpus for sarcasm , SARC 3 .",Experimental Results,Dataset,sarcasm_detection,1,2,1,0,,0.00263173,0,negative,0.000104341,0.000877463,8.07E-05,3.20E-06,0.00031089,4.41E-05,2.26E-05,0.000106214,1.82E-05,0.998108061,2.19E-05,0.000301647,6.24E-07
8688,sarcasm_detection1,224,This dataset contains more than a million examples of sarcastic / non - sarcastic statements made in the social media site Reddit .,Experimental Results,Dataset,sarcasm_detection,1,3,1,0,,0.001497109,0,negative,8.84E-05,5.86E-05,4.14E-05,0.000336519,0.006447856,0.00016966,2.72E-05,2.46E-05,4.13E-06,0.99272652,3.07E-05,4.25E-05,2.08E-06
8689,sarcasm_detection1,225,"Reddit comprises of topic - specific discussion forums , also known as subreddits , each titled by a post .",Experimental Results,Dataset,sarcasm_detection,1,4,1,0,,0.003913056,0,negative,8.90E-05,0.000417023,0.00040663,8.41E-05,0.000886663,0.000200817,0.000216361,0.000103313,2.66E-05,0.991592122,0.00553488,0.00041735,2.52E-05
8690,sarcasm_detection1,226,"In each forum , users communicate either by commenting to the titled post or other 's comments , resulting in a tree - like conversation structure .",Experimental Results,Dataset,sarcasm_detection,1,5,1,0,,0.00390016,0,negative,0.000127975,0.001569958,0.000886468,5.36E-07,2.82E-05,2.29E-05,7.19E-06,9.67E-05,0.001061562,0.995950673,7.58E-05,0.000171092,9.94E-07
8691,sarcasm_detection1,227,"This structure can be unraveled to a linear format , thus creating a discourse of the comments by keeping the topological constraints intact .",Experimental Results,Dataset,sarcasm_detection,1,6,1,0,,0.000955925,0,negative,7.43E-05,0.000106149,2.65E-05,8.31E-08,1.22E-06,4.33E-06,6.20E-07,1.80E-05,0.000244571,0.999443758,2.01E-05,6.04E-05,9.45E-08
8692,sarcasm_detection1,228,Each comment is accompanied with its author details and parent comments ( if any ) which is subsequently used for our contextual processing .,Experimental Results,Dataset,sarcasm_detection,1,7,1,0,,0.000140078,0,negative,1.95E-05,4.91E-05,5.59E-06,1.41E-06,3.77E-05,2.38E-05,1.53E-06,3.76E-05,2.18E-05,0.999780733,1.58E-06,1.97E-05,1.21E-07
8693,sarcasm_detection1,229,It is important to note that almost all comments in the SARC dataset are composed of a single sentence .,Experimental Results,Dataset,sarcasm_detection,1,8,1,0,,0.000358515,0,negative,1.38E-05,1.22E-06,6.84E-07,6.90E-07,8.84E-06,5.75E-06,1.07E-06,3.76E-06,1.28E-07,0.999938679,5.83E-06,1.95E-05,4.50E-08
8694,sarcasm_detection1,230,We consider three variants of the SARC dataset in our experiments .,Experimental Results,Dataset,sarcasm_detection,1,9,1,0,,0.027965496,0,negative,7.81E-05,0.00082474,0.000173078,5.13E-07,5.70E-05,3.03E-05,2.49E-05,0.000174567,2.41E-05,0.997962865,1.69E-05,0.000632445,5.99E-07
8695,sarcasm_detection1,231,Main balanced :,Experimental Results,Dataset,sarcasm_detection,1,10,1,0,,0.024167153,0,negative,0.000656478,0.000268818,0.020628455,3.61E-07,3.76E-06,6.73E-05,2.71E-05,0.000110446,0.000731093,0.976714384,0.000158309,0.000630655,2.85E-06
8696,sarcasm_detection1,232,This is the primary dataset which contains a balanced distribution of both sarcastic and non-sarcastic comments .,Experimental Results,Dataset,sarcasm_detection,1,11,1,0,,0.000550603,0,negative,6.07E-05,4.73E-05,4.71E-05,1.53E-05,0.001515295,4.56E-05,7.41E-06,1.67E-05,4.25E-06,0.998184019,2.47E-06,5.35E-05,4.36E-07
8697,sarcasm_detection1,233,The dataset contains comments from 1246058 users ( 118940 in training and 56118 in testing set ) distributed across 6534 forums ( 3868 in training and 2666 in testing set ) .,Experimental Results,Dataset,sarcasm_detection,1,12,1,0,,0.001736013,0,negative,0.00013771,0.000114735,5.11E-05,3.01E-05,0.003724299,7.79E-05,1.50E-05,3.31E-05,6.57E-06,0.99569123,4.07E-06,0.000113158,1.09E-06
8698,sarcasm_detection1,234,Main imbalanced :,Experimental Results,Dataset,sarcasm_detection,1,13,1,0,,0.005653656,0,negative,0.00099908,0.00012531,0.022192866,4.63E-07,7.24E-06,0.000106346,7.26E-05,0.00012131,0.000113024,0.973977578,5.12E-05,0.002230248,2.75E-06
8699,sarcasm_detection1,235,"To emulate real - world scenarios where the sarcastic comments are typically lesser than non-sarcastic ones , we use an imbalanced version of the Main dataset .",Experimental Results,Dataset,sarcasm_detection,1,14,1,0,,0.000289805,0,negative,3.49E-05,0.000288945,5.79E-05,5.13E-07,7.70E-05,3.30E-05,1.01E-05,0.000144582,1.69E-05,0.999221657,1.17E-06,0.000113179,1.94E-07
8700,sarcasm_detection1,236,"Specifically , we maintain a 20 ? 80 ratio ( approx . ) between the sarcastic and non-sarcastic comments in both training / testing sets .",Experimental Results,Dataset,sarcasm_detection,1,15,1,0,,0.000759027,0,negative,5.92E-05,0.001567945,3.31E-05,1.09E-06,8.90E-06,0.001224471,6.49E-05,0.024974304,0.000644792,0.971321683,1.63E-05,8.04E-05,3.05E-06
8701,sarcasm_detection1,237,Pol :,Experimental Results,Dataset,sarcasm_detection,1,16,1,0,,0.000129225,0,negative,0.000513857,3.14E-05,0.000237844,2.71E-07,2.63E-06,1.21E-05,7.51E-06,3.35E-05,3.13E-05,0.997839691,2.18E-05,0.001267497,6.51E-07
8702,sarcasm_detection1,238,"To further test the effectiveness of our user embeddings , we perform experiments on a subset of Main , comprising of forums associated with the topic of politics .",Experimental Results,Dataset,sarcasm_detection,1,17,1,0,,0.000715341,0,negative,0.000154306,0.000189168,2.43E-05,7.49E-07,0.000129974,1.84E-05,9.61E-06,5.94E-05,5.65E-06,0.999057669,6.18E-07,0.000349986,1.97E-07
8703,sarcasm_detection1,239,The choice of using SARC for our experiments comes with multiple reasons .,Experimental Results,Dataset,sarcasm_detection,1,18,1,0,,3.26E-05,0,negative,3.24E-06,3.25E-06,3.61E-07,8.29E-08,3.77E-07,3.39E-06,5.74E-07,1.26E-05,8.17E-07,0.999935636,2.58E-05,1.38E-05,4.50E-08
8704,sarcasm_detection1,240,"First , this corpus is the first of its kind that was purposely developed to investigate the necessity of contextual information in sarcasm classification .",Experimental Results,Dataset,sarcasm_detection,1,19,1,0,,0.000488956,0,negative,4.75E-05,3.33E-05,1.48E-05,1.16E-05,0.001060138,2.19E-05,7.21E-06,1.32E-05,3.00E-06,0.998688318,6.16E-06,9.24E-05,5.61E-07
8705,sarcasm_detection1,241,This characteristic aligns well with the main goal of this paper .,Experimental Results,Dataset,sarcasm_detection,1,20,1,0,,9.69E-06,0,negative,3.67E-06,3.99E-06,4.66E-07,1.81E-07,1.06E-06,4.53E-06,3.27E-07,9.73E-06,4.54E-06,0.999962604,1.27E-06,7.59E-06,3.46E-08
8706,sarcasm_detection1,242,"Second , the large size of the corpus allows for statistically - relevant analyses .",Experimental Results,Dataset,sarcasm_detection,1,21,1,0,,0.001332742,0,negative,0.000238503,7.56E-06,7.06E-06,8.01E-07,2.44E-05,1.18E-05,4.18E-06,1.36E-05,1.72E-06,0.999357651,1.55E-06,0.000331057,1.73E-07
8707,sarcasm_detection1,243,"Third , the dataset annotations contain a small false - positive rate for sarcastic labels thus providing reliable annotations .",Experimental Results,Dataset,sarcasm_detection,1,22,1,0,,0.000294447,0,negative,0.000453026,1.14E-05,7.11E-06,1.09E-06,4.72E-05,1.17E-05,4.05E-06,1.44E-05,1.05E-06,0.998960933,1.52E-06,0.000486324,2.14E-07
8708,sarcasm_detection1,244,"Also , its self - annotation scheme rules out the annotation errors induced by third - party annotators .",Experimental Results,Dataset,sarcasm_detection,1,23,1,0,,0.000738679,0,negative,0.000150903,0.000119814,9.48E-05,1.92E-07,1.62E-05,5.26E-06,2.73E-06,1.71E-05,4.83E-05,0.999069799,4.25E-06,0.00047043,3.02E-07
8709,sarcasm_detection1,245,"Finally , the corpus structure provides meta-data ( e.g. , user information ) for its comments , which is useful for contextual modeling .",Experimental Results,Dataset,sarcasm_detection,1,24,1,0,,0.003903081,0,negative,0.000385071,0.000329879,0.000175731,1.79E-07,1.27E-05,5.97E-06,2.59E-06,2.74E-05,0.000453242,0.998291972,2.19E-06,0.00031269,3.85E-07
8710,sarcasm_detection1,246,Training details,,,sarcasm_detection,1,0,1,0,,0.002340091,0,negative,6.05E-05,0.001338931,1.43E-05,0.000123468,5.30E-05,0.00076714,0.000215992,0.00645336,0.000307534,0.988848599,0.00167076,0.000130888,1.56E-05
8711,sarcasm_detection1,247,We holdout 10 % of the training data for validation .,Training details,Training details,sarcasm_detection,1,1,1,1,hyperparameters,0.982760527,1,experimental-setup,5.53E-06,9.01E-06,2.59E-06,8.83E-07,5.30E-07,0.642740802,0.000495824,0.348029316,4.84E-06,0.008700363,1.81E-06,3.26E-06,5.23E-06
8712,sarcasm_detection1,248,Hyper- parameter tuning is performed using this validation set through RandomSearch .,Training details,Training details,sarcasm_detection,1,2,1,0,,0.335404215,0,experimental-setup,1.27E-05,4.98E-05,1.28E-05,1.27E-06,8.42E-07,0.567474516,0.000301792,0.411640912,4.25E-05,0.020450909,2.72E-06,4.04E-06,5.26E-06
8713,sarcasm_detection1,249,"To optimize the parameters , Adam optimizer ( Kingma and Ba , 2014 ) is used , starting with an initial learning rate of 1e ? 4 .",Training details,Training details,sarcasm_detection,1,3,1,1,hyperparameters,0.994612232,1,experimental-setup,3.28E-06,6.26E-06,1.89E-06,1.61E-06,3.36E-07,0.69264986,0.000387713,0.304821313,3.63E-06,0.002117544,7.14E-07,1.34E-06,4.51E-06
8714,sarcasm_detection1,250,"The learnable parameters in the network consists of ? = { U d , D , W",Training details,Training details,sarcasm_detection,1,4,1,0,,0.009271028,0,experimental-setup,3.36E-05,4.47E-05,3.36E-05,3.73E-06,4.75E-06,0.559011987,0.000555656,0.210587791,8.83E-05,0.229593594,1.05E-05,2.30E-05,8.83E-06
8715,sarcasm_detection1,251,Training termination is decided using early stopping technique with a patience of 12 .,Training details,Training details,sarcasm_detection,1,5,1,1,hyperparameters,0.980087329,1,experimental-setup,5.55E-06,1.14E-05,3.05E-06,1.79E-06,5.53E-07,0.592622163,0.000474538,0.402956908,1.28E-05,0.003896067,2.65E-06,3.03E-06,9.51E-06
8716,sarcasm_detection1,252,"For the batched - modeling of comments in CNNs , each comment is either restricted or padded to 100 words for uniformity .",Training details,Training details,sarcasm_detection,1,6,1,1,hyperparameters,0.894610633,1,experimental-setup,8.31E-06,2.93E-05,1.45E-05,2.53E-06,2.73E-06,0.66398172,0.000724878,0.323962745,1.37E-05,0.011244756,1.38E-06,4.94E-06,8.51E-06
8717,sarcasm_detection1,253,"The optimal hyper - parameters are found to be {d s , d p , d t , K} = 100 , d em = 300 , k s = 2 , M = 128 , and ? = ReLU ( Implementation details are provided in the supplementary ) .",Training details,Training details,sarcasm_detection,1,7,1,0,,0.946902903,1,experimental-setup,4.60E-06,6.26E-06,1.23E-06,1.01E-06,4.95E-07,0.686504381,0.000451314,0.307270974,3.52E-06,0.005745665,9.96E-07,4.55E-06,5.02E-06
8718,sarcasm_detection1,254,?,Training details,Training details,sarcasm_detection,1,8,1,0,,0.001615295,0,negative,3.15E-05,6.49E-06,1.77E-05,2.95E-06,2.49E-06,0.175945349,0.000559868,0.029375887,2.18E-05,0.793950636,2.51E-05,5.41E-05,6.14E-06
8719,sarcasm_detection1,255,5 % : significantly better than CUE - CNN .,Training details,Training details,sarcasm_detection,1,9,1,0,,0.94803571,1,experimental-setup,0.012934993,9.37E-05,0.001306607,0.000163064,0.000137097,0.399615095,0.125105295,0.143782565,3.04E-05,0.127023755,0.000103504,0.186937278,0.002766658
8720,sarcasm_detection1,256,Baseline Models,,,sarcasm_detection,1,0,1,0,,0.01144179,0,negative,7.95E-05,0.000152953,0.000324645,2.06E-06,2.38E-06,0.00054363,0.001153762,0.004225388,0.000103339,0.986423474,0.003492121,0.003477786,1.90E-05
8721,sarcasm_detection1,257,Here we describe the state - of - the - art methods and baselines that we compare CASCADE with .,Baseline Models,Baseline Models,sarcasm_detection,1,1,1,0,,0.062772494,0,negative,5.12E-05,2.47E-05,0.356268777,5.15E-06,3.52E-06,0.000140329,0.001022563,0.001170247,1.82E-05,0.640378089,1.66E-05,0.000870362,3.02E-05
8722,sarcasm_detection1,258,Bag - of - Words :,Baseline Models,Baseline Models,sarcasm_detection,1,2,1,1,baselines,0.993609454,1,baselines,2.69E-07,1.32E-07,0.998969132,1.00E-09,5.92E-10,4.62E-07,2.08E-05,4.86E-06,3.09E-07,0.000994145,3.60E-07,9.31E-06,2.00E-07
8723,sarcasm_detection1,259,This model uses a comment 's word - counts as features in a vector .,Baseline Models,Baseline Models,sarcasm_detection,1,3,1,1,baselines,0.926622196,1,baselines,1.10E-06,4.04E-07,0.995048181,2.03E-08,5.88E-09,3.56E-06,3.42E-05,4.40E-05,1.18E-05,0.004851337,7.11E-07,3.05E-06,1.70E-06
8724,sarcasm_detection1,260,The size of the vector is the vocabulary size of the training dataset .,Baseline Models,Baseline Models,sarcasm_detection,1,4,1,0,,0.941095459,1,negative,3.06E-05,3.74E-05,0.093013949,2.30E-06,4.00E-07,0.003448703,0.003637564,0.230517986,0.000174679,0.668852496,2.63E-05,0.000197233,6.04E-05
8725,sarcasm_detection1,261,CNN : We compare our model with this individual CNN version .,Baseline Models,Baseline Models,sarcasm_detection,1,5,1,1,baselines,0.940797027,1,baselines,1.57E-06,1.80E-07,0.995865112,7.94E-08,2.10E-08,9.23E-06,0.00054019,4.25E-05,5.16E-07,0.003450413,7.13E-07,8.26E-05,6.86E-06
8726,sarcasm_detection1,262,This CNN is capable of modeling only the content of a comment .,Baseline Models,Baseline Models,sarcasm_detection,1,6,1,0,,0.800427038,1,baselines,8.33E-07,1.51E-07,0.997068771,9.37E-09,4.29E-09,1.10E-06,1.92E-05,8.91E-06,1.62E-06,0.002891187,3.14E-07,7.02E-06,8.99E-07
8727,sarcasm_detection1,263,The architecture is similar to the CNN used in CASCADE ( see Section 3.2 ) .,Baseline Models,Baseline Models,sarcasm_detection,1,7,1,0,,0.35052005,0,baselines,3.18E-05,7.88E-06,0.885050591,3.32E-06,5.16E-07,0.000231917,0.001223712,0.002193844,0.000118621,0.110998089,5.07E-06,6.67E-05,6.80E-05
8728,sarcasm_detection1,264,CNN - SVM :,Baseline Models,Baseline Models,sarcasm_detection,1,8,1,1,baselines,0.987753448,1,baselines,1.42E-07,2.17E-08,0.99960571,6.77E-10,3.24E-10,4.22E-07,2.63E-05,2.37E-06,9.03E-08,0.000359936,4.84E-08,4.76E-06,1.98E-07
8729,sarcasm_detection1,265,"This model proposed by consists of a CNN for content modeling and other pre-trained CNNs for extracting sentiment , emotion and personality features from the given comment .",Baseline Models,Baseline Models,sarcasm_detection,1,9,1,1,baselines,0.869816117,1,baselines,1.36E-06,4.59E-07,0.997226376,5.38E-08,1.97E-08,2.59E-06,4.96E-05,2.12E-05,4.78E-06,0.002676952,8.95E-07,9.38E-06,6.33E-06
8730,sarcasm_detection1,266,All the features are concatenated and fed into an SVM for classification .,Baseline Models,Baseline Models,sarcasm_detection,1,10,1,0,,0.917485441,1,baselines,3.13E-05,1.01E-05,0.815997827,2.21E-07,1.33E-07,7.03E-05,0.000330569,0.002099393,0.000154803,0.181209239,1.15E-06,8.18E-05,1.32E-05
8731,sarcasm_detection1,267,CUE - CNN :,Baseline Models,Baseline Models,sarcasm_detection,1,11,1,1,baselines,0.990895425,1,baselines,1.95E-07,2.36E-08,0.999655069,5.51E-10,3.81E-10,1.85E-07,1.95E-05,1.10E-06,7.43E-08,0.000315698,4.43E-08,7.92E-06,1.85E-07
8732,sarcasm_detection1,268,This method proposed by also models user embeddings with a method akin to ParagraphVector .,Baseline Models,Baseline Models,sarcasm_detection,1,12,1,1,baselines,0.615423602,1,baselines,1.20E-05,1.71E-06,0.964417088,5.68E-07,1.90E-07,3.00E-05,0.000476057,0.000205437,4.92E-06,0.034672855,4.11E-06,0.000150587,2.44E-05
8733,sarcasm_detection1,269,Their embeddings are then combined with a CNN thus forming the CUE - CNN model .,Baseline Models,Baseline Models,sarcasm_detection,1,13,1,0,,0.860356374,1,baselines,3.10E-05,6.34E-06,0.884089685,3.96E-07,1.97E-07,3.04E-05,0.000186021,0.000672723,0.000252806,0.11464349,9.86E-07,6.19E-05,2.41E-05
8734,sarcasm_detection1,270,We compare with this model to analyze the efficiency of our embeddings as opposed to theirs .,Baseline Models,Baseline Models,sarcasm_detection,1,14,1,0,,0.04814302,0,baselines,1.27E-05,6.69E-06,0.658178451,1.27E-07,2.31E-07,1.37E-05,0.000226977,0.000209428,1.02E-05,0.340971184,1.20E-06,0.000365069,4.01E-06
8735,sarcasm_detection1,271,Released software 4 is used to produce results on the SARC dataset .,Baseline Models,Baseline Models,sarcasm_detection,1,15,1,0,,0.573216408,1,baselines,0.000273309,2.46E-05,0.502973165,0.00036789,2.02E-05,0.005796017,0.027483133,0.02291183,6.73E-05,0.434463445,1.31E-05,0.004393588,0.001212378
8736,sarcasm_detection1,272,presents the performance results on the SARC datasets .,Baseline Models,Baseline Models,sarcasm_detection,1,16,1,0,,0.743984647,1,baselines,0.000145623,8.71E-07,0.702031488,8.69E-08,2.34E-07,6.44E-06,0.001263977,6.32E-05,5.55E-07,0.248528513,2.53E-06,0.047942897,1.36E-05
8737,sarcasm_detection1,273,CASCADE manages to achieve major improvement across all datasets with statistical significance .,Baseline Models,Baseline Models,sarcasm_detection,1,17,1,1,results,0.915013071,1,results,0.001695968,1.11E-06,0.03731036,1.44E-06,6.20E-07,1.91E-05,0.009400738,0.00026982,7.30E-07,0.015078982,2.91E-06,0.935872979,0.000345207
8738,sarcasm_detection1,274,The lowest performance is obtained by the Bag - of - words approach whereas all neural architectures outperform it .,Baseline Models,Baseline Models,sarcasm_detection,1,18,1,1,results,0.952387646,1,results,0.000705978,9.43E-07,0.026734908,8.55E-07,4.71E-07,2.33E-05,0.012672886,0.000339321,4.33E-07,0.029616752,3.33E-06,0.929726119,0.000174735
8739,sarcasm_detection1,275,"Amongst the neural networks , the CNN baseline receives the least performance .",Baseline Models,Baseline Models,sarcasm_detection,1,19,1,1,results,0.980234051,1,results,0.001480919,6.68E-07,0.057892879,7.65E-07,5.22E-07,2.10E-05,0.010992633,0.000266923,3.85E-07,0.033525933,1.18E-06,0.895673011,0.000143198
8740,sarcasm_detection1,276,CASCADE comfortably beats the state - of - the - art neural models CNN - SVM and CUE - CNN .,Baseline Models,Baseline Models,sarcasm_detection,1,20,1,1,results,0.90819886,1,results,0.000492046,1.29E-06,0.125106311,1.85E-06,7.67E-07,3.66E-05,0.022077969,0.000345591,8.56E-07,0.023436621,3.66E-06,0.827857398,0.000639016
8741,sarcasm_detection1,277,Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .,Baseline Models,Baseline Models,sarcasm_detection,1,21,1,1,results,0.287929503,0,results,0.001142846,1.69E-06,0.216981139,5.36E-07,1.13E-06,1.57E-05,0.003331804,0.000120538,1.93E-06,0.276380514,1.58E-06,0.501953452,6.71E-05
8742,sarcasm_detection1,278,"We further compare our proposed user -profiling method with that of CUE - CNN , with absolute differences shown in the bottom row of .",Baseline Models,Baseline Models,sarcasm_detection,1,22,1,0,,0.274790291,0,negative,0.000164699,5.05E-06,0.239093108,1.16E-07,5.55E-07,1.40E-05,0.00176352,0.000231347,2.15E-06,0.719593445,7.79E-07,0.039122123,9.16E-06
8743,sarcasm_detection1,279,"Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .",Baseline Models,Baseline Models,sarcasm_detection,1,23,1,1,results,0.125270871,0,negative,0.000426504,1.60E-05,0.130395738,4.45E-07,2.82E-06,2.43E-05,0.000845898,0.000472107,7.81E-06,0.857392434,3.54E-07,0.010403706,1.19E-05
8744,sarcasm_detection1,280,"As seen in the table , CASCADE without personality features drops in performance to a range similar to CUE - CNN .",Baseline Models,Baseline Models,sarcasm_detection,1,24,1,0,,0.873169182,1,results,0.010326395,1.70E-06,0.060889227,1.38E-06,1.37E-06,1.68E-05,0.006867709,0.000202114,9.60E-07,0.040765472,1.44E-06,0.880730125,0.000195314
8745,sarcasm_detection1,281,This suggests that the combination of stylometric and personality features are indeed crucial for the improved performance of CASCADE .,Baseline Models,Baseline Models,sarcasm_detection,1,25,1,0,,0.381018684,0,negative,0.011501574,9.08E-06,0.134016421,1.73E-06,5.64E-06,2.31E-05,0.001374576,0.000322285,1.14E-05,0.708249108,1.15E-06,0.144432831,5.11E-05
8746,sarcasm_detection1,282,Results,,,sarcasm_detection,1,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
8747,sarcasm_detection1,283,Ablation Study,,,sarcasm_detection,1,0,1,0,,0.006818876,0,negative,0.046739945,0.000279772,0.002808551,0.000142874,8.76E-05,0.000276199,0.001210224,0.00046093,0.000147782,0.934438164,0.002965146,0.010376843,6.60E-05
8748,sarcasm_detection1,284,We experiment on multiple variants of CASCADE so as to analyze the importance of the various features present in its architecture .,Ablation Study,Ablation Study,sarcasm_detection,1,1,1,0,,0.172013203,0,negative,0.424231157,0.000791649,0.010623982,2.58E-05,0.000762916,0.000639908,0.000835638,6.19E-05,0.000852718,0.559601294,6.55E-05,2.24E-05,0.001485195
8749,sarcasm_detection1,285,provides the results of all the combinations .,Ablation Study,Ablation Study,sarcasm_detection,1,2,1,0,,0.012027993,0,negative,0.058883649,3.14E-06,0.000390911,4.23E-06,3.08E-05,8.47E-05,2.90E-05,5.35E-06,4.65E-05,0.940268238,2.05E-05,2.61E-06,0.000230335
8750,sarcasm_detection1,286,"First , we test performance for the content - based CNN only ( row 1 ) .",Ablation Study,Ablation Study,sarcasm_detection,1,3,1,1,ablation-analysis,0.728083719,1,ablation-analysis,0.897683915,1.60E-05,0.000893815,7.62E-07,2.52E-05,1.83E-05,0.000139182,1.92E-06,2.28E-05,0.10110831,5.03E-06,2.81E-05,5.66E-05
8751,sarcasm_detection1,287,This setting provides the worst relative performance with almost 10 % lesser accuracy than optimal .,Ablation Study,Ablation Study,sarcasm_detection,1,4,1,1,ablation-analysis,0.566562807,1,ablation-analysis,0.952242456,9.82E-06,0.000304695,5.06E-06,4.84E-05,4.99E-05,0.000141046,5.40E-06,1.88E-05,0.046703934,4.15E-06,2.28E-05,0.000443509
8752,sarcasm_detection1,288,"Next , we include contextual features to this network .",Ablation Study,Ablation Study,sarcasm_detection,1,5,1,1,ablation-analysis,0.23624993,0,ablation-analysis,0.682088689,0.000160757,0.077200401,9.43E-07,1.96E-05,7.00E-05,8.53E-05,7.62E-06,0.0179985,0.222163931,1.22E-05,6.43E-06,0.000185527
8753,sarcasm_detection1,289,"Here , the effect of discourse features is primarily seen in the Pol dataset getting an increase of 3 % in F1 ( row 2 ) .",Ablation Study,Ablation Study,sarcasm_detection,1,6,1,1,ablation-analysis,0.899449339,1,ablation-analysis,0.999095193,7.87E-08,5.62E-06,6.25E-08,4.35E-07,3.59E-07,9.78E-06,3.17E-08,1.63E-07,0.000868278,2.17E-07,5.83E-06,1.39E-05
8754,sarcasm_detection1,290,A major boost in performance is observed ( 8 ? 12 % accuracy and F1 ) when user embeddings are introduced ( row 5 ) .,Ablation Study,Ablation Study,sarcasm_detection,1,7,1,1,ablation-analysis,0.96821092,1,ablation-analysis,0.999280641,8.31E-08,6.13E-06,5.75E-08,3.09E-07,3.29E-07,1.12E-05,3.43E-08,1.61E-07,0.000675005,1.60E-07,7.91E-06,1.79E-05
8755,sarcasm_detection1,291,Visualization of the user embedding cluster ( Section 4.6 ) provides insights for this positive trend .,Ablation Study,Ablation Study,sarcasm_detection,1,8,1,0,,0.010939809,0,ablation-analysis,0.799539325,1.47E-06,0.000145049,3.60E-07,5.92E-06,9.51E-06,1.77E-05,5.86E-07,1.21E-05,0.200220517,3.34E-06,7.43E-06,3.67E-05
8756,sarcasm_detection1,292,"Overall , CASCADE consisting of CNN with user embeddings and contextual discourse features provide the best performance in all three datasets ( row 6 ) .",Ablation Study,Ablation Study,sarcasm_detection,1,9,1,1,ablation-analysis,0.982674791,1,ablation-analysis,0.997575313,9.12E-07,5.52E-05,3.30E-07,1.37E-06,4.34E-06,0.000138322,4.90E-07,1.71E-06,0.001962925,1.10E-06,4.12E-05,0.000216834
8757,sarcasm_detection1,293,We challenge the use of CCA for the generation of user embeddings and thus replace it with simple concatenation .,Ablation Study,Ablation Study,sarcasm_detection,1,10,1,1,ablation-analysis,0.16338607,0,negative,0.402966724,8.94E-06,0.000620807,7.25E-06,2.43E-05,5.01E-05,1.97E-05,2.41E-06,4.27E-05,0.595838782,6.87E-05,7.23E-06,0.000342342
8758,sarcasm_detection1,294,This however causes a significant drop in performance ( row 3 ) .,Ablation Study,Ablation Study,sarcasm_detection,1,11,1,1,ablation-analysis,0.004855065,0,ablation-analysis,0.923631124,1.64E-07,6.76E-05,2.38E-07,3.45E-06,2.10E-06,3.13E-06,7.14E-08,1.60E-06,0.076278529,3.49E-07,1.99E-06,9.65E-06
8759,sarcasm_detection1,295,Improvement is not observed even when discourse features are used with these concatenated user embeddings ( row 4 ) .,Ablation Study,Ablation Study,sarcasm_detection,1,12,1,0,,0.899200325,1,ablation-analysis,0.997603339,1.71E-07,8.05E-06,1.50E-07,6.27E-07,1.45E-06,3.61E-05,1.44E-07,3.34E-07,0.002267336,4.23E-07,1.57E-05,6.62E-05
8760,sarcasm_detection1,296,We assume the increase in parameters caused by concatenation for this performance degradation .,Ablation Study,Ablation Study,sarcasm_detection,1,13,1,0,,0.004199252,0,ablation-analysis,0.632865389,4.29E-06,0.000541509,1.16E-06,1.84E-05,2.22E-05,1.64E-05,2.23E-06,8.14E-05,0.366350544,2.13E-06,3.89E-06,9.05E-05
8761,sarcasm_detection1,297,"CCA on the other hand creates succinct representations with maximal information , giving better results .",Ablation Study,Ablation Study,sarcasm_detection,1,14,1,0,,0.91990521,1,ablation-analysis,0.978014533,2.68E-06,0.001157553,8.39E-08,1.39E-06,4.20E-06,0.000122496,3.29E-07,8.32E-06,0.02053737,5.47E-06,5.74E-05,8.82E-05
8762,sarcasm_detection1,298,User Embedding Analysis,Ablation Study,,sarcasm_detection,1,15,1,0,,0.125678787,0,ablation-analysis,0.789399169,4.87E-06,0.000581451,5.41E-06,2.35E-05,5.14E-05,0.000281633,2.77E-06,3.16E-05,0.203010077,0.000164636,6.57E-05,0.006377651
8763,sarcasm_detection1,299,We investigate the learnt user embeddings in more detail .,Ablation Study,User Embedding Analysis,sarcasm_detection,1,16,1,0,,1.61E-05,0,negative,0.000216894,1.21E-06,9.94E-07,7.61E-08,2.87E-07,4.93E-06,6.66E-06,2.73E-06,1.27E-05,0.99974854,1.75E-07,4.57E-06,2.53E-07
8764,sarcasm_detection1,300,"In particular , we plot random samples of users on a 2D - plane using t- SNE .",Ablation Study,User Embedding Analysis,sarcasm_detection,1,17,1,0,,0.000314956,0,negative,0.004017248,9.20E-05,8.30E-05,1.87E-07,8.75E-06,2.27E-05,0.000243301,7.55E-06,0.000230915,0.995118474,3.31E-06,0.000168597,4.06E-06
8765,sarcasm_detection1,301,The users who have greater sarcastic comments ( atleast 2 more than the other type ) are termed as sarcastic users ( colored red ) .,Ablation Study,User Embedding Analysis,sarcasm_detection,1,18,1,0,,3.93E-06,0,negative,9.70E-05,3.35E-07,3.98E-06,6.31E-09,1.66E-07,4.98E-06,1.79E-05,1.42E-06,5.97E-06,0.999862841,7.90E-08,5.02E-06,3.04E-07
8766,sarcasm_detection1,302,"Conversely , the users having lesser sarcastic comments are called non-sarcastic users ( colored green ) .",Ablation Study,User Embedding Analysis,sarcasm_detection,1,19,1,0,,1.75E-06,0,negative,2.16E-05,5.29E-07,4.31E-06,4.52E-09,9.00E-08,3.26E-06,5.76E-06,1.06E-06,2.84E-05,0.999933051,1.41E-07,1.55E-06,2.00E-07
8767,sarcasm_detection1,303,Equal number of users from both the categories are plotted .,Ablation Study,User Embedding Analysis,sarcasm_detection,1,20,1,0,,2.63E-06,0,negative,1.00E-04,3.55E-07,1.03E-06,1.59E-08,1.15E-07,1.26E-05,5.39E-05,4.71E-06,1.09E-05,0.999808482,4.18E-07,6.04E-06,1.53E-06
8768,sarcasm_detection1,304,We aim to analyze the reason behind the performance boost provided by the user embeddings as shown in .,Ablation Study,User Embedding Analysis,sarcasm_detection,1,21,1,0,,1.21E-05,0,negative,0.000478239,5.79E-07,1.08E-06,4.80E-08,4.77E-07,1.94E-06,2.66E-06,5.18E-07,4.60E-06,0.999501497,3.61E-07,7.63E-06,3.79E-07
8769,sarcasm_detection1,305,We see in that both the user types belong to similar distributions .,Ablation Study,User Embedding Analysis,sarcasm_detection,1,22,1,0,,0.000103837,0,negative,0.015421194,3.89E-07,1.02E-06,4.13E-08,3.43E-07,3.71E-06,0.000122461,1.43E-06,1.82E-06,0.983972517,3.60E-07,0.000472839,1.88E-06
8770,sarcasm_detection1,306,"However , the sarcastic users have a greater spread than the non-sarcastic ones ( red belt around the green region ) .",Ablation Study,User Embedding Analysis,sarcasm_detection,1,23,1,0,,1.05E-05,0,negative,0.040451595,4.79E-07,1.70E-05,1.70E-08,6.18E-07,2.00E-06,8.92E-05,5.16E-07,3.80E-06,0.95893627,1.55E-07,0.000497373,9.26E-07
8771,sarcasm_detection1,307,This is also evident from the variances of the distributions where the sarcastic distribution comprises of 10.92 variance as opposed to 5.20 variance of the non-sarcastic distribution .,Ablation Study,User Embedding Analysis,sarcasm_detection,1,24,1,0,,0.000119052,0,negative,0.050484957,2.41E-07,2.05E-06,4.20E-08,6.69E-07,4.93E-06,0.000260001,1.15E-06,8.80E-07,0.948255071,2.25E-07,0.000987415,2.36E-06
8772,sarcasm_detection1,308,We can infer from this observation that the user embeddings belonging to this non-overlapping red-region provide discriminative information regarding the sarcastic tendencies of their users .,Ablation Study,User Embedding Analysis,sarcasm_detection,1,25,1,0,,1.00E-05,0,negative,0.002899321,5.99E-07,2.87E-06,6.94E-09,1.51E-07,8.84E-07,7.12E-06,2.65E-07,1.31E-05,0.99704474,8.40E-08,3.06E-05,1.97E-07
8773,sarcasm_detection1,309,Case Studies,Ablation Study,,sarcasm_detection,1,26,1,0,,0.022114776,0,negative,0.135195371,1.31E-05,0.001215191,3.29E-06,6.91E-05,9.35E-05,0.000392367,9.97E-06,0.000117605,0.857108201,1.90E-05,2.38E-05,0.005739457
8774,sarcasm_detection1,310,"Results demonstrate that discourse features provide an improvement over baselines , especially on the Pol dataset .",Ablation Study,Case Studies,sarcasm_detection,1,27,1,0,,0.095661029,0,ablation-analysis,0.710932367,4.58E-05,4.89E-05,2.25E-06,1.77E-05,5.45E-05,0.001777792,5.88E-05,2.14E-05,0.274200481,2.05E-05,0.012765786,5.38E-05
8775,sarcasm_detection1,311,This signifies the greater role of the contextual cues for classifying comments in this dataset over the other dataset variants used in our experiment .,Ablation Study,Case Studies,sarcasm_detection,1,28,1,0,,3.81E-06,0,negative,0.002229297,5.12E-06,5.42E-06,4.10E-08,2.16E-06,4.19E-06,7.32E-06,4.57E-06,1.21E-05,0.997690232,7.22E-07,3.87E-05,2.17E-07
8776,sarcasm_detection1,312,"Below , we present a couple of cases from the Pol dataset where our model correctly identifies the sarcasm which is evident only with the neighboring comments .",Ablation Study,Case Studies,sarcasm_detection,1,29,1,0,,4.85E-05,0,negative,0.002989024,6.05E-06,1.90E-05,2.90E-07,6.15E-05,3.89E-06,5.85E-06,1.20E-06,1.51E-06,0.996869377,3.06E-07,4.19E-05,1.66E-07
8777,sarcasm_detection1,313,"The previous state - of - the - art CUE - CNN , however , misclassifies them .",Ablation Study,Case Studies,sarcasm_detection,1,30,1,0,,2.22E-05,0,negative,0.000143585,1.39E-06,4.59E-06,9.76E-08,2.56E-06,3.10E-06,1.82E-06,2.08E-06,2.62E-06,0.999822367,1.25E-05,2.79E-06,4.70E-07
8778,sarcasm_detection1,314,"For the comment Whew , I feel much better now ! , it s sarcasm is evident only when it s previous comment is seen",Ablation Study,Case Studies,sarcasm_detection,1,31,1,0,,1.64E-07,0,negative,3.60E-05,3.79E-07,1.08E-06,5.94E-07,6.32E-06,1.04E-05,7.73E-07,2.01E-06,1.53E-06,0.999939881,4.27E-07,3.91E-07,2.09E-07
8779,sarcasm_detection1,315,So all of the US presidents are terrorists for the last 5 years .,Ablation Study,Case Studies,sarcasm_detection,1,32,1,0,,1.14E-06,0,negative,8.64E-05,7.54E-07,1.78E-06,2.22E-06,4.08E-05,1.37E-05,1.16E-06,2.29E-06,1.64E-06,0.999847861,4.38E-07,5.44E-07,5.26E-07
8780,sarcasm_detection1,316,The comment The part where Obama signed it .,Ablation Study,Case Studies,sarcasm_detection,1,33,1,0,,1.48E-07,0,negative,5.30E-05,4.37E-07,1.45E-06,7.26E-07,7.18E-06,1.36E-05,7.03E-07,2.79E-06,2.34E-06,0.999917005,1.65E-07,3.47E-07,2.42E-07
8781,sarcasm_detection1,317,does n't seem to be sarcastic until looked upon as a remark to its previous comment,Ablation Study,Case Studies,sarcasm_detection,1,34,1,0,,1.00E-07,0,negative,1.64E-05,3.39E-07,6.16E-07,3.38E-07,2.44E-06,4.27E-06,4.72E-07,1.47E-06,1.54E-06,0.999970852,8.32E-07,2.47E-07,1.69E-07
8782,sarcasm_detection1,318,What part of this would be unconstitutional ?.,Ablation Study,Case Studies,sarcasm_detection,1,35,1,0,,2.39E-07,0,negative,2.78E-05,1.32E-06,9.77E-07,1.13E-07,8.54E-07,5.75E-06,9.18E-07,6.33E-06,1.00E-05,0.999944232,9.19E-07,4.56E-07,2.87E-07
8783,sarcasm_detection1,319,Such observations indicate the impact of discourse features .,Ablation Study,Case Studies,sarcasm_detection,1,36,1,0,,1.35E-06,0,negative,0.000131139,3.13E-07,7.45E-07,1.33E-08,4.61E-07,1.62E-06,7.46E-07,1.35E-06,1.44E-06,0.99986026,1.20E-07,1.73E-06,5.85E-08
8784,sarcasm_detection1,320,"However , sometimes contextual cues from the previous comments are not enough and misclassifications are observed due to lack of necessary commonsense and background knowledge about the topic of discussion .",Ablation Study,Case Studies,sarcasm_detection,1,37,1,0,,1.18E-06,0,negative,0.000102278,4.42E-07,1.10E-06,9.66E-08,4.96E-06,1.05E-06,3.58E-07,4.85E-07,5.24E-07,0.999886635,5.07E-07,1.48E-06,7.72E-08
8785,sarcasm_detection1,321,There are also other cases where our model fails despite the presence of contextual information from the previous comments .,Ablation Study,Case Studies,sarcasm_detection,1,38,1,0,,1.38E-06,0,negative,0.000114495,3.13E-07,2.05E-06,6.86E-08,3.82E-06,2.08E-06,6.98E-07,6.20E-07,4.69E-07,0.999873377,1.17E-07,1.83E-06,7.21E-08
8786,sarcasm_detection1,322,"During exploration , this is primarily observed for contextual comments which are very long .",Ablation Study,Case Studies,sarcasm_detection,1,39,1,0,,2.77E-05,0,negative,0.000860253,3.54E-06,1.39E-05,9.03E-08,1.40E-05,2.35E-06,1.46E-06,1.04E-06,3.43E-06,0.999092259,2.59E-07,7.29E-06,1.78E-07
8787,sarcasm_detection1,323,"Thus , sequential discourse modeling using RNNs maybe better suited for such cases .",Ablation Study,Case Studies,sarcasm_detection,1,40,1,0,,1.07E-05,0,negative,7.55E-05,1.93E-06,2.31E-06,3.40E-07,2.93E-06,6.97E-06,3.02E-06,4.95E-06,4.07E-06,0.999875881,1.75E-05,3.30E-06,1.32E-06
8788,sarcasm_detection1,324,"Also , in the case of user embeddings , CASCADE Main Pol user dis - balanced imbalanced cca concat .",Ablation Study,Case Studies,sarcasm_detection,1,41,1,0,,3.11E-06,0,negative,0.004785658,1.29E-05,0.000823257,1.19E-08,1.49E-06,3.05E-06,1.53E-05,1.40E-06,4.56E-05,0.994239317,4.60E-07,7.12E-05,4.47E-07
8789,sarcasm_detection1,325,course Acc. F1 Acc.,Ablation Study,Case Studies,sarcasm_detection,1,42,1,0,,6.52E-06,0,negative,0.000123426,1.65E-06,3.33E-06,1.89E-07,6.58E-06,3.99E-05,2.39E-05,2.21E-05,4.77E-06,0.999764592,4.08E-07,6.71E-06,2.35E-06
8790,sarcasm_detection1,326,F1,Ablation Study,,sarcasm_detection,1,43,1,0,,0.002402397,0,negative,0.019518733,5.99E-07,0.000101061,1.38E-07,2.11E-06,1.39E-05,1.59E-05,1.86E-06,2.09E-05,0.980176974,1.37E-07,8.39E-07,0.000146911
8791,sarcasm_detection1,327,Acc. misclassifications were common for users with lesser historical posts .,Ablation Study,F1,sarcasm_detection,1,44,1,0,,0.03468442,0,negative,0.071975557,1.38E-06,6.63E-05,4.81E-06,0.000135035,2.06E-05,1.32E-05,1.32E-06,5.71E-06,0.927664053,1.68E-07,7.64E-06,0.000104334
8792,sarcasm_detection1,328,"In such scenarios , potential solutions would be to create user networks and derive information from similar users within the network .",Ablation Study,F1,sarcasm_detection,1,45,1,0,,0.000312781,0,negative,0.001582954,3.79E-06,5.55E-05,3.15E-06,1.96E-05,2.14E-05,3.82E-06,2.98E-06,3.80E-05,0.998147908,2.37E-06,1.70E-06,0.000116695
8793,sarcasm_detection1,329,These are some of the issues which we plan to address in future work .,Ablation Study,F1,sarcasm_detection,1,46,1,0,,5.04E-05,0,negative,0.003672243,1.33E-06,1.95E-05,2.10E-06,1.18E-05,2.69E-05,3.85E-06,3.02E-06,2.10E-05,0.996203162,1.49E-07,6.78E-07,3.43E-05
8794,sarcasm_detection1,330,Conclusion,,,sarcasm_detection,1,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
8795,question_generation0,1,title,,,question_generation,0,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
8796,question_generation0,2,Neural Question Generation from Text : A Preliminary Study,title,,question_generation,0,1,1,1,research-problem,0.997718825,1,research-problem,2.52E-08,4.57E-06,4.59E-08,4.67E-08,3.47E-08,7.14E-08,1.25E-06,9.72E-07,4.19E-07,0.00164538,0.998346923,2.15E-07,4.42E-08
8797,question_generation0,3,abstract,,,question_generation,0,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
8798,question_generation0,4,Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub- spans of the given passage .,abstract,abstract,question_generation,0,1,1,1,research-problem,0.855770123,1,research-problem,1.73E-08,3.73E-06,7.72E-09,2.18E-06,4.42E-07,3.83E-07,4.15E-07,1.20E-06,2.47E-07,0.012831493,0.98715981,1.15E-08,5.39E-08
8799,question_generation0,5,Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions .,abstract,abstract,question_generation,0,2,1,0,,0.012253291,0,research-problem,2.55E-07,5.40E-05,7.94E-08,7.16E-06,4.60E-06,4.16E-06,1.46E-06,1.64E-05,2.41E-06,0.180371976,0.819537256,9.17E-08,1.59E-07
8800,question_generation0,6,"In this work , we propose to apply the neural encoderdecoder model to generate meaningful and diverse questions from natural language sentences .",abstract,abstract,question_generation,0,3,1,0,,0.822831475,1,research-problem,7.29E-06,0.058754855,5.87E-05,6.44E-06,6.35E-05,1.22E-05,1.36E-05,0.000179697,0.003649981,0.073588822,0.863657968,5.01E-06,1.98E-06
8801,question_generation0,7,"The encoder reads the input text and the answer position , to produce an answer - aware input representation , which is fed to the decoder to generate an answer focused question .",abstract,abstract,question_generation,0,4,1,0,,0.572030294,1,negative,1.20E-05,0.098517975,0.000280742,2.50E-06,9.52E-05,2.38E-05,7.01E-06,0.000259308,0.159931122,0.578215809,0.162650344,3.07E-06,1.07E-06
8802,question_generation0,8,"We conduct a preliminary study on neural question generation from text with the SQuAD dataset , and the experiment results show that our method can produce fluent and diverse questions .",abstract,abstract,question_generation,0,5,1,0,,0.360445658,0,research-problem,2.50E-05,0.03901386,3.05E-05,3.47E-05,0.001118255,2.87E-05,8.73E-05,0.000348201,0.000299889,0.407059138,0.551891403,5.78E-05,5.26E-06
8803,question_generation0,9,Introduction,,,question_generation,0,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
8804,question_generation0,10,"Automatic question generation from natural language text aims to generate questions taking text as input , which has the potential value of education purpose ) .",Introduction,Introduction,question_generation,0,1,1,1,research-problem,0.92806266,1,research-problem,6.32E-07,7.22E-05,2.11E-07,1.14E-05,1.85E-05,4.33E-06,8.97E-06,3.61E-06,1.84E-05,0.035122835,0.964736526,7.17E-07,1.67E-06
8805,question_generation0,11,"As the reverse task of question answering , question generation also has the potential for providing a large scale corpus of question - answer pairs .",Introduction,Introduction,question_generation,0,2,1,1,research-problem,0.850523833,1,research-problem,5.95E-07,0.000102216,1.70E-07,9.29E-06,2.01E-05,4.77E-06,9.80E-06,5.14E-06,2.01E-05,0.050619787,0.949205719,8.58E-07,1.52E-06
8806,question_generation0,12,Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions .,Introduction,Introduction,question_generation,0,3,1,0,,0.596262357,1,research-problem,1.85E-06,0.000364485,4.43E-07,9.80E-06,3.50E-05,9.27E-06,1.62E-05,1.22E-05,4.53E-05,0.090368528,0.909133295,1.76E-06,1.87E-06
8807,question_generation0,13,"However , these methods heavily rely on human - designed transformation and generation rules , which can not be easily adopted to other domains .",Introduction,Introduction,question_generation,0,4,1,0,,0.021772278,0,negative,5.84E-06,0.00237951,1.06E-06,3.03E-05,0.00012776,5.63E-05,2.23E-05,9.33E-05,0.000634732,0.543093287,0.453548831,3.65E-06,3.07E-06
8808,question_generation0,14,"Instead of generating questions from texts , proposed a neu - * Contribution during internship at Microsoft Research .",Introduction,Introduction,question_generation,0,5,1,0,,0.004123183,0,negative,1.10E-05,0.002432781,1.05E-05,0.000148456,0.000576379,0.000179459,4.12E-05,8.29E-05,0.002035196,0.630535254,0.363934256,4.56E-06,8.09E-06
8809,question_generation0,15,ral network method to generate factoid questions from structured data .,Introduction,Introduction,question_generation,0,6,1,0,,0.348134519,0,research-problem,3.46E-06,0.002131004,5.25E-06,1.56E-06,7.76E-06,1.57E-05,4.73E-05,4.60E-05,0.007025954,0.067466819,0.923235436,8.75E-06,5.07E-06
8810,question_generation0,16,"In this work we conduct a preliminary study on question generation from text with neural networks , which is denoted as the Neural Question Generation ( NQG ) framework , to generate natural language questions from text without pre-defined rules .",Introduction,Introduction,question_generation,0,7,1,1,model,0.968822675,1,research-problem,0.000100096,0.311882141,0.000264366,4.06E-05,0.003369399,7.77E-05,0.000643265,0.000169179,0.032661426,0.124816818,0.525748942,0.000188214,3.78E-05
8811,question_generation0,17,The Neural Question Generation framework extends the sequence - to - sequence models by enriching the encoder with answer and lexical features to generate answer focused questions .,Introduction,Introduction,question_generation,0,8,1,1,model,0.960163833,1,model,7.70E-06,0.064529129,0.000266413,6.84E-07,7.01E-05,1.39E-05,2.11E-05,2.32E-05,0.900931487,0.019422444,0.014706348,5.29E-06,2.16E-06
8812,question_generation0,18,"Concretely , the encoder reads not only the input sentence , but also the answer position indicator and lexical features .",Introduction,Introduction,question_generation,0,9,1,1,model,0.935598354,1,model,2.38E-06,0.019571578,1.65E-05,7.25E-08,1.30E-05,2.80E-06,1.77E-06,5.65E-06,0.975641302,0.004452684,0.000291552,5.12E-07,2.01E-07
8813,question_generation0,19,"The answer position feature denotes the answer span in the input sentence , which is essential to generate answer relevant questions .",Introduction,Introduction,question_generation,0,10,1,1,model,0.896119891,1,model,7.75E-06,0.047708826,3.15E-05,5.22E-07,5.13E-05,3.83E-05,7.43E-06,0.000107921,0.931382666,0.020185421,0.00047631,1.25E-06,7.24E-07
8814,question_generation0,20,The lexical features include part - of - speech ( POS ) and named entity ( NER ) tags to help produce better sentence encoding .,Introduction,Introduction,question_generation,0,11,1,1,model,0.891001836,1,model,2.50E-05,0.105075832,8.11E-05,4.43E-06,0.000477092,0.000177507,3.46E-05,0.000277233,0.845274642,0.047939656,0.000626326,3.87E-06,2.72E-06
8815,question_generation0,21,"Lastly , the decoder with attention mechanism generates an answer specific question of the sentence .",Introduction,Introduction,question_generation,0,12,1,1,model,0.915407778,1,model,1.72E-05,0.022950502,0.000128082,6.25E-07,0.000108658,1.38E-05,9.20E-06,1.42E-05,0.963150457,0.013247607,0.000355759,2.83E-06,9.73E-07
8816,question_generation0,22,Large - scale manually annotated passage and question pairs play a crucial role in developing question generation systems .,Introduction,Introduction,question_generation,0,13,1,0,,0.628901529,1,research-problem,4.15E-06,0.001164542,1.22E-06,4.23E-05,0.000344549,3.54E-05,7.72E-05,4.44E-05,0.000121247,0.262564982,0.735582603,8.05E-06,9.43E-06
8817,question_generation0,23,We propose to adapt the recently released Stanford Question Answering Dataset ( SQuAD ) as the training and development datasets for the question generation task .,Introduction,Introduction,question_generation,0,14,1,0,,0.915679893,1,dataset,8.48E-05,0.254706368,0.000349844,0.00135902,0.589687221,0.000494577,0.001003004,0.000281663,0.006602132,0.128163553,0.017047739,0.000120629,9.94E-05
8818,question_generation0,24,"In SQuAD , the answers are labeled as subsequences in the given sentences by crowed sourcing , and it contains more than 100K questions which makes it feasible to train our neural network models .",Introduction,Introduction,question_generation,0,15,1,0,,0.377622485,0,dataset,1.15E-05,0.010214764,3.46E-05,0.001576427,0.909378947,0.000553455,0.000223374,6.60E-05,0.000272744,0.077233319,0.000405662,9.71E-06,1.95E-05
8819,question_generation0,25,"We conduct the experiments on SQuAD , and the experiment results show the neural network models can produce fluent and diverse questions from text .",Introduction,Introduction,question_generation,0,16,1,0,,0.094508343,0,negative,0.000227101,0.408951949,0.000102714,4.48E-05,0.020686449,0.000433108,0.002171392,0.001089122,0.017289915,0.540188148,0.007916818,0.000838339,6.01E-05
8820,question_generation0,26,Approach,,,question_generation,0,0,1,0,,6.30E-05,0,negative,0.000256653,0.000405673,1.67E-05,0.000511976,2.16E-05,0.00097816,0.00011784,0.00317846,0.000653531,0.990596627,0.003176926,4.36E-05,4.23E-05
8821,question_generation0,27,"In this section , we introduce the NQG framework , which consists of a feature - rich encoder and an arXiv : 1704.01792v3 [ cs. CL ] 18 Apr 2017 attention - based decoder .",Approach,Approach,question_generation,0,1,1,0,,0.503018749,1,approach,1.38E-05,0.783209461,0.00011018,4.04E-06,0.000319667,2.46E-05,7.06E-06,0.000204773,0.167487462,0.043031652,0.005580661,5.16E-06,1.45E-06
8822,question_generation0,28,provides an overview of our NQG framework .,Approach,Approach,question_generation,0,2,1,0,,0.030420448,0,negative,3.61E-06,0.010440633,3.34E-06,4.64E-05,0.000761935,4.94E-05,2.56E-06,0.00014223,0.001836923,0.98092746,0.005783749,1.11E-06,6.75E-07
8823,question_generation0,29,Feature - Rich Encoder,Approach,,question_generation,0,3,1,0,,0.080543821,0,model,5.28E-05,0.15228983,0.000202977,2.05E-05,0.000298081,0.000177912,1.99E-05,0.000931242,0.617998367,0.222584617,0.005403001,1.51E-05,5.57E-06
8824,question_generation0,30,"In the NQG framework , we use Gated Recurrent Unit ( GRU ) to build the encoder .",Approach,Feature - Rich Encoder,question_generation,0,4,1,0,,0.010268609,0,negative,4.41E-05,0.000851347,0.001160324,2.51E-06,2.79E-06,6.66E-05,1.47E-05,0.000486965,0.004417507,0.992234464,0.000619494,8.40E-05,1.52E-05
8825,question_generation0,31,"To capture more context information , we use bidirectional GRU ( BiGRU ) to read the inputs in both forward and backward orders .",Approach,Feature - Rich Encoder,question_generation,0,5,1,0,,0.013559816,0,negative,0.000342692,0.005234944,0.026599485,3.45E-06,2.13E-05,0.000162268,0.0001048,0.000669923,0.027147776,0.938350078,0.000548896,0.000776333,3.81E-05
8826,question_generation0,32,"Inspired by ; , the BiGRU encoder not only reads the sentence words , but also handcrafted features , to produce a sequence of word - and - feature vectors .",Approach,Feature - Rich Encoder,question_generation,0,6,1,0,,0.000736608,0,negative,7.48E-05,0.000918404,0.002399402,5.89E-07,3.55E-06,2.26E-05,7.12E-06,0.000138592,0.00894946,0.987332794,4.70E-05,0.000101775,3.86E-06
8827,question_generation0,33,"We concatenate the word vector , lexical feature embedding vectors and answer position indicator embedding vector as the input of BiGRU encoder .",Approach,Feature - Rich Encoder,question_generation,0,7,1,0,,0.00120887,0,negative,1.78E-05,0.000352128,2.82E-05,8.51E-07,1.09E-06,0.000360129,1.57E-05,0.009485748,0.004994814,0.984714833,1.12E-05,1.22E-05,5.34E-06
8828,question_generation0,34,"Concretely , the BiGRU encoder reads the concatenated sentence word vector , lexical features , and answer position feature , x = ( x 1 , x 2 , . . . , x n ) , to produce two sequences of hidden vectors , i.e. , the forward sequence ( h 1 , h 2 , . . . , h n ) and the backward sequence ( h 1 , h 2 , . . . , h n ) .",Approach,Feature - Rich Encoder,question_generation,0,8,1,0,,2.13E-05,0,negative,5.52E-06,0.000110135,5.03E-05,1.55E-07,3.96E-07,1.11E-05,1.37E-06,0.000122455,0.004719429,0.994942629,2.67E-05,8.28E-06,1.53E-06
8829,question_generation0,35,"Lastly , the output sequence of the encoder is the concatenation of the two sequences , i.e. ,",Approach,Feature - Rich Encoder,question_generation,0,9,1,0,,7.74E-06,0,negative,4.21E-06,2.40E-05,1.31E-05,3.62E-08,1.36E-07,2.41E-06,3.09E-07,2.86E-05,0.000358201,0.999557535,3.18E-06,8.19E-06,1.63E-07
8830,question_generation0,36,Answer Position Feature,Approach,,question_generation,0,10,1,0,,0.039318221,0,negative,4.09E-05,0.086103592,0.000235252,1.83E-05,0.000239784,0.000333743,6.43E-05,0.001159377,0.315864368,0.557743942,0.03815311,3.05E-05,1.28E-05
8831,question_generation0,37,"To generate a question with respect to a specific answer in a sentence , we propose using answer position feature to locate the target answer .",Approach,Answer Position Feature,question_generation,0,11,1,0,,0.000808613,0,negative,0.000637308,0.002408276,0.000938271,3.95E-06,1.90E-05,2.52E-05,1.68E-05,0.000118601,0.002851055,0.989472992,0.001017734,0.002474783,1.60E-05
8832,question_generation0,38,"In this work , the BIO tagging scheme is used to label the position of a target answer .",Approach,Answer Position Feature,question_generation,0,12,1,0,,0.000928994,0,negative,0.000250911,0.002002583,0.000422642,1.75E-06,9.12E-06,1.36E-05,8.06E-06,0.00010947,0.002924112,0.993055959,0.000326168,0.000869618,6.04E-06
8833,question_generation0,39,"In this scheme , tag B denotes the start of an answer , tag I continues the answer and tag O marks words that do not form part of an answer .",Approach,Answer Position Feature,question_generation,0,13,1,0,,2.67E-06,0,negative,6.62E-06,9.58E-06,3.54E-06,3.42E-08,1.22E-07,2.03E-06,3.12E-07,1.77E-05,0.000211703,0.999733251,1.75E-06,1.32E-05,1.57E-07
8834,question_generation0,40,The BIO tags of answer position are embedded to real - valued vectors throu and fed to the featurerich encoder .,Approach,Answer Position Feature,question_generation,0,14,1,0,,0.000198066,0,negative,9.72E-05,9.47E-05,7.66E-05,9.32E-08,5.06E-07,3.69E-06,1.26E-06,4.22E-05,0.002282262,0.997348504,1.77E-06,5.07E-05,4.38E-07
8835,question_generation0,41,"With the BIO tagging feature , the answer position is encoded to the hidden vectors and used to generate answer focused questions .",Approach,Answer Position Feature,question_generation,0,15,1,0,,0.000945583,0,negative,0.000338465,0.000215401,0.000451874,2.37E-07,1.51E-06,4.48E-06,2.63E-06,2.82E-05,0.004582576,0.994102945,7.71E-06,0.000262524,1.42E-06
8836,question_generation0,42,Lexical Features,Approach,,question_generation,0,16,1,0,,0.003821386,0,negative,3.37E-05,0.038651129,0.000108613,5.65E-06,0.000730824,0.00057494,6.01E-05,0.001169503,0.034244481,0.923150998,0.001228327,3.91E-05,2.65E-06
8837,question_generation0,43,"Besides the sentence words , we also feed other lexical features to the encoder .",Approach,Lexical Features,question_generation,0,17,1,0,,0.000138144,0,negative,7.50E-05,0.000166994,8.78E-05,7.69E-08,6.78E-07,4.59E-06,2.05E-06,3.47E-05,0.004109336,0.995470991,1.48E-06,4.59E-05,3.84E-07
8838,question_generation0,44,"To encode more linguistic information , we select word case , POS and NER tags as the lexical features .",Approach,Lexical Features,question_generation,0,18,1,0,,0.001359183,0,negative,4.85E-05,0.000223081,2.65E-05,1.53E-07,2.08E-06,2.41E-05,6.45E-06,0.000357255,0.000610967,0.998660455,3.24E-07,3.97E-05,3.35E-07
8839,question_generation0,45,"As an intermediate layer of full parsing , POS tag feature is important in many NLP tasks , such as information extraction and dependency parsing .",Approach,Lexical Features,question_generation,0,19,1,0,,0.000605598,0,negative,0.00010374,5.90E-05,2.79E-05,7.48E-06,3.07E-06,2.36E-05,4.13E-05,4.60E-05,6.14E-05,0.992126818,0.006363139,0.001107127,2.94E-05
8840,question_generation0,46,"Considering that SQuAD is constructed using Wikipedia articles , which contain lots of named entities , we add NER feature to help detecting them .",Approach,Lexical Features,question_generation,0,20,1,0,,3.36E-05,0,negative,0.00074122,0.000118067,0.000104091,1.34E-07,4.68E-06,5.72E-06,8.63E-06,3.05E-05,0.00010151,0.997028609,1.07E-06,0.001855241,5.41E-07
8841,question_generation0,47,Attention - Based Decoder,Approach,,question_generation,0,21,1,0,,0.039060708,0,model,4.34E-06,0.041169278,0.000115903,9.29E-07,2.54E-05,4.17E-05,1.53E-05,0.000245252,0.868566116,0.085013066,0.004794696,5.17E-06,2.86E-06
8842,question_generation0,48,We employ an attention - based GRU decoder to decode the sentence and answer information to generate questions .,Approach,Attention - Based Decoder,question_generation,0,22,1,0,,0.001676714,0,negative,0.00012434,0.000603225,0.000921761,1.16E-06,3.01E-06,1.33E-05,1.05E-05,0.000249147,0.016655599,0.981119441,1.38E-05,0.000267206,1.75E-05
8843,question_generation0,49,"At decoding time step t , the GRU decoder reads the previous word embedding w t?1 and context vector c t?1 to compute the new hidden state st .",Approach,Attention - Based Decoder,question_generation,0,23,1,0,,5.80E-06,0,negative,7.85E-06,9.00E-06,1.64E-05,3.15E-08,8.82E-08,9.54E-07,7.14E-07,2.40E-05,0.000193305,0.999695688,7.77E-07,5.08E-05,4.33E-07
8844,question_generation0,50,We use a linear layer with the last backward encoder hidden state h 1 to initialize the decoder GRU hidden state .,Approach,Attention - Based Decoder,question_generation,0,24,1,0,,2.58E-05,0,negative,6.78E-05,8.38E-05,3.82E-05,1.46E-06,1.19E-06,0.000104657,2.27E-05,0.00461058,0.001442895,0.993540061,1.39E-06,7.32E-05,1.20E-05
8845,question_generation0,51,"The context vector ct for current time step t is computed through the concatenate attention mechanism , which matches the current decoder state st with each encoder hidden state hi to get an importance score .",Approach,Attention - Based Decoder,question_generation,0,25,1,0,,4.89E-06,0,negative,7.72E-06,2.37E-05,1.41E-05,3.13E-08,1.19E-07,1.85E-06,7.03E-07,8.77E-05,0.000517584,0.999324035,3.21E-07,2.18E-05,3.82E-07
8846,question_generation0,52,The importance scores are then normalized to get the current context vector by weighted sum :,Approach,Attention - Based Decoder,question_generation,0,26,1,0,,5.15E-06,0,negative,1.54E-05,3.63E-06,1.89E-05,6.72E-09,3.88E-08,5.08E-07,2.92E-07,1.21E-05,6.58E-05,0.999844358,5.80E-08,3.88E-05,7.40E-08
8847,question_generation0,53,"We then combine the previous word embedding w t ?1 , the current context vector ct , and the decoder state st to get the readout state rt .",Approach,Attention - Based Decoder,question_generation,0,27,1,0,,3.55E-06,0,negative,6.08E-06,4.30E-06,1.34E-05,3.29E-08,6.65E-08,1.12E-06,6.37E-07,2.49E-05,0.000210045,0.999718503,2.32E-07,2.02E-05,4.78E-07
8848,question_generation0,54,The readout state is passed through a maxout hidden layer to predict the next word with a softmax layer over the decoder vocabulary :,Approach,Attention - Based Decoder,question_generation,0,28,1,0,,5.96E-06,0,negative,1.29E-05,5.01E-06,3.05E-05,3.16E-08,6.72E-08,1.97E-06,9.17E-07,4.99E-05,0.000280649,0.999589723,1.76E-07,2.76E-05,5.06E-07
8849,question_generation0,55,where rt is a 2 d - dimensional vector .,Approach,Attention - Based Decoder,question_generation,0,29,1,0,,9.67E-07,0,negative,3.45E-06,1.03E-06,1.67E-06,7.62E-09,2.15E-08,5.59E-07,3.13E-07,1.41E-05,1.07E-05,0.999918423,9.46E-08,4.95E-05,9.15E-08
8850,question_generation0,56,Copy Mechanism,Approach,,question_generation,0,30,1,0,,0.004010851,0,negative,1.95E-05,0.035245105,0.00011134,0.000129205,0.002693439,0.000473862,7.28E-05,0.000913544,0.047694288,0.910777911,0.001827726,2.30E-05,1.83E-05
8851,question_generation0,57,"To deal with the rare and unknown words problem , propose using pointing mechanism to copy rare words from source sentence .",Approach,Copy Mechanism,question_generation,0,31,1,0,,7.37E-06,0,negative,3.40E-05,0.000159239,3.06E-05,5.55E-06,2.07E-05,5.27E-05,9.06E-06,0.00016759,0.000217869,0.999181633,4.81E-05,6.62E-05,6.64E-06
8852,question_generation0,58,We apply this pointing method in our NQG system .,Approach,Copy Mechanism,question_generation,0,32,1,0,,5.99E-05,0,negative,0.000156198,0.001955348,0.000318433,1.97E-06,0.00010365,4.34E-05,2.48E-05,0.00016428,0.001915957,0.994589069,1.40E-05,0.000706516,6.41E-06
8853,question_generation0,59,"When decoding word t , the copy switch takes current decoder state st and context vector ct as input and generates the probability p of copying a word from source sentence :",Approach,Copy Mechanism,question_generation,0,33,1,0,,1.65E-06,0,negative,8.37E-06,4.69E-05,1.49E-05,1.52E-08,5.36E-07,1.21E-06,4.30E-07,1.14E-05,0.000366737,0.999535695,3.92E-07,1.33E-05,5.84E-08
8854,question_generation0,60,where ?,Approach,Copy Mechanism,question_generation,0,34,1,0,,8.02E-08,0,negative,7.69E-07,4.21E-06,3.82E-07,4.42E-08,1.65E-07,4.31E-06,3.11E-07,2.27E-05,3.34E-05,0.999931452,3.08E-07,1.92E-06,4.26E-08
8855,question_generation0,61,is sigmoid function .,Approach,Copy Mechanism,question_generation,0,35,1,0,,2.67E-05,0,negative,1.06E-05,0.000202167,1.06E-05,8.50E-07,2.88E-06,0.000158202,1.60E-05,0.003056543,0.001823991,0.994693422,4.36E-06,1.68E-05,3.68E-06
8856,question_generation0,62,We reuse the attention probability in equation 4 to decide which word to copy .,Approach,Copy Mechanism,question_generation,0,36,1,0,,1.15E-06,0,negative,3.23E-05,0.000232378,3.58E-05,5.34E-08,1.95E-06,6.56E-06,2.04E-06,8.22E-05,0.00353531,0.996056955,1.49E-07,1.40E-05,2.16E-07
8857,question_generation0,63,Experiments and Results,,,question_generation,0,0,1,0,,0.022779798,0,negative,2.87E-05,1.11E-05,2.71E-06,1.71E-07,6.24E-07,1.96E-05,9.54E-05,0.000143494,1.93E-06,0.997728407,0.000542957,0.001423986,9.31E-07
8858,question_generation0,64,We use the SQuAD dataset as our training data .,Experiments and Results,Experiments and Results,question_generation,0,1,1,0,,0.125061587,0,negative,0.000361673,7.25E-06,0.002424194,1.40E-06,4.18E-06,0.000292932,0.004905109,0.000351806,1.19E-06,0.982333031,3.51E-06,0.009224094,8.96E-05
8859,question_generation0,65,SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles .,Experiments and Results,Experiments and Results,question_generation,0,2,1,0,,0.00701146,0,negative,0.003047412,2.19E-05,0.003483788,0.006577265,0.000973879,0.008067495,0.031598593,0.000487702,1.25E-05,0.92266589,9.59E-05,0.008078426,0.014889197
8860,question_generation0,66,"We extract sentence - answer- question triples to build the training , development and test sets 1 .",Experiments and Results,Experiments and Results,question_generation,0,3,1,0,,0.015369862,0,negative,0.000982818,1.60E-05,0.00284447,9.63E-05,0.000193473,0.001099123,0.004258038,0.000360711,7.72E-06,0.986479988,1.08E-05,0.00296248,0.000687993
8861,question_generation0,67,"Since the test set is not publicly available , we randomly halve the development set to construct the new development and test sets .",Experiments and Results,Experiments and Results,question_generation,0,4,1,0,,0.001899771,0,negative,0.000944419,6.92E-06,0.000977779,5.35E-07,8.25E-07,0.001023956,0.002079704,0.003172583,5.08E-06,0.99015246,8.86E-07,0.00160187,3.30E-05
8862,question_generation0,68,"The extracted training , development and test sets contain 86,635 , 8,965 and 8,964 triples respectively .",Experiments and Results,Experiments and Results,question_generation,0,5,1,0,,0.012342182,0,negative,0.000538513,3.00E-06,0.000337309,2.70E-05,4.69E-05,0.000819028,0.002018681,0.000292827,1.93E-06,0.994460824,1.10E-06,0.001249188,0.00020375
8863,question_generation0,69,We introduce the implementation details in the appendix .,Experiments and Results,Experiments and Results,question_generation,0,6,1,0,,0.00016769,0,negative,0.000177819,1.47E-06,3.21E-05,7.69E-06,1.08E-06,0.000294763,0.000167029,0.000373582,2.92E-06,0.998472813,8.27E-07,0.00044643,2.15E-05
8864,question_generation0,70,We conduct several experiments and ablation tests as follows :,Experiments and Results,Experiments and Results,question_generation,0,7,1,0,,0.001981908,0,negative,0.006469237,1.86E-06,0.00483744,1.93E-06,2.76E-06,8.11E-05,0.000635934,4.81E-05,2.57E-06,0.980985669,1.52E-06,0.006894469,3.74E-05
8865,question_generation0,71,PCFG - Trans,Experiments and Results,Experiments and Results,question_generation,0,8,1,1,baselines,0.04162388,0,negative,0.000444683,4.34E-06,0.080812476,1.21E-06,3.47E-07,0.000771226,0.039701052,0.000922296,1.22E-05,0.824564972,0.000169235,0.0507717,0.001824248
8866,question_generation0,72,The rule - based system 1 modified on the code released by .,Experiments and Results,Experiments and Results,question_generation,0,9,1,1,baselines,0.083262564,0,negative,0.000185741,4.97E-06,0.134347319,8.80E-07,4.61E-07,0.000521823,0.002304595,0.000534386,4.92E-05,0.860371574,2.37E-05,0.001217296,0.00043808
8867,question_generation0,73,We modified the code so that it can generate question based on a given word span .,Experiments and Results,Experiments and Results,question_generation,0,10,1,0,,0.002006045,0,negative,0.00211673,7.13E-06,0.00210616,5.61E-06,5.17E-06,0.000609757,0.00077837,0.000586894,2.54E-05,0.991674562,1.93E-06,0.001963366,0.000118955
8868,question_generation0,74,s 2 s+ att,Experiments and Results,Experiments and Results,question_generation,0,11,1,1,baselines,0.001736455,0,negative,0.001677892,1.60E-06,0.000786401,3.10E-06,1.09E-06,0.000611104,0.002329018,0.000445486,5.28E-06,0.984078161,6.53E-06,0.009719503,0.00033483
8869,question_generation0,75,We implement a seq2seq with attention as the baseline method .,Experiments and Results,Experiments and Results,question_generation,0,12,1,1,baselines,0.23874581,0,negative,0.001166705,4.13E-05,0.456452338,2.02E-06,1.61E-06,0.00206242,0.016654765,0.003114985,7.86E-05,0.517364174,5.35E-06,0.002520982,0.000534815
8870,question_generation0,76,NQG,Experiments and Results,,question_generation,0,13,1,1,baselines,0.003552853,0,negative,0.000325873,1.98E-06,0.000556949,5.10E-06,1.05E-06,0.000613762,0.0041162,0.001020198,5.93E-06,0.983657157,2.59E-05,0.008577509,0.001092403
8871,question_generation0,77,We extend the s 2s+ att with our feature - rich encoder to build the NQG system .,Experiments and Results,NQG,question_generation,0,14,1,1,baselines,0.165434967,0,negative,0.001496027,1.49E-06,0.009021713,3.89E-07,1.65E-07,1.64E-05,0.000876161,1.72E-05,2.17E-05,0.954425986,2.11E-06,0.030922508,0.00319817
8872,question_generation0,78,"NQG + Based on NQG , we incorporate copy mechanism to deal with rare words problem .",Experiments and Results,NQG,question_generation,0,15,1,1,baselines,0.543698787,1,negative,0.00059176,1.49E-06,0.223741575,9.37E-07,1.70E-07,6.36E-05,0.011044643,3.27E-05,7.73E-06,0.629240063,2.87E-05,0.086591728,0.048654876
8873,question_generation0,79,"NQG + Pretrain Based on NQG + , we initialize the word embedding matrix with pre-trained GloVe vectors .",Experiments and Results,NQG,question_generation,0,16,1,1,baselines,0.60350918,1,negative,0.000653553,1.15E-06,0.018242941,7.76E-07,1.45E-07,7.75E-05,0.007961646,8.60E-05,4.04E-06,0.915231168,2.55E-06,0.045760717,0.011977787
8874,question_generation0,80,"NQG + STshare Based on NQG + , we make the encoder and decoder share the same embedding matrix .",Experiments and Results,NQG,question_generation,0,17,1,1,baselines,0.349915207,0,negative,0.000723668,1.07E-06,0.129156937,3.40E-07,9.00E-08,2.42E-05,0.006024865,1.42E-05,6.07E-06,0.70866735,1.53E-05,0.140536537,0.014829303
8875,question_generation0,81,NQG ++,Experiments and Results,NQG,question_generation,0,18,1,1,baselines,0.000413933,0,negative,0.000497548,3.19E-08,0.000156377,6.16E-07,3.61E-08,1.33E-05,0.000348422,6.12E-06,5.63E-07,0.974543391,3.31E-07,0.016752337,0.007680883
8876,question_generation0,82,"Based on NQG + , we use both pre-train word embedding and STshare methods , to further improve the performance .",Experiments and Results,NQG,question_generation,0,19,1,1,baselines,0.404951447,0,negative,0.004793021,1.88E-06,0.003367328,7.32E-07,3.31E-07,1.73E-05,0.002641224,2.60E-05,4.84E-06,0.80143314,8.38E-07,0.182661529,0.005051837
8877,question_generation0,83,is removed from NQG model .,Experiments and Results,NQG,question_generation,0,20,1,0,,0.000289775,0,negative,0.000785822,6.61E-08,0.000246067,1.29E-07,4.50E-08,3.64E-06,5.06E-05,4.69E-06,1.15E-06,0.992712665,8.14E-08,0.005618747,0.000576337
8878,question_generation0,84,Results and Analysis,,,question_generation,0,0,1,0,,0.004153659,0,negative,4.92E-05,4.74E-05,7.12E-06,1.41E-07,3.63E-07,1.90E-05,8.00E-05,0.000295611,1.21E-05,0.995144313,0.001609051,0.002734443,1.25E-06
8879,question_generation0,85,We report BLEU - 4 score as the evaluation metric of our NQG system . :,Results and Analysis,Results and Analysis,question_generation,0,1,1,0,,0.001925746,0,negative,0.001079479,9.66E-07,0.000203702,4.44E-07,4.75E-07,1.54E-05,0.000180018,5.22E-05,5.76E-07,0.9056799,6.03E-06,0.092768636,1.22E-05
8880,question_generation0,86,"BLEU evaluation scores of baseline methods , different NQG framework configurations and some ablation tests .",Results and Analysis,Results and Analysis,question_generation,0,2,1,0,,0.003839168,0,negative,0.000443488,5.27E-07,9.90E-05,1.08E-07,8.92E-08,1.19E-05,0.000601338,6.34E-05,1.67E-07,0.665221548,2.79E-05,0.333514917,1.56E-05
8881,question_generation0,87,shows the BLEU - 4 scores of different settings .,Results and Analysis,Results and Analysis,question_generation,0,3,1,0,,0.015884097,0,negative,0.000282904,6.05E-07,5.04E-05,2.53E-07,1.21E-07,3.60E-05,0.000216139,0.000200037,5.39E-07,0.949633844,1.09E-05,0.049549822,1.84E-05
8882,question_generation0,88,We report the beam search results on both development and test sets .,Results and Analysis,Results and Analysis,question_generation,0,4,1,0,,0.323193227,0,negative,0.002704864,3.50E-06,0.000317697,4.76E-07,7.57E-07,9.00E-06,0.000326328,3.53E-05,1.00E-06,0.592181197,5.50E-06,0.404396707,1.76E-05
8883,question_generation0,89,Our NQG framework outperforms the PCFG - Trans and s 2s + att baselines by a large margin .,Results and Analysis,Results and Analysis,question_generation,0,5,1,1,results,0.975832622,1,results,0.003033908,5.66E-07,9.93E-05,3.37E-07,1.30E-07,3.64E-06,0.000703766,1.74E-05,1.37E-07,0.020980144,4.63E-06,0.975110857,4.52E-05
8884,question_generation0,90,This shows that the lexical features and answer position indicator can benefit the question generation .,Results and Analysis,Results and Analysis,question_generation,0,6,1,0,,0.383490237,0,results,0.023105667,7.99E-07,0.000141788,1.41E-07,2.14E-07,3.53E-06,0.000120176,1.41E-05,6.97E-07,0.431477463,2.39E-06,0.545126347,6.67E-06
8885,question_generation0,91,"With the help of copy mechanism , NQG + has a 2.05 BLEU improvement since it solves the rare words problem .",Results and Analysis,Results and Analysis,question_generation,0,7,1,1,results,0.969186537,1,results,0.003651401,2.87E-07,0.000100541,1.81E-07,9.36E-08,2.56E-06,0.000779702,1.21E-05,8.73E-08,0.013689637,2.91E-06,0.981717097,4.34E-05
8886,question_generation0,92,"The extended version , NQG ++ , has 1.11 BLEU score gain over NQG + , which shows that initializing with pre-trained word vectors and sharing them between encoder and decoder help learn better word representation .",Results and Analysis,Results and Analysis,question_generation,0,8,1,1,results,0.985108819,1,results,0.006313872,2.99E-07,0.000102302,2.19E-07,1.06E-07,3.15E-06,0.000812802,1.58E-05,1.01E-07,0.017332417,1.65E-06,0.975372413,4.49E-05
8887,question_generation0,93,Human Evaluation,,,question_generation,0,0,1,0,,0.044121139,0,negative,2.21E-05,1.44E-05,3.59E-06,1.38E-07,3.98E-07,5.02E-05,0.000192147,0.000763632,3.87E-06,0.994976842,0.001167944,0.002802357,2.41E-06
8888,question_generation0,94,We evaluate the PCFG - Trans baseline and NQG ++ with human judges .,Human Evaluation,Human Evaluation,question_generation,0,1,1,0,,0.000144094,0,negative,0.000114674,1.76E-06,0.005368861,4.01E-08,7.36E-08,2.79E-05,0.001354217,0.000124225,1.54E-07,0.974570345,2.62E-06,0.01843255,2.63E-06
8889,question_generation0,95,"The rating scheme is , Good ( 3 ) - The question is meaningful and matches the sentence and answer very well ; Borderline ( 2 ) - The question matches the sentence and answer , more or less ; Bad ( 1 ) - The question either does not make sense or matches the sentence and answer .",Human Evaluation,Human Evaluation,question_generation,0,2,1,0,,1.79E-05,0,negative,2.03E-05,3.08E-07,0.000309377,4.32E-08,1.76E-08,0.000107052,9.49E-05,0.000544866,7.75E-07,0.998424982,9.41E-07,0.000494727,1.74E-06
8890,question_generation0,96,We provide more detailed rating examples in the supplementary material .,Human Evaluation,Human Evaluation,question_generation,0,3,1,0,,1.64E-06,0,negative,2.84E-05,6.04E-08,1.27E-05,1.21E-07,3.72E-08,1.73E-05,3.76E-05,5.38E-05,1.08E-07,0.999495545,1.21E-07,0.00035371,4.26E-07
8891,question_generation0,97,Three human raters labeled 200 questions sampled from the test set to judge if the generated question matches the given sentence and answer span .,Human Evaluation,Human Evaluation,question_generation,0,4,1,0,,5.63E-05,0,negative,8.57E-05,5.71E-07,0.000317451,2.37E-07,5.72E-07,5.43E-05,0.000551614,0.000172956,2.54E-07,0.996654141,6.59E-07,0.002157145,4.37E-06
8892,question_generation0,98,The inter-rater aggreement is measured with Fleiss ' kappa .,Human Evaluation,Human Evaluation,question_generation,0,5,1,0,,1.30E-05,0,negative,0.00010771,6.83E-07,0.00106673,6.22E-08,7.71E-08,8.46E-05,0.000362145,0.000387511,3.90E-07,0.995917999,6.77E-07,0.002069076,2.36E-06
8893,question_generation0,99,Model,,,question_generation,0,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
8894,question_generation0,100,AvgScore Fleiss ' kappa reports the human judge results .,Model,Model,question_generation,0,1,1,0,,0.000766498,0,negative,0.000425924,5.34E-05,0.000206303,5.75E-05,6.66E-05,0.000816022,0.000479429,0.001163129,0.000772585,0.993956856,2.10E-05,0.001948719,3.25E-05
8895,question_generation0,101,The kappa scores show a moderate agreement between the human raters .,Model,Model,question_generation,0,2,1,0,,0.082295763,0,negative,0.002398385,6.27E-05,7.57E-05,1.69E-05,1.58E-05,0.000398823,0.00199631,0.002602917,0.000375751,0.850940082,0.000182042,0.14079363,0.000140805
8896,question_generation0,102,"Our NQG ++ outperforms the PCFG - Trans baseline by 0.76 score , which shows that the questions generated by NQG ++ are more related to the given sentence and answer span .",Model,Model,question_generation,0,3,1,0,,0.851175162,1,results,0.004038868,1.21E-05,0.000110849,3.46E-06,6.25E-06,2.34E-05,0.002822232,0.000141984,1.09E-05,0.018741376,2.20E-05,0.973895983,0.000170687
8897,question_generation0,103,Ablation Test,,,question_generation,0,0,1,0,,0.015479539,0,negative,0.03994102,0.00023604,0.002466813,0.000400472,0.00019302,0.000397422,0.001562496,0.000392813,0.000151941,0.944187437,0.002822704,0.007162837,8.50E-05
8898,question_generation0,104,"The answer position indicator , as expected , plays a crucial role in answer focused question generation as shown in the NQG ?",Ablation Test,Ablation Test,question_generation,0,1,1,1,ablation-analysis,0.90065798,1,ablation-analysis,0.86826294,2.09E-05,0.000354218,1.19E-06,6.39E-06,3.58E-05,0.000157862,3.25E-06,0.000103637,0.130132604,0.000381475,1.65E-05,0.000523263
8899,question_generation0,105,Answer ablation test .,Ablation Test,Ablation Test,question_generation,0,2,1,0,,0.062749291,0,negative,0.190902202,1.73E-06,0.002174216,2.42E-06,3.12E-05,0.000101533,6.67E-05,1.61E-06,2.63E-05,0.806370516,3.29E-05,4.34E-06,0.000284381
8900,question_generation0,106,"Without it , the performance drops terribly since the decoder has no information about the answer subsequence .",Ablation Test,Ablation Test,question_generation,0,3,1,0,,0.708188426,1,ablation-analysis,0.856274722,1.16E-06,0.000150194,4.27E-07,4.01E-06,1.05E-05,1.73E-05,5.86E-07,1.28E-05,0.143484045,6.59E-06,3.82E-06,3.38E-05
8901,question_generation0,107,"Ablation tests , NQG ? Case , NQG?",Ablation Test,Ablation Test,question_generation,0,4,1,0,,0.424224946,0,ablation-analysis,0.550789852,7.08E-06,0.003845406,1.78E-05,0.000153817,0.00012796,0.000409929,1.62E-06,2.91E-05,0.441163348,0.000319762,2.20E-05,0.003112328
8902,question_generation0,108,POS and NQG ?,Ablation Test,Ablation Test,question_generation,0,5,1,0,,0.120283727,0,negative,0.120376986,1.24E-05,0.005786044,3.64E-06,7.67E-05,0.000143141,0.000821963,3.71E-06,4.13E-05,0.866528563,0.001190169,2.87E-05,0.004986714
8903,question_generation0,109,"NER , show that word case , POS and NER tag features contributes to question generation .",Ablation Test,Ablation Test,question_generation,0,6,1,1,ablation-analysis,0.442812033,0,negative,0.411399582,2.69E-05,0.008889615,5.57E-06,3.37E-05,0.000134805,0.001140145,4.07E-06,8.43E-05,0.532482015,0.023056792,5.54E-05,0.022687058
8904,question_generation0,110,provides three examples generated by NQG ++.,Ablation Test,Ablation Test,question_generation,0,7,1,0,,0.013277732,0,negative,0.061352522,1.76E-06,0.000661638,2.18E-05,0.000566735,0.000103311,0.000145295,1.03E-06,4.73E-06,0.935390496,2.61E-05,7.10E-06,0.001717482
8905,question_generation0,111,The words with underline are the target answers .,Ablation Test,Ablation Test,question_generation,0,8,1,0,,0.000580909,0,negative,0.008222107,1.44E-05,0.000580049,1.72E-06,9.68E-06,0.000360676,8.56E-05,2.76E-05,0.000589506,0.989719353,2.19E-05,4.37E-07,0.000366944
8906,question_generation0,112,"These three examples are with different question types , namely WHEN , WHAT and WHO respectively .",Ablation Test,Ablation Test,question_generation,0,9,1,0,,0.000524229,0,negative,0.003679262,1.43E-06,9.29E-05,2.03E-06,6.53E-05,4.22E-05,1.34E-05,1.12E-06,1.01E-05,0.996008347,5.55E-06,5.15E-07,7.79E-05
8907,question_generation0,113,It can be observed that the decoder can ' copy ' spans from input sentences to generate the questions .,Ablation Test,Ablation Test,question_generation,0,10,1,0,,0.004759822,0,negative,0.363893628,1.10E-05,0.002111488,3.87E-07,5.19E-06,2.29E-05,3.02E-05,2.05E-06,0.000819985,0.633032339,7.86E-06,2.79E-06,6.02E-05
8908,question_generation0,114,"Besides the underlined words , other meaningful spans can also be used as answer to generate correct answer focused questions .",Ablation Test,Ablation Test,question_generation,0,11,1,0,,0.001121642,0,negative,0.034829419,2.10E-06,0.000215945,1.20E-06,2.32E-05,5.39E-05,2.04E-05,2.27E-06,5.41E-05,0.964670874,7.22E-06,1.55E-06,0.00011783
8909,question_generation0,115,Case Study,Ablation Test,,question_generation,0,12,1,0,,0.031148399,0,negative,0.094046921,4.01E-05,0.002751319,2.45E-05,0.000190353,0.000442141,0.000579449,2.89E-05,0.000492449,0.890635155,0.000212037,9.78E-06,0.010546813
8910,question_generation0,116,"Type of Generated Questions Following , we classify the questions into different types , i.e. , WHAT , HOW , WHO , WHEN , WHICH , WHERE , WHY and OTHER .",Ablation Test,Case Study,question_generation,0,13,1,0,,6.13E-05,0,negative,3.35E-05,3.84E-07,1.33E-06,2.96E-08,2.73E-06,1.23E-05,1.02E-05,9.52E-07,7.06E-07,0.999934928,3.96E-07,2.44E-06,8.51E-08
8911,question_generation0,117,We evaluate the precision and recall of each question types .,Ablation Test,Case Study,question_generation,0,14,1,0,,5.46E-05,0,negative,2.01E-05,1.14E-06,4.95E-07,8.67E-09,8.18E-07,7.44E-06,8.47E-06,2.33E-06,1.65E-06,0.99995499,2.45E-07,2.30E-06,4.98E-08
8912,question_generation0,118,provides the precision and recall metrics of different question types .,Ablation Test,Case Study,question_generation,0,15,1,0,,0.000104686,0,negative,3.53E-05,2.77E-06,3.33E-06,9.10E-07,1.54E-05,3.07E-05,1.28E-05,3.10E-06,1.17E-05,0.999878563,2.83E-06,1.85E-06,7.68E-07
8913,question_generation0,119,The precision I :,Ablation Test,Case Study,question_generation,0,16,1,0,,0.001561779,0,negative,2.64E-05,1.82E-06,3.60E-06,5.18E-08,3.76E-07,3.07E-05,2.22E-05,7.03E-06,3.42E-05,0.999866444,4.89E-06,1.90E-06,3.32E-07
8914,question_generation0,120,"in 1226 , immediately after returning from the west , genghis khan began a retaliatory attack on the tanguts .",Ablation Test,Case Study,question_generation,0,17,1,0,,2.63E-05,0,negative,0.000108454,5.64E-07,2.31E-06,4.93E-07,2.25E-05,1.67E-05,9.78E-06,6.86E-07,1.69E-06,0.999833435,1.24E-06,1.59E-06,5.33E-07
8915,question_generation0,121,G : in which year did genghis khan strike against the tanguts ?,Ablation Test,Case Study,question_generation,0,18,1,0,,4.48E-06,0,negative,2.13E-05,1.48E-07,6.34E-07,1.95E-07,1.64E-06,1.08E-05,2.62E-06,5.72E-07,8.47E-07,0.999959702,1.06E-06,3.69E-07,1.34E-07
8916,question_generation0,122,O : in what year did genghis khan begin a retaliatory attack on the tanguts ?,Ablation Test,Case Study,question_generation,0,19,1,0,,1.51E-05,0,negative,2.60E-05,6.77E-07,1.42E-06,8.95E-07,3.22E-06,2.15E-05,8.65E-06,1.65E-06,2.64E-06,0.999920084,1.15E-05,7.78E-07,9.83E-07
8917,question_generation0,123,"I : in week 10 , manning suffered a partial tear of the plantar fasciitis in his left foot .",Ablation Test,Case Study,question_generation,0,20,1,0,,1.07E-05,0,negative,0.001849964,1.33E-06,5.88E-05,8.83E-07,1.61E-05,3.32E-05,4.10E-05,9.45E-07,7.41E-06,0.99797121,9.54E-07,1.72E-05,1.02E-06
8918,question_generation0,124,"G : in the 10th week of the 2015 season , what injury was peyton manning dealing with ?",Ablation Test,Case Study,question_generation,0,21,1,0,,7.41E-06,0,negative,2.16E-05,2.61E-07,6.86E-07,3.42E-07,2.16E-06,1.59E-05,3.73E-06,1.02E-06,1.54E-06,0.999951142,9.10E-07,4.27E-07,2.38E-07
8919,question_generation0,125,O : what did manning suffer in his left foot ?,Ablation Test,Case Study,question_generation,0,22,1,0,,2.16E-05,0,negative,2.57E-05,3.15E-07,1.87E-06,3.15E-07,3.89E-06,1.16E-05,5.27E-06,6.38E-07,1.73E-06,0.999946493,1.10E-06,7.04E-07,4.08E-07
8920,question_generation0,126,I :,Ablation Test,Case Study,question_generation,0,23,1,0,,2.22E-05,0,negative,3.21E-05,3.89E-07,1.47E-06,1.24E-08,1.82E-07,4.24E-06,3.01E-06,7.29E-07,6.84E-06,0.999949943,2.12E-07,8.52E-07,6.17E-08
8921,question_generation0,127,"like the lombardi trophy , the "" 50 "" will be designed by tiffany & co . .",Ablation Test,Case Study,question_generation,0,24,1,0,,3.76E-05,0,negative,3.72E-05,1.07E-06,4.32E-06,2.88E-07,8.98E-06,4.81E-05,1.74E-05,3.59E-06,1.08E-05,0.999866424,1.60E-07,1.16E-06,5.35E-07
8922,question_generation0,128,G : who designed the vince lombardi trophy ?,Ablation Test,Case Study,question_generation,0,25,1,0,,1.99E-05,0,negative,2.68E-05,1.39E-07,7.64E-07,3.75E-07,4.14E-06,1.24E-05,2.71E-06,4.33E-07,7.44E-07,0.999950506,3.31E-07,3.99E-07,2.00E-07
8923,question_generation0,129,O : who designed the lombardi trophy ?:,Ablation Test,Case Study,question_generation,0,26,1,0,,7.98E-05,0,negative,7.81E-05,4.52E-07,5.40E-06,7.22E-08,3.10E-06,6.16E-06,5.42E-06,3.71E-07,2.24E-06,0.999895806,4.21E-07,2.20E-06,2.25E-07
8924,question_generation0,130,"Examples of generated questions , I is the input sentence , G is the gold question and O is the NQG ++ generated question .",Ablation Test,Case Study,question_generation,0,27,1,0,,5.98E-06,0,negative,6.56E-06,2.85E-07,5.26E-07,1.99E-08,1.42E-06,1.69E-05,1.65E-05,2.40E-06,1.09E-06,0.999952579,1.13E-07,1.41E-06,2.55E-07
8925,question_generation0,131,The underlined words are the target answers .,Ablation Test,Case Study,question_generation,0,28,1,0,,4.59E-06,0,negative,3.37E-06,7.82E-07,3.27E-07,2.82E-08,3.94E-07,1.83E-05,6.22E-06,5.08E-06,1.35E-05,0.999951361,1.69E-07,3.63E-07,1.29E-07
8926,question_generation0,132,"For the majority question types , WHAT , HOW , WHO and WHEN types , our NQG ++ model performs well for both precision and recall .",Ablation Test,Case Study,question_generation,0,29,1,0,,0.687309334,1,negative,0.135603898,1.77E-05,2.81E-05,4.70E-06,3.61E-05,0.00046797,0.068260793,0.000103982,6.08E-06,0.732248502,2.64E-05,0.062640693,0.000555022
8927,question_generation0,133,"For type WHICH , it can be observed that neither precision nor recall are acceptable .",Ablation Test,Case Study,question_generation,0,30,1,0,,0.001513703,0,negative,0.004096869,2.85E-07,3.24E-06,1.79E-08,2.96E-06,4.28E-06,9.49E-05,5.19E-07,2.39E-07,0.995565514,1.86E-07,0.000230712,2.85E-07
8928,question_generation0,134,"Two reasons may cause this : a ) some WHICH - type questions can be asked in other manners , e.g. , ' which team ' can be replaced with ' who' ; b) WHICH - type questions account for about 7.2 % in training data , which may not be sufficient to learn to generate this type of questions .",Ablation Test,Case Study,question_generation,0,31,1,0,,3.60E-05,0,negative,1.69E-05,6.77E-07,5.33E-07,1.07E-07,3.90E-06,1.61E-05,1.03E-05,1.86E-06,9.75E-07,0.999946517,4.16E-07,1.27E-06,3.80E-07
8929,question_generation0,135,The same reason can also affect the precision and recall of WHY - type questions .,Ablation Test,Case Study,question_generation,0,32,1,0,,2.29E-05,0,negative,1.46E-05,1.22E-07,2.07E-07,1.09E-08,2.67E-07,2.53E-06,2.85E-06,4.24E-07,6.38E-07,0.999976979,2.79E-07,9.93E-07,6.80E-08
8930,question_generation0,136,Conclusion and Future Work,,,question_generation,0,0,1,0,,0.000993755,0,negative,6.55E-05,5.57E-05,6.74E-06,8.33E-07,5.66E-07,8.35E-05,7.13E-05,0.000591015,4.69E-05,0.993684537,0.004983168,0.000405376,4.84E-06
8931,question_generation0,142,A Implementation Details,,,question_generation,0,0,1,0,,0.001181912,0,negative,8.83E-05,0.001021637,1.64E-05,0.000210437,0.000103111,0.000708779,0.000454491,0.003601627,0.000158694,0.989483574,0.0038694,0.000248988,3.46E-05
8932,question_generation0,143,A.1 Model Parameters,,,question_generation,0,0,1,0,,0.015118288,0,negative,0.000205344,0.00021654,1.63E-05,1.33E-05,4.09E-06,0.000144135,7.29E-05,0.001423034,0.00011359,0.995614457,0.001800708,0.000365211,1.04E-05
8933,question_generation0,144,We use the same vocabulary for both encoder and decoder .,A.1 Model Parameters,A.1 Model Parameters,question_generation,0,1,1,0,,0.029630782,0,negative,0.000431429,0.023039258,0.001842239,3.28E-05,0.000175761,0.006755506,0.00036427,0.069291913,0.030438871,0.8667063,0.000788261,0.000103389,3.00E-05
8934,question_generation0,145,"The vocabulary is collected from the training data and we keep the top 20,000 frequent words .",A.1 Model Parameters,A.1 Model Parameters,question_generation,0,2,1,0,,0.002485029,0,negative,0.000264953,0.003500408,0.000170902,5.07E-05,0.0004247,0.018729616,0.000340844,0.117105022,0.001277658,0.857874085,0.000175209,5.75E-05,2.84E-05
8935,question_generation0,146,We set the word embedding size to 300 and all GRU hidden state sizes to 512 .,A.1 Model Parameters,A.1 Model Parameters,question_generation,0,3,1,0,,0.282998925,0,hyperparameters,0.000383817,0.006782986,0.000210123,0.000105885,8.32E-05,0.075496844,0.001739392,0.696016896,0.005859564,0.211031047,0.001965134,0.000127352,0.000197805
8936,question_generation0,147,The lexical and answer position features are embedded to 32 - dimensional vectors .,A.1 Model Parameters,A.1 Model Parameters,question_generation,0,4,1,0,,0.100465499,0,negative,0.000341691,0.012519506,0.000733481,3.12E-05,7.34E-05,0.033332274,0.000609094,0.258010329,0.055737777,0.637301952,0.001189271,5.35E-05,6.66E-05
8937,question_generation0,148,We use dropout with probability p = 0.5 .,A.1 Model Parameters,A.1 Model Parameters,question_generation,0,5,1,0,,0.221511977,0,hyperparameters,0.000631859,0.010054943,0.000387437,0.000492188,0.000291239,0.103499406,0.002427431,0.731950193,0.005006531,0.143972962,0.000813547,0.000114708,0.000357555
8938,question_generation0,149,"During testing , we use beam search with beam size 12 .",A.1 Model Parameters,A.1 Model Parameters,question_generation,0,6,1,0,,0.303833775,0,negative,0.000585827,0.003888049,0.000568944,0.000236636,0.00074273,0.076261825,0.001857625,0.083725633,0.001436081,0.829757181,0.000579208,0.000250235,0.000110027
8939,question_generation0,150,A.2 Lexical Feature Annotation,A.1 Model Parameters,,question_generation,0,7,1,0,,0.000347914,0,negative,0.001774519,0.000491975,0.000960353,3.62E-06,1.66E-05,0.000102553,0.000113039,0.000367159,0.000800185,0.953662601,0.039578864,0.002109318,1.92E-05
8940,question_generation0,151,We use Stanford CoreNLP v 3.7.0 to annotate POS and NER tags in sentences with its default configuration and pre-trained models .,A.1 Model Parameters,A.2 Lexical Feature Annotation,question_generation,0,8,1,0,,0.105432365,0,negative,2.20E-05,0.001155974,0.000230271,0.000191559,0.000614997,0.004698876,0.000258723,0.005625434,0.000584087,0.986500797,7.73E-05,2.55E-05,1.44E-05
8941,question_generation0,152,A.3 Model Training,,,question_generation,0,0,1,0,,0.079670952,0,negative,0.001003967,0.000317369,0.000303013,1.06E-06,2.09E-06,6.65E-05,0.000170526,0.000778074,0.000104075,0.985486188,0.004713545,0.007044293,9.27E-06
8942,question_generation0,153,"We initialize model parameters randomly using a Gaussian distribution with Xavier scheme ( Glorot and Bengio , 2010 ) .",A.3 Model Training,A.3 Model Training,question_generation,0,1,1,0,,0.045015947,0,negative,0.0013447,0.0005041,0.000223678,2.93E-05,6.63E-06,0.003347693,0.000257553,0.096534042,0.0019625,0.895170719,0.00011988,0.000230743,0.000268455
8943,question_generation0,154,We use a combination of Adam and simple SGD as our the optimizing algorithms .,A.3 Model Training,A.3 Model Training,question_generation,0,2,1,0,,0.113911807,0,negative,0.012008225,0.002828732,0.001609062,0.00084174,9.20E-05,0.012313942,0.001817553,0.195695462,0.004952118,0.7645893,0.000167646,0.001019741,0.002064453
8944,question_generation0,155,"The training is separated into two phases , the first phase is optimizing the loss function with Adam and the second is with simple SGD .",A.3 Model Training,A.3 Model Training,question_generation,0,3,1,0,,0.003138718,0,negative,0.004819556,0.001359594,0.001482693,1.60E-05,2.14E-05,5.40E-05,2.59E-05,0.001201598,0.003590683,0.986856613,6.12E-05,0.000439495,7.13E-05
8945,question_generation0,156,"For the Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .",A.3 Model Training,A.3 Model Training,question_generation,0,4,1,0,,0.039475861,0,negative,0.005323041,0.000846718,0.000241453,0.000373687,4.35E-05,0.01578044,0.001606974,0.28072088,0.001363939,0.689801716,0.000258122,0.001227503,0.002412026
8946,question_generation0,157,"We use Adam optimizer until the BLEU score on the development set drops for six consecutive tests ( we test the BLEU score on the development set for every 1,000 batches ) .",A.3 Model Training,A.3 Model Training,question_generation,0,5,1,0,,0.058861423,0,negative,0.007266512,0.001106113,0.000456019,8.75E-05,2.97E-05,0.004499119,0.001103665,0.113138273,0.002093883,0.86740273,0.000224945,0.001024345,0.001567113
8947,question_generation0,158,Then we switch to a simple SGD optimizer with initial learning rate ? = 0.5 and halve it if the BLEU score on the development set drops for twelve consecutive tests .,A.3 Model Training,A.3 Model Training,question_generation,0,6,1,0,,0.005196677,0,negative,0.01156368,0.000415263,0.002059292,1.04E-05,7.56E-06,0.000341963,0.000216297,0.006060815,0.001030712,0.976161375,7.53E-05,0.001882352,0.000175024
8948,question_generation0,159,"We also apply gradient clipping with range [ ? 5 , 5 ] for both Adam and SGD phases .",A.3 Model Training,A.3 Model Training,question_generation,0,7,1,0,,0.115617158,0,negative,0.044007725,0.003417134,0.002608737,0.000514589,0.0001313,0.009807996,0.003054091,0.161855173,0.004333243,0.763963271,0.000155836,0.002840137,0.003310769
8949,question_generation0,160,"To both speedup the training and converge quickly , we use mini-batch size 64 by grid search .",A.3 Model Training,A.3 Model Training,question_generation,0,8,1,0,,0.080888953,0,negative,0.008070525,0.001405394,0.000555128,0.00038071,6.67E-05,0.014937966,0.002711301,0.270007181,0.002476581,0.690707989,0.000287076,0.001683813,0.00670963
8950,question_generation0,161,B Human Evaluation Examples,,,question_generation,0,0,1,0,,0.003557523,0,negative,3.00E-05,3.13E-05,1.92E-05,8.29E-08,2.19E-07,1.46E-05,0.000168736,0.000352207,6.41E-06,0.982083016,0.011636969,0.005654422,2.85E-06
8951,question_generation0,162,We evaluate the PCFG - Trans baseline and NQG ++ with human judges .,B Human Evaluation Examples,B Human Evaluation Examples,question_generation,0,1,1,0,,0.00068076,0,negative,4.84E-05,2.54E-07,0.000940616,2.05E-08,1.30E-08,4.49E-06,0.000456721,2.53E-05,7.63E-08,0.975334404,7.78E-07,0.023185257,3.74E-06
8952,question_generation0,163,The rating scheme is provided in .,B Human Evaluation Examples,B Human Evaluation Examples,question_generation,0,2,1,0,,0.000308989,0,negative,0.000105684,9.10E-08,0.000257097,2.86E-07,2.16E-08,1.08E-05,5.38E-05,4.06E-05,1.10E-06,0.997609481,2.90E-07,0.001909535,1.11E-05
8953,question_generation0,164,The human judges are asked to label the generated questions if they match the given sentence and answer span according to the rating scheme and examples .,B Human Evaluation Examples,B Human Evaluation Examples,question_generation,0,3,1,0,,5.15E-05,0,negative,2.38E-05,1.08E-07,0.000175839,1.15E-08,9.09E-09,1.75E-06,1.22E-05,1.45E-05,2.81E-07,0.998685929,1.21E-07,0.001084465,1.06E-06
8954,question_generation0,165,We provide some example questions with different scores in .,B Human Evaluation Examples,B Human Evaluation Examples,question_generation,0,4,1,0,,3.69E-06,0,negative,1.04E-05,4.85E-09,3.63E-06,1.25E-08,2.66E-09,1.18E-06,5.67E-06,4.54E-06,3.26E-08,0.999499725,2.95E-08,0.000474347,4.03E-07
8955,question_generation0,166,For the first,B Human Evaluation Examples,,question_generation,0,5,1,0,,1.49E-05,0,negative,1.93E-05,1.19E-08,9.97E-05,4.09E-09,1.13E-09,9.02E-07,1.11E-05,3.43E-06,7.91E-08,0.998487342,9.70E-08,0.001377358,6.59E-07
8956,question_generation0,167,Score,B Human Evaluation Examples,,question_generation,0,6,1,0,,9.02E-06,0,negative,5.77E-05,6.52E-08,3.01E-05,2.35E-07,1.15E-08,2.41E-05,0.000196991,0.000132648,5.81E-07,0.989168317,2.34E-06,0.01034692,3.99E-05
8957,question_generation0,168,Rating scheme,B Human Evaluation Examples,,question_generation,0,7,1,0,,5.17E-05,0,negative,8.11E-05,1.42E-07,0.000489056,2.08E-07,2.05E-08,2.52E-05,0.000263344,0.000134394,1.45E-06,0.988940527,1.06E-06,0.010024949,3.85E-05
8958,question_generation0,169,3 : Good,B Human Evaluation Examples,Rating scheme,question_generation,0,8,1,0,,3.11E-05,0,negative,8.31E-05,5.96E-08,0.000130824,4.52E-08,1.98E-08,1.24E-05,2.67E-05,2.31E-05,4.79E-07,0.995070913,3.62E-07,0.004649686,2.27E-06
8959,question_generation0,170,The question is meaningful and matches the sentence and answer very well 2 : Borderline,B Human Evaluation Examples,Rating scheme,question_generation,0,9,1,0,,2.79E-06,0,negative,8.92E-06,1.43E-08,7.01E-06,2.53E-08,1.08E-08,4.53E-06,9.23E-06,8.06E-06,7.25E-08,0.9990301,2.71E-07,0.000930857,9.11E-07
8960,question_generation0,171,"The question matches the sentence and answer , more or less 1 : Bad",B Human Evaluation Examples,Rating scheme,question_generation,0,10,1,0,,8.73E-07,0,negative,2.66E-06,1.42E-08,3.28E-06,2.02E-08,8.20E-09,3.54E-06,2.47E-06,7.17E-06,1.54E-07,0.999873231,1.18E-07,0.000106916,4.13E-07
8961,question_generation0,172,The question either does not make sense or matches the sentence and answer : Human rating scheme .,B Human Evaluation Examples,Rating scheme,question_generation,0,11,1,0,,1.41E-06,0,negative,5.61E-06,7.86E-08,4.02E-05,2.49E-08,1.33E-08,3.66E-06,5.55E-06,1.36E-05,3.08E-07,0.999520771,1.62E-06,0.000406583,1.95E-06
8962,question_generation0,173,"score 3 example , the question makes sense and the target answer "" reason "" can be used to answer it given the input sentence .",B Human Evaluation Examples,Rating scheme,question_generation,0,12,1,0,,9.42E-07,0,negative,5.46E-06,1.45E-08,2.87E-06,2.44E-08,1.11E-08,3.08E-06,1.69E-06,6.87E-06,1.77E-07,0.999883052,6.56E-08,9.63E-05,3.61E-07
8963,question_generation0,174,"For the second score 2 example , the question is inadequate for answering the sentence since the answer is about prime number .",B Human Evaluation Examples,Rating scheme,question_generation,0,13,1,0,,5.15E-06,0,negative,1.25E-05,9.42E-09,2.52E-06,8.77E-09,2.31E-08,1.63E-06,3.85E-06,4.19E-06,3.20E-08,0.999372176,2.91E-08,0.000602815,2.44E-07
8964,question_generation0,175,"However , given the sentence , a reasonable person will give the targeted answer of the question .",B Human Evaluation Examples,Rating scheme,question_generation,0,14,1,0,,1.05E-06,0,negative,3.66E-06,8.66E-09,6.83E-07,4.65E-09,4.68E-09,5.74E-07,3.96E-07,2.60E-06,6.11E-08,0.999933893,2.31E-08,5.80E-05,7.32E-08
8965,question_generation0,176,"For the third score 1 example , the question is totally wrong given the sentence and answer .",B Human Evaluation Examples,Rating scheme,question_generation,0,15,1,0,,3.12E-06,0,negative,7.67E-06,7.61E-09,2.00E-06,1.12E-08,2.65E-08,1.74E-06,2.91E-06,3.84E-06,3.17E-08,0.999608361,1.45E-08,0.00037316,2.28E-07
8966,question_generation0,177,Score Sentence and generated question,B Human Evaluation Examples,Rating scheme,question_generation,0,16,1,0,,4.70E-06,0,negative,6.01E-05,5.83E-08,1.36E-05,7.74E-08,4.00E-08,9.20E-06,5.78E-05,2.70E-05,3.01E-07,0.989270724,8.50E-07,0.010549722,1.06E-05
8967,question_generation0,178,"3 I : -lsb -... - rsb- for reason is the greatest enemy that faith has ; it never comes to the aid of spiritual things . """,B Human Evaluation Examples,Rating scheme,question_generation,0,17,1,0,,1.19E-06,0,negative,6.21E-05,4.71E-08,2.21E-05,3.05E-07,6.67E-08,1.37E-05,1.35E-05,1.65E-05,5.96E-07,0.999136317,2.22E-07,0.00073137,3.25E-06
8968,question_generation0,179,O : what is the biggest enemy that faith has have ?,B Human Evaluation Examples,Rating scheme,question_generation,0,18,1,0,,4.05E-06,0,negative,1.23E-05,5.85E-08,8.91E-06,1.15E-07,2.62E-08,5.65E-06,2.67E-06,1.55E-05,5.42E-07,0.999843826,1.69E-07,0.000108764,1.51E-06
8969,question_generation0,180,"2 I : in all other rows - lrb - a = 1 , 2 , 4 , 5 , 7 , and 8 - rrb - there are infinitely many prime numbers .",B Human Evaluation Examples,Rating scheme,question_generation,0,19,1,0,,6.66E-06,0,negative,3.34E-05,5.06E-08,8.98E-05,4.06E-09,8.38E-09,2.34E-06,7.15E-06,7.60E-06,2.40E-07,0.99835678,4.48E-08,0.00150229,2.88E-07
8970,question_generation0,181,O : how many numbers are in all other rows ?,B Human Evaluation Examples,Rating scheme,question_generation,0,20,1,0,,9.36E-06,0,negative,3.66E-06,5.67E-08,1.17E-05,1.30E-08,4.78E-09,3.66E-06,2.96E-06,2.12E-05,5.49E-07,0.999863095,1.26E-07,9.23E-05,7.08E-07
8971,question_generation0,182,"1 I : while genghis khan never conquered all of china , his grandson kublai khan completed that conquest and established the yuan dynasty that is often credited with re-uniting china .",B Human Evaluation Examples,Rating scheme,question_generation,0,21,1,0,,1.67E-06,0,negative,2.32E-05,6.11E-08,5.15E-06,2.26E-06,8.59E-08,2.07E-05,6.09E-06,1.92E-05,8.96E-07,0.999789324,4.22E-07,0.000128093,4.51E-06
8972,question_generation0,183,O : who did kublai khan defeat that conquered all of china ?,B Human Evaluation Examples,Rating scheme,question_generation,0,22,1,0,,1.18E-06,0,negative,1.09E-05,3.38E-08,8.61E-06,8.70E-08,2.70E-08,3.25E-06,1.90E-06,7.27E-06,3.66E-07,0.999883478,1.03E-07,8.27E-05,1.27E-06
8973,natural_language_inference59,1,title,,,natural_language_inference,59,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
8974,natural_language_inference59,2,Neural Paraphrase Identification of Questions with Noisy Pretraining,title,,natural_language_inference,59,1,1,1,research-problem,0.991061638,1,research-problem,2.59E-08,6.33E-06,3.37E-08,1.29E-07,9.24E-08,8.13E-08,1.27E-06,8.99E-07,2.36E-07,0.00193214,0.998058557,1.47E-07,5.81E-08
8975,natural_language_inference59,3,abstract,,,natural_language_inference,59,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
8976,natural_language_inference59,4,We present a solution to the problem of paraphrase identification of questions .,abstract,abstract,natural_language_inference,59,1,1,1,research-problem,0.848698924,1,research-problem,5.95E-07,0.000654924,9.94E-07,4.80E-06,1.31E-05,1.17E-06,2.43E-06,8.22E-06,1.50E-05,0.01870173,0.980596158,4.19E-07,4.40E-07
8977,natural_language_inference59,5,"We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model ( Parikh et al. , 2016 ) results in accurate performance on this task , while being far simpler than many competing neural architectures .",abstract,abstract,natural_language_inference,59,2,1,0,,0.214718322,0,negative,5.19E-05,0.070591603,5.13E-05,7.90E-05,0.001588669,4.27E-05,6.03E-05,0.000365992,0.000726448,0.555124193,0.371263659,4.85E-05,5.85E-06
8978,natural_language_inference59,6,"Furthermore , when the model is pretrained on a noisy dataset of automatically collected question paraphrases , it obtains the best reported performance on the dataset .",abstract,abstract,natural_language_inference,59,3,1,0,,0.011288513,0,negative,0.00017954,0.002240024,6.80E-06,3.28E-05,9.69E-05,4.00E-05,0.000145171,0.000697125,3.61E-05,0.734497793,0.261197845,0.000823898,5.99E-06
8979,natural_language_inference59,7,Introduction,,,natural_language_inference,59,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
8980,natural_language_inference59,8,Question paraphrase identification is a widely useful NLP application .,Introduction,Introduction,natural_language_inference,59,1,1,1,research-problem,0.450915116,0,research-problem,1.27E-06,0.00013065,4.32E-07,2.15E-05,4.35E-05,5.89E-06,2.62E-05,5.04E-06,1.68E-05,0.030385348,0.969357717,1.84E-06,3.80E-06
8981,natural_language_inference59,9,"For example , in question - andanswer ( QA ) forums ubiquitous on the Web , there are vast numbers of duplicate questions .",Introduction,Introduction,natural_language_inference,59,2,1,0,,0.469610447,0,research-problem,1.40E-06,0.00026385,3.18E-07,3.24E-05,8.56E-05,2.05E-05,1.70E-05,1.63E-05,5.14E-05,0.175957527,0.823549273,1.37E-06,3.08E-06
8982,natural_language_inference59,10,Identifying these duplicates and consolidating their answers increases the efficiency of such QA forums .,Introduction,Introduction,natural_language_inference,59,3,1,0,,0.674871993,1,research-problem,5.31E-06,0.000326094,5.78E-07,8.67E-05,0.000227733,2.11E-05,2.09E-05,1.24E-05,5.95E-05,0.204386522,0.794845805,2.85E-06,4.54E-06
8983,natural_language_inference59,11,"Moreover , identifying questions with the same semantic content could help Web-scale question answering systems thatare increasingly concentrating on retrieving focused answers to users ' queries .",Introduction,Introduction,natural_language_inference,59,4,1,0,,0.389186176,0,research-problem,7.97E-06,0.002883327,2.01E-06,3.68E-05,0.000279406,2.65E-05,3.38E-05,3.52E-05,0.001057323,0.220222141,0.77539784,1.19E-05,5.91E-06
8984,natural_language_inference59,12,"Here , we focus on a recent dataset published by the QA website Quora.com containing over 400 K annotated question pairs containing binary paraphrase labels .",Introduction,Introduction,natural_language_inference,59,5,1,0,,0.277964667,0,dataset,2.62E-05,0.018456503,4.03E-05,0.005422638,0.712835615,0.000335092,0.00035621,6.01E-05,0.00043208,0.230393363,0.031579125,2.15E-05,4.14E-05
8985,natural_language_inference59,13,"We believe that this dataset presents a great opportunity to the NLP research community and practitioners due to its scale and quality ; it can result in systems that accurately identify duplicate questions , thus increasing the quality of many QA forums .",Introduction,Introduction,natural_language_inference,59,6,1,0,,0.751056474,1,negative,6.06E-05,0.012592956,2.08E-05,0.005676411,0.428878114,0.000748864,0.000423649,0.000135295,0.000805926,0.538457968,0.012110538,5.17E-05,3.72E-05
8986,natural_language_inference59,14,"We examine a simple model family , the decomposable attention model of , that has shown promise in modeling natural language inference and has inspired recent work on similar tasks .",Introduction,Introduction,natural_language_inference,59,7,1,1,approach,0.761974471,1,approach,0.000111148,0.710518973,0.000230978,0.000681203,0.046371435,0.000606421,0.000353727,0.000816418,0.06578647,0.155228168,0.019190788,5.82E-05,4.61E-05
8987,natural_language_inference59,15,We present two contributions .,Introduction,Introduction,natural_language_inference,59,8,1,0,,0.00602589,0,negative,2.81E-05,0.009982156,1.05E-05,0.000119854,0.001122634,0.00025644,3.23E-05,0.000147315,0.015001591,0.954268117,0.019019974,6.09E-06,4.82E-06
8988,natural_language_inference59,16,"First , to mitigate data sparsity , we modify the input representation of the decomposable attention model to use sums of character n-gram embeddings instead of word embeddings .",Introduction,Introduction,natural_language_inference,59,9,1,1,approach,0.965440267,1,approach,0.001232319,0.732555921,0.00029138,0.000118188,0.010762478,0.000231867,0.000148683,0.000326336,0.2134221,0.040257636,0.000577527,6.12E-05,1.44E-05
8989,natural_language_inference59,17,"We show that this model trained on the Quora dataset produces comparable or better results with respect to several complex neural architectures , all using pretrained word embeddings .",Introduction,Introduction,natural_language_inference,59,10,1,0,,0.568969588,1,negative,0.000555545,0.220634217,5.31E-05,7.95E-06,0.000519986,0.000191518,0.001434279,0.000997864,0.092504449,0.627091602,0.054255718,0.001717748,3.60E-05
8990,natural_language_inference59,18,"Second , to significantly improve our model performance , we pretrain all our model parameters on the noisy , automatically collected question - paraphrase corpus Paralex , followed by fine - tuning the parameters on the Quora dataset .",Introduction,Introduction,natural_language_inference,59,11,1,1,approach,0.744918535,1,approach,0.00032426,0.583519179,0.00017965,0.001008814,0.313686066,0.000658058,0.000527433,0.000523533,0.010555887,0.08835614,0.000507282,0.000117103,3.66E-05
8991,natural_language_inference59,19,"This two - stage training procedure achieves the best result on the Quora dataset to date , and is also significantly better than learning only the character n-gram embeddings during the pretraining stage .",Introduction,Introduction,natural_language_inference,59,12,1,0,,0.104374216,0,approach,0.0011214,0.503483021,0.000233683,5.66E-05,0.003230391,0.000543435,0.003859017,0.00189538,0.106902474,0.337571985,0.038687956,0.002255901,0.000158797
8992,natural_language_inference59,20,Related Work,,,natural_language_inference,59,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
8993,natural_language_inference59,36,Approach,,,natural_language_inference,59,0,1,0,,6.30E-05,0,negative,0.000256653,0.000405673,1.67E-05,0.000511976,2.16E-05,0.00097816,0.00011784,0.00317846,0.000653531,0.990596627,0.003176926,4.36E-05,4.23E-05
8994,natural_language_inference59,37,"Our starting point is the decomposable attention model , which despite its simplicity and efficiency has been shown to work remarkably well for the related task of natural language inference .",Approach,Approach,natural_language_inference,59,1,1,0,,0.511870861,1,approach,1.95E-05,0.551106477,0.00010746,4.46E-05,0.000916703,0.00011461,1.84E-05,0.000580888,0.067635954,0.335688633,0.043753682,9.20E-06,3.91E-06
8995,natural_language_inference59,38,We extend this model with character n-gram embeddings and noisy pretraining for the task of question paraphrase identification .,Approach,Approach,natural_language_inference,59,2,1,0,,0.664258523,1,approach,1.70E-05,0.849968221,0.000125732,7.81E-06,0.000561275,5.82E-05,1.83E-05,0.000583266,0.095159949,0.046817004,0.006669653,1.05E-05,3.15E-06
8996,natural_language_inference59,39,Problem Formulation,Approach,,natural_language_inference,59,3,1,0,,0.003199899,0,negative,1.66E-06,0.046081987,1.94E-06,7.33E-06,4.80E-05,0.000113446,4.55E-06,0.001833332,0.019484007,0.897967323,0.034453911,1.51E-06,1.01E-06
8997,natural_language_inference59,40,"Leta = ( a 1 , . . . , a a ) and b = ( b 1 , . . . , b b ) be two input texts consisting of a and b tokens , respectively .",Approach,Problem Formulation,natural_language_inference,59,4,1,0,,1.44E-06,0,negative,1.38E-07,0.000128113,2.80E-07,1.71E-08,2.14E-07,2.38E-05,3.00E-07,0.000340534,0.000163916,0.999294775,4.73E-05,6.43E-07,2.05E-08
8998,natural_language_inference59,41,"We assume that each a i , b j ?",Approach,Problem Formulation,natural_language_inference,59,5,1,0,,1.29E-07,0,negative,2.85E-06,6.35E-05,7.27E-07,3.76E-08,6.31E-07,8.43E-06,2.07E-07,7.73E-05,3.44E-05,0.999797579,9.74E-06,4.63E-06,1.18E-08
8999,natural_language_inference59,42,Rd is encoded in a vector of dimension d.,Approach,Problem Formulation,natural_language_inference,59,6,1,0,,7.80E-06,0,negative,1.04E-06,0.000921361,6.10E-06,8.27E-08,1.08E-06,0.000159733,1.79E-06,0.002210606,0.001663636,0.994910539,0.000122083,1.77E-06,1.77E-07
9000,natural_language_inference59,43,"A context window of size c is subsequently applied , such that the input to the model ( ? , b ) consists of partly overlap -",Approach,Problem Formulation,natural_language_inference,59,7,1,0,,8.94E-06,0,negative,0.00015677,0.001561488,0.000142609,2.31E-07,8.07E-06,5.16E-05,2.78E-06,0.000401388,0.001450911,0.996111193,3.72E-05,7.55E-05,2.58E-07
9001,natural_language_inference59,44,1 } is a binary label indicating whether a is a paraphrase of b or not .,Approach,Problem Formulation,natural_language_inference,59,8,1,0,,1.57E-06,0,negative,5.28E-07,0.000288852,1.86E-06,3.61E-08,4.57E-07,4.92E-05,3.56E-07,0.00072145,0.000592658,0.998324314,1.95E-05,7.11E-07,3.31E-08
9002,natural_language_inference59,45,"Our goal is to predict the correct label y given a pair of previously unseen texts ( a , b ) .",Approach,Problem Formulation,natural_language_inference,59,9,1,0,,2.22E-06,0,negative,1.53E-06,0.002224536,2.50E-06,3.11E-06,9.96E-06,5.04E-05,3.92E-06,0.000825337,0.000440115,0.986075642,0.01035194,9.51E-06,1.48E-06
9003,natural_language_inference59,46,The Decomposable Attention Model,,,natural_language_inference,59,0,1,0,,0.774338794,1,research-problem,2.21E-05,0.000482072,0.000273059,1.49E-06,5.51E-07,7.12E-05,0.000375649,0.001313618,0.000995642,0.215701511,0.779999536,0.000730797,3.28E-05
9004,natural_language_inference59,47,"The DECATT model divides the prediction into three steps : Attend , Compare and Aggregate .",The Decomposable Attention Model,The Decomposable Attention Model,natural_language_inference,59,1,1,0,,0.000282641,0,negative,7.55E-05,4.08E-05,0.013747685,3.51E-07,3.06E-07,2.11E-05,2.43E-05,0.00012686,0.002761456,0.983005161,3.64E-05,0.000156579,3.50E-06
9005,natural_language_inference59,48,"Due to lack of space , we only provide a brief outline below and refer to for further details on each of these steps .",The Decomposable Attention Model,The Decomposable Attention Model,natural_language_inference,59,2,1,0,,6.51E-07,0,negative,7.22E-06,1.38E-06,9.74E-07,8.64E-07,4.13E-08,4.14E-06,7.19E-07,3.53E-05,2.53E-05,0.999908632,6.26E-07,1.48E-05,7.86E-08
9006,natural_language_inference59,49,Attend .,The Decomposable Attention Model,,natural_language_inference,59,3,1,0,,3.08E-06,0,negative,8.78E-05,1.09E-06,2.34E-05,2.24E-06,1.50E-07,2.24E-05,1.63E-05,9.38E-05,3.55E-05,0.9992096,2.14E-06,0.000504226,1.48E-06
9007,natural_language_inference59,50,"First , the elements of ?",The Decomposable Attention Model,Attend .,natural_language_inference,59,4,1,0,,2.41E-06,0,negative,9.96E-05,6.95E-08,9.22E-05,2.31E-08,6.04E-09,4.00E-07,3.34E-07,2.78E-06,2.89E-06,0.999561585,1.59E-07,0.000239485,4.45E-07
9008,natural_language_inference59,51,andb are aligned using a variant of neural attention to decompose the problem into the comparison of aligned phrases .,The Decomposable Attention Model,Attend .,natural_language_inference,59,5,1,0,,6.47E-05,0,negative,0.000540259,1.35E-05,0.04785291,4.06E-07,3.36E-07,5.18E-06,9.08E-06,2.85E-05,0.000557265,0.949441681,1.91E-05,0.001492912,3.89E-05
9009,natural_language_inference59,52,-1,The Decomposable Attention Model,Attend .,natural_language_inference,59,6,1,0,,5.80E-06,0,negative,3.29E-05,1.73E-07,2.45E-05,1.28E-07,1.12E-08,7.95E-07,3.90E-07,9.04E-06,1.30E-05,0.99984981,5.63E-07,6.51E-05,3.51E-06
9010,natural_language_inference59,53,The function F is a feedforward network .,The Decomposable Attention Model,Attend .,natural_language_inference,59,7,1,0,,3.01E-05,0,negative,5.48E-05,1.40E-06,0.000486348,1.42E-07,2.73E-08,2.83E-06,1.24E-06,7.61E-05,0.000330265,0.999009963,1.29E-06,2.96E-05,5.99E-06
9011,natural_language_inference59,54,The aligned phrases are computed as follows :,The Decomposable Attention Model,Attend .,natural_language_inference,59,8,1,0,,5.31E-06,0,negative,3.30E-05,2.76E-07,0.000376008,1.36E-08,5.89E-09,5.26E-07,3.34E-07,6.59E-06,2.03E-05,0.999512071,2.36E-07,5.00E-05,6.96E-07
9012,natural_language_inference59,55,Here ?,The Decomposable Attention Model,Attend .,natural_language_inference,59,9,1,0,,1.81E-06,0,negative,2.22E-05,1.89E-07,1.44E-05,1.74E-07,1.22E-08,1.76E-06,8.02E-07,1.97E-05,1.38E-05,0.999849457,9.97E-07,7.10E-05,5.56E-06
9013,natural_language_inference59,56,i is the subphrase inb that is ( softly ) aligned to ?,The Decomposable Attention Model,Attend .,natural_language_inference,59,10,1,0,,1.23E-06,0,negative,3.20E-05,1.19E-07,2.81E-05,2.99E-07,1.44E-08,1.15E-06,3.71E-07,6.80E-06,8.20E-06,0.999825497,5.63E-07,9.08E-05,6.09E-06
9014,natural_language_inference59,57,i and vice versa for ? j .,The Decomposable Attention Model,Attend .,natural_language_inference,59,11,1,0,,9.51E-07,0,negative,5.00E-05,7.76E-08,7.43E-05,1.84E-08,7.61E-09,3.19E-07,2.85E-07,2.40E-06,4.31E-06,0.999603616,1.82E-07,0.000263759,7.48E-07
9015,natural_language_inference59,58,"Optionally , the inputs a andb to ( 1 ) can be replaced by input representations passed through a "" self - attention "" step to capture longer context .",The Decomposable Attention Model,Attend .,natural_language_inference,59,12,1,0,,4.19E-06,0,negative,0.000216393,4.18E-07,0.000226019,1.96E-08,1.14E-08,2.19E-07,2.93E-07,3.55E-06,4.55E-05,0.999351419,1.52E-07,0.000155537,5.02E-07
9016,natural_language_inference59,59,"In this optional step , we modify the input representations using "" self- attention "" to encode compositional relationships between words within each sentence , as proposed by .",The Decomposable Attention Model,Attend .,natural_language_inference,59,13,1,0,,0.00294391,0,negative,0.008058878,2.21E-05,0.007868905,9.86E-07,8.20E-07,3.02E-06,8.29E-06,2.43E-05,0.000955438,0.979283268,3.67E-06,0.003731526,3.88E-05
9017,natural_language_inference59,60,"Similar to , we define",The Decomposable Attention Model,Attend .,natural_language_inference,59,14,1,0,,1.31E-06,0,negative,2.11E-05,1.17E-07,2.76E-05,2.35E-08,5.56E-09,2.24E-07,1.34E-07,3.90E-06,1.08E-05,0.999895165,8.45E-08,4.02E-05,6.29E-07
9018,natural_language_inference59,61,The function F self and F self are feedforward networks .,The Decomposable Attention Model,Attend .,natural_language_inference,59,15,1,0,,9.49E-05,0,negative,2.76E-05,5.21E-07,0.000329541,5.53E-08,1.53E-08,1.59E-06,7.97E-07,4.28E-05,9.42E-05,0.999479143,2.25E-07,2.06E-05,2.85E-06
9019,natural_language_inference59,62,The self - aligned phrases are then computed as follows :,The Decomposable Attention Model,Attend .,natural_language_inference,59,16,1,0,,2.49E-06,0,negative,3.30E-05,1.55E-07,0.000201231,5.72E-09,4.05E-09,2.32E-07,2.36E-07,3.16E-06,1.60E-05,0.999695406,7.68E-08,5.01E-05,4.51E-07
9020,natural_language_inference59,63,where d i?j is a learned distance - sensitive bias term .,The Decomposable Attention Model,Attend .,natural_language_inference,59,17,1,0,,5.63E-07,0,negative,4.15E-05,1.03E-07,6.77E-05,1.26E-08,5.96E-09,1.98E-07,1.68E-07,3.20E-06,5.06E-06,0.999796784,8.98E-08,8.46E-05,5.48E-07
9021,natural_language_inference59,64,Subsequent steps then use modified input representations defined as ?,The Decomposable Attention Model,Attend .,natural_language_inference,59,18,1,0,,9.48E-06,0,negative,5.79E-05,1.03E-07,0.000186999,6.77E-09,6.52E-09,1.28E-07,1.77E-07,1.42E-06,1.09E-05,0.999639901,4.39E-08,0.000102112,3.14E-07
9022,natural_language_inference59,65,Compare .,The Decomposable Attention Model,,natural_language_inference,59,19,1,0,,2.84E-06,0,negative,5.48E-06,1.78E-07,1.11E-05,8.95E-09,1.25E-08,1.46E-06,3.47E-06,1.59E-05,2.52E-06,0.999709554,9.58E-08,0.000250101,9.03E-08
9023,natural_language_inference59,66,"Second , we separately compare the aligned phrases",The Decomposable Attention Model,Compare .,natural_language_inference,59,20,1,0,,0.000392207,0,negative,7.16E-05,7.48E-08,0.001436814,3.86E-09,2.99E-09,1.05E-07,1.07E-06,1.76E-06,2.10E-06,0.995463406,8.89E-08,0.003021707,1.23E-06
9024,natural_language_inference59,67,"where the brackets [ , ] denote concatenation .",The Decomposable Attention Model,Compare .,natural_language_inference,59,21,1,0,,7.42E-06,0,negative,3.75E-06,3.56E-08,2.22E-05,2.82E-08,2.54E-09,2.58E-07,2.15E-07,5.79E-06,2.78E-06,0.999910047,6.62E-08,5.19E-05,2.94E-06
9025,natural_language_inference59,68,Aggregate .,The Decomposable Attention Model,,natural_language_inference,59,22,1,0,,1.48E-05,0,negative,1.97E-05,3.98E-07,9.66E-06,2.07E-08,3.33E-08,1.47E-06,6.56E-06,2.13E-05,7.49E-06,0.998700402,1.60E-07,0.001232464,3.32E-07
9026,natural_language_inference59,69,"Finally , the sets {v 1 , i } a i =1 and {v 2 , j } b j=1 are aggregated by summation .",The Decomposable Attention Model,Aggregate .,natural_language_inference,59,23,1,0,,3.47E-07,0,negative,6.43E-05,1.41E-07,0.000267278,1.90E-09,4.78E-09,7.78E-08,1.87E-07,1.61E-06,1.42E-05,0.999576793,1.37E-08,7.52E-05,1.56E-07
9027,natural_language_inference59,70,"The sum of two sets is concatenated and passed through another feedforward network followed by a linear layer , to predict the label ?.",The Decomposable Attention Model,Aggregate .,natural_language_inference,59,24,1,0,,3.73E-06,0,negative,4.08E-05,2.68E-07,0.000647698,1.30E-08,1.27E-08,2.26E-07,3.38E-07,5.41E-06,5.30E-05,0.999213357,5.34E-08,3.73E-05,1.48E-06
9028,natural_language_inference59,71,Character,The Decomposable Attention Model,,natural_language_inference,59,25,1,0,,7.80E-06,0,negative,1.90E-05,9.13E-07,1.60E-05,8.05E-08,9.03E-08,3.62E-06,9.75E-06,4.49E-05,2.93E-05,0.998750486,4.32E-07,0.001124099,1.25E-06
9029,natural_language_inference59,72,n- Gram Word Encodings,The Decomposable Attention Model,Character,natural_language_inference,59,26,1,0,,0.000388224,0,negative,1.06E-05,1.77E-06,0.00044794,3.93E-08,1.16E-08,1.28E-06,2.67E-05,2.41E-05,6.42E-05,0.991367341,7.53E-05,0.007965877,1.48E-05
9030,natural_language_inference59,73,Parikh et al .,The Decomposable Attention Model,Character,natural_language_inference,59,27,1,0,,3.17E-05,0,negative,0.000276054,2.26E-07,0.000155985,3.41E-07,5.83E-08,1.52E-06,5.04E-06,1.22E-05,1.17E-05,0.99766189,2.77E-07,0.001869029,5.77E-06
9031,natural_language_inference59,74,"assume that each token a i , b j ?",The Decomposable Attention Model,Character,natural_language_inference,59,28,1,0,,7.96E-08,0,negative,6.46E-06,7.16E-08,1.12E-05,8.59E-10,1.81E-09,3.38E-08,1.53E-07,1.19E-06,2.03E-06,0.99973139,2.56E-08,0.000247427,3.92E-08
9032,natural_language_inference59,75,"Rd is directly embedded in a vector of dimension d ; in practice , they used pretrained word embeddings .",The Decomposable Attention Model,Character,natural_language_inference,59,29,1,0,,1.21E-06,0,negative,8.09E-06,8.84E-07,1.68E-05,2.12E-08,1.61E-08,6.70E-07,1.22E-06,4.41E-05,1.65E-05,0.99977513,1.88E-07,0.000135506,8.45E-07
9033,natural_language_inference59,76,"Inspired by prior work mentioned in Section 2 , we use an alternative approach and instead represent each token as a sum of its embedded character ngrams .",The Decomposable Attention Model,Character,natural_language_inference,59,30,1,0,,4.42E-06,0,negative,6.26E-05,6.09E-06,0.000174697,2.68E-07,1.31E-07,1.37E-06,2.52E-06,3.29E-05,4.48E-05,0.999119824,1.09E-06,0.000549582,4.08E-06
9034,natural_language_inference59,77,This allows for more effective parameter sharing at a small additional computational cost .,The Decomposable Attention Model,Character,natural_language_inference,59,31,1,0,,2.93E-05,0,negative,0.000159985,5.18E-07,1.34E-05,2.39E-08,2.08E-08,1.56E-07,7.13E-07,5.06E-06,1.17E-05,0.998480069,3.46E-08,0.001327898,4.36E-07
9035,natural_language_inference59,78,"As observed in Section 4 , this leads to better results compared to word embeddings .",The Decomposable Attention Model,Character,natural_language_inference,59,32,1,0,,0.001993811,0,negative,0.002065807,1.45E-07,7.32E-06,8.13E-08,2.75E-08,8.60E-07,7.04E-05,2.08E-05,5.96E-07,0.509613564,1.16E-07,0.488212483,7.81E-06
9036,natural_language_inference59,79,Noisy Pretraining,,,natural_language_inference,59,0,1,0,,0.225672575,0,negative,0.001782086,0.000248313,0.000219829,1.65E-05,1.41E-05,0.00032116,0.000794677,0.002504762,8.87E-05,0.977690986,0.003233769,0.013040448,4.47E-05
9037,natural_language_inference59,80,"While character n-gram encodings help in effective parameter sharing , data sparsity remains an issue .",Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,1,1,0,,0.082551821,0,negative,0.000985556,6.81E-06,0.000210702,5.82E-06,1.50E-06,9.79E-05,0.000177791,0.00011354,3.05E-06,0.989974459,0.007672415,0.000590864,0.000159625
9038,natural_language_inference59,81,Pretraining embeddings with a task - agnostic objective on large - scale corpora is a common remedy to this problem .,Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,2,1,0,,0.000500333,0,negative,0.000119494,8.56E-06,7.68E-05,2.82E-05,3.08E-06,0.000223898,6.91E-05,0.000298991,7.83E-06,0.994303001,0.004600721,3.76E-05,0.000222624
9039,natural_language_inference59,82,"However , such pretraining is limited in the following ways .",Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,3,1,0,,2.59E-05,0,negative,7.71E-05,1.72E-06,8.51E-05,1.22E-06,9.83E-07,3.19E-05,1.37E-05,4.27E-05,1.83E-06,0.999512101,0.000196944,2.61E-05,8.68E-06
9040,natural_language_inference59,83,"First , it only applies to the input representation , leaving subsequent parts of the model to random initialization .",Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,4,1,0,,0.004968894,0,negative,0.005357928,0.000231904,0.034790275,2.48E-06,9.11E-06,9.58E-05,0.000160972,0.000198742,0.0003908,0.957838912,4.86E-05,0.000844049,3.04E-05
9041,natural_language_inference59,84,"Second , there maybe a domain mismatch unless embeddings are pretrained on the same domain as the end task ( e.g. , questions ) .",Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,5,1,0,,0.000263419,0,negative,0.000335937,2.09E-06,0.00011029,5.61E-07,9.40E-07,2.40E-05,1.14E-05,3.65E-05,2.38E-06,0.999382199,1.14E-05,7.89E-05,3.39E-06
9042,natural_language_inference59,85,"Finally , since the objective used for pretraining differs from that of the end task ( e.g. , paraphrase identification ) , the embeddings maybe suboptimal .",Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,6,1,0,,0.000275673,0,negative,0.000193626,2.61E-06,2.75E-05,2.57E-06,8.32E-07,6.46E-05,2.24E-05,0.000145328,3.30E-06,0.999361952,6.52E-05,9.54E-05,1.48E-05
9043,natural_language_inference59,86,"As an alternative to task - agnostic pretraining of embeddings on a very large corpus , we propose to pretrain all parameters of the model on a modest - sized corpus of automatically gathered , and therefore noisy examples , drawn from a similar domain .",Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,7,1,0,,0.001683625,0,negative,0.002593471,0.001074816,0.005734793,1.14E-05,6.96E-05,0.000256314,0.000159366,0.000904882,0.000643783,0.988150392,2.48E-05,0.000324144,5.22E-05
9044,natural_language_inference59,87,2,Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,8,1,0,,7.66E-06,0,negative,1.71E-05,5.15E-07,1.84E-05,1.09E-07,1.15E-07,2.42E-05,5.78E-06,5.26E-05,7.41E-06,0.999866081,1.20E-06,4.69E-06,1.78E-06
9045,natural_language_inference59,88,"As observed in Section 4 , such noisy pretraining of the full model results in more accurate performance compared to using pretrained embeddings , as well as compared to only pretraining embeddings on the noisy in - domain corpus .",Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,9,1,0,,0.165444274,0,negative,0.224599472,1.27E-05,0.000288692,3.96E-06,1.02E-05,0.000110677,0.004482641,0.000282093,3.58E-06,0.438705497,2.88E-05,0.331124651,0.000346988
9046,natural_language_inference59,89,3,Noisy Pretraining,Noisy Pretraining,natural_language_inference,59,10,1,0,,1.06E-05,0,negative,2.87E-05,4.08E-07,1.45E-05,1.32E-07,1.68E-07,2.25E-05,5.36E-06,4.19E-05,5.41E-06,0.999870665,6.42E-07,7.84E-06,1.85E-06
9047,natural_language_inference59,90,Experiments,,,natural_language_inference,59,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
9048,natural_language_inference59,91,Implementation Details,,,natural_language_inference,59,0,1,0,,0.045796543,0,negative,4.47E-05,0.001220595,9.47E-06,3.20E-05,1.55E-05,0.000465852,0.000169332,0.007430528,0.000344669,0.986360185,0.003666023,0.000225373,1.58E-05
9049,natural_language_inference59,92,Datasets,Implementation Details,,natural_language_inference,59,1,1,0,,0.001960476,0,negative,3.23E-05,1.85E-05,6.72E-05,4.08E-06,1.77E-06,0.092240121,0.00080012,0.055789579,3.72E-05,0.850502334,0.000371365,0.000119702,1.57E-05
9050,natural_language_inference59,93,"We evaluate our models on the Quora question paraphrase dataset which contains over 400,000 question pairs with binary labels .",Implementation Details,Datasets,natural_language_inference,59,2,1,0,,0.048876645,0,negative,0.00018192,0.000516828,0.00252724,3.46E-05,0.000867362,0.108485073,0.005634912,0.01335174,2.13E-05,0.867603982,8.19E-05,0.000633923,5.92E-05
9051,natural_language_inference59,94,"We use the same data and split as , with 10,000 question pairs each for development and test , who also provide preprocessed and tokenized question pairs .",Implementation Details,Datasets,natural_language_inference,59,3,1,0,,0.000745535,0,negative,2.67E-05,1.65E-05,0.000550502,2.85E-05,0.000940617,0.091285488,0.000990289,0.002072567,3.09E-06,0.904050295,6.43E-06,1.97E-05,9.20E-06
9052,natural_language_inference59,95,"We duplicated the training set , which has approximately 36 % positive and 64 % negative pairs , by adding question pairs in reverse order ( since our model is not symmetric ) .",Implementation Details,Datasets,natural_language_inference,59,4,1,0,,0.008430395,0,negative,0.000162216,3.94E-05,0.00057932,3.47E-05,0.00117691,0.187889832,0.001133646,0.006970139,7.29E-06,0.801955162,2.92E-06,3.46E-05,1.38E-05
9053,natural_language_inference59,96,"When pretraining the full model parameters , we use the Paralex corpus , which consists of 36 million noisy paraphrase pairs including duplicate reversed paraphrases .",Implementation Details,Datasets,natural_language_inference,59,5,1,0,,0.04282692,0,negative,0.000126937,0.00015651,0.001144515,4.97E-05,0.00290422,0.108542857,0.002346796,0.005958551,8.44E-06,0.878578401,1.31E-05,0.000142938,2.71E-05
9054,natural_language_inference59,97,"We created 64 million artificial negative paraphrase pairs ( reflecting the class balance of the Quora training set ) by combining the following three types of negatives in equal proportions : ( 1 ) random unrelated questions , ( 2 ) random questions that share a single word , and ( 3 ) random questions that share all but one word .",Implementation Details,Datasets,natural_language_inference,59,6,1,0,,0.009595252,0,negative,0.000172613,0.000256058,0.001818744,0.000142107,0.010229299,0.116595987,0.003345997,0.004808712,1.70E-05,0.862411199,3.51E-05,0.000113279,5.40E-05
9055,natural_language_inference59,98,5 Hyperparameters,Implementation Details,Datasets,natural_language_inference,59,7,1,0,,0.0011323,0,experimental-setup,1.73E-05,2.20E-05,5.68E-05,1.48E-06,1.21E-06,0.657816742,0.000601019,0.241924569,4.00E-05,0.09949346,6.94E-06,7.07E-06,1.14E-05
9056,natural_language_inference59,99,"We tuned the following hyperparameters by grid search on the development set ( settings for our best model are in parenthesis ) : embedding dimension ( 300 ) , shape of all feedforward networks ( two layers with 400 and 200 width ) , character n -gram sizes ( 5 ) , context size ( 1 ) , learning rate ( 0.1 for both pretraining and tuning ) , batch size ( 256 for pretraining and 64 for tuning ) , dropout ratio ( 0.1 for tuning ) and prediction threshold ( positive paraphrase for a score ? 0.3 ) .",Implementation Details,Datasets,natural_language_inference,59,8,1,1,hyperparameters,0.843192619,1,experimental-setup,6.23E-06,6.40E-06,8.62E-06,5.83E-06,2.41E-06,0.873699494,0.000626296,0.114445028,3.89E-06,0.011180777,1.12E-06,3.24E-06,1.07E-05
9057,natural_language_inference59,100,"We examined whether self - attention helps or not for all model variants , and found that it does for our best model .",Implementation Details,Datasets,natural_language_inference,59,9,1,0,,0.068018676,0,negative,0.000695303,2.34E-05,9.86E-05,7.38E-06,2.03E-05,0.314871004,0.001267859,0.044441674,1.64E-05,0.638073074,1.61E-05,0.000445372,2.35E-05
9058,natural_language_inference59,101,Note that we tried multiple orders of character n-.,Implementation Details,Datasets,natural_language_inference,59,10,1,0,,0.000640577,0,negative,5.16E-05,2.02E-06,0.000133114,1.04E-06,7.46E-06,0.024809299,6.84E-05,0.000912539,3.33E-06,0.97399044,1.11E-06,1.84E-05,1.25E-06
9059,natural_language_inference59,102,"grams with n ? { 3 , 4 , 5 } both individually and separately but 5 - grams alone worked better than these alternatives .",Implementation Details,Datasets,natural_language_inference,59,11,1,0,,0.424387449,0,experimental-setup,0.000697022,8.02E-06,8.52E-05,4.85E-05,2.53E-05,0.805210974,0.007230787,0.05165582,5.22E-06,0.131576699,3.20E-05,0.003182009,0.000242447
9060,natural_language_inference59,103,Baselines,,,natural_language_inference,59,0,1,0,,0.002699267,0,negative,1.90E-05,5.96E-05,8.38E-05,2.15E-07,4.25E-07,0.000133907,0.000174591,0.001395164,4.67E-05,0.996483638,0.000841338,0.000759529,2.11E-06
9061,natural_language_inference59,104,We implemented several baseline models .,Baselines,Baselines,natural_language_inference,59,1,1,0,,0.176003012,0,baselines,5.03E-06,1.00E-06,0.764215176,1.07E-07,3.13E-08,0.000102219,0.000379874,0.001099135,2.19E-06,0.234083219,2.63E-06,0.000106659,2.73E-06
9062,natural_language_inference59,105,"In our first two baselines , each question is represented by concatenating the sum of its unigram word embeddings and the sum of its trigram vectors , where each trigram vector is a concatenation of 3 adjacent word embeddings .",Baselines,Baselines,natural_language_inference,59,2,1,0,,0.756835487,1,baselines,1.53E-06,6.73E-07,0.988231788,6.82E-09,4.48E-09,4.52E-06,3.52E-05,8.56E-05,8.44E-07,0.011621933,4.79E-07,1.70E-05,4.21E-07
9063,natural_language_inference59,106,The two question representations are then concatenated and fed to a feedforward network of shape .,Baselines,Baselines,natural_language_inference,59,3,1,0,,0.583163549,1,baselines,8.68E-06,1.47E-06,0.95077164,9.22E-08,1.86E-08,1.17E-05,4.28E-05,0.00023989,3.06E-05,0.04886453,2.02E-06,2.11E-05,5.47E-06
9064,natural_language_inference59,107,"We call these FFNN word and FFNN char ; in the latter , word embeddings are just sums of character n-gram embeddings .",Baselines,Baselines,natural_language_inference,59,4,1,0,,0.600753396,1,baselines,1.98E-06,2.69E-07,0.986115604,1.31E-08,5.64E-09,2.03E-06,1.86E-05,2.20E-05,8.44E-07,0.013819696,5.38E-07,1.79E-05,5.71E-07
9065,natural_language_inference59,108,"Second , we compare purely supervised variants of decomposable attention model , namely a word - based model with - out any pretrained embeddings ( DECATT word ) , a word - based model with GloVe embeddings ( DECATT glove ) , a character ngram model ( DECATT char ) without pretrained embeddings and DECATT paralex ?",Baselines,Baselines,natural_language_inference,59,5,1,0,,0.963519506,1,baselines,5.51E-06,9.10E-07,0.989137585,4.54E-08,2.65E-08,3.11E-06,9.74E-05,2.82E-05,3.21E-07,0.010596679,2.07E-06,0.000126555,1.55E-06
9066,natural_language_inference59,109,char whose character n-gram embeddings are pretrained with Paralex while all other parameters are learned from scratch on Quora .,Baselines,Baselines,natural_language_inference,59,6,1,0,,0.796716839,1,baselines,2.25E-05,2.54E-06,0.866051747,2.46E-06,4.97E-07,0.000263261,0.002112573,0.002463635,3.30E-06,0.128571652,2.87E-06,0.000463134,3.99E-05
9067,natural_language_inference59,110,Finally we present a baseline where a word - based model is pretrained completely on Paralex ( pt - DECATT word ) and our best model which is a character n-gram model pretrained completely on Paralex ( pt - DECATT char ) .,Baselines,Baselines,natural_language_inference,59,7,1,0,,0.982453394,1,baselines,2.37E-06,2.27E-07,0.997424084,6.30E-09,4.94E-09,7.39E-07,4.34E-05,6.75E-06,1.66E-07,0.002422748,4.92E-07,9.84E-05,6.78E-07
9068,natural_language_inference59,111,"Note that in case of character n-gram based models , for tokens shorter than n characters , we backoff and emit the token itself .",Baselines,Baselines,natural_language_inference,59,8,1,0,,0.175282573,0,baselines,7.05E-05,2.05E-06,0.67662447,1.31E-07,6.02E-08,1.16E-05,4.27E-05,0.000201477,8.56E-06,0.322882138,1.00E-06,0.00015246,2.80E-06
9069,natural_language_inference59,112,"Also , boundary markers were added at the beginning and end of each word .",Baselines,Baselines,natural_language_inference,59,9,1,0,,0.503948027,1,baselines,0.000270439,1.27E-05,0.535369539,1.00E-06,4.69E-07,0.000231966,0.000369807,0.008085676,9.39E-05,0.455203416,1.92E-06,0.000334733,2.44E-05
9070,natural_language_inference59,113,Results,,,natural_language_inference,59,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
9071,natural_language_inference59,114,"Other than our baselines , we compare within .",Results,Results,natural_language_inference,59,1,1,0,,0.003363683,0,negative,0.002975406,3.94E-05,0.002303876,2.07E-05,1.80E-05,0.000223997,0.000913229,0.000788867,5.30E-05,0.977351426,2.69E-05,0.015117003,0.000168163
9072,natural_language_inference59,115,"We observe that the simple FFNN baselines work better than more complex Siamese and Multi - Perspective CNN or LSTM models , more so if character n-gram based embeddings are used .",Results,Results,natural_language_inference,59,2,1,1,results,0.966006432,1,results,0.018470389,1.26E-05,0.000156045,6.80E-05,7.58E-06,0.000672601,0.015900718,0.003141611,1.31E-05,0.101155741,0.000142566,0.856706715,0.003552293
9073,natural_language_inference59,116,"Our basic decomposable attention model DECATT word without pre-trained embeddings is better than most of the models , all of which used GloVe embeddings .",Results,Results,natural_language_inference,59,3,1,1,results,0.970319498,1,results,0.003050051,1.87E-06,0.000117673,1.21E-05,2.00E-06,8.48E-05,0.0121233,0.000323082,1.38E-06,0.020343987,3.15E-05,0.961905526,0.002002831
9074,natural_language_inference59,117,An interesting observation is that DECATT char model without any pretrained embeddings outperforms DE - CATT glove that uses task - agnostic GloVe embeddings .,Results,Results,natural_language_inference,59,4,1,1,results,0.96984281,1,results,0.013153424,1.49E-06,6.25E-05,3.56E-06,1.21E-06,2.46E-05,0.004331258,0.000120532,7.78E-07,0.02947359,1.07E-05,0.952482939,0.000333394
9075,natural_language_inference59,118,"Furthermore , when character n-gram embeddings are pre-trained in a task - specific manner in DECATT paralex ? char model , we observe a significant boost in performance .",Results,Results,natural_language_inference,59,5,1,1,results,0.975107387,1,results,0.035795784,1.67E-06,0.000104004,2.38E-06,1.08E-06,8.41E-06,0.002591331,4.33E-05,8.40E-07,0.016641086,5.28E-06,0.944526047,0.000278742
9076,natural_language_inference59,119,The final two rows of the table show results achieved by pt - DECATT word and pt - DECATT char .,Results,Results,natural_language_inference,59,6,1,0,,0.065452973,0,negative,0.004293931,2.66E-06,0.000189825,1.21E-06,2.74E-06,2.73E-05,0.001016972,0.000104478,1.97E-06,0.744486551,6.51E-06,0.249798057,6.78E-05
9077,natural_language_inference59,120,We note that the former falls short of the DE - CATT paralex ?,Results,Results,natural_language_inference,59,7,1,0,,0.009228896,0,negative,0.00521069,7.86E-06,0.00021701,4.21E-05,7.10E-06,0.000274457,0.000496155,0.000666025,3.66E-05,0.973293191,8.47E-05,0.01920071,0.00046341
9078,natural_language_inference59,121,"char , which shows that character ngram representations are powerful .",Results,Results,natural_language_inference,59,8,1,0,,0.749502739,1,results,0.030742831,1.30E-06,0.000268224,5.12E-06,2.61E-06,3.31E-05,0.003444184,9.29E-05,1.37E-06,0.07473134,1.04E-05,0.890160597,0.00050602
9079,natural_language_inference59,122,"Finally , we note that our best performing model is pt - DECATT char , which leverages the full power of character embeddings and pretraining the model on Paralex .",Results,Results,natural_language_inference,59,9,1,1,results,0.96192245,1,results,0.097138914,2.23E-05,0.004631863,0.000104592,2.75E-05,0.0002746,0.012111923,0.000467778,2.22E-05,0.134107161,4.54E-05,0.748803154,0.002242666
9080,natural_language_inference59,123,Noisy pretraining gives more significant gains in case of smaller human annotated data as can be seen in where non-pretrained DECATT char and pretrained pt - DECATT char are compared on a logarithmic scale of number of Quora examples .,Results,Results,natural_language_inference,59,10,1,0,,0.90868263,1,results,0.021723576,6.80E-07,5.40E-05,7.15E-07,4.62E-07,5.51E-06,0.001951856,2.71E-05,3.25E-07,0.020789782,3.22E-06,0.955323408,0.000119353
9081,natural_language_inference59,124,It also gives an important insight into trade off between having more but costly human annotated data versus cheap but noisy pretraining .,Results,Results,natural_language_inference,59,11,1,0,,0.017696748,0,negative,0.029185564,2.26E-05,0.000929529,6.29E-06,8.96E-06,3.64E-05,0.000294435,0.000136529,5.12E-05,0.863825432,9.22E-06,0.105402657,9.12E-05
9082,natural_language_inference59,125,"shows some example predictions from the DE - CATT glove , DECATT char and the pt - DECATT char models .",Results,Results,natural_language_inference,59,12,1,0,,0.00378294,0,negative,0.003712418,6.21E-06,0.000748719,1.81E-05,1.15E-05,0.000150317,0.000678531,0.000368277,2.71E-05,0.971594651,9.89E-06,0.022165742,0.000508525
9083,natural_language_inference59,126,The GloVe - trained model often makes mistakes related to spelling and tokenization artifacts .,Results,Results,natural_language_inference,59,13,1,0,,0.095666242,0,negative,0.018192915,5.24E-06,0.000261517,1.58E-05,8.89E-06,5.59E-05,0.000930812,0.000164322,7.84E-06,0.828315372,2.53E-05,0.15138708,0.000629068
9084,natural_language_inference59,127,"We observed that hyperparameter tuning resulted in settings where non-pretrained models did not use self - attention while the pretrained character based model did , thus learning better long term context at it s input layer ; this is reflected in example D which shows an alternation that our best model captures .",Results,Results,natural_language_inference,59,14,1,0,,0.015224943,0,negative,0.043003431,1.03E-05,0.000236368,1.11E-05,1.24E-05,9.47E-05,0.000868005,0.000362123,1.17E-05,0.814730663,9.32E-06,0.140414433,0.00023531
9085,natural_language_inference59,128,"Finally , E and F show pairs that present complex paraphrases that none of our models capture .",Results,Results,natural_language_inference,59,15,1,0,,0.025291344,0,negative,0.004897248,5.94E-06,0.000425946,1.38E-05,2.29E-05,7.21E-05,0.000679173,0.000218266,9.91E-06,0.944529445,3.49E-06,0.048871461,0.000250233
9086,natural_language_inference59,129,Conclusion and Future Work,,,natural_language_inference,59,0,1,0,,0.000993755,0,negative,6.55E-05,5.57E-05,6.74E-06,8.33E-07,5.66E-07,8.35E-05,7.13E-05,0.000591015,4.69E-05,0.993684537,0.004983168,0.000405376,4.84E-06
9087,machine-translation5,1,title,,,machine-translation,5,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
9088,machine-translation5,2,Tilde 's Machine Translation Systems for WMT 2018,title,title,machine-translation,5,1,1,1,research-problem,0.983365835,1,research-problem,1.04E-07,1.09E-05,2.19E-07,2.77E-07,2.51E-07,3.73E-07,3.79E-06,2.69E-06,1.05E-06,0.005203621,0.994776013,6.12E-07,1.52E-07
9089,machine-translation5,3,abstract,,,machine-translation,5,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
9090,machine-translation5,4,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,abstract,abstract,machine-translation,5,1,1,1,research-problem,0.359089279,0,research-problem,6.54E-06,0.003043234,8.50E-06,0.000776935,0.002609022,2.33E-05,4.23E-05,2.90E-05,3.58E-05,0.128627221,0.864791132,2.34E-06,4.73E-06
9091,machine-translation5,5,"We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results .",abstract,abstract,machine-translation,5,2,1,0,,0.002583268,0,negative,1.49E-06,0.005701138,7.53E-07,5.84E-05,0.000508094,2.64E-05,2.77E-06,0.000142577,0.000335581,0.965920702,0.02730127,5.36E-07,3.78E-07
9092,machine-translation5,6,"For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions .",abstract,abstract,machine-translation,5,3,1,0,,0.089828563,0,negative,7.32E-06,0.018093352,2.31E-05,0.000322794,0.008757766,0.0001246,0.000147546,0.000413191,0.000162253,0.512201313,0.459725349,1.33E-05,8.20E-06
9093,machine-translation5,7,The submitted systems were trained using Transformer models .,abstract,abstract,machine-translation,5,4,1,0,,0.159555342,0,negative,4.37E-06,0.003172096,7.53E-06,4.90E-05,0.000582994,0.000351489,1.73E-05,0.001131532,0.000165019,0.979963126,0.01455376,1.01E-06,8.07E-07
9094,machine-translation5,8,Introduction,,,machine-translation,5,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
9095,machine-translation5,9,Neural machine translation ( NMT ) is a rapidly changing research area .,Introduction,Introduction,machine-translation,5,1,1,1,research-problem,0.971462574,1,research-problem,8.89E-07,0.000139514,5.38E-07,2.56E-06,7.83E-06,2.24E-06,1.55E-05,3.09E-06,3.19E-05,0.018891093,0.980901419,1.63E-06,1.77E-06
9096,machine-translation5,10,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",Introduction,Introduction,machine-translation,5,2,1,0,,0.900951411,1,research-problem,1.91E-06,0.000582998,9.24E-07,2.87E-06,1.12E-05,4.51E-06,1.24E-05,7.70E-06,0.000235935,0.062864162,0.93627186,2.19E-06,1.26E-06
9097,machine-translation5,11,The state - of - the - art in 2016 were shallow attention - based recurrent neural networks ( RNN ) with gated recurrent units ( GRU ) in recurrent layers .,Introduction,Introduction,machine-translation,5,3,1,0,,0.084676152,0,research-problem,1.13E-05,0.004322118,2.65E-05,6.21E-05,0.000120516,0.000306697,0.000131349,0.000256016,0.010199504,0.32087243,0.663666456,7.33E-06,1.76E-05
9098,machine-translation5,12,"In 2017 , multiplicative long short - term memory ( MLSTM ) units and deep GRU models were introduced in NMT .",Introduction,Introduction,machine-translation,5,4,1,0,,0.207283259,0,negative,3.91E-05,0.006541246,4.12E-05,4.36E-05,0.000136455,0.000349749,0.000205869,0.000278269,0.030654292,0.511907135,0.449766337,1.68E-05,1.98E-05
9099,machine-translation5,13,"The same year , selfattentional ( Transformer ) models were introduced .",Introduction,Introduction,machine-translation,5,5,1,0,,0.013243999,0,negative,3.24E-05,0.006730212,2.64E-05,3.15E-05,0.000170098,0.000362205,0.000100856,0.000259232,0.049330263,0.793474023,0.149465365,8.78E-06,8.62E-06
9100,machine-translation5,14,"Consequently , in 2018 , most of the top scoring systems in the shared task on news translation of the Third Conference on Machine Translation ( WMT ) were trained using Transformer models",Introduction,Introduction,machine-translation,5,6,1,0,,0.167886614,0,research-problem,8.12E-06,0.001711697,3.59E-06,2.09E-05,8.91E-05,4.86E-05,3.32E-05,4.73E-05,0.000733372,0.385719088,0.611576287,4.94E-06,3.74E-06
9101,machine-translation5,15,1 .,Introduction,Introduction,machine-translation,5,7,1,0,,0.003006204,0,negative,9.49E-06,0.002655741,1.62E-06,8.56E-06,8.61E-05,0.000153157,1.21E-05,0.000174776,0.012080487,0.979795347,0.005019584,2.02E-06,9.36E-07
9102,machine-translation5,16,"However , it is already evident that the state - of - the - art architectures will 1 All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models .",Introduction,Introduction,machine-translation,5,8,1,0,,0.031049157,0,negative,1.62E-05,0.001488617,4.09E-06,1.84E-05,0.000227807,8.26E-05,5.51E-05,6.27E-05,0.000743341,0.893663204,0.10361621,1.84E-05,3.37E-06
9103,machine-translation5,17,be pushed even further in 2018 .,Introduction,Introduction,machine-translation,5,9,1,0,,0.011725378,0,negative,6.15E-05,0.005419421,1.08E-05,0.000138031,0.000807115,0.000389396,8.96E-05,0.000269792,0.015274327,0.95866039,0.018853897,1.56E-05,1.01E-05
9104,machine-translation5,18,"For instance , have recently proposed RNMT + models that combine deep LSTM - based models with multi-head attention and showed that the models outperform Transformer models .",Introduction,Introduction,machine-translation,5,10,1,0,,0.01635346,0,negative,1.14E-05,0.00214805,1.08E-05,1.81E-05,5.91E-05,0.000209449,0.000118408,0.000186714,0.007430688,0.705962976,0.283824225,1.08E-05,9.24E-06
9105,machine-translation5,19,"In WMT 2017 , Tilde participated with MLSTM - based NMT systems .",Introduction,Introduction,machine-translation,5,11,1,0,,0.033447863,0,negative,4.50E-05,0.012972471,6.59E-05,7.05E-05,0.001141427,0.000248653,0.00068025,0.000172333,0.008144587,0.59057617,0.385749941,8.74E-05,4.54E-05
9106,machine-translation5,20,"In this paper , we compare the MLSTMbased models with Transformer models for English - Estonian and Estonian - English and we show that the state - of - the - art of WMT 2017 is well behind the new models .",Introduction,Introduction,machine-translation,5,12,1,0,,0.744523609,1,approach,0.000183543,0.680429426,0.000294355,1.25E-05,0.004338844,9.56E-05,0.000574013,0.000202711,0.111614411,0.179613882,0.022335097,0.000291249,1.44E-05
9107,machine-translation5,21,"Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",Introduction,Introduction,machine-translation,5,13,1,0,,0.665493672,1,negative,3.83E-05,0.009057348,2.41E-05,3.84E-05,0.002778057,9.16E-05,0.000131527,6.93E-05,0.001034928,0.905420618,0.081273164,3.51E-05,7.70E-06
9108,machine-translation5,22,The paper is further structured as follows :,Introduction,Introduction,machine-translation,5,14,1,0,,0.001965822,0,negative,1.06E-05,0.004457583,4.93E-06,0.001386938,0.003482064,0.000817737,5.79E-05,0.000351386,0.003127241,0.978119286,0.008171314,2.51E-06,1.05E-05
9109,machine-translation5,23,"Section 2 provides an overview of systems submitted for the WMT 2018 shared task on news translation , Section 3 describes the data used to train the NMT systems and the data pre-processing workflows , Section 4 describes all NMT systems trained and experiments on handling of named entities and combination of systems , Section 5 provides automatic evaluation results , and Section 6 concludes the paper .",Introduction,Introduction,machine-translation,5,15,1,0,,0.023016676,0,negative,1.03E-05,0.019202844,6.56E-06,0.000180669,0.009865918,0.00023713,8.61E-05,0.000243397,0.003479815,0.941087688,0.025577316,1.14E-05,1.10E-05
9110,machine-translation5,24,System Overview,,,machine-translation,5,0,1,0,,0.083893117,0,negative,5.01E-05,0.00081054,1.93E-05,6.78E-06,8.15E-06,0.000172287,0.000131461,0.00269378,0.00047629,0.990458453,0.004693067,0.000459706,2.01E-05
9111,machine-translation5,25,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .",System Overview,System Overview,machine-translation,5,1,1,1,model,0.040648772,0,negative,0.00040478,0.000231838,0.000667512,0.000151489,0.000394435,0.004717683,0.008035726,0.001032769,4.62E-05,0.973033283,0.006763375,0.00435121,0.000169663
9112,machine-translation5,26,The following is a list of the five MT systems submitted :,System Overview,System Overview,machine-translation,5,2,1,1,,1.93E-06,0,negative,1.18E-05,1.50E-06,3.03E-06,1.58E-06,2.55E-06,0.000112218,1.20E-05,3.06E-05,2.75E-06,0.999767595,3.29E-05,2.09E-05,4.82E-07
9113,machine-translation5,27,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,System Overview,System Overview,machine-translation,5,3,1,1,,0.002566237,0,negative,0.000336215,0.000552701,0.041160261,3.33E-06,9.96E-06,0.000987683,0.002761172,0.000779373,0.000261725,0.920807192,0.01471583,0.017571157,5.34E-05
9114,machine-translation5,28,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,System Overview,System Overview,machine-translation,5,4,1,1,,0.000481357,0,negative,5.69E-05,0.000208375,6.88E-05,3.07E-06,9.36E-06,0.027910754,0.000378968,0.029704348,0.000397915,0.941199612,1.81E-05,3.83E-05,5.43E-06
9115,machine-translation5,29,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,System Overview,System Overview,machine-translation,5,5,1,1,model,0.00090775,0,negative,0.000352211,0.00040614,0.017400064,4.20E-06,1.38E-05,0.000665074,0.002779349,0.00057536,0.000135107,0.94376848,0.014057949,0.019789158,5.31E-05
9116,machine-translation5,30,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .",System Overview,System Overview,machine-translation,5,6,1,1,model,1.56E-05,0,negative,0.000186755,9.78E-06,0.000130452,4.81E-07,4.56E-06,0.000352705,5.07E-05,0.000135856,9.95E-06,0.998874401,6.26E-06,0.000237692,4.43E-07
9117,machine-translation5,31,A constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,System Overview,System Overview,machine-translation,5,7,1,1,model,0.001195285,0,negative,0.001538969,0.002546453,0.107689601,2.75E-05,0.000127067,0.003448482,0.013338132,0.002330973,0.001292433,0.80345305,0.012548687,0.051197719,0.00046092
9118,machine-translation5,32,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,System Overview,System Overview,machine-translation,5,8,1,1,model,0.001005443,0,negative,0.000446707,0.000357056,0.007565428,2.91E-06,6.27E-06,0.000669196,0.005900992,0.000869521,0.000114249,0.885324552,0.026676281,0.071977043,8.98E-05
9119,machine-translation5,33,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,System Overview,System Overview,machine-translation,5,9,1,1,model,0.000209044,0,negative,0.000158758,0.000497676,0.000493847,1.92E-06,2.17E-05,0.005332314,0.000367505,0.006224821,0.000566422,0.986119981,2.07E-05,0.000189,5.34E-06
9120,machine-translation5,34,Data,System Overview,,machine-translation,5,10,1,0,,1.97E-06,0,negative,1.09E-06,4.51E-06,1.55E-06,8.09E-08,2.50E-07,0.000315431,1.08E-05,0.000398213,1.56E-05,0.999236426,9.16E-06,6.61E-06,2.10E-07
9121,machine-translation5,35,"Data preparation was done using one of two distinct workflows - we used the full workflow for tilde - c - nmt , tilde - nc - nmt and tilde - c - nmt - comb submissions .",System Overview,Data,machine-translation,5,11,1,0,,2.83E-05,0,negative,3.31E-05,7.24E-06,2.77E-05,9.95E-06,8.45E-05,0.000209548,1.71E-05,1.30E-05,5.76E-06,0.999583675,1.93E-06,5.66E-06,8.42E-07
9122,machine-translation5,36,For the tilde - c - nmt - 2 bt submission we used the light data preparation workflow .,System Overview,Data,machine-translation,5,12,1,0,,0.000245293,0,negative,0.000240887,6.33E-06,3.80E-05,0.000318127,0.000399635,0.000925823,6.42E-05,1.74E-05,6.34E-06,0.997939787,7.78E-06,2.73E-05,8.43E-06
9123,machine-translation5,37,Full Workflow,System Overview,,machine-translation,5,13,1,0,,0.000945533,0,negative,0.000107203,5.96E-05,4.43E-05,2.66E-05,1.78E-05,0.001781199,0.000391499,0.000939623,0.000347489,0.995237402,0.000187908,0.000823535,3.58E-05
9124,machine-translation5,38,"First , we trained constrained system baseline models using the filtered datasets .",System Overview,Full Workflow,machine-translation,5,14,1,0,,0.001067018,0,negative,0.001724304,0.000149691,0.000522695,4.20E-07,1.97E-05,0.000156361,0.000480152,6.54E-05,0.000161464,0.996335051,1.20E-06,0.000382578,9.31E-07
9125,machine-translation5,39,"For baseline models , we used the MLSTM and transf configurations ( see ) .",System Overview,Full Workflow,machine-translation,5,15,1,0,,0.002068766,0,negative,3.47E-05,1.51E-05,0.000179804,2.52E-07,2.03E-06,0.001409902,0.001271896,0.000400172,6.84E-05,0.996575231,1.64E-06,3.93E-05,1.59E-06
9126,machine-translation5,40,"Then , we used the best - performing models ( based on translation quality on the vali -dation set ) , which were the Transformer models ( see , and back - translated monolingual data .",System Overview,Full Workflow,machine-translation,5,16,1,0,,8.94E-05,0,negative,0.00020639,1.37E-05,0.000169772,7.65E-08,6.00E-06,8.08E-05,0.000292932,2.43E-05,1.39E-05,0.999050335,3.31E-07,0.000141257,2.43E-07
9127,machine-translation5,41,"As mentioned before , for the unconstrained systems , we back - translated the monolingual data using pre-existing MLSTM - based NMT systems .",System Overview,Full Workflow,machine-translation,5,17,1,0,,0.000503679,0,negative,0.000206236,0.000114074,0.000376976,3.57E-07,2.80E-05,0.000184534,0.000438753,8.02E-05,0.000114788,0.99834557,1.18E-06,0.000108195,1.13E-06
9128,machine-translation5,42,"Then , using the final training data ( parallel and the two synthetic corpora ) , we trained final Transformer models .",System Overview,Full Workflow,machine-translation,5,18,1,0,,0.00052704,0,negative,0.000497511,4.15E-05,0.000141914,1.58E-07,7.52E-06,7.45E-05,0.000225506,4.78E-05,0.000118744,0.99868676,5.36E-07,0.000156781,7.49E-07
9129,machine-translation5,43,"For the constrained scenario , we trained multiple models ( three for each translation direction ) by experimenting with multiple model configurations .",System Overview,Full Workflow,machine-translation,5,19,1,0,,0.016502502,0,negative,0.001072164,0.000359349,0.000338643,7.54E-07,3.70E-05,0.000299232,0.000921438,0.000167934,0.00038519,0.995989155,2.07E-06,0.000424173,2.93E-06
9130,machine-translation5,44,"For the unconstrained scenario , we trained one model in each of the directions .",System Overview,Full Workflow,machine-translation,5,20,1,0,,0.000390124,0,negative,0.000223418,7.58E-05,7.21E-05,2.91E-07,5.73E-06,0.000538541,0.000500674,0.000566374,0.000377734,0.997597533,7.54E-07,3.94E-05,1.61E-06
9131,machine-translation5,45,"In order to acquire the translations for the submissions , we performed model averaging and ensembling as follows :",System Overview,Full Workflow,machine-translation,5,21,1,0,,0.000201019,0,negative,0.000163994,2.95E-05,0.000117113,5.51E-08,1.89E-06,8.60E-05,9.25E-05,4.86E-05,0.000154582,0.999284704,4.62E-07,2.02E-05,4.47E-07
9132,machine-translation5,46,"For the tilde - c - nmt ( constrained NMT ) systems , we performed model averaging of the best four models ( according to perplexity ) of the three different run NMT systems and deployed the averaged models in an ensemble .",System Overview,Full Workflow,machine-translation,5,22,1,0,,0.014775404,0,negative,0.000596761,0.000216524,0.001143818,1.32E-06,7.13E-05,0.000714297,0.00362732,0.000188677,0.00016162,0.992521093,3.07E-06,0.000743524,1.07E-05
9133,machine-translation5,47,"For the tilde - nc - nmt ( unconstrained NMT ) systems , we performed model averaging of the best four models .",System Overview,Full Workflow,machine-translation,5,23,1,0,,0.015087,0,negative,0.000387014,3.67E-05,0.000221327,2.05E-07,7.78E-06,0.000454696,0.001247448,0.000141514,6.11E-05,0.997257928,8.86E-07,0.000181283,2.11E-06
9134,machine-translation5,48,"For the tilde - c - nmt - comb Estonian - English system , we performed majority voting ( see Section 4.3 ) of translations produced by six different runs of different constrained systems ( using best BLEU models , averaged models , ensembled averaged models , ensembled models , and larger beam search ( 10 instead of 5 ) ) .",System Overview,Full Workflow,machine-translation,5,24,1,0,,0.004191926,0,negative,0.000258841,3.49E-05,0.000250652,1.31E-07,1.92E-05,0.000134096,0.001427775,3.66E-05,1.56E-05,0.997127911,7.28E-07,0.00069182,1.80E-06
9135,machine-translation5,49,Data Filtering,System Overview,,machine-translation,5,25,1,0,,0.001741633,0,negative,0.000285558,2.06E-05,0.000119696,7.15E-06,5.16E-05,0.003905403,0.000770364,0.000723199,3.16E-05,0.99238888,8.59E-06,0.001664716,2.27E-05
9136,machine-translation5,50,"As NMT systems are sensitive to noise in parallel data , all parallel data were filtered using the parallel data filtering methods described by .",System Overview,Data Filtering,machine-translation,5,26,1,0,,7.52E-06,0,negative,2.85E-05,5.37E-07,6.47E-06,2.37E-08,3.53E-07,1.10E-05,1.83E-05,2.88E-06,2.71E-06,0.999918446,5.30E-08,1.07E-05,1.33E-07
9137,machine-translation5,51,"The parallel corpora filtering methods remove sentence pairs that have indications of data corruption or low parallelity ( e.g. , source - target length ratio , content overlap , digit mismatch , language adherence , etc. ) issues .",System Overview,Data Filtering,machine-translation,5,27,1,0,,0.000160796,0,negative,6.71E-05,1.45E-06,1.41E-05,2.55E-07,8.41E-07,8.21E-06,3.26E-05,3.07E-06,3.38E-06,0.999776802,1.64E-05,7.16E-05,4.20E-06
9138,machine-translation5,52,"Contrary to Tilde 's submissions for WMT 2017 , isolated sentence pair filtering for the WMT 2018 submissions was supplemented with a maximum content overlap filter ( i.e. only one target sentence for each source sentence was preserved and vice versa based on the content overlap filter 's score for each sentence pair ) .",System Overview,Data Filtering,machine-translation,5,28,1,0,,1.92E-05,0,negative,0.000779217,1.52E-05,0.000103694,4.21E-07,8.40E-06,1.59E-05,6.17E-05,5.10E-06,2.73E-05,0.998750626,8.99E-07,0.000229867,1.69E-06
9139,machine-translation5,53,"For filtering , we required probabilistic dictionaries , which were obtained from the parallel corpora ( different dictionaries for the constrained and unconstrained scenarios ) using fast align .",System Overview,Data Filtering,machine-translation,5,29,1,0,,9.37E-05,0,negative,3.73E-05,1.23E-05,1.91E-05,1.66E-07,1.26E-06,0.000108124,0.000115853,9.73E-05,0.000136071,0.99946038,3.21E-07,9.58E-06,2.27E-06
9140,machine-translation5,54,The dictionaries were filtered using the transliteration - based probabilistic dictionary filtering method by .,System Overview,Data Filtering,machine-translation,5,30,1,0,,0.000214282,0,negative,4.17E-05,1.83E-06,6.42E-06,2.72E-08,4.03E-07,1.75E-05,2.55E-05,1.50E-05,1.97E-05,0.999866302,3.77E-08,5.26E-06,3.29E-07
9141,machine-translation5,55,"During filtering , we identified that one of the corpora that were provided by the organisers contained a significant amount of data corruption .",System Overview,Data Filtering,machine-translation,5,31,1,0,,1.52E-05,0,negative,0.00016918,5.47E-08,4.00E-07,1.24E-06,3.85E-06,7.74E-06,4.96E-06,3.52E-07,2.82E-07,0.999801017,4.19E-08,1.05E-05,3.83E-07
9142,machine-translation5,56,It was the Estonian ?,System Overview,Data Filtering,machine-translation,5,32,1,0,,8.86E-07,0,negative,2.25E-06,1.06E-07,2.80E-07,3.47E-08,3.28E-08,5.65E-06,4.76E-06,3.61E-06,3.13E-06,0.999977419,4.04E-07,2.09E-06,2.42E-07
9143,machine-translation5,57,English ParaCrawl corpus,System Overview,Data Filtering,machine-translation,5,33,1,0,,0.00662213,0,negative,0.000326335,1.69E-06,8.20E-05,1.24E-07,1.20E-06,1.42E-05,0.009357407,5.41E-06,1.89E-06,0.965587133,2.47E-05,0.024501677,9.63E-05
9144,machine-translation5,58,3 .,System Overview,Data Filtering,machine-translation,5,34,1,0,,2.72E-06,0,negative,6.51E-06,6.15E-08,2.41E-07,2.95E-09,9.71E-09,1.04E-06,2.17E-06,9.36E-07,2.17E-06,0.999983703,2.45E-08,3.08E-06,4.36E-08
9145,machine-translation5,59,The corpus consisted of 1.30 million sentence pairs out of which 0.77 million were identified as being corrupt .,System Overview,Data Filtering,machine-translation,5,35,1,0,,0.000257061,0,negative,4.59E-05,1.36E-06,6.15E-06,3.05E-06,0.000186105,1.86E-05,3.08E-05,1.70E-06,1.22E-06,0.999670832,6.81E-08,3.14E-05,2.88E-06
9146,machine-translation5,60,"To reduce the high level of noise , this corpus was filtered using stricter content overlap ( a threshold of 0.3 instead of 0.1 ) and language adherence filters ( both the language detection and the valid alphabet filters had to validate a sentence pair instead of just one of the filters ) than all other corpora .",System Overview,Data Filtering,machine-translation,5,36,1,0,,1.23E-05,0,negative,3.59E-05,2.55E-07,3.86E-06,6.07E-08,8.02E-06,5.03E-06,1.46E-05,7.74E-07,3.75E-07,0.9999117,5.58E-09,1.93E-05,1.84E-07
9147,machine-translation5,61,"As a result , only 0.17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems .",System Overview,Data Filtering,machine-translation,5,37,1,0,,4.08E-05,0,negative,0.00015879,4.62E-07,1.31E-06,1.78E-07,9.43E-06,7.86E-06,5.24E-05,2.47E-06,5.52E-07,0.99969287,3.95E-08,7.16E-05,1.98E-06
9148,machine-translation5,62,"Due to the quality concerns , the corpus was not used for training of the unconstrained systems .",System Overview,Data Filtering,machine-translation,5,38,1,0,,2.90E-06,0,negative,2.76E-06,4.19E-08,2.89E-07,3.97E-08,1.09E-06,2.11E-06,2.12E-06,5.28E-07,1.85E-07,0.999987742,7.54E-09,3.00E-06,8.70E-08
9149,machine-translation5,63,The corpora statistics before and after filtering are provided in .,System Overview,Data Filtering,machine-translation,5,39,1,0,,6.30E-06,0,negative,1.01E-05,1.29E-08,2.43E-07,1.02E-08,3.08E-07,1.12E-06,4.27E-06,2.41E-07,4.04E-08,0.999965443,2.19E-09,1.81E-05,5.53E-08
9150,machine-translation5,64,Data Pre-processing,System Overview,,machine-translation,5,40,1,0,,4.53E-05,0,negative,7.28E-06,1.17E-05,4.25E-06,1.43E-06,9.34E-06,0.001364997,0.000121726,0.000968047,2.82E-05,0.997380284,2.40E-06,9.46E-05,5.67E-06
9151,machine-translation5,65,All corpora were pre-processed using the parallel data pre-processing workflow from the Tilde MT platform ) that performs the following pre-processing steps :,System Overview,Data Pre-processing,machine-translation,5,41,1,0,,3.61E-05,0,negative,0.000256413,1.91E-05,0.000195317,2.98E-07,3.29E-05,4.77E-05,7.78E-05,1.26E-05,1.84E-05,0.999220566,9.12E-08,0.000117475,1.21E-06
9152,machine-translation5,66,"First , parallel corpora are cleaned by removing HTML and XML tags , decoding escaped symbols , normalising whitespaces and punctuation marks , replacing control characters with spaces , etc .",System Overview,Data Pre-processing,machine-translation,5,42,1,0,,5.19E-06,0,negative,0.000500009,2.63E-06,2.32E-05,7.25E-07,4.55E-05,7.01E-06,5.72E-06,9.99E-07,2.79E-06,0.999354682,7.16E-08,5.64E-05,3.13E-07
9153,machine-translation5,67,This step is performed only on the training data .,System Overview,Data Pre-processing,machine-translation,5,43,1,0,,1.28E-06,0,negative,2.42E-05,2.00E-06,1.72E-06,6.04E-09,1.71E-07,8.02E-07,5.46E-07,2.05E-06,2.81E-05,0.999933585,3.02E-08,6.69E-06,4.94E-08
9154,machine-translation5,68,"Then , non-translatable entities , such as email addresses , URLs , file paths , etc. are identified and replaced with place - holders .",System Overview,Data Pre-processing,machine-translation,5,44,1,0,,2.08E-06,0,negative,1.74E-05,2.18E-06,2.99E-06,7.57E-09,9.10E-07,9.82E-07,7.50E-07,1.67E-06,1.39E-05,0.999952586,1.17E-08,6.59E-06,3.01E-08
9155,machine-translation5,69,This allows reducing data sparsity where it is not needed .,System Overview,Data Pre-processing,machine-translation,5,45,1,0,,1.40E-06,0,negative,4.22E-05,9.22E-07,2.58E-06,1.46E-08,2.75E-07,1.52E-06,6.51E-07,1.64E-06,1.03E-05,0.999932158,7.27E-09,7.69E-06,3.63E-08
9156,machine-translation5,70,"Then , the data are tokenised using the Tilde MT regular expression - based tokeniser .",System Overview,Data Pre-processing,machine-translation,5,46,1,0,,0.000152626,0,negative,8.27E-05,2.41E-05,0.000277252,2.10E-08,1.44E-06,1.31E-05,2.38E-05,1.16E-05,0.000445943,0.999096602,4.78E-08,2.27E-05,8.04E-07
9157,machine-translation5,71,The Moses truecasing script truecase .,System Overview,Data Pre-processing,machine-translation,5,47,1,0,,3.61E-06,0,negative,3.35E-05,7.77E-07,1.34E-05,6.77E-08,5.61E-07,1.57E-05,0.000116499,6.97E-06,4.26E-06,0.99937166,2.16E-06,0.000429106,5.45E-06
9158,machine-translation5,72,perl is used to truecase the first word of every sentence .,System Overview,Data Pre-processing,machine-translation,5,48,1,0,,5.85E-06,0,negative,1.57E-05,1.36E-05,0.000163089,5.33E-07,1.15E-05,0.000215628,0.000168625,0.000181004,5.70E-05,0.99913164,2.07E-07,2.56E-05,1.59E-05
9159,machine-translation5,73,"Then , tokens are split into sub - word units using byte - pair encoding ( BPE ) .",System Overview,Data Pre-processing,machine-translation,5,49,1,0,,0.000247908,0,negative,0.000195851,8.40E-05,0.000375662,3.69E-08,2.13E-06,6.62E-06,2.10E-05,8.14E-06,0.002505662,0.996743895,2.18E-07,5.54E-05,1.35E-06
9160,machine-translation5,74,"For the constrained and unconstrained systems , we use BPE models consisting of 24,500 and 49,500 merging operations respectively .",System Overview,Data Pre-processing,machine-translation,5,50,1,0,,9.88E-05,0,negative,3.47E-05,2.43E-05,7.67E-06,2.15E-07,3.19E-06,0.00057189,0.000504093,0.00132938,8.80E-05,0.997406593,1.06E-07,2.11E-05,8.78E-06
9161,machine-translation5,75,"Finally , data for the constrained systems are factored using an averaged perceptron - based morpho-syntactic tagger for Estonian and the lexicalized probabilistic parser , we introduce also a factor indicating a word part 's position in a word ( beginning , middle , end , or the word part represents the whole word - B , I , E , or O ) .",System Overview,Data Pre-processing,machine-translation,5,51,1,0,,2.60E-07,0,negative,7.04E-05,1.14E-06,9.12E-06,8.17E-09,1.50E-06,6.15E-06,9.28E-06,3.45E-06,3.88E-06,0.999871202,2.55E-09,2.38E-05,8.21E-08
9162,machine-translation5,76,"As a result , the Estonian data consist of the the following factors : word part , position , lemma , and morpho-syntactic tag .",System Overview,Data Pre-processing,machine-translation,5,52,1,0,,1.01E-07,0,negative,3.68E-06,1.56E-07,5.56E-07,7.85E-09,6.94E-07,1.79E-06,6.03E-07,9.36E-07,8.73E-07,0.999988413,2.32E-09,2.27E-06,2.15E-08
9163,machine-translation5,77,"The English data consist of the following factors : word part , position , lemma , part - of - speech tag , and syntactic function .",System Overview,Data Pre-processing,machine-translation,5,53,1,0,,4.80E-07,0,negative,2.67E-06,2.45E-07,1.39E-06,1.08E-08,2.44E-06,5.91E-06,3.11E-06,2.51E-06,5.07E-07,0.999976387,1.09E-09,4.75E-06,5.50E-08
9164,machine-translation5,78,Synthetic Data,System Overview,,machine-translation,5,54,1,0,,0.039595415,0,negative,0.000389601,4.79E-05,0.000287631,1.23E-07,3.83E-06,0.000285277,0.018964943,0.00065509,3.82E-05,0.729062415,5.89E-06,0.250151142,0.000107894
9165,machine-translation5,79,"Similarly to Tilde 's 2017 systems , we submitted systems that were trained using synthetic data :",System Overview,Synthetic Data,machine-translation,5,55,1,0,,0.099774402,0,negative,1.49E-05,6.23E-09,9.28E-06,3.22E-09,1.37E-08,1.23E-07,1.54E-05,6.48E-07,1.29E-08,0.995562348,4.32E-10,0.004393938,3.33E-06
9166,machine-translation5,80,"1 ) back - translated data , and 2 ) data infused with unknown token identifiers .",System Overview,Synthetic Data,machine-translation,5,56,1,0,,0.000642383,0,negative,1.07E-05,4.88E-09,1.43E-05,7.60E-10,5.68E-09,3.94E-08,3.14E-06,3.18E-07,3.10E-08,0.998550769,2.11E-10,0.001419801,9.12E-07
9167,machine-translation5,81,"The back - translated data allow performing domain adaptation and the second type of synthetic data allow training NMT models thatare robust to unknown phenomena ( e.g. , code - mixed content , target language words in the source text , rare or unseen words , etc . ) .",System Overview,Synthetic Data,machine-translation,5,57,1,0,,0.000539253,0,negative,1.17E-05,8.01E-09,1.48E-05,1.91E-09,1.15E-08,5.49E-08,6.16E-06,3.71E-07,3.83E-08,0.996988622,6.21E-10,0.002975304,2.86E-06
9168,machine-translation5,82,"To create the synthetic corpora with unknown phenomena , we extracted fast align the parallel corpora and randomly replaced one to three unambiguously ( one - to - one ) aligned content words with unknown word identifiers .",System Overview,Synthetic Data,machine-translation,5,58,1,0,,0.002166317,0,negative,6.89E-06,1.14E-08,1.61E-06,4.43E-09,1.88E-08,2.05E-07,5.09E-06,3.83E-06,6.57E-08,0.999766454,1.04E-10,0.00021262,3.21E-06
9169,machine-translation5,83,"These synthetic corpora were added to the parallel corpora , thereby almost doubling the sizes of the available training data .",System Overview,Synthetic Data,machine-translation,5,59,1,0,,0.001381195,0,negative,4.34E-05,8.30E-09,8.04E-06,2.69E-09,1.77E-08,1.14E-07,1.08E-05,1.02E-06,6.42E-08,0.998591043,1.15E-10,0.001342583,2.91E-06
9170,machine-translation5,84,The back - translated data were acquired from two sources :,System Overview,Synthetic Data,machine-translation,5,60,1,0,,0.00230953,0,negative,2.02E-05,1.24E-08,3.10E-05,3.92E-09,1.98E-08,1.19E-07,7.70E-06,6.65E-07,6.74E-08,0.998688926,2.06E-10,0.001248553,2.76E-06
9171,machine-translation5,85,"1 ) the constrained system data were acquired from initial Transformer - based NMT systems that were trained on the filtered and preprocessed parallel data , which were supplemented with the unknown phenomena infused data , and 2 ) the unconstrained system data were acquired from pre-existing unconstrained MLSTM - based NMT systems - the NMT systems that were developed by Tilde for the Estonian EU Council Presidency in 2017 .",System Overview,Synthetic Data,machine-translation,5,61,1,0,,0.002362941,0,negative,1.26E-05,2.29E-08,1.91E-05,5.95E-09,3.25E-08,1.49E-07,7.22E-06,1.31E-06,1.09E-07,0.999171331,2.59E-10,0.000783711,4.38E-06
9172,machine-translation5,86,"In order to limit noise , the back - translated data were filtered using the same parallel data filtering methods that were described in Section 3.1.1 ( although with a higher threshold for the content overlap filter ) .",System Overview,Synthetic Data,machine-translation,5,62,1,0,,0.002617135,0,negative,5.20E-05,1.11E-08,1.34E-05,3.13E-09,6.46E-09,2.15E-07,9.44E-06,1.33E-06,1.27E-07,0.999128075,1.48E-10,0.000791566,3.90E-06
9173,machine-translation5,87,"Furthermore , in order to train the final systems , we also generated unknown phenomena infused data for the back - translated filtered data , thereby also almost doubling the sizes of the back - translated data .",System Overview,Synthetic Data,machine-translation,5,63,1,0,,0.001363006,0,negative,4.15E-05,7.37E-09,3.63E-06,2.91E-09,2.03E-08,9.01E-08,5.23E-06,9.33E-07,3.78E-08,0.99874522,5.76E-11,0.001200948,2.38E-06
9174,machine-translation5,88,The synthetic corpora statistics and the sizes of the total training data are given in .,System Overview,Synthetic Data,machine-translation,5,64,1,0,,0.00029744,0,negative,1.47E-06,3.44E-10,8.89E-08,4.55E-10,7.66E-10,3.25E-08,1.21E-06,4.56E-07,2.54E-09,0.99967432,1.83E-11,0.000321896,5.20E-07
9175,machine-translation5,89,Light Workflow,System Overview,,machine-translation,5,65,1,0,,0.010068926,0,negative,0.000114634,1.47E-05,9.77E-05,5.87E-05,0.000105394,0.002642464,0.001279081,0.00064747,6.29E-05,0.991433377,2.64E-06,0.003098297,0.000442676
9176,machine-translation5,90,The light workflow was used to produce the tilde - c - nmt - 2 bt ( constrained NMT with two sets of back - translated data ) systems .,System Overview,Light Workflow,machine-translation,5,66,1,0,,0.001370814,0,negative,9.89E-05,3.07E-05,0.000261765,3.46E-07,0.000101739,0.000140764,0.00610797,1.29E-05,9.51E-05,0.99283685,2.34E-08,0.000260638,5.23E-05
9177,machine-translation5,91,"First , we trained baseline models using only filtered parallel datasets ( Parallel - only in ) .",System Overview,Light Workflow,machine-translation,5,67,1,0,,0.005540206,0,negative,0.000180899,5.30E-05,0.000193589,2.43E-08,2.63E-05,2.55E-05,0.002258332,4.05E-06,7.12E-05,0.997060787,6.52E-09,0.000123358,3.00E-06
9178,machine-translation5,92,"Then , we back - translated the first batches of monolingual news data and trained intermediate NMT systems ( Parallel + First Back - translated ) .",System Overview,Light Workflow,machine-translation,5,68,1,0,,0.007338457,0,negative,0.000271767,5.16E-05,0.000476095,5.67E-08,6.20E-05,3.73E-05,0.003943718,5.21E-06,0.000168472,0.994805374,7.20E-09,0.000162369,1.61E-05
9179,machine-translation5,93,"Finally , we used the intermediate NMT systems to backtranslate the second batches of monolingual news data and trained final NMT systems ( Parallel + Second Back - translated ) .",System Overview,Light Workflow,machine-translation,5,69,1,0,,0.015860573,0,negative,0.000409473,6.28E-05,0.000516061,6.96E-08,5.87E-05,7.54E-05,0.010130419,1.18E-05,0.000203294,0.988277222,6.68E-09,0.000224474,3.03E-05
9180,machine-translation5,94,"The training progress in shows that the English - Estonian system benefits from the additional data , but the system in the other direction - not so much .",System Overview,Light Workflow,machine-translation,5,70,1,0,,0.039249085,0,negative,0.006936701,9.57E-07,3.61E-06,4.97E-08,2.65E-06,2.12E-05,0.029383276,7.07E-06,2.36E-06,0.948353867,1.47E-08,0.015246794,4.15E-05
9181,machine-translation5,95,"For the final translations , we used a postprocessing script to replace consecutive repeating n-grams and repeating ngrams that have a preposition between them ( i.e. , victim of the victim ) with a single n-gram .",System Overview,Light Workflow,machine-translation,5,71,1,0,,0.021447194,0,negative,0.000118569,2.11E-05,3.07E-05,4.87E-08,2.16E-05,4.71E-05,0.001265183,1.28E-05,0.000121486,0.998334635,2.53E-09,2.02E-05,6.54E-06
9182,machine-translation5,96,"This problem was more apparent in RNN - based NMT systems , but it was also noticable in our Transformer model outputs .",System Overview,Light Workflow,machine-translation,5,72,1,0,,2.81E-05,0,negative,1.01E-05,9.63E-08,3.51E-07,9.19E-09,4.26E-07,5.40E-06,4.51E-05,5.34E-07,6.70E-07,0.999929162,4.30E-09,7.77E-06,3.57E-07
9183,machine-translation5,97,NMT,System Overview,,machine-translation,5,73,1,0,,2.57E-05,0,negative,3.03E-05,4.21E-06,4.04E-05,7.20E-07,1.28E-05,0.000303967,0.00019074,0.000279389,2.51E-05,0.998774176,6.35E-08,0.000303886,3.42E-05
9184,machine-translation5,98,Systems,,,machine-translation,5,0,1,0,,0.003527455,0,negative,0.001099614,8.01E-05,0.000347478,5.52E-06,2.02E-05,0.000149153,0.001808826,0.000489155,2.62E-05,0.967336444,0.002055064,0.026553315,2.89E-05
9185,machine-translation5,99,"In order to train the NMT systems , we used the Nematus ( Sennrich et al. , 2017 b ) ( for MLSTM models ) and Sockeye ) ( for Transformer models ) toolkits .",Systems,Systems,machine-translation,5,1,1,0,,0.924983474,1,negative,0.001167904,2.52E-05,0.013152855,7.39E-05,9.26E-06,0.016629633,0.062054705,0.00443931,0.000149786,0.897199606,1.12E-05,0.000134191,0.004952432
9186,machine-translation5,100,"All models were trained until convergence ( i.e. , until an early stopping criterion was met ) .",Systems,Systems,machine-translation,5,2,1,0,,0.861711061,1,negative,0.001182693,2.42E-05,0.00201987,6.63E-06,1.20E-06,0.002638049,0.010832135,0.005604674,0.000246562,0.976539385,8.28E-06,4.99E-05,0.000846391
9187,machine-translation5,101,Figure 1 : NMT system training progress ( BLEU scores on the validation set ) for English - Estonian ( left ) and,Systems,Systems,machine-translation,5,3,1,0,,0.016241974,0,negative,0.011130038,1.39E-06,0.041728856,5.12E-07,6.84E-07,4.75E-05,0.042458175,1.75E-05,3.55E-06,0.853963929,3.10E-05,0.049210922,0.001405972
9188,machine-translation5,102,Estonian - English ( right ) .,Systems,Systems,machine-translation,5,4,1,0,,0.002187969,0,negative,0.000618333,3.47E-07,0.002293521,8.47E-07,9.69E-07,5.61E-05,0.001971188,1.32E-05,3.56E-06,0.994190748,1.71E-06,0.00058926,0.000260293
9189,machine-translation5,103,"Note that batch size may differ between different architectures and BLEU scores are calculated on raw ( token level ) pre-processed validation sets , therefore , the scores are slightly higher than evaluation results for the final translations !",Systems,Systems,machine-translation,5,5,1,0,,0.000310606,0,negative,0.001749374,2.79E-07,0.000424021,8.36E-08,1.36E-07,1.81E-05,0.000523091,1.02E-05,1.02E-06,0.99695855,2.99E-07,0.00030497,9.86E-06
9190,machine-translation5,104,Automatic Post - editing of Named Entities,Systems,,machine-translation,5,6,1,0,,0.968415635,1,negative,0.012368269,1.75E-05,0.014877806,1.63E-05,5.22E-06,0.000188817,0.280898268,8.84E-05,1.83E-05,0.457452149,0.003521866,0.124020239,0.106526946
9191,machine-translation5,105,"NMT models so far have struggled with translating rare or unseen words ( not different surface forms , but rather different words ) correctly .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,7,1,0,,0.42650767,0,negative,0.000233147,2.17E-06,6.44E-06,7.31E-06,2.02E-06,8.66E-05,0.017008464,1.01E-05,3.23E-06,0.972594072,0.008640208,0.000389263,0.001016933
9192,machine-translation5,106,"Named entities and non-translatable entities ( various product names , identifiers , etc. ) are often rare or unknown .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,8,1,0,,0.002087733,0,negative,3.03E-05,2.58E-07,1.37E-06,3.77E-07,6.46E-07,9.97E-06,0.000657077,1.37E-06,8.16E-07,0.999217025,2.82E-05,4.06E-05,1.19E-05
9193,machine-translation5,107,"In order to aid the NMT model in translating such tokens better , we extracted named entity and non-translatable token dictionaries from the parallel corpora .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,9,1,0,,0.005082438,0,negative,0.000605478,2.68E-05,9.06E-05,1.04E-05,0.000822832,0.000302764,0.018935764,2.65E-05,2.80E-05,0.97882,3.84E-06,0.000219875,0.000107197
9194,machine-translation5,108,"This was done by performing word alignment of the parallel corpora using fast align and searching ( in a language - agnostic manner ) for transliterated source - target word pairs using a similarity metric based on Levenshtein distance , which start with upper-case letters .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,10,1,0,,0.004242144,0,negative,0.003552097,0.000242329,0.000948843,3.68E-06,0.0001185,0.00017829,0.023286321,2.41E-05,0.000441082,0.970362463,1.37E-05,0.000752786,7.57E-05
9195,machine-translation5,109,The dictionaries consist of 15.6 ( 94.7 ) thousand and 6.2 ( 149.8 ) thousand entries for the constrained ( unconstrained ) English - Estonian and Estonian - English NMT systems respectively .,Systems,Automatic Post - editing of Named Entities,machine-translation,5,11,1,0,,0.009179334,0,negative,0.000499244,1.02E-05,4.60E-05,4.46E-05,0.000748848,0.000644223,0.024256874,2.60E-05,1.09E-05,0.973242407,2.13E-06,0.000125921,0.000342613
9196,machine-translation5,110,"When the NMT systems had translated a sentence , source - to - target word alignment was extracted from the source sentence and the translation .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,12,1,0,,0.00117826,0,negative,0.00043587,1.27E-05,0.000180752,1.35E-07,6.86E-06,2.48E-05,0.003680286,3.48E-06,6.89E-05,0.995414196,4.31E-06,0.000157829,9.93E-06
9197,machine-translation5,111,"Then named entity recognition ( based on dictionary look - up ) was performed on the source text and , if a named entity was found , the target translation was validated against the entries in the dic-tionary .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,13,1,0,,0.004358451,0,negative,0.000892286,9.02E-06,0.00017508,2.23E-07,9.97E-06,2.82E-05,0.009134179,3.35E-06,3.37E-05,0.989301287,3.61E-06,0.000387484,2.16E-05
9198,machine-translation5,112,"In order to capture different surface forms , a stemming tool was used .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,14,1,0,,0.002222614,0,negative,0.000385825,1.01E-05,4.51E-05,5.55E-07,8.69E-06,8.25E-05,0.002940669,1.59E-05,0.00010973,0.996313919,1.37E-06,7.45E-05,1.11E-05
9199,machine-translation5,113,"If a translation was contradicting the entries in the dictionary , it was replaced with the closest matching ( by looking for the longest matching suffix ) translation from the dictionary .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,15,1,0,,0.00024151,0,negative,0.00056368,3.08E-06,3.56E-05,5.22E-08,1.81E-06,1.41E-05,0.002308653,2.88E-06,2.11E-05,0.99696378,3.99E-07,8.19E-05,3.01E-06
9200,machine-translation5,114,"The automatic post-editing method for named entities has a marginal impact on translation quality , however , manual analysis showed that more named entities were corrected than ruined .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,16,1,0,,0.852748571,1,negative,0.041971259,8.67E-06,6.63E-05,4.49E-06,3.16E-05,4.98E-05,0.162724353,8.03E-06,4.16E-06,0.768547875,0.000124386,0.0252784,0.001180587
9201,machine-translation5,115,System Combination,,,machine-translation,5,0,1,0,,0.118440898,0,negative,0.011325244,0.001184578,0.014895123,2.04E-05,4.98E-05,0.000316277,0.002035012,0.001086693,0.000753099,0.895902327,0.008827507,0.063485833,0.000118086
9202,machine-translation5,116,We attempted to increase the quality of existing translations by employing a voting scheme in which multiple machine translation outputs are combined to produce a single translation .,System Combination,System Combination,machine-translation,5,1,1,0,,0.012672259,0,negative,0.018151229,9.11E-05,0.003053341,2.25E-05,1.27E-05,5.61E-05,8.10E-05,2.73E-05,0.000199842,0.969097472,0.006078654,0.000873737,0.002255061
9203,machine-translation5,117,We used a custom implementation of the majority voting algorithm to combine six of our best - scoring outputs in the Estonian - English translation direction in the constrained scenario .,System Combination,System Combination,machine-translation,5,2,1,0,,0.001337479,0,negative,0.005562413,0.000183379,0.010048284,1.29E-05,1.96E-05,0.000255521,0.000207271,0.000230744,0.001925344,0.980863137,1.81E-05,9.32E-05,0.000580179
9204,machine-translation5,118,We did not perform the combination for English - Estonian due to lack of support for alignment extraction for Estonian in Meteor .,System Combination,System Combination,machine-translation,5,3,1,0,,5.14E-05,0,negative,0.018202467,2.24E-06,0.000907432,3.08E-06,1.14E-05,2.45E-05,3.22E-05,4.97E-06,1.45E-05,0.980317452,5.44E-06,0.000407159,6.71E-05
9205,machine-translation5,119,MT system translation combination happens on the sentence level .,System Combination,System Combination,machine-translation,5,4,1,0,,0.001843593,0,negative,0.009022348,2.04E-05,0.00085497,1.56E-05,2.91E-06,4.56E-05,0.000439743,2.66E-05,6.81E-05,0.909831778,0.049624593,0.002480437,0.02756699
9206,machine-translation5,120,The majority voting scheme assumes a single base translation hypothesis ( primary hypothesis ) which is aligned at the word level to each of the other hypotheses ( secondary hypotheses ) .,System Combination,System Combination,machine-translation,5,5,1,0,,0.002240144,0,negative,0.003782678,0.00012049,0.190293549,4.78E-07,1.27E-06,2.14E-05,5.86E-05,2.31E-05,0.003062738,0.802347286,9.10E-05,0.000107406,9.00E-05
9207,machine-translation5,121,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis .,System Combination,System Combination,machine-translation,5,6,1,0,,0.000284514,0,negative,0.000826318,3.04E-05,0.000661438,2.71E-07,1.65E-06,7.26E-06,6.65E-06,1.47E-05,0.001100932,0.997297199,5.16E-06,2.98E-05,1.82E-05
9208,machine-translation5,122,The table is then used to count the number of occurrences of different translations .,System Combination,System Combination,machine-translation,5,7,1,0,,3.05E-05,0,negative,0.000195107,1.16E-06,0.000116398,2.49E-08,1.10E-07,1.62E-06,9.70E-07,2.46E-06,9.63E-05,0.999576462,6.79E-07,6.45E-06,2.27E-06
9209,machine-translation5,123,The word translations with the highest count at each position constitute the resulting combined hypothesis .,System Combination,System Combination,machine-translation,5,8,1,0,,6.77E-05,0,negative,0.000913158,7.64E-06,0.000775556,9.57E-08,5.22E-07,3.72E-06,3.37E-06,7.01E-06,0.000434141,0.997825946,1.37E-06,1.71E-05,1.04E-05
9210,machine-translation5,124,To acquire the necessary word alignments we used Meteor .,System Combination,System Combination,machine-translation,5,9,1,0,,0.035725715,0,negative,0.006902399,5.92E-05,0.003019851,9.94E-05,5.46E-05,0.002967751,0.001174333,0.001274044,0.000504175,0.974918124,1.33E-05,0.000142927,0.00886983
9211,machine-translation5,125,Meteor outputs were then converted to a more easily manageable form using the Jane toolkit ) ( we used an awk script distributed with Jane ) .,System Combination,System Combination,machine-translation,5,10,1,0,,0.00058956,0,negative,0.001360615,3.03E-06,0.000657324,1.03E-06,2.28E-06,2.19E-05,1.57E-05,1.27E-05,8.14E-05,0.997765017,1.25E-06,3.21E-05,4.57E-05
9212,machine-translation5,126,The majority voting algorithm was implemented in Python .,System Combination,System Combination,machine-translation,5,11,1,0,,0.111945086,0,negative,0.003051253,0.000136748,0.002588928,0.000176394,1.52E-05,0.003554102,0.001425719,0.003380396,0.00238533,0.966939276,7.14E-05,0.000112458,0.016162864
9213,machine-translation5,127,Results,,,machine-translation,5,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
9214,machine-translation5,128,We performed automatic evaluation of the NMT systems using the SacreBLEU evaluation tool .,Results,Results,machine-translation,5,1,1,0,,0.142833575,0,negative,0.039658471,0.000186741,0.006059196,0.00012476,0.000251952,0.000289761,0.006549445,0.000591898,8.65E-05,0.574015097,0.000283468,0.37055625,0.00134646
9215,machine-translation5,129,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,Results,Results,machine-translation,5,2,1,1,results,0.969595926,1,results,0.005206738,3.01E-07,2.17E-05,4.49E-07,2.65E-07,7.30E-06,0.002135064,3.67E-05,1.30E-07,0.025315833,5.74E-06,0.967184822,8.49E-05
9216,machine-translation5,130,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .",Results,Results,machine-translation,5,3,1,1,results,0.935095887,1,results,0.013971995,7.30E-07,4.25E-05,1.61E-06,7.49E-07,1.33E-05,0.002160373,6.01E-05,5.66E-07,0.031669876,7.76E-06,0.951905224,0.000165199
9217,machine-translation5,131,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,Results,Results,machine-translation,5,4,1,1,results,0.978915548,1,results,0.018503994,1.13E-06,5.53E-05,4.13E-06,1.10E-06,2.84E-05,0.004461492,0.000118355,6.72E-07,0.021630081,1.16E-05,0.954853727,0.000330001
9218,machine-translation5,132,"Although the unconstrained models were not trained on factored data , the datasets were 17 times larger than the constrained datasets .",Results,Results,machine-translation,5,5,1,0,,0.109344814,0,negative,0.03203,2.78E-05,0.000439008,0.000154686,0.000140084,0.000788411,0.005086841,0.001991709,3.69E-05,0.867228594,2.67E-05,0.090246845,0.001802377
9219,machine-translation5,133,"However , the difference is rather minimal and shows that the current NMT architectures may notable to learn effectively from large datasets .",Results,Results,machine-translation,5,6,1,0,,0.012508162,0,results,0.02884175,4.15E-06,0.000501279,2.00E-06,2.49E-06,1.89E-05,0.001055254,5.75E-05,2.70E-06,0.358847357,2.52E-05,0.610529952,0.000111422
9220,machine-translation5,134,The official human evaluation results ( see Table 5 ) from the WMT 2018 shared task on news translation our unconstrained scenario systems ( tilde - nc - nmt ) ranked significantly higher than any other submission for both translation directions .,Results,Results,machine-translation,5,7,1,0,,0.950726463,1,results,0.000987607,4.15E-07,2.75E-05,8.74E-07,8.51E-07,6.67E-06,0.003833491,2.44E-05,1.39E-07,0.030741678,9.76E-06,0.964024858,0.000341755
9221,machine-translation5,135,"Our best constrained systems were the second highest ranked systems among all constrained scenario systems , at the same time sharing the same cluster with the highest ranked systems .",Results,Results,machine-translation,5,8,1,0,,0.89984624,1,results,0.04543742,2.20E-05,0.004417073,2.55E-05,3.15E-05,0.000143013,0.006743153,0.000326187,2.05E-05,0.337299564,2.13E-05,0.60490805,0.000604772
9222,machine-translation5,136,Conclusion,,,machine-translation,5,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
9223,question_answering1,1,title,,,question_answering,1,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
9224,question_answering1,2,BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,title,title,question_answering,1,1,1,1,research-problem,0.999102342,1,research-problem,2.40E-08,8.37E-06,5.71E-08,3.73E-08,2.48E-08,7.02E-08,7.12E-07,1.45E-06,1.65E-06,0.001413844,0.998573606,1.25E-07,3.56E-08
9225,question_answering1,3,abstract,,,question_answering,1,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
9226,question_answering1,4,"Machine comprehension ( MC ) , answering a query about a given context paragraph , requires modeling complex interactions between the context and the query .",abstract,abstract,question_answering,1,1,1,1,research-problem,0.981221049,1,research-problem,1.38E-08,2.49E-06,1.07E-08,8.71E-07,1.61E-07,1.39E-07,4.61E-07,5.10E-07,1.57E-07,0.003756739,0.996238383,1.38E-08,4.87E-08
9227,question_answering1,5,"Recently , attention mechanisms have been successfully extended to MC .",abstract,abstract,question_answering,1,2,1,1,research-problem,0.394686493,0,research-problem,8.87E-08,5.33E-05,5.59E-08,3.16E-07,3.09E-07,7.18E-07,3.92E-07,8.99E-06,9.98E-06,0.063305433,0.936620338,5.57E-08,3.41E-08
9228,question_answering1,6,"Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed - size vector , couple attentions temporally , and / or often form a uni-directional attention .",abstract,abstract,question_answering,1,3,1,0,,0.013141023,0,negative,9.47E-07,0.000537635,5.72E-07,4.31E-05,2.38E-05,3.19E-05,3.75E-06,0.000130487,4.34E-05,0.56904862,0.43013503,2.52E-07,5.08E-07
9229,question_answering1,7,"In this paper we introduce the Bi- Directional Attention Flow ( BIDAF ) network , a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query - aware context representation without early summarization .",abstract,abstract,question_answering,1,4,1,0,,0.805959965,1,research-problem,1.96E-05,0.075202366,0.000369179,7.23E-06,9.86E-05,2.19E-05,2.66E-05,0.000196692,0.022937014,0.083397211,0.817707124,1.25E-05,4.07E-06
9230,question_answering1,8,Our experimental evaluations show that our model achieves the state - of - the - art results in Stanford Question Answering Dataset ( SQuAD ) and CNN / DailyMail cloze test .,abstract,abstract,question_answering,1,5,1,0,,0.007611326,0,negative,7.92E-05,0.001573184,4.36E-06,8.32E-06,6.02E-05,2.50E-05,0.000129115,0.000436571,2.68E-05,0.7390003,0.257699418,0.000953488,4.18E-06
9231,question_answering1,9,INTRODUCTION,,,question_answering,1,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
9232,question_answering1,10,The tasks of machine comprehension ( MC ) and question answering ( QA ) have gained significant popularity over the past few years within the natural language processing and computer vision communities .,INTRODUCTION,INTRODUCTION,question_answering,1,1,1,1,research-problem,0.971081785,1,research-problem,5.31E-07,6.43E-05,1.63E-07,6.28E-06,1.04E-05,2.64E-06,1.11E-05,2.73E-06,1.33E-05,0.021986842,0.977899652,7.37E-07,1.44E-06
9233,question_answering1,11,Systems trained end - to - end now achieve promising results on a variety of tasks in the text and image domains .,INTRODUCTION,INTRODUCTION,question_answering,1,2,1,0,,0.054494647,0,research-problem,2.45E-06,0.000516389,7.14E-07,3.50E-06,1.85E-05,1.57E-05,4.02E-05,2.95E-05,0.000263929,0.191315399,0.807779531,1.08E-05,3.32E-06
9234,question_answering1,12,"One of the key factors to the advancement has been the use of neural attention mechanism , which enables the system to focus on a targeted are a within a context paragraph ( for MC ) or within an image ( for Visual QA ) , that is most relevant to answer the question .",INTRODUCTION,INTRODUCTION,question_answering,1,3,1,0,,0.094955553,0,research-problem,1.11E-05,0.001710027,2.66E-06,3.73E-05,8.17E-05,6.00E-05,2.11E-05,5.59E-05,0.001463194,0.410421788,0.586127716,4.08E-06,3.37E-06
9235,question_answering1,13,Attention mechanisms in previous works typically have one or more of the following characteristics .,INTRODUCTION,INTRODUCTION,question_answering,1,4,1,0,,0.004324906,0,negative,8.09E-06,0.008980323,5.75E-06,6.01E-06,6.17E-05,0.000116395,3.52E-05,0.000244376,0.020795901,0.710330115,0.259406677,6.46E-06,3.09E-06
9236,question_answering1,14,"First , the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed - size vector .",INTRODUCTION,INTRODUCTION,question_answering,1,5,1,0,,0.421260506,0,negative,3.75E-05,0.012082794,6.23E-06,5.43E-05,0.000330501,0.00013316,2.75E-05,0.000213117,0.005142585,0.849568293,0.132392569,7.47E-06,3.94E-06
9237,question_answering1,15,"Second , in the text domain , they are often temporally dynamic , whereby the attention weights at the current time step are a function of the attended vector at the previous time step .",INTRODUCTION,INTRODUCTION,question_answering,1,6,1,0,,0.096528567,0,negative,8.03E-06,0.004836574,4.81E-06,3.22E-05,0.000137272,9.24E-05,2.95E-05,9.74E-05,0.002744816,0.535069399,0.456938393,4.90E-06,4.24E-06
9238,question_answering1,16,"Third , they are usually uni-directional , wherein the query attends on the context paragraph or the image .",INTRODUCTION,INTRODUCTION,question_answering,1,7,1,0,,0.013882263,0,negative,2.22E-05,0.012420678,1.81E-05,0.000141175,0.000988724,0.000318065,6.66E-05,0.000196539,0.004979138,0.798338392,0.182491166,9.38E-06,9.81E-06
9239,question_answering1,17,"In this paper , we introduce the Bi- Directional Attention Flow ( BIDAF ) network , a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity ) .",INTRODUCTION,INTRODUCTION,question_answering,1,8,1,1,model,0.981158958,1,model,1.97E-05,0.092784302,0.000166503,1.22E-06,9.28E-05,1.85E-05,2.92E-05,3.35E-05,0.888028067,0.008660439,0.01015448,8.09E-06,3.26E-06
9240,question_answering1,18,"BIDAF includes character - level , word - level , and contextual embeddings , and uses bi-directional attention flow to obtain a query - aware context representation .",INTRODUCTION,INTRODUCTION,question_answering,1,9,1,1,model,0.961812672,1,model,5.81E-05,0.338781298,0.002538507,8.48E-06,0.001712768,0.000146651,0.000245483,0.000163037,0.612613415,0.037944038,0.005726771,4.22E-05,1.92E-05
9241,question_answering1,19,Our attention mechanism offers following improvements to the previously popular attention paradigms .,INTRODUCTION,INTRODUCTION,question_answering,1,10,1,0,,0.522339699,1,model,0.000337131,0.116558305,0.000183503,1.86E-05,0.001019934,9.41E-05,0.000113117,0.000109876,0.631909208,0.232243043,0.017310937,9.30E-05,9.24E-06
9242,question_answering1,20,"First , our attention layer is not used to summarize the context paragraph into a fixed - size vector .",INTRODUCTION,INTRODUCTION,question_answering,1,11,1,0,,0.193755869,0,negative,0.001631009,0.078651804,7.11E-05,0.000539607,0.015001486,0.000543218,0.00019526,0.000409798,0.008087342,0.888603979,0.006140147,0.000109449,1.58E-05
9243,question_answering1,21,"Instead , the attention is computed for every time step , and the attended vector at each time step , along with the representations from previous layers , is allowed to flow through to the subsequent modeling layer .",INTRODUCTION,INTRODUCTION,question_answering,1,12,1,1,model,0.855718602,1,model,9.37E-06,0.068768061,1.87E-05,2.50E-07,4.17E-05,1.00E-05,3.67E-06,3.07E-05,0.918121127,0.012741586,0.000253636,9.13E-07,2.91E-07
9244,question_answering1,22,This reduces the information loss caused by early summarization .,INTRODUCTION,INTRODUCTION,question_answering,1,13,1,0,,0.776298916,1,negative,0.001315875,0.110453209,0.000181898,3.25E-05,0.003858159,0.00011436,0.000250393,0.000146653,0.091158484,0.778844858,0.013207648,0.000425099,1.09E-05
9245,question_answering1,23,"Second , we use a memory - less attention mechanism .",INTRODUCTION,INTRODUCTION,question_answering,1,14,1,1,model,0.552439346,1,model,0.000340743,0.192807456,0.000535742,8.09E-06,0.00106499,0.000105743,9.49E-05,0.000128534,0.699165128,0.103414514,0.00229356,3.49E-05,5.63E-06
9246,question_answering1,24,"That is , while we iteratively compute attention through time as in , the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step .",INTRODUCTION,INTRODUCTION,question_answering,1,15,1,0,,0.283605895,0,model,2.51E-05,0.144252375,3.47E-05,9.66E-07,0.000206896,1.72E-05,8.23E-06,4.68E-05,0.805164652,0.049678098,0.000560237,4.15E-06,6.06E-07
9247,question_answering1,25,We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer .,INTRODUCTION,INTRODUCTION,question_answering,1,16,1,0,,0.030040174,0,negative,0.000130874,0.038831619,2.76E-05,0.000122074,0.005153436,0.000253192,3.81E-05,0.000241354,0.040267245,0.913554326,0.001363132,1.32E-05,3.82E-06
9248,question_answering1,26,"It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .",INTRODUCTION,INTRODUCTION,question_answering,1,17,1,1,model,0.821866489,1,model,2.99E-05,0.036914534,6.34E-05,2.69E-07,5.31E-05,8.20E-06,6.77E-06,1.37E-05,0.948163719,0.01456638,0.000176934,2.53E-06,4.53E-07
9249,question_answering1,27,It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps .,INTRODUCTION,INTRODUCTION,question_answering,1,18,1,1,model,0.847007553,1,model,2.47E-05,0.068862201,7.14E-05,5.13E-07,0.000112446,1.25E-05,1.19E-05,2.15E-05,0.905552387,0.024904608,0.000420199,4.90E-06,7.35E-07
9250,question_answering1,28,Our experiments show that memory - less attention gives a clear advantage over dynamic attention .,INTRODUCTION,INTRODUCTION,question_answering,1,19,1,0,,0.037037869,0,negative,0.002997808,0.150787096,6.06E-05,2.90E-05,0.001587333,0.000414472,0.00176188,0.001192518,0.080529394,0.753929931,0.005650084,0.001024701,3.51E-05
9251,question_answering1,29,"Third , we use attention mechanisms in both directions , query - to - context and context - to - query , which provide complimentary information to each other .",INTRODUCTION,INTRODUCTION,question_answering,1,20,1,1,model,0.907938417,1,model,4.24E-05,0.13649664,0.000164404,1.51E-06,0.00034589,2.99E-05,3.10E-05,3.82E-05,0.851244598,0.011452832,0.000144916,6.06E-06,1.74E-06
9252,question_answering1,30,Our BIDAF model 1 outperforms all previous approaches on the highly - competitive Stanford Question Answering Dataset ( SQuAD ) test set leaderboard at the time of submission .,INTRODUCTION,INTRODUCTION,question_answering,1,21,1,0,,0.085218702,0,negative,0.002126224,0.080264175,0.00016701,0.000273964,0.005625208,0.001478397,0.057552246,0.003422775,0.006016155,0.788569016,0.02531965,0.028107789,0.001077391
9253,question_answering1,31,"With a modification to only the output layer , BIDAF achieves the state - of - the - art results on the CNN / DailyMail cloze test .",INTRODUCTION,INTRODUCTION,question_answering,1,22,1,0,,0.138820665,0,negative,0.003088801,0.082809169,0.00036967,0.000139345,0.005369734,0.001174822,0.04818217,0.002285298,0.012771881,0.802737053,0.014098238,0.026186075,0.000787743
9254,question_answering1,32,"We also provide an in - depth ablation study of our model on the SQuAD development set , visualize the intermediate feature spaces in our model , and analyse its performance as compared to a more traditional language model for machine comprehension .",INTRODUCTION,INTRODUCTION,question_answering,1,23,1,0,,0.026495412,0,negative,0.000251875,0.190022863,0.000133191,0.000323823,0.139656427,0.000455256,0.000532995,0.000233842,0.011329202,0.655943321,0.000911128,0.000184033,2.20E-05
9255,question_answering1,33,MODEL,,,question_answering,1,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
9256,question_answering1,34,Our machine comprehension model is a hierarchical multi-stage process and consists of six layers ( ) :,MODEL,MODEL,question_answering,1,1,1,0,,0.089429884,0,model,0.000516925,0.003477552,0.010801042,1.74E-05,4.46E-05,0.000254334,0.0002854,0.001193834,0.539329703,0.441426428,0.001645737,0.000781782,0.000225297
9257,question_answering1,35,1 .,MODEL,MODEL,question_answering,1,2,1,0,,0.000205886,0,negative,4.67E-05,2.65E-05,1.38E-05,1.09E-06,9.14E-07,5.07E-05,1.24E-05,0.000330543,0.001634781,0.997805094,1.26E-05,6.27E-05,2.08E-06
9258,question_answering1,36,Character Embedding Layer maps each word to a vector space using character - level CNNs .,MODEL,MODEL,question_answering,1,3,1,0,,0.359860616,0,negative,0.000414985,0.002441318,0.011216929,2.08E-05,3.03E-05,0.000956346,0.000768924,0.00813875,0.45338645,0.521515127,0.000367186,0.000423933,0.000318972
9259,question_answering1,37,2 . Word Embedding,MODEL,MODEL,question_answering,1,4,1,0,,0.069692023,0,negative,0.000294468,0.002454538,0.024992695,6.45E-06,9.37E-06,0.000382124,0.000616091,0.002161875,0.247042001,0.717053301,0.002672533,0.002121338,0.000193218
9260,question_answering1,38,Layer maps each word to a vector space using a pre-trained word embedding model .,MODEL,MODEL,question_answering,1,5,1,0,,0.253854933,0,negative,0.001010135,0.003842969,0.007628984,5.32E-05,7.00E-05,0.000729941,0.000645266,0.009169542,0.474514994,0.50107266,0.000263799,0.000628195,0.000370291
9261,question_answering1,39,3 .,MODEL,MODEL,question_answering,1,6,1,0,,0.001905008,0,negative,8.42E-05,2.84E-05,2.71E-05,6.64E-07,8.48E-07,4.53E-05,2.21E-05,0.000290582,0.002032543,0.997316439,8.29E-06,0.000141111,2.47E-06
9262,question_answering1,40,Contextual Embedding Layer utilizes contextual cues from surrounding words to refine the embedding of the words .,MODEL,MODEL,question_answering,1,7,1,0,,0.450114619,0,negative,0.000571634,0.00206674,0.013154348,1.68E-05,3.38E-05,0.000400351,0.000382051,0.002864527,0.414063761,0.565379349,0.000201569,0.000596797,0.000268325
9263,question_answering1,41,These first three layers are applied to both the query and context .,MODEL,MODEL,question_answering,1,8,1,0,,0.050602374,0,negative,0.000207698,0.000746829,0.002657367,1.10E-06,3.85E-06,7.58E-05,5.90E-05,0.000745879,0.371736492,0.623550989,3.70E-05,0.000160575,1.74E-05
9264,question_answering1,42,Attention Flow,MODEL,,question_answering,1,9,1,0,,0.042206401,0,negative,0.002108242,0.001130385,0.009950546,2.87E-05,3.72E-05,0.000470596,0.001402363,0.002037955,0.107355555,0.862306802,0.000553395,0.012216971,0.000401334
9265,question_answering1,43,Layer .,MODEL,,question_answering,1,10,1,0,,0.064733533,0,negative,0.000318009,0.000851686,0.001671681,1.74E-05,1.94E-05,0.000370519,0.000355141,0.002705033,0.235526948,0.756737271,0.000309957,0.000860583,0.00025645
9266,question_answering1,44,Attention flow layer is responsible for linking and fusing information from the context and the query words .,MODEL,Layer .,question_answering,1,11,1,0,,0.000428491,0,negative,0.000154637,0.00174421,0.000549307,1.07E-05,1.38E-05,0.00015834,2.76E-05,0.004715773,0.109203793,0.883257883,5.92E-05,7.24E-05,3.24E-05
9267,question_answering1,45,"Unlike previously popular attention mechanisms , the attention flow layer is not used to summarize the query and context into single feature vectors .",MODEL,Layer .,question_answering,1,12,1,0,,0.000127857,0,negative,0.000650929,0.004203484,0.00094738,1.01E-06,1.21E-05,1.62E-05,6.91E-06,0.000474999,0.022522067,0.970730579,4.05E-05,0.000391497,2.42E-06
9268,question_answering1,46,"Instead , the attention vector at each time step , along with the embeddings from previous layers , are allowed to flow through to the subsequent modeling layer .",MODEL,Layer .,question_answering,1,13,1,0,,1.08E-05,0,negative,3.08E-05,0.000734281,7.56E-05,9.29E-08,6.65E-07,1.08E-05,1.15E-06,0.000740257,0.019220161,0.979170547,3.87E-06,1.15E-05,2.55E-07
9269,question_answering1,47,This reduces the information loss caused by early summarization .,MODEL,Layer .,question_answering,1,14,1,0,,0.000179921,0,negative,0.001024906,0.000227969,0.000223546,4.78E-07,5.59E-06,6.34E-06,7.10E-06,0.000126039,0.000887013,0.994041946,1.17E-05,0.003435995,1.34E-06
9270,question_answering1,48,The inputs to the layer are contextual vector representations of the context H and the query U .,MODEL,Layer .,question_answering,1,15,1,0,,2.60E-05,0,negative,1.01E-05,0.000317924,4.23E-05,3.73E-07,1.13E-06,2.84E-05,2.31E-06,0.001444529,0.014338259,0.983801247,4.29E-06,8.05E-06,1.08E-06
9271,question_answering1,49,"The outputs of the layer are the query - aware vector representations of the context words , G , along with the contextual embeddings from the previous layer .",MODEL,Layer .,question_answering,1,16,1,0,,1.38E-05,0,negative,1.02E-05,0.000397275,4.76E-05,2.34E-07,9.41E-07,1.63E-05,1.78E-06,0.000858527,0.017154673,0.981499224,3.91E-06,8.47E-06,8.58E-07
9272,question_answering1,50,"In this layer , we compute attentions in two directions : from context to query as well as from query to context .",MODEL,Layer .,question_answering,1,17,1,0,,0.000447243,0,negative,0.000142718,0.003811099,0.00178385,1.32E-06,1.08E-05,2.91E-05,8.95E-06,0.000668292,0.205410604,0.787991001,3.40E-05,9.98E-05,8.41E-06
9273,question_answering1,51,"Both of these attentions , which will be discussed below , are derived from a shared similarity matrix , S ? R T J , between the contextual embeddings of the context ( H ) and the query ( U ) , where S tj indicates the similarity between t- th context word and j- th query word .",MODEL,Layer .,question_answering,1,18,1,0,,8.18E-07,0,negative,3.64E-05,0.000330779,0.000421823,5.56E-08,9.51E-07,3.59E-06,1.07E-06,0.000100684,0.009206365,0.989860191,2.88E-06,3.49E-05,2.91E-07
9274,question_answering1,52,The similarity matrix is computed by,MODEL,Layer .,question_answering,1,19,1,0,,3.66E-06,0,negative,9.86E-06,3.88E-05,2.25E-05,2.49E-08,1.86E-07,4.28E-06,7.31E-07,0.000184847,0.000902364,0.998815409,1.04E-06,1.98E-05,1.32E-07
9275,question_answering1,53,where ?,MODEL,Layer .,question_answering,1,20,1,0,,7.77E-08,0,negative,1.67E-06,8.77E-06,1.23E-06,1.25E-07,1.25E-07,5.52E-06,4.47E-07,0.000158785,0.000178735,0.999638807,1.95E-06,3.71E-06,1.30E-07
9276,question_answering1,54,"is a trainable scalar function that encodes the similarity between it s two input vectors , H :t is t-th column vector of H , and U :j is j-th column vector of U , We choose ?( h , u ) = w ( S ) [ h ; u ; h u ] , where w ( S ) ?",MODEL,Layer .,question_answering,1,21,1,0,,1.79E-06,0,negative,5.84E-06,4.79E-05,1.21E-05,5.13E-08,3.48E-07,6.70E-06,1.02E-06,0.000312104,0.000494902,0.999101823,1.28E-06,1.57E-05,1.76E-07
9277,question_answering1,55,"R 6d is a trainable weight vector , is elementwise multiplication , [ ; ] is vector concatenation across row , and implicit multiplication is matrix multiplication .",MODEL,Layer .,question_answering,1,22,1,0,,1.22E-06,0,negative,8.80E-06,8.27E-05,1.49E-05,2.70E-07,1.05E-06,1.11E-05,1.31E-06,0.00045705,0.000821282,0.998589468,1.13E-06,1.06E-05,3.30E-07
9278,question_answering1,56,Now we use S to obtain the attentions and the attended vectors in both directions .,MODEL,Layer .,question_answering,1,23,1,0,,2.98E-06,0,negative,1.38E-05,3.73E-05,2.52E-05,2.10E-08,2.38E-07,3.07E-06,7.57E-07,0.000116628,0.00117405,0.998611037,3.17E-07,1.75E-05,1.38E-07
9279,question_answering1,57,Context - to - query Attention .,MODEL,Layer .,question_answering,1,24,1,0,,1.45E-05,0,negative,2.00E-05,0.000224982,0.001738086,3.96E-08,5.24E-07,5.85E-06,3.65E-06,0.000140476,0.005442878,0.9922322,1.87E-05,0.000171496,1.08E-06
9280,question_answering1,58,Context - to - query ( C2Q ) attention signifies which query words are most relevant to each context word .,MODEL,Layer .,question_answering,1,25,1,0,,0.002349542,0,negative,0.000125002,0.002409568,0.00143831,2.73E-05,3.36E-05,0.000102964,0.00019754,0.002166507,0.004000886,0.971305649,0.0154724,0.002534895,0.000185389
9281,question_answering1,59,Leta t ?,MODEL,Layer .,question_answering,1,26,1,0,,1.39E-07,0,negative,6.69E-06,3.74E-06,2.69E-06,2.95E-08,1.78E-07,2.35E-06,5.93E-07,6.31E-05,5.96E-05,0.999840595,3.07E-07,2.00E-05,1.01E-07
9282,question_answering1,60,"R J represent the attention weights on the query words by t-th context word , a tj = 1 for all t.",MODEL,Layer .,question_answering,1,27,1,0,,5.40E-07,0,negative,5.29E-06,2.94E-05,7.46E-06,3.33E-08,2.67E-07,2.94E-06,5.47E-07,0.000163244,0.00064226,0.999135336,3.06E-07,1.28E-05,1.17E-07
9283,question_answering1,61,The attention weight is computed by at = softmax ( S t : ),MODEL,Layer .,question_answering,1,28,1,0,,2.85E-06,0,negative,1.14E-05,0.000301576,3.50E-05,2.48E-07,1.35E-06,6.19E-05,8.31E-06,0.004624252,0.006274079,0.988656808,2.29E-06,2.05E-05,2.35E-06
9284,question_answering1,62,"? R J , and subsequently each attended query vector is ? :t = j a tj U :j .",MODEL,Layer .,question_answering,1,29,1,0,,4.64E-07,0,negative,0.000109967,2.33E-05,5.73E-05,1.54E-07,2.38E-06,3.82E-06,2.28E-06,6.49E-05,0.000123179,0.999211323,6.55E-07,0.000400412,3.67E-07
9285,question_answering1,63,Hence ?,MODEL,Layer .,question_answering,1,30,1,0,,1.32E-07,0,negative,3.18E-06,3.21E-06,9.44E-07,2.96E-08,1.14E-07,2.65E-06,4.37E-07,7.06E-05,5.31E-05,0.999849779,5.19E-07,1.53E-05,8.36E-08
9286,question_answering1,64,is a 2 d - by - T matrix containing the attended query vectors for the entire context .,MODEL,Layer .,question_answering,1,31,1,0,,5.72E-06,0,negative,1.30E-05,0.000181074,2.81E-05,2.62E-07,2.31E-06,1.13E-05,2.30E-06,0.000607993,0.002353936,0.996774137,7.24E-07,2.40E-05,8.72E-07
9287,question_answering1,65,Query - to - context Attention .,MODEL,Layer .,question_answering,1,32,1,0,,8.01E-06,0,negative,1.45E-05,8.43E-05,0.000692527,3.22E-08,5.49E-07,4.46E-06,3.28E-06,9.63E-05,0.002483487,0.996480677,3.18E-06,0.000135718,1.04E-06
9288,question_answering1,66,Query - to - context ( Q2C ) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query .,MODEL,Layer .,question_answering,1,33,1,0,,0.000769147,0,negative,6.71E-05,0.001126086,0.000209689,4.19E-05,2.78E-05,0.000109457,0.000155538,0.002765276,0.002290011,0.985317101,0.006140687,0.001533075,0.000216147
9289,question_answering1,67,We obtain the attention weights on the context words by b = softmax ( max col ( S ) ) ?,MODEL,Layer .,question_answering,1,34,1,0,,2.19E-06,0,negative,2.12E-05,9.01E-05,2.57E-05,8.36E-08,1.20E-06,1.77E-05,4.09E-06,0.000887741,0.001409037,0.997505835,2.28E-07,3.65E-05,6.53E-07
9290,question_answering1,68,"R T , where the maximum function ( max col ) is performed across the column .",MODEL,Layer .,question_answering,1,35,1,0,,9.13E-07,0,negative,6.39E-05,2.74E-05,6.75E-05,6.87E-08,1.11E-06,2.75E-06,1.61E-06,6.88E-05,0.000195504,0.999365584,2.81E-07,0.000205179,3.28E-07
9291,question_answering1,69,Then the attended context vector ish = tb t H :t ?,MODEL,Layer .,question_answering,1,36,1,0,,3.87E-07,0,negative,3.01E-05,2.58E-05,5.09E-05,1.19E-08,4.43E-07,1.42E-06,1.39E-06,3.56E-05,0.000192216,0.999407115,2.47E-07,0.000254657,1.21E-07
9292,question_answering1,70,R 2 d .,MODEL,Layer .,question_answering,1,37,1,0,,2.55E-06,0,negative,4.54E-05,1.20E-05,6.53E-06,1.42E-07,6.82E-07,8.36E-06,3.31E-06,0.000297945,0.000115136,0.999347719,3.25E-07,0.000161669,7.06E-07
9293,question_answering1,71,"This vector indicates the weighted sum of the most important words in the context with respect to the query.h is tiled T times across the column , thus giving H ?",MODEL,Layer .,question_answering,1,38,1,0,,6.96E-07,0,negative,6.64E-06,3.79E-05,1.55E-05,2.58E-08,5.21E-07,7.12E-06,2.43E-06,0.000262113,0.000829355,0.998810622,2.55E-07,2.71E-05,4.06E-07
9294,question_answering1,72,R 2 dT .,MODEL,Layer .,question_answering,1,39,1,0,,1.07E-06,0,negative,4.60E-05,2.03E-05,8.90E-06,5.25E-07,1.74E-06,1.22E-05,4.08E-06,0.000345065,0.000258911,0.999182728,5.51E-07,0.000117406,1.62E-06
9295,question_answering1,73,"Finally , the contextual embeddings and the attention vectors are combined together to yield G , where each column vector can be considered as the query - aware representation of each context word .",MODEL,Layer .,question_answering,1,40,1,0,,2.47E-05,0,negative,2.60E-05,0.000394225,4.11E-05,7.19E-08,1.61E-06,4.07E-06,1.78E-06,0.000329925,0.011352365,0.987813934,2.76E-07,3.38E-05,7.95E-07
9296,question_answering1,74,We define G by,MODEL,Layer .,question_answering,1,41,1,0,,2.80E-07,0,negative,3.26E-06,8.43E-06,3.08E-06,7.24E-08,4.67E-07,2.47E-06,5.57E-07,9.32E-05,0.000159162,0.999717655,5.83E-08,1.14E-05,1.61E-07
9297,question_answering1,75,"where G :t is the t - th column vector ( corresponding to t- th context word ) , ?",MODEL,Layer .,question_answering,1,42,1,0,,2.78E-07,0,negative,5.37E-06,6.73E-06,4.00E-06,4.02E-08,3.51E-07,2.06E-06,5.01E-07,7.80E-05,5.79E-05,0.999820203,5.32E-08,2.46E-05,1.21E-07
9298,question_answering1,76,"is a trainable vector function that fuses its ( three ) input vectors , and d G is the output dimension of the ?",MODEL,Layer .,question_answering,1,43,1,0,,3.13E-07,0,negative,1.35E-05,3.18E-05,1.52E-05,2.18E-07,2.18E-06,4.65E-06,1.09E-06,0.000199331,0.000357105,0.999350332,1.14E-07,2.41E-05,4.25E-07
9299,question_answering1,77,function .,MODEL,Layer .,question_answering,1,44,1,0,,1.28E-06,0,negative,3.16E-06,1.77E-05,5.77E-06,1.02E-07,5.31E-07,1.33E-05,1.63E-06,0.000665288,0.000672192,0.998610242,1.30E-07,9.44E-06,5.91E-07
9300,question_answering1,78,While the ?,MODEL,Layer .,question_answering,1,45,1,0,,1.47E-07,0,negative,7.41E-06,2.00E-06,7.43E-06,3.46E-09,9.94E-08,6.86E-07,3.67E-07,1.97E-05,3.62E-05,0.999895035,1.63E-08,3.10E-05,5.02E-08
9301,question_answering1,79,"function can bean arbitrary trainable neural network , such as multi - layer perceptron , a simple concatenation as following still shows good performance in our experiments :",MODEL,Layer .,question_answering,1,46,1,0,,3.65E-06,0,negative,0.00010543,4.14E-05,0.000125144,4.36E-08,1.41E-06,3.16E-06,2.92E-06,9.90E-05,0.000647643,0.998673068,1.25E-07,0.000300159,5.28E-07
9302,question_answering1,80,5 .,MODEL,Layer .,question_answering,1,47,1,0,,7.52E-07,0,negative,1.13E-05,6.07E-06,2.24E-06,1.21E-07,4.98E-07,8.00E-06,1.30E-06,0.000368355,0.000130915,0.999448704,2.27E-08,2.20E-05,4.51E-07
9303,question_answering1,81,Modeling Layer .,,,question_answering,1,0,1,0,,0.375350367,0,negative,0.000300065,0.002759637,0.000172568,3.96E-05,1.79E-05,0.000509968,0.000308298,0.008370819,0.004782367,0.956673688,0.025208779,0.000749016,0.000107247
9304,question_answering1,82,"The input to the modeling layer is G , which encodes the query - aware representations of context words .",Modeling Layer .,Modeling Layer .,question_answering,1,1,1,0,,0.000359727,0,negative,2.03E-05,0.000547039,9.07E-06,6.40E-07,1.38E-06,0.000336045,9.39E-06,0.007491526,0.009502892,0.98201981,5.43E-05,6.18E-06,1.48E-06
9305,question_answering1,83,The output of the modeling layer captures the interaction among the context words conditioned on the query .,Modeling Layer .,Modeling Layer .,question_answering,1,2,1,0,,0.002124064,0,negative,6.70E-05,0.001106089,9.44E-05,8.21E-07,2.91E-06,0.000238987,1.10E-05,0.003024335,0.071993153,0.923371603,7.70E-05,1.02E-05,2.53E-06
9306,question_answering1,84,"This is different from the contextual embedding layer , which captures the interaction among context words independent of the query .",Modeling Layer .,Modeling Layer .,question_answering,1,3,1,0,,0.00037636,0,negative,0.000335106,0.000784482,0.000482726,2.88E-07,2.36E-06,3.52E-05,5.25E-06,0.000262635,0.017617169,0.980345643,6.30E-05,6.51E-05,1.01E-06
9307,question_answering1,85,"We use two layers of bi-directional LSTM , with the output size of d for each direction .",Modeling Layer .,Modeling Layer .,question_answering,1,4,1,0,,0.024175088,0,hyperparameters,0.000347112,0.006882009,0.000243186,5.90E-05,4.64E-05,0.043481719,0.001712213,0.469478264,0.036610486,0.440608464,0.000251847,7.73E-05,0.000201947
9308,question_answering1,86,Hence we obtain a matrix M ?,Modeling Layer .,Modeling Layer .,question_answering,1,5,1,0,,4.79E-05,0,negative,8.48E-05,6.07E-05,7.27E-06,1.38E-07,9.96E-07,4.31E-05,7.54E-06,0.000272261,0.000320061,0.999042523,2.80E-05,0.000132218,3.38E-07
9309,question_answering1,87,"R 2 d T , which is passed onto the output layer to predict the answer .",Modeling Layer .,Modeling Layer .,question_answering,1,6,1,0,,7.02E-05,0,negative,0.001105991,0.000135821,0.000127607,5.55E-07,5.05E-06,4.92E-05,1.56E-05,0.000291093,0.000455605,0.997446536,2.00E-05,0.00034598,9.68E-07
9310,question_answering1,88,Each column vector of M is expected to contain contextual information about the word with respect to the entire context paragraph and the query .,Modeling Layer .,Modeling Layer .,question_answering,1,7,1,0,,8.90E-05,0,negative,3.39E-05,0.000252135,6.09E-06,1.67E-07,9.82E-07,6.61E-05,2.89E-06,0.001199519,0.003387083,0.995029482,9.40E-06,1.19E-05,3.30E-07
9311,question_answering1,89,6 . Output Layer .,Modeling Layer .,Modeling Layer .,question_answering,1,8,1,0,,0.001325242,0,negative,5.30E-05,0.000177636,4.50E-05,1.62E-06,1.84E-06,0.000962476,3.67E-05,0.007012653,0.010198781,0.981428158,5.90E-05,1.63E-05,6.90E-06
9312,question_answering1,90,The output layer is application - specific .,Modeling Layer .,Modeling Layer .,question_answering,1,9,1,0,,0.012447438,0,negative,0.000140979,0.00287674,0.000449771,1.74E-06,8.15E-06,0.000339845,2.49E-05,0.005192518,0.104438393,0.886410837,8.38E-05,2.52E-05,7.18E-06
9313,question_answering1,91,"The modular nature of BIDAF allows us to easily swap out the output layer based on the task , with the rest of the architecture remaining exactly the same .",Modeling Layer .,Modeling Layer .,question_answering,1,10,1,0,,4.23E-05,0,negative,0.00133029,0.002608831,0.000237332,4.84E-06,3.92E-05,0.000129817,2.42E-05,0.000760283,0.016100655,0.97841387,4.12E-05,0.000304723,4.76E-06
9314,question_answering1,92,"Here , we describe the output layer for the QA task .",Modeling Layer .,Modeling Layer .,question_answering,1,11,1,0,,0.000152077,0,negative,0.000129819,0.00222699,8.32E-05,1.63E-06,1.10E-05,7.20E-05,1.08E-05,0.000878285,0.02701732,0.969414162,9.19E-05,5.88E-05,4.22E-06
9315,question_answering1,93,"In section 5 , we use a slight modification of this output layer for cloze - style comprehension .",Modeling Layer .,Modeling Layer .,question_answering,1,12,1,0,,4.05E-05,0,negative,0.001331336,0.003772987,0.00113821,3.05E-06,3.48E-05,0.000111887,3.64E-05,0.00078015,0.039053723,0.953272877,8.34E-05,0.000372501,8.75E-06
9316,question_answering1,94,The QA task requires the model to find a sub-phrase of the paragraph to answer the query .,Modeling Layer .,Modeling Layer .,question_answering,1,13,1,0,,0.001932446,0,negative,5.51E-05,0.000170799,9.35E-06,0.000137016,2.86E-05,0.000319718,0.000233049,0.001299384,0.000273851,0.94195224,0.055236537,0.000181072,0.000103219
9317,question_answering1,95,The phrase is derived by predicting the start and the end indices of the phrase in the paragraph .,Modeling Layer .,Modeling Layer .,question_answering,1,14,1,0,,0.000190508,0,negative,0.000131697,0.000981658,0.00013232,2.23E-06,2.85E-05,0.000128076,1.57E-05,0.000849952,0.016945794,0.980701524,1.79E-05,6.05E-05,4.16E-06
9318,question_answering1,96,We obtain the probability distribution of the start index over the entire paragraph by,Modeling Layer .,Modeling Layer .,question_answering,1,15,1,0,,1.63E-05,0,negative,2.77E-05,5.76E-05,3.17E-06,1.14E-07,8.74E-07,2.72E-05,2.93E-06,0.000333495,0.000959622,0.998566218,4.40E-06,1.65E-05,2.60E-07
9319,question_answering1,97,where w ( p 1 ) ?,Modeling Layer .,Modeling Layer .,question_answering,1,16,1,0,,2.09E-05,0,negative,8.45E-05,3.66E-05,4.07E-06,3.16E-07,1.30E-06,8.72E-05,1.01E-05,0.000618916,0.000157918,0.998918895,5.17E-06,7.44E-05,5.03E-07
9320,question_answering1,98,R 10d is a trainable weight vector .,Modeling Layer .,Modeling Layer .,question_answering,1,17,1,0,,0.000796807,0,negative,4.56E-05,0.000432495,8.47E-06,1.32E-06,3.11E-06,0.002791047,7.27E-05,0.077508024,0.002411331,0.916681961,1.26E-05,2.39E-05,7.43E-06
9321,question_answering1,99,"For the end index of the answer phrase , we pass M to another bidirectional LSTM layer and obtain M 2 ?",Modeling Layer .,Modeling Layer .,question_answering,1,18,1,0,,4.99E-05,0,negative,0.000348154,8.04E-05,1.51E-05,2.14E-07,3.78E-06,0.000100496,3.05E-05,0.000735637,0.000287309,0.997999883,4.86E-06,0.000392745,9.52E-07
9322,question_answering1,100,R 2 d T .,Modeling Layer .,Modeling Layer .,question_answering,1,19,1,0,,0.000191859,0,negative,0.000538242,3.83E-05,8.91E-06,1.91E-06,4.92E-06,0.000122348,2.71E-05,0.000739047,0.000176573,0.99806262,7.06E-06,0.000270793,2.21E-06
9323,question_answering1,101,Then we use M 2 to obtain the probability distribution of the end index in a similar manner :,Modeling Layer .,Modeling Layer .,question_answering,1,20,1,0,,1.05E-05,0,negative,0.000107435,3.15E-05,2.43E-05,1.96E-08,4.18E-07,8.49E-06,2.19E-06,5.61E-05,0.000479683,0.999243254,7.44E-07,4.58E-05,9.26E-08
9324,question_answering1,102,Training .,,,question_answering,1,0,1,0,,0.001900875,0,negative,1.69E-05,0.000333967,1.22E-05,3.81E-07,4.77E-07,0.00060272,5.28E-05,0.022892721,0.000477568,0.974443126,0.001098141,6.63E-05,2.81E-06
9325,question_answering1,103,"We define the training loss ( to be minimized ) as the sum of the negative log probabilities of the true start and end indices by the predicted distributions , averaged over all examples :",Training .,Training .,question_answering,1,1,1,0,,0.002909975,0,negative,5.38E-05,8.42E-05,0.000110634,1.30E-06,4.35E-07,0.000356363,2.16E-05,0.085727208,3.69E-05,0.913511821,3.13E-05,5.69E-05,7.67E-06
9326,question_answering1,104,where ?,Training .,Training .,question_answering,1,2,1,0,,0.00034569,0,negative,8.64E-06,5.38E-06,1.77E-05,6.90E-07,8.10E-08,0.000170989,7.22E-06,0.013739711,8.60E-06,0.986008092,1.76E-05,1.29E-05,2.45E-06
9327,question_answering1,105,"is the set of all trainable weights in the model ( the weights and biases of CNN filters and LSTM cells , w ( S ) , w ( p 1 ) and w ( p 2 ) ) , N is the number of examples in the dataset , y 1 i and y 2 i are the true start and end indices of the i - th example , respectively , and pk indicates the k - th value of the vector p.",Training .,Training .,question_answering,1,3,1,0,,0.003533795,0,negative,1.67E-05,5.19E-05,3.23E-05,6.18E-07,3.13E-07,0.000422483,1.54E-05,0.105314622,2.46E-05,0.894065996,1.48E-05,3.59E-05,4.44E-06
9328,question_answering1,106,Test .,Training .,,question_answering,1,4,1,0,,0.004915106,0,negative,6.08E-05,5.89E-06,3.54E-05,2.25E-07,1.45E-07,0.000654794,5.66E-05,0.056932661,2.70E-06,0.941709917,5.85E-06,0.000531065,4.00E-06
9329,question_answering1,107,"The answer span ( k , l ) where k ?",Training .,Test .,question_answering,1,5,1,0,,1.02E-05,0,negative,1.91E-05,4.10E-07,6.97E-05,1.53E-08,4.79E-08,0.000141632,4.79E-06,0.000154668,7.33E-07,0.999584326,2.76E-06,2.15E-05,3.07E-07
9330,question_answering1,108,"l with the maximum value of p 1 k p 2 l is chosen , which can be computed in linear time with dynamic programming .",Training .,Test .,question_answering,1,6,1,0,,5.32E-05,0,negative,0.000299717,1.12E-05,7.92E-05,6.69E-07,3.17E-06,0.001684597,5.37E-05,0.004254148,7.41E-06,0.993434899,4.52E-06,0.000162054,4.73E-06
9331,question_answering1,109,5 .,Training .,Test .,question_answering,1,7,1,0,,7.40E-05,0,negative,0.000111667,1.55E-06,1.58E-05,3.88E-07,3.84E-07,0.001239936,1.20E-05,0.001588367,4.10E-06,0.997002743,7.46E-07,2.01E-05,2.17E-06
9332,question_answering1,110,Modeling Layer employs a Recurrent Neural Network to scan the context .,Training .,Test .,question_answering,1,8,1,0,,0.043208609,0,negative,0.002132045,0.000428268,0.042998804,8.57E-05,3.87E-05,0.015542048,0.00097772,0.024913314,0.004169544,0.906925903,0.000185525,0.000258144,0.001344352
9333,question_answering1,111,Output,Training .,,question_answering,1,9,1,0,,0.020218552,0,negative,5.39E-05,4.98E-05,0.000897353,2.53E-06,8.07E-07,0.000781155,0.00027056,0.098148077,0.000115058,0.898304432,0.000335342,0.000910804,0.00013022
9334,question_answering1,112,Layer provides an answer to the query .,Training .,Output,question_answering,1,10,1,0,,0.000328412,0,negative,6.79E-05,9.13E-05,0.000197359,3.48E-06,2.30E-06,0.000161175,7.76E-06,0.002762771,0.000814448,0.995779633,2.58E-05,6.19E-05,2.43E-05
9335,question_answering1,113,1 .,Training .,Output,question_answering,1,11,1,0,,1.28E-05,0,negative,7.22E-06,2.45E-06,4.71E-07,7.88E-08,5.61E-08,2.89E-05,3.60E-07,0.000712678,5.45E-06,0.999237003,6.68E-07,4.42E-06,1.95E-07
9336,question_answering1,114,Character Embedding Layer .,Training .,,question_answering,1,12,1,0,,0.414909636,0,negative,0.0006442,0.000180419,0.028160492,1.23E-05,4.24E-06,0.002953573,0.000764062,0.241327222,0.001153182,0.722150703,0.000136629,0.001705298,0.000807668
9337,question_answering1,115,Character embedding layer is responsible for mapping each word to a high - dimensional vector space .,Training .,Character Embedding Layer .,question_answering,1,13,1,0,,0.001017748,0,negative,0.000174032,0.001046398,0.002330845,1.32E-05,9.20E-06,0.005615697,0.000127931,0.396367967,0.009341062,0.584684882,4.04E-05,3.90E-05,0.000209419
9338,question_answering1,116,"Let {x 1 , . . . x T } and {q 1 , . . . q J } represent the words in the input context paragraph and query , respectively .",Training .,Character Embedding Layer .,question_answering,1,14,1,0,,2.19E-06,0,negative,9.48E-07,1.04E-05,3.42E-06,4.88E-08,1.01E-07,0.00014165,1.47E-06,0.011853349,3.24E-05,0.987952503,1.09E-06,2.26E-06,4.01E-07
9339,question_answering1,117,"Following , we obtain the characterlevel embedding of each word using Convolutional Neural Networks ( CNN ) .",Training .,Character Embedding Layer .,question_answering,1,15,1,0,,0.003866702,0,negative,0.000153301,0.001559169,0.013606996,1.32E-06,5.33E-06,0.001239137,4.93E-05,0.042991857,0.008831263,0.931429651,2.37E-05,6.34E-05,4.56E-05
9340,question_answering1,118,"Characters are embedded into vectors , which can be considered as 1D inputs to the CNN , and whose size is the input channel size of the CNN .",Training .,Character Embedding Layer .,question_answering,1,16,1,0,,0.001284843,0,negative,9.82E-06,0.000433198,0.000454689,5.19E-07,1.63E-06,0.003114338,4.33E-05,0.285592512,0.001985069,0.708329462,9.28E-06,1.16E-05,1.46E-05
9341,question_answering1,119,The outputs of the CNN are max - pooled over the entire width to obtain a fixed - size vector for each word .,Training .,Character Embedding Layer .,question_answering,1,17,1,0,,0.002127994,0,hyperparameters,2.86E-05,0.000509129,0.000204552,7.40E-07,1.47E-06,0.004150467,3.98E-05,0.502703908,0.001777466,0.490558182,3.21E-06,8.66E-06,1.38E-05
9342,question_answering1,120,2 . Word Embedding Layer .,Training .,Character Embedding Layer .,question_answering,1,18,1,0,,0.001736299,0,negative,0.000135148,0.000128927,0.001582483,6.11E-06,3.16E-06,0.00308445,8.36E-05,0.110239266,0.002801229,0.881741104,1.97E-05,6.05E-05,0.000114347
9343,question_answering1,121,Word embedding layer also maps each word to a high - dimensional vector space .,Training .,Character Embedding Layer .,question_answering,1,19,1,0,,0.004143115,0,negative,0.000137427,0.000468441,0.004397884,6.58E-06,6.89E-06,0.00522782,0.000171197,0.291206532,0.008194409,0.689832474,2.64E-05,6.17E-05,0.00026222
9344,question_answering1,122,"We use pre-trained word vectors , GloVe , to obtain the fixed word embedding of each word .",Training .,Character Embedding Layer .,question_answering,1,20,1,0,,0.021688762,0,hyperparameters,4.08E-06,0.000111974,1.39E-05,5.57E-07,4.44E-07,0.005321403,4.35E-05,0.939429365,0.000145277,0.054917329,5.82E-07,2.32E-06,9.30E-06
9345,question_answering1,123,The concatenation of the character and word embedding vectors is passed to a two - layer Highway Network .,Training .,Character Embedding Layer .,question_answering,1,21,1,0,,0.000399842,0,negative,0.000128876,0.00072908,0.003348344,1.57E-06,3.97E-06,0.001996184,6.37E-05,0.125761936,0.012342284,0.855519448,9.15E-06,3.77E-05,5.78E-05
9346,question_answering1,124,"The outputs of the Highway Network are two sequences of ddimensional vectors , or more conveniently , two matrices : X ? R d T for the context and Q ?",Training .,Character Embedding Layer .,question_answering,1,22,1,0,,7.71E-06,0,negative,1.42E-05,0.000114474,0.000238202,2.09E-07,1.29E-06,0.000969946,2.31E-05,0.042145592,0.000341052,0.956113273,3.38E-06,3.03E-05,4.96E-06
9347,question_answering1,125,R d J for the query .,Training .,Character Embedding Layer .,question_answering,1,23,1,0,,4.61E-06,0,negative,1.14E-05,4.46E-06,0.000111395,2.58E-08,2.48E-07,2.62E-05,3.07E-06,0.001217046,1.00E-05,0.998543961,1.15E-06,7.01E-05,9.35E-07
9348,question_answering1,126,3 .,Training .,Character Embedding Layer .,question_answering,1,24,1,0,,9.65E-06,0,negative,1.73E-05,5.73E-06,8.41E-06,1.74E-07,3.11E-07,0.000213964,4.04E-06,0.008566411,2.43E-05,0.991140099,5.15E-07,1.72E-05,1.59E-06
9349,question_answering1,127,Contextual Embedding Layer .,Training .,,question_answering,1,25,1,0,,0.142949068,0,negative,0.001256626,0.000189585,0.024148193,1.05E-05,6.80E-06,0.001567092,0.000470759,0.154974625,0.001154634,0.813331357,1.38E-05,0.001998819,0.00087726
9350,question_answering1,128,We use a Long Short - Term Memory Network ( LSTM ) on top of the embeddings provided by the previous layers to model the temporal interactions between words .,Training .,Contextual Embedding Layer .,question_answering,1,26,1,0,,0.007404763,0,negative,0.000136004,0.003496528,0.004346062,1.71E-05,3.54E-05,0.00622744,0.000272384,0.470407441,0.028446782,0.486037686,1.78E-05,3.65E-05,0.000522819
9351,question_answering1,129,"We place an LSTM in both directions , and concatenate the outputs of the two LSTMs .",Training .,Contextual Embedding Layer .,question_answering,1,27,1,0,,3.69E-05,0,negative,0.000111325,0.001547108,0.002967338,1.69E-06,9.31E-06,0.000851817,4.01E-05,0.052120425,0.015629936,0.926616476,7.87E-06,2.82E-05,6.85E-05
9352,question_answering1,130,Hence we obtain H ?,Training .,Contextual Embedding Layer .,question_answering,1,28,1,0,,3.87E-06,0,negative,7.22E-05,3.06E-05,3.41E-05,4.17E-07,2.58E-06,0.000155562,1.70E-05,0.00715149,5.99E-05,0.992273895,5.43E-06,0.000188998,7.76E-06
9353,question_answering1,131,"R 2 dT from the context word vectors X , and U ?",Training .,Contextual Embedding Layer .,question_answering,1,29,1,0,,2.96E-06,0,negative,4.22E-05,7.58E-05,7.30E-05,5.97E-07,4.20E-06,0.000212741,2.32E-05,0.01993394,5.74E-05,0.979405044,5.85E-06,0.000153184,1.29E-05
9354,question_answering1,132,"R 2 d J from query word vectors Q . Note that each column vector of H and U is 2 d - dimensional because of the concatenation of the outputs of the forward and backward LSTMs , each with d-dimensional output .",Training .,Contextual Embedding Layer .,question_answering,1,30,1,0,,2.87E-06,0,negative,0.000215857,0.000120246,0.000420655,4.49E-07,1.56E-05,7.07E-05,1.78E-05,0.002795089,7.06E-05,0.995717111,2.32E-06,0.00054614,7.51E-06
9355,question_answering1,133,"It is worth noting that the first three layers of the model are computing features from the query and context at different levels of granularity , akin to the multi-stage feature computation of convolutional neural networks in the computer vision field .",Training .,Contextual Embedding Layer .,question_answering,1,31,1,0,,3.34E-05,0,negative,0.000109403,0.000717658,0.001065631,2.00E-06,1.43E-05,0.000124234,8.54E-06,0.004898428,0.006100805,0.986893886,3.38E-06,3.92E-05,2.25E-05
9356,question_answering1,134,RELATED WORK,Training .,Contextual Embedding Layer .,question_answering,1,32,1,0,,6.23E-07,0,negative,7.40E-06,1.31E-05,1.84E-05,3.98E-07,1.52E-06,9.85E-05,1.18E-05,0.004308012,1.78E-05,0.995472914,6.99E-06,3.40E-05,9.19E-06
9357,question_answering1,135,Machine comprehension .,Training .,Contextual Embedding Layer .,question_answering,1,33,1,0,,0.000244471,0,negative,0.000347151,0.000355218,0.000632659,3.66E-05,7.11E-05,0.000983584,0.00433045,0.033089279,0.00012135,0.928965645,0.007427331,0.020283898,0.003355778
9358,question_answering1,136,A significant contributor to the advancement of MC models has been the availability of large datasets .,Training .,Contextual Embedding Layer .,question_answering,1,34,1,0,,0.000151989,0,negative,6.67E-05,0.000163652,3.88E-05,6.00E-05,3.22E-05,0.000495236,7.60E-05,0.02541067,0.000164985,0.972588191,0.000469151,0.000202123,0.000232303
9359,question_answering1,137,Early datasets such as MCTest were too small to train end - to - end neural models .,Training .,Contextual Embedding Layer .,question_answering,1,35,1,0,,2.42E-05,0,negative,1.67E-05,4.97E-05,5.19E-05,7.88E-06,1.95E-05,0.000619438,9.36E-05,0.020620847,4.05E-05,0.978134126,7.85E-05,0.000170249,9.71E-05
9360,question_answering1,138,"Massive cloze test datasets ( CNN / DailyMail by and Childrens Book Test by ) , enabled the application of deep neural architectures to this task .",Training .,Contextual Embedding Layer .,question_answering,1,36,1,0,,0.000149286,0,negative,4.49E-05,7.65E-05,0.000132673,9.66E-05,0.000369587,0.000824316,0.000208638,0.008543856,3.30E-05,0.988082808,8.08E-05,0.001001991,0.000504441
9361,question_answering1,139,"More recently , released the Stanford Question Answering ( SQuAD ) dataset with over 100,000 questions .",Training .,Contextual Embedding Layer .,question_answering,1,37,1,0,,0.000159232,0,negative,2.86E-05,4.89E-05,0.000151365,5.32E-05,0.000170522,0.000713259,0.000362233,0.010259123,2.32E-05,0.98630914,0.000232314,0.000730653,0.000917541
9362,question_answering1,140,We evaluate the performance of our comprehension system on both SQuAD and CNN / DailyMail datasets .,Training .,Contextual Embedding Layer .,question_answering,1,38,1,0,,0.000968972,0,negative,5.67E-05,0.000427019,0.000123458,2.52E-07,1.15E-05,0.000107348,4.95E-05,0.010227208,4.04E-05,0.987646477,1.20E-06,0.001298154,1.09E-05
9363,question_answering1,141,Previous works in end - to - end machine comprehension use attention mechanisms in three distinct ways .,Training .,Contextual Embedding Layer .,question_answering,1,39,1,0,,0.000225562,0,negative,0.000104473,0.000677854,0.001077177,7.55E-06,2.47E-05,0.000301436,0.000320306,0.013038204,0.000260966,0.979352893,0.003056338,0.001473489,0.000304573
9364,question_answering1,142,"The first group ( largely inspired by ) uses a dynamic attention mechanism , in which the attention weights are updated dynamically given the query and the context as well as the previous attention .",Training .,Contextual Embedding Layer .,question_answering,1,40,1,0,,0.001207293,0,negative,8.85E-05,0.000242565,0.00510153,3.49E-06,2.80E-05,0.00047046,9.45E-05,0.012538646,0.000576635,0.98057367,7.43E-06,0.000137913,0.00013658
9365,question_answering1,143,Hermann argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets .,Training .,Contextual Embedding Layer .,question_answering,1,41,1,0,,1.74E-05,0,negative,5.21E-05,1.32E-05,0.000137607,5.37E-07,3.59E-06,0.000192495,5.80E-05,0.006573263,2.74E-05,0.992325164,4.93E-06,0.00057031,4.14E-05
9366,question_answering1,144,show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy .,Training .,Contextual Embedding Layer .,question_answering,1,42,1,0,,0.007492023,0,negative,0.000666253,4.82E-05,0.00025921,4.79E-07,8.31E-06,5.89E-05,4.69E-05,0.003046037,5.16E-05,0.991793978,1.73E-06,0.003992566,2.59E-05
9367,question_answering1,145,reverse the direction of the attention ( attending on query words as the context RNN progresses ) for SQ uAD .,Training .,Contextual Embedding Layer .,question_answering,1,43,1,0,,1.78E-05,0,negative,0.000170195,0.000251571,0.000822587,1.15E-05,4.13E-05,0.000358226,8.71E-05,0.015506169,0.000312378,0.981535138,2.78E-05,0.00061577,0.000260143
9368,question_answering1,146,"In contrast to these models , BIDAF uses a memory - less attention mechanism .",Training .,Contextual Embedding Layer .,question_answering,1,44,1,0,,3.60E-05,0,negative,0.000203474,0.000612253,0.035690149,4.64E-07,1.47E-05,9.01E-05,4.61E-05,0.00259946,0.002726039,0.957705804,4.50E-06,0.000247768,5.91E-05
9369,question_answering1,147,"The second group computes the attention weights once , which are then fed into an output layer for final prediction ( e.g. , ) .",Training .,Contextual Embedding Layer .,question_answering,1,45,1,0,,0.000250219,0,negative,0.000240574,0.000403652,0.010932095,7.58E-07,2.32E-05,8.42E-05,2.63E-05,0.003324437,0.002347803,0.982473536,4.32E-07,0.000103728,3.93E-05
9370,question_answering1,148,Attention - over- attention model ) uses a 2D similarity matrix between the query and context words ( similar to Equation 1 ) to compute the weighted average of query - to - context attention .,Training .,Contextual Embedding Layer .,question_answering,1,46,1,0,,8.73E-05,0,negative,0.00011957,0.000665359,0.271499317,4.56E-07,1.40E-05,0.000168405,0.000133585,0.003386153,0.008130175,0.715456545,6.64E-06,0.0002683,0.000151536
9371,question_answering1,149,"In contrast to these models , BIDAF does not summarize the two modalities in the attention layer and instead lets the attention vectors flow into the modeling ( RNN ) layer .",Training .,Contextual Embedding Layer .,question_answering,1,47,1,0,,7.10E-05,0,negative,0.00024406,0.00131762,0.032437536,8.08E-07,2.95E-05,0.000101039,5.56E-05,0.003387201,0.002844898,0.959090712,4.07E-06,0.000402043,8.50E-05
9372,question_answering1,150,"The third group ( considered as variants of Memory Network ) repeats computing an attention vector between the query and the context through multiple layers , typically referred to as multi-hop .",Training .,Contextual Embedding Layer .,question_answering,1,48,1,0,,0.000232096,0,negative,0.00010669,0.000312884,0.012570784,2.38E-06,3.53E-05,0.000256395,8.61E-05,0.005415964,0.00100083,0.979878788,3.83E-06,0.0001702,0.000159929
9373,question_answering1,151,combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops .,Training .,Contextual Embedding Layer .,question_answering,1,49,1,0,,0.00012422,0,negative,9.07E-05,0.000197399,0.004270866,8.11E-06,2.87E-05,0.000365007,0.000223646,0.010326223,0.00089069,0.98194833,4.98E-05,0.00075086,0.000849673
9374,question_answering1,152,One can also extend our BIDAF model to incorporate multiple hops .,Training .,Contextual Embedding Layer .,question_answering,1,50,1,0,,8.00E-05,0,negative,2.05E-05,6.78E-05,0.000123764,3.95E-07,1.85E-06,0.000233803,1.75E-05,0.01447835,0.001139592,0.983846152,8.74E-07,3.73E-05,3.22E-05
9375,question_answering1,153,Visual question answering .,Training .,Contextual Embedding Layer .,question_answering,1,51,1,0,,0.000329828,0,negative,0.002079985,0.000853307,0.004439964,0.000362536,0.001822302,0.002735386,0.048034313,0.042642663,0.000121568,0.486342305,0.000636474,0.352220789,0.057708407
9376,question_answering1,154,The task of question answering has also gained a lot of interest in the computer vision community .,Training .,Contextual Embedding Layer .,question_answering,1,52,1,0,,0.000892659,0,negative,2.11E-05,8.32E-05,5.79E-05,6.15E-05,3.13E-05,0.000548554,0.000301724,0.023378246,4.92E-05,0.973322346,0.000785222,0.000402958,0.000956694
9377,question_answering1,155,"Early works on visual question answering ( VQA ) involved encoding the question using an RNN , encoding the image using a CNN and combining them to answer the question .",Training .,Contextual Embedding Layer .,question_answering,1,53,1,0,,0.005486381,0,negative,0.000124637,0.000499658,0.000620516,0.000196041,0.00016346,0.001404531,0.001643924,0.038493724,0.000173523,0.946408165,0.002827854,0.002525798,0.004918169
9378,question_answering1,156,Attention mechanisms have also been successfully employed for the VQA task and can be broadly clustered based on the granularity of their attention and the approach to construct the attention matrix .,Training .,Contextual Embedding Layer .,question_answering,1,54,1,0,,6.06E-06,0,negative,1.63E-05,4.06E-05,5.61E-05,2.34E-06,5.61E-06,0.000319291,5.76E-05,0.015792124,0.000120235,0.9833528,1.14E-05,0.000102952,0.000122639
9379,question_answering1,157,"At the coarse level of granularity , the question attends to different patches in the image .",Training .,Contextual Embedding Layer .,question_answering,1,55,1,0,,5.64E-06,0,negative,1.78E-05,8.89E-05,9.67E-05,1.51E-07,6.12E-06,1.06E-05,1.71E-06,0.000724027,0.000369894,0.998644146,1.10E-07,3.55E-05,4.36E-06
9380,question_answering1,158,"At a finer level , each question word attends to each image patch and the highest attention value for each spatial location ) is adopted .",Training .,Contextual Embedding Layer .,question_answering,1,56,1,0,,2.53E-05,0,negative,0.000227827,0.001212678,0.003935944,5.32E-07,2.81E-05,0.000122339,4.60E-05,0.011141643,0.011801466,0.971329411,1.91E-07,9.46E-05,5.94E-05
9381,question_answering1,159,"A hybrid approach is to combine questions representations at multiple levels of granularity ( unigrams , bigrams , trigrams ) .",Training .,Contextual Embedding Layer .,question_answering,1,57,1,0,,0.000817747,0,negative,2.88E-05,0.000205676,0.000563951,4.95E-06,2.40E-05,0.000337408,5.20E-05,0.014101833,0.000442069,0.983873006,3.93E-06,0.00011899,0.000243366
9382,question_answering1,160,"Several approaches to constructing the attention matrix have been used including element - wise product , element - wise sum , concatenation and Multimodal Compact Bilinear Pooling .",Training .,Contextual Embedding Layer .,question_answering,1,58,1,0,,1.70E-05,0,negative,2.06E-05,3.94E-05,4.76E-05,5.35E-06,9.28E-06,0.000450107,4.35E-05,0.020421182,6.47E-05,0.978715685,2.55E-06,8.19E-05,9.82E-05
9383,question_answering1,161,"have recently shown that in addition to attending from the question to image patches , attending from the image back to the question words provides an improvement on the VQA task .",Training .,Contextual Embedding Layer .,question_answering,1,59,1,0,,6.91E-05,0,negative,3.87E-05,4.90E-05,0.0001371,1.30E-06,1.06E-05,0.00011069,0.000232888,0.005232521,3.21E-05,0.991866025,3.46E-05,0.002027742,0.000226668
9384,question_answering1,162,"This finding in the visual domain is consistent with our finding in the language domain , where our bi-directional attention between the query and context provides improved results .",Training .,Contextual Embedding Layer .,question_answering,1,60,1,0,,0.003949456,0,negative,0.001884424,7.50E-05,6.75E-05,3.58E-06,5.51E-05,0.000789956,0.002362664,0.057828614,2.97E-05,0.815311428,6.63E-07,0.121034671,0.000556738
9385,question_answering1,163,"Their model , however , uses the attention weights directly in the output layer and does not take advantage of the attention flow to the modeling layer .",Training .,Contextual Embedding Layer .,question_answering,1,61,1,0,,1.33E-05,0,negative,1.23E-05,5.71E-05,0.000161876,2.46E-06,1.13E-05,0.000484332,5.43E-05,0.024350761,0.000164878,0.974498052,8.96E-07,6.88E-05,0.000132901
9386,question_answering1,164,QUESTION ANSWERING EXPERIMENTS,Training .,Contextual Embedding Layer .,question_answering,1,62,1,0,,0.018245617,0,negative,0.001615264,0.000752344,0.00345995,0.009203775,0.024011366,0.015575421,0.063278419,0.075995144,0.000103702,0.538792005,6.19E-05,0.121655459,0.145495225
9387,question_answering1,165,"In this section , we evaluate our model on the task of question answering using the recently released SQuAD , which has gained a huge attention over a few months .",Training .,Contextual Embedding Layer .,question_answering,1,63,1,0,,0.002911386,0,negative,0.000217058,0.006311047,0.002137041,2.48E-05,0.002016454,0.000598775,0.00076486,0.040734849,0.000390214,0.942315766,1.12E-06,0.003881032,0.000606986
9388,question_answering1,166,"In the next section , we evaluate our model on the task of cloze - style reading comprehension .",Training .,Contextual Embedding Layer .,question_answering,1,64,1,0,,6.19E-05,0,negative,4.61E-05,0.00033742,0.000157547,1.15E-06,0.000126472,8.77E-05,7.92E-05,0.00703308,6.41E-05,0.991143016,8.96E-08,0.000874836,4.94E-05
9389,question_answering1,167,Dataset .,Training .,,question_answering,1,65,1,0,,0.000542747,0,negative,1.95E-05,6.07E-07,3.18E-05,2.57E-07,1.32E-06,4.22E-05,5.72E-05,0.003608916,6.61E-07,0.994223752,1.66E-08,0.001968353,4.55E-05
9390,question_answering1,168,"SQuAD is a machine comprehension dataset on a large set of Wikipedia articles , with more than 100,000 questions .",Training .,Dataset .,question_answering,1,66,1,0,,0.004661006,0,negative,8.15E-05,4.54E-05,0.000458157,0.000185273,0.036523575,0.002272382,0.001482118,0.000672227,1.53E-06,0.952523238,1.85E-07,0.000881613,0.004872841
9391,question_answering1,169,The answer to each question is always a span in the context .,Training .,Dataset .,question_answering,1,67,1,0,,2.06E-05,0,negative,2.74E-06,1.34E-06,1.32E-05,1.39E-08,2.17E-06,1.39E-05,2.26E-06,3.87E-05,3.92E-06,0.999914506,2.11E-09,5.04E-06,2.13E-06
9392,question_answering1,170,The model is given a credit if its answer matches one of the human written answers .,Training .,Dataset .,question_answering,1,68,1,0,,0.000101338,0,negative,1.76E-05,7.75E-06,8.05E-05,1.45E-08,4.64E-06,1.91E-05,7.09E-06,0.000101024,1.72E-05,0.99971835,2.83E-09,1.89E-05,7.86E-06
9393,question_answering1,171,"Two metrics are used to evaluate models : Exact Match ( EM ) and a softer metric , F1 score , which measures the weighted average of the precision and recall rate at character level .",Training .,Dataset .,question_answering,1,69,1,0,,0.000432395,0,negative,1.87E-05,4.72E-05,0.000293374,8.52E-08,2.72E-05,6.87E-05,6.10E-05,0.000243959,8.59E-06,0.999096238,3.31E-08,0.000100233,3.48E-05
9394,question_answering1,172,The dataset consists of 90k / 10 k Model Details .,Training .,Dataset .,question_answering,1,70,1,0,,0.003472994,0,negative,1.15E-05,3.41E-06,4.98E-05,5.48E-06,0.002998849,0.000645628,7.62E-05,0.000292657,3.22E-07,0.995813841,6.78E-10,2.56E-05,7.68E-05
9395,question_answering1,173,The model architecture used for this task is depicted in .,Training .,Dataset .,question_answering,1,71,1,0,,6.02E-05,0,negative,2.07E-05,1.45E-05,0.000133734,1.27E-06,1.66E-05,0.000215144,3.75E-05,0.000605721,0.000134136,0.998531797,7.25E-08,1.92E-05,0.00026969
9396,question_answering1,174,Each paragraph and question are tokenized by a regular - expression - based word tokenizer ( PTB Tokenizer ) and fed into the model .,Training .,Dataset .,question_answering,1,72,1,1,experimental-setup,0.179737077,0,negative,0.000181006,0.000484419,0.003576469,6.26E-07,0.000151664,0.00044383,0.000288647,0.003252572,0.00095279,0.990023421,1.54E-08,5.07E-05,0.00059388
9397,question_answering1,175,"We use 100 1D filters for CNN char embedding , each with a width of 5 .",Training .,Dataset .,question_answering,1,73,1,1,experimental-setup,0.592407494,1,hyperparameters,9.96E-05,0.000171205,0.000100136,2.72E-05,0.000245939,0.100068984,0.0113641,0.580453596,5.04E-05,0.294564512,3.59E-08,6.31E-05,0.012791281
9398,question_answering1,176,The hidden state size ( d ) of the model is 100 .,Training .,Dataset .,question_answering,1,74,1,1,experimental-setup,0.555026927,1,hyperparameters,5.00E-05,6.89E-05,2.92E-05,3.27E-06,3.40E-05,0.031743952,0.004678303,0.614302343,3.30E-05,0.345431078,3.55E-08,5.10E-05,0.003574948
9399,question_answering1,177,The model has about 2.6 million parameters .,Training .,Dataset .,question_answering,1,75,1,1,experimental-setup,0.01063396,0,negative,8.00E-05,4.28E-05,6.61E-05,1.39E-05,0.000438008,0.010295038,0.001255169,0.026741622,2.26E-05,0.957245943,2.69E-08,8.99E-05,0.003708883
9400,question_answering1,178,"We use the AdaDelta ( Zeiler , 2012 ) optimizer , with a minibatch size of 60 and an initial learning rate of 0.5 , for 12 epochs .",Training .,Dataset .,question_answering,1,76,1,1,experimental-setup,0.853926025,1,hyperparameters,9.55E-05,0.000118046,0.00011489,0.000151652,0.000412901,0.200650483,0.018281951,0.5905843,2.22E-05,0.168028945,2.08E-08,7.97E-05,0.021459485
9401,question_answering1,179,"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the answers .",Training .,Dataset .,question_answering,1,77,1,1,experimental-setup,0.77411853,1,hyperparameters,0.000252526,0.0002232,8.86E-05,5.54E-05,0.000289077,0.07035931,0.010275676,0.770164793,5.63E-05,0.132927277,1.59E-08,6.76E-05,0.015240274
9402,question_answering1,180,"During training , the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999 .",Training .,Dataset .,question_answering,1,78,1,1,experimental-setup,0.150902081,0,negative,0.000141622,0.000209476,7.50E-05,1.85E-06,7.53E-05,0.011977334,0.002737908,0.245007871,0.000105666,0.737449484,1.72E-08,5.40E-05,0.002164436
9403,question_answering1,181,"At test time , the moving averages instead of the raw weights are used .",Training .,Dataset .,question_answering,1,79,1,1,experimental-setup,0.000333017,0,negative,0.000257041,2.71E-05,0.000466663,2.23E-08,1.87E-05,4.23E-05,8.77E-05,0.000212071,2.45E-05,0.998729585,8.29E-10,0.000109772,2.46E-05
9404,question_answering1,182,The training process takes roughly 20 hours on a single Titan X GPU .,Training .,Dataset .,question_answering,1,80,1,1,experimental-setup,0.118504354,0,negative,9.63E-05,3.76E-05,0.000100759,2.88E-05,0.000705494,0.041525664,0.008934434,0.048846003,9.01E-06,0.878136204,2.52E-08,0.000319943,0.021259876
9405,question_answering1,183,We also train an ensemble model consisting of 12 training runs with the identical architecture and hyper-parameters .,Training .,Dataset .,question_answering,1,81,1,0,,0.010445382,0,negative,0.000162085,0.00032581,0.002177224,2.38E-06,0.00063888,0.003040346,0.003087071,0.014301713,0.000119875,0.973113955,5.19E-09,0.000117364,0.00291329
9406,question_answering1,184,"At test time , we choose the answer with the highest sum of confidence scores amongst the 12 runs for each question .",Training .,Dataset .,question_answering,1,82,1,0,,6.20E-05,0,negative,2.27E-05,1.05E-05,3.24E-05,2.98E-08,2.22E-05,0.000102791,9.36E-05,0.000619679,1.84E-06,0.999020138,3.15E-10,4.64E-05,2.76E-05
9407,question_answering1,185,Results .,,,question_answering,1,0,1,0,,0.069344238,0,negative,0.000149093,0.000171125,3.77E-06,8.04E-06,7.37E-06,0.000132721,0.000186531,0.001647586,2.63E-05,0.994330833,0.002583223,0.000747263,6.19E-06
9408,question_answering1,186,The results of our model and competing approaches on the hidden test are summarized in .,Results .,Results .,question_answering,1,1,1,0,,0.361656433,0,negative,0.013292303,1.95E-05,0.000817382,1.41E-05,3.21E-05,0.000626703,0.019939272,0.00071131,6.29E-06,0.654797142,0.000228502,0.30916345,0.000351969
9409,question_answering1,187,"BIDAF ( ensemble ) achieves an EM score of 73.3 and an F 1 score of 81.1 , outperforming all previous approaches .",Results .,Results .,question_answering,1,2,1,1,results,0.969197967,1,results,0.007351231,1.32E-05,0.000949846,2.19E-05,2.06E-05,0.000371867,0.092012671,0.00073049,2.69E-06,0.0274579,9.89E-05,0.867998669,0.002969916
9410,question_answering1,188,Ablations .,,,question_answering,1,0,1,0,,0.015390897,0,negative,0.020295162,0.000136236,0.00138563,4.09E-05,4.00E-05,0.000150293,0.000421412,0.000266363,5.46E-05,0.971568223,0.00072722,0.004898798,1.51E-05
9411,question_answering1,189,shows the performance of our model and its ablations on the SQuAD dev set .,Ablations .,Ablations .,question_answering,1,1,1,0,,0.399358531,0,ablation-analysis,0.514398306,1.30E-06,0.000857521,3.51E-07,3.77E-06,3.28E-05,0.000111219,2.90E-06,3.69E-06,0.48422317,1.92E-05,6.75E-05,0.000278307
9412,question_answering1,190,Both char - level and word - level embeddings contribute towards the model 's performance .,Ablations .,Ablations .,question_answering,1,2,1,1,ablation-analysis,0.956225544,1,ablation-analysis,0.997634734,6.50E-07,6.32E-05,2.85E-07,6.74E-07,1.87E-06,3.65E-06,2.28E-07,6.80E-06,0.002255783,3.38E-07,3.33E-06,2.84E-05
9413,question_answering1,191,"We conjecture that word - level embedding is better at representing the semantics of each word as a whole , while char - level embedding can better handle out - of - vocab ( OOV ) or rare words .",Ablations .,Ablations .,question_answering,1,3,1,0,,0.044580299,0,negative,0.068328546,1.21E-05,0.000982016,2.06E-06,3.91E-06,0.000186791,2.72E-05,3.35E-05,0.000381669,0.929406675,7.94E-05,3.91E-06,0.000552242
9414,question_answering1,192,"To evaluate bidirectional attention , we remove C2Q and Q2C attentions .",Ablations .,Ablations .,question_answering,1,4,1,0,,0.874898102,1,ablation-analysis,0.946645418,2.66E-05,0.008580387,3.35E-07,4.20E-06,6.26E-05,0.000110699,9.90E-06,0.000208948,0.044264324,1.23E-06,4.82E-06,8.06E-05
9415,question_answering1,193,"For ablating C2Q attention , we replace the attended question vector ?",Ablations .,Ablations .,question_answering,1,5,1,0,,0.205822399,0,negative,0.226237109,1.39E-05,0.020087753,3.68E-07,3.37E-06,0.000115657,5.87E-05,1.69E-05,0.000377205,0.752955571,8.07E-06,4.93E-06,0.000120486
9416,question_answering1,194,with the average of the output vectors of the question 's contextual embedding layer ( LSTM ) .,Ablations .,Ablations .,question_answering,1,6,1,0,,0.059871759,0,negative,0.298840051,1.12E-05,0.034866859,1.57E-06,8.44E-06,0.000201128,8.83E-05,2.25E-05,0.000461887,0.66482628,1.47E-05,6.58E-06,0.000650562
9417,question_answering1,195,C2Q attention proves to be critical with a drop of more than 10 points on both metrics .,Ablations .,Ablations .,question_answering,1,7,1,1,ablation-analysis,0.933859272,1,ablation-analysis,0.999504435,3.37E-08,2.89E-06,7.10E-08,2.49E-07,3.50E-07,8.23E-06,3.07E-08,4.50E-08,0.000450041,8.76E-08,8.53E-06,2.50E-05
9418,question_answering1,196,"For ablating Q2C attention , the output of the attention layer , G , does not include terms that have the attended Q2C vectors , H .",Ablations .,Ablations .,question_answering,1,8,1,0,,0.026200121,0,negative,0.493260495,1.50E-05,0.005912323,3.49E-07,5.91E-06,2.67E-05,1.97E-05,4.11E-06,0.000236542,0.500435596,9.57E-06,1.02E-05,6.35E-05
9419,question_answering1,197,"To evaluate the attention flow , we study a dynamic attention model , where the attention is dynamically computed within the modeling layer 's LSTM , following previous work .",Ablations .,Ablations .,question_answering,1,9,1,0,,0.415063782,0,ablation-analysis,0.543102576,0.001704186,0.102817941,2.50E-06,3.44E-05,0.000214876,0.000364994,4.31E-05,0.040484212,0.310370356,0.000133817,2.55E-05,0.000701619
9420,question_answering1,198,"This is in contrast with our approach , where the attention is pre-computed before flowing to the modeling layer .",Ablations .,Ablations .,question_answering,1,10,1,0,,0.030304564,0,ablation-analysis,0.494678243,9.83E-05,0.011255642,4.28E-06,3.42E-05,6.46E-05,4.18E-05,1.09E-05,0.000620454,0.492797845,4.13E-05,1.78E-05,0.000334526
9421,question_answering1,199,"Despite being a simpler attention mechanism , our proposed static attention outperforms the dynamically computed attention by more than 3 points .",Ablations .,Ablations .,question_answering,1,11,1,1,ablation-analysis,0.942521183,1,ablation-analysis,0.997056235,4.58E-07,1.62E-05,2.37E-07,1.33E-06,2.82E-06,7.66E-05,3.97E-07,5.99E-07,0.002468153,4.21E-07,5.05E-05,0.000326124
9422,question_answering1,200,We conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer .,Ablations .,Ablations .,question_answering,1,12,1,0,,0.045801591,0,negative,0.10704337,4.07E-06,0.000386162,1.34E-06,4.42E-06,9.28E-05,2.55E-05,1.38E-05,0.00016452,0.891870328,7.31E-06,3.07E-06,0.000383252
9423,question_answering1,201,We also show the performance of BIDAF with several different definitions of ? and ?,Ablations .,Ablations .,question_answering,1,13,1,0,,0.020751283,0,negative,0.493832193,6.75E-06,0.000397826,2.57E-07,7.23E-06,2.24E-05,0.000119924,3.23E-06,9.39E-06,0.505452696,4.14E-06,6.20E-05,8.20E-05
9424,question_answering1,202,functions ( Equation and 2 ) in Appendix B. Visualizations .,Ablations .,Ablations .,question_answering,1,14,1,0,,0.013005837,0,negative,0.036211728,3.79E-06,0.000278147,3.61E-06,8.41E-06,0.000212348,4.48E-05,1.95E-05,9.95E-05,0.96245397,1.30E-05,2.67E-06,0.000648552
9425,question_answering1,203,We now provide a qualitative analysis of our model on the SQuAD dev set .,Ablations .,Ablations .,question_answering,1,15,1,0,,0.039635531,0,negative,0.1083985,1.03E-05,0.000272569,2.11E-06,2.23E-05,5.85E-05,5.37E-05,8.70E-06,3.92E-05,0.890737485,6.18E-06,1.35E-05,0.000376999
9426,question_answering1,204,"First , we visualize the feature spaces after the word and contextual embedding layers .",Ablations .,Ablations .,question_answering,1,16,1,0,,0.587463332,1,ablation-analysis,0.524453053,5.23E-05,0.003479695,1.72E-06,2.78E-05,5.63E-05,5.24E-05,9.69E-06,0.000648228,0.470991078,2.72E-06,1.71E-05,0.000207814
9427,question_answering1,205,These two layers are responsible for aligning the embeddings between the query and context words which are the inputs to the subsequent attention layer .,Ablations .,Ablations .,question_answering,1,17,1,0,,0.172474952,0,negative,0.178883376,0.000260713,0.067546973,9.11E-06,5.28E-05,0.000305424,0.00010389,6.29E-05,0.063974713,0.684132663,2.59E-05,4.94E-06,0.004636579
9428,question_answering1,206,"To visualize the embeddings , we choose a few frequent query words in the dev data and look at the context words that have the highest cosine similarity to the query words .",Ablations .,Ablations .,question_answering,1,18,1,0,,0.136786161,0,negative,0.075148822,0.000120421,0.002692078,2.19E-06,5.30E-05,0.000439652,0.000127739,0.00012968,0.000938552,0.919744361,1.37E-06,2.63E-06,0.000599463
9429,question_answering1,207,"At the word embedding layer , query words such as When , Where and Who are not well aligned to possible answers in the context , but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer .",Ablations .,Ablations .,question_answering,1,19,1,1,ablation-analysis,0.064033882,0,ablation-analysis,0.787562998,7.32E-06,0.008130384,1.64E-07,4.44E-06,1.01E-05,2.62E-05,1.08E-06,0.000253691,0.203887186,2.08E-06,1.25E-05,0.000101903
9430,question_answering1,208,"When begins to match years ,",Ablations .,Ablations .,question_answering,1,20,1,0,,0.001690497,0,negative,0.089014823,1.23E-06,0.001706533,1.34E-07,3.05E-06,1.98E-05,1.92E-05,1.82E-06,3.63E-05,0.909097896,1.30E-06,5.63E-06,9.22E-05
9431,question_answering1,209,"Where matches locations , and Who matches names .",Ablations .,Ablations .,question_answering,1,21,1,0,,0.005659985,0,negative,0.013719846,1.24E-06,0.000778866,1.69E-06,3.51E-05,7.15E-05,1.08E-05,2.79E-06,2.09E-05,0.985103787,1.01E-06,1.74E-06,0.000250804
9432,question_answering1,210,We also visualize these two feature spaces using t- SNE in .,Ablations .,Ablations .,question_answering,1,22,1,0,,0.060683271,0,negative,0.235297738,2.36E-05,0.015282865,1.97E-06,4.97E-05,9.27E-05,6.24E-05,7.81E-06,0.000410332,0.748343247,1.90E-06,1.13E-05,0.000414337
9433,question_answering1,211,t- SNE is performed on a large fraction of dev data but we only plot data points corresponding to the months of the year .,Ablations .,Ablations .,question_answering,1,23,1,0,,0.017005194,0,negative,0.092606135,3.05E-06,0.000887081,1.77E-05,0.001060338,0.000132583,6.02E-05,1.96E-06,3.67E-06,0.904084817,2.85E-06,1.02E-05,0.001129394
9434,question_answering1,212,"An interesting pattern emerges in the Word space , where May is separated from the rest of the months because May has multiple meanings in the English language .",Ablations .,Ablations .,question_answering,1,24,1,0,,0.244394159,0,ablation-analysis,0.740698116,2.91E-06,0.000424186,1.06E-07,3.03E-06,1.31E-05,0.00010339,1.65E-06,1.42E-05,0.258518312,1.44E-06,4.14E-05,0.000178207
9435,question_answering1,213,The contextual embedding layer uses contextual cues from surrounding words and is able to separate the usages of the word May .,Ablations .,Ablations .,question_answering,1,25,1,0,,0.689821287,1,ablation-analysis,0.365606862,0.000227143,0.236118431,5.76E-06,4.32E-05,0.000266753,0.00024441,4.96E-05,0.068845486,0.319816493,1.57E-05,9.26E-06,0.008750862
9436,question_answering1,214,Finally we visualize the attention matrices for some question - context tuples in the dev data in .,Ablations .,Ablations .,question_answering,1,26,1,0,,0.040883843,0,negative,0.169690167,3.25E-06,0.000425183,5.39E-07,2.19E-05,3.64E-05,4.50E-05,5.05E-06,1.74E-05,0.829594019,4.22E-07,1.13E-05,0.000149508
9437,question_answering1,215,"In the first example , Where matches locations and in the second example , many matches quantities and numerical symbols .",Ablations .,Ablations .,question_answering,1,27,1,0,,0.002612876,0,negative,0.006327186,8.75E-07,0.000196633,8.02E-07,8.95E-05,3.26E-05,8.51E-06,1.36E-06,4.70E-06,0.993200223,3.40E-07,1.67E-06,0.000135578
9438,question_answering1,216,"Also , entities in the question typically attend to the same entities in the context , thus providing a feature for the model to localize possible answers .",Ablations .,Ablations .,question_answering,1,28,1,0,,0.004850212,0,negative,0.049350721,5.72E-06,0.001175899,1.52E-06,2.62E-05,3.01E-05,6.69E-06,2.85E-06,0.000223424,0.949033722,5.76E-07,2.20E-06,0.00014035
9439,question_answering1,217,Discussions .,Ablations .,,question_answering,1,29,1,0,,0.005539919,0,negative,0.135131633,1.63E-06,0.000274644,1.51E-07,3.00E-06,3.45E-05,6.52E-05,3.62E-06,1.34E-05,0.864219538,9.97E-07,8.73E-06,0.000243019
9440,question_answering1,218,We analyse the performance of our our model with a traditional language - featurebased baseline .,Ablations .,Discussions .,question_answering,1,30,1,0,,0.005236088,0,negative,0.001732492,0.000170721,0.000256822,3.39E-07,0.000153737,9.11E-05,6.38E-05,2.04E-05,5.03E-05,0.99743358,2.21E-06,2.22E-05,2.33E-06
9441,question_answering1,219,shows a Venn diagram of the dev set questions correctly answered by the models .,Ablations .,Discussions .,question_answering,1,31,1,0,,2.33E-05,0,negative,0.000663158,2.42E-05,0.000145987,1.40E-06,5.89E-05,0.000253547,3.62E-05,3.23E-05,0.000141712,0.998625577,4.96E-06,3.64E-06,8.46E-06
9442,question_answering1,220,Our model is able to answer more than 86 % of the questions correctly answered by the baseline .,Ablations .,Discussions .,question_answering,1,32,1,0,,0.253959192,0,negative,0.290505511,0.000306644,0.000877871,1.61E-05,0.001889015,0.000505136,0.013559468,0.000110282,6.13E-05,0.678728062,5.65E-05,0.012370164,0.001014035
9443,question_answering1,221,The 14 % thatare incorrectly answered does not have a clear pattern .,Ablations .,Discussions .,question_answering,1,33,1,0,,4.59E-05,0,negative,0.007881628,4.18E-06,8.93E-05,1.40E-06,0.000712829,6.90E-05,2.66E-05,2.98E-06,3.45E-06,0.991188891,3.66E-07,1.73E-05,1.98E-06
9444,question_answering1,222,This suggests that neural architectures are able to exploit much of the information captured by the language features .,Ablations .,Discussions .,question_answering,1,34,1,0,,4.46E-05,0,negative,0.001234361,1.87E-05,6.29E-05,2.05E-07,1.44E-05,3.50E-05,6.08E-06,8.23E-06,0.000118184,0.998497642,1.27E-06,1.77E-06,1.24E-06
9445,question_answering1,223,We also break this comparison down by the first words in the questions ) .,Ablations .,Discussions .,question_answering,1,35,1,0,,6.59E-06,0,negative,0.000231685,7.37E-06,2.69E-05,1.71E-06,0.00021718,0.000115314,6.35E-06,8.33E-06,1.24E-05,0.99937081,2.18E-07,7.65E-07,9.79E-07
9446,question_answering1,224,Our model outperforms the traditional baseline comfortably in every category .,Ablations .,Discussions .,question_answering,1,36,1,0,,0.502481083,1,ablation-analysis,0.634829669,0.000169124,0.000502914,4.18E-05,0.000941076,0.001018387,0.028651896,0.000254409,2.92E-05,0.316976207,2.22E-05,0.014250992,0.002312181
9447,question_answering1,225,Error Analysis .,Ablations .,,question_answering,1,37,1,0,,0.003205522,0,negative,0.03162637,3.81E-07,5.45E-05,5.87E-07,1.02E-05,4.05E-05,2.01E-05,2.77E-06,2.90E-06,0.967927507,1.28E-07,2.18E-06,0.000311812
9448,question_answering1,226,We randomly select 50 incorrect questions ( based on EM ) and categorize them into 6 classes .,Ablations .,Error Analysis .,question_answering,1,38,1,0,,0.001642781,0,negative,0.000126826,2.70E-05,6.00E-06,7.66E-07,0.000651898,0.000446465,8.67E-05,5.89E-05,7.33E-06,0.998583638,5.98E-08,1.41E-06,3.02E-06
9449,question_answering1,227,"50 % of errors are due to the imprecise boundaries of the answers , 28 % involve syntactic complications and ambiguities , 14 % are paraphrase problems , 4 % require external knowledge , 2 % need multiple sentences to answer , and 2 % are due to mistakes during tokenization .",Ablations .,Error Analysis .,question_answering,1,39,1,0,,0.010822955,0,negative,0.000700969,1.79E-06,4.60E-06,1.39E-06,0.00073117,7.82E-05,2.44E-05,1.56E-06,5.95E-07,0.998448549,6.83E-08,4.73E-06,1.97E-06
9450,question_answering1,228,See Appendix,Ablations .,,question_answering,1,40,1,0,,0.000639739,0,negative,0.013838542,1.71E-06,7.09E-05,7.93E-06,4.07E-05,0.000117411,3.51E-05,1.84E-05,1.41E-05,0.984056528,3.09E-07,1.53E-06,0.001796786
9451,question_answering1,229,A for the examples of the error modes .,Ablations .,See Appendix,question_answering,1,41,1,0,,2.67E-05,0,negative,6.25E-05,1.66E-05,2.00E-05,4.23E-07,3.76E-05,6.00E-05,1.77E-05,4.18E-05,4.26E-05,0.999697271,7.38E-07,1.64E-06,1.12E-06
9452,question_answering1,230,CLOZE TEST EXPERIMENTS,Ablations .,See Appendix,question_answering,1,42,1,0,,0.003322122,0,negative,0.000936433,0.000150491,0.000429808,7.47E-05,0.004289265,0.002599593,0.002930054,0.000468267,0.000105415,0.987621103,2.73E-05,0.000116118,0.000251428
9453,question_answering1,231,We also evaluate our model on the task of cloze - style reading comprehension using the CNN and Daily Mail datasets .,Ablations .,See Appendix,question_answering,1,43,1,0,,0.005137092,0,negative,0.001242064,0.011174485,0.003568567,8.96E-06,0.017689872,0.000503621,0.003091823,0.000315912,0.000445242,0.961651499,1.05E-05,0.000229171,6.83E-05
9454,question_answering1,232,Dataset .,Ablations .,,question_answering,1,44,1,0,,0.000512549,0,negative,0.008761513,1.56E-07,0.000107441,2.40E-07,6.50E-06,3.24E-05,2.94E-05,1.81E-06,1.47E-06,0.99020664,9.86E-08,3.12E-06,0.000849193
9455,question_answering1,233,"In a cloze test , the reader is asked to fill in words that have been removed from a passage , for measuring one 's ability to comprehend text .",Ablations .,Dataset .,question_answering,1,45,1,0,,6.69E-05,0,negative,5.92E-05,4.39E-05,3.52E-05,3.62E-05,0.002653159,0.000370494,7.02E-05,2.26E-05,7.93E-06,0.996557649,0.000104514,4.72E-06,3.42E-05
9456,question_answering1,234,"Hermann et al. have recently compiled a massive Cloze - style comprehension dataset , consisting of 300 k / 4 k / 3 k and 879k / 65 k / 53 k ( train / dev / test ) examples from CNN and DailyMail news articles , respectively .",Ablations .,Dataset .,question_answering,1,46,1,0,,0.000230255,0,negative,6.35E-05,3.57E-05,0.000141668,7.23E-05,0.025937895,0.000669651,0.000126733,1.56E-05,4.16E-06,0.972860636,1.32E-05,5.74E-06,5.32E-05
9457,question_answering1,235,Each example has a news article and an incomplete sentence extracted from the human - written summary of the article .,Ablations .,Dataset .,question_answering,1,47,1,0,,1.59E-05,0,negative,3.24E-05,3.29E-05,4.40E-05,2.16E-06,0.01343862,0.000112597,1.68E-05,7.63E-06,6.16E-06,0.986302058,6.77E-08,2.89E-06,1.71E-06
9458,question_answering1,236,"To distinguish this task from language modeling and force one to refer to the article to predict the correct missing word , the missing word is always a named entity , anonymized with a random ID .",Ablations .,Dataset .,question_answering,1,48,1,0,,6.47E-06,0,negative,9.92E-06,1.27E-05,1.56E-05,2.81E-05,0.003040911,0.000198266,1.02E-05,7.96E-06,3.44E-06,0.99666059,5.63E-06,8.47E-07,5.86E-06
9459,question_answering1,237,"Also , the IDs must be shuffled constantly during test , which is also critical for full anonymization .",Ablations .,Dataset .,question_answering,1,49,1,0,,0.001602139,0,negative,0.000926819,0.000102556,2.11E-05,3.37E-06,0.000334958,0.000181342,3.90E-05,4.02E-05,6.39E-05,0.9982652,2.88E-06,1.05E-05,8.21E-06
9460,question_answering1,238,Model Details .,,,question_answering,1,0,1,0,,0.00146841,0,negative,1.25E-05,0.0002325,8.32E-06,1.42E-06,1.60E-06,0.000118387,3.78E-05,0.002512224,9.21E-05,0.996364984,0.000513778,0.000102618,1.70E-06
9461,question_answering1,239,The model architecture used for this task is very similar to that for SQuAD ( Section 4 ) with only a few small changes to adapt it to the cloze test .,Model Details .,Model Details .,question_answering,1,1,1,0,,0.009879064,0,negative,0.000786513,0.000118448,0.082705305,7.44E-05,3.17E-05,0.267095368,0.023670622,0.085469841,0.00015857,0.538101465,0.000121304,0.001318914,0.000347537
9462,question_answering1,240,"Since each answer in the CNN / DailyMail datasets is always a single word ( entity ) , we only need to predict the start index ( p 1 ) ; the prediction for the end index ( p 2 ) is omitted from the loss function .",Model Details .,Model Details .,question_answering,1,2,1,0,,0.021255413,0,negative,0.007599319,0.000271262,0.039639244,5.27E-06,8.15E-06,0.013938024,0.000957171,0.038265988,0.000225622,0.894622891,3.75E-05,0.004373726,5.58E-05
9463,question_answering1,241,"Also , we mask out all non-entity words in the final classification layer so that they are forced to be excluded from possible answers .",Model Details .,Model Details .,question_answering,1,3,1,0,,0.543466213,1,negative,0.005659767,0.000527375,0.098365552,8.31E-06,1.13E-05,0.045106119,0.001351789,0.124437706,0.001361785,0.722047206,1.73E-05,0.001016722,8.91E-05
9464,question_answering1,242,Another important difference from SQuAD is that the answer entity might appear more than once in the context paragraph .,Model Details .,Model Details .,question_answering,1,4,1,0,,0.003557,0,negative,0.002253438,6.06E-05,0.02925931,1.11E-05,1.22E-05,0.009153337,0.001209707,0.005399368,3.91E-05,0.949674378,7.37E-05,0.002804576,4.92E-05
9465,question_answering1,243,"To address this , we follow a similar strategy from .",Model Details .,Model Details .,question_answering,1,5,1,0,,0.004333755,0,negative,0.005260062,8.13E-05,0.091114507,2.82E-05,1.71E-05,0.010848756,0.001296732,0.006369925,0.000153828,0.883440212,1.85E-05,0.001291404,7.95E-05
9466,question_answering1,244,"During training , after we obtain p 1 , we sum all probability values of the entity instances in the context that correspond to the correct answer .",Model Details .,Model Details .,question_answering,1,6,1,0,,0.01794246,0,negative,0.00118541,6.14E-05,0.022505613,8.04E-07,1.83E-06,0.013107093,0.000558738,0.035452881,0.000123951,0.925756596,5.16E-06,0.001221428,1.91E-05
9467,question_answering1,245,Then the loss function is computed from the summed probability .,Model Details .,Model Details .,question_answering,1,7,1,0,,0.046069341,0,negative,0.000211673,2.30E-05,0.006827272,1.07E-06,1.07E-06,0.017527014,0.000283345,0.045929258,0.000219902,0.92884255,5.38E-06,0.000109417,1.91E-05
9468,question_answering1,246,"We use a minibatch size of 48 and train for 8 epochs , with early stop when the accuracy on validation data starts to drop .",Model Details .,Model Details .,question_answering,1,8,1,0,,0.96461316,1,hyperparameters,3.01E-05,1.22E-05,7.13E-05,5.50E-06,5.87E-07,0.346981768,0.003792653,0.640218868,9.45E-06,0.008774036,3.02E-06,3.41E-05,6.65E-05
9469,question_answering1,247,"Inspired by the window - based method , we split each article into short sentences where each sentence is a 19 - word window around each entity ( hence the same word might appear in multiple sentences ) .",Model Details .,Model Details .,question_answering,1,9,1,0,,0.151927521,0,negative,0.004150372,0.000839922,0.229225363,1.74E-05,7.70E-05,0.027047052,0.005564382,0.042428059,0.000312623,0.683424671,2.29E-05,0.006675544,0.000214756
9470,question_answering1,248,"The RNNs in BIDAF are not feed - forwarded or back - propagated across sentences , which speedup the training process by parallelization .",Model Details .,Model Details .,question_answering,1,10,1,0,,0.435796064,0,negative,0.003949838,0.000273372,0.187997389,1.86E-05,3.69E-05,0.022519454,0.003878311,0.019217859,0.000160543,0.756622779,4.52E-05,0.00508671,0.000193033
9471,question_answering1,249,The entire training process takes roughly 60 hours on eight Titan X GPUs .,Model Details .,Model Details .,question_answering,1,11,1,0,,0.830059493,1,experimental-setup,0.000129026,2.99E-05,0.000393428,8.31E-05,1.24E-05,0.605509546,0.019351055,0.296053709,2.80E-05,0.077338366,1.98E-05,0.000528295,0.00052342
9472,question_answering1,250,The other hyper - parameters are identical to the model described in Section 4 .,Model Details .,Model Details .,question_answering,1,12,1,0,,0.059786881,0,hyperparameters,0.000188345,2.01E-05,0.000573619,2.22E-06,1.23E-06,0.217183955,0.001534028,0.538262858,4.31E-05,0.242028021,1.44E-06,0.00013237,2.87E-05
9473,question_answering1,251,Results .,,,question_answering,1,0,1,0,,0.069344238,0,negative,0.000149093,0.000171125,3.77E-06,8.04E-06,7.37E-06,0.000132721,0.000186531,0.001647586,2.63E-05,0.994330833,0.002583223,0.000747263,6.19E-06
9474,question_answering1,252,The results of our single - run models and competing approaches on the CNN / DailyMail datasets are summarized in 61.6 63.0 70.5 69.0 MemNN 63.4 6.8 -- AS Reader 68.6 69.5 75.0 73.9 DER Network 71.3 72.9,Results .,Results .,question_answering,1,1,1,0,,0.007448692,0,negative,0.012658505,2.50E-05,0.001109618,9.34E-06,2.60E-05,0.000633168,0.032019272,0.001048959,5.50E-06,0.488518725,0.000127023,0.463474419,0.000344412
9475,question_answering1,253,-- Iterative Attention 72.6 73.3 -- EpiReader 73.4 74.0 -- Stanford AR 73.8 73.6 77.6 76.6 GAReader 73.0 73.8 76.7 75.7 A o A Reader 73.1 74.4 -- ReasoNet 72.9 74.7 77.6 76.6 BIDAF ( Ours ) 76.3 76.9 80.3 79.6 MemNN * 66.2 69.4 -- ASReader * 73.9 75.4 78.7 77.7 Iterative Attention *,Results .,Results .,question_answering,1,2,1,0,,0.002480625,0,negative,0.010716993,8.72E-05,0.008019836,0.000352621,0.000408139,0.012022474,0.072301853,0.005581819,9.19E-05,0.844844928,0.000921758,0.042221868,0.002428664
9476,question_answering1,254,CONCLUSION,,,question_answering,1,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
9477,sentiment_analysis18,1,title,,,sentiment_analysis,18,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
9478,sentiment_analysis18,2,Effective Attention Modeling for Aspect - Level Sentiment Classification,title,,sentiment_analysis,18,1,1,1,research-problem,0.998206655,1,research-problem,5.92E-08,1.53E-05,8.64E-08,2.25E-07,1.18E-07,1.40E-07,1.55E-06,2.15E-06,9.47E-07,0.002119403,0.997859758,2.10E-07,1.00E-07
9479,sentiment_analysis18,3,abstract,,,sentiment_analysis,18,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
9480,sentiment_analysis18,4,Aspect - level sentiment classification aims to determine the sentiment polarity of a review sentence towards an opinion target .,abstract,abstract,sentiment_analysis,18,1,1,0,,0.676813644,1,research-problem,2.03E-08,5.21E-06,2.00E-08,8.22E-07,2.76E-07,1.75E-07,4.15E-07,8.19E-07,2.95E-07,0.005459484,0.994532392,1.84E-08,5.57E-08
9481,sentiment_analysis18,5,A sentence could contain multiple sentiment - target pairs ; thus the main challenge of this task is to separate different opinion contexts for different targets .,abstract,abstract,sentiment_analysis,18,2,1,0,,0.211036633,0,research-problem,3.80E-08,4.21E-05,2.64E-08,4.06E-06,2.53E-06,7.79E-07,4.67E-07,4.75E-06,1.97E-06,0.057704084,0.942239062,3.13E-08,8.27E-08
9482,sentiment_analysis18,6,"To this end , attention mechanism has played an important role in previous state - of - the - art neural models .",abstract,abstract,sentiment_analysis,18,3,1,0,,0.040655086,0,research-problem,1.37E-07,6.58E-05,5.40E-08,2.10E-06,8.22E-07,2.68E-06,6.47E-07,2.10E-05,1.31E-05,0.097937577,0.901955922,4.76E-08,9.50E-08
9483,sentiment_analysis18,7,The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations .,abstract,abstract,sentiment_analysis,18,4,1,0,,0.102195017,0,negative,1.18E-05,0.071599445,0.000143314,2.87E-06,5.54E-05,2.73E-05,3.26E-06,0.000304232,0.283164216,0.58318554,0.061500242,1.63E-06,7.81E-07
9484,sentiment_analysis18,8,We build upon this line of research and propose two novel approaches for improving the effectiveness of attention .,abstract,abstract,sentiment_analysis,18,5,1,0,,0.010984492,0,negative,4.40E-05,0.25945449,5.45E-05,9.68E-05,0.001361775,6.08E-05,1.56E-05,0.000545635,0.010618098,0.607749807,0.119985984,9.32E-06,3.24E-06
9485,sentiment_analysis18,9,"First , we propose a method for target representation that better captures the semantic meaning of the opinion target .",abstract,abstract,sentiment_analysis,18,6,1,0,,0.201529511,0,negative,0.000117852,0.411043421,7.53E-05,0.000172395,0.002167897,7.83E-05,2.40E-05,0.000901011,0.011148043,0.494083649,0.08016393,1.86E-05,5.54E-06
9486,sentiment_analysis18,10,"Second , we introduce an attention model that incorporates syntactic information into the attention mechanism .",abstract,abstract,sentiment_analysis,18,7,1,0,,0.148812622,0,approach,0.000208872,0.407002648,0.000548799,4.96E-05,0.001116945,0.000106161,2.91E-05,0.000776398,0.186155022,0.370400638,0.033578231,2.12E-05,6.32E-06
9487,sentiment_analysis18,11,"We experiment on attention - based LSTM ( Long Short - Term Memory ) models using the datasets from SemEval 2014 , 2015 , and 2016 .",abstract,abstract,sentiment_analysis,18,8,1,0,,0.077842567,0,negative,1.94E-05,0.382033993,0.000185018,0.000185631,0.007622774,0.000557204,0.000158308,0.005243651,0.00407208,0.558191284,0.041702226,1.82E-05,1.02E-05
9488,sentiment_analysis18,12,The experimental results show that the conventional attention - based LSTM can be substantially improved by incorporating the two approaches .,abstract,abstract,sentiment_analysis,18,9,1,0,,0.019944152,0,negative,0.000587206,0.009609022,8.90E-06,1.57E-05,6.78E-05,4.50E-05,7.72E-05,0.001135388,0.000645723,0.818851873,0.168399119,0.000551804,5.22E-06
9489,sentiment_analysis18,13,This work is licenced under a Creative Commons Attribution 4.0 International Licence .,abstract,abstract,sentiment_analysis,18,10,1,0,,0.001911585,0,negative,2.85E-06,0.003499613,1.79E-06,0.00126172,0.000284426,0.000254349,6.46E-06,0.000991885,0.000902129,0.974003749,0.018788751,3.96E-07,1.89E-06
9490,sentiment_analysis18,14,Licence details : http:// creativecommons.org/licenses/by/4.0/,abstract,abstract,sentiment_analysis,18,11,1,0,,0.001198256,0,negative,9.34E-06,0.001798725,2.71E-06,0.033694718,0.001441599,0.000599538,1.07E-05,0.000549655,0.000415877,0.952043366,0.009429894,3.74E-07,3.47E-06
9491,sentiment_analysis18,15,Introduction,,,sentiment_analysis,18,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
9492,sentiment_analysis18,16,Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .,Introduction,Introduction,sentiment_analysis,18,1,1,1,research-problem,0.887101171,1,research-problem,1.60E-06,0.000275906,6.93E-07,1.23E-05,3.26E-05,4.32E-06,1.86E-05,5.39E-06,4.97E-05,0.023439793,0.976154348,1.91E-06,2.81E-06
9493,sentiment_analysis18,17,"Given a sentence and an opinion target ( also called aspect expression ) occurring in the sentence , the task aims to determine the sentiment polarity of the sentence towards the opinion target .",Introduction,Introduction,sentiment_analysis,18,2,1,0,,0.75230716,1,research-problem,1.65E-06,0.002481321,1.16E-06,2.00E-05,9.67E-05,1.32E-05,1.25E-05,2.67E-05,0.000735109,0.100967134,0.895639199,2.05E-06,3.28E-06
9494,sentiment_analysis18,18,An opinion target or target for short refers to a word or a phrase ( a sequence of words ) describing an aspect of an entity .,Introduction,Introduction,sentiment_analysis,18,3,1,0,,0.035140905,0,negative,1.00E-05,0.014772517,1.98E-05,0.000283685,0.011007314,0.00031588,9.57E-05,0.000220499,0.007168137,0.822192975,0.143884358,9.76E-06,1.93E-05
9495,sentiment_analysis18,19,"For example , in the sentence "" This little place has acute interior decor and affordable prices "" , the targets are interior decor and prices , and they belong to the aspects ambience and price respectively .",Introduction,Introduction,sentiment_analysis,18,4,1,0,,0.002241875,0,negative,5.30E-06,0.000624035,1.29E-06,0.000125715,0.002020377,0.000156265,1.84E-05,4.54E-05,0.000403559,0.986821727,0.00977384,2.04E-06,2.04E-06
9496,sentiment_analysis18,20,"Compared to document - level or sentence - level sentiment classification , the main challenge of aspectlevel sentiment classification is to differentiate sentiments towards different targets when there are multiple targets in a sentence .",Introduction,Introduction,sentiment_analysis,18,5,1,0,,0.751124638,1,research-problem,1.29E-06,0.000562941,5.34E-07,5.99E-06,2.25E-05,4.53E-06,1.28E-05,9.03E-06,8.82E-05,0.044635403,0.954652855,2.06E-06,1.82E-06
9497,sentiment_analysis18,21,"For instance , the sentence "" The appetizers are ok , but the service is slow . "" expresses a neutral sentiment on the target appetizers and a negative sentiment on the target service .",Introduction,Introduction,sentiment_analysis,18,6,1,0,,0.002136492,0,negative,7.46E-06,0.002844571,3.53E-06,0.000106553,0.002533265,0.000315483,4.53E-05,0.000166068,0.002688318,0.974507816,0.016771307,4.66E-06,5.66E-06
9498,sentiment_analysis18,22,"To this end , attention mechanism has played an important role in state - of - the - art neural models for this task .",Introduction,Introduction,sentiment_analysis,18,7,1,0,,0.196305859,0,research-problem,3.96E-06,0.001586146,1.22E-06,1.23E-05,2.62E-05,4.38E-05,2.20E-05,7.61E-05,0.001861414,0.243320681,0.753040173,2.72E-06,3.17E-06
9499,sentiment_analysis18,23,"It assigns a positive weight att i for each context word w i , which can be interpreted as the probability that w i is the right word to focus on when inferring the sentiment polarity of the given target .",Introduction,Introduction,sentiment_analysis,18,8,1,0,,0.437120861,0,model,3.00E-05,0.096002871,6.54E-05,2.53E-06,0.00018615,4.74E-05,1.23E-05,0.000137165,0.867884187,0.034544417,0.001082548,3.56E-06,1.49E-06
9500,sentiment_analysis18,24,The weight att i is generally computed as a function of the hidden representation hi of w i and the target representation t as follows :,Introduction,Introduction,sentiment_analysis,18,9,1,0,,0.240730525,0,model,4.28E-05,0.061003836,4.68E-05,5.07E-06,0.00024166,0.000166085,2.45E-05,0.000386322,0.475933349,0.456941971,0.005197482,7.85E-06,2.33E-06
9501,sentiment_analysis18,25,It has been shown that adding an attention model substantially improves the accuracy of aspect - level sentiment classification .,Introduction,Introduction,sentiment_analysis,18,10,1,0,,0.679151613,1,research-problem,2.31E-05,0.003429116,3.80E-06,4.62E-06,3.76E-05,1.25E-05,7.01E-05,3.72E-05,0.000809296,0.214251561,0.781282571,3.41E-05,4.39E-06
9502,sentiment_analysis18,26,Our work builds upon this line of research .,Introduction,Introduction,sentiment_analysis,18,11,1,0,,0.02314406,0,negative,5.75E-05,0.191743072,7.02E-05,0.000699607,0.025516738,0.000963302,0.000153203,0.000767531,0.110160564,0.659077581,0.010744099,2.12E-05,2.53E-05
9503,sentiment_analysis18,27,We propose two novel approaches for improving the effectiveness of attention models .,Introduction,Introduction,sentiment_analysis,18,12,1,1,approach,0.858386095,1,approach,0.000254361,0.628734305,0.000315339,7.88E-05,0.003909737,0.000123729,0.000177206,0.000229379,0.197942693,0.120415442,0.04771616,8.23E-05,2.05E-05
9504,sentiment_analysis18,28,The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .,Introduction,Introduction,sentiment_analysis,18,13,1,1,approach,0.651533725,1,negative,0.000257924,0.265635334,0.00053933,0.000182908,0.008919026,0.000295323,0.000204883,0.000228978,0.086588298,0.575322144,0.061737021,6.78E-05,2.10E-05
9505,sentiment_analysis18,29,The target representation is crucial since attention weights are computed based on it as shown in Eq.,Introduction,Introduction,sentiment_analysis,18,14,1,0,,0.428196234,0,model,0.000689762,0.177968061,0.000255506,4.91E-06,0.001131099,7.29E-05,0.000110304,0.000117383,0.487246313,0.328600796,0.003700354,9.92E-05,3.44E-06
9506,sentiment_analysis18,30,1 .,Introduction,Introduction,sentiment_analysis,18,15,1,0,,0.002256325,0,negative,8.61E-06,0.002772063,1.80E-06,9.20E-06,0.00011476,0.000184353,1.69E-05,0.000219919,0.011974071,0.982775216,0.001919525,2.39E-06,1.16E-06
9507,sentiment_analysis18,31,"In representing the target , we are mapping a word or a phrase into a vector in Rd .",Introduction,Introduction,sentiment_analysis,18,16,1,0,,0.301163752,0,model,3.81E-06,0.087715238,1.25E-05,2.28E-06,0.000141782,9.29E-05,1.85E-05,0.000361567,0.779496772,0.128565402,0.003584898,2.22E-06,2.18E-06
9508,sentiment_analysis18,32,"Ideally , targets that are semantically similar should be mapped to vectors that are close together in Rd .",Introduction,Introduction,sentiment_analysis,18,17,1,0,,0.620070028,1,model,7.97E-05,0.185402777,7.00E-05,2.30E-06,0.000498323,5.81E-05,4.01E-05,0.000158951,0.473450364,0.33716242,0.003046286,2.87E-05,2.06E-06
9509,sentiment_analysis18,33,"However , previous neural attention models simply map a target by averaging its component word vectors .",Introduction,Introduction,sentiment_analysis,18,18,1,0,,0.062373397,0,negative,1.47E-05,0.012458472,1.60E-05,3.15E-05,0.000222376,0.000183115,0.000104183,0.000209063,0.011781237,0.681625362,0.293330513,1.30E-05,1.04E-05
9510,sentiment_analysis18,34,"This may work fine for targets that only contain one word but may fail to capture the semantics of more complex expressions , as also mentioned by .",Introduction,Introduction,sentiment_analysis,18,19,1,0,,0.008353829,0,negative,3.57E-05,0.001772255,2.41E-06,1.46E-05,0.000181824,0.000134757,2.95E-05,0.000109884,0.001986125,0.992645751,0.003076057,9.61E-06,1.56E-06
9511,sentiment_analysis18,35,"For example , we can not obtain a good representation for "" hot dog "" by averaging the word vectors of "" hot "" and "" dog "" .",Introduction,Introduction,sentiment_analysis,18,20,1,0,,0.002107916,0,negative,1.33E-05,0.002048781,2.22E-06,1.60E-06,9.18E-05,4.19E-05,3.45E-05,5.29E-05,0.001546735,0.985089945,0.011055976,1.93E-05,9.97E-07
9512,sentiment_analysis18,36,Hot would be close to words like warm or cold and dog would be close to animals like cat .,Introduction,Introduction,sentiment_analysis,18,21,1,0,,0.001181018,0,negative,6.57E-06,0.004031779,7.35E-06,4.01E-05,0.001627559,0.000761685,8.15E-05,0.00041486,0.005687428,0.986424447,0.000905588,6.05E-06,5.05E-06
9513,sentiment_analysis18,37,The average would not be close to other food like burgers or spaghetti .,Introduction,Introduction,sentiment_analysis,18,22,1,0,,0.001614211,0,negative,1.12E-05,0.001706875,3.74E-06,2.94E-05,0.001295207,0.000299115,8.38E-05,0.00020348,0.001704553,0.993710377,0.000936751,1.20E-05,3.37E-06
9514,sentiment_analysis18,38,"Another example is "" hong kong style food "" .",Introduction,Introduction,sentiment_analysis,18,23,1,0,,0.000434419,0,negative,4.31E-06,0.001775639,3.86E-06,3.37E-05,0.001362642,0.000264038,8.79E-05,0.000203477,0.001201042,0.992353824,0.002695206,8.45E-06,5.87E-06
9515,sentiment_analysis18,39,"As it consists of many words , the averaged word vector could be faraway from "" food "" in vector space .",Introduction,Introduction,sentiment_analysis,18,24,1,0,,0.007275659,0,negative,6.92E-05,0.031536593,2.89E-05,2.71E-06,0.000483791,0.000147194,6.27E-05,0.000244932,0.11054906,0.855971339,0.000877966,2.34E-05,2.28E-06
9516,sentiment_analysis18,40,"To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .",Introduction,Introduction,sentiment_analysis,18,25,1,1,approach,0.761320026,1,model,3.23E-05,0.263360056,0.000104218,2.57E-06,0.000623084,3.62E-05,2.33E-05,7.51E-05,0.706484895,0.02898077,0.000269753,5.71E-06,2.10E-06
9517,sentiment_analysis18,41,We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the aspect embeddings .,Introduction,Introduction,sentiment_analysis,18,26,1,1,approach,0.923099336,1,model,1.10E-05,0.08300229,3.72E-05,2.93E-06,0.00021396,8.47E-05,3.25E-05,0.000250922,0.905785259,0.010487606,8.70E-05,1.25E-06,3.43E-06
9518,sentiment_analysis18,42,The weight vector represents the probability distribution over aspects for the given target .,Introduction,Introduction,sentiment_analysis,18,27,1,0,,0.829813354,1,model,3.12E-06,0.036383934,6.04E-06,5.05E-07,3.50E-05,0.000148899,2.91E-05,0.000999386,0.903614916,0.058545129,0.000231467,9.56E-07,1.52E-06
9519,sentiment_analysis18,43,The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sentiment .,Introduction,Introduction,sentiment_analysis,18,28,1,1,approach,0.95314447,1,model,9.54E-06,0.057631375,2.94E-05,5.10E-07,8.15E-05,1.77E-05,1.29E-05,4.82E-05,0.936886548,0.005236425,4.38E-05,9.50E-07,1.11E-06
9520,sentiment_analysis18,44,"We found the learned embeddings to be semantically meaningful , i.e. , embeddings of words that are semantically related appear close to the same aspect embedding .",Introduction,Introduction,sentiment_analysis,18,29,1,0,,0.049176475,0,negative,0.000316395,0.118543716,2.79E-05,9.89E-05,0.011254089,0.001020123,0.000597808,0.001608193,0.029443302,0.836467064,0.000446882,0.0001515,2.41E-05
9521,sentiment_analysis18,45,"For example , embeddings of the words service , servers , staff , and courteous appear close to the same aspect embedding , which we interpret to represent the aspect service .",Introduction,Introduction,sentiment_analysis,18,30,1,0,,0.007447574,0,negative,3.92E-05,0.015246357,1.90E-05,2.17E-05,0.002104729,0.000339376,0.000129505,0.00025572,0.008457035,0.972665684,0.000686228,3.04E-05,5.09E-06
9522,sentiment_analysis18,46,Our second approach exploits syntactic information to construct a syntax - based attention model .,Introduction,Introduction,sentiment_analysis,18,31,1,1,approach,0.845039032,1,approach,0.000127512,0.504895059,0.000602212,1.28E-05,0.003738144,0.000130161,0.000200859,0.000142399,0.403710198,0.085706801,0.000678329,4.42E-05,1.14E-05
9523,sentiment_analysis18,47,The attention models used in previous works give equal importance to all context words .,Introduction,Introduction,sentiment_analysis,18,32,1,0,,0.404408789,0,model,2.97E-05,0.125954451,5.10E-05,4.17E-06,0.000224353,0.000319103,0.000161722,0.001353951,0.602876391,0.267603752,0.00140077,1.23E-05,8.34E-06
9524,sentiment_analysis18,48,"In that case , the computed attention weights rely entirely on the semantic associations between context words and the target .",Introduction,Introduction,sentiment_analysis,18,33,1,0,,0.010482261,0,negative,1.86E-05,0.014381208,7.46E-06,1.02E-05,0.000336944,0.00015718,3.73E-05,0.000279849,0.031442013,0.952480287,0.000837795,8.89E-06,2.28E-06
9525,sentiment_analysis18,49,"However , this may not be sufficient for differentiating opinions words for different targets .",Introduction,Introduction,sentiment_analysis,18,34,1,0,,0.00481774,0,negative,1.28E-05,0.001623134,1.90E-06,8.26E-06,0.000405174,6.54E-05,4.23E-05,8.64E-05,0.001050265,0.996032949,0.000658934,1.10E-05,1.46E-06
9526,sentiment_analysis18,50,"Instead , our syntax - based attention mechanism selectively focuses on a small subset of context words that are close to the target on the syntactic path which is obtained by applying a dependency parser on the review sentence .",Introduction,Introduction,sentiment_analysis,18,35,1,1,approach,0.938935247,1,model,1.49E-05,0.096174809,4.25E-05,1.95E-07,0.000120082,8.37E-06,1.44E-05,1.67E-05,0.891412437,0.012163503,2.94E-05,2.14E-06,5.79E-07
9527,sentiment_analysis18,51,"We conducted experiments on attention - based LSTM models using the SemEval 2014 , 2015 , and 2016 datasets .",Introduction,Introduction,sentiment_analysis,18,36,1,0,,0.122116031,0,negative,6.88E-05,0.284924156,0.000130556,9.09E-05,0.105389375,0.001568206,0.001983983,0.00121109,0.00886768,0.595468458,0.000121511,0.000143986,3.13E-05
9528,sentiment_analysis18,52,"The results show that attention - based LSTM can be substantially improved by incorporating our two proposed methods , and that the resulting model outperforms all baseline methods on aspect - level sentiment classification .",Introduction,Introduction,sentiment_analysis,18,37,1,0,,0.143364436,0,negative,0.006698029,0.242905304,0.000181729,3.39E-05,0.003209348,0.000564082,0.016347052,0.00148964,0.063776808,0.656395602,0.001149232,0.007064,0.00018529
9529,sentiment_analysis18,53,Related Work,,,sentiment_analysis,18,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
9530,sentiment_analysis18,67,Model Description,,,sentiment_analysis,18,0,1,0,,0.005147828,0,negative,4.39E-05,0.001071123,7.58E-05,6.83E-06,5.42E-06,0.000325056,0.000218788,0.008009065,0.001950553,0.970198676,0.017521335,0.000530572,4.29E-05
9531,sentiment_analysis18,68,We propose two approaches to improve the effectiveness of the attention mechanism .,Model Description,Model Description,sentiment_analysis,18,1,1,0,,0.038527358,0,negative,0.001163071,0.001111287,0.000912908,1.07E-05,1.34E-05,0.000223748,6.96E-05,0.001588,0.001870993,0.991951574,0.000236963,0.000837854,9.82E-06
9532,sentiment_analysis18,69,The approaches maybe applied more generally but we use them on attention - based LSTM as it has been widely used in previous works for sentiment analysis .,Model Description,Model Description,sentiment_analysis,18,2,1,0,,0.001305212,0,negative,5.41E-05,7.31E-05,0.000123777,1.85E-06,1.79E-06,0.000305521,5.32E-05,0.001242992,9.87E-05,0.997900554,2.78E-05,0.000114906,1.69E-06
9533,sentiment_analysis18,70,We first give the task definition in ( 3.1 ) .,Model Description,Model Description,sentiment_analysis,18,3,1,0,,6.82E-05,0,negative,7.45E-06,1.94E-05,2.80E-06,2.55E-07,2.64E-07,4.68E-05,4.31E-06,0.000546467,8.14E-05,0.999247703,1.20E-05,3.08E-05,3.72E-07
9534,sentiment_analysis18,71,"Then , we briefly describe the architecture of attention - based LSTM ( 3.2 ) and introduce the two proposed approaches ( 3.3 & 3.4 ) .",Model Description,Model Description,sentiment_analysis,18,4,1,0,,5.78E-05,0,negative,1.72E-05,0.00011343,2.06E-05,2.96E-06,1.70E-06,0.000174583,1.51E-05,0.001204974,0.000915311,0.997474957,3.29E-05,2.40E-05,2.22E-06
9535,sentiment_analysis18,72,"Finally , we describe the over all architecture of our model for aspect - level sentiment classification and the training objective ( 3.5 ) .",Model Description,Model Description,sentiment_analysis,18,5,1,0,,2.15E-05,0,negative,8.56E-05,0.000200694,2.10E-05,4.22E-06,3.35E-06,0.000159159,1.91E-05,0.001255735,0.00067215,0.997468678,1.23E-05,9.60E-05,2.06E-06
9536,sentiment_analysis18,73,Task Definition and Notation,Model Description,,sentiment_analysis,18,6,1,0,,0.000170708,0,negative,1.36E-05,3.86E-05,1.70E-05,4.71E-06,1.45E-06,0.000156554,0.000162072,0.00125806,0.00013818,0.994583903,0.003235248,0.000371175,1.94E-05
9537,sentiment_analysis18,74,"Given a review sentence s = ( w 1 , w 2 , ... , w n ) consisting of n words , and an opinion target occurring in the sentence a = ( a 1 , a 2 , ... , am ) consisting of a subsequence of m continuous words from s , aspect - level sentiment classification aims to determine the sentiment polarity of sentence s towards the opinion target a .",Model Description,Task Definition and Notation,sentiment_analysis,18,7,1,0,,0.000241103,0,negative,7.75E-06,0.000121167,1.43E-05,1.21E-05,4.91E-06,0.000161962,0.000330927,0.000419946,0.000137679,0.854662997,0.144034081,6.49E-05,2.74E-05
9538,sentiment_analysis18,75,"When dealing with a text corpus , we begin by associating each word w with a continuous feature vector e w ?",Model Description,Task Definition and Notation,sentiment_analysis,18,8,1,0,,1.36E-05,0,negative,4.99E-06,7.98E-05,5.99E-06,1.42E-07,6.72E-07,5.25E-05,6.94E-06,0.000195776,0.000369724,0.99924302,3.53E-05,4.87E-06,1.98E-07
9539,sentiment_analysis18,76,"Rd , also known as word embedding , where d denotes the embedding dimension .",Model Description,Task Definition and Notation,sentiment_analysis,18,9,1,0,,8.65E-05,0,negative,1.30E-06,2.94E-05,9.18E-06,1.56E-07,1.89E-07,6.81E-05,2.66E-05,0.000351862,0.000156525,0.998879714,0.000471249,5.03E-06,6.93E-07
9540,sentiment_analysis18,77,"The vectors associated with the words correspond to the rows of a word embedding matrix E ? RV d , where V is the vocabulary size .",Model Description,Task Definition and Notation,sentiment_analysis,18,10,1,0,,7.23E-05,0,negative,2.04E-06,2.43E-05,1.76E-06,8.19E-08,3.10E-07,0.00019284,1.13E-05,0.00078524,0.000303292,0.998673319,3.66E-06,1.62E-06,1.44E-07
9541,sentiment_analysis18,78,Attention - based LSTM,Model Description,Task Definition and Notation,sentiment_analysis,18,11,1,0,,0.002303576,0,negative,5.93E-05,0.000244177,0.004321557,2.63E-06,3.86E-06,0.000787093,0.001367529,0.001249953,0.014614408,0.97448385,0.002587283,0.000231185,4.72E-05
9542,sentiment_analysis18,79,We briefly describe a conventional attention - based LSTM in this subsection .,Model Description,Task Definition and Notation,sentiment_analysis,18,12,1,0,,0.000135442,0,negative,5.41E-06,0.0002535,2.39E-05,2.96E-07,1.57E-06,8.79E-05,3.39E-05,0.000398533,0.001069508,0.997942795,0.00016662,1.50E-05,9.63E-07
9543,sentiment_analysis18,80,"Given a sequence of word embeddings {e w 1 , e w 2 , ... , e wn } of a sentence s , LSTM with trainable parameters ?",Model Description,Task Definition and Notation,sentiment_analysis,18,13,1,0,,1.06E-05,0,negative,2.02E-06,1.53E-05,2.53E-06,1.92E-07,4.60E-07,3.09E-05,9.48E-06,0.000135269,5.01E-05,0.99969123,5.65E-05,5.74E-06,2.69E-07
9544,sentiment_analysis18,81,"lstm makes use of three gates to discard or pass the information through time , and outputs a sequence of hidden vectors h = {h 1 , h 2 , ... , h n }.",Model Description,Task Definition and Notation,sentiment_analysis,18,14,1,0,,0.000548349,0,negative,1.44E-05,0.000622689,0.001564112,9.40E-07,5.56E-06,0.000416356,0.000153321,0.00096493,0.029629397,0.966413157,0.000196491,1.05E-05,8.13E-06
9545,sentiment_analysis18,82,The sentence representation z s used for sentiment classification is then computed as the weighted summation of hidden vectors .,Model Description,Task Definition and Notation,sentiment_analysis,18,15,1,0,,1.53E-05,0,negative,6.39E-06,6.67E-05,1.53E-05,3.65E-08,4.45E-07,1.58E-05,4.36E-06,8.95E-05,0.000930822,0.998857573,7.25E-06,5.62E-06,1.33E-07
9546,sentiment_analysis18,83,"A positive weight pi is computed for each hi , which can be interpreted as the probability that w i is the right word to focus on when inferring the sentiment polarity of the opinion target a .",Model Description,Task Definition and Notation,sentiment_analysis,18,16,1,0,,2.39E-05,0,negative,1.14E-05,0.000220884,1.01E-05,7.23E-08,7.26E-07,6.52E-05,1.03E-05,0.000714086,0.002966436,0.995989107,6.04E-06,5.37E-06,2.31E-07
9547,sentiment_analysis18,84,"The value pi is computed by an attention model , which conditions on the hidden vector hi as well as the target representation .",Model Description,Task Definition and Notation,sentiment_analysis,18,17,1,0,,4.92E-05,0,negative,5.34E-06,0.000111933,4.54E-06,1.04E-07,5.62E-07,9.01E-05,1.24E-05,0.001048936,0.001589497,0.997126515,6.16E-06,3.58E-06,3.16E-07
9548,sentiment_analysis18,85,"In previous works , the attention process is usually described with the following equations :",Model Description,Task Definition and Notation,sentiment_analysis,18,18,1,0,,2.32E-05,0,negative,3.25E-06,4.21E-05,1.59E-05,3.83E-08,2.45E-07,3.03E-05,9.58E-06,0.000120831,0.001391952,0.998341237,3.99E-05,4.51E-06,2.08E-07
9549,sentiment_analysis18,86,"where f score is a function that computes a score for word w i according to the semantic association between hi and t s , and t sis the vector representation of the given target .",Model Description,Task Definition and Notation,sentiment_analysis,18,19,1,0,,4.62E-06,0,negative,6.98E-06,8.74E-06,7.73E-06,1.32E-07,1.03E-06,2.33E-05,5.08E-06,5.79E-05,7.79E-05,0.999803948,1.39E-06,5.76E-06,1.52E-07
9550,sentiment_analysis18,87,Target Representation,Model Description,,sentiment_analysis,18,20,1,0,,0.000166497,0,negative,2.35E-05,4.37E-05,0.00020255,8.36E-07,1.12E-06,0.000243301,0.00024868,0.001786162,0.00184283,0.994405895,0.000109251,0.001069809,2.24E-05
9551,sentiment_analysis18,88,"Most previous work , we represent the target as a weighted summation of aspect embeddings , as illustrated in .",Model Description,Target Representation,sentiment_analysis,18,21,1,0,,6.73E-05,0,negative,8.09E-06,8.44E-06,4.21E-05,4.55E-07,1.90E-07,3.12E-05,1.30E-05,0.000263062,8.73E-05,0.999488585,1.77E-05,3.62E-05,3.70E-06
9552,sentiment_analysis18,89,"An aspect embedding matrix is represented by T ? R Kd , where K , the number of aspects defined by the user , is much smaller than V .",Model Description,Target Representation,sentiment_analysis,18,22,1,0,,1.39E-05,0,negative,1.45E-06,2.61E-06,4.05E-06,1.63E-08,3.05E-08,8.80E-06,2.25E-06,0.000252431,7.15E-05,0.999649369,3.70E-07,6.80E-06,2.74E-07
9553,sentiment_analysis18,90,The process is formalized as follows :,Model Description,Target Representation,sentiment_analysis,18,23,1,0,,4.68E-06,0,negative,1.43E-06,3.86E-07,7.51E-06,3.89E-09,1.03E-08,6.09E-07,4.94E-07,6.09E-06,3.62E-05,0.999941359,1.35E-07,5.69E-06,7.68E-08
9554,sentiment_analysis18,91,where Average returns the mean of the input vectors .,Model Description,Target Representation,sentiment_analysis,18,24,1,0,,7.92E-06,0,negative,3.26E-06,4.00E-07,1.86E-05,2.23E-08,4.40E-08,3.46E-06,2.10E-06,2.30E-05,8.08E-06,0.999924248,1.41E-07,1.64E-05,2.40E-07
9555,sentiment_analysis18,92,c s captures both target information and context information .,Model Description,Target Representation,sentiment_analysis,18,25,1,0,,0.001519678,0,negative,6.45E-05,1.15E-05,0.001345298,3.58E-08,2.93E-07,2.79E-06,6.22E-06,2.14E-05,0.000259949,0.998032968,7.52E-07,0.000253305,1.03E-06
9556,sentiment_analysis18,93,"qt is the weight vector over K aspect embeddings , where each weight represents the probability that the target belongs to the related aspect .",Model Description,Target Representation,sentiment_analysis,18,26,1,0,,8.64E-06,0,negative,1.84E-06,5.11E-06,5.22E-06,3.58E-08,4.35E-08,8.06E-06,2.14E-06,0.000306505,0.000150932,0.999515341,1.67E-07,4.22E-06,3.91E-07
9557,sentiment_analysis18,94,W t and b tare a weight matrix and a bias vector respectively .,Model Description,Target Representation,sentiment_analysis,18,27,1,0,,6.81E-05,0,negative,1.57E-06,2.68E-06,2.59E-06,1.98E-08,2.82E-08,6.99E-06,1.47E-06,0.000250967,8.55E-05,0.999644517,1.01E-07,3.36E-06,2.03E-07
9558,sentiment_analysis18,95,We would like the learned aspect embeddings to be meaningful and semantically coherent .,Model Description,Target Representation,sentiment_analysis,18,28,1,0,,9.92E-06,0,negative,2.67E-06,6.45E-07,7.52E-07,4.02E-08,2.78E-08,6.28E-06,1.49E-06,0.000112635,1.24E-05,0.999843774,9.93E-08,1.90E-05,2.33E-07
9559,sentiment_analysis18,96,This would allow us to interpret an aspect by looking at its nearby words in vector space .,Model Description,Target Representation,sentiment_analysis,18,29,1,0,,2.35E-06,0,negative,1.26E-06,2.60E-07,1.96E-06,3.82E-09,1.36E-08,8.32E-07,2.70E-07,6.27E-06,1.43E-05,0.999969345,2.46E-08,5.40E-06,3.88E-08
9560,sentiment_analysis18,97,"However , the aspect embedding matrix T is randomly initialized .",Model Description,Target Representation,sentiment_analysis,18,30,1,0,,7.69E-05,0,negative,5.50E-06,5.27E-06,2.34E-06,2.45E-08,5.70E-08,1.07E-05,3.22E-06,0.000820004,5.85E-05,0.999083702,6.06E-08,1.03E-05,2.79E-07
9561,sentiment_analysis18,98,It is difficult to obtain coherent aspect embeddings if we only rely on the training of the sentiment classifier .,Model Description,Target Representation,sentiment_analysis,18,31,1,0,,0.000361716,0,negative,1.01E-05,1.70E-06,2.37E-06,7.78E-08,7.91E-08,2.73E-06,2.94E-06,4.79E-05,7.38E-06,0.999741585,4.16E-06,0.000178004,9.53E-07
9562,sentiment_analysis18,99,"Therefore , we add an unsupervised objective function to ensure the quality of the aspect embeddings , which is jointly trained with the attention - based LSTM .",Model Description,Target Representation,sentiment_analysis,18,32,1,0,,0.000735596,0,negative,9.69E-05,7.61E-05,6.65E-05,3.73E-08,3.93E-07,3.51E-06,3.76E-06,0.00013451,0.00113558,0.99839142,1.75E-07,9.06E-05,6.54E-07
9563,sentiment_analysis18,100,"Indeed , we can understand the process shown by Eq. as an autoencoder , where we first reduce c s from d dimensions to K dimensions with softmax non-linearity .",Model Description,Target Representation,sentiment_analysis,18,33,1,0,,2.79E-06,0,negative,1.95E-06,7.72E-07,7.13E-06,5.47E-09,1.55E-08,7.99E-07,5.08E-07,1.06E-05,8.76E-05,0.9998855,5.79E-08,4.96E-06,1.03E-07
9564,sentiment_analysis18,101,"Only the dimensions that are relevant to the aspects are retained in qt , whereas the other dimensions are removed .",Model Description,Target Representation,sentiment_analysis,18,34,1,0,,7.00E-05,0,negative,4.20E-05,4.16E-06,3.55E-05,8.93E-09,1.33E-07,1.09E-06,1.19E-06,2.19E-05,6.89E-05,0.99979039,1.72E-08,3.45E-05,1.39E-07
9565,sentiment_analysis18,102,Then we reconstruct c s from qt through linear combination of aspect embeddings .,Model Description,Target Representation,sentiment_analysis,18,35,1,0,,3.24E-05,0,negative,1.24E-05,4.26E-06,6.32E-05,1.41E-08,1.13E-07,9.68E-07,1.10E-06,1.46E-05,7.72E-05,0.999790585,4.05E-08,3.52E-05,2.76E-07
9566,sentiment_analysis18,103,The unsupervised objective is thus to minimize the reconstruction error as shown below :,Model Description,Target Representation,sentiment_analysis,18,36,1,0,,1.91E-05,0,negative,3.30E-06,1.81E-06,9.68E-06,2.93E-09,1.76E-08,5.19E-07,3.83E-07,1.28E-05,5.12E-05,0.999906569,5.24E-08,1.35E-05,8.29E-08
9567,sentiment_analysis18,104,where cosine similarity CosSim ( ) is used as the similarity measure .,Model Description,Target Representation,sentiment_analysis,18,37,1,0,,6.37E-06,0,negative,1.45E-06,2.58E-07,5.34E-06,5.53E-09,1.47E-08,2.85E-06,1.63E-06,5.14E-05,7.65E-06,0.999920382,2.66E-08,8.80E-06,1.59E-07
9568,sentiment_analysis18,105,denotes a very small positive number .,Model Description,Target Representation,sentiment_analysis,18,38,1,0,,0.00010182,0,negative,8.52E-06,3.03E-06,1.61E-06,1.49E-07,1.16E-07,2.75E-05,9.53E-06,0.001432774,3.58E-05,0.998443361,1.35E-07,3.58E-05,1.73E-06
9569,sentiment_analysis18,106,We set it to 10 ?7 in all experiments .,Model Description,Target Representation,sentiment_analysis,18,39,1,0,,0.010514129,0,negative,2.41E-05,2.66E-05,5.58E-06,1.58E-06,7.14E-07,0.00246352,0.000486068,0.11278156,0.000133486,0.883933062,4.20E-07,0.000104699,3.86E-05
9570,sentiment_analysis18,107,"D denotes all training samples , ( s , a ) denotes a sentencetarget pair , and ? = {E , T , W t , b t } is the set of trainable parameters .",Model Description,Target Representation,sentiment_analysis,18,40,1,0,,1.68E-06,0,negative,3.98E-07,7.10E-07,6.50E-07,8.93E-09,2.32E-08,2.31E-06,7.64E-07,0.000108294,9.71E-06,0.999873907,1.44E-08,3.09E-06,1.17E-07
9571,sentiment_analysis18,108,The learning process can also be viewed as multi-task learning where the unsupervised objective shown as Eq. 9 is an auxiliary task .,Model Description,Target Representation,sentiment_analysis,18,41,1,0,,1.86E-05,0,negative,1.24E-06,4.09E-06,9.70E-06,1.59E-08,3.90E-08,1.41E-06,8.64E-07,3.98E-05,0.000129408,0.999805258,2.38E-07,7.55E-06,3.98E-07
9572,sentiment_analysis18,109,Multi- task learning can help to reduce the amount of data required for learning and to improve the model generalization ability .,Model Description,Target Representation,sentiment_analysis,18,42,1,0,,0.005137602,0,negative,2.27E-05,1.58E-06,5.88E-06,1.55E-07,2.19E-07,4.07E-06,1.15E-05,4.64E-05,9.69E-06,0.999119517,2.41E-06,0.000770301,5.59E-06
9573,sentiment_analysis18,110,Syntax - based Attention Mechanism,Model Description,Target Representation,sentiment_analysis,18,43,1,0,,0.000152529,0,negative,8.33E-06,6.27E-06,0.000176951,5.20E-08,9.93E-08,9.10E-06,5.39E-05,0.000113252,0.000636709,0.998581509,1.09E-05,0.000384443,1.85E-05
9574,sentiment_analysis18,111,"The attention mechanism used in previous works gives equal importance to all context words , where the attention weight is merely a measure of semantic association between the target and the context word .",Model Description,Target Representation,sentiment_analysis,18,44,1,0,,5.69E-05,0,negative,4.65E-06,3.92E-06,2.07E-05,1.11E-08,5.81E-08,2.35E-06,1.94E-06,6.77E-05,0.000119273,0.999762899,8.73E-08,1.60E-05,4.38E-07
9575,sentiment_analysis18,112,But intuitively not all words are equally important for determining the polarity of a target .,Model Description,Target Representation,sentiment_analysis,18,45,1,0,,1.10E-05,0,negative,2.12E-06,9.68E-08,1.04E-06,1.59E-09,1.38E-08,2.58E-07,4.12E-07,3.94E-06,1.16E-06,0.999959775,1.43E-08,3.11E-05,5.07E-08
9576,sentiment_analysis18,113,"Words that appear near the target or have a modifier relation to the target , for example , are more important and should receive higher weight .",Model Description,Target Representation,sentiment_analysis,18,46,1,0,,1.74E-05,0,negative,1.85E-05,2.56E-07,3.84E-06,6.02E-09,7.28E-08,1.39E-06,2.05E-06,1.91E-05,3.72E-06,0.999786722,4.81E-09,0.000164106,2.01E-07
9577,sentiment_analysis18,114,This is particularly true for opinion words that express sentiment and when there are multiple targets and multiple opinion words in one sentence .,Model Description,Target Representation,sentiment_analysis,18,47,1,0,,0.000187291,0,negative,3.53E-06,3.46E-07,1.30E-06,3.09E-08,4.42E-08,1.34E-06,1.31E-06,2.18E-05,3.71E-06,0.9999019,1.16E-07,6.41E-05,4.49E-07
9578,sentiment_analysis18,115,"To address this issue , we propose an attention mechanism that also encodes the syntactic structure of a sentence , where syntactic information is obtained from a dependency parser .",Model Description,Target Representation,sentiment_analysis,18,48,1,0,,0.002550794,0,negative,7.58E-05,0.000180086,0.000667187,1.36E-07,2.02E-06,6.72E-06,2.10E-05,0.000101206,0.005695743,0.993046613,8.24E-07,0.000192919,9.74E-06
9579,sentiment_analysis18,116,shows the dependency tree of an example sentence .,Model Description,Target Representation,sentiment_analysis,18,49,1,0,,8.77E-06,0,negative,6.10E-07,1.77E-07,2.52E-06,9.55E-09,3.25E-08,1.68E-06,1.41E-06,2.33E-05,9.61E-06,0.999949091,2.03E-08,1.10E-05,5.70E-07
9580,sentiment_analysis18,117,The opinion words that are closer to the target in the dependency tree are more relevant for determining its sentiment .,Model Description,Target Representation,sentiment_analysis,18,50,1,0,,0.000750899,0,negative,0.000236206,1.95E-06,1.82E-05,1.73E-08,3.17E-07,1.45E-06,8.40E-06,3.23E-05,1.51E-05,0.997991567,8.57E-09,0.001693633,9.60E-07
9581,sentiment_analysis18,118,"In our model , we define the location l of a context word as its distance to the target 1 along the dependency path .",Model Description,Target Representation,sentiment_analysis,18,51,1,0,,5.90E-06,0,negative,1.06E-06,2.65E-06,6.07E-06,6.58E-09,6.58E-08,1.63E-06,9.29E-07,5.24E-05,9.17E-05,0.999838395,1.31E-08,4.79E-06,2.73E-07
9582,sentiment_analysis18,119,The attention model selectively attends to a small window of context words based on their location .,Model Description,Target Representation,sentiment_analysis,18,52,1,0,,0.001014422,0,negative,1.37E-05,1.71E-05,0.000340343,2.10E-08,2.24E-07,4.05E-06,8.12E-06,8.63E-05,0.003052571,0.996454082,6.04E-08,1.95E-05,3.88E-06
9583,sentiment_analysis18,120,We use ws to denote the attention window size .,Model Description,Target Representation,sentiment_analysis,18,53,1,0,,2.99E-05,0,negative,7.80E-07,2.17E-06,1.63E-06,9.67E-09,3.69E-08,9.43E-06,5.19E-06,0.0008064,4.38E-05,0.999123883,1.13E-08,5.99E-06,6.96E-07
9584,sentiment_analysis18,121,"In our experiment , we ignore context words whose location is larger than ws and for context words within the window , different weights are applied so that words closer to the target receive more attention .",Model Description,Target Representation,sentiment_analysis,18,54,1,0,,0.000259034,0,negative,1.51E-05,1.35E-05,3.44E-05,1.56E-08,3.58E-07,8.13E-06,9.55E-06,0.000276423,0.000167634,0.999446318,5.31E-09,2.77E-05,8.54E-07
9585,sentiment_analysis18,122,The details of the proposed syntax - based attention model are described as follows :,Model Description,Target Representation,sentiment_analysis,18,55,1,0,,4.32E-06,0,negative,2.41E-06,1.33E-06,6.00E-06,2.38E-08,1.09E-07,1.36E-06,9.15E-07,2.71E-05,4.94E-05,0.999904406,8.57E-09,6.15E-06,7.53E-07
9586,sentiment_analysis18,123,where t sis the target representation constructed using the method described in 3.3 .,Model Description,Target Representation,sentiment_analysis,18,56,1,0,,1.42E-06,0,negative,1.38E-06,9.38E-08,1.45E-06,9.22E-10,1.34E-08,2.51E-07,3.36E-07,4.39E-06,1.51E-06,0.999976254,9.09E-10,1.43E-05,4.75E-08
9587,sentiment_analysis18,124,We adopt a simple score function as follows :,Model Description,Target Representation,sentiment_analysis,18,57,1,0,,2.94E-06,0,negative,1.61E-06,4.47E-07,7.27E-06,1.66E-09,1.83E-08,6.41E-07,5.33E-07,1.87E-05,2.74E-05,0.999937568,1.61E-09,5.65E-06,1.26E-07
9588,sentiment_analysis18,125,where W a ?,Model Description,Target Representation,sentiment_analysis,18,58,1,0,,5.94E-07,0,negative,7.06E-07,6.00E-08,4.10E-07,1.72E-09,9.35E-09,5.61E-07,4.87E-07,9.83E-06,1.08E-06,0.999977124,1.49E-09,9.66E-06,7.44E-08
9589,sentiment_analysis18,126,R dd is a trainable weight matrix .,Model Description,Target Representation,sentiment_analysis,18,59,1,0,,1.78E-05,0,negative,2.23E-06,1.40E-06,1.86E-06,3.64E-08,1.01E-07,5.59E-06,3.23E-06,0.000362746,2.63E-05,0.999581665,6.25E-09,1.36E-05,1.20E-06
9590,sentiment_analysis18,127,Overall Architecture and Training Objective,Model Description,,sentiment_analysis,18,60,1,0,,0.001501581,0,negative,2.09E-05,3.85E-05,4.47E-05,1.36E-06,3.22E-06,0.000133175,0.000105275,0.002165118,0.001950841,0.995279842,5.96E-07,0.000205128,5.13E-05
9591,sentiment_analysis18,128,"After incorporating the two proposed approaches into the attention - based LSTM , our final model is illustrated in .",Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,61,1,0,,0.000246516,0,negative,1.98E-05,6.85E-05,8.02E-05,1.56E-07,1.02E-05,1.27E-05,5.72E-06,0.000110949,0.001845319,0.997802974,1.14E-07,4.14E-05,1.87E-06
9592,sentiment_analysis18,129,The attention - based LSTM component is associated with the categorical cross entropy loss of sentiment classification .,Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,62,1,0,,0.000103491,0,negative,0.000290889,0.002224293,0.002269028,4.06E-07,5.85E-05,3.39E-05,4.73E-05,0.000372881,0.02162162,0.972891527,4.49E-07,0.000181083,8.09E-06
9593,sentiment_analysis18,130,The loss function is given below :,Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,63,1,0,,4.39E-05,0,negative,1.05E-06,2.98E-05,4.22E-06,6.93E-08,1.25E-06,3.46E-05,2.94E-06,0.00154536,0.000537318,0.997840484,7.26E-08,1.93E-06,8.33E-07
9594,sentiment_analysis18,131,"where C is the collection of sentiment classes , P g ( s , a ) ( c ) is either 1 or 0 , indicating whether the gold label is c for ( s , a ) , and P ( s , a ) ( c ) is the predicted probability that ( s , a ) has sentiment class c. ? =",Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,64,1,0,,3.19E-07,0,negative,6.47E-07,6.03E-06,2.11E-06,8.30E-09,8.02E-07,4.98E-06,8.36E-07,9.35E-05,2.88E-05,0.999858178,1.34E-08,4.05E-06,8.55E-08
9595,sentiment_analysis18,132,"{E , T , W t , b t , W a , ? lstm } is the set of trainable parameters .",Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,65,1,0,,1.36E-06,0,negative,9.75E-07,2.14E-05,1.59E-06,8.76E-08,3.75E-06,3.12E-05,2.27E-06,0.001194344,9.31E-05,0.99864827,9.56E-09,2.49E-06,4.84E-07
9596,sentiment_analysis18,133,The aspect embeddings in T may become similar to each other during training .,Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,66,1,0,,5.55E-06,0,negative,3.74E-06,2.12E-05,3.26E-06,5.27E-08,2.58E-06,8.31E-06,1.30E-06,0.000226638,0.00014069,0.999584403,1.17E-08,7.49E-06,3.23E-07
9597,sentiment_analysis18,134,"To ensure diversity , we employ a regularization term to enforce the uniqueness of each aspect embedding :",Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,67,1,0,,0.000439853,0,negative,7.92E-05,0.002011076,7.20E-05,1.01E-06,5.04E-05,0.000152854,3.69E-05,0.008475769,0.008093343,0.980984656,1.09E-07,3.47E-05,8.04E-06
9598,sentiment_analysis18,135,"where I is the identity matrix , T norm is the L 2 normalization of T , and denotes the sum of all entries in the matrix .",Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,68,1,0,,2.43E-06,0,negative,1.00E-06,9.88E-06,1.92E-06,9.48E-08,2.70E-06,1.49E-05,1.33E-06,0.000262256,4.77E-05,0.999654714,1.28E-08,3.12E-06,3.52E-07
9599,sentiment_analysis18,136,R reaches the minimum when the dot product between any two different aspect embeddings is zero .,Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,69,1,0,,5.05E-05,0,negative,7.77E-05,7.63E-05,2.15E-05,4.61E-07,2.86E-05,2.72E-05,1.25E-05,0.000568107,0.000201763,0.998810902,4.92E-08,0.000172635,2.18E-06
9600,sentiment_analysis18,137,"Thus , the regularization term aims to enforce orthogonality among the rows of T , which punishes redundancy between aspect embeddings .",Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,70,1,0,,8.43E-05,0,negative,2.55E-05,0.00054146,2.63E-05,1.22E-07,1.24E-05,2.19E-05,5.77E-06,0.001085638,0.004352023,0.993914584,2.21E-08,1.30E-05,1.26E-06
9601,sentiment_analysis18,138,The final objective function of our model is defined as :,Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,71,1,0,,6.00E-06,0,negative,9.72E-07,2.43E-05,3.19E-06,3.26E-08,1.23E-06,1.03E-05,1.96E-06,0.000357542,0.000254369,0.999338629,5.72E-08,6.92E-06,5.43E-07
9602,sentiment_analysis18,139,where ? u and ?,Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,72,1,0,,4.25E-07,0,negative,1.22E-06,3.26E-06,1.40E-06,3.30E-08,2.34E-06,8.56E-06,1.49E-06,0.000103891,1.11E-05,0.999856396,5.27E-09,1.02E-05,1.59E-07
9603,sentiment_analysis18,140,rare hyperparameters that control the weights of the unsupervised objective described in 3.3 and the regularization term respectively .,Model Description,Overall Architecture and Training Objective,sentiment_analysis,18,73,1,0,,1.56E-05,0,negative,1.65E-06,4.59E-05,2.53E-06,1.72E-07,3.55E-06,0.000133575,1.12E-05,0.007671429,0.000222082,0.991901508,1.14E-08,4.64E-06,1.75E-06
9604,sentiment_analysis18,141,Experiments,,,sentiment_analysis,18,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
9605,sentiment_analysis18,142,Datasets,Experiments,,sentiment_analysis,18,1,1,0,,0.000821926,0,negative,3.21E-05,1.82E-05,0.000126856,3.75E-06,1.22E-06,0.00346732,0.000759479,0.019482954,2.36E-05,0.975294593,0.000400807,0.000329957,5.91E-05
9606,sentiment_analysis18,143,"We evaluate our proposed model on four benchmark datasets , taken from SemEval 2014 task 4 , SemEval 2015 task 12 , and SemEval 2016 task 5 2 .",Experiments,Datasets,sentiment_analysis,18,2,1,0,,0.005293874,0,negative,7.77E-06,3.77E-05,7.16E-06,2.03E-07,9.54E-07,0.000236316,1.95E-05,0.000859289,2.46E-06,0.998716958,1.04E-05,0.000101001,2.84E-07
9607,sentiment_analysis18,144,"Each training and test sample in the 2014 datasets consists of the review sentence , the opinion target , and the sentiment polarity towards the opinion target .",Experiments,Datasets,sentiment_analysis,18,3,1,0,,0.00173304,0,negative,1.95E-06,3.54E-06,5.54E-06,2.93E-07,5.96E-06,0.000146825,4.57E-06,0.000192272,7.04E-07,0.999634199,4.62E-07,3.62E-06,6.72E-08
9608,sentiment_analysis18,145,"Following previous works , we remove samples with conflicting polarity in the 2014 datasets - the number of samples in that class is very small and incorporating it will make the training dataset extremely unbalanced .",Experiments,Datasets,sentiment_analysis,18,4,1,0,,0.001596919,0,negative,0.00056254,3.98E-05,0.00012961,1.09E-06,1.46E-05,0.000147663,7.32E-06,0.000149172,6.06E-06,0.998806342,1.07E-06,0.000134475,2.53E-07
9609,sentiment_analysis18,146,"The data format in the 2015 and 2016 datasets is a bit different , where each opinion target is also associated with one or multiple aspects and thus can have multiple sentiment polarities .",Experiments,Datasets,sentiment_analysis,18,5,1,0,,0.000468991,0,negative,1.10E-06,1.78E-06,5.78E-06,1.80E-07,9.80E-07,5.78E-05,3.99E-06,4.69E-05,6.07E-07,0.999861522,9.02E-06,1.02E-05,1.04E-07
9610,sentiment_analysis18,147,Below is an example :,Experiments,Datasets,sentiment_analysis,18,6,1,0,,1.47E-05,0,negative,8.21E-07,1.65E-07,1.87E-06,1.88E-08,1.16E-07,1.08E-05,7.59E-07,1.08E-05,3.20E-07,0.999969762,8.49E-07,3.73E-06,1.95E-08
9611,sentiment_analysis18,148,The food was delicious but expensive .,Experiments,Datasets,sentiment_analysis,18,7,1,0,,0.002353675,0,negative,6.88E-06,1.69E-06,1.89E-06,5.81E-07,1.02E-06,0.000267744,6.01E-06,0.000261854,1.24E-06,0.999419284,4.58E-06,2.69E-05,2.97E-07
9612,sentiment_analysis18,149,"( target = "" food "" , aspect = food#quality , polarity = Pos ) ( target = "" food "" , aspect = food#prices , polarity = Neg )",Experiments,Datasets,sentiment_analysis,18,8,1,0,,0.000293231,0,negative,2.97E-06,1.07E-06,6.54E-06,2.17E-07,3.62E-07,0.000366447,1.13E-05,0.000404884,1.31E-06,0.999174277,4.88E-06,2.54E-05,3.34E-07
9613,sentiment_analysis18,150,"Since our model only takes a sentence and an opinion target as input , without using the aspect information , we remove a sample in both training and test sets if the opinion target has different polarities as the example above .",Experiments,Datasets,sentiment_analysis,18,9,1,0,,0.000412907,0,negative,1.60E-05,4.71E-06,1.02E-05,9.61E-08,2.03E-06,7.89E-05,1.48E-06,0.000171026,2.18E-06,0.999705172,1.16E-07,8.13E-06,4.40E-08
9614,sentiment_analysis18,151,This removes about 5 % and 4 % of test samples from the 2015 and 2016 datasets respectively .,Experiments,Datasets,sentiment_analysis,18,10,1,0,,0.003893141,0,negative,0.002844303,7.39E-06,4.10E-05,2.88E-06,2.98E-05,0.000127209,1.37E-05,6.70E-05,3.04E-06,0.996505437,6.49E-07,0.000356946,7.50E-07
9615,sentiment_analysis18,152,Statistics of the resulting datasets are presented in .,Experiments,Datasets,sentiment_analysis,18,11,1,0,,0.002171449,0,negative,4.00E-06,1.88E-07,5.12E-07,1.47E-08,1.38E-07,2.12E-05,1.63E-06,2.99E-05,1.00E-07,0.999884648,2.08E-07,5.75E-05,2.01E-08
9616,sentiment_analysis18,153,We initialize word embeddings using the 300 - dimension Glo Ve vectors supplied by and we use the dependency parser from spa,Experiments,Datasets,sentiment_analysis,18,12,1,0,,0.78412651,1,hyperparameters,2.54E-05,0.00015733,2.28E-05,7.34E-06,2.65E-06,0.122526548,0.000734958,0.504772719,7.55E-05,0.371574349,1.67E-05,6.45E-05,1.93E-05
9617,sentiment_analysis18,154,Cy 3 to obtain dependency paths of review sentences .,Experiments,Datasets,sentiment_analysis,18,13,1,0,,0.006227373,0,negative,3.08E-05,6.92E-06,0.000253238,2.26E-07,9.75E-07,6.69E-05,6.07E-06,9.59E-05,1.71E-05,0.999362815,1.15E-05,0.000146362,1.13E-06
9618,sentiment_analysis18,155,We randomly select 20 % of the original training data as the development set and only use the remaining 80 % for training .,Experiments,Datasets,sentiment_analysis,18,14,1,0,,0.237634066,0,negative,1.82E-05,0.000121158,1.23E-05,1.24E-06,4.20E-06,0.006517138,8.91E-05,0.057046439,2.65E-05,0.936118004,2.67E-06,3.99E-05,3.20E-06
9619,sentiment_analysis18,156,Values for the hyperparameters are obtained empirically on the development set of one task and are fixed for all other experiments .,Experiments,Datasets,sentiment_analysis,18,15,1,0,,0.023801679,0,negative,1.49E-05,7.31E-05,3.89E-06,8.70E-07,9.88E-07,0.010091003,5.96E-05,0.099939502,2.83E-05,0.889739495,3.55E-06,4.29E-05,1.92E-06
9620,sentiment_analysis18,157,"The dimension of the LSTM hidden vectors is set to 300 , the objective weights ? u and ?",Experiments,Datasets,sentiment_analysis,18,16,1,0,,0.614721723,1,negative,2.64E-05,0.000102679,9.13E-06,1.44E-06,1.24E-06,0.028512696,0.000223219,0.317516957,5.35E-05,0.653466961,1.02E-05,6.87E-05,6.84E-06
9621,sentiment_analysis18,158,"rare set to 1 and 0.1 respectively , the attention window size ws is set to 5 and the number of aspects K is set to 8 .",Experiments,Datasets,sentiment_analysis,18,17,1,0,,0.433631312,0,hyperparameters,4.86E-05,0.000224954,9.03E-06,8.93E-06,4.28E-06,0.04456304,0.000315971,0.517791315,7.82E-05,0.436834366,9.16E-06,9.32E-05,1.89E-05
9622,sentiment_analysis18,159,We use RMSProp with base learning rate set to 0.001 and decay rate set to 0.9 for network training .,Experiments,Datasets,sentiment_analysis,18,18,1,0,,0.889610943,1,hyperparameters,4.59E-05,0.000223945,3.15E-05,2.17E-05,6.18E-06,0.141621143,0.001016636,0.650444888,8.69E-05,0.206382146,9.08E-06,6.11E-05,4.89E-05
9623,sentiment_analysis18,160,The minibatch size is set to 32 .,Experiments,Datasets,sentiment_analysis,18,19,1,0,,0.791675276,1,hyperparameters,2.22E-05,0.000115467,1.15E-05,4.77E-06,2.11E-06,0.070170647,0.000532908,0.544895154,6.25E-05,0.384075285,1.16E-05,7.07E-05,2.52E-05
9624,sentiment_analysis18,161,"As a regularizer , we apply dropout with probability 0.5 to the LSTM layer and the output layer .",Experiments,Datasets,sentiment_analysis,18,20,1,0,,0.794834705,1,hyperparameters,0.000195996,0.000430958,6.00E-05,4.02E-05,2.13E-05,0.100743874,0.000906103,0.547392659,0.000173147,0.349852175,4.43E-06,0.000111181,6.80E-05
9625,sentiment_analysis18,162,"We train the network for a fix number of epochs and select the best model according to the performance on the development set , and evaluate it on the test set .",Experiments,Datasets,sentiment_analysis,18,21,1,0,,0.104232258,0,negative,3.23E-05,9.03E-05,1.45E-05,2.08E-07,1.29E-06,0.001266655,2.83E-05,0.011900564,3.41E-05,0.986573818,8.81E-07,5.60E-05,1.00E-06
9626,sentiment_analysis18,163,Model Comparisons,,,sentiment_analysis,18,0,1,0,,0.052526978,0,negative,4.26E-05,4.75E-05,9.60E-05,2.53E-07,5.11E-07,0.000184526,0.000359423,0.001874779,2.36E-05,0.99510079,0.000663316,0.001603964,2.70E-06
9627,sentiment_analysis18,164,We compare our model with the following baselines :,Model Comparisons,Model Comparisons,sentiment_analysis,18,1,1,0,,0.104213443,0,baselines,5.41E-06,9.01E-08,0.929104392,9.22E-09,4.25E-09,6.76E-06,0.000193996,3.26E-05,7.24E-08,0.070394177,3.64E-07,0.000260655,1.43E-06
9628,sentiment_analysis18,165,( 1 ) Feature - based SVM :,Model Comparisons,Model Comparisons,sentiment_analysis,18,2,1,1,baselines,0.995449054,1,baselines,4.23E-07,1.42E-08,0.999313835,5.60E-10,1.33E-10,3.98E-07,8.28E-06,2.28E-06,3.55E-08,0.000668866,1.16E-07,5.44E-06,3.21E-07
9629,sentiment_analysis18,166,We compare with the reported results of a top system in SemEval 2014 .,Model Comparisons,Model Comparisons,sentiment_analysis,18,3,1,1,baselines,0.115953105,0,negative,0.000114984,5.34E-07,0.45332115,1.35E-07,1.21E-07,1.51E-05,0.000882374,9.49E-05,2.48E-07,0.526939057,3.24E-06,0.018607616,2.06E-05
9630,sentiment_analysis18,167,We could not directly compare with the reported results from SemEval 2015 and 2016 as their model inputs are different from ours ( aspect is also one of their inputs ) .,Model Comparisons,Model Comparisons,sentiment_analysis,18,4,1,0,,0.025458428,0,negative,6.44E-05,1.74E-07,0.077415129,2.92E-07,1.10E-07,5.36E-05,0.00047408,0.000264651,2.88E-07,0.917305269,1.58E-06,0.004398971,2.15E-05
9631,sentiment_analysis18,168,( 2 ) LSTM : An LSTM network is built on top of word embeddings .,Model Comparisons,Model Comparisons,sentiment_analysis,18,5,1,1,baselines,0.99449441,1,baselines,5.20E-07,7.39E-09,0.999652145,1.06E-09,2.33E-10,2.17E-07,5.97E-06,7.32E-07,3.59E-08,0.000335673,4.53E-08,4.17E-06,4.85E-07
9632,sentiment_analysis18,169,The mean over hidden vectors is used as the sentence representation .,Model Comparisons,Model Comparisons,sentiment_analysis,18,6,1,0,,0.778304693,1,baselines,1.29E-05,4.60E-07,0.906057497,2.29E-08,6.08E-09,3.03E-05,8.28E-05,0.000758745,3.85E-06,0.09300525,3.43E-07,4.23E-05,5.48E-06
9633,sentiment_analysis18,170,"We produce the results of TDLSTM , ATAE - LSTM , and MM with the source codes released by their authors .",Model Comparisons,Model Comparisons,sentiment_analysis,18,7,1,0,,0.314197706,0,negative,0.000109431,5.90E-07,0.032413487,2.50E-05,9.81E-07,0.000760014,0.001551142,0.001819979,1.94E-06,0.962275873,1.14E-06,0.000937972,0.000102429
9634,sentiment_analysis18,171,We re-implement RAM following the instructions in its paper as the code is not available .,Model Comparisons,Model Comparisons,sentiment_analysis,18,8,1,0,,0.035483786,0,baselines,0.000256197,7.68E-07,0.72228505,3.08E-06,3.54E-07,0.000155415,0.000668212,0.000528369,4.05E-06,0.275591015,7.32E-07,0.00043627,7.05E-05
9635,sentiment_analysis18,172,The comparison results are shown in .,Model Comparisons,Model Comparisons,sentiment_analysis,18,9,1,0,,0.455429888,0,negative,9.20E-05,5.24E-08,0.054533979,1.82E-08,1.33E-08,1.19E-05,0.000544117,0.000117499,6.49E-08,0.914875422,6.10E-07,0.02981602,8.28E-06
9636,sentiment_analysis18,173,Both accuracy and macro - F1 are used for evaluation as the label distributions are unbalanced .,Model Comparisons,Model Comparisons,sentiment_analysis,18,10,1,0,,0.014066358,0,negative,2.04E-05,3.31E-07,0.019056717,4.08E-08,2.00E-08,5.93E-05,0.000247008,0.00139652,3.40E-07,0.978386754,3.79E-07,0.000826951,5.22E-06
9637,sentiment_analysis18,174,The reported numbers are obtained as the average value over 5 different runs with random initializations for each method .,Model Comparisons,Model Comparisons,sentiment_analysis,18,11,1,0,,0.051248985,0,negative,2.06E-05,2.42E-07,0.006175678,1.15E-07,3.57E-08,0.00011801,0.000430762,0.002431329,4.91E-07,0.989863564,4.88E-07,0.000943938,1.48E-05
9638,sentiment_analysis18,175,Significant test results are included for testing the robustness of methods under random parameter initializations .,Model Comparisons,Model Comparisons,sentiment_analysis,18,12,1,0,,0.067478758,0,negative,0.00049698,8.00E-07,0.016370665,1.77E-06,4.85E-07,6.01E-05,0.000340852,0.000937751,2.18E-06,0.974089211,8.38E-07,0.00764033,5.81E-05
9639,sentiment_analysis18,176,We also show the effect of each proposed approach : LSTM + ATT +,Model Comparisons,Model Comparisons,sentiment_analysis,18,13,1,0,,0.275389588,0,baselines,0.001709867,2.21E-07,0.555470428,8.58E-08,5.64E-08,1.62E-05,0.002283642,0.000101289,2.22E-07,0.280487373,1.12E-06,0.159883887,4.56E-05
9640,sentiment_analysis18,177,TarRep denotes the model where the proposed target representation is used while the attention model remains the same as LSTM + ATT ; LSTM + Syn ATT denotes the model where only the conventional attention is replaced by our syntax - based attention ; and LSTM + Syn ATT + TarRep denotes the full model with both approaches integrated as shown in .,Model Comparisons,Model Comparisons,sentiment_analysis,18,14,1,0,,0.069713048,0,baselines,4.20E-05,3.54E-07,0.723414145,1.50E-07,7.09E-08,2.73E-05,0.000315698,0.000203024,1.72E-06,0.275408644,4.69E-07,0.000559157,2.73E-05
9641,sentiment_analysis18,178,We make the following observations :,Model Comparisons,Model Comparisons,sentiment_analysis,18,15,1,0,,0.004882436,0,negative,0.000119259,2.87E-07,0.018101588,4.29E-07,9.91E-08,2.85E-05,0.000124109,0.000222261,9.41E-07,0.979391933,5.45E-07,0.001976307,3.37E-05
9642,sentiment_analysis18,179,"1 ) Feature - based SVM is still a strong baseline , our best model achieves competitive results on D1 and D2 without relying on so many manually - designed features and external resources .",Model Comparisons,Model Comparisons,sentiment_analysis,18,16,1,1,results,0.840195972,1,results,0.001653334,1.92E-07,0.068701018,2.95E-07,1.11E-07,1.09E-05,0.003149613,9.07E-05,7.56E-08,0.044530849,1.75E-06,0.881704154,0.000156975
9643,sentiment_analysis18,180,"2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .",Model Comparisons,Model Comparisons,sentiment_analysis,18,17,1,1,results,0.970313571,1,results,0.002298518,2.26E-07,0.01154006,3.64E-07,1.21E-07,6.57E-06,0.003076477,8.31E-05,7.30E-08,0.014233681,8.82E-07,0.968535988,0.000223948
9644,sentiment_analysis18,181,"3 ) Compared with LSTM + ATT , all three settings of our model are able to achieve statistically significant improvements ( p < 0.05 ) on all datasets .",Model Comparisons,Model Comparisons,sentiment_analysis,18,18,1,1,results,0.981969104,1,results,0.001768184,1.43E-07,0.005571536,2.37E-07,8.52E-08,5.35E-06,0.002686657,6.95E-05,4.35E-08,0.013955306,6.44E-07,0.975783049,0.000159262
9645,sentiment_analysis18,182,This demonstrates that both proposed approaches are effective .,Model Comparisons,Model Comparisons,sentiment_analysis,18,19,1,0,,0.261879248,0,negative,0.000691251,3.33E-07,0.015748758,2.46E-07,1.12E-07,2.81E-05,0.000616572,0.000457393,8.22E-07,0.945930173,7.45E-07,0.036462492,6.30E-05
9646,sentiment_analysis18,183,4 ) The integrated full model over all achieves the best performance compared to using only one of the two proposed approaches .,Model Comparisons,Model Comparisons,sentiment_analysis,18,20,1,1,results,0.971161553,1,results,0.00121995,1.96E-07,0.019736768,3.24E-07,8.89E-08,9.74E-06,0.00456086,0.000125187,7.35E-08,0.021073578,1.04E-06,0.953041195,0.000230998
9647,sentiment_analysis18,184,"This indicates that the two proposed approaches are complementary , thus further improvements could be obtained when combining them .",Model Comparisons,Model Comparisons,sentiment_analysis,18,21,1,0,,0.179147763,0,negative,0.002970846,6.01E-07,0.136506661,1.23E-07,1.50E-07,8.43E-06,0.000197984,6.96E-05,1.50E-06,0.815955784,4.43E-07,0.044268468,1.94E-05
9648,sentiment_analysis18,185,"5 ) The proposed target representation is more helpful on restaurant domain ( D1 , D3 , and D4 ) than laptop domain ( D2 ) .",Model Comparisons,Model Comparisons,sentiment_analysis,18,22,1,1,results,0.959051236,1,results,0.002361879,1.21E-07,0.004703609,1.51E-07,7.19E-08,4.79E-06,0.001837604,8.11E-05,3.82E-08,0.021222881,4.20E-07,0.969688178,9.92E-05
9649,sentiment_analysis18,186,"A plausible reason is that restaurant domain has clearer aspects for opinion targets , while it is much harder to determine the aspects for many opinion targets in the laptop domain .",Model Comparisons,Model Comparisons,sentiment_analysis,18,23,1,0,,0.019906242,0,negative,5.00E-05,2.90E-07,0.023756284,2.27E-07,1.25E-07,2.09E-05,0.000212988,0.000280901,4.91E-07,0.972040122,1.76E-06,0.003574429,6.15E-05
9650,sentiment_analysis18,187,"Since our model represents the target as weighted summation of aspect embeddings , domains with clear aspects may benefit more from the model .",Model Comparisons,Model Comparisons,sentiment_analysis,18,24,1,0,,0.690251677,1,negative,0.012806039,9.73E-07,0.151787069,5.39E-07,3.69E-07,1.97E-05,0.000485066,0.000292386,2.44E-06,0.733086999,3.97E-07,0.10142712,9.09E-05
9651,sentiment_analysis18,188,Model Analysis,,,sentiment_analysis,18,0,1,0,,0.034720463,0,negative,0.004267025,5.00E-05,0.000110679,0.000135804,2.74E-05,0.000268943,0.000813065,0.000416357,6.31E-05,0.988621093,0.002427733,0.002754386,4.44E-05
9652,sentiment_analysis18,189,We conduct more detailed analysis of the proposed approaches quantitatively and qualitatively .,Model Analysis,Model Analysis,sentiment_analysis,18,1,1,0,,0.001710296,0,negative,0.003765609,1.27E-05,1.43E-05,1.17E-06,1.48E-05,2.91E-05,1.24E-05,7.34E-06,1.86E-05,0.996114227,6.26E-06,2.64E-06,7.83E-07
9653,sentiment_analysis18,190,"By examining the final test outputs of the relevant models , we try to investigate what kind of errors made by the baseline can be more effectively treated by our proposed approaches .",Model Analysis,Model Analysis,sentiment_analysis,18,2,1,0,,0.002459242,0,negative,0.02339581,1.06E-05,5.98E-05,5.07E-08,3.41E-06,3.97E-06,9.22E-06,1.30E-06,1.40E-05,0.97648261,5.20E-06,1.38E-05,1.90E-07
9654,sentiment_analysis18,191,Impact of Syntax - based Attention Syntax - based attention is supposed to better differentiate opinion contexts for different targets when there are multiple targets appearing in the sentence .,Model Analysis,Model Analysis,sentiment_analysis,18,3,1,0,,0.629057386,1,ablation-analysis,0.984113757,2.02E-06,0.000139617,9.22E-08,1.00E-06,3.01E-06,0.00012656,4.16E-07,2.12E-06,0.015492565,7.92E-06,0.000108943,1.97E-06
9655,sentiment_analysis18,192,"To verify this , we compare LSTM + Syn ATT with LSTM and LSTM + ATT on sentences grouped by their number of targets .",Model Analysis,Model Analysis,sentiment_analysis,18,4,1,0,,0.060356387,0,negative,0.051643943,0.000229251,0.005108084,3.91E-07,4.69E-05,5.05E-05,0.000758051,1.09E-05,0.000100771,0.941867537,3.55E-05,0.000143734,4.42E-06
9656,sentiment_analysis18,193,shows the accuracies on the test sets of Restaurant14 ( D1 ) and Laptop14 ( D2 ) .,Model Analysis,Model Analysis,sentiment_analysis,18,5,1,0,,0.007846956,0,negative,0.010636739,2.38E-06,6.66E-05,2.91E-07,5.23E-06,4.09E-05,0.000223062,6.38E-06,4.13E-06,0.988944108,2.48E-05,4.02E-05,5.23E-06
9657,sentiment_analysis18,194,LSTM + SynATT performs the best on all groups .,Model Analysis,Model Analysis,sentiment_analysis,18,6,1,0,,0.908186336,1,ablation-analysis,0.921967745,4.06E-06,0.000345656,3.50E-06,1.28E-05,0.000118116,0.00693231,1.58E-05,3.00E-06,0.067610789,2.12E-05,0.002752092,0.000212943
9658,sentiment_analysis18,195,"In particular , it performs substantially better on groups with two or three targets .",Model Analysis,Model Analysis,sentiment_analysis,18,7,1,0,,0.512485724,1,ablation-analysis,0.94111165,3.52E-06,4.29E-05,9.35E-07,6.32E-06,1.28E-05,0.000547059,3.44E-06,1.64E-06,0.05706914,1.14E-05,0.001171366,1.78E-05
9659,sentiment_analysis18,196,"By analyzing a number of examples from these groups , we find that the proposed syntax - based attention is more effective in capturing the relevant opinion context for a given target when there are multiple targets in the sentence .",Model Analysis,Model Analysis,sentiment_analysis,18,8,1,0,,0.11207269,0,ablation-analysis,0.916746429,1.65E-05,4.91E-05,1.45E-07,3.03E-06,5.98E-06,0.000265258,2.52E-06,8.28E-06,0.082369082,8.63E-06,0.000521352,3.74E-06
9660,sentiment_analysis18,197,"Two examples are given in , where our syntax - based attention successfully captures the correct opinion word towards the target of interest , whereas since conventional attention only relies on semantic association between words and the target , it fails by mis-attending to the opinion word towards other target which has similar aspect semantics .",Model Analysis,Model Analysis,sentiment_analysis,18,9,1,0,,0.005711438,0,negative,0.004870939,5.98E-06,0.000120021,1.43E-06,4.13E-05,1.68E-05,1.89E-05,1.24E-06,1.65E-05,0.994876227,2.12E-05,6.84E-06,2.64E-06
9661,sentiment_analysis18,198,"In addition , we observe that all models perform poorly on the group with more than three targets .",Model Analysis,Model Analysis,sentiment_analysis,18,10,1,0,,0.847120813,1,ablation-analysis,0.961949074,2.11E-06,1.40E-05,1.77E-06,7.86E-06,1.80E-05,0.000499775,2.98E-06,8.61E-07,0.036992082,8.12E-06,0.000479681,2.37E-05
9662,sentiment_analysis18,199,"By analyzing the errors , we find two main causes .",Model Analysis,Model Analysis,sentiment_analysis,18,11,1,0,,0.017532506,0,negative,0.062998883,4.04E-06,0.000188001,9.23E-07,4.31E-05,1.96E-05,1.85E-05,1.23E-06,9.34E-06,0.936703444,2.19E-06,8.75E-06,1.94E-06
9663,sentiment_analysis18,200,"First , those sentences are relatively long , involving more complex opinion expressions and sentence structures .",Model Analysis,Model Analysis,sentiment_analysis,18,12,1,0,,0.004375727,0,negative,0.001741231,1.96E-06,2.21E-05,1.07E-06,8.04E-05,1.25E-05,2.06E-05,8.34E-07,8.26E-07,0.998091544,2.17E-05,2.99E-06,2.25E-06
9664,sentiment_analysis18,201,"Second , the proportion of neutral samples :",Model Analysis,Model Analysis,sentiment_analysis,18,13,1,0,,0.010617794,0,negative,0.112375752,7.07E-06,0.003246556,3.77E-08,1.95E-06,8.03E-06,6.53E-05,1.20E-06,6.01E-05,0.884198001,3.13E-06,3.20E-05,8.06E-07
9665,sentiment_analysis18,202,Top 5 representative words of the eight discovered aspects on Restaurant14 contained in this group is much higher than in other groups .,Model Analysis,Model Analysis,sentiment_analysis,18,14,1,0,,0.173191567,0,ablation-analysis,0.504035074,1.07E-05,0.000352265,1.53E-05,0.002190837,8.30E-05,0.001254964,3.97E-06,4.88E-06,0.491417863,1.21E-05,0.000519242,9.97E-05
9666,sentiment_analysis18,203,"Since the number of neutral samples in the training set is small , the trained classifier has difficulties in predicting neutral samples in the test set .",Model Analysis,Model Analysis,sentiment_analysis,18,15,1,0,,0.012924104,0,negative,0.166438451,1.24E-05,5.29E-05,3.37E-07,6.35E-06,1.09E-05,9.28E-05,3.78E-06,1.39E-05,0.833160292,6.28E-05,0.000139998,5.11E-06
9667,sentiment_analysis18,204,Impact of Target Representation,Model Analysis,,sentiment_analysis,18,16,1,0,,0.470488248,0,ablation-analysis,0.958521429,1.88E-06,0.000244465,2.16E-06,2.78E-05,1.06E-05,0.00012998,3.97E-07,3.61E-06,0.040944335,3.82E-06,0.000100643,8.88E-06
9668,sentiment_analysis18,205,"To investigate how the proposed target representation helps to improve performance , we extract test examples from Restaurant14 which are mis-classified by LSTM + ATT but are correctly classified by LSTM + ATT + TarRep .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,17,1,0,,0.009810845,0,negative,0.004102179,1.68E-06,9.61E-05,6.56E-08,1.84E-06,7.60E-07,7.52E-06,1.74E-06,1.71E-06,0.994728951,1.77E-07,0.001055605,1.63E-06
9669,sentiment_analysis18,206,"Among these examples , 56 % are associated with opinion targets consisting of more than one words .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,18,1,0,,0.021806705,0,negative,0.002224923,9.92E-08,2.90E-05,1.46E-06,1.03E-05,1.61E-06,2.02E-06,4.64E-07,3.95E-07,0.997543357,7.95E-08,0.000177354,8.98E-06
9670,sentiment_analysis18,207,"Two examples are shown in where the targets are "" green chillis "" and "" boutique selection of wines "" respectively .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,19,1,0,,0.001865895,0,negative,0.000216899,1.15E-07,2.69E-05,8.95E-08,4.62E-07,7.16E-07,1.08E-06,7.07E-07,1.61E-06,0.999645754,1.66E-07,0.000101024,4.45E-06
9671,sentiment_analysis18,208,uses t- SNE visualization to show the comparison of the learned target representations between our method and the method of averaging word vectors used in previous works on these two examples .,Model Analysis,Impact of Target Representation,sentiment_analysis,18,20,1,0,,0.000879526,0,negative,0.00055552,1.91E-07,0.000113473,2.85E-07,4.20E-07,3.86E-06,6.16E-06,4.00E-06,2.37E-06,0.999135444,2.55E-07,0.000159348,1.87E-05
9672,sentiment_analysis18,209,"In , we can observe that simply averaging the component word vectors fails to capture the correct semantics of both targets , as the target representations are faraway from the food - related words in the embedding space .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,21,1,0,,0.236610756,0,ablation-analysis,0.608512794,3.40E-07,3.24E-05,1.28E-07,1.68E-07,9.94E-07,6.83E-05,1.94E-06,7.31E-07,0.355124021,7.48E-07,0.036240787,1.66E-05
9673,sentiment_analysis18,210,"Due to the inaccurate representation of target , as shown in , LSTM + ATT fails to attend to the right opinion context in both examples .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,22,1,0,,0.064191337,0,ablation-analysis,0.558356941,2.72E-07,6.43E-05,2.36E-07,3.46E-07,1.17E-06,9.52E-05,1.39E-06,6.11E-07,0.367262718,7.68E-07,0.074173365,4.27E-05
9674,sentiment_analysis18,211,"Our proposed target representation is able to capture the correct aspect semantics for both targets and as a result , the attention mechanism can capture the correct opinion context .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,23,1,0,,0.110850601,0,negative,0.013220276,1.67E-05,0.000652045,1.68E-07,4.81E-07,8.56E-07,7.22E-06,2.72E-06,0.000481628,0.981628567,6.07E-06,0.003966263,1.70E-05
9675,sentiment_analysis18,212,"Furthermore , the proposed target representation also outputs aspect embeddings after the training process .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,24,1,0,,0.043573043,0,negative,0.003028665,3.10E-06,0.000357322,2.17E-08,1.00E-07,2.07E-07,8.50E-07,8.08E-07,0.000158848,0.996131655,3.64E-07,0.000316878,1.19E-06
9676,sentiment_analysis18,213,Each aspect can be interpreted by its nearby words in vector space .,Model Analysis,Impact of Target Representation,sentiment_analysis,18,25,1,0,,0.00327491,0,negative,0.00060848,8.02E-07,0.000496817,1.66E-08,8.52E-08,2.73E-07,5.10E-07,6.92E-07,7.75E-05,0.998719049,1.90E-07,9.44E-05,1.16E-06
9677,sentiment_analysis18,214,presents top representative words of the eight discovered aspects on Restaurant 14 .,Model Analysis,Impact of Target Representation,sentiment_analysis,18,26,1,0,,0.003470528,0,negative,0.001656991,2.38E-07,0.000122294,1.98E-06,4.26E-06,7.45E-06,1.70E-05,3.70E-06,1.79E-06,0.996865257,5.83E-07,0.001089448,0.000229024
9678,sentiment_analysis18,215,The words are ranked based on their cosine similarities with the aspect embedding .,Model Analysis,Impact of Target Representation,sentiment_analysis,18,27,1,0,,0.016178453,0,negative,0.001273669,8.07E-06,0.000447742,8.33E-08,3.82E-07,1.67E-06,4.30E-06,9.96E-06,0.000287829,0.997870945,3.52E-07,8.37E-05,1.13E-05
9679,sentiment_analysis18,216,"As shown , each aspect is semantically coherent and our model is able to discover the typical aspects of a restaurant such as food , ambience , service , and price .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,28,1,0,,0.024080645,0,negative,0.021294173,2.04E-06,0.000253092,8.26E-08,6.11E-07,4.94E-07,5.84E-06,9.19E-07,2.61E-05,0.973768914,3.95E-07,0.004639941,7.44E-06
9680,sentiment_analysis18,217,"Since qt in Equation represents the probability distribution over aspects for the input target , our model could additionally be used to map the input target to an aspect .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,29,1,0,,0.000354776,0,negative,0.000304495,3.65E-07,3.71E-05,4.11E-08,6.70E-08,2.95E-07,3.61E-07,1.39E-06,3.64E-05,0.99959606,8.21E-08,2.20E-05,1.47E-06
9681,sentiment_analysis18,218,"We did not conduct further experiments on this since it is not our main focus in this work , but it could bean interesting direction to explore in future .",Model Analysis,Impact of Target Representation,sentiment_analysis,18,30,1,0,,0.000211042,0,negative,0.00074906,9.03E-08,7.27E-06,8.40E-08,1.17E-07,1.26E-06,1.66E-06,1.65E-06,1.35E-06,0.999149198,9.56E-08,8.30E-05,5.13E-06
9682,sentiment_analysis18,219,Remaining Error Analysis,Model Analysis,,sentiment_analysis,18,31,1,0,,0.027685958,0,negative,0.010612842,5.33E-07,3.74E-05,1.72E-06,3.11E-05,3.38E-05,5.88E-05,1.11E-06,2.43E-06,0.98920166,8.80E-07,7.43E-06,1.03E-05
9683,sentiment_analysis18,220,"We additionally conduct a careful analysis of a subset of errors made by our full model , in order to better understand its limitations .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,32,1,0,,0.000204658,0,negative,0.000953383,4.69E-05,3.81E-05,1.20E-05,0.004493296,0.000134408,3.00E-05,3.78E-06,1.50E-05,0.994269393,3.50E-07,1.83E-06,1.44E-06
9684,sentiment_analysis18,221,"To do that , we randomly sample 100 examples with classification errors on the test set of Restaurant14 , and classify them into several error categories .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,33,1,0,,0.000137597,0,negative,0.000712318,0.00026358,7.13E-05,1.64E-06,0.001967748,0.000437385,0.000475354,6.22E-05,3.59E-05,0.995962934,3.99E-07,3.03E-06,6.15E-06
9685,sentiment_analysis18,222,"shows the top three error categories , the corresponding proportions , and some representative examples for each category .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,34,1,0,,2.24E-05,0,negative,0.000109711,1.73E-06,9.58E-06,9.76E-07,0.000244809,9.01E-05,3.41E-05,2.48E-06,1.28E-06,0.999503472,1.21E-07,6.97E-07,9.66E-07
9686,sentiment_analysis18,223,"The top category is Neutral , which denotes examples where the gold sentiment label is neutral .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,35,1,0,,0.000150641,0,negative,0.000115442,2.21E-05,0.00017893,2.56E-07,8.71E-05,0.000240429,0.000146826,2.00E-05,3.56E-05,0.99914917,3.29E-07,1.19E-06,2.63E-06
9687,sentiment_analysis18,224,There are two main groups of errors under this category :,Model Analysis,Remaining Error Analysis,sentiment_analysis,18,36,1,0,,1.34E-05,0,negative,0.000294501,8.06E-07,3.61E-05,1.54E-07,4.22E-05,1.18E-05,1.69E-05,3.09E-07,8.84E-07,0.999594037,7.75E-07,1.06E-06,5.07E-07
9688,sentiment_analysis18,225,( 1 ) The polarity of the target is affected by other sentiment words in the sentence .,Model Analysis,Remaining Error Analysis,sentiment_analysis,18,37,1,0,,0.040318244,0,negative,0.030347606,5.94E-05,0.001003217,6.97E-07,0.000246687,4.34E-05,7.23E-05,1.96E-06,7.06E-05,0.968137049,1.25E-06,1.39E-05,1.93E-06
9689,sentiment_analysis18,226,"As shown in example 1 ) , the sentence holds a positive sentiment on atmosphere , but expresses no specific opinion on drinks .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,38,1,0,,1.72E-05,0,negative,7.87E-05,2.05E-06,9.84E-06,1.70E-07,0.000158084,1.53E-05,1.46E-05,6.92E-07,1.75E-06,0.999717209,3.50E-07,8.32E-07,4.60E-07
9690,sentiment_analysis18,227,"However , affected by the word perfect , the predicted sentiment towards drinks is positive .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,39,1,0,,0.000614405,0,negative,0.21197319,7.29E-06,8.11E-05,9.10E-07,0.000630452,4.27E-05,0.000653463,2.09E-06,1.98E-06,0.786423115,6.13E-07,0.000177854,5.27E-06
9691,sentiment_analysis18,228,"Although the proposed attention mechanism aims to address this type of errors , it still fails on complex examples ; ( 2 ) The sentence is objective , with no opinion expression such as example 2 ) .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,40,1,0,,3.34E-05,0,negative,0.003964125,1.43E-05,6.15E-05,2.79E-07,0.000128236,1.84E-05,0.000105389,9.10E-07,2.99E-06,0.995672448,1.06E-05,1.90E-05,1.87E-06
9692,sentiment_analysis18,229,"Since there are many more positive training examples , the predictions on such neutral examples are often biased towards positive sentiment .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,41,1,0,,8.03E-05,0,negative,0.001451537,2.16E-05,4.12E-05,4.24E-08,1.86E-05,8.40E-06,4.12E-05,1.48E-06,1.53E-05,0.998390378,1.94E-06,7.57E-06,7.93E-07
9693,sentiment_analysis18,230,"The second most common error category is Complex , which includes examples with implicit opinion expressions ( example 3 ) or those that require deep comprehension to be understood ( example 4 ) .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,42,1,0,,0.000327037,0,negative,0.000521028,8.01E-07,1.28E-05,1.10E-06,0.000685776,4.60E-05,4.15E-05,6.59E-07,2.33E-07,0.998686618,1.06E-07,2.07E-06,1.29E-06
9694,sentiment_analysis18,231,"This type of errors is difficult to handle with current techniques , especially when trying to build an end -toend neural network .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,43,1,0,,2.80E-05,0,negative,0.000151132,2.68E-06,6.76E-06,1.77E-06,7.80E-05,5.96E-05,2.72E-05,2.03E-06,2.47E-06,0.999661344,3.46E-06,6.99E-07,2.83E-06
9695,sentiment_analysis18,232,The diversity and low frequency of those expressions make it hard for a statistical approach to capture their patterns .,Model Analysis,Remaining Error Analysis,sentiment_analysis,18,44,1,0,,1.26E-05,0,negative,5.34E-05,1.72E-06,3.89E-06,3.10E-07,4.21E-05,2.22E-05,1.17E-05,9.33E-07,1.55E-06,0.999858736,2.16E-06,4.62E-07,7.89E-07
9696,sentiment_analysis18,233,"For errors made on examples with negation words , we believe this is due to the insufficient training data such that LSTM can not effectively capture certain sequential patterns .",Model Analysis,Remaining Error Analysis,sentiment_analysis,18,45,1,0,,3.78E-05,0,negative,0.000195029,1.15E-06,3.98E-06,1.85E-07,0.000118922,4.82E-05,3.85E-05,1.68E-06,7.01E-07,0.999589647,9.76E-08,1.09E-06,9.04E-07
9697,sentiment_analysis18,234,Conclusion,,,sentiment_analysis,18,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
9698,natural_language_inference50,1,title,,,natural_language_inference,50,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
9699,natural_language_inference50,2,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,title,title,natural_language_inference,50,1,1,1,research-problem,0.998620283,1,research-problem,2.08E-08,5.40E-06,3.31E-08,1.37E-07,4.22E-08,1.58E-07,1.18E-06,1.85E-06,6.65E-07,0.001813523,0.998176841,8.98E-08,6.63E-08
9700,natural_language_inference50,3,abstract,,,natural_language_inference,50,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
9701,natural_language_inference50,4,The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers .,abstract,abstract,natural_language_inference,50,1,1,0,,0.099894431,0,research-problem,4.20E-08,2.14E-05,6.90E-08,7.50E-07,2.67E-07,1.36E-06,6.21E-07,7.98E-06,4.56E-06,0.031875369,0.968087464,2.81E-08,6.25E-08
9702,natural_language_inference50,5,"Given that recent architectural innovations are mostly new word interaction layers or attentionbased matching mechanisms , it seems to be a well - established fact that these components are mandatory for good performance .",abstract,abstract,natural_language_inference,50,2,1,0,,0.004880283,0,negative,8.51E-07,0.000179669,8.79E-08,7.73E-06,6.99E-06,7.24E-06,6.17E-07,4.62E-05,4.63E-05,0.810906892,0.188797174,1.70E-07,1.19E-07
9703,natural_language_inference50,6,"Unfortunately , the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications .",abstract,abstract,natural_language_inference,50,3,1,0,,0.001243017,0,negative,6.78E-07,0.000137116,7.18E-08,1.32E-05,7.70E-06,6.88E-06,8.17E-07,3.32E-05,2.07E-05,0.67877994,0.32099932,1.54E-07,1.45E-07
9704,natural_language_inference50,7,"As such , this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures .",abstract,abstract,natural_language_inference,50,4,1,0,,0.178409174,0,research-problem,1.92E-07,0.001186244,2.18E-07,2.81E-06,4.32E-06,4.10E-06,8.00E-07,4.36E-05,0.000128714,0.218761933,0.779866864,1.02E-07,1.22E-07
9705,natural_language_inference50,8,We propose a simple but novel deep learning architecture for fast and efficient question - answer ranking and retrieval .,abstract,abstract,natural_language_inference,50,5,1,0,,0.716161322,1,research-problem,6.41E-06,0.023715681,4.92E-05,1.41E-05,6.10E-05,1.10E-05,9.95E-06,9.73E-05,0.003338538,0.054702281,0.917989076,2.77E-06,2.75E-06
9706,natural_language_inference50,9,"More specifically , our proposed model , HyperQA , is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi - Perspective CNNs on multiple QA benchmarks .",abstract,abstract,natural_language_inference,50,6,1,0,,0.087113244,0,research-problem,5.08E-05,0.242333703,0.000237362,1.62E-05,0.000437847,4.92E-05,6.74E-05,0.000853,0.014787975,0.303510508,0.437524389,0.000124092,7.46E-06
9707,natural_language_inference50,10,The novelty behind Hyper QA is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space .,abstract,abstract,natural_language_inference,50,7,1,0,,0.470666255,0,research-problem,6.60E-08,9.81E-05,1.38E-07,6.25E-07,5.17E-07,5.77E-07,6.87E-07,6.72E-06,6.88E-06,0.023770013,0.97611547,7.82E-08,8.10E-08
9708,natural_language_inference50,11,This empowers our model with a self - organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers .,abstract,abstract,natural_language_inference,50,8,1,0,,0.132214828,0,negative,5.47E-05,0.082905044,6.66E-05,2.16E-05,0.00050857,4.24E-05,5.61E-06,0.000324168,0.034657875,0.866814925,0.014590383,6.97E-06,1.24E-06
9709,natural_language_inference50,12,"Our model requires no feature engineering , no similarity matrix matching , no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks .",abstract,abstract,natural_language_inference,50,9,1,0,,0.066200902,0,negative,0.000221759,0.120684258,5.88E-05,0.000131256,0.00137175,0.000137,6.11E-05,0.001435006,0.005476947,0.770657336,0.09966239,9.36E-05,8.72E-06
9710,natural_language_inference50,13,narrative,abstract,abstract,natural_language_inference,50,10,1,0,,0.007392531,0,research-problem,1.47E-05,0.002525271,3.15E-05,7.42E-05,0.000183872,9.20E-05,5.47E-05,0.000293284,0.001780218,0.444883991,0.550047512,1.29E-05,5.85E-06
9711,natural_language_inference50,14,are as follows :,abstract,abstract,natural_language_inference,50,11,1,0,,0.001942705,0,negative,6.48E-06,0.005771889,1.19E-05,2.53E-06,7.66E-05,8.92E-06,1.15E-06,8.42E-05,0.001808488,0.986323277,0.005902993,1.48E-06,1.18E-07
9712,natural_language_inference50,15,"Firstly , representations of questions and answers are first learned via a neural encoder such as the long short - term memory ( LSTM ) network or convolutional neural network ( CNN ) .",abstract,abstract,natural_language_inference,50,12,1,0,,0.437135952,0,negative,3.94E-05,0.0878054,0.000230928,3.73E-05,0.000614723,5.73E-05,1.35E-05,0.000386501,0.016246277,0.821913521,0.0726473,5.24E-06,2.56E-06
9713,natural_language_inference50,16,"Secondly , these representations of questions and answers are composed by an interaction function to produce an over all matching score .",abstract,abstract,natural_language_inference,50,13,1,0,,0.01745543,0,negative,2.21E-05,0.098896521,0.000300105,5.10E-06,0.000228754,2.75E-05,5.19E-06,0.000222958,0.192936864,0.689603565,0.017747016,3.32E-06,9.96E-07
9714,natural_language_inference50,17,The design of the interaction function between question and answer representations lives at the heart of deep learning QA research .,abstract,abstract,natural_language_inference,50,14,1,0,,0.480215298,0,research-problem,7.91E-08,8.59E-05,4.33E-08,5.25E-06,1.53E-06,2.40E-06,9.79E-07,1.51E-05,8.05E-06,0.080993194,0.918887154,6.31E-08,1.83E-07
9715,natural_language_inference50,18,"While it is simply possible to combine QA representations with simple feed forward neural networks or other composition functions , a huge bulk of recent work is concerned with designing novel word interaction layers that model the relationship between the words in the QA pairs .",abstract,abstract,natural_language_inference,50,15,1,0,,0.204639377,0,research-problem,3.24E-07,0.000204723,1.37E-07,7.12E-06,3.34E-06,6.84E-06,2.19E-06,4.65E-05,2.32E-05,0.187107451,0.812597674,1.98E-07,3.03E-07
9716,natural_language_inference50,19,"For example , similarity matrix based matching , soft attention alignment and attentive pooling are highly popular techniques for improving the performance of neural ranking models .",abstract,abstract,natural_language_inference,50,16,1,0,,0.004579659,0,research-problem,3.79E-07,0.000121243,1.65E-07,1.08E-05,4.85E-06,8.13E-06,3.42E-06,3.83E-05,1.09E-05,0.226119175,0.773681979,2.75E-07,4.12E-07
9717,natural_language_inference50,20,"Apparently , it seems to be well - established that grid - based matching is essential to good performance .",abstract,abstract,natural_language_inference,50,17,1,0,,0.002778389,0,negative,2.33E-06,0.000121234,1.71E-07,4.31E-06,7.97E-06,1.03E-05,2.58E-06,6.73E-05,2.09E-05,0.879338542,0.120422742,1.37E-06,2.04E-07
9718,natural_language_inference50,21,"Notably , these new innovations come with trade - offs such as huge computational cost that lead to significantly longer training times and also a larger memory footprint .",abstract,abstract,natural_language_inference,50,18,1,0,,0.001409238,0,negative,2.40E-06,0.000612537,3.34E-07,5.79E-05,3.54E-05,2.66E-05,2.54E-06,0.000124189,8.76E-05,0.922642278,0.076407125,5.71E-07,4.72E-07
9719,natural_language_inference50,22,"Additionally , it is good to consider that the base neural encoder employed also contributes to the computational cost of these neural ranking models , e.g. , LSTM networks are known to be over-parameterized and also incur a parameter and runtime cost of quadratic scale .",abstract,abstract,natural_language_inference,50,19,1,0,,0.000374076,0,negative,3.14E-06,0.00106181,3.47E-07,1.30E-05,2.23E-05,2.47E-05,1.15E-06,0.000197873,0.00031105,0.990667313,0.007696522,5.76E-07,1.73E-07
9720,natural_language_inference50,23,"It also seems to be a well - established fact that a neural encoder ( such as the LSTM , Gated Recurrent Unit ( GRU ) , CNN , etc. ) must be first selected for learning individual representations of questions and answers and is generally treated as mandatory for good performance .",abstract,abstract,natural_language_inference,50,20,1,0,,0.001659131,0,negative,5.45E-07,0.000173487,1.18E-07,2.19E-05,1.17E-05,1.88E-05,2.03E-06,0.000116509,3.07E-05,0.830263042,0.169360643,2.60E-07,3.06E-07
9721,natural_language_inference50,24,"In this paper , we propose an extremely simple neural ranking model for question answering that achieves highly competitive results on several benchmarks with only a fraction of the runtime and only 40K - 90 K parameters ( as opposed to millions ) .",abstract,abstract,natural_language_inference,50,21,1,1,model,0.789067016,1,research-problem,1.56E-05,0.059196914,0.00015634,1.31E-05,0.00015978,2.10E-05,6.68E-05,0.000282891,0.004975045,0.099989537,0.835086023,3.03E-05,6.69E-06
9722,natural_language_inference50,25,Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space .,abstract,abstract,natural_language_inference,50,22,1,1,model,0.317779029,0,approach,1.49E-05,0.354369307,0.000592913,5.01E-06,0.000224509,4.71E-05,3.01E-05,0.000756557,0.220803631,0.284357716,0.138779599,1.44E-05,4.29E-06
9723,natural_language_inference50,26,Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially .,abstract,abstract,natural_language_inference,50,23,1,1,model,0.003149567,0,negative,1.73E-06,0.016872501,5.77E-05,1.87E-05,7.32E-05,0.000137419,4.25E-05,0.00170319,0.008368845,0.544945691,0.427769692,2.84E-06,6.06E-06
9724,natural_language_inference50,27,"Intuitively , this makes it suitable for learning embeddings that reflect a natural hierarchy ( e.g. , networks , text , etc. ) which we believe might benefit neural ranking models for QA .",abstract,abstract,natural_language_inference,50,24,1,0,,0.048632854,0,negative,1.76E-05,0.054677995,3.41E-05,7.16E-06,0.000418845,2.13E-05,5.18E-06,0.000194215,0.010284469,0.926032058,0.008296751,9.66E-06,7.14E-07
9725,natural_language_inference50,28,"Notably , our work is inspired by the recently incepted Poincar embeddings which demonstrates the effectiveness of inducing a structural ( hierarchical ) bias in the embedding space for improved generalization .",abstract,abstract,natural_language_inference,50,25,1,0,,0.021609805,0,negative,5.96E-05,0.25203566,4.71E-05,0.000279024,0.0041642,0.000123368,2.14E-05,0.00098416,0.010458519,0.721404589,0.010401996,1.51E-05,5.20E-06
9726,natural_language_inference50,29,"In our early empirical experiments , we discovered that a simple feed forward neural network trained in Hyperbolic space is capable of outperforming more sophisticated models on several standard benchmark datasets .",abstract,abstract,natural_language_inference,50,26,1,0,,0.08207975,0,negative,1.69E-05,0.005196748,2.68E-06,3.13E-05,0.000101069,0.000114218,5.15E-05,0.001321158,0.000518965,0.857760294,0.134831815,4.79E-05,5.51E-06
9727,natural_language_inference50,30,We believe that this can be attributed to two reasons .,abstract,abstract,natural_language_inference,50,27,1,0,,0.000100588,0,negative,7.00E-07,0.0005899,3.79E-07,1.02E-05,4.58E-05,2.35E-05,1.14E-06,0.000123633,0.000232421,0.996629338,0.002342633,2.60E-07,1.51E-07
9728,natural_language_inference50,31,"Firstly , latent hierarchies are prominent in QA .",abstract,abstract,natural_language_inference,50,28,1,0,,0.022182263,0,research-problem,1.52E-06,0.00050832,7.06E-07,4.41E-06,1.28E-05,3.82E-06,7.46E-06,3.35E-05,1.76E-05,0.358105603,0.641301411,2.38E-06,5.65E-07
9729,natural_language_inference50,32,"Aside from the natural hierarchy of questions and answers , conceptual hierarchies also exist .",abstract,abstract,natural_language_inference,50,29,1,0,,0.000257118,0,negative,2.66E-07,0.000400033,2.57E-07,3.20E-06,2.56E-05,1.55E-05,1.86E-06,7.58E-05,0.000143787,0.984248254,0.015084963,3.83E-07,1.32E-07
9730,natural_language_inference50,33,"Secondly , natural language is inherently hierarchical which can be traced to power law distributions such as Zipf 's law .",abstract,abstract,natural_language_inference,50,30,1,0,,0.001120339,0,negative,8.73E-07,0.000351491,6.48E-07,3.03E-05,6.96E-05,1.67E-05,4.20E-06,4.42E-05,2.83E-05,0.889482436,0.109970314,4.88E-07,4.53E-07
9731,natural_language_inference50,34,The key contributions in this paper are as follows :,abstract,abstract,natural_language_inference,50,31,1,0,,5.82E-05,0,negative,2.17E-06,0.001608695,1.21E-06,0.000222785,0.000236228,0.000118459,6.13E-06,0.000329779,0.0003548,0.991135883,0.005981985,5.95E-07,1.29E-06
9732,natural_language_inference50,35,We propose a new neural ranking model for ranking question answer pairs .,abstract,abstract,natural_language_inference,50,32,1,0,,0.333321909,0,research-problem,1.95E-05,0.157955188,0.000268324,1.21E-05,0.000216102,4.59E-05,7.12E-05,0.000744863,0.051323578,0.274336912,0.514969382,2.63E-05,1.06E-05
9733,natural_language_inference50,36,"For the first time , our proposed model , HyperQA , performs matching of questions and answers in Hyperbolic space .",abstract,abstract,natural_language_inference,50,33,1,0,,0.531337619,1,research-problem,2.03E-05,0.097656457,0.00029092,1.46E-05,0.000565037,3.46E-05,0.000174372,0.000316635,0.005372339,0.349723219,0.545697073,0.00012248,1.20E-05
9734,natural_language_inference50,37,"To the best of our knowledge , we are the first to model QA pairs in Hyperbolic space .",abstract,abstract,natural_language_inference,50,34,1,0,,0.016963705,0,negative,8.38E-06,0.057207388,4.32E-05,9.11E-05,0.001082958,0.000159368,7.57E-05,0.00127449,0.004124477,0.818092518,0.117815288,1.57E-05,9.42E-06
9735,natural_language_inference50,38,"While hyperbolic geometry and embeddings have been explored in the domains of complex networks or graphs , our work is the first to investigate the suitability of this metric space for question answering .",abstract,abstract,natural_language_inference,50,35,1,0,,0.161375228,0,negative,7.25E-06,0.044812996,1.57E-05,0.000184615,0.000983177,0.00010931,3.92E-05,0.000787409,0.00141957,0.836318851,0.115307205,1.00E-05,4.73E-06
9736,natural_language_inference50,39,Hyper,abstract,,natural_language_inference,50,36,1,0,,0.000264839,0,negative,2.26E-06,0.000680904,8.15E-07,2.06E-05,0.000118637,5.99E-05,6.13E-06,0.000302183,0.000446941,0.996845363,0.001513775,1.96E-06,5.26E-07
9737,natural_language_inference50,40,"QA is an extremely fast and parameter efficient model that achieves very competitive results on multiple QA benchmarks such as TrecQA , WikiQA and YahooCQA .",abstract,Hyper,natural_language_inference,50,37,1,0,,0.300035308,0,research-problem,2.24E-06,0.00091136,3.98E-06,0.000533537,0.000768312,0.000181613,0.000216645,0.000182865,2.18E-05,0.393365938,0.603782929,5.73E-06,2.31E-05
9738,natural_language_inference50,41,The efficiency and speed of Hyper QA are attributed by the fact that we do not use any sophisticated neural encoder and have no complicated word interaction layer .,abstract,Hyper,natural_language_inference,50,38,1,0,,0.007576465,0,negative,3.69E-05,0.006628162,2.66E-05,1.23E-05,0.000728352,4.12E-05,2.92E-05,9.84E-05,0.000426682,0.981715922,0.010223995,3.07E-05,1.56E-06
9739,natural_language_inference50,42,"In fact , Hyper QA is a mere single layered neural network with only 90K parameters .",abstract,Hyper,natural_language_inference,50,39,1,0,,0.011181579,0,negative,2.66E-05,0.075143396,0.000145687,0.001428871,0.040958558,0.020071221,0.000599426,0.027551767,0.007886654,0.825506818,0.000612123,9.50E-06,5.94E-05
9740,natural_language_inference50,43,"Very surprisingly , Hyper QA actually outperforms many state - of - the - art models such as Attentive Pooling BiLSTMs and Multi - Perspective CNNs .",abstract,Hyper,natural_language_inference,50,40,1,0,,0.117704423,0,negative,0.000510019,0.011366531,7.34E-05,0.000233534,0.006697565,0.000942887,0.00882924,0.003011905,0.000145636,0.945026683,0.012500894,0.010517265,0.000144489
9741,natural_language_inference50,44,We believe that this allows us to reconsider whether many of these complex word interaction layers are really necessary for good performance .,abstract,Hyper,natural_language_inference,50,41,1,0,,0.00026071,0,negative,1.53E-06,0.000719448,3.40E-07,6.46E-06,8.96E-05,6.51E-05,2.65E-06,0.000200813,0.000380222,0.998269078,0.000264024,5.98E-07,2.24E-07
9742,natural_language_inference50,45,We conduct extensive qualitative analysis of both the learned QA embeddings and word embeddings .,abstract,Hyper,natural_language_inference,50,42,1,0,,0.00226947,0,negative,4.65E-06,0.011092272,1.54E-06,0.000126389,0.00813569,0.000194736,8.32E-06,0.000586921,0.000259246,0.979521145,6.64E-05,1.96E-06,7.22E-07
9743,natural_language_inference50,46,We discover several interesting properties of QA embeddings in Hyperbolic space .,abstract,Hyper,natural_language_inference,50,43,1,0,,0.034889765,0,negative,1.08E-05,0.033845258,6.33E-06,5.03E-05,0.000997913,0.000192841,7.51E-05,0.00106696,0.002784809,0.947320975,0.013620229,2.26E-05,5.91E-06
9744,natural_language_inference50,47,"Due to its compositional nature , we find that our model learns to self - organize not only at the QA level but also at the word - level .",abstract,Hyper,natural_language_inference,50,44,1,0,,0.241450808,0,negative,0.000837924,0.28094826,8.54E-05,5.29E-05,0.005886595,0.000200321,0.000117148,0.001370193,0.036133597,0.673449378,0.000765548,0.000142144,1.06E-05
9745,natural_language_inference50,48,Our qualitative studies enable us to gain a better intuition pertaining to the good performance of our model .,abstract,Hyper,natural_language_inference,50,45,1,0,,0.001541414,0,negative,1.53E-05,0.000590688,1.04E-06,1.54E-05,0.00171642,5.56E-05,1.13E-05,8.19E-05,5.41E-05,0.997358371,8.84E-05,1.11E-05,4.24E-07
9746,natural_language_inference50,49,RELATED WORK,abstract,Hyper,natural_language_inference,50,46,1,0,,6.92E-05,0,negative,4.51E-07,0.0003386,5.96E-07,4.66E-06,0.00010059,5.64E-05,8.65E-06,0.000114509,5.78E-05,0.998231421,0.001085062,7.60E-07,4.70E-07
9747,natural_language_inference50,50,Many prior works have established the fact that there are mainly two key ingredients to a powerful neural ranking model .,abstract,Hyper,natural_language_inference,50,47,1,0,,0.000119092,0,negative,7.47E-07,0.001175412,1.20E-06,1.46E-05,5.53E-05,7.06E-05,1.40E-05,0.000225232,0.000179074,0.951548522,0.046712802,1.08E-06,1.37E-06
9748,natural_language_inference50,51,"First , an effective neural encoder and second , an expressive word interaction layer .",abstract,Hyper,natural_language_inference,50,48,1,0,,0.099952542,0,model,0.000122983,0.211248665,0.00093449,0.000182379,0.012517712,0.000413167,5.72E-05,0.00075422,0.410418009,0.363062813,0.000259307,1.11E-05,1.80E-05
9749,natural_language_inference50,52,"The first ingredient is often treated as a given , i.e. , the top performing models always use a neural encoder such as the CNN or LSTM .",abstract,Hyper,natural_language_inference,50,49,1,0,,0.000132311,0,negative,1.77E-06,0.000904173,1.02E-06,5.68E-05,0.000370888,0.000120805,3.03E-06,0.000258841,0.00020381,0.997943963,0.000133974,3.75E-07,5.08E-07
9750,natural_language_inference50,53,"In fact , many top performing models adopt convolutional encoders for sentence representation .",abstract,Hyper,natural_language_inference,50,50,1,0,,0.004774131,0,negative,1.81E-06,0.000885547,1.99E-06,3.51E-05,0.000191721,0.000148147,1.99E-05,0.000286359,0.00010235,0.992606212,0.005717111,1.66E-06,2.10E-06
9751,natural_language_inference50,54,The usage of recurrent models is also notable .,abstract,Hyper,natural_language_inference,50,51,1,0,,0.00351612,0,negative,4.55E-06,0.000546246,2.42E-06,3.24E-05,0.000602112,0.000123097,1.45E-05,0.000178139,7.18E-05,0.998120632,0.000300905,2.34E-06,8.74E-07
9752,natural_language_inference50,55,The key component in which many recent models differ at is at the interaction layer .,abstract,Hyper,natural_language_inference,50,52,1,0,,0.001464782,0,negative,1.72E-05,0.007898846,1.05E-05,0.000484325,0.001833778,0.000462282,2.50E-05,0.000758889,0.002140188,0.985460676,0.000899945,1.92E-06,6.41E-06
9753,natural_language_inference50,56,"Early works often combined QA embeddings ' as it is ' , i.e. , representations are learned first and then combined .",abstract,Hyper,natural_language_inference,50,53,1,0,,0.001861225,0,negative,1.54E-06,0.001460942,2.57E-06,2.56E-05,0.00022017,0.000177991,2.56E-05,0.000334465,0.000144508,0.991918405,0.005684075,1.80E-06,2.41E-06
9754,natural_language_inference50,57,"For example , Yu et al .",abstract,Hyper,natural_language_inference,50,54,1,0,,0.000100169,0,negative,3.09E-05,0.001894027,2.92E-05,3.95E-06,0.002056378,3.17E-05,1.10E-05,4.73E-05,0.000270432,0.99554848,7.10E-05,5.12E-06,4.59E-07
9755,natural_language_inference50,58,used CNN representations as feature inputs to a logistic regression model .,abstract,Hyper,natural_language_inference,50,55,1,0,,0.000145588,0,negative,8.57E-06,0.000830241,2.03E-05,0.000199642,0.00739815,0.000974136,2.19E-05,0.000360965,0.00027894,0.989880432,2.28E-05,1.04E-06,2.81E-06
9756,natural_language_inference50,59,The end - to - end CNN - based model of Severyn and Moschitti combines the CNN encoded representations of question and answer using a multi -layered perceptron ( MLP ) .,abstract,Hyper,natural_language_inference,50,56,1,0,,0.315011125,0,model,4.15E-05,0.210232162,0.005090541,1.15E-05,0.003048713,0.000289557,0.000168843,0.000437253,0.488353387,0.291856601,0.00042673,1.85E-05,2.47E-05
9757,natural_language_inference50,60,"Recently , a myriad of composition functions have been proposed as well , e.g. , tensor layers in Qiu et al .",abstract,Hyper,natural_language_inference,50,57,1,0,,0.000277066,0,negative,2.08E-06,0.001500335,4.27E-06,1.94E-05,0.000169235,0.000303613,2.44E-05,0.000788846,0.000519681,0.995860866,0.000803194,1.44E-06,2.59E-06
9758,natural_language_inference50,61,and holographic layers in Tay et al ..,abstract,Hyper,natural_language_inference,50,58,1,0,,0.000279988,0,negative,3.81E-05,0.002484896,0.000109988,0.000202738,0.007019549,0.000622457,6.06E-05,0.000315474,0.001005834,0.987970919,0.000156468,6.67E-06,6.32E-06
9759,natural_language_inference50,62,It has been recently fashionable to model the relationships between question and answer using similarity matrices .,abstract,Hyper,natural_language_inference,50,59,1,0,,0.011170876,0,negative,4.60E-07,0.001030156,1.20E-06,1.42E-05,6.60E-05,0.00010963,2.64E-05,0.000331578,0.000158972,0.975862822,0.02239495,1.15E-06,2.41E-06
9760,natural_language_inference50,63,"Intuitively , this enables more fine - grained matching across words in question and answer sentences .",abstract,Hyper,natural_language_inference,50,60,1,0,,0.030401157,0,negative,1.14E-05,0.034624033,4.60E-05,2.56E-06,0.001066433,5.21E-05,7.87E-06,0.000231453,0.036291342,0.927643769,1.78E-05,4.41E-06,9.29E-07
9761,natural_language_inference50,64,The Multi- Perspective CNN ( MP - CNN ) compared two sentences via a wide diversity of pooling functions and filter widths aiming to capture ' multi-perspectives ' between two sentences .,abstract,Hyper,natural_language_inference,50,61,1,0,,0.663975633,1,approach,5.62E-05,0.512710256,0.010334134,4.50E-05,0.026584054,0.000935239,0.000748721,0.001249258,0.115762948,0.330934348,0.000497392,6.95E-05,7.30E-05
9762,natural_language_inference50,65,The attention based neural matching ( a NMM ) model of Yang et al. performed soft - attention alignment by first measuring the pairwise word similarity between each word in question and answer .,abstract,Hyper,natural_language_inference,50,62,1,0,,0.017071288,0,negative,2.95E-05,0.055904025,0.001810302,3.80E-05,0.005004384,0.000710264,0.000582013,0.000887076,0.013556149,0.916846488,0.004550246,3.94E-05,4.21E-05
9763,natural_language_inference50,66,The attentive pooling models of Santos et al. ( AP - BiLSTM and AP - CNN ) utilized this soft - attention alignment to learn weighted representations of question and answer thatare dependent of each other .,abstract,Hyper,natural_language_inference,50,63,1,0,,0.002350111,0,negative,1.39E-05,0.0127245,0.000179616,7.92E-05,0.003137744,0.001021524,0.000122686,0.000979982,0.006219405,0.975183873,0.000318606,4.39E-06,1.45E-05
9764,natural_language_inference50,67,Zhang et al. extended AP - CNN to 3D tensor - based attentive pooling ( AI - CNN ) .,abstract,Hyper,natural_language_inference,50,64,1,0,,0.002452782,0,negative,1.43E-05,0.035216397,0.000254869,0.000108738,0.003617409,0.002076393,0.000537003,0.004811732,0.00902677,0.94301609,0.001236056,1.61E-05,6.81E-05
9765,natural_language_inference50,68,"A recent work , the Cross Temporal Recurrent Network ( CTRN ) proposed a pairwise gating mechanism for joint learning of QA pairs .",abstract,Hyper,natural_language_inference,50,65,1,0,,0.027409653,0,negative,3.36E-06,0.003571724,3.23E-05,2.33E-05,0.000280953,0.000407951,0.000158109,0.000847374,0.001345252,0.987424022,0.005886465,4.24E-06,1.50E-05
9766,natural_language_inference50,69,"Unfortunately , these models actually introduce a prohibitive computational cost to the model usually for a very marginal performance gain .",abstract,Hyper,natural_language_inference,50,66,1,0,,0.000323439,0,negative,5.62E-06,0.000538498,1.07E-06,0.00015526,0.000742224,0.000227047,1.12E-05,0.000341024,6.80E-05,0.997837772,6.92E-05,1.34E-06,1.75E-06
9767,natural_language_inference50,70,"Notably , it is easy to see that similarity matrix based matching incurs a computational cost of quadratic scale .",abstract,Hyper,natural_language_inference,50,67,1,0,,0.001064253,0,negative,8.21E-06,0.003784136,4.31E-06,5.55E-06,0.000259559,9.94E-05,2.78E-05,0.000472763,0.000620503,0.994525425,0.00018451,6.10E-06,1.82E-06
9768,natural_language_inference50,71,"Representation ability such as dimension size of word or CNN / RNN embeddings are naturally also quite restricted , i.e. , increasing any of these dimensions can cause computation or memory requirements to explode .",abstract,Hyper,natural_language_inference,50,68,1,0,,0.000436107,0,negative,1.02E-06,0.000596381,8.77E-07,7.46E-05,0.000289943,0.000261147,1.39E-05,0.000559348,7.26E-05,0.997973385,0.000154172,1.14E-06,1.48E-06
9769,natural_language_inference50,72,"Moreover , it is not uncommon for models such as AI - CNN or AP - BiLSTM to spend more than 30 minutes on a single epoch on QA datasets thatare only medium sized .",abstract,Hyper,natural_language_inference,50,69,1,0,,0.000674765,0,negative,1.32E-06,0.000478343,9.64E-07,3.46E-05,0.000643437,0.000240327,4.17E-05,0.000509774,2.06E-05,0.997792662,0.00023057,2.54E-06,3.25E-06
9770,natural_language_inference50,73,Let us not forget that these models still have to be extensively tuned which aggravates the impracticality problem posed by some of these models .,abstract,Hyper,natural_language_inference,50,70,1,0,,0.000123758,0,negative,1.63E-06,0.000194591,4.61E-07,3.31E-05,0.000333056,9.54E-05,3.48E-06,0.000140223,3.25E-05,0.999150375,1.44E-05,4.27E-07,4.11E-07
9771,natural_language_inference50,74,"In this paper , we seek a new paradigm for neural ranking for QA .",abstract,Hyper,natural_language_inference,50,71,1,0,,0.319244801,0,negative,9.04E-05,0.405492192,0.001157488,4.79E-05,0.011246401,0.000561568,0.001598563,0.001672044,0.013105135,0.559047141,0.00569816,0.000202857,8.02E-05
9772,natural_language_inference50,75,"While many recent works try to out - stack each other with new layers , we strip down our network instead .",abstract,Hyper,natural_language_inference,50,72,1,0,,0.000988984,0,negative,2.29E-05,0.013071445,5.47E-05,0.00033083,0.01403381,0.000679341,4.48E-05,0.000586155,0.000889459,0.970239434,3.61E-05,4.44E-06,6.62E-06
9773,natural_language_inference50,76,Our work is inspired by the very recent Poincar embeddings which demonstrates the superiority and efficiency of generalization in Hyperbolic space .,abstract,Hyper,natural_language_inference,50,73,1,0,,0.003110693,0,negative,7.82E-06,0.058932175,4.42E-05,0.000225458,0.015897725,0.000732273,7.50E-05,0.001400892,0.00370315,0.918900076,6.50E-05,5.35E-06,1.08E-05
9774,natural_language_inference50,77,"Moreover , this alleviates many overfitting and complexity issues that Euclidean embeddings might face especially if the data has intrinsic hierarchical structure .",abstract,Hyper,natural_language_inference,50,74,1,0,,0.025809565,0,negative,7.19E-05,0.154329251,4.61E-05,0.000224585,0.015991293,0.000925345,0.000187015,0.002694558,0.011453144,0.814007396,2.17E-05,3.27E-05,1.51E-05
9775,natural_language_inference50,78,"It is good to note that power - law distributions , such as Zipf 's law , have been known to be from innate hierarchical structure .",abstract,Hyper,natural_language_inference,50,75,1,0,,7.32E-05,0,negative,3.71E-07,0.000219144,5.39E-07,3.12E-06,7.46E-05,0.000147954,1.12E-05,0.000388472,5.23E-05,0.999079023,2.23E-05,5.17E-07,5.14E-07
9776,natural_language_inference50,79,"Specifically , the defining characteristic of Hyperbolic space is a much quicker expansion relative to that of Euclidean space which makes naturally equipped for modeling hierarchical structure .",abstract,Hyper,natural_language_inference,50,76,1,0,,0.000488359,0,negative,0.000152861,0.095626318,0.000196774,4.23E-05,0.017822946,0.00032774,0.0001221,0.000662007,0.021027097,0.863964755,1.09E-05,3.67E-05,7.50E-06
9777,natural_language_inference50,80,"The concept of Hyperbolic spaces has been applied to domains such as complex network modeling , social networks and geographic routing .",abstract,Hyper,natural_language_inference,50,77,1,0,,0.002129903,0,negative,8.64E-07,0.001336072,2.88E-06,2.28E-05,0.000178411,0.000474452,9.21E-05,0.001379979,0.000172756,0.995480798,0.000849945,2.12E-06,6.75E-06
9778,natural_language_inference50,81,There are several key geometric intuitions regarding Hyperbolic spaces .,abstract,Hyper,natural_language_inference,50,78,1,0,,1.84E-05,0,negative,3.66E-07,0.000587201,1.45E-06,9.45E-06,0.000207432,0.000294022,2.52E-05,0.00064628,9.04E-05,0.998090098,4.55E-05,8.46E-07,1.82E-06
9779,natural_language_inference50,82,"Firstly , the concept of distance and are a is warped in Hyperbolic spaces .",abstract,Hyper,natural_language_inference,50,79,1,0,,0.000257264,0,negative,7.62E-06,0.014423116,2.36E-05,7.69E-05,0.005640552,0.000552931,5.40E-05,0.001023763,0.000787686,0.97737735,2.40E-05,3.56E-06,4.92E-06
9780,natural_language_inference50,83,"Specifically , each tile in ( a ) is of equal are a in Hyperbolic space but diminishes towards zero in Euclidean space towards the boundary .",abstract,Hyper,natural_language_inference,50,80,1,0,,0.000366783,0,negative,0.00013144,0.039229336,5.96E-05,1.05E-05,0.007837571,0.000305658,7.68E-05,0.001122841,0.00771252,0.943489385,1.96E-06,1.95E-05,2.88E-06
9781,natural_language_inference50,84,"Secondly , Hyperbolic spaces are conformal , i.e. , angles in Hyperbolic spaces and Euclidean spaces are identical .",abstract,Hyper,natural_language_inference,50,81,1,0,,0.000775589,0,negative,5.04E-06,0.025304579,5.33E-05,2.94E-06,0.001992909,9.18E-05,3.45E-05,0.000371273,0.00336123,0.968762463,1.24E-05,5.99E-06,1.54E-06
9782,natural_language_inference50,85,"In ( b ) , the arcs on the curve are parallel lines thatare orthogonal to the boundary .",abstract,Hyper,natural_language_inference,50,82,1,0,,0.000227072,0,negative,1.40E-05,0.01650987,9.41E-05,2.99E-06,0.002517285,0.000167467,2.96E-05,0.000380287,0.009112137,0.971165209,1.29E-06,4.28E-06,1.57E-06
9783,natural_language_inference50,86,"Finally , hyperbolic spaces can be regarded as larger spaces relative to Euclidean spaces due to the fact that the concept of relative distance can be expressed much better , i.e. , not only does the distance between two vectors encode information but also where a vector is placed in Hyperbolic space .",abstract,Hyper,natural_language_inference,50,83,1,0,,0.000613784,0,negative,8.47E-07,0.00336395,9.98E-06,5.69E-07,0.000123988,5.92E-05,2.16E-05,0.0003014,0.001491517,0.994591667,3.13E-05,3.07E-06,9.28E-07
9784,natural_language_inference50,87,This enables efficient representation learning .,abstract,Hyper,natural_language_inference,50,84,1,0,,0.001107992,0,negative,8.09E-06,0.013411214,4.10E-05,3.26E-06,0.0018268,6.63E-05,1.92E-05,0.000145901,0.003760562,0.980705057,4.55E-06,6.43E-06,1.60E-06
9785,natural_language_inference50,88,"In Nickel et al. , the authors applied the hyperbolic distance ( specifically , the Poincar distance ) to model taxonomic entities and graph nodes .",abstract,Hyper,natural_language_inference,50,85,1,0,,0.000528981,0,negative,4.94E-06,0.009365598,8.41E-05,1.47E-05,0.001821272,0.000551712,0.000107801,0.001080057,0.001763272,0.985169325,2.56E-05,3.29E-06,8.35E-06
9786,natural_language_inference50,89,"Notably , our work , to the best of our knowledge , is the only work that learns QA embeddings in Hyperbolic space .",abstract,Hyper,natural_language_inference,50,86,1,0,,0.000692674,0,negative,4.96E-06,0.016374287,4.09E-05,9.12E-05,0.00872946,0.0007321,0.000118086,0.001379535,0.000529623,0.971969313,1.54E-05,5.88E-06,9.24E-06
9787,natural_language_inference50,90,"Moreover , questions and answers introduce an interesting layer of complexity to the problem since QA embeddings are in fact compositions of their constituent word embeddings .",abstract,Hyper,natural_language_inference,50,87,1,0,,0.001532697,0,negative,2.65E-06,0.000647289,1.77E-06,2.16E-05,0.000964146,0.000116048,1.13E-05,0.000176912,9.39E-05,0.997956088,5.92E-06,1.22E-06,1.12E-06
9788,natural_language_inference50,91,"On the other hand , nodes in a graph and taxonomic entities in are already at it s most abstract form , i.e. , symbolic objects .",abstract,Hyper,natural_language_inference,50,88,1,0,,0.000154665,0,negative,8.79E-07,0.000526259,3.43E-06,8.23E-06,0.002055961,7.09E-05,4.14E-06,6.58E-05,7.95E-05,0.99718291,1.17E-06,4.46E-07,4.02E-07
9789,natural_language_inference50,92,"As such , we believe it would be interesting to investigate the impacts of QA in Hyperbolic space in lieu of the added compositional nature .",abstract,Hyper,natural_language_inference,50,89,1,0,,0.000902391,0,negative,1.59E-06,0.000678021,1.30E-06,2.41E-06,0.000272877,0.000185808,3.26E-05,0.000717,0.000168242,0.997934371,1.98E-06,2.32E-06,1.49E-06
9790,natural_language_inference50,93,OUR PROPOSED APPROACH,abstract,Hyper,natural_language_inference,50,90,1,0,,0.000336894,0,negative,4.48E-06,0.005349636,2.03E-05,0.000183817,0.002180073,0.002889399,0.000336669,0.005532181,0.002076411,0.98133679,2.50E-05,5.40E-06,5.99E-05
9791,natural_language_inference50,94,This section outlines the over all architecture of our proposed model .,abstract,Hyper,natural_language_inference,50,91,1,0,,0.00043388,0,negative,5.43E-06,0.023106841,3.52E-05,6.00E-05,0.005439065,0.000736607,6.97E-05,0.001729107,0.012251674,0.956547754,2.58E-06,1.63E-06,1.44E-05
9792,natural_language_inference50,95,"Similar to many neural ranking models for QA , our network has ' two ' sides with shared parameters , i.e. , one for question and another for answer .",abstract,Hyper,natural_language_inference,50,92,1,0,,0.008276109,0,approach,2.77E-05,0.39881291,0.000986494,1.02E-05,0.011039342,0.000384854,0.000316582,0.001159268,0.197152478,0.39005947,5.97E-06,1.57E-05,2.90E-05
9793,natural_language_inference50,96,"However , since we optimize for a pairwise ranking loss , the model takes in a positive ( correct ) answer and a negative ( wrong ) answer and aims to maximize the margin between the scores of the correct QA pair and the negative QA pair .",abstract,Hyper,natural_language_inference,50,93,1,0,,0.000804678,0,negative,7.15E-06,0.039079332,7.55E-05,3.43E-07,0.001015574,3.17E-05,1.69E-05,0.000188677,0.027742882,0.931836265,8.10E-07,3.86E-06,9.66E-07
9794,natural_language_inference50,97,depicts the over all model architecture .,abstract,Hyper,natural_language_inference,50,94,1,0,,0.000274762,0,negative,3.41E-06,0.007984817,5.27E-05,1.89E-05,0.001299239,0.00066988,7.92E-05,0.00168292,0.015999223,0.97219198,2.01E-06,1.22E-06,1.46E-05
9795,natural_language_inference50,98,Embedding Layer,abstract,,natural_language_inference,50,95,1,0,,0.610962598,1,negative,3.80E-05,0.045468862,0.000336737,6.26E-05,0.001906592,0.002856992,0.001429381,0.042980091,0.087094055,0.817465065,1.72E-05,2.42E-05,0.000320198
9796,natural_language_inference50,99,"Our model accepts three sequences as an input , i.e. , the question ( denoted as q ) , the correct answer ( denoted as a ) and a randomly sampled corrupted answer ( denoted as a ? ) .",abstract,Embedding Layer,natural_language_inference,50,96,1,0,,0.000680479,0,negative,2.57E-07,0.000305066,3.71E-06,4.40E-08,3.11E-05,2.59E-05,7.77E-06,0.000152609,0.001712118,0.99775974,1.91E-08,4.55E-07,1.13E-06
9797,natural_language_inference50,100,Each sequence consists of M words where M q and M a are predefined maximum sequence lengths for questions and answers respectively .,abstract,Embedding Layer,natural_language_inference,50,97,1,0,,0.000140909,0,negative,5.02E-07,4.75E-05,9.98E-07,1.15E-07,0.000197446,8.69E-05,1.44E-05,0.000178155,4.07E-05,0.999431917,2.02E-09,4.67E-07,8.64E-07
9798,natural_language_inference50,101,Each word is represented as a one - hot vector ( representing a word in the vocabulary ) .,abstract,Embedding Layer,natural_language_inference,50,98,1,0,,0.001931641,0,negative,7.86E-07,0.000253368,4.24E-06,2.43E-08,1.51E-05,3.01E-05,7.13E-06,0.000136786,0.004892023,0.994659485,4.00E-09,2.22E-07,6.95E-07
9799,natural_language_inference50,102,"As such , this layer is a look - up layer that converts each word into a low - dimensional vector by indexing onto the word embedding matrix .",abstract,Embedding Layer,natural_language_inference,50,99,1,0,,0.001584428,0,negative,8.62E-06,0.00050223,3.99E-05,8.85E-07,0.000114205,9.30E-05,1.84E-05,0.000139684,0.01101253,0.98806268,1.50E-08,7.03E-07,7.11E-06
9800,natural_language_inference50,103,"In our implementation , we initialize this layer with pretrained word embeddings .",abstract,Embedding Layer,natural_language_inference,50,100,1,0,,0.006858748,0,negative,4.66E-06,0.001108864,5.91E-06,3.39E-06,0.000244496,0.002399814,0.000181168,0.015191773,0.001728272,0.979106053,9.85E-09,8.95E-07,2.47E-05
9801,natural_language_inference50,104,Note that this layer is not updated during training .,abstract,Embedding Layer,natural_language_inference,50,101,1,0,,1.23E-05,0,negative,1.01E-06,8.93E-06,3.91E-07,1.21E-08,3.57E-06,3.71E-06,4.97E-07,9.72E-06,4.93E-05,0.999922641,3.07E-10,1.52E-07,3.53E-08
9802,natural_language_inference50,105,"Instead , we utilize a projection layer that learns a task - specific projection of the embeddings .",abstract,Embedding Layer,natural_language_inference,50,102,1,0,,0.012757806,0,negative,3.25E-05,0.001927452,5.43E-05,2.20E-07,0.000153505,3.22E-05,1.66E-05,6.77E-05,0.008627587,0.989083951,5.58E-09,1.97E-06,1.97E-06
9803,natural_language_inference50,106,Projection Layer,abstract,,natural_language_inference,50,103,1,0,,0.007962833,0,negative,3.85E-05,0.016204569,0.000936137,6.75E-05,0.002356999,0.003006019,0.001654816,0.012242075,0.060129198,0.903031499,5.52E-06,2.25E-05,0.000304602
9804,natural_language_inference50,107,"In order to learn a task - specific representation for each word , we utilize a projection layer .",abstract,Projection Layer,natural_language_inference,50,104,1,0,,0.002145194,0,negative,0.000126019,0.004968022,0.000258893,6.50E-07,0.000321866,0.00011607,0.000238506,0.000271231,0.06341689,0.930254154,2.17E-08,4.86E-06,2.28E-05
9805,natural_language_inference50,108,The projection layer is essentially a single layered neural network that is applied to each word in all three sequences .,abstract,Projection Layer,natural_language_inference,50,105,1,0,,0.000362176,0,negative,2.52E-05,0.00081711,5.79E-05,5.20E-06,0.000206294,0.000414046,0.000178208,0.00080443,0.012846606,0.984599161,1.77E-08,1.15E-06,4.47E-05
9806,natural_language_inference50,109,Learning QA,abstract,,natural_language_inference,50,106,1,0,,0.617593583,1,dataset,2.46E-05,0.003836052,0.000127687,0.005898094,0.519315186,0.013909702,0.034952859,0.010747934,1.64E-05,0.409131307,3.08E-06,0.000559274,0.001477828
9807,natural_language_inference50,110,Representations,abstract,,natural_language_inference,50,107,1,0,,0.000112511,0,negative,2.80E-06,0.00160451,6.23E-05,7.62E-07,0.000320139,0.000225189,0.000528488,0.00093111,0.001258148,0.99502092,3.47E-06,2.92E-05,1.29E-05
9808,natural_language_inference50,111,"In order to learn question and answer representations , we simply take the sum of all word embeddings in the sequence .",abstract,Representations,natural_language_inference,50,108,1,0,,2.56E-06,0,negative,3.62E-06,5.64E-05,1.04E-05,6.24E-09,8.93E-06,1.58E-05,3.10E-05,2.95E-05,0.000179089,0.999663863,2.11E-10,6.95E-07,6.04E-07
9809,natural_language_inference50,112,"where * = {q , a , a ? }.",abstract,Representations,natural_language_inference,50,109,1,0,,1.66E-07,0,negative,4.74E-08,7.40E-08,8.05E-08,3.68E-10,1.68E-07,1.18E-06,1.16E-06,8.02E-07,3.33E-07,0.99999603,3.54E-11,1.09E-07,1.41E-08
9810,natural_language_inference50,113,M is the predefined max sequence length ( specific to question and answer ) and,abstract,Representations,natural_language_inference,50,110,1,0,,8.11E-07,0,negative,4.51E-07,4.96E-06,5.35E-07,2.22E-08,2.89E-06,6.01E-05,4.72E-05,0.000174451,1.65E-05,0.999691337,4.40E-10,6.42E-07,9.39E-07
9811,natural_language_inference50,114,embeddings of the sequence .,abstract,Representations,natural_language_inference,50,111,1,0,,3.48E-06,0,negative,2.85E-07,1.63E-06,2.19E-06,1.34E-09,4.49E-07,4.48E-06,9.91E-06,4.72E-06,2.72E-05,0.999948381,2.38E-10,4.81E-07,2.34E-07
9812,natural_language_inference50,115,This is essentially the neural bagof - words ( NBoW ) representation .,abstract,Representations,natural_language_inference,50,112,1,0,,0.000131347,0,negative,7.61E-06,4.80E-05,0.000842234,3.92E-09,9.17E-06,5.86E-06,7.83E-05,2.46E-06,0.000915824,0.998083344,1.66E-09,4.87E-06,2.28E-06
9813,natural_language_inference50,116,"Unlike popular neural encoders such as LSTM or CNN , the NBOW representation does not add any parameters and is much more efficient .",abstract,Representations,natural_language_inference,50,113,1,0,,5.22E-05,0,negative,0.000205573,8.91E-05,5.68E-05,2.65E-08,2.74E-05,2.85E-05,0.002019858,2.68E-05,5.68E-05,0.997150865,1.03E-09,0.000330427,7.82E-06
9814,natural_language_inference50,117,"Additionally , we constrain the question and answer embeddings to the unit ball before passing to the next layer , i.e. , ?y * ? ?",abstract,Representations,natural_language_inference,50,114,1,0,,1.53E-06,0,negative,2.59E-06,1.31E-05,3.21E-06,4.87E-09,4.90E-06,9.12E-06,1.69E-05,1.48E-05,4.79E-05,0.999886445,3.90E-11,8.14E-07,2.17E-07
9815,natural_language_inference50,118,1 . This is easily done via y * = y * ?y * ? when ?y * ? >,abstract,Representations,natural_language_inference,50,115,1,0,,3.45E-07,0,negative,3.88E-07,2.11E-07,3.12E-07,3.60E-09,1.17E-06,8.92E-06,7.76E-06,3.47E-06,5.02E-07,0.999976512,5.26E-11,6.63E-07,7.58E-08
9816,natural_language_inference50,119,1 . Note that this projection of QA embeddings onto the unit ball is mandatory and absolutely crucial for Hyper QA to even work .,abstract,Representations,natural_language_inference,50,116,1,0,,4.98E-06,0,negative,1.43E-06,3.49E-07,3.10E-07,8.18E-08,2.90E-06,2.99E-05,2.24E-05,9.85E-06,6.24E-07,0.999929492,1.68E-10,1.94E-06,7.32E-07
9817,natural_language_inference50,120,Hyperbolic Representations of QA Pairs,abstract,,natural_language_inference,50,117,1,0,,0.006523331,0,negative,1.05E-05,0.011264002,0.000144079,7.29E-07,0.000264711,0.000226474,0.004690328,0.00225285,0.003020318,0.977677034,3.30E-05,0.000337183,7.88E-05
9818,natural_language_inference50,121,Neural ranking models are mainly characterized by the interaction function between question and answer representations .,abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,118,1,0,,5.85E-06,0,negative,6.68E-07,4.35E-06,6.54E-06,2.14E-08,4.16E-06,4.23E-05,0.001004705,9.34E-06,9.09E-06,0.998904371,2.72E-08,3.43E-06,1.10E-05
9819,natural_language_inference50,122,"In our work , we mainly adopt the hyperbolic 1 distance function to model the relationships between questions and answers .",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,119,1,0,,0.000229986,0,negative,3.21E-06,0.000121168,4.20E-05,5.46E-08,5.74E-05,0.000160058,0.002211976,8.84E-05,0.000173698,0.997117912,1.21E-09,2.74E-06,2.14E-05
9820,natural_language_inference50,123,"Formally , let",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,120,1,0,,3.21E-07,0,negative,8.94E-08,5.78E-07,2.70E-07,2.68E-10,9.06E-07,2.12E-06,2.18E-05,9.93E-07,6.85E-06,0.999966148,1.07E-11,1.97E-07,6.90E-08
9821,natural_language_inference50,124,"be the open d-dimensional unit ball , our model corresponds to the Riemannian manifold ( B d , ? x ) and is equipped with the Riemannian metric tensor given as follows :",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,121,1,0,,1.21E-06,0,negative,3.15E-07,4.14E-06,1.34E-06,4.71E-09,1.03E-05,1.89E-05,0.000119505,8.68E-06,9.43E-06,0.999826062,3.89E-11,4.42E-07,8.81E-07
9822,natural_language_inference50,125,where ?,abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,122,1,0,,1.07E-07,0,negative,6.13E-08,1.87E-07,1.16E-07,2.51E-09,8.23E-07,9.80E-06,2.13E-05,2.28E-06,1.32E-06,0.999963856,2.62E-11,8.70E-08,1.51E-07
9823,natural_language_inference50,126,E is the Euclidean metric tensor .,abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,123,1,0,,6.07E-06,0,negative,6.11E-07,3.25E-06,8.43E-07,1.58E-08,8.58E-06,6.60E-05,0.000163715,3.56E-05,1.30E-05,0.999706416,2.76E-11,2.24E-07,1.63E-06
9824,natural_language_inference50,127,The hyperbolic distance function between question and answer is defined as :,abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,124,1,0,,8.71E-07,0,negative,2.25E-07,1.47E-06,1.19E-06,1.62E-09,1.99E-06,1.38E-05,7.57E-05,6.60E-06,1.63E-05,0.99988196,5.25E-11,1.99E-07,6.49E-07
9825,natural_language_inference50,128,where ?.?,abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,125,1,0,,4.35E-08,0,negative,4.73E-08,8.90E-08,7.55E-08,4.11E-09,1.63E-06,9.26E-06,1.37E-05,1.30E-06,3.46E-07,0.999973342,5.77E-12,6.95E-08,1.08E-07
9826,natural_language_inference50,129,"denotes the Euclidean norm and q , a ?",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,126,1,0,,1.44E-06,0,negative,9.47E-08,8.73E-07,2.35E-07,1.72E-09,1.61E-06,9.74E-06,4.44E-05,4.61E-06,3.33E-06,0.999934727,1.76E-11,1.37E-07,2.36E-07
9827,natural_language_inference50,130,Rd are the question and answer embeddings respectively .,abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,127,1,0,,6.58E-07,0,negative,1.24E-07,6.01E-06,3.11E-06,4.62E-09,4.12E-06,3.90E-05,0.000508686,2.82E-05,2.27E-05,0.999381074,3.05E-10,6.62E-07,6.31E-06
9828,natural_language_inference50,131,"Note that arcosh is the inverse hyperbolic cosine function , i.e. , arcoshx = ln ( x + ( x 2 ? 1 ) ) .",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,128,1,0,,9.34E-07,0,negative,2.72E-07,1.30E-06,7.99E-07,3.60E-09,6.00E-06,3.05E-05,0.000120178,7.89E-06,4.25E-06,0.999827963,1.56E-11,2.63E-07,5.54E-07
9829,natural_language_inference50,132,"Notably , d ( q , a ) changes smoothly with respect to the position of q and a which enables the automatic discovery of latent hierarchies .",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,129,1,0,,0.000200723,0,negative,5.76E-05,2.48E-05,7.50E-06,9.01E-09,2.34E-05,8.18E-06,0.001005192,3.58E-06,3.30E-05,0.99881636,8.46E-11,1.79E-05,2.51E-06
9830,natural_language_inference50,133,"As mentioned earlier , the distance increases exponentially as the norm of the vectors approaches 1 . As such , the latent hierarchies of QA embeddings are captured through the norm of the vectors .",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,130,1,0,,1.97E-05,0,negative,5.57E-05,1.64E-05,2.87E-06,6.95E-08,2.78E-05,0.000184039,0.009928332,0.000116302,1.88E-05,0.989584341,2.19E-10,1.89E-05,4.65E-05
9831,natural_language_inference50,134,"From a geometric perspective , the origin can be seen as the root of a tree that branches out towards the boundaries of the hyperbolic ball .",abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,131,1,0,,1.38E-06,0,negative,1.94E-07,2.19E-06,1.05E-06,3.04E-09,4.12E-06,8.27E-06,3.48E-05,3.52E-06,2.97E-05,0.999915532,1.14E-11,1.58E-07,5.46E-07
9832,natural_language_inference50,135,This self - organizing ability of the hyperbolic distance is visually and qualitatively analyzed in later sections .,abstract,Hyperbolic Representations of QA Pairs,natural_language_inference,50,132,1,0,,8.33E-07,0,negative,9.37E-07,6.44E-07,5.40E-07,5.97E-10,2.57E-06,2.80E-06,5.59E-05,7.33E-07,1.97E-06,0.999933008,3.57E-12,7.38E-07,1.46E-07
9833,natural_language_inference50,136,Gradient Derivation .,abstract,,natural_language_inference,50,133,1,0,,0.002846701,0,negative,1.34E-06,0.00227993,1.44E-05,1.25E-06,0.000508858,0.000507012,0.00012468,0.004199465,0.000480048,0.99187659,4.45E-09,7.48E-07,5.70E-06
9834,natural_language_inference50,137,"Amongst the other models of Hyperbolic geometry , the hyperbolic Poincar distance is differentiable .",abstract,Gradient Derivation .,natural_language_inference,50,134,1,0,,6.63E-05,0,negative,3.10E-06,0.000362111,6.74E-05,1.19E-07,0.000896764,0.000564332,0.00197529,4.09E-05,0.000168574,0.995913714,3.12E-09,1.21E-06,6.51E-06
9835,natural_language_inference50,138,Let,abstract,,natural_language_inference,50,135,1,0,,2.36E-05,0,negative,1.16E-07,6.51E-05,4.50E-07,1.34E-07,7.75E-05,7.23E-05,1.75E-05,0.00081675,2.31E-05,0.998926137,9.08E-10,3.81E-07,5.25E-07
9836,natural_language_inference50,139,The partial derivate w.r.t to ?,abstract,Let,natural_language_inference,50,136,1,0,,7.20E-05,0,negative,4.23E-07,5.53E-05,2.16E-06,1.48E-07,0.000310993,0.000182665,4.47E-05,0.000180671,1.91E-05,0.999202352,3.55E-10,3.25E-07,1.15E-06
9837,natural_language_inference50,140,is defined as :,abstract,Let,natural_language_inference,50,137,1,0,,0.000377203,0,negative,1.09E-07,0.00011612,1.43E-06,6.53E-08,8.84E-05,7.73E-05,2.14E-05,0.000323176,7.36E-05,0.99929731,1.54E-09,1.43E-07,9.87E-07
9838,natural_language_inference50,141,Similarity Scoring Layer,abstract,,natural_language_inference,50,138,1,0,,0.012556123,0,negative,1.45E-05,0.003884034,0.000383003,1.11E-05,0.00147693,0.002599776,0.002411413,0.014098268,0.00777607,0.966955532,5.31E-08,1.27E-05,0.000376601
9839,natural_language_inference50,142,"Finally , we pass the hyperbolic distance through a linear transformation described as follows :",abstract,Similarity Scoring Layer,natural_language_inference,50,139,1,0,,2.54E-06,0,negative,1.14E-06,7.75E-06,1.00E-05,9.78E-10,3.41E-06,2.05E-06,6.38E-06,2.94E-06,0.000118946,0.999846808,4.63E-12,3.44E-07,1.86E-07
9840,natural_language_inference50,143,where w f ?,abstract,Similarity Scoring Layer,natural_language_inference,50,140,1,0,,2.32E-08,0,negative,3.99E-08,1.29E-07,3.53E-08,1.25E-10,2.09E-07,5.60E-07,8.76E-07,1.03E-06,2.87E-07,0.999996725,9.18E-13,1.01E-07,9.92E-09
9841,natural_language_inference50,144,R 1 and bf ?,abstract,Similarity Scoring Layer,natural_language_inference,50,141,1,0,,1.41E-06,0,negative,2.47E-06,4.49E-06,7.96E-07,2.60E-08,1.38E-05,0.000127791,0.000468681,0.000245647,3.76E-06,0.999121324,3.43E-11,4.42E-06,6.79E-06
9842,natural_language_inference50,145,R 1 are scalar parameters of this layer .,abstract,Similarity Scoring Layer,natural_language_inference,50,142,1,0,,2.22E-07,0,negative,2.03E-07,2.55E-06,1.73E-07,3.00E-09,2.95E-06,1.05E-05,7.94E-06,4.51E-05,6.78E-06,0.999923489,1.48E-12,1.54E-07,1.55E-07
9843,natural_language_inference50,146,"The performance of this layer is empirically motivated by its performance and was selected amongst other variants such as exp ( ? d ( q , a ) ) , non-linear activations such as sigmoid function or the raw hyperbolic distance .",abstract,Similarity Scoring Layer,natural_language_inference,50,143,1,0,,3.24E-07,0,negative,3.26E-07,1.26E-05,2.22E-06,1.67E-08,1.63E-05,8.09E-05,3.95E-05,0.000228689,3.07E-05,0.99958737,2.38E-12,1.50E-07,1.23E-06
9844,natural_language_inference50,147,Optimization and Learning,abstract,,natural_language_inference,50,144,1,0,,0.003640132,0,negative,5.22E-07,0.0007667,1.07E-05,1.29E-05,0.000872046,0.001195787,0.001179854,0.009815869,4.57E-05,0.98602148,8.77E-08,6.68E-06,7.17E-05
9845,natural_language_inference50,148,This section describes the optimization and learning process of Hyper QA .,abstract,Optimization and Learning,natural_language_inference,50,145,1,0,,0.000123266,0,negative,1.76E-06,7.04E-05,6.87E-06,2.36E-08,0.000207918,3.90E-05,0.000304018,2.95E-05,1.59E-05,0.999319812,3.04E-11,1.36E-06,3.50E-06
9846,natural_language_inference50,149,"Our model learns via a pairwise ranking loss , which is well suited for metric - based learning algorithms .",abstract,Optimization and Learning,natural_language_inference,50,146,1,0,,0.000349757,0,negative,9.79E-06,0.000729014,0.000113418,2.76E-08,0.000201346,8.58E-05,0.000629743,6.03E-05,0.000823864,0.997334446,4.46E-11,1.59E-06,1.07E-05
9847,natural_language_inference50,150,3.6.1 Pairwise Hinge Loss .,abstract,Optimization and Learning,natural_language_inference,50,147,1,0,,0.000331568,0,negative,2.23E-05,7.70E-06,2.88E-05,9.11E-10,1.26E-05,1.26E-05,0.000955496,2.57E-06,6.04E-06,0.998942808,8.16E-12,7.33E-06,1.75E-06
9848,natural_language_inference50,151,Our network minimizes the pairwise hinge loss which is defined as follows :,abstract,Optimization and Learning,natural_language_inference,50,148,1,0,,0.000164356,0,negative,8.11E-07,2.20E-05,1.23E-05,2.15E-09,1.79E-05,1.97E-05,8.70E-05,1.42E-05,5.67E-05,0.999767812,5.70E-12,1.99E-07,1.44E-06
9849,natural_language_inference50,152,where ?,abstract,Optimization and Learning,natural_language_inference,50,149,1,0,,9.92E-07,0,negative,3.04E-08,1.55E-07,8.12E-08,1.67E-09,1.31E-06,1.67E-05,1.29E-05,5.23E-06,3.61E-07,0.999963026,1.75E-12,2.65E-08,1.31E-07
9850,natural_language_inference50,153,"q is the set of all QA pairs for question q , s ( q , a ) is the score between q and a , and ?",abstract,Optimization and Learning,natural_language_inference,50,150,1,0,,6.27E-06,0,negative,6.62E-08,1.42E-06,4.91E-07,5.28E-09,2.18E-05,2.20E-05,4.58E-05,1.06E-05,4.85E-07,0.999896462,3.66E-12,1.01E-07,7.66E-07
9851,natural_language_inference50,154,is the margin which controls the extent of discrimination between positive QA pairs and corrupted QA pairs .,abstract,Optimization and Learning,natural_language_inference,50,151,1,0,,0.000181442,0,negative,1.74E-06,5.50E-06,2.71E-06,8.67E-09,4.24E-05,4.21E-05,0.000102003,2.50E-05,3.85E-06,0.999773033,2.26E-12,3.07E-07,1.38E-06
9852,natural_language_inference50,155,The adoption of the pairwise hinge loss is motivated by the good empirical results demonstrated in Rao et al ..,abstract,Optimization and Learning,natural_language_inference,50,152,1,0,,2.75E-05,0,negative,5.46E-07,7.88E-06,3.32E-06,1.32E-08,3.12E-05,0.000165801,0.00036726,7.54E-05,3.27E-06,0.999340841,7.85E-12,2.96E-07,4.17E-06
9853,natural_language_inference50,156,"Additionally , we also adopt the mix sampling strategy for sampling negative samples as described in their work .",abstract,Optimization and Learning,natural_language_inference,50,153,1,0,,2.68E-05,0,negative,4.18E-06,0.000107688,5.46E-05,1.15E-07,0.00050204,0.00040739,0.000792312,0.000172099,6.33E-05,0.99788195,3.23E-12,5.18E-07,1.37E-05
9854,natural_language_inference50,157,3.6.2 Gradient Conversion .,abstract,Optimization and Learning,natural_language_inference,50,154,1,0,,3.95E-05,0,negative,5.94E-06,1.87E-06,1.31E-05,5.09E-10,6.86E-06,1.03E-05,0.000219824,1.71E-06,4.48E-06,0.999734119,1.23E-12,1.11E-06,6.83E-07
9855,natural_language_inference50,158,"Since our network learns in Hyperbolic space , parameters have to be learned via stochastic Riemannian optimization methods such as RSGD .",abstract,Optimization and Learning,natural_language_inference,50,155,1,0,,7.78E-05,0,negative,1.88E-06,6.20E-06,8.25E-07,2.00E-08,5.10E-05,0.000135699,0.000143946,7.72E-05,2.77E-06,0.999578386,1.03E-12,2.07E-07,1.86E-06
9856,natural_language_inference50,159,where ? ?,abstract,Optimization and Learning,natural_language_inference,50,156,1,0,,4.89E-07,0,negative,1.97E-08,8.14E-08,5.15E-08,1.08E-09,1.34E-06,1.44E-05,1.23E-05,3.72E-06,1.41E-07,0.999967865,4.08E-13,2.06E-08,1.07E-07
9857,natural_language_inference50,160,t denotes a retraction onto B at ? . ?,abstract,Optimization and Learning,natural_language_inference,50,157,1,0,,6.70E-06,0,negative,5.28E-08,1.69E-07,1.31E-07,7.27E-10,2.40E-06,9.31E-06,2.33E-05,3.58E-06,1.41E-07,0.999960702,3.20E-13,5.55E-08,1.97E-07
9858,natural_language_inference50,161,is the learning rate and ?,abstract,Optimization and Learning,natural_language_inference,50,158,1,0,,0.00015412,0,negative,4.86E-07,8.13E-06,8.82E-07,4.48E-08,3.50E-05,0.000828245,0.001259221,0.001390716,6.45E-06,0.996450613,4.30E-12,2.78E-07,1.99E-05
9859,natural_language_inference50,162,R ?(? t ) is the Riemannian gradient with respect to ? t .,abstract,Optimization and Learning,natural_language_inference,50,159,1,0,,1.61E-05,0,negative,1.62E-07,8.49E-07,6.86E-07,1.54E-09,7.70E-06,2.86E-05,7.36E-05,1.15E-05,6.68E-07,0.999875523,5.67E-13,8.52E-08,6.55E-07
9860,natural_language_inference50,163,"Fortunately , the Riemannian gradient can be easily derived from the Euclidean gradient in this case .",abstract,Optimization and Learning,natural_language_inference,50,160,1,0,,1.51E-05,0,negative,8.49E-08,1.24E-06,4.58E-07,6.86E-10,2.35E-06,9.97E-06,4.06E-05,9.06E-06,1.84E-06,0.999933748,1.53E-12,9.31E-08,5.18E-07
9861,natural_language_inference50,164,"In order to do so , we can simply scale the Euclidean gradient by the inverse of the metric tensor ? ? 1 ? .",abstract,Optimization and Learning,natural_language_inference,50,161,1,0,,9.96E-06,0,negative,5.40E-07,1.97E-06,9.49E-07,6.50E-09,1.33E-05,3.40E-05,3.54E-05,1.40E-05,3.06E-06,0.99989602,4.15E-13,5.19E-08,7.42E-07
9862,natural_language_inference50,165,"Overall , the final gradients used to update the parameters are :",abstract,Optimization and Learning,natural_language_inference,50,162,1,0,,0.000715853,0,negative,5.17E-06,6.48E-06,3.77E-06,2.99E-09,2.73E-05,6.21E-05,0.000601481,2.65E-05,8.84E-06,0.999254691,5.35E-13,6.57E-07,3.05E-06
9863,natural_language_inference50,166,"Due to the lack of space , we refer interested readers to for more details .",abstract,Optimization and Learning,natural_language_inference,50,163,1,0,,8.14E-06,0,negative,3.52E-08,1.27E-07,5.87E-08,5.18E-09,4.53E-06,2.43E-05,1.25E-05,5.40E-06,1.27E-07,0.999952751,9.14E-14,2.14E-08,1.29E-07
9864,natural_language_inference50,167,"For practical purposes , we simply utilize the automatic gradient feature of Tensor Flow but convert the gradients with Equation before updating the parameters .",abstract,Optimization and Learning,natural_language_inference,50,164,1,0,,0.000218875,0,negative,8.29E-06,3.45E-05,1.44E-05,3.74E-08,0.000214371,9.95E-05,0.000276314,3.22E-05,1.28E-05,0.999303599,3.78E-13,5.53E-07,3.43E-06
9865,natural_language_inference50,168,EXPERIMENTS,,,natural_language_inference,50,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
9866,natural_language_inference50,169,This section describes our empirical evaluation and its results .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,50,1,1,0,,0.000835646,0,negative,0.000164234,7.13E-05,9.07E-05,1.46E-05,4.27E-05,0.001066337,0.000555743,0.00641089,1.31E-05,0.990831589,2.72E-05,0.000669251,4.24E-05
9867,natural_language_inference50,170,Datasets,EXPERIMENTS,,natural_language_inference,50,2,1,0,,0.00081385,0,negative,3.26E-05,1.84E-05,0.000125621,3.75E-06,1.28E-06,0.003393241,0.000768251,0.01928633,2.40E-05,0.975581952,0.000355785,0.000347768,6.11E-05
9868,natural_language_inference50,171,"In the spirit of experimental rigor , we conduct our empirical evaluation based on four popular and well - studied benchmark datasets for question answering .",EXPERIMENTS,Datasets,natural_language_inference,50,3,1,0,,0.003253795,0,negative,2.90E-05,0.000185474,2.43E-05,1.88E-06,1.73E-05,0.000148018,1.63E-05,0.000336341,7.67E-06,0.999121346,8.00E-06,0.000103893,4.09E-07
9869,natural_language_inference50,172,YahooCQA,EXPERIMENTS,,natural_language_inference,50,4,1,1,baselines,0.003356061,0,experiments,0.000888438,6.86E-05,0.001516185,0.000432733,0.000248353,0.002928212,0.671454627,0.006846451,7.72E-06,0.083377901,0.008892142,0.202535861,0.020802764
9870,natural_language_inference50,173,- This is a benchmark dataset for communitybased question answering that was collected from Yahoo Answers .,EXPERIMENTS,YahooCQA,natural_language_inference,50,5,1,0,,0.07887555,0,tasks,0.001059679,1.73E-06,0.000286849,0.000554832,0.000464037,0.000566566,0.054026451,4.29E-06,2.52E-06,0.448743229,1.67E-05,0.001275688,0.492997391
9871,natural_language_inference50,174,"In this dataset , the answer lengths are relatively longer than TrecQA and Wiki QA .",EXPERIMENTS,YahooCQA,natural_language_inference,50,6,1,0,,0.067566959,0,negative,0.001113628,2.16E-07,5.80E-05,2.58E-06,2.69E-05,6.78E-05,0.078605433,1.77E-06,1.95E-07,0.893658323,1.75E-06,0.007803212,0.018660238
9872,natural_language_inference50,175,"Therefore , we filtered answers that have more than 50 words and less than 5 characters .",EXPERIMENTS,YahooCQA,natural_language_inference,50,7,1,0,,0.047676835,0,negative,0.001537043,8.33E-07,6.13E-05,2.43E-06,1.94E-05,0.000308462,0.021492607,1.56E-05,3.45E-06,0.969219732,7.26E-07,0.000303029,0.007035485
9873,natural_language_inference50,176,The train - dev - test splits for this dataset are provided by . WikiQA - This is a recently popular benchmark dataset for open - domain question answering based on factual questions from Wikipedia and Bing search logs . SemEvalCQA,EXPERIMENTS,YahooCQA,natural_language_inference,50,8,1,0,,0.10422785,0,negative,0.000367845,3.71E-07,0.000130811,6.84E-05,4.69E-05,0.000427138,0.095999161,4.94E-06,6.44E-07,0.477770106,5.88E-06,0.001616304,0.423561499
9874,natural_language_inference50,177,- This is a well - studied benchmark dataset from SemEval - 2016 Task 3 Subtask A ( CQA ) .,EXPERIMENTS,YahooCQA,natural_language_inference,50,9,1,0,,0.117748886,0,negative,0.000738239,7.46E-07,0.000179805,8.52E-05,0.000113232,0.000191806,0.058374694,2.15E-06,8.26E-07,0.495402422,2.08E-05,0.002165308,0.442724826
9875,natural_language_inference50,178,This is are al world dataset obtained from Qatar Living Forums .,EXPERIMENTS,YahooCQA,natural_language_inference,50,10,1,0,,0.03727077,0,negative,0.001012412,8.89E-07,0.000423096,1.40E-05,9.76E-05,0.000157759,0.045301332,3.14E-06,1.48E-06,0.873864588,2.77E-06,0.002892883,0.076227981
9876,natural_language_inference50,179,"In this dataset , there are ten answers in each question ' thread ' which are marked as ' Good ' , ' Potentially Useful ' or ''Bad ' .",EXPERIMENTS,YahooCQA,natural_language_inference,50,11,1,0,,0.009126004,0,negative,0.000179217,5.29E-07,6.35E-05,3.64E-06,4.44E-05,0.000165255,0.013030339,5.77E-06,1.09E-06,0.970813236,4.68E-07,0.00022702,0.015465519
9877,natural_language_inference50,180,We treat ' Good ' as positive and anything else as negative labels . TrecQA,EXPERIMENTS,YahooCQA,natural_language_inference,50,12,1,0,,0.015412503,0,negative,0.000707956,2.46E-07,0.000152977,1.01E-05,1.33E-05,0.000258608,0.137653696,5.07E-06,6.78E-07,0.5275979,3.25E-06,0.00436829,0.329227868
9878,natural_language_inference50,181,- This is the benchmark dataset provided by Wang et al ..,EXPERIMENTS,YahooCQA,natural_language_inference,50,13,1,0,,0.017369505,0,negative,0.000225895,7.28E-08,5.64E-05,1.11E-06,1.24E-05,3.58E-05,0.007504756,7.27E-07,1.43E-07,0.988612412,1.36E-07,0.000610334,0.002939866
9879,natural_language_inference50,182,"This dataset was collected from TREC QA tracks 8 - 13 and is comprised of factoid based questions which mainly answer the ' who' , ' what ' , ' where ' , ' when ' and ' why ' types of questions .",EXPERIMENTS,YahooCQA,natural_language_inference,50,14,1,0,,0.023373797,0,negative,0.000297912,9.21E-07,0.000133974,5.03E-05,0.000239912,0.000278944,0.023528536,3.50E-06,1.14E-06,0.870251818,2.95E-06,0.000650738,0.10455939
9880,natural_language_inference50,183,"There are two versions , namely clean and raw , as noted by which we evaluate our models on .",EXPERIMENTS,YahooCQA,natural_language_inference,50,15,1,0,,0.070508953,0,negative,0.000455298,7.99E-07,0.000837231,2.42E-06,1.10E-05,0.000140943,0.051220819,4.47E-06,3.16E-06,0.91974261,1.32E-06,0.001341028,0.026238898
9881,natural_language_inference50,184,Statistics pertaining to each dataset is given in .,EXPERIMENTS,YahooCQA,natural_language_inference,50,16,1,0,,0.003815254,0,negative,0.000134263,6.29E-08,4.57E-06,2.16E-07,5.93E-07,4.29E-05,0.007481984,3.18E-06,4.87E-07,0.990689486,1.84E-07,0.000318853,0.001323204
9882,natural_language_inference50,185,Compared Baselines,,,natural_language_inference,50,0,1,0,,0.007299438,0,negative,2.12E-05,7.14E-05,8.13E-05,1.27E-07,2.94E-07,9.07E-05,0.000140822,0.001385433,5.12E-05,0.995638665,0.001204706,0.00131247,1.67E-06
9883,natural_language_inference50,186,"In this section , we introduce the baselines for comparison .",Compared Baselines,Compared Baselines,natural_language_inference,50,1,1,0,,0.028403305,0,negative,3.68E-05,9.14E-07,0.120705281,4.90E-08,2.70E-08,1.28E-05,4.47E-05,0.000272003,7.00E-07,0.877745645,9.67E-07,0.001178283,1.80E-06
9884,natural_language_inference50,187,YahooCQA,Compared Baselines,,natural_language_inference,50,2,1,1,baselines,0.066676143,0,results,0.000528861,2.90E-06,0.333642388,3.13E-05,2.10E-06,0.000192209,0.073180237,0.00063661,8.49E-07,0.078353585,0.001067575,0.505878172,0.00648326
9885,natural_language_inference50,188,- The key competitors of this dataset are the Neural Tensor LSTM ( NTN - LSTM ) and HD - LSTM from Tay et al.,Compared Baselines,YahooCQA,natural_language_inference,50,3,1,1,baselines,0.978013526,1,negative,0.000415915,8.76E-08,0.368222267,5.84E-07,1.37E-07,2.89E-05,0.001229731,1.73E-05,2.36E-07,0.596427647,1.16E-06,0.033541065,0.000115042
9886,natural_language_inference50,189,"along with their implementation of the Convolutional Neural Tensor Network , vanilla CNN model , and the Okapi BM - 25 benchmark .",Compared Baselines,YahooCQA,natural_language_inference,50,4,1,1,baselines,0.011163462,0,negative,7.37E-05,2.36E-08,0.012505203,3.70E-07,4.31E-08,2.15E-05,0.000232207,1.89E-05,1.12E-07,0.978722079,5.74E-07,0.008361555,6.37E-05
9887,natural_language_inference50,190,"Additionally , we also report our own implementations of QA - BiLSTM , QA - CNN , AP - BiLSTM and AP - CNN on this dataset based on our experimental setup . WikiQA",Compared Baselines,YahooCQA,natural_language_inference,50,5,1,1,baselines,0.275161172,0,negative,0.000428646,1.20E-07,0.070029331,7.95E-06,6.22E-07,7.11E-05,0.003287312,2.80E-05,3.83E-07,0.83423388,4.00E-06,0.089989551,0.001919169
9888,natural_language_inference50,191,"- The key competitors of this dataset are the Paragraph Vector ( PV ) and PV + Cnt models of Le and Mikolv , CNN + Cnt model from Yu et al. and LCLR ( Yih et al . ) .",Compared Baselines,YahooCQA,natural_language_inference,50,6,1,1,baselines,0.688853519,1,negative,0.000465393,6.76E-08,0.12748907,1.04E-06,2.82E-07,3.05E-05,0.001215454,1.54E-05,1.96E-07,0.829767234,6.90E-07,0.040849077,0.000165634
9889,natural_language_inference50,192,These three baselines are reported in the original Wik - iQA paper .,Compared Baselines,YahooCQA,natural_language_inference,50,7,1,0,,0.0173995,0,negative,2.93E-05,1.19E-07,0.061786393,1.41E-07,3.35E-08,2.29E-05,0.000187231,4.91E-05,6.05E-07,0.934560306,1.39E-06,0.003323383,3.91E-05
9890,natural_language_inference50,193,"Additionally and due to longstanding nature of this dataset , there have been a huge number of works based on traditional feature engineering approaches which we also report .",Compared Baselines,YahooCQA,natural_language_inference,50,8,1,0,,0.014076771,0,negative,6.06E-05,4.26E-08,0.001725051,6.25E-07,4.29E-08,1.81E-05,9.06E-05,1.92E-05,1.33E-07,0.991470788,4.64E-06,0.006466562,0.000143549
9891,natural_language_inference50,194,"For the clean version of this dataset , we also compare with AP - CNN and QA - BiLSTM / CNN .",Compared Baselines,YahooCQA,natural_language_inference,50,9,1,1,baselines,0.435139668,0,negative,7.31E-05,1.07E-07,0.04030488,2.87E-08,1.88E-08,6.68E-06,0.000483453,2.04E-05,9.94E-08,0.936382222,2.89E-07,0.022719172,9.48E-06
9892,natural_language_inference50,195,"Since the training splits are standard , we are able to directly report the results from the original papers .",Compared Baselines,YahooCQA,natural_language_inference,50,10,1,0,,0.017728274,0,negative,5.62E-05,4.79E-08,0.001339662,9.79E-08,3.27E-08,6.61E-06,6.03E-05,1.86E-05,1.34E-07,0.990705921,1.25E-07,0.007796038,1.63E-05
9893,natural_language_inference50,196,Evaluation Protocol,,,natural_language_inference,50,0,1,0,,0.005207174,0,negative,1.26E-05,3.74E-05,2.98E-06,3.13E-07,7.41E-07,3.08E-05,2.91E-05,0.000589147,1.40E-05,0.998662665,0.000419084,0.000200158,9.64E-07
9894,natural_language_inference50,197,This section describes the key evaluation protocol / metrics and implementation details of our experiments .,Evaluation Protocol,Evaluation Protocol,natural_language_inference,50,1,1,0,,1.21E-05,0,negative,8.80E-05,2.82E-05,0.000141952,3.65E-06,4.37E-06,0.000178115,7.21E-05,0.001355798,1.10E-05,0.99800951,6.24E-06,9.60E-05,5.09E-06
9895,natural_language_inference50,198,Metrics .,Evaluation Protocol,,natural_language_inference,50,2,1,0,,0.000400543,0,negative,7.98E-05,2.41E-06,0.000944829,1.02E-07,1.84E-07,0.000205296,0.000219074,0.000919875,1.63E-06,0.997006808,9.17E-06,0.00060785,2.96E-06
9896,natural_language_inference50,199,We adopt a dataset specific evaluation protocol in which we follow the prior work in their evaluation protocols .,Evaluation Protocol,Metrics .,natural_language_inference,50,3,1,0,,5.14E-06,0,negative,0.000196326,0.000164104,0.00155152,1.17E-06,1.34E-05,0.000189191,4.32E-05,0.000558837,4.84E-05,0.99711595,1.43E-05,0.000100337,3.29E-06
9897,natural_language_inference50,200,"Specifically , TrecQA and WikiQA adopt the Mean Reciprocal Rank ( MRR ) and MAP ( Mean Average Precision ) metrics which are commonplace in IR research .",Evaluation Protocol,Metrics .,natural_language_inference,50,4,1,0,,0.001743761,0,negative,9.64E-05,1.95E-05,0.001541073,1.13E-06,1.47E-06,0.000182494,0.000541369,0.000321865,3.22E-06,0.986671139,0.010097703,0.000492889,2.97E-05
9898,natural_language_inference50,201,"On the other hand , YahooCQA and Se - m EvalCQA evaluate on MAP and Precision@1 ( abbreviated P@1 ) which is determined based on whether the top predicted answer is the ground truth .",Evaluation Protocol,Metrics .,natural_language_inference,50,5,1,0,,0.000407332,0,negative,5.71E-05,6.24E-06,0.000297534,5.47E-07,1.30E-06,7.13E-05,0.000172016,0.000149829,7.32E-07,0.996827531,0.002079343,0.000326389,1.01E-05
9899,natural_language_inference50,202,"For all competitor methods , we report the performance results from the original paper .",Evaluation Protocol,Metrics .,natural_language_inference,50,6,1,0,,2.19E-05,0,negative,3.52E-05,1.26E-06,2.55E-05,7.82E-08,8.79E-07,5.31E-05,2.11E-05,9.59E-05,4.97E-07,0.999666063,6.42E-07,9.95E-05,2.56E-07
9900,natural_language_inference50,203,Training Time & Parameter,,,natural_language_inference,50,0,1,0,,0.009047856,0,negative,5.74E-05,0.001376042,1.96E-05,7.94E-06,3.03E-06,0.001361057,0.000130922,0.038831341,0.00113307,0.952767368,0.004197954,9.45E-05,1.98E-05
9901,natural_language_inference50,204,Size .,Training Time & Parameter,,natural_language_inference,50,1,1,0,,0.000401917,0,hyperparameters,6.44E-06,0.000248862,2.98E-05,1.59E-06,1.16E-06,0.009475937,4.41E-05,0.550829212,0.000187816,0.439006286,0.000153785,1.08E-05,4.21E-06
9902,natural_language_inference50,205,"Additionally , we report the parameter size and runtime ( seconds per epoch ) of selected models .",Training Time & Parameter,Size .,natural_language_inference,50,2,1,0,,4.52E-05,0,negative,1.92E-05,0.000185121,4.73E-06,7.46E-06,1.76E-05,0.002436592,1.22E-05,0.033703751,3.01E-05,0.963555682,1.13E-05,1.52E-05,1.10E-06
9903,natural_language_inference50,206,We selectively re-implement some of the key competitors with the best performance and benchmark their training time on our machine / GPU ( a single Nvidia GTX1070 ) .,Training Time & Parameter,Size .,natural_language_inference,50,3,1,0,,0.036498063,0,negative,8.04E-05,0.001592096,0.000123608,6.68E-05,0.000138828,0.084586351,0.000300683,0.395311299,0.000151351,0.517596624,1.61E-05,2.45E-05,1.14E-05
9904,natural_language_inference50,207,"For reporting the parameter size and training time , we try our best to follow the hyperparameters stated in the original papers .",Training Time & Parameter,Size .,natural_language_inference,50,4,1,0,,7.51E-06,0,negative,2.43E-05,0.000251385,5.64E-06,1.29E-05,1.13E-05,0.014104229,2.46E-05,0.217761674,5.07E-05,0.767734808,7.77E-06,8.78E-06,1.85E-06
9905,natural_language_inference50,208,"As such , the same model can have different training time and parameter size on different datasets .",Training Time & Parameter,Size .,natural_language_inference,50,5,1,0,,1.50E-05,0,negative,1.17E-05,0.000151153,4.71E-06,9.17E-07,1.24E-06,0.000307111,3.29E-06,0.009740994,4.65E-05,0.989558464,0.000162429,1.06E-05,7.56E-07
9906,natural_language_inference50,209,Hyperparameters .,,,natural_language_inference,50,0,1,0,,0.005675136,0,negative,2.99E-05,0.001575046,1.54E-05,1.90E-06,1.24E-06,0.002471029,8.40E-05,0.146311514,0.001686393,0.847159282,0.000609562,4.72E-05,7.56E-06
9907,natural_language_inference50,210,Hyper QA is implemented in Tensor - Flow .,Hyperparameters .,Hyperparameters .,natural_language_inference,50,1,1,1,hyperparameters,0.993923244,1,hyperparameters,5.61E-06,6.74E-05,0.000123014,1.69E-05,5.04E-07,0.013609876,0.000133672,0.981524955,2.66E-05,0.004469313,8.03E-06,1.86E-06,1.22E-05
9908,natural_language_inference50,211,"We adopt the AdaGrad optimizer with initial learning rate tuned amongst { 0.2 , 0.1 , 0.05 , 0.01 } .",Hyperparameters .,Hyperparameters .,natural_language_inference,50,2,1,1,hyperparameters,0.997234175,1,hyperparameters,7.75E-07,1.35E-05,2.18E-06,1.41E-06,6.53E-08,0.002917446,1.95E-05,0.996192491,3.40E-06,0.000846047,1.01E-06,3.58E-07,1.84E-06
9909,natural_language_inference50,212,"The batch size is tuned amongst { 50 , 100 , 200 } .",Hyperparameters .,Hyperparameters .,natural_language_inference,50,3,1,1,hyperparameters,0.994825217,1,hyperparameters,6.86E-07,1.39E-05,1.38E-06,6.71E-07,6.30E-08,0.00189589,1.22E-05,0.996427902,2.97E-06,0.00164077,1.30E-06,6.08E-07,1.62E-06
9910,natural_language_inference50,213,Models are trained for 25 epochs and the model parameters are saved each time the performance on the validation set is topped .,Hyperparameters .,Hyperparameters .,natural_language_inference,50,4,1,1,hyperparameters,0.965420971,1,hyperparameters,1.45E-06,2.60E-05,4.97E-06,5.92E-07,1.08E-07,0.002172807,1.74E-05,0.994625758,6.48E-06,0.003139666,1.79E-06,6.67E-07,2.27E-06
9911,natural_language_inference50,214,"The dimension of the projection layer is tuned amongst { 100 , 200 , 300 , 400 } .",Hyperparameters .,Hyperparameters .,natural_language_inference,50,5,1,1,hyperparameters,0.994963895,1,hyperparameters,7.69E-07,1.37E-05,1.67E-06,4.46E-07,5.79E-08,0.001626977,1.13E-05,0.996990073,3.57E-06,0.001348205,1.02E-06,6.43E-07,1.56E-06
9912,natural_language_inference50,215,"L2 regularization is tuned amongst { 0.001 , 0.0001 , 0.00001 }.",Hyperparameters .,Hyperparameters .,natural_language_inference,50,6,1,1,hyperparameters,0.99562811,1,hyperparameters,5.79E-07,1.13E-05,9.67E-07,2.89E-07,3.89E-08,0.001206935,6.57E-06,0.997468462,2.89E-06,0.001299681,7.11E-07,4.85E-07,1.09E-06
9913,natural_language_inference50,216,The negative sampling rate is tuned from 2 to 8 .,Hyperparameters .,Hyperparameters .,natural_language_inference,50,7,1,1,hyperparameters,0.992466873,1,hyperparameters,5.83E-07,1.15E-05,1.25E-06,3.92E-07,4.89E-08,0.001216979,8.11E-06,0.997474268,2.98E-06,0.001281483,6.67E-07,4.79E-07,1.29E-06
9914,natural_language_inference50,217,"Finally , the margin ?",Hyperparameters .,Hyperparameters .,natural_language_inference,50,8,1,0,,0.887389692,1,negative,1.56E-05,3.49E-05,0.000131132,1.04E-06,5.94E-07,0.000628435,1.35E-05,0.12828934,2.55E-05,0.870777809,4.49E-05,3.37E-05,3.49E-06
9915,natural_language_inference50,218,"is tuned amongst { 1 , 2 , 5 , 10 , 20 }.",Hyperparameters .,Hyperparameters .,natural_language_inference,50,9,1,0,,0.875761637,1,hyperparameters,1.32E-06,2.17E-05,1.84E-06,6.31E-07,1.24E-07,0.001509407,9.58E-06,0.993423502,4.14E-06,0.005022889,1.18E-06,1.90E-06,1.81E-06
9916,natural_language_inference50,219,"For TrecQA , Wiki QA and YahooCQA , we initialize the embedding layer with GloVe and use the version with d = 300 and trained on 840 billion words .",Hyperparameters .,Hyperparameters .,natural_language_inference,50,10,1,0,,0.995707519,1,hyperparameters,2.03E-06,4.54E-05,1.42E-05,1.10E-06,3.12E-07,0.004646864,6.26E-05,0.991104564,4.77E-06,0.00410831,1.81E-06,3.63E-06,4.40E-06
9917,natural_language_inference50,220,"For SemEvalCQA , we train our own Skipgram model using the unannotated corpus provided by the task .",Hyperparameters .,Hyperparameters .,natural_language_inference,50,11,1,0,,0.857367522,1,hyperparameters,0.000620779,0.006437525,0.038148803,0.00011442,0.000580399,0.009620735,0.002643487,0.667926903,0.000214153,0.270940598,7.83E-05,0.002452731,0.000221132
9918,natural_language_inference50,221,"In this case , the embedding dimension is tuned amongst { 100 , 200 , 300 } .",Hyperparameters .,Hyperparameters .,natural_language_inference,50,12,1,0,,0.985102531,1,hyperparameters,8.07E-07,1.52E-05,1.16E-06,4.14E-07,8.71E-08,0.001479532,1.01E-05,0.996636985,2.51E-06,0.00185006,5.05E-07,1.10E-06,1.50E-06
9919,natural_language_inference50,222,Embeddings are not updated during training .,Hyperparameters .,Hyperparameters .,natural_language_inference,50,13,1,0,,0.081604385,0,hyperparameters,1.21E-05,0.000117511,8.64E-05,2.96E-06,1.09E-06,0.001754701,1.08E-05,0.828529631,8.92E-05,0.169379382,3.04E-06,6.49E-06,6.72E-06
9920,natural_language_inference50,223,"For the SemEvalCQA dataset , we concatenated the raw QA embeddings before passing into the final layer since we found that it improves performance .",Hyperparameters .,Hyperparameters .,natural_language_inference,50,14,1,0,,0.979314512,1,negative,0.003987179,0.001032064,0.013426286,7.51E-06,4.79E-05,0.004664678,0.000804584,0.460522261,6.02E-05,0.500111531,1.53E-05,0.015244987,7.56E-05
9921,natural_language_inference50,224,Results and Analysis,,,natural_language_inference,50,0,1,0,,0.004153659,0,negative,4.92E-05,4.74E-05,7.12E-06,1.41E-07,3.63E-07,1.90E-05,8.00E-05,0.000295611,1.21E-05,0.995144313,0.001609051,0.002734443,1.25E-06
9922,natural_language_inference50,225,"In this section , we present our empirical results on all datasets .",Results and Analysis,Results and Analysis,natural_language_inference,50,1,1,0,,0.016640683,0,negative,0.00091588,2.91E-06,4.12E-05,6.12E-07,6.01E-07,1.71E-05,0.000110687,9.49E-05,1.20E-06,0.965598528,2.61E-06,0.033207695,6.07E-06
9923,natural_language_inference50,226,"For all reported results , the best result is in boldface and the second best is underlined .",Results and Analysis,Results and Analysis,natural_language_inference,50,2,1,0,,0.000301416,0,negative,9.80E-05,1.29E-06,4.83E-05,6.31E-07,1.97E-07,3.81E-05,3.56E-05,0.000178664,3.22E-06,0.996978343,4.89E-06,0.002606184,6.53E-06
9924,natural_language_inference50,227,reports the experimental results on SemEvalCQA .,Results and Analysis,Results and Analysis,natural_language_inference,50,3,1,1,results,0.148502058,0,negative,0.02752909,5.19E-06,0.017024697,9.03E-06,4.35E-06,4.26E-05,0.00092329,5.07E-05,8.45E-06,0.691729648,6.77E-05,0.262464778,0.000140531
9925,natural_language_inference50,228,Our proposed approach achieves highly competitive performance on this dataset .,Results and Analysis,Results and Analysis,natural_language_inference,50,4,1,1,results,0.554748777,1,results,0.000726077,4.51E-08,1.32E-05,3.48E-08,2.00E-08,9.73E-07,0.000246983,4.22E-06,1.12E-08,0.01800123,9.99E-07,0.980998898,7.32E-06
9926,natural_language_inference50,229,"Specifically , we have obtained the best P@1 performance over all , outperforming the state - of - the - art AI - CNN model by 3 % in terms of P@1 .",Results and Analysis,Results and Analysis,natural_language_inference,50,5,1,1,results,0.964193435,1,results,0.002859888,3.60E-07,3.32E-05,4.82E-07,1.65E-07,4.93E-06,0.000794304,2.36E-05,1.30E-07,0.022909142,2.42E-06,0.973324051,4.73E-05
9927,natural_language_inference50,230,The performance of our model on MAP is marginally short from the best performing model .,Results and Analysis,Results and Analysis,natural_language_inference,50,6,1,1,results,0.956606518,1,results,0.002326298,7.76E-08,2.64E-05,1.71E-07,5.46E-08,1.99E-06,0.000510188,7.05E-06,3.55E-08,0.014449013,1.48E-06,0.982648131,2.91E-05
9928,natural_language_inference50,231,"Notably , AI - CNN has benefited from external handcrafted features .",Results and Analysis,Results and Analysis,natural_language_inference,50,7,1,0,,0.407089646,0,negative,0.004080603,5.04E-06,0.001337773,5.06E-06,1.28E-06,7.23E-05,0.000593517,0.000177381,4.44E-06,0.841400401,0.000181174,0.151944057,0.000197012
9929,natural_language_inference50,232,"As such , comparing AI - CNN ( w/ o features ) with Hyper QA shows that our proposed model is a superior neural ranking model .",Results and Analysis,Results and Analysis,natural_language_inference,50,8,1,0,,0.938614435,1,results,0.005982131,1.88E-07,3.06E-05,7.71E-08,6.19E-08,2.71E-06,0.000264737,1.39E-05,6.04E-08,0.084046809,1.01E-06,0.909651912,5.75E-06
9930,natural_language_inference50,233,"Next , we draw the readers attention to the time cost of AI - CNN .",Results and Analysis,Results and Analysis,natural_language_inference,50,9,1,0,,0.000651895,0,negative,0.001355478,2.44E-06,8.91E-05,4.49E-07,3.26E-07,7.77E-06,1.01E-05,4.80E-05,4.96E-06,0.994377987,1.24E-06,0.004098004,4.16E-06
9931,natural_language_inference50,234,The training time per epoch is ?,Results and Analysis,Results and Analysis,natural_language_inference,50,10,1,0,,0.309037985,0,negative,0.001590466,2.39E-05,0.000265007,4.39E-06,1.66E-06,0.002133119,0.001549249,0.029302681,3.80E-05,0.953498507,3.88E-05,0.011255048,0.000299202
9932,natural_language_inference50,235,3250s per epoch which is about 300 times longer than our model .,Results and Analysis,Results and Analysis,natural_language_inference,50,11,1,0,,0.010294158,0,negative,0.10714549,2.47E-05,0.00310744,3.71E-05,1.71E-05,0.000313067,0.002181555,0.00096512,5.83E-05,0.627001052,2.81E-05,0.258084733,0.001036252
9933,natural_language_inference50,236,"AI - CNN is extremely cost prohibitive , i.e. , attentive pooling is already very expensive and yet AI - CNN performs 3 D attentive pooling .",Results and Analysis,Results and Analysis,natural_language_inference,50,12,1,0,,0.087057774,0,negative,0.003314601,1.37E-05,0.001999841,6.06E-05,3.63E-06,0.000496465,0.003100715,0.000933062,2.46E-05,0.795591132,0.001917214,0.188558428,0.003985985
9934,natural_language_inference50,237,"Evidently , its performance can be easily superseded in a much smaller training time and parameter cost .",Results and Analysis,Results and Analysis,natural_language_inference,50,13,1,0,,0.005954499,0,negative,0.019908789,3.65E-06,0.000239194,1.18E-06,8.91E-07,1.04E-05,7.20E-05,4.98E-05,6.73E-06,0.834752956,3.53E-06,0.144930061,2.09E-05
9935,natural_language_inference50,238,This raises questions about the effectiveness of the 3D attentive pooling mechanism .,Results and Analysis,Results and Analysis,natural_language_inference,50,14,1,0,,0.000793764,0,negative,0.000320458,5.59E-07,2.45E-05,2.14E-07,1.05E-07,7.61E-06,1.40E-05,4.00E-05,1.64E-06,0.994096318,3.91E-06,0.005485034,5.62E-06
9936,natural_language_inference50,239,reports the results on TrecQA ( raw ) .,Results and Analysis,Results and Analysis,natural_language_inference,50,15,1,1,results,0.744974367,1,results,0.003727705,5.27E-07,0.001225644,8.70E-07,1.24E-06,7.80E-06,0.001114114,1.22E-05,3.32E-07,0.238855107,6.36E-06,0.754951519,9.66E-05
9937,natural_language_inference50,240,Hyper QA achieves very competitive performance on both MAP and MRR metrics .,Results and Analysis,Results and Analysis,natural_language_inference,50,16,1,1,results,0.901057528,1,results,0.001091298,7.99E-08,5.71E-05,6.14E-08,3.28E-08,9.00E-07,0.000514274,3.80E-06,2.22E-08,0.009048726,1.65E-06,0.989253458,2.86E-05
9938,natural_language_inference50,241,"Specifically , Hyper QA outperforms the basic CNN model of ( S&M ) by 2 % ? 3 % in terms of MAP / MRR .",Results and Analysis,Results and Analysis,natural_language_inference,50,17,1,1,results,0.97741106,1,results,0.011968178,3.41E-07,3.97E-05,6.29E-07,1.95E-07,3.46E-06,0.000930014,1.74E-05,1.35E-07,0.017404328,1.02E-06,0.96954406,9.05E-05
9939,natural_language_inference50,242,"Moreover , the CNN ( S&M ) model uses handcrafted features which Hyper QA does not require .",Results and Analysis,Results and Analysis,natural_language_inference,50,18,1,0,,0.003066891,0,negative,0.002918784,3.93E-06,0.001736593,1.85E-06,1.14E-06,2.55E-05,0.000176919,6.36E-05,3.75E-06,0.932036389,3.75E-05,0.062907746,8.63E-05
9940,natural_language_inference50,243,"Similarly , the a NMM model and HD - LSTM also benefit from additional features but are outperformed by Hyper QA .",Results and Analysis,Results and Analysis,natural_language_inference,50,19,1,0,,0.973623994,1,results,0.052764901,2.14E-07,4.42E-05,6.34E-07,1.51E-07,3.91E-06,0.000636149,1.55E-05,1.79E-07,0.026689837,4.86E-07,0.919798418,4.54E-05
9941,natural_language_inference50,244,Hyper QA also outperforms MP - CNN but is around 10 times faster and has 100 times less parameters .,Results and Analysis,Results and Analysis,natural_language_inference,50,20,1,0,,0.461412775,0,results,0.010592568,3.65E-06,0.000573447,1.82E-05,2.82E-06,0.000240999,0.009547863,0.000756054,3.31E-06,0.115673527,1.13E-05,0.860429382,0.002146824
9942,natural_language_inference50,245,MP - CNN consists of a huge number of filter banks and utilizes heavy parameterization to match multiple perspectives of questions and answers .,Results and Analysis,Results and Analysis,natural_language_inference,50,21,1,0,,0.05974196,0,negative,0.008092519,6.32E-05,0.02141061,7.00E-05,1.43E-05,0.000391264,0.001901648,0.001007152,8.39E-05,0.854687216,0.000365814,0.107475824,0.004436609
9943,natural_language_inference50,246,"On the other hand , our proposed HyperQA is merely a single layered neural network with 90K parameters and yet outperforms MP - CNN .",Results and Analysis,Results and Analysis,natural_language_inference,50,22,1,0,,0.793991614,1,results,0.043588903,7.43E-06,0.000439669,4.34E-06,2.21E-06,3.89E-05,0.00168016,0.000244196,3.64E-06,0.135086899,3.76E-06,0.818564034,0.000335904
9944,natural_language_inference50,247,"Similarly , reports the results on TrecQA ( clean ) .",Results and Analysis,Results and Analysis,natural_language_inference,50,23,1,1,results,0.15260255,0,negative,0.00136581,5.32E-07,0.000773504,1.94E-07,6.53E-07,4.91E-06,0.000209051,1.67E-05,4.00E-07,0.806523595,1.39E-06,0.191078796,2.45E-05
9945,natural_language_inference50,248,"Similarly , Hyper QA also outperforms MP - CNN , AP - CNN and QA - CNN .",Results and Analysis,Results and Analysis,natural_language_inference,50,24,1,1,results,0.977641804,1,results,0.003003446,6.67E-08,1.83E-05,1.37E-07,5.57E-08,1.44E-06,0.000635115,6.19E-06,2.56E-08,0.011462685,3.39E-07,0.984829906,4.23E-05
9946,natural_language_inference50,249,"On both datasets , the performance of HyperQA is competitive to Rank MP - CNN .",Results and Analysis,Results and Analysis,natural_language_inference,50,25,1,0,,0.95508521,1,results,0.003376401,9.12E-08,1.27E-05,1.11E-07,5.61E-08,1.35E-06,0.00054981,7.72E-06,3.01E-08,0.016352288,2.53E-07,0.979665665,3.35E-05
9947,natural_language_inference50,250,Experimental Results on WikiQA .,Results and Analysis,Results and Analysis,natural_language_inference,50,26,1,0,,0.1309153,0,results,0.000424834,1.11E-07,5.03E-05,2.22E-08,3.13E-08,7.03E-07,0.000504445,3.71E-06,2.61E-08,0.044342048,7.73E-07,0.954656268,1.68E-05
9948,natural_language_inference50,251,Overall analysis .,Results and Analysis,Results and Analysis,natural_language_inference,50,27,1,0,,0.000767399,0,negative,0.000878238,3.48E-07,5.01E-05,6.17E-08,1.16E-07,4.39E-06,1.99E-05,2.86E-05,1.28E-06,0.984867558,1.93E-07,0.014144063,5.12E-06
9949,natural_language_inference50,252,"Overall , we summarize the key findings of our experiments .",Results and Analysis,Results and Analysis,natural_language_inference,50,28,1,0,,0.006023057,0,negative,0.002642669,3.69E-06,3.04E-05,1.30E-05,2.66E-06,6.58E-05,0.000112103,0.000302319,1.05E-05,0.97428983,1.30E-06,0.022416461,0.000109232
9950,natural_language_inference50,253,"It is possible to achieve very competitive performance with small parameterization , and no word matching or interaction layers .",Results and Analysis,Results and Analysis,natural_language_inference,50,29,1,0,,0.062544017,0,negative,0.009076788,1.84E-06,0.000234311,7.32E-07,5.85E-07,7.92E-06,0.000239993,5.41E-05,3.37E-06,0.53823883,2.22E-06,0.452039683,9.96E-05
9951,natural_language_inference50,254,Hyper QA outperforms complex models such as MP - CNN and AP - BiLSTM on multiple datasets .,Results and Analysis,Results and Analysis,natural_language_inference,50,30,1,0,,0.935990756,1,results,0.0029409,2.51E-07,2.25E-05,5.98E-07,1.59E-07,6.51E-06,0.001147181,3.80E-05,1.28E-07,0.031940313,6.05E-07,0.963729201,0.000173652
9952,natural_language_inference50,255,"The relative performance of HyperQA is significantly better on large datasets , e.g. , YahooCQA ( 253K training pairs ) as opposed to smaller ones like WikiQA ( 5.9 K training pairs ) .",Results and Analysis,Results and Analysis,natural_language_inference,50,31,1,0,,0.722906158,1,results,0.001008659,9.16E-08,1.23E-05,7.66E-08,5.60E-08,1.00E-06,0.000515108,5.98E-06,2.68E-08,0.024316144,3.09E-07,0.974098535,4.17E-05
9953,natural_language_inference50,256,We believe that this is due to the fact that Hyperbolic space is seemingly larger than Euclidean space . Hyper,Results and Analysis,Results and Analysis,natural_language_inference,50,32,1,0,,8.38E-05,0,negative,0.000705282,5.09E-07,4.67E-05,3.79E-07,4.07E-07,8.10E-06,1.29E-05,4.25E-05,1.91E-06,0.996192749,1.43E-07,0.002978111,1.04E-05
9954,natural_language_inference50,257,QA is extremely fast and trains at 10 ? 20 times faster than complex models like MP - CNN .,Results and Analysis,Results and Analysis,natural_language_inference,50,33,1,0,,0.054554165,0,results,0.002495266,6.11E-06,0.000392752,1.61E-05,3.08E-06,6.43E-05,0.003308451,0.000196216,3.27E-06,0.47875373,0.000265129,0.507811609,0.006684033
9955,natural_language_inference50,258,"Note that if CPUs are used instead of GPUs ( which speed convolutions up significantly ) , this disparity would be significantly larger . In this section , we study the effects of the QA embedding size on performance .",Results and Analysis,Results and Analysis,natural_language_inference,50,34,1,0,,0.040477844,0,negative,0.004343712,9.35E-06,0.000108571,5.11E-07,1.07E-06,8.75E-06,7.82E-05,0.000121872,6.54E-06,0.951501694,4.21E-07,0.043801269,1.80E-05
9956,natural_language_inference50,259,describes the relationship between QA embedding size ( d ) and MAP on the WikiQA dataset .,Results and Analysis,Results and Analysis,natural_language_inference,50,35,1,0,,0.005519454,0,negative,0.005138413,8.33E-06,0.001244596,8.63E-06,3.04E-06,2.58E-05,0.000283586,0.000149711,1.58E-05,0.888600856,1.28E-05,0.103923784,0.00058464
9957,natural_language_inference50,260,"Additionally , we include a simple baseline ( Cosine QA ) which is exactly the same as Hyper QA but uses cosine similarity instead of hyperbolic distance .",Results and Analysis,Results and Analysis,natural_language_inference,50,36,1,0,,0.26050107,0,negative,0.01045709,0.000108036,0.074119454,2.48E-06,1.09E-05,4.87E-05,0.000822901,0.000282376,9.80E-05,0.838300443,2.80E-06,0.07542729,0.000319513
9958,natural_language_inference50,261,"The MAP scores of three other reported models ( MP - CNN , CNN - Cnt and PV - Cnt ) are also reported for reference .",Results and Analysis,Results and Analysis,natural_language_inference,50,37,1,0,,0.011949056,0,negative,9.79E-05,2.10E-07,2.55E-05,3.43E-08,1.07E-07,2.52E-06,2.01E-05,2.65E-05,3.09E-07,0.991794489,3.71E-08,0.008028702,3.65E-06
9959,natural_language_inference50,262,"Firstly , we notice the disparity between Hyper QA and Cosine QA in terms of performance .",Results and Analysis,Results and Analysis,natural_language_inference,50,38,1,0,,0.152084555,0,negative,0.009581207,1.02E-06,0.000179185,1.38E-07,2.98E-07,1.96E-06,8.04E-05,1.40E-05,8.10E-07,0.672709405,4.56E-07,0.317415145,1.60E-05
9960,natural_language_inference50,263,This is also observed across other datasets but is not reported due to the lack of space .,Results and Analysis,Results and Analysis,natural_language_inference,50,39,1,0,,0.000147605,0,negative,0.000588179,4.26E-07,2.63E-05,5.17E-07,3.88E-07,8.49E-06,3.08E-05,5.70E-05,8.26E-07,0.986689427,1.52E-07,0.01257678,2.07E-05
9961,natural_language_inference50,264,While Cosine,Results and Analysis,,natural_language_inference,50,40,1,0,,0.065369268,0,negative,0.000302453,1.16E-06,0.001790798,1.76E-07,3.45E-07,1.61E-05,0.000139839,7.97E-05,6.29E-06,0.978309855,8.29E-07,0.019260898,9.16E-05
9962,natural_language_inference50,265,"QA maintains a stable performance throughout embedding size , the performance of Hyper QA rapidly improves at d > 150 . In fact , the performance of Hyper QA at d = 150 ( 45 K parameters ) is already similar to the Multi - Perspective CNN which contains 10 million parameters .",Results and Analysis,While Cosine,natural_language_inference,50,41,1,0,,0.113541935,0,results,0.002336749,8.62E-08,1.29E-05,5.05E-07,4.30E-08,4.09E-06,0.000362953,2.41E-05,1.77E-07,0.149064964,1.71E-07,0.844775548,0.0034178
9963,natural_language_inference50,266,"Moreover , the performance of Hyper QA outperforms MP - CNN with d = 250-300 .",Results and Analysis,While Cosine,natural_language_inference,50,42,1,0,,0.267692862,0,results,0.006526117,3.10E-08,6.33E-06,4.04E-07,2.43E-08,1.75E-06,0.000214662,6.54E-06,5.69E-08,0.066398299,3.06E-08,0.925519358,0.001326392
9964,natural_language_inference50,267,Effects of QA Embedding Size,Results and Analysis,,natural_language_inference,50,43,1,0,,0.459046464,0,results,0.128062887,5.28E-07,8.31E-05,7.51E-07,4.73E-07,3.40E-06,0.00051356,1.63E-05,4.96E-07,0.220089634,1.72E-07,0.651160695,6.80E-05
9965,natural_language_inference50,268,DISCUSSION AND ANALYSIS,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,44,1,0,,8.17E-06,0,negative,0.000319043,6.71E-08,5.92E-06,9.94E-08,7.10E-08,3.39E-06,5.98E-06,6.66E-06,5.34E-07,0.98919115,4.64E-08,0.010426803,4.02E-05
9966,natural_language_inference50,269,This section delves into qualitative analysis of our model and aims to investigate the following research questions :,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,45,1,0,,1.72E-05,0,negative,0.000392222,4.02E-07,1.64E-05,3.33E-07,2.77E-07,2.93E-06,2.53E-06,9.51E-06,1.65E-06,0.99298064,9.81E-08,0.006547013,4.60E-05
9967,natural_language_inference50,270,( 1 ) RQ1 : Is there any hierarchical structure learned in the QA embeddings ?,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,46,1,0,,0.000209616,0,negative,0.00025008,2.15E-07,7.80E-06,5.84E-07,1.39E-07,6.96E-06,6.47E-06,2.42E-05,8.28E-07,0.990145237,2.88E-07,0.009435511,0.000121665
9968,natural_language_inference50,271,How are QA embeddings organized in the final embedding space of Hyper QA ?,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,47,1,0,,9.24E-06,0,negative,9.61E-05,4.36E-07,9.32E-06,2.84E-07,7.84E-08,5.55E-06,4.95E-06,2.85E-05,2.48E-06,0.995040881,6.01E-07,0.004682316,0.000128552
9969,natural_language_inference50,272,( 2 ) RQ2 : What are the impacts of embedding compositional embeddings in hyperbolic space ?,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,48,1,0,,0.000274337,0,negative,0.000218715,1.56E-07,6.92E-06,1.37E-07,6.61E-08,3.09E-06,4.59E-06,1.38E-05,6.65E-07,0.989877662,1.13E-07,0.009813733,6.03E-05
9970,natural_language_inference50,273,Is there an impact on the constituent word embeddings ?,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,49,1,0,,2.69E-05,0,negative,0.000188127,1.94E-07,4.26E-06,1.43E-07,4.98E-08,3.87E-06,4.64E-06,3.60E-05,1.41E-06,0.995419966,3.66E-08,0.004291043,5.02E-05
9971,natural_language_inference50,274,( 3 ) RQ3 : Are we able to derive any insight about how word interaction and matching happens in Hyper QA ?.,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,50,1,0,,0.000435589,0,negative,0.000296928,1.70E-07,6.18E-06,2.60E-07,1.24E-07,5.79E-06,1.06E-05,2.48E-05,6.47E-07,0.980672947,1.29E-07,0.01878933,0.00019209
9972,natural_language_inference50,275,QA embeddings are extracted from the network as discussed in Section 3.3 .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,51,1,0,,9.87E-05,0,negative,0.000508289,2.57E-06,6.16E-05,8.04E-07,7.56E-07,2.75E-05,1.54E-05,0.000598121,1.53E-05,0.996229427,1.23E-08,0.002308284,0.000231811
9973,natural_language_inference50,276,We observe that question embeddings form a ' sphere ' over answer embeddings .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,52,1,0,,0.000161335,0,negative,0.003778481,9.40E-07,3.07E-05,2.44E-07,2.76E-07,2.24E-06,7.33E-06,1.37E-05,4.96E-06,0.958074724,3.31E-08,0.038027765,5.86E-05
9974,natural_language_inference50,277,"Contrastingly , this is not exhibited when the cosine similarity is used as shown in ( b ) .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,53,1,0,,1.86E-05,0,negative,0.00177138,8.34E-08,4.34E-06,2.99E-08,5.03E-08,8.16E-07,5.30E-06,6.08E-06,2.37E-07,0.955263303,1.02E-08,0.042936903,1.15E-05
9975,natural_language_inference50,278,It is important to note that these are embeddings from the test set which have not been trained and therefore the model is not explicitly told whether a particular textual input is a question or answer .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,54,1,0,,1.24E-05,0,negative,0.00025616,4.57E-08,4.31E-06,4.84E-08,1.78E-07,1.26E-06,9.27E-07,4.20E-06,1.63E-07,0.998014417,7.90E-10,0.00171485,3.43E-06
9976,natural_language_inference50,279,This demonstrates the innate ability of Hyper QA to self - organize and learn latent hierarchies which directly answers RQ1 .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,55,1,0,,1.17E-05,0,negative,0.000430586,6.96E-08,9.19E-06,2.28E-08,5.65E-08,4.72E-07,1.67E-06,1.46E-06,3.66E-07,0.985939333,9.03E-09,0.013607525,9.24E-06
9977,natural_language_inference50,280,"Additionally , ( a ) shows a histogram of the vector norms of question and answer embeddings .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,56,1,0,,0.000101884,0,negative,0.00029177,1.76E-07,1.48E-05,7.95E-08,1.16E-07,2.25E-06,4.99E-06,1.72E-05,1.12E-06,0.989502226,6.40E-09,0.010104281,6.10E-05
9978,natural_language_inference50,281,We can clearly see that questions in general have a higher vector norm 2 and are at a different hierarchical level from answers .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,57,1,0,,0.000110382,0,negative,0.003788938,8.42E-08,6.58E-06,5.92E-08,1.11E-07,1.29E-06,1.63E-05,5.56E-06,1.86E-07,0.86138544,7.16E-09,0.134768084,2.73E-05
9979,natural_language_inference50,282,"In order to further understand what the model is doing , we delve deeper into the visualization at word - level .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,58,1,0,,2.90E-05,0,negative,0.000653541,2.84E-07,9.48E-06,1.86E-08,9.69E-08,5.37E-07,7.60E-07,3.17E-06,1.62E-06,0.995679772,2.14E-09,0.003646592,4.12E-06
9980,natural_language_inference50,283,shows some examples of words at each hierarchical level of the sphere on TrecQA .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,59,1,0,,8.68E-06,0,negative,6.24E-05,2.00E-08,5.67E-06,5.55E-08,1.80E-07,1.57E-06,2.39E-06,3.28E-06,7.30E-08,0.99600847,1.25E-09,0.003887379,2.85E-05
9981,natural_language_inference50,284,Recall that the vector norms 3 allow us to infer the distance of the word embedding from the origin which depicts its hierarchical level in our context .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,60,1,0,,1.87E-05,0,negative,0.000254325,1.35E-07,3.07E-05,6.21E-09,2.53E-08,2.68E-07,6.02E-07,1.60E-06,1.32E-06,0.994448693,2.21E-09,0.00525983,2.45E-06
9982,natural_language_inference50,285,"Interestingly , we found that HyperQA exhibits self - organizing ability even at word - level .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,61,1,0,,0.030036957,0,results,0.006907105,1.60E-07,7.26E-06,5.03E-08,8.37E-08,9.89E-07,5.78E-05,5.45E-06,2.01E-07,0.328542491,1.49E-08,0.664381116,9.72E-05
9983,natural_language_inference50,286,"Specifically , we notice that the words closer to the origin are common words such as ' to ' , ' and ' which do not have much semantic values for QA problems .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,62,1,0,,0.000232626,0,negative,0.005750743,7.38E-08,8.08E-06,3.31E-08,1.20E-07,7.79E-07,6.94E-06,3.22E-06,1.48E-07,0.928232857,1.62E-09,0.065985887,1.11E-05
9984,natural_language_inference50,287,"At the middle of the hierarchy (?w ? ? 3 ) , we notice that there are more verbs .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,63,1,0,,2.29E-05,0,negative,0.000491303,3.58E-08,5.67E-06,1.08E-07,1.99E-07,2.40E-06,3.98E-06,5.30E-06,1.57E-07,0.992678582,1.58E-09,0.006776872,3.54E-05
9985,natural_language_inference50,288,"Finally , as we move towards the surface of the ' sphere ' , the words become rarer and reflect more domain - specific words such as ' ebay ' and ' spielberg ' .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,64,1,0,,1.35E-05,0,negative,0.000894284,5.79E-08,6.59E-06,1.61E-07,4.27E-07,1.40E-06,2.00E-06,3.34E-06,2.02E-07,0.993638079,1.30E-09,0.005434453,1.90E-05
9986,natural_language_inference50,289,"Moreover , we also found many names and proper nouns occurring at this hierarchical level .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,65,1,0,,0.000116769,0,negative,0.00050815,4.85E-08,4.64E-06,1.70E-07,6.80E-07,1.96E-06,3.32E-06,5.32E-06,1.50E-07,0.994007889,7.30E-10,0.005432037,3.56E-05
9987,natural_language_inference50,290,"Additionally , we also observe that words such as ' where ' or ' what ' have relatively high vector norms and located quite high up in the hierarchy .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,66,1,0,,0.000561965,0,negative,0.011118359,2.40E-07,6.80E-06,3.95E-07,4.56E-07,4.16E-06,6.33E-05,2.41E-05,2.79E-07,0.778233055,3.96E-09,0.210337048,0.000211721
9988,natural_language_inference50,291,This is in concert with which shows the question embeddings form a sphere around the answer embeddings .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,67,1,0,,1.35E-05,0,negative,0.000317168,1.62E-07,1.92E-05,2.64E-08,6.02E-08,6.61E-07,1.32E-06,4.02E-06,2.43E-06,0.997322032,1.32E-09,0.002316923,1.60E-05
9989,natural_language_inference50,292,"At last , we parsed QA pairs word - by - word according to hierarchical level ( based on their vector norm ) .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,68,1,0,,0.001248276,0,negative,0.001175031,5.29E-06,0.000443624,1.56E-07,2.69E-06,4.00E-06,2.49E-05,1.88E-05,1.07E-05,0.980008133,6.70E-09,0.018063068,0.000243598
9990,natural_language_inference50,293,reports the outcome of this experiment where H 1 ?,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,69,1,0,,1.89E-06,0,negative,0.000422663,9.34E-08,2.45E-05,1.31E-08,1.22E-07,4.01E-07,4.37E-06,1.16E-06,1.39E-07,0.975899946,3.68E-09,0.023633959,1.26E-05
9991,natural_language_inference50,294,H 5 are hierarchical levels based on vector norms .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,70,1,0,,0.000155595,0,negative,0.001016901,3.80E-07,0.001023362,7.50E-08,2.70E-07,2.33E-06,6.80E-06,7.93E-06,6.23E-06,0.990992657,3.50E-09,0.00685658,8.65E-05
9992,natural_language_inference50,295,"First , we find that questions often start with the over all context and drill down into more specific query words .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,71,1,0,,0.000415973,0,negative,0.000962312,1.28E-07,6.29E-06,2.30E-07,4.61E-07,2.39E-06,1.44E-05,8.86E-06,9.72E-08,0.96127364,2.73E-09,0.037593412,0.000137769
9993,natural_language_inference50,296,"Take the first sample in for example , it begins at atop level with ' burger king ' and then drills down progressively to ' what is gross sales ? ' .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,72,1,0,,1.13E-06,0,negative,7.37E-05,2.00E-08,7.26E-06,4.53E-08,2.04E-07,1.04E-06,1.45E-06,2.32E-06,7.08E-08,0.998160594,3.59E-10,0.001735007,1.83E-05
9994,natural_language_inference50,297,"Similarly in the second example , it begins with ' florence nightingale ' and drills down to ' famous ' at H3 in which a match is being found with ' nursing ' in the same hierarchical level .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,73,1,0,,2.44E-06,0,negative,7.13E-05,1.83E-08,7.31E-06,1.23E-08,1.34E-07,4.12E-07,1.10E-06,1.13E-06,8.76E-08,0.997530498,1.86E-10,0.002377399,1.06E-05
9995,natural_language_inference50,298,"Overall , based on our qualitative analysis , we observe that , Hyper QA builds two hierarchical structures at the word - level ( in vector space ) towards the middle which strongly facilitates word - level matching .",Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,74,1,0,,0.039297158,0,negative,0.014551854,3.16E-07,3.19E-05,7.23E-08,1.51E-07,1.50E-06,7.99E-05,6.81E-06,6.42E-07,0.597862415,5.76E-09,0.387203747,0.000260636
9996,natural_language_inference50,299,Note that word embeddings are not constrained to ?x ?,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,75,1,0,,1.13E-06,0,negative,5.34E-05,4.50E-08,2.30E-06,4.97E-09,1.62E-08,6.01E-07,6.88E-07,8.79E-06,3.42E-07,0.999172349,1.74E-10,0.000756892,4.55E-06
9997,natural_language_inference50,300,< 1 .,Results and Analysis,Effects of QA Embedding Size,natural_language_inference,50,76,1,0,,7.76E-06,0,negative,0.000135901,1.07E-07,3.29E-06,2.33E-07,7.94E-08,3.65E-06,3.60E-06,4.32E-05,7.37E-07,0.99768791,2.63E-09,0.001977,0.000144263
9998,natural_language_inference50,301,Analysis of QA,Results and Analysis,,natural_language_inference,50,77,1,0,,0.004879823,0,negative,0.000584167,4.19E-07,7.85E-05,3.10E-07,4.75E-07,2.92E-06,0.000157497,3.37E-05,6.05E-07,0.945171892,1.35E-07,0.053446079,0.000523226
9999,natural_language_inference50,302,Embeddings,Results and Analysis,,natural_language_inference,50,78,1,0,,0.007209558,0,negative,0.000226842,4.03E-06,0.000616706,8.72E-07,1.24E-06,4.89E-05,0.000676109,0.000898046,1.49E-05,0.98208758,5.86E-08,0.011701694,0.003723007
10000,natural_language_inference50,303,Analysis of Word Embeddings,Results and Analysis,,natural_language_inference,50,79,1,0,,0.002513052,0,negative,0.000772019,2.18E-06,0.000313198,4.24E-07,5.58E-07,9.63E-06,0.001476207,8.84E-05,3.80E-06,0.838586194,7.01E-07,0.156536349,0.002210346
10001,natural_language_inference50,304,"Pertaining to answers , it seems like the model builds a hierarchy by splitting on conjunctive words ( ' and ' ) , i.e. , the root node of the tree starts by conjunctive words at splits sentences into semantic phrases .",Results and Analysis,Analysis of Word Embeddings,natural_language_inference,50,80,1,0,,2.07E-06,0,negative,1.88E-05,1.07E-08,8.56E-06,3.59E-10,8.36E-09,2.61E-08,8.99E-08,1.45E-07,2.57E-07,0.99991334,5.79E-11,5.85E-05,2.84E-07
10002,natural_language_inference50,305,"Overall , depicts our key intuitions regarding the inner workings of Hyper QA which explains both RQ2 and RQ3 .",Results and Analysis,Analysis of Word Embeddings,natural_language_inference,50,81,1,0,,3.05E-06,0,negative,3.47E-05,4.08E-09,2.93E-07,2.37E-09,7.01E-09,1.52E-07,1.40E-06,1.02E-06,1.36E-08,0.999273571,1.36E-10,0.000685818,2.97E-06
10003,natural_language_inference50,306,This is also supported by ( b ) which shows the majority of the word norms are clustered with ?w ? ?,Results and Analysis,Analysis of Word Embeddings,natural_language_inference,50,82,1,0,,1.90E-06,0,negative,0.000449125,5.76E-09,3.32E-06,2.10E-10,7.78E-09,4.54E-08,2.29E-06,2.26E-07,1.26E-08,0.996436601,2.47E-11,0.003107974,3.96E-07
10004,natural_language_inference50,307,3 . This would be reasonable considering that the leaf nodes of both question and answer hierarchies would reside in the middle .,Results and Analysis,Analysis of Word Embeddings,natural_language_inference,50,83,1,0,,6.19E-07,0,negative,1.03E-05,3.91E-09,6.23E-07,1.20E-09,4.26E-09,1.54E-07,2.64E-07,1.36E-06,3.36E-08,0.999949172,2.95E-11,3.74E-05,7.02E-07
10005,natural_language_inference50,308,Answer Question,Results and Analysis,,natural_language_inference,50,84,1,0,,3.37E-05,0,negative,4.93E-05,9.56E-08,7.21E-06,1.31E-07,1.00E-07,4.52E-06,1.87E-05,9.55E-05,5.24E-07,0.998533872,3.44E-09,0.001195079,9.48E-05
10006,natural_language_inference50,309,Starts with the main context and branches to more specific query words .,Results and Analysis,Answer Question,natural_language_inference,50,85,1,0,,1.75E-05,0,negative,3.60E-05,1.24E-06,0.000122068,2.26E-08,6.22E-07,3.57E-06,1.17E-05,1.95E-05,3.89E-06,0.999533408,9.14E-10,0.00025775,1.02E-05
10007,natural_language_inference50,310,Splits root node by,Results and Analysis,Answer Question,natural_language_inference,50,86,1,0,,7.04E-06,0,negative,2.98E-05,2.80E-07,6.69E-05,6.14E-09,1.87E-07,1.81E-06,1.01E-05,6.96E-06,1.33E-06,0.999430493,5.09E-10,0.000447337,4.84E-06
10008,natural_language_inference50,311,CONCLUSION,,,natural_language_inference,50,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
10009,natural_language_inference93,1,title,,,natural_language_inference,93,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
10010,natural_language_inference93,2,Gated Self - Matching Networks for Reading Comprehension and Question Answering,title,title,natural_language_inference,93,1,1,1,research-problem,0.997878107,1,research-problem,2.64E-08,4.52E-06,5.65E-08,6.42E-08,3.13E-08,7.52E-08,6.86E-07,9.23E-07,1.10E-06,0.001481003,0.998511391,8.71E-08,4.22E-08
10011,natural_language_inference93,3,abstract,,,natural_language_inference,93,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
10012,natural_language_inference93,4,"In this paper , we present the gated selfmatching networks for reading comprehension style question answering , which aims to answer questions from a given passage .",abstract,abstract,natural_language_inference,93,1,1,1,research-problem,0.913564364,1,research-problem,5.19E-07,0.000508811,3.38E-06,5.48E-07,1.93E-06,5.50E-07,2.19E-06,6.52E-06,3.59E-05,0.007524215,0.991914541,6.81E-07,2.41E-07
10013,natural_language_inference93,5,We first match the question and passage with gated attention - based recurrent networks to obtain the question - aware passage representation .,abstract,abstract,natural_language_inference,93,2,1,0,,0.362086945,0,negative,2.45E-05,0.297285376,0.000429124,5.97E-06,0.000236336,5.00E-05,1.87E-05,0.000616661,0.149157054,0.330062176,0.222102406,8.60E-06,3.04E-06
10014,natural_language_inference93,6,"Then we propose a self - matching attention mechanism to refine the representation by matching the passage against itself , which effectively encodes information from the whole passage .",abstract,abstract,natural_language_inference,93,3,1,0,,0.213707054,0,negative,6.00E-05,0.287181573,0.000239409,1.44E-05,0.000307693,5.97E-05,1.10E-05,0.000645331,0.266861658,0.3963734,0.048236337,6.71E-06,2.75E-06
10015,natural_language_inference93,7,We finally employ the pointer networks to locate the positions of answers from the passages .,abstract,abstract,natural_language_inference,93,4,1,0,,0.283343179,0,negative,2.62E-05,0.095215912,0.000431224,8.01E-06,0.000142802,8.05E-05,1.35E-05,0.000572407,0.316701464,0.524369084,0.062432463,4.06E-06,2.32E-06
10016,natural_language_inference93,8,We conduct extensive experiments on the SQuAD dataset .,abstract,abstract,natural_language_inference,93,5,1,0,,0.01351627,0,negative,3.13E-05,0.025288221,8.27E-06,1.84E-05,0.000673464,4.00E-05,3.11E-05,0.000517224,0.000175138,0.944804962,0.028368966,4.16E-05,1.32E-06
10017,natural_language_inference93,9,"The single model achieves 71.3 % on the evaluation metrics of exact match on the hidden test set , while the ensemble model further boosts the results to 75.9 % .",abstract,abstract,natural_language_inference,93,6,1,0,,0.048957507,0,negative,0.003537537,0.009645333,5.48E-05,0.000301179,0.001020681,0.000215777,0.001278536,0.003136586,0.000225103,0.82998647,0.144791156,0.005726516,8.03E-05
10018,natural_language_inference93,10,"At the time of submission of the paper , our model holds the first place on the SQuAD leaderboard for both single and ensemble model .",abstract,abstract,natural_language_inference,93,7,1,0,,0.010203269,0,negative,3.13E-05,0.002823766,3.30E-05,0.006804366,0.019347843,0.000223344,5.52E-05,8.88E-05,0.000164497,0.889160396,0.081255241,6.43E-06,5.88E-06
10019,natural_language_inference93,11,Introduction,,,natural_language_inference,93,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
10020,natural_language_inference93,12,"In this paper , we focus on reading comprehension style question answering which aims to answer questions given a passage or document .",Introduction,Introduction,natural_language_inference,93,1,1,0,,0.962465425,1,research-problem,1.01E-05,0.013749189,1.29E-05,1.57E-05,0.00032267,1.31E-05,9.01E-05,2.70E-05,0.001515146,0.0392305,0.944988374,1.56E-05,9.52E-06
10021,natural_language_inference93,13,"We specifically focus on the Stanford Question Answering Dataset ( SQuAD ) , a largescale dataset for reading comprehension and question answering which is manually created through crowdsourcing .",Introduction,Introduction,natural_language_inference,93,2,1,0,,0.549994937,1,dataset,9.89E-05,0.366893373,0.000247961,0.001371105,0.453980246,0.000374929,0.000529513,0.000265582,0.008422544,0.144517758,0.023145601,9.35E-05,5.90E-05
10022,natural_language_inference93,14,"SQuAD constrains answers to the space of all possible spans within the reference passage , which is different from cloze - style reading comprehension datasets ( Hermann et al. , * Contribution during internship at Microsoft Research . Equal contribution .",Introduction,Introduction,natural_language_inference,93,3,1,0,,0.669765337,1,research-problem,1.38E-06,0.000981271,1.25E-06,4.43E-06,1.73E-05,8.93E-06,1.36E-05,1.55E-05,0.000576584,0.096552542,0.901822751,2.74E-06,1.67E-06
10023,natural_language_inference93,15,2015 ; in which answers are single words or entities .,Introduction,Introduction,natural_language_inference,93,4,1,0,,0.172513885,0,negative,0.000153734,0.006133076,5.19E-05,0.000107609,0.007057258,0.000142887,0.000116643,5.44E-05,0.001813284,0.938565383,0.045747268,4.92E-05,7.42E-06
10024,natural_language_inference93,16,"Moreover , SQuAD requires different forms of logical reasoning to infer the answer .",Introduction,Introduction,natural_language_inference,93,5,1,0,,0.032011301,0,negative,1.63E-05,0.00632137,7.60E-06,7.46E-06,0.000190382,1.93E-05,2.20E-05,2.63E-05,0.001934585,0.543577341,0.447866007,9.51E-06,1.87E-06
10025,natural_language_inference93,17,Rapid progress has been made since the release of the SQuAD dataset .,Introduction,Introduction,natural_language_inference,93,6,1,0,,0.035541496,0,negative,1.12E-05,0.001607609,3.27E-06,8.42E-05,0.000230087,0.000130023,9.67E-05,0.000108356,0.001266472,0.552148071,0.444289449,1.22E-05,1.23E-05
10026,natural_language_inference93,18,"build question - aware passage representation with match - LSTM , and predict answer boundaries in the passage with pointer networks .",Introduction,Introduction,natural_language_inference,93,7,1,0,,0.116207166,0,negative,0.000103306,0.03384435,0.000324918,0.000210885,0.002708505,0.000720889,0.000378231,0.00041654,0.074993907,0.724571589,0.161592925,6.43E-05,6.96E-05
10027,natural_language_inference93,19,introduce bi-directional attention flow networks to model question - passage pairs at multiple levels of granularity .,Introduction,Introduction,natural_language_inference,93,8,1,0,,0.067744269,0,research-problem,6.57E-05,0.038062118,7.26E-05,0.000464302,0.001509656,0.000502776,0.000414404,0.00063914,0.026248468,0.386671175,0.545202499,5.59E-05,9.13E-05
10028,natural_language_inference93,20,propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions .,Introduction,Introduction,natural_language_inference,93,9,1,0,,0.051770884,0,negative,5.18E-05,0.045185769,0.000105719,0.000393999,0.001975209,0.001009096,0.000215546,0.000740118,0.085137634,0.720312023,0.144810424,2.02E-05,4.25E-05
10029,natural_language_inference93,21,and predict answers by ranking continuous text spans within passages .,Introduction,Introduction,natural_language_inference,93,10,1,0,,0.062289774,0,research-problem,5.06E-06,0.008662492,2.48E-05,2.05E-05,0.000340288,0.000101531,7.83E-05,9.03E-05,0.012903313,0.481002975,0.496747051,1.26E-05,1.08E-05
10030,natural_language_inference93,22,"Inspired by , we introduce a gated self - matching network , illustrated in , an end - to - end neural network model for reading comprehension and question answering .",Introduction,Introduction,natural_language_inference,93,11,1,1,model,0.945701333,1,model,6.01E-05,0.154047515,0.000308455,1.62E-05,0.001187115,6.83E-05,6.26E-05,7.29E-05,0.797618153,0.034809546,0.011718446,2.04E-05,1.03E-05
10031,natural_language_inference93,23,Our model consists of four parts :,Introduction,Introduction,natural_language_inference,93,12,1,1,model,0.882844873,1,model,1.29E-05,0.026135987,6.62E-05,2.04E-06,0.000125248,3.19E-05,1.73E-05,3.42E-05,0.950723835,0.020807832,0.002035811,3.73E-06,2.94E-06
10032,natural_language_inference93,24,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",Introduction,Introduction,natural_language_inference,93,13,1,1,model,0.962192979,1,model,1.18E-05,0.02248772,9.12E-05,5.70E-07,6.40E-05,1.23E-05,8.15E-06,1.40E-05,0.971158313,0.00578003,0.000368719,1.95E-06,1.20E-06
10033,natural_language_inference93,25,The key contributions of this work are three - fold .,Introduction,Introduction,natural_language_inference,93,14,1,0,,0.002086687,0,negative,1.77E-05,0.014468131,1.02E-05,9.76E-05,0.000731254,0.000342816,5.59E-05,0.000267576,0.016244804,0.940627173,0.027121158,8.04E-06,7.64E-06
10034,natural_language_inference93,26,"First , we propose a gated attention - based recurrent network , which adds an additional gate to the attention - based recurrent networks , to account for the fact that words in the passage are of different importance to answer a particular question for reading comprehension and question answering .",Introduction,Introduction,natural_language_inference,93,15,1,0,,0.968091145,1,model,4.14E-05,0.152270603,0.000123506,3.11E-06,0.00032505,2.68E-05,2.81E-05,4.63E-05,0.837175739,0.009048186,0.000899464,8.64E-06,3.12E-06
10035,natural_language_inference93,27,"In , words in a passage with their corresponding attention - weighted question context are en - coded together to produce question - aware passage representation .",Introduction,Introduction,natural_language_inference,93,16,1,0,,0.834568284,1,model,2.14E-05,0.106876205,0.000146478,1.99E-06,0.000372644,1.94E-05,1.45E-05,3.17E-05,0.871049384,0.020375347,0.001083984,4.95E-06,2.03E-06
10036,natural_language_inference93,28,"By introducing a gating mechanism , our gated attention - based recurrent network assigns different levels of importance to passage parts depending on their relevance to the question , masking out irrelevant passage parts and emphasizing the important ones .",Introduction,Introduction,natural_language_inference,93,17,1,0,,0.952493468,1,model,1.50E-05,0.089018142,2.98E-05,1.04E-06,0.000152125,1.10E-05,6.88E-06,2.39E-05,0.904040254,0.006498513,0.000200266,2.21E-06,8.79E-07
10037,natural_language_inference93,29,"Second , we introduce a self - matching mechanism , which can effectively aggregate evidence from the whole passage to infer the answer .",Introduction,Introduction,natural_language_inference,93,18,1,0,,0.868039841,1,model,3.56E-05,0.174488897,7.22E-05,3.82E-06,0.00053401,3.08E-05,2.01E-05,6.05E-05,0.813419452,0.01108324,0.000243717,5.28E-06,2.47E-06
10038,natural_language_inference93,30,"Through a gated matching layer , the resulting question - aware passage representation effectively encodes question information for each passage word .",Introduction,Introduction,natural_language_inference,93,19,1,0,,0.946229969,1,model,4.72E-06,0.029367431,2.39E-05,1.18E-07,2.55E-05,3.25E-06,3.10E-06,6.31E-06,0.966775518,0.003647058,0.000141763,9.57E-07,3.23E-07
10039,natural_language_inference93,31,"However , recurrent networks can only memorize limited passage context in practice despite its theoretical capability .",Introduction,Introduction,natural_language_inference,93,20,1,0,,0.010356531,0,negative,5.66E-06,0.001886917,2.31E-06,1.08E-05,5.34E-05,7.82E-05,6.26E-05,0.000108304,0.001675246,0.73628082,0.259822445,8.77E-06,4.56E-06
10040,natural_language_inference93,32,One answer candidate is often unaware of the clues in other parts of the passage .,Introduction,Introduction,natural_language_inference,93,21,1,0,,0.005045914,0,negative,5.83E-06,0.001867311,2.28E-06,1.54E-05,0.000533063,5.40E-05,2.74E-05,3.51E-05,0.000963089,0.971913627,0.024573086,7.70E-06,2.08E-06
10041,natural_language_inference93,33,"To address this problem , we propose a self - matching layer to dynamically refine passage representation with information from the whole passage .",Introduction,Introduction,natural_language_inference,93,22,1,0,,0.955956366,1,model,9.63E-06,0.063706088,5.13E-05,5.03E-07,8.53E-05,9.51E-06,9.91E-06,1.65E-05,0.932151422,0.003746954,0.000209807,2.00E-06,1.11E-06
10042,natural_language_inference93,34,"Based on question - aware passage representation , we employ gated attention - based recurrent networks on passage against passage itself , aggregating evidence relevant to the current passage word from every word in the passage .",Introduction,Introduction,natural_language_inference,93,23,1,0,,0.932595332,1,model,2.00E-05,0.297696606,0.000144848,1.24E-06,0.000387871,2.23E-05,5.37E-05,4.16E-05,0.691066162,0.009763741,0.00078827,1.02E-05,3.36E-06
10043,natural_language_inference93,35,"A gated attention - based recurrent network layer and self - matching layer dynamically enrich each passage representation with information aggregated from both question and passage , enabling subsequent network to better predict answers .",Introduction,Introduction,natural_language_inference,93,24,1,0,,0.952038189,1,model,1.46E-05,0.039587893,6.20E-05,1.42E-06,0.000131031,2.03E-05,1.56E-05,3.38E-05,0.954646653,0.00536418,0.000118656,1.81E-06,2.03E-06
10044,natural_language_inference93,36,"Lastly , the proposed method yields state - of - theart results against strong baselines .",Introduction,Introduction,natural_language_inference,93,25,1,0,,0.054804015,0,negative,0.002778398,0.015507416,4.93E-05,6.46E-05,0.002414376,0.000359277,0.012106557,0.000715032,0.001660388,0.947676207,0.004071891,0.012468086,0.000128422
10045,natural_language_inference93,37,"Our single model achieves 71.3 % exact match accuracy on the hidden SQuAD test set , while the ensemble model further boosts the result to 75.9 % .",Introduction,Introduction,natural_language_inference,93,26,1,0,,0.076133776,0,negative,0.005727611,0.067908942,0.000305128,0.000407727,0.014417591,0.001743135,0.087126513,0.003138682,0.006588857,0.762958091,0.006941317,0.041011422,0.001724984
10046,natural_language_inference93,38,"At the time 1 of submission of this paper , our model holds the first place on the SQuAD leader board .",Introduction,Introduction,natural_language_inference,93,27,1,0,,0.05390831,0,negative,0.000188455,0.128881402,0.000475966,0.000271144,0.085118277,0.000487069,0.000358554,0.0001466,0.067869998,0.714431845,0.001651744,9.50E-05,2.40E-05
10047,natural_language_inference93,39,Task Description,Introduction,,natural_language_inference,93,28,1,0,,0.0107305,0,negative,4.32E-06,0.018942909,1.07E-05,4.46E-05,0.006846231,0.000920976,0.000767454,0.001073878,0.004788129,0.960469027,0.006058073,4.07E-05,3.30E-05
10048,natural_language_inference93,40,"For reading comprehension style question answering , a passage P and question Q are given , our task is to predict an answer A to question Q based on information found in P .",Introduction,Task Description,natural_language_inference,93,29,1,0,,0.000422741,0,negative,1.93E-05,9.74E-05,9.84E-06,5.75E-05,4.58E-05,0.000127571,0.000179029,6.95E-05,6.30E-05,0.964550242,0.034620715,8.89E-05,7.12E-05
10049,natural_language_inference93,41,The SQuAD dataset further constrains answer A to be a continuous subspan of passage P. Answer A often includes nonentities and can be much longer phrases .,Introduction,Task Description,natural_language_inference,93,30,1,0,,9.38E-06,0,negative,3.08E-06,1.60E-06,1.42E-06,1.58E-06,7.48E-05,2.40E-05,4.09E-06,4.42E-06,9.08E-07,0.999875498,1.86E-06,6.33E-06,4.24E-07
10050,natural_language_inference93,42,This setup challenges us to understand and reason about both the question and passage in order to infer the answer .,Introduction,Task Description,natural_language_inference,93,31,1,0,,1.44E-05,0,negative,1.60E-06,2.86E-06,2.42E-07,3.77E-07,2.47E-06,4.31E-06,4.96E-07,2.94E-06,9.92E-06,0.9999698,2.65E-06,2.26E-06,7.40E-08
10051,natural_language_inference93,43,shows a simple example from the SQuAD dataset .,Introduction,Task Description,natural_language_inference,93,32,1,0,,3.47E-05,0,negative,2.41E-06,7.69E-07,8.62E-07,2.81E-08,2.33E-06,3.37E-06,1.50E-06,2.30E-06,2.10E-06,0.999974678,2.20E-07,9.39E-06,4.18E-08
10052,natural_language_inference93,44,"1 On Feb. 6 , 2017",Introduction,Task Description,natural_language_inference,93,33,1,0,,2.16E-06,0,negative,4.04E-05,5.81E-06,1.92E-06,4.78E-07,6.09E-06,7.24E-06,1.95E-06,8.85E-06,3.72E-05,0.999878245,2.78E-07,1.14E-05,1.04E-07
10053,natural_language_inference93,45,Passage : Tesla later approached Morgan to ask for more funds to build a more powerful transmitter .,Introduction,Task Description,natural_language_inference,93,34,1,0,,1.29E-06,0,negative,2.00E-06,2.54E-07,1.53E-07,7.41E-07,2.85E-06,1.39E-05,1.01E-06,3.17E-06,1.26E-06,0.99997276,4.72E-07,1.34E-06,1.10E-07
10054,natural_language_inference93,46,"When asked where all the money had gone , Tesla responded by saying that he was affected by the Panic of 1901 , which he ( Morgan ) had caused .",Introduction,Task Description,natural_language_inference,93,35,1,0,,1.59E-06,0,negative,8.02E-07,2.38E-07,8.69E-08,1.27E-08,3.47E-07,2.41E-06,4.59E-07,1.90E-06,1.78E-06,0.999990449,1.88E-07,1.31E-06,1.94E-08
10055,natural_language_inference93,47,Morgan was shocked by the reminder of his part in the stock market crash and by Tesla 's breach of contract by asking for more funds .,Introduction,Task Description,natural_language_inference,93,36,1,0,,3.02E-06,0,negative,3.49E-06,1.07E-06,3.83E-07,3.77E-07,4.39E-06,1.31E-05,1.79E-06,5.54E-06,4.09E-06,0.999960905,7.67E-07,3.95E-06,1.80E-07
10056,natural_language_inference93,48,"Tesla wrote another plea to Morgan , but it was also fruitless .",Introduction,Task Description,natural_language_inference,93,37,1,0,,1.97E-06,0,negative,1.90E-06,3.06E-07,3.12E-07,4.15E-08,1.16E-06,4.65E-06,1.06E-06,2.23E-06,1.61E-06,0.999983897,3.96E-07,2.38E-06,5.94E-08
10057,natural_language_inference93,49,"Morgan still owed Tesla money on the original agreement , and Tesla had been facing foreclosure even before construction of the tower began .",Introduction,Task Description,natural_language_inference,93,38,1,0,,4.44E-06,0,negative,1.54E-05,1.77E-06,2.13E-06,6.22E-08,3.11E-06,6.25E-06,3.80E-06,2.62E-06,7.52E-06,0.99994059,6.50E-07,1.59E-05,1.64E-07
10058,natural_language_inference93,50,Question :,Introduction,Task Description,natural_language_inference,93,39,1,0,,7.25E-07,0,negative,4.81E-07,1.24E-06,1.19E-07,7.11E-08,2.05E-07,5.70E-06,5.26E-07,9.68E-06,1.18E-05,0.999967875,1.58E-06,6.66E-07,6.95E-08
10059,natural_language_inference93,51,On what did Tesla blame for the loss of the initial money ?,Introduction,Task Description,natural_language_inference,93,40,1,0,,7.88E-07,0,negative,6.42E-07,5.43E-07,1.42E-07,1.00E-08,1.09E-07,3.45E-06,6.25E-07,4.81E-06,5.70E-06,0.999982558,2.80E-07,1.09E-06,3.71E-08
10060,natural_language_inference93,52,Answer : Panic of 1901 3 Gated Self - Matching,Introduction,Task Description,natural_language_inference,93,41,1,0,,7.77E-06,0,negative,1.83E-06,1.11E-06,8.16E-07,1.25E-07,6.80E-07,1.50E-05,5.33E-06,8.49E-06,1.77E-05,0.999937741,5.60E-06,5.04E-06,4.98E-07
10061,natural_language_inference93,53,Networks gives an overview of the gated selfmatching networks .,Introduction,Task Description,natural_language_inference,93,42,1,0,,1.90E-05,0,negative,6.14E-06,6.53E-06,3.23E-05,4.20E-08,9.34E-07,5.70E-06,5.98E-06,4.37E-06,0.000162271,0.999753082,7.39E-06,1.47E-05,6.03E-07
10062,natural_language_inference93,54,"First , the question and passage are processed by a bi-directional recurrent network separately .",Introduction,Task Description,natural_language_inference,93,43,1,0,,0.003549184,0,negative,0.000150695,0.000744789,0.000299242,1.53E-07,1.31E-05,1.31E-05,1.66E-05,1.91E-05,0.016867193,0.981830348,2.47E-06,4.14E-05,1.91E-06
10063,natural_language_inference93,55,"We then match the question and passage with gated attention - based recurrent networks , obtaining question - aware representation for the passage .",Introduction,Task Description,natural_language_inference,93,44,1,0,,0.000892858,0,negative,1.71E-05,0.000157806,0.000125861,2.26E-08,2.73E-06,3.49E-06,4.27E-06,4.74E-06,0.004784481,0.994882455,9.73E-07,1.56E-05,4.71E-07
10064,natural_language_inference93,56,"On top of that , we apply self - matching attention to aggregate evidence from the whole passage and refine the passage representation , which is then fed into the output layer to predict the boundary of the answer span .",Introduction,Task Description,natural_language_inference,93,45,1,0,,0.002277093,0,negative,0.000142319,0.001147115,0.000193228,1.65E-07,1.48E-05,1.24E-05,1.37E-05,2.92E-05,0.023185037,0.975234214,1.14E-06,2.50E-05,1.72E-06
10065,natural_language_inference93,57,Question and Passage Encoder,Introduction,,natural_language_inference,93,46,1,0,,0.063718607,0,negative,0.000102745,0.031227546,0.001190396,9.14E-05,0.00686324,0.001998572,0.003845712,0.000679966,0.217837734,0.734281492,0.001422018,0.000229018,0.000230114
10066,natural_language_inference93,58,Consider a question Q = {w Q t } m t =1 and a passage P = {w Pt } n t=1 .,Introduction,Question and Passage Encoder,natural_language_inference,93,47,1,0,,3.06E-06,0,negative,1.02E-06,1.50E-07,3.57E-07,1.27E-09,1.62E-08,1.05E-07,2.95E-07,7.18E-07,4.04E-06,0.999983352,1.51E-08,9.85E-06,8.11E-08
10067,natural_language_inference93,59,We first convert the words to their respective word - level embeddings ( {e Q t } m t=1 and {e Pt } n t=1 ) and character - level embeddings ( {c Q t } m t=1 and {c Pt } n t=1 ) .,Introduction,Question and Passage Encoder,natural_language_inference,93,48,1,0,,0.000186744,0,negative,1.93E-05,4.61E-06,1.23E-05,5.75E-09,1.09E-07,6.49E-07,2.92E-06,4.69E-06,0.000246359,0.999690155,2.58E-08,1.83E-05,5.44E-07
10068,natural_language_inference93,60,The character - level embeddings are generated by taking the final hidden states of a bi-directional recurrent neural network ( RNN ) applied to embeddings of characters in the token .,Introduction,Question and Passage Encoder,natural_language_inference,93,49,1,0,,0.002842059,0,negative,1.13E-05,1.89E-05,1.06E-05,6.32E-08,4.99E-07,4.56E-06,7.71E-06,7.32E-05,0.00122057,0.998639457,8.45E-08,9.13E-06,3.97E-06
10069,natural_language_inference93,61,Such character - level embeddings have been shown to be helpful to deal with out - ofvocab ( OOV ) tokens .,Introduction,Question and Passage Encoder,natural_language_inference,93,50,1,0,,0.001709583,0,negative,1.53E-05,5.36E-07,1.72E-06,9.20E-08,8.79E-08,2.36E-06,7.32E-06,6.71E-06,5.87E-06,0.999769208,2.32E-06,0.000180142,8.37E-06
10070,natural_language_inference93,62,"We then use a bi-directional RNN to produce new representation u Q 1 , . . . , u Q m and u P 1 , . . . , u P n of all words in the question and passage respectively :",Introduction,Question and Passage Encoder,natural_language_inference,93,51,1,0,,9.52E-06,0,negative,1.05E-05,7.70E-07,1.39E-05,3.02E-09,6.45E-08,1.53E-07,6.59E-07,6.21E-07,6.72E-05,0.999887589,1.79E-08,1.83E-05,1.86E-07
10071,natural_language_inference93,63,Vector,Introduction,,natural_language_inference,93,52,1,0,,0.001971288,0,negative,8.27E-06,0.013991534,2.55E-05,1.21E-06,8.31E-05,0.000551144,0.000617936,0.001356729,0.314197404,0.668924068,0.000215126,1.18E-05,1.62E-05
10072,natural_language_inference93,64,Gated Attention - based Recurrent Networks,Introduction,,natural_language_inference,93,53,1,0,,0.049898312,0,negative,4.91E-05,0.030235791,0.000272562,7.57E-06,0.000125589,0.000574931,0.007671472,0.000728553,0.297998752,0.638123121,0.023709103,0.000290464,0.000213032
10073,natural_language_inference93,65,We propose a gated attention - based recurrent network to incorporate question information into passage representation .,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,54,1,0,,0.000814755,0,negative,0.000448406,0.005148042,0.001916084,2.40E-06,7.22E-05,3.18E-05,0.00013387,6.77E-05,0.040103401,0.951423761,4.14E-05,0.00058099,2.99E-05
10074,natural_language_inference93,66,"It is a variant of attentionbased recurrent networks , with an additional gate to determine the importance of information in the passage regarding a question .",Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,55,1,0,,3.79E-05,0,negative,0.000237157,0.000189101,0.005585182,1.22E-06,4.41E-05,1.93E-05,7.26E-05,1.55E-05,0.003054089,0.990527461,1.60E-06,0.00024019,1.26E-05
10075,natural_language_inference93,67,"Given question and passage representation {u Q t } m t =1 and {u Pt } n t = 1 , propose generating sentence - pair representation {v Pt } n t =1 via soft - alignment of words in the question and passage as follows :",Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,56,1,0,,8.86E-07,0,negative,2.34E-06,1.55E-05,5.59E-06,8.41E-09,2.83E-07,5.14E-07,7.37E-07,3.16E-06,0.000288497,0.99967477,2.48E-07,8.26E-06,9.99E-08
10076,natural_language_inference93,68,where,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,57,1,0,,7.94E-08,0,negative,4.13E-07,2.05E-07,9.46E-08,4.17E-09,3.53E-08,5.15E-07,2.44E-07,2.01E-06,6.14E-06,0.999989053,8.47E-09,1.27E-06,1.10E-08
10077,natural_language_inference93,69,is an attentionpooling vector of the whole question ( u Q ) :,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,58,1,0,,1.48E-06,0,negative,2.00E-06,5.76E-06,5.64E-06,2.63E-09,2.00E-07,3.20E-07,4.34E-07,1.57E-06,0.000105749,0.99987346,4.32E-08,4.79E-06,3.26E-08
10078,natural_language_inference93,70,Each passage representation,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,59,1,0,,5.50E-06,0,negative,1.37E-05,1.23E-05,3.84E-05,2.08E-08,1.06E-06,1.46E-06,8.64E-06,3.82E-06,0.000590864,0.999192874,4.44E-07,0.000135562,8.45E-07
10079,natural_language_inference93,71,v,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,60,1,0,,4.80E-07,0,negative,6.88E-07,4.03E-07,1.78E-07,4.96E-09,3.77E-08,7.95E-07,7.84E-07,3.13E-06,1.99E-05,0.99997114,3.53E-08,2.86E-06,4.56E-08
10080,natural_language_inference93,72,Pt dynamically incorporates aggregated matching information from the whole question .,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,61,1,0,,0.001055193,0,negative,4.46E-05,0.000200286,0.000475006,4.32E-08,3.71E-06,1.80E-06,9.19E-06,5.89E-06,0.011181969,0.988015438,6.81E-07,6.00E-05,1.38E-06
10081,natural_language_inference93,73,"Wang and Jiang ( 2016 a ) introduce match - LSTM , which takes u",Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,62,1,0,,1.10E-05,0,negative,3.63E-05,4.05E-05,0.000395935,6.50E-09,1.02E-06,1.04E-06,1.06E-05,1.81E-06,0.000985159,0.998455665,1.87E-07,7.14E-05,3.67E-07
10082,natural_language_inference93,74,Pt as an additional input into the recurrent network :,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,63,1,0,,1.56E-06,0,negative,1.05E-05,6.09E-06,9.27E-06,7.33E-09,3.74E-07,4.45E-07,9.93E-07,1.98E-06,0.000189115,0.999766758,2.40E-08,1.44E-05,7.09E-08
10083,natural_language_inference93,75,"To determine the importance of passage parts and attend to the ones relevant to the question , we add another gate to the input ( [u Pt , ct ] ) of RNN :",Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,64,1,0,,7.81E-07,0,negative,5.45E-05,1.86E-05,8.48E-05,3.93E-08,2.20E-06,1.71E-06,3.29E-06,4.51E-06,0.000852692,0.998957467,3.30E-08,1.97E-05,4.38E-07
10084,natural_language_inference93,76,"Different from the gates in LSTM or GRU , the additional gate is based on the current passage word and its attention - pooling vector of the question , which focuses on the relation between the question and current passage word .",Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,65,1,0,,0.000199833,0,negative,5.83E-05,0.00016442,0.000106193,7.91E-08,6.48E-06,2.52E-06,6.46E-06,1.24E-05,0.005578589,0.994045451,7.24E-08,1.81E-05,9.46E-07
10085,natural_language_inference93,77,The gate effectively model the phenomenon that only parts of the passage are relevant to the question in reading comprehension and question answering .,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,66,1,0,,7.87E-05,0,negative,1.68E-05,2.79E-05,6.35E-05,1.79E-07,3.59E-06,3.69E-06,5.44E-06,8.14E-06,0.002199488,0.997649765,4.07E-07,1.89E-05,2.25E-06
10086,natural_language_inference93,78,We call this gated attention - based recurrent networks .,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,67,1,0,,3.79E-05,0,negative,1.15E-05,3.36E-05,0.000213882,3.83E-08,1.99E-06,1.70E-06,5.88E-06,3.57E-06,0.002076509,0.997626673,2.84E-07,2.31E-05,1.28E-06
10087,natural_language_inference93,79,"It can be applied to variants of RNN , such as GRU and LSTM .",Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,68,1,0,,6.28E-06,0,negative,2.92E-06,3.31E-06,4.18E-06,3.70E-09,1.95E-07,3.83E-07,9.27E-07,1.44E-06,5.17E-05,0.999922858,1.79E-08,1.20E-05,5.56E-08
10088,natural_language_inference93,80,We also conduct experiments to show the effectiveness of the additional gate on both GRU and LSTM .,Introduction,Gated Attention - based Recurrent Networks,natural_language_inference,93,69,1,0,,9.39E-05,0,negative,0.000128756,1.93E-05,1.65E-06,1.54E-08,1.87E-06,1.27E-06,1.41E-05,8.93E-06,1.63E-05,0.999485597,3.77E-09,0.000322052,1.56E-07
10089,natural_language_inference93,81,Self - Matching Attention,Introduction,,natural_language_inference,93,70,1,0,,0.033836193,0,negative,5.75E-05,0.037737502,0.00056565,2.64E-06,0.00027055,0.000404743,0.007331709,0.000410497,0.442479915,0.510129536,0.000293141,0.000160633,0.000155977
10090,natural_language_inference93,82,"Through gated attention - based recurrent networks , question - aware passage representation {v Pt } n t=1 is generated to pinpoint important parts in the passage .",Introduction,Self - Matching Attention,natural_language_inference,93,71,1,0,,2.13E-05,0,negative,1.76E-05,1.28E-05,2.91E-05,1.04E-08,4.96E-07,2.34E-07,9.09E-07,2.19E-06,0.000368252,0.999521217,2.20E-08,4.67E-05,4.74E-07
10091,natural_language_inference93,83,One problem with such representation is that it has very limited knowledge of context .,Introduction,Self - Matching Attention,natural_language_inference,93,72,1,0,,9.96E-07,0,negative,3.75E-06,3.01E-07,2.43E-06,1.36E-07,1.77E-07,1.85E-06,1.72E-06,3.44E-06,1.94E-06,0.999929947,2.15E-07,5.18E-05,2.29E-06
10092,natural_language_inference93,84,One answer candidate is often oblivious to important cues in the passage outside its surrounding window .,Introduction,Self - Matching Attention,natural_language_inference,93,73,1,0,,4.98E-07,0,negative,1.38E-06,7.86E-08,9.10E-07,2.30E-08,1.02E-07,6.81E-07,5.00E-07,1.96E-06,1.01E-06,0.999976177,1.76E-08,1.66E-05,5.14E-07
10093,natural_language_inference93,85,"Moreover , there exists some sort of lexical or syntactic divergence between the question and passage in the majority of SQuAD dataset .",Introduction,Self - Matching Attention,natural_language_inference,93,74,1,0,,9.85E-07,0,negative,4.06E-06,1.01E-07,1.23E-06,7.42E-09,1.72E-07,3.42E-07,1.15E-06,1.14E-06,2.59E-07,0.9999194,6.71E-09,7.18E-05,3.65E-07
10094,natural_language_inference93,86,Passage context is necessary to infer the answer .,Introduction,Self - Matching Attention,natural_language_inference,93,75,1,0,,7.31E-05,0,negative,2.85E-05,1.90E-07,1.53E-06,1.02E-07,5.01E-07,6.39E-07,1.08E-06,1.76E-06,1.61E-06,0.999848936,1.16E-08,0.000113664,1.46E-06
10095,natural_language_inference93,87,"To address this problem , we propose directly matching the question - aware passage representation against itself .",Introduction,Self - Matching Attention,natural_language_inference,93,76,1,0,,0.000253574,0,negative,0.00016822,6.44E-05,0.000121159,5.04E-08,2.14E-06,1.06E-06,5.27E-06,5.44E-06,0.000571709,0.99887281,4.47E-08,0.000184709,3.00E-06
10096,natural_language_inference93,88,It dynamically collects evidence from the whole passage for words in passage and encodes the evidence relevant to the current passage word and its matching question information into the passage representation h Pt :,Introduction,Self - Matching Attention,natural_language_inference,93,77,1,0,,1.28E-05,0,negative,1.18E-05,3.87E-06,0.000243863,3.66E-09,2.06E-07,2.33E-07,1.53E-06,9.19E-07,0.000472774,0.999240484,1.26E-08,2.35E-05,7.75E-07
10097,natural_language_inference93,89,"where ct = att ( v P , v Pt ) is an attention - pooling vector of the whole passage ( v P ) :",Introduction,Self - Matching Attention,natural_language_inference,93,78,1,0,,2.70E-07,0,negative,1.68E-06,2.01E-07,1.45E-06,8.50E-10,2.18E-08,8.43E-08,1.82E-07,8.20E-07,6.53E-06,0.999983045,1.01E-09,5.91E-06,7.12E-08
10098,natural_language_inference93,90,"An additional gate as in gated attention - based recurrent networks is applied to [v Pt , ct ] to adaptively control the input of RNN .",Introduction,Self - Matching Attention,natural_language_inference,93,79,1,0,,8.08E-06,0,negative,7.86E-05,1.04E-05,7.03E-05,9.76E-08,1.11E-06,1.95E-06,3.86E-06,2.02E-05,0.000792849,0.998996881,8.45E-09,1.88E-05,4.95E-06
10099,natural_language_inference93,91,Self - matching extracts evidence from the whole passage according to the current passage word and question information .,Introduction,Self - Matching Attention,natural_language_inference,93,80,1,0,,0.000370699,0,negative,3.13E-05,4.31E-06,0.000406401,3.69E-08,1.49E-06,8.88E-07,4.36E-06,2.90E-06,0.000124769,0.999347305,1.16E-08,7.08E-05,5.41E-06
10100,natural_language_inference93,92,Output Layer,Introduction,,natural_language_inference,93,81,1,0,,0.0931434,0,negative,3.85E-05,0.016845797,0.000252013,7.88E-06,0.000706605,0.002295489,0.00439263,0.003206919,0.278914073,0.693084243,7.37E-06,2.27E-05,0.000225848
10101,natural_language_inference93,93,We follow and use pointer networks to predict the start and end position of the answer .,Introduction,Output Layer,natural_language_inference,93,82,1,0,,0.000334602,0,negative,7.36E-06,3.35E-06,2.50E-05,5.61E-08,1.31E-06,6.86E-07,3.08E-07,7.56E-06,0.00018227,0.999764875,3.22E-09,2.26E-06,4.99E-06
10102,natural_language_inference93,94,"In addition , we use an attention - pooling over the question representation to generate the initial hidden vector for the pointer network .",Introduction,Output Layer,natural_language_inference,93,83,1,0,,0.001890509,0,negative,5.78E-05,0.000155124,0.000112691,2.85E-08,3.47E-06,1.17E-06,1.82E-06,7.79E-05,0.007459216,0.992116725,5.89E-09,5.75E-06,8.25E-06
10103,natural_language_inference93,95,"Given the passage representation {h Pt } n t =1 , the attention mechanism is utilized as a pointer to select the start position ( p 1 ) and end position ( p 2 ) from the passage , which can be formulated as follows :",Introduction,Output Layer,natural_language_inference,93,84,1,0,,1.06E-06,0,negative,5.74E-06,2.01E-06,8.42E-06,1.91E-09,1.33E-07,5.36E-08,5.22E-08,2.13E-06,0.000115219,0.999864062,8.30E-10,1.93E-06,2.50E-07
10104,natural_language_inference93,96,Here ha t ? 1 represents the last hidden state of the answer recurrent network ( pointer network ) .,Introduction,Output Layer,natural_language_inference,93,85,1,0,,9.20E-07,0,negative,1.35E-06,2.85E-07,1.35E-06,3.48E-10,3.24E-08,2.80E-08,2.42E-08,1.04E-06,1.56E-05,0.999979464,1.31E-10,7.25E-07,6.22E-08
10105,natural_language_inference93,97,The input of the answer recurrent network is the attention - pooling vector based on current predicted probability at :,Introduction,Output Layer,natural_language_inference,93,86,1,0,,1.00E-05,0,negative,2.20E-06,1.38E-06,3.89E-06,2.48E-09,1.20E-07,1.23E-07,7.38E-08,5.83E-06,7.89E-05,0.999906116,5.82E-10,8.61E-07,4.72E-07
10106,natural_language_inference93,98,"When predicting the start position , ha t?1 represents the initial hidden state of the answer recurrent network .",Introduction,Output Layer,natural_language_inference,93,87,1,0,,1.85E-06,0,negative,1.31E-06,7.64E-07,1.14E-06,3.00E-10,3.67E-08,3.21E-08,3.72E-08,2.31E-06,2.72E-05,0.999966149,2.29E-10,9.51E-07,9.16E-08
10107,natural_language_inference93,99,We utilize the question vector r Q as the initial state of the answer recurrent network .,Introduction,Output Layer,natural_language_inference,93,88,1,0,,4.06E-06,0,negative,1.12E-06,1.76E-06,8.53E-07,5.76E-09,2.39E-07,4.22E-07,1.87E-07,6.05E-05,3.65E-05,0.999896783,2.25E-10,6.68E-07,9.83E-07
10108,natural_language_inference93,100,"r Q = att ( u Q , V Q r ) is an attention - pooling vector of the question based on the parameter V Q r :",Introduction,Output Layer,natural_language_inference,93,89,1,0,,6.55E-07,0,negative,1.02E-06,2.21E-07,6.86E-07,7.07E-10,5.58E-08,2.55E-08,1.61E-08,8.41E-07,3.79E-06,0.999992616,7.97E-11,6.65E-07,6.85E-08
10109,natural_language_inference93,101,"To train the network , we minimize the sum of the negative log probabilities of the ground truth start and end position by the predicted distributions .",Introduction,Output Layer,natural_language_inference,93,90,1,0,,1.74E-05,0,negative,1.32E-05,1.45E-05,5.79E-06,3.64E-09,7.94E-07,3.11E-07,3.11E-07,3.07E-05,0.000162258,0.999769147,2.40E-10,2.22E-06,8.46E-07
10110,natural_language_inference93,102,Experiment,,,natural_language_inference,93,0,1,0,,0.02032449,0,negative,2.65E-05,7.60E-05,1.08E-06,1.79E-06,9.59E-07,0.000215726,5.34E-05,0.005112434,4.95E-05,0.993225469,0.001033374,0.000199963,3.87E-06
10111,natural_language_inference93,103,Implementation Details,,,natural_language_inference,93,0,1,0,,0.045796543,0,negative,4.47E-05,0.001220595,9.47E-06,3.20E-05,1.55E-05,0.000465852,0.000169332,0.007430528,0.000344669,0.986360185,0.003666023,0.000225373,1.58E-05
10112,natural_language_inference93,104,"We specially focus on the SQuAD dataset to train and evaluate our model , which has garnered a huge attention over the past few months .",Implementation Details,Implementation Details,natural_language_inference,93,1,1,0,,0.368183969,0,negative,0.001428637,0.004621671,0.002964484,0.000613977,0.001836889,0.088005649,0.010655436,0.057241245,0.00023964,0.827377545,0.001140016,0.003468643,0.000406168
10113,natural_language_inference93,105,"SQuAD is composed of 100,000 + questions posed by crowd workers on 536 Wikipedia articles .",Implementation Details,Implementation Details,natural_language_inference,93,2,1,0,,0.043689407,0,experimental-setup,0.000974312,0.000761904,0.001301565,0.040353476,0.020978566,0.562786269,0.025393472,0.028960495,0.000126133,0.315366715,0.00079156,0.000465382,0.001740151
10114,natural_language_inference93,106,"The dataset is randomly partitioned into a training set ( 80 % ) , a development set ( 10 % ) , and a test set ( 10 % ) .",Implementation Details,Implementation Details,natural_language_inference,93,3,1,0,,0.677963816,1,hyperparameters,8.05E-05,0.000191125,7.86E-05,1.12E-05,4.12E-05,0.326366425,0.001491199,0.364951403,6.50E-05,0.30660414,1.47E-05,7.61E-05,2.84E-05
10115,natural_language_inference93,107,The answer to every question is a segment of the corresponding passage .,Implementation Details,Implementation Details,natural_language_inference,93,4,1,0,,0.001842594,0,negative,0.000121311,9.34E-05,0.000336319,1.91E-05,4.42E-05,0.036640718,0.000596661,0.018328966,0.000137617,0.943275295,0.00020027,0.00017083,3.53E-05
10116,natural_language_inference93,108,We use the tokenizer from Stanford CoreNLP to preprocess each passage and question .,Implementation Details,Implementation Details,natural_language_inference,93,5,1,1,experimental-setup,0.968212813,1,experimental-setup,3.67E-05,7.96E-05,7.20E-05,6.90E-05,1.71E-05,0.753338344,0.002471847,0.223701596,5.09E-05,0.0200882,5.88E-06,1.27E-05,5.61E-05
10117,natural_language_inference93,109,The Gated Recurrent Unit variant of LSTM is used throughout our model .,Implementation Details,Implementation Details,natural_language_inference,93,6,1,1,experimental-setup,0.53566908,1,experimental-setup,7.59E-05,0.000256707,0.000994007,1.06E-05,3.44E-06,0.510578803,0.00223997,0.44509443,0.000628898,0.039973901,4.11E-05,2.17E-05,8.05E-05
10118,natural_language_inference93,110,"For word embedding , we use pretrained case - sensitive GloVe embeddings 2 ( Pennington et al. , 2014 ) for both questions and passages , and it is fixed during training ; We use zero vectors to represent all out - of - vocab words .",Implementation Details,Implementation Details,natural_language_inference,93,7,1,1,experimental-setup,0.991079074,1,hyperparameters,1.40E-05,5.29E-05,1.46E-05,3.64E-06,9.98E-07,0.456276633,0.000668747,0.536770469,2.90E-05,0.006145602,2.54E-06,5.39E-06,1.55E-05
10119,natural_language_inference93,111,"We utilize 1 layer of bi-directional GRU to compute character - level embeddings and 3 layers of bi-directional GRU to encode questions and passages , the gated attention - based recurrent network for question and passage matching is also encoded bidirectionally in our experiment .",Implementation Details,Implementation Details,natural_language_inference,93,8,1,1,experimental-setup,0.912087198,1,experimental-setup,0.000120445,0.000419266,0.000649285,1.76E-05,1.12E-05,0.505854405,0.003074557,0.456518073,0.00048344,0.032674468,2.32E-05,4.23E-05,0.000111766
10120,natural_language_inference93,112,The hidden vector length is set to 75 for all layers .,Implementation Details,Implementation Details,natural_language_inference,93,9,1,1,experimental-setup,0.990536291,1,hyperparameters,7.28E-06,1.72E-05,3.02E-06,2.24E-06,4.98E-07,0.3578553,0.000546135,0.638446647,1.21E-05,0.003087567,2.28E-06,5.04E-06,1.47E-05
10121,natural_language_inference93,113,The hidden size used to compute attention scores is also 75 .,Implementation Details,Implementation Details,natural_language_inference,93,10,1,1,experimental-setup,0.952921444,1,hyperparameters,2.15E-05,4.06E-05,1.42E-05,4.02E-06,1.35E-06,0.408610723,0.00110841,0.583153836,3.20E-05,0.006961192,5.06E-06,1.36E-05,3.35E-05
10122,natural_language_inference93,114,We also apply dropout between layers with a dropout rate of 0.2 .,Implementation Details,Implementation Details,natural_language_inference,93,11,1,1,experimental-setup,0.994234453,1,hyperparameters,4.28E-05,5.48E-05,1.93E-05,1.58E-05,2.30E-06,0.492979221,0.001420842,0.502443788,2.87E-05,0.002924424,1.84E-06,1.17E-05,5.45E-05
10123,natural_language_inference93,115,"The model is optimized with AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 1 .",Implementation Details,Implementation Details,natural_language_inference,93,12,1,1,experimental-setup,0.994438408,1,hyperparameters,8.01E-06,1.69E-05,5.82E-06,3.01E-06,5.33E-07,0.47281169,0.000684732,0.523691341,1.12E-05,0.002745638,1.14E-06,3.75E-06,1.63E-05
10124,natural_language_inference93,116,The ? and used in AdaDelta are 0.95 and 1e ? 6 respectively .,Implementation Details,Implementation Details,natural_language_inference,93,13,1,1,experimental-setup,0.863427551,1,hyperparameters,9.62E-06,1.65E-05,3.81E-06,2.67E-06,7.93E-07,0.432883535,0.000559492,0.559121719,1.16E-05,0.007362633,2.33E-06,9.20E-06,1.61E-05
10125,natural_language_inference93,117,Dev Set,Implementation Details,,natural_language_inference,93,14,1,0,,0.101378414,0,experimental-setup,9.12E-05,2.30E-05,8.09E-05,1.50E-05,1.49E-05,0.511144001,0.002868214,0.207660251,3.22E-05,0.277682564,8.43E-06,0.000324932,5.44E-05
10126,natural_language_inference93,118,Test Set,Implementation Details,,natural_language_inference,93,15,1,0,,0.022501514,0,negative,0.000152092,9.87E-05,0.000459811,3.04E-06,1.55E-05,0.082181451,0.009899324,0.103130525,3.73E-05,0.796110183,0.000114689,0.007722555,7.49E-05
10127,natural_language_inference93,119,Single model EM / F1 EM / F1 LR Baseline 40.0 / 51.0 40.4 / 51.0 Dynamic Chunk Reader 62.5 / 71.2 62.5 / 71.0 Match- LSTM with Ans - Ptr 64.1 / 73.9 64.7 / 73.7 Dynamic Coattention Networks 65.4 / 75.6 66.2 / 75.9 RaSoR 66.4 / 74.9 -/ - BiDAF 68,Implementation Details,Test Set,natural_language_inference,93,16,1,0,,0.147769597,0,experimental-setup,0.000145893,8.68E-07,0.000166343,2.25E-06,2.11E-06,0.408443936,0.192690325,0.010660327,2.05E-06,0.384596044,2.54E-06,0.001497558,0.001789765
10128,natural_language_inference93,120,Main Results,,,natural_language_inference,93,0,1,0,,0.204992025,0,negative,3.39E-05,5.69E-05,4.65E-06,1.50E-07,2.86E-07,2.47E-05,2.53E-05,0.000366261,2.49E-05,0.998372142,0.000758001,0.000332078,7.58E-07
10129,natural_language_inference93,121,Two metrics are utilized to evaluate model performance : Exact Match ( EM ) and F1 score .,Main Results,Main Results,natural_language_inference,93,1,1,0,,0.00937948,0,negative,0.000514656,7.83E-05,0.002350521,1.86E-06,2.35E-06,7.14E-05,6.69E-05,0.000509973,3.03E-05,0.988839588,0.000146061,0.007370734,1.73E-05
10130,natural_language_inference93,122,EM measures the percentage of the prediction that matches one of the ground truth answers exactly .,Main Results,Main Results,natural_language_inference,93,2,1,0,,0.006501271,0,negative,0.000625423,4.78E-05,0.048011467,1.59E-06,1.73E-06,0.00016622,0.000123302,0.000801187,8.26E-05,0.945922051,0.000217947,0.003950384,4.83E-05
10131,natural_language_inference93,123,F1 measures the overlap between the prediction and ground truth answers which takes the maximum F 1 over all of the ground truth answers .,Main Results,Main Results,natural_language_inference,93,3,1,0,,0.004169293,0,negative,0.000246033,2.03E-05,0.000660861,1.19E-06,1.04E-06,0.000112934,3.03E-05,0.000812933,2.48E-05,0.996695369,3.00E-05,0.001350954,1.34E-05
10132,natural_language_inference93,124,The scores on dev set are evaluated by the official script 3 .,Main Results,Main Results,natural_language_inference,93,4,1,0,,0.008516357,0,negative,0.000353611,9.35E-06,0.000285281,4.78E-07,8.07E-07,0.000346913,8.46E-05,0.002485817,7.87E-06,0.994702768,3.24E-06,0.001712699,6.58E-06
10133,natural_language_inference93,125,"Since the test set is hidden , we are required to submit the model to Stanford NLP group to obtain the test scores .",Main Results,Main Results,natural_language_inference,93,5,1,0,,0.000485851,0,negative,0.000972053,1.11E-05,0.000243999,2.28E-06,3.59E-06,3.60E-05,2.23E-05,0.000211822,1.62E-05,0.996451551,7.90E-06,0.002012246,9.00E-06
10134,natural_language_inference93,126,Ablation Study,,,natural_language_inference,93,0,1,0,,0.006818876,0,negative,0.046739945,0.000279772,0.002808551,0.000142874,8.76E-05,0.000276199,0.001210224,0.00046093,0.000147782,0.934438164,0.002965146,0.010376843,6.60E-05
10135,natural_language_inference93,127,We do ablation tests on the dev set to analyze the contribution of components of gated self - matching networks .,Ablation Study,Ablation Study,natural_language_inference,93,1,1,0,,0.027758295,0,ablation-analysis,0.862838663,3.31E-06,0.000524822,1.85E-06,7.09E-05,1.24E-05,1.39E-05,3.99E-07,1.22E-05,0.136488096,2.42E-06,4.96E-06,2.61E-05
10136,natural_language_inference93,128,"As illustrated in , the gated : Part of the attention matrices for self - matching .",Ablation Study,Ablation Study,natural_language_inference,93,2,1,0,,0.032343377,0,ablation-analysis,0.526044006,2.82E-05,0.083822651,2.42E-06,1.44E-05,7.44E-05,4.25E-05,5.45E-06,0.00467148,0.38483504,4.83E-05,5.60E-06,0.000405642
10137,natural_language_inference93,129,Each row is the attention weights of the whole passage for the current passage word .,Ablation Study,Ablation Study,natural_language_inference,93,3,1,0,,0.017661878,0,negative,0.1212399,0.00017763,0.01092256,3.29E-06,3.43E-05,0.000298946,7.59E-05,7.26E-05,0.014983106,0.851583048,6.39E-05,2.36E-06,0.000542436
10138,natural_language_inference93,130,The darker the color is the higher the weight is .,Ablation Study,Ablation Study,natural_language_inference,93,4,1,0,,0.002393085,0,negative,0.044897711,1.53E-06,0.000279342,1.96E-06,9.19E-06,2.94E-05,4.26E-06,2.02E-06,3.96E-05,0.954667967,1.23E-05,1.26E-06,5.34E-05
10139,natural_language_inference93,131,Some key evidence relevant to the question - passage tuple is more encoded into answer candidates .,Ablation Study,Ablation Study,natural_language_inference,93,5,1,0,,0.100377156,0,ablation-analysis,0.671819106,1.26E-05,0.004322098,2.32E-07,7.18E-06,9.55E-06,1.20E-05,1.19E-06,0.000464923,0.323313513,6.07E-06,5.24E-06,2.63E-05
10140,natural_language_inference93,132,attention - based recurrent network ( GARNN ) and self - matching attention mechanism positively contribute to the final results of gated self - matching networks .,Ablation Study,Ablation Study,natural_language_inference,93,6,1,1,ablation-analysis,0.955650964,1,ablation-analysis,0.889202913,1.58E-05,0.00585477,8.45E-07,4.00E-06,1.63E-05,7.43E-05,1.31E-06,0.000174606,0.102924365,0.000976194,2.10E-05,0.000733578
10141,natural_language_inference93,133,"Removing self - matching results in 3.5 point EM drop , which reveals that information in the passage plays an important role .",Ablation Study,Ablation Study,natural_language_inference,93,7,1,1,ablation-analysis,0.98192843,1,ablation-analysis,0.999653525,6.06E-08,2.60E-06,1.11E-07,4.99E-07,3.21E-07,3.23E-06,1.94E-08,1.75E-07,0.000330213,3.72E-08,7.73E-07,8.44E-06
10142,natural_language_inference93,134,Characterlevel embeddings contribute towards the model 's performance since it can better handle out - ofvocab or rare words .,Ablation Study,Ablation Study,natural_language_inference,93,8,1,1,ablation-analysis,0.963551514,1,ablation-analysis,0.99841457,1.75E-07,1.72E-05,3.79E-07,8.86E-07,1.03E-06,3.69E-06,5.83E-08,1.35E-06,0.001533983,2.08E-07,2.63E-06,2.39E-05
10143,natural_language_inference93,135,"To show the effectiveness of GARNN for variant RNNs , we conduct experiments on the base model of different variant RNNs .",Ablation Study,Ablation Study,natural_language_inference,93,9,1,0,,0.085417301,0,ablation-analysis,0.678711338,2.50E-05,0.000194251,1.43E-07,6.45E-06,1.21E-05,0.000104479,2.83E-06,2.19E-05,0.320873379,3.98E-06,2.34E-05,2.08E-05
10144,natural_language_inference93,136,"The base model match the question and passage via a variant of attentionbased recurrent network , and employ pointer networks to predict the answer .",Ablation Study,Ablation Study,natural_language_inference,93,10,1,0,,0.199392325,0,baselines,0.215224438,0.000158717,0.380117676,3.44E-06,4.45E-05,0.000149656,0.000112076,1.17E-05,0.043939894,0.358909119,4.02E-05,3.97E-06,0.001284675
10145,natural_language_inference93,137,Character - level embeddings are not utilized .,Ablation Study,Ablation Study,natural_language_inference,93,11,1,1,ablation-analysis,0.057881992,0,ablation-analysis,0.672201892,6.26E-06,0.001229756,5.34E-06,3.73E-05,9.48E-05,2.88E-05,5.61E-06,5.12E-05,0.326195123,1.63E-06,4.19E-06,0.000137966
10146,natural_language_inference93,138,"As shown in , the gate introduced in question and passage matching layer is helpful for both GRU and LSTM on the SQuAD dataset .",Ablation Study,Ablation Study,natural_language_inference,93,12,1,1,ablation-analysis,0.938233084,1,ablation-analysis,0.999432756,1.15E-07,3.78E-06,2.78E-08,1.26E-07,5.15E-07,2.42E-05,7.48E-08,2.28E-07,0.000501851,1.23E-07,9.98E-06,2.62E-05
10147,natural_language_inference93,139,Discussion,Ablation Study,,natural_language_inference,93,13,1,0,,0.001345646,0,negative,0.081371266,5.36E-06,0.000208327,4.59E-06,1.27E-05,7.87E-05,3.80E-05,8.29E-06,0.000116892,0.917443077,4.06E-05,2.12E-06,0.000669992
10148,natural_language_inference93,140,Encoding Evidence from Passage,Ablation Study,,natural_language_inference,93,14,1,0,,0.035507959,0,ablation-analysis,0.582301901,0.000110953,0.004595874,5.32E-06,2.55E-05,9.48E-05,0.001982174,1.05E-05,0.000536022,0.354383477,0.01706877,0.000238926,0.038645862
10149,natural_language_inference93,141,"To show the ability of the model for encoding evidence from the passage , we draw the align - ment of the passage against itself in self - matching .",Ablation Study,Encoding Evidence from Passage,natural_language_inference,93,15,1,0,,8.55E-06,0,negative,0.000673975,4.67E-06,6.17E-05,2.01E-08,2.04E-06,2.47E-06,1.05E-05,1.25E-06,3.36E-05,0.999198815,4.66E-07,1.03E-05,1.21E-07
10150,natural_language_inference93,142,"The attention weights are shown in , in which the darker the color is the higher the weight is .",Ablation Study,Encoding Evidence from Passage,natural_language_inference,93,16,1,0,,0.000195097,0,negative,0.000352305,8.16E-06,2.11E-05,2.33E-07,1.61E-06,6.96E-05,3.72E-05,5.40E-05,0.000260666,0.999189463,1.39E-06,3.09E-06,1.12E-06
10151,natural_language_inference93,143,We can see that key evidence aggregated from the whole passage is more encoded into the answer candidates .,Ablation Study,Encoding Evidence from Passage,natural_language_inference,93,17,1,0,,0.01569009,0,ablation-analysis,0.658904552,1.23E-05,3.86E-05,3.20E-07,3.87E-06,7.67E-06,0.00042252,6.95E-06,5.51E-05,0.33893394,4.31E-06,0.001604829,5.06E-06
10152,natural_language_inference93,144,"For example , the answer "" Egg of Columbus "" pays more attention to the key information "" Tesla "" , "" device "" and the lexical variation word "" known "" thatare relevant to the question - passage tuple .",Ablation Study,Encoding Evidence from Passage,natural_language_inference,93,18,1,0,,1.55E-05,0,negative,0.000823239,3.62E-07,1.17E-05,5.31E-08,5.80E-06,2.80E-06,5.73E-06,5.12E-07,1.99E-06,0.999137832,3.82E-07,9.41E-06,1.67E-07
10153,natural_language_inference93,145,"The answer "" world classic of epoch- making oratory "" mainly focuses on the evidence "" Michael Mullet "" , "" speech "" and lexical variation word "" considers "" .",Ablation Study,Encoding Evidence from Passage,natural_language_inference,93,19,1,0,,2.48E-05,0,negative,0.000176115,3.34E-07,6.01E-06,1.84E-07,4.73E-06,5.63E-06,5.69E-06,8.98E-07,1.91E-06,0.999791746,3.17E-06,3.06E-06,5.22E-07
10154,natural_language_inference93,146,"For other words , the attention weights are more evenly distributed between evidence and some irrelevant parts .",Ablation Study,Encoding Evidence from Passage,natural_language_inference,93,20,1,0,,0.000771651,0,negative,0.01411308,9.10E-05,0.000191556,2.77E-07,5.11E-06,1.37E-05,5.14E-05,1.93E-05,0.001156425,0.984287323,2.53E-06,6.67E-05,1.52E-06
10155,natural_language_inference93,147,Selfmatching do adaptively aggregate evidence for words in passage .,Ablation Study,Encoding Evidence from Passage,natural_language_inference,93,21,1,0,,0.00069576,0,negative,0.004064301,4.79E-05,0.003118537,1.91E-06,1.21E-05,4.67E-05,0.000361187,9.85E-06,0.000417461,0.99075677,0.000950183,0.000136379,7.67E-05
10156,natural_language_inference93,148,Result Analysis,,,natural_language_inference,93,0,1,0,,0.000601765,0,negative,1.37E-05,3.72E-05,1.33E-06,2.64E-07,1.89E-07,6.01E-05,7.71E-05,0.001320129,1.84E-05,0.993161332,0.00486798,0.000440059,2.18E-06
10157,natural_language_inference93,149,"To further analyse the model 's performance , we analyse the F1 score for different question types ) , different answer lengths ( ) , different passage lengths ) and different question lengths ) of our model and its ablation models .",Result Analysis,Result Analysis,natural_language_inference,93,1,1,0,,0.013904631,0,negative,0.000373367,9.65E-06,4.93E-05,2.15E-07,5.45E-07,2.92E-05,5.73E-05,0.000187348,1.29E-06,0.986863117,1.38E-05,0.012413956,9.55E-07
10158,natural_language_inference93,150,"As we can see , both four models show the same trend .",Result Analysis,Result Analysis,natural_language_inference,93,2,1,0,,0.426529136,0,negative,0.00722465,7.38E-06,9.62E-05,2.91E-06,1.17E-06,0.000406847,0.001398025,0.00164418,3.73E-06,0.697632322,0.000114772,0.291443893,2.39E-05
10159,natural_language_inference93,151,"The questions are split into different groups based on a set of question words we have defined , including "" what "" , "" how "" , "" who "" , "" when "" , "" which "" , "" where "" , and "" why "" .",Result Analysis,Result Analysis,natural_language_inference,93,3,1,0,,0.002447636,0,negative,0.000114324,1.40E-05,4.64E-05,1.29E-06,3.29E-06,0.000192746,6.96E-05,0.00088608,7.01E-06,0.997797621,5.16E-06,0.000860516,2.05E-06
10160,natural_language_inference93,152,"As we can see , our model is better at "" when "" and "" who "" questions , but poorly on "" why "" questions .",Result Analysis,Result Analysis,natural_language_inference,93,4,1,0,,0.167100688,0,results,0.007315583,3.54E-06,6.84E-05,5.50E-06,1.38E-06,0.00029139,0.001652763,0.00099832,1.87E-06,0.282644634,4.59E-05,0.706931769,3.90E-05
10161,natural_language_inference93,153,"This is mainly because the answers to why questions can be very diverse , and they are not restricted to any certain type of phrases .",Result Analysis,Result Analysis,natural_language_inference,93,5,1,0,,0.000828037,0,negative,6.87E-05,1.82E-06,7.12E-06,4.55E-07,1.41E-07,2.04E-05,1.59E-05,0.00010377,9.73E-07,0.998856128,0.000108246,0.000815497,8.09E-07
10162,natural_language_inference93,154,"From the Graph 3 ( b ) , the performance of our model obviously drops with the increase of answer length .",Result Analysis,Result Analysis,natural_language_inference,93,6,1,0,,0.898562177,1,results,0.019965995,2.16E-06,4.62E-05,3.11E-07,3.15E-07,1.75E-05,0.000548823,0.000156011,2.85E-07,0.125000979,2.65E-05,0.854226251,8.58E-06
10163,natural_language_inference93,155,Longer answers are harder to predict .,Result Analysis,Result Analysis,natural_language_inference,93,7,1,0,,0.087699257,0,negative,0.000143702,5.74E-07,8.94E-06,2.62E-07,1.85E-07,3.21E-05,8.68E-05,0.000116727,3.57E-07,0.989519467,3.19E-05,0.010057179,1.83E-06
10164,natural_language_inference93,156,"From Graph 3 ( c ) and 3 ( d ) , we discover that the performance remains stable with the increase in length , the obvious fluctuation in longer passages and questions is mainly because the proportion is too small .",Result Analysis,Result Analysis,natural_language_inference,93,8,1,0,,0.489830339,0,results,0.025036611,2.51E-06,5.47E-05,3.81E-07,4.21E-07,2.35E-05,0.000436157,0.000198668,4.51E-07,0.239973669,1.60E-05,0.734250075,6.88E-06
10165,natural_language_inference93,157,Our model is largely agnostic to long passages and focuses on important part of the passage .,Result Analysis,Result Analysis,natural_language_inference,93,9,1,0,,0.002358245,0,negative,0.003945418,0.000314486,0.002638496,1.02E-05,8.74E-06,0.00034674,0.000180511,0.001455927,0.00014825,0.97610882,0.000173077,0.014638564,3.08E-05
10166,natural_language_inference93,158,Related Work,,,natural_language_inference,93,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
10167,natural_language_inference93,203,Conclusion,,,natural_language_inference,93,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
10168,text_summarization3,1,title,,,text_summarization,3,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
10169,text_summarization3,2,Concept Pointer Network for Abstractive Summarization,title,,text_summarization,3,1,1,1,research-problem,0.995996526,1,research-problem,2.95E-08,2.98E-06,4.43E-08,2.50E-07,9.41E-08,1.38E-07,1.83E-06,8.25E-07,2.84E-07,0.001673492,0.99831983,1.14E-07,9.34E-08
10170,text_summarization3,3,abstract,,,text_summarization,3,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
10171,text_summarization3,4,A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details .,abstract,abstract,text_summarization,3,1,1,0,,0.004879624,0,research-problem,9.01E-07,0.000176818,1.88E-07,5.35E-06,1.24E-05,1.39E-06,1.36E-06,1.02E-05,5.80E-06,0.173739764,0.826044943,6.13E-07,1.99E-07
10172,text_summarization3,5,"Inspired by the popular pointer generator sequence - tosequence model , this paper presents a concept pointer network for improving these aspects of abstractive summarization .",abstract,abstract,text_summarization,3,2,1,0,,0.81845942,1,research-problem,1.75E-05,0.063824971,0.000168798,6.62E-06,0.000107156,1.43E-05,1.61E-05,0.000140732,0.011390844,0.106784215,0.817517653,8.90E-06,2.21E-06
10173,text_summarization3,6,"The network leverages knowledge - based , context - aware conceptualizations to derive an extended set of candidate concepts .",abstract,abstract,text_summarization,3,3,1,0,,0.2492046,0,negative,2.89E-05,0.155701487,0.000770393,7.18E-06,0.000255107,5.77E-05,1.13E-05,0.00037774,0.300304348,0.481523197,0.06095618,4.28E-06,2.19E-06
10174,text_summarization3,7,The model then points to the most appropriate choice using both the concept set and original source text .,abstract,abstract,text_summarization,3,4,1,0,,0.002542591,0,negative,2.14E-06,0.007896025,2.41E-06,3.80E-06,2.39E-05,3.26E-05,1.58E-06,0.000471978,0.004492495,0.953184003,0.033888245,5.85E-07,2.18E-07
10175,text_summarization3,8,This joint approach generates abstractive summaries with higher - level semantic concepts .,abstract,abstract,text_summarization,3,5,1,0,,0.068734337,0,research-problem,6.55E-06,0.017348833,4.97E-05,3.97E-06,8.26E-05,6.51E-06,3.81E-06,5.88E-05,0.002608878,0.36756105,0.612264752,3.89E-06,6.41E-07
10176,text_summarization3,9,"The training model is also optimized in a way that adapts to different data , which is based on a novel method of distantly - supervised learning guided by reference summaries and testing set .",abstract,abstract,text_summarization,3,6,1,0,,0.19294165,0,approach,1.68E-05,0.501300735,6.63E-05,5.61E-06,0.000147557,4.67E-05,6.47E-06,0.001249635,0.199203416,0.273424628,0.024528091,2.88E-06,1.21E-06
10177,text_summarization3,10,"Overall , the proposed approach provides statistically significant improvements over several state - of - the - art models on both the DUC - 2004 and Gigaword datasets .",abstract,abstract,text_summarization,3,7,1,0,,0.032043239,0,negative,0.000300942,0.004080563,7.49E-06,0.000133083,0.00023811,0.000136238,0.000646189,0.002153262,8.80E-05,0.569349218,0.421160573,0.001671909,3.44E-05
10178,text_summarization3,11,A human evaluation of the model 's abstractive abilities also supports the quality of the summaries produced within this framework .,abstract,abstract,text_summarization,3,8,1,0,,0.004994081,0,negative,6.72E-06,0.000347203,2.67E-07,4.02E-05,0.000191422,1.66E-05,4.37E-06,7.89E-05,3.84E-05,0.958863013,0.040409119,3.35E-06,4.39E-07
10179,text_summarization3,12,"* Corresponding author Figure 1 : "" summary1 "" only copies keyword from the source text , while "" summary2 "" generates new concepts to convey the meaning .",abstract,abstract,text_summarization,3,9,1,0,,0.00077065,0,negative,1.08E-06,0.000440555,1.95E-06,0.000111954,9.80E-05,3.40E-05,5.64E-06,6.79E-05,0.0002475,0.773205954,0.225784128,3.23E-07,1.01E-06
10180,text_summarization3,13,Introduction,,,text_summarization,3,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
10181,text_summarization3,14,Abstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .,Introduction,Introduction,text_summarization,3,1,1,1,research-problem,0.938877493,1,research-problem,2.46E-06,0.00023872,1.41E-06,8.64E-06,2.59E-05,4.09E-06,4.15E-05,4.59E-06,3.94E-05,0.01903441,0.980590652,4.19E-06,4.05E-06
10182,text_summarization3,15,"In tandem with seq2seq models , pointer generator was developed by as a solution to tackle the rare words and out - of - vocabulary ( OOV ) problem associated with generative - based models .",Introduction,Introduction,text_summarization,3,2,1,0,,0.831315484,1,model,0.000251224,0.205455347,0.00255878,0.000131937,0.004956882,0.000456616,0.000398525,0.000370659,0.463837717,0.240585011,0.080876025,7.65E-05,4.48E-05
10183,text_summarization3,16,The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab - ulary distribution and the source text .,Introduction,Introduction,text_summarization,3,3,1,0,,0.820809547,1,model,4.48E-05,0.10158902,0.000179697,1.09E-05,0.000441367,7.55E-05,2.06E-05,9.17E-05,0.805472772,0.083952498,0.008111123,6.82E-06,3.26E-06
10184,text_summarization3,17,"Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries , title summarization , etc .",Introduction,Introduction,text_summarization,3,4,1,0,,0.019998694,0,research-problem,3.14E-06,0.000759278,2.25E-06,6.37E-06,1.41E-05,6.17E-05,3.90E-05,8.17E-05,0.001425819,0.303806727,0.693792276,3.87E-06,3.68E-06
10185,text_summarization3,18,"However , the current power of abstractive summarization falls short of their potential .",Introduction,Introduction,text_summarization,3,5,1,0,,0.106981993,0,research-problem,3.15E-06,0.000188362,7.36E-07,1.84E-05,4.99E-05,1.30E-05,3.03E-05,9.26E-06,4.20E-05,0.127008438,0.872629077,3.88E-06,3.49E-06
10186,text_summarization3,19,"As the example in shows , a seq2seq model with a pointer mechanism ( marked as the direct pointer ) is likely to merely copy parts of the original text to form a summary using keywords and phrases , such as "" 317 athletes "" .",Introduction,Introduction,text_summarization,3,6,1,0,,0.020300965,0,negative,1.61E-05,0.002709822,3.84E-06,3.08E-05,0.00113436,9.17E-05,2.02E-05,4.37E-05,0.0019055,0.968391566,0.025644336,6.20E-06,1.87E-06
10187,text_summarization3,20,"Conversely , a more humanlike summary would be based on one 's own understanding of the detail in the words , expressed as higher - level concepts drawn from world knowledge - like using the word "" group "" to replace "" athletes and officials "" .",Introduction,Introduction,text_summarization,3,7,1,0,,0.002634935,0,negative,8.33E-06,0.00358136,3.73E-06,3.15E-05,0.001079519,0.000185931,1.60E-05,9.87E-05,0.005242656,0.982664405,0.007083324,3.17E-06,1.41E-06
10188,text_summarization3,21,"This indicates that a good summary should not simply copy original material , it should also generate new and even abstract concepts that reflect high - level semantics .",Introduction,Introduction,text_summarization,3,8,1,0,,0.024218682,0,negative,2.60E-05,0.003821826,4.19E-06,1.15E-05,0.000582699,3.89E-05,8.86E-06,2.75E-05,0.003347658,0.985406241,0.006718895,5.13E-06,6.51E-07
10189,text_summarization3,22,"Therefore , a pointer generator network that solely considers the source material to generate a summary does not adequately satisfy the needs of high - quality abstractive summarization .",Introduction,Introduction,text_summarization,3,9,1,0,,0.049226113,0,research-problem,1.37E-05,0.003395814,2.24E-06,3.61E-06,5.47E-05,9.86E-06,2.83E-05,2.28E-05,0.000996754,0.368444349,0.627008596,1.73E-05,1.83E-06
10190,text_summarization3,23,We argue that concepts have a greater ability to express deeper meanings than verbatim words .,Introduction,Introduction,text_summarization,3,10,1,0,,0.010548262,0,negative,2.75E-05,0.005895077,3.50E-06,2.92E-05,0.000544804,0.000155646,2.71E-05,0.000158893,0.006278871,0.965909582,0.020959915,7.52E-06,2.38E-06
10191,text_summarization3,24,"As such , it is essential to explore the potential of us - ing concepts from world knowledge to assist with abstractive summarization .",Introduction,Introduction,text_summarization,3,11,1,0,,0.081730391,0,negative,7.77E-06,0.003457973,1.41E-06,2.76E-05,0.000160575,5.42E-05,3.40E-05,8.52E-05,0.001551857,0.565911246,0.428696305,7.66E-06,4.18E-06
10192,text_summarization3,25,Our developed model not only points to informative source texts but also leverages conceptual words from human knowledge in the summaries it generates .,Introduction,Introduction,text_summarization,3,12,1,0,,0.87672684,1,model,2.28E-05,0.096194664,4.30E-05,9.44E-07,0.000169758,1.19E-05,9.22E-06,2.48E-05,0.883905764,0.018325103,0.001283519,7.45E-06,1.13E-06
10193,text_summarization3,26,"Hence , in this paper , we propose a novel model based on a concept pointer generator that encourages the generation of conceptual and abstract words .",Introduction,Introduction,text_summarization,3,13,1,1,model,0.963736016,1,model,1.59E-05,0.083874121,8.28E-05,5.09E-07,7.33E-05,1.28E-05,1.45E-05,2.22E-05,0.904813535,0.008541022,0.002543836,4.34E-06,1.17E-06
10194,text_summarization3,27,"As a hidden benefit , the model also alleviates the OOV problems .",Introduction,Introduction,text_summarization,3,14,1,1,model,0.216021402,0,negative,0.012521636,0.058303213,0.000144283,0.000234692,0.006434167,0.000525261,0.001819997,0.000785086,0.038250245,0.873260919,0.005901035,0.001754942,6.45E-05
10195,text_summarization3,28,"Our model uses pointer network to capture the salient information from a source text , and then employs another pointer to generalize the detailed words according to their upper level of expressions .",Introduction,Introduction,text_summarization,3,15,1,1,model,0.924114204,1,model,5.43E-06,0.027081582,7.43E-05,3.30E-07,4.56E-05,9.58E-06,7.15E-06,1.35E-05,0.967861211,0.00457264,0.000326347,1.37E-06,8.51E-07
10196,text_summarization3,29,"Finally , the output is also consistent with language model by the seq2seq generator .",Introduction,Introduction,text_summarization,3,16,1,0,,0.501125455,1,negative,0.001990126,0.098717044,9.71E-05,1.27E-05,0.001246567,0.000204123,0.002308159,0.000641987,0.092644282,0.79216344,0.008134668,0.00181044,2.94E-05
10197,text_summarization3,30,Unique to our concept pointer is a set of concept candidates particular for a word that is drawn from a huge knowledge base .,Introduction,Introduction,text_summarization,3,17,1,0,,0.507190588,1,model,4.96E-05,0.252435635,0.000197635,6.70E-06,0.003443994,6.98E-05,5.27E-05,8.07E-05,0.675053353,0.067719779,0.000869336,1.73E-05,3.34E-06
10198,text_summarization3,31,"The set of candidates adheres to a concept distribution , where the probability of each concept being generated is linked to how strongly the candidate represents each word .",Introduction,Introduction,text_summarization,3,18,1,0,,0.860519796,1,model,3.38E-06,0.099218171,1.59E-05,2.83E-07,0.000104717,1.25E-05,5.48E-06,5.08E-05,0.885302452,0.015118269,0.000166725,9.33E-07,4.03E-07
10199,text_summarization3,32,"Moreover , the concept distribution is iteratively updated to better explain the target word given the context of the source material and inherent semantics in the texts .",Introduction,Introduction,text_summarization,3,19,1,0,,0.966280708,1,model,3.80E-06,0.046183326,1.02E-05,9.48E-08,2.14E-05,4.63E-06,2.95E-06,1.80E-05,0.949292799,0.004380251,8.19E-05,4.67E-07,2.10E-07
10200,text_summarization3,33,"Hence , the learned concept pointer points to the most suitable and expressive concepts or words .",Introduction,Introduction,text_summarization,3,20,1,0,,0.618164476,1,model,3.20E-06,0.035581612,1.33E-05,6.59E-08,2.58E-05,4.32E-06,2.46E-06,1.46E-05,0.935553217,0.028601421,0.000198965,8.46E-07,1.43E-07
10201,text_summarization3,34,The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .,Introduction,Introduction,text_summarization,3,21,1,1,model,0.951456404,1,model,1.12E-05,0.180527376,9.43E-06,3.32E-06,0.00015635,0.000399345,7.21E-05,0.003775794,0.772161616,0.042365776,0.000509805,2.63E-06,5.27E-06
10202,text_summarization3,35,"The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .",Introduction,Introduction,text_summarization,3,22,1,1,model,0.95212425,1,model,4.67E-06,0.072530945,2.10E-05,1.18E-07,4.66E-05,4.51E-06,4.47E-06,1.38E-05,0.921135226,0.00616064,7.69E-05,8.24E-07,2.59E-07
10203,text_summarization3,36,"Overall , the contributions of this paper are : 1 ) a novel concept pointer generator network that leverages context - aware conceptualization and a concept pointer , both of which are jointly integrated into the generator to deliver informative and abstract - oriented summaries ; 2 ) a novel distant supervision training strategy that favors model adaptation and generalization , which results in performance that outperforms the wellaccepted evaluation - based reinforcement learning optimization on a test - only dataset in terms of ROUGE metrics ; 3 ) a statistical analysis of quantitative results and human evaluations from comparative experiments with several state - of - the - art models that shows the proposed method provides promising performance .",Introduction,Introduction,text_summarization,3,23,1,0,,0.572032005,1,approach,0.000164993,0.689584364,0.000197735,1.69E-05,0.002971801,8.26E-05,0.000229591,0.000195099,0.263141403,0.041122742,0.002202149,7.53E-05,1.53E-05
10204,text_summarization3,37,Related Work,,,text_summarization,3,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
10205,text_summarization3,187,Results and Analysis,,,text_summarization,3,0,1,0,,0.004153659,0,negative,4.92E-05,4.74E-05,7.12E-06,1.41E-07,3.63E-07,1.90E-05,8.00E-05,0.000295611,1.21E-05,0.995144313,0.001609051,0.002734443,1.25E-06
10206,text_summarization3,188,"The following analysis focuses on investigating whether our model is , first , able to generate abstract and new concepts , and , second , how the over all quality performs against the baselines .",Results and Analysis,Results and Analysis,text_summarization,3,1,1,0,,0.001692393,0,negative,0.002579616,2.90E-06,0.000155458,2.45E-07,3.16E-07,6.32E-06,3.14E-05,3.16E-05,1.78E-06,0.96481939,2.97E-06,0.032364791,3.27E-06
10207,text_summarization3,189,Quantitative Analysis,Results and Analysis,,text_summarization,3,2,1,0,,0.03101486,0,negative,0.001504189,3.92E-06,0.000596897,2.78E-06,8.04E-07,0.000111189,0.000749907,0.000311317,9.27E-06,0.912806028,6.78E-05,0.083736751,9.92E-05
10208,text_summarization3,190,The results are presented in .,Results and Analysis,Quantitative Analysis,text_summarization,3,3,1,0,,1.24E-05,0,negative,0.000378364,1.18E-07,3.34E-06,6.16E-09,7.61E-08,9.58E-07,1.17E-06,1.13E-06,1.10E-07,0.99934164,4.50E-07,0.000272613,2.54E-08
10209,text_summarization3,191,We observe that our model outperformed all the strong state of - the - art models on both datasets in all metrics except for RG - 2 on Gigaword .,Results and Analysis,Quantitative Analysis,text_summarization,3,4,1,1,results,0.342487115,0,negative,0.036140754,8.72E-06,3.90E-05,1.31E-06,2.46E-06,5.44E-05,0.000782057,9.47E-05,1.29E-06,0.524381466,9.39E-05,0.438375348,2.45E-05
10210,text_summarization3,192,"In terms of the pointer generator performance , the improvements made by our concept pointer are statistically significant ( p < 0.01 ) across all metrics .",Results and Analysis,Quantitative Analysis,text_summarization,3,5,1,1,results,0.278567263,0,results,0.210805492,3.32E-06,3.25E-05,2.52E-07,1.23E-06,7.57E-06,0.00038669,1.75E-05,4.50E-07,0.354384415,2.00E-05,0.434334218,6.34E-06
10211,text_summarization3,193,OOV and Summary Length : OOV is another major challenge for current abstractive summa - : ROUGE F1 evaluation results on the Gigaword and ROUGE recall on DUC - 2004 test set .,Results and Analysis,Quantitative Analysis,text_summarization,3,6,1,0,,0.007129971,0,negative,0.004315942,1.15E-05,0.000224892,4.64E-07,1.04E-06,9.21E-06,0.000294586,1.53E-05,1.01E-06,0.93653026,0.013024249,0.045550716,2.08E-05
10212,text_summarization3,194,The results with mark are taken from the corresponding papers .,Results and Analysis,Quantitative Analysis,text_summarization,3,7,1,0,,3.99E-07,0,negative,2.70E-05,1.17E-07,6.93E-07,4.07E-08,2.43E-07,3.69E-06,4.75E-07,2.72E-06,1.75E-07,0.99995694,1.84E-07,7.66E-06,2.59E-08
10213,text_summarization3,195,Underlined scores are the best without additional optimization .,Results and Analysis,Quantitative Analysis,text_summarization,3,8,1,0,,2.69E-06,0,negative,4.74E-05,8.74E-07,4.32E-06,2.79E-08,5.65E-08,1.68E-05,2.46E-06,3.55E-05,2.93E-06,0.999850753,1.50E-06,3.72E-05,1.10E-07
10214,text_summarization3,196,Bold scores are the best between the two optimization strategies .,Results and Analysis,Quantitative Analysis,text_summarization,3,9,1,0,,1.85E-06,0,negative,6.27E-05,6.15E-07,3.03E-06,2.01E-08,4.90E-08,1.98E-05,3.11E-06,4.01E-05,1.91E-06,0.999809757,9.72E-07,5.78E-05,1.20E-07
10215,text_summarization3,197,mark indicates the improvements from the baselines to the concept pointer are statistically significant using a two - tailed t- test ( p < 0.01 ) .,Results and Analysis,Quantitative Analysis,text_summarization,3,10,1,0,,1.98E-05,0,negative,0.000777178,1.31E-06,3.65E-06,5.06E-07,9.98E-07,4.30E-05,1.31E-05,5.39E-05,1.82E-06,0.998654851,3.00E-06,0.000445514,1.21E-06
10216,text_summarization3,198,Models,,,text_summarization,3,0,1,0,,0.013378693,0,negative,0.001769058,0.000281584,0.001312991,0.00016118,0.000150821,0.001858418,0.032853537,0.00468419,0.00015464,0.882857056,0.025147519,0.048083361,0.000685644
10217,text_summarization3,199,Gigaword DUC-2004 RG-1 RG - 2 RG- L RG- 1 RG- 2 RG- L ABS + 29.76 11.88 26.96 28.18 8.46 23.81 Luong - NMT 33.10 14.45 30.71 28.55 8.79 24.43 RAS - Elman 33.78 15.97 31.15 28.97 8.26 24.06 lvt5 k-lsent 35.30 16.64 32.62 28.61 9.42 25.24 SEASS 36.15 17.54 33.63 29.21 9.56 25.51 Seq2seq + att ( our impl. ) 33.11 14.67 31.06 28.57 9.31 24.81 Pointer - generator ( our impl. ) rization models .,Models,Models,text_summarization,3,1,1,0,,0.007057809,0,negative,0.000760765,7.20E-07,0.00402486,6.42E-06,8.25E-06,0.000543915,0.196617744,6.97E-05,3.34E-06,0.793862233,5.55E-06,0.003633935,0.000462546
10218,text_summarization3,200,"Although generating longer summaries or less UNKs is not our focus , our model still showed improvements in this regard .",Models,Models,text_summarization,3,2,1,0,,0.442711178,0,results,0.02071722,7.12E-07,0.000623309,1.19E-06,1.23E-06,7.00E-05,0.392367488,5.34E-05,6.79E-07,0.12297257,5.16E-06,0.4626004,0.000586645
10219,text_summarization3,201,We counted the number of UNKs and all generated summary words and measured the proportions in both testing sets .,Models,Models,text_summarization,3,3,1,0,,0.090711109,0,negative,6.00E-05,1.14E-06,0.000750291,1.64E-07,1.02E-06,0.000180367,0.009386289,0.000101859,9.47E-06,0.989446413,4.40E-07,5.58E-05,6.74E-06
10220,text_summarization3,202,"The OOV percentages dropped from 4.02 % to 1.12 % on Gigaword and from 2.08 % to 0.23 % on DUC - 2004 , which demonstrates that our model is effective at alleviating OOV problems .",Models,Models,text_summarization,3,4,1,0,,0.942661564,1,ablation-analysis,0.412686746,6.21E-06,0.001936004,3.39E-06,7.25E-06,9.08E-05,0.324764506,6.16E-05,6.29E-06,0.12064595,7.91E-06,0.138870153,0.00091313
10221,text_summarization3,203,This result also supports the superior of the concept pointer over the baseline pointer generator .,Models,Models,text_summarization,3,5,1,0,,0.132364866,0,negative,0.004931772,1.16E-06,0.001639361,1.50E-07,4.28E-07,5.52E-05,0.086086008,2.89E-05,4.83E-06,0.888911677,1.32E-06,0.018303871,3.53E-05
10222,text_summarization3,204,"From the statistics , we found that the summaries generated by our concept pointer averaged around 10.46 words , while the pointer generator summaries averaged 10.28 words per summary on DUC - 2004 .",Models,Models,text_summarization,3,6,1,0,,0.707124859,1,negative,0.001620041,2.02E-06,0.000388208,3.80E-06,1.40E-05,0.000581469,0.452456882,0.000275326,4.05E-06,0.530775667,3.47E-06,0.012879938,0.000995155
10223,text_summarization3,205,This shows the concept pointer is able to capture more salient content by generating relatively longer summaries .,Models,Models,text_summarization,3,7,1,0,,0.736586701,1,negative,0.052485924,3.64E-06,0.003944222,6.78E-07,2.22E-06,7.93E-05,0.183719097,4.58E-05,1.06E-05,0.688642999,1.66E-06,0.070943273,0.000120659
10224,text_summarization3,206,"Abstractiveness : According to , abstractiveness scores are computed as the percentage of novel n-grams in the generated summaries thatare not included in the source documents .",Models,Models,text_summarization,3,8,1,0,,0.482102095,0,negative,0.00013281,6.45E-06,0.027800236,1.14E-06,3.51E-06,0.000606177,0.078158497,0.000313811,4.27E-05,0.892009421,8.30E-06,0.000682198,0.000234773
10225,text_summarization3,207,"As shown in , compared with human - written summaries which receive the highest novelty in terms of abstractiveness , our concept pointer generator achieves closest performance with human - written summaries against the baseline .",Models,Models,text_summarization,3,9,1,0,,0.942913917,1,experiments,0.005218418,7.33E-07,0.000730665,2.42E-07,4.82E-07,2.54E-05,0.497351851,2.52E-05,3.71E-07,0.05010827,1.54E-06,0.446141246,0.000395556
10226,text_summarization3,208,This result demonstrates a further advantage of our model in producing new and abstract concepts .,Models,Models,text_summarization,3,10,1,0,,0.459361778,0,negative,0.00500029,1.10E-06,0.00158097,1.72E-07,9.43E-07,2.73E-05,0.054173675,1.29E-05,4.98E-06,0.908732722,1.09E-06,0.030420109,4.38E-05
10227,text_summarization3,209,Our model is designed to improve semantic relevance and promote higher abstraction .,Models,Models,text_summarization,3,11,1,0,,0.688412497,1,baselines,0.000601833,0.00010618,0.66956166,1.77E-06,6.63E-06,0.000237011,0.015268176,0.000138996,0.01203694,0.301632801,9.24E-06,0.000153393,0.000245372
10228,text_summarization3,210,More generated summary examples can be found in Appendix B.,Models,Models,text_summarization,3,12,1,0,,0.012408573,0,negative,5.14E-05,4.47E-07,0.000122709,1.18E-06,2.48E-06,0.000121122,0.007229434,6.31E-05,4.40E-06,0.992224131,2.02E-07,0.000160014,1.94E-05
10229,text_summarization3,211,Analysis on Training Strategies,,,text_summarization,3,0,1,0,,0.001638271,0,negative,0.001461289,8.97E-05,9.26E-05,2.24E-05,9.60E-06,0.000142882,0.000406408,0.000477962,5.97E-05,0.988189689,0.007317709,0.001707089,2.31E-05
10230,text_summarization3,212,"To evaluate the relative impact of each training strategy with the model , we tested different combinations for comparison with each other and against the baselines .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,1,1,0,,0.000537283,0,negative,0.004362283,2.76E-05,1.34E-05,9.69E-08,1.52E-06,3.87E-05,2.02E-05,2.53E-05,4.71E-05,0.995444269,5.62E-06,1.37E-05,3.22E-07
10231,text_summarization3,213,Context - aware Conceptualization :,Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,2,1,0,,0.038853881,0,negative,0.099523473,0.000755281,0.008222394,2.17E-06,1.09E-05,4.08E-05,0.001856098,2.49E-05,0.000442699,0.800275804,0.084191534,0.004476503,0.000177451
10232,text_summarization3,214,"To investigate the impact of training with both the number of concepts k and the concept update strategy mentioned in Eqs. ( 6 ) and , we chose a dif - ferent number of concept candidates , i.e. , k = 1 , 2 , 3 , 4 , 5 , to for the context - aware conceptualization update strategy .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,3,1,0,,0.000789123,0,negative,0.049008173,0.000414079,5.80E-05,9.50E-07,2.19E-05,0.000138375,0.000157898,0.000130701,0.000183123,0.949795961,1.69E-05,7.13E-05,2.69E-06
10233,text_summarization3,215,Performance was fully evaluated with the three ROUGE metrics as shown in .,Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,4,1,0,,0.094052182,0,negative,0.11192255,1.31E-05,7.02E-05,5.91E-08,1.39E-06,6.72E-06,0.000185414,4.61E-06,7.54E-06,0.884951021,5.34E-05,0.002781945,2.11E-06
10234,text_summarization3,216,"The results only vary slightly according to the number of concepts with the random selection strategy ( Eq. ) , as shown in ( a ) and 3 ( b ) .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,5,1,0,,0.04763358,0,ablation-analysis,0.554768503,1.03E-05,1.89E-05,2.56E-07,1.90E-06,1.27E-05,0.000363964,1.19E-05,3.84E-06,0.440189686,5.39E-05,0.00455863,5.53E-06
10235,text_summarization3,217,This indicates that a random strategy is not very sensitive to the number of extracted topics .,Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,6,1,0,,0.000268481,0,negative,0.174932511,5.41E-06,1.76E-05,2.58E-07,4.62E-06,9.57E-06,2.44E-05,4.62E-06,5.65E-06,0.824845256,6.32E-06,0.000143019,8.22E-07
10236,text_summarization3,218,"This is , in part , because the concept pointer mayor may not be able to point to the correct concepts from multiple candidates .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,7,1,0,,2.72E-05,0,negative,0.002702009,3.45E-06,1.25E-05,4.09E-08,1.07E-06,2.54E-06,1.57E-06,1.11E-06,9.03E-06,0.997253579,5.32E-06,7.62E-06,1.36E-07
10237,text_summarization3,219,"While in ( c ) and 3 ( d ) , the optimum settings are clearly apparent , i.e. , k = 1 on Gigaword and k = 2 on DUC - 2004 .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,8,1,0,,0.003812944,0,negative,0.258607172,6.72E-05,2.88E-05,7.74E-06,3.56E-05,0.0001782,0.000263166,0.000109594,3.80E-05,0.740121127,4.72E-05,0.000478486,1.77E-05
10238,text_summarization3,220,"Overall , the hard assignment strategy ( Eq. ( 6 ) ) provided the best performance in practical terms , while random selection ( Eq. ) performs stably with different settings .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,9,1,0,,0.483825134,0,ablation-analysis,0.864144764,3.03E-05,2.96E-05,1.14E-05,2.42E-05,0.000119892,0.001178516,5.62E-05,1.27E-05,0.130471914,6.57E-05,0.003751636,0.000103213
10239,text_summarization3,221,"Training with DS vs. RL : As shown in Table 1 , our model with either a distant supervision strategy ( concept pointer + DS ) or reinforcement learning ( concept pointer + RL ) were both superior to the basic concept pointer generator on both datasets .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,10,1,0,,0.773458568,1,ablation-analysis,0.668841265,3.70E-05,0.000111671,6.35E-07,3.77E-06,6.09E-05,0.003568526,3.82E-05,1.62E-05,0.313586039,7.90E-05,0.013601122,5.57E-05
10240,text_summarization3,222,"Further , the relative improvement of the concept pointer + DS over the concept pointer + RL ranged from 3.5 % to 9.6 % on DUC - 2004 but was inferior to concept pointer + RL on Gigaword .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,11,1,0,,0.081720135,0,ablation-analysis,0.907627519,3.14E-06,3.43E-05,5.50E-07,4.96E-06,1.18E-05,0.000950994,4.36E-06,1.12E-06,0.084742302,1.05E-05,0.006573117,3.53E-05
10241,text_summarization3,223,"In comparing the results , it is clear that DS training has a noticeable effect when the testing set is substantially semantically different from the training set but provides less improvement than RL when the two are close .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,12,1,0,,0.379731068,0,ablation-analysis,0.980306504,1.77E-06,4.80E-06,2.46E-07,1.98E-06,3.41E-06,0.000160771,1.69E-06,6.46E-07,0.018749393,1.63E-06,0.000761836,5.33E-06
10242,text_summarization3,224,"From this analysis , we conclude that the DS strategy is better for model adaption with abstractive summarization .",Analysis on Training Strategies,Analysis on Training Strategies,text_summarization,3,13,1,0,,0.0990471,0,ablation-analysis,0.772132159,1.10E-05,3.25E-05,4.41E-07,4.29E-06,1.62E-05,0.000521076,7.94E-06,4.52E-06,0.223356414,3.28E-05,0.003865679,1.51E-05
10243,text_summarization3,225,Human Evaluations,,,text_summarization,3,0,1,0,,0.061917351,0,negative,1.81E-05,1.32E-05,4.31E-06,1.08E-07,3.56E-07,3.20E-05,0.000219148,0.000502102,2.34E-06,0.994125603,0.001855289,0.00322549,1.96E-06
10244,text_summarization3,226,"To explore the correctness of our model using human judgment , we conducted a manual evaluation with 20 post-graduate volunteers .",Human Evaluations,Human Evaluations,text_summarization,3,1,1,0,,8.81E-05,0,negative,0.000296038,3.49E-07,0.000667216,1.75E-07,5.18E-07,1.69E-05,0.001086364,4.19E-05,9.36E-08,0.982783078,9.99E-07,0.015100204,6.20E-06
10245,text_summarization3,227,"We primarily used the following criteria to assess the generated summaries : abstraction , i.e. , Are the abstract concepts contained in the summary appropriate ? ; and over all quality , i.e. , Is the summary readable , informative , relevant , etc . ?",Human Evaluations,Human Evaluations,text_summarization,3,2,1,0,,2.94E-05,0,negative,3.67E-05,1.55E-07,0.000369979,6.89E-08,4.01E-08,3.64E-05,0.000188357,9.34E-05,1.35E-07,0.997168786,1.18E-06,0.00210216,2.69E-06
10246,text_summarization3,228,"To conduct the evaluation , we randomly selected 20 examples from the DUC 2004 testing set and asked the volunteers to subjectively assess the summaries .",Human Evaluations,Human Evaluations,text_summarization,3,3,1,0,,0.000214163,0,negative,6.83E-05,2.67E-07,0.000125019,5.09E-08,1.03E-07,2.11E-05,0.000396064,0.000111012,8.44E-08,0.996862713,2.41E-07,0.002412897,2.19E-06
10247,text_summarization3,229,"Each example consisted of an article and three summaries , i.e. , a summary by the seq2seq model , the pointer generator model , and our proposed concept pointer model .",Human Evaluations,Human Evaluations,text_summarization,3,4,1,0,,0.000213825,0,negative,4.66E-05,1.56E-07,0.00040487,1.52E-07,2.16E-07,2.92E-05,0.000519858,7.40E-05,1.56E-07,0.996040017,4.36E-07,0.002877931,6.40E-06
10248,text_summarization3,230,The volunteers chose the best summaries for each of the articles according to the above criteria ( can be multiple choices ) .,Human Evaluations,Human Evaluations,text_summarization,3,5,1,0,,7.55E-05,0,negative,2.19E-05,7.65E-08,0.000381955,2.76E-08,2.79E-08,2.45E-05,0.000125205,6.97E-05,1.32E-07,0.998469318,1.22E-07,0.000905659,1.35E-06
10249,text_summarization3,231,"Obviously , the summaries were randomly shuffled , and the model used to produce each was unknown to prevent bias .",Human Evaluations,Human Evaluations,text_summarization,3,6,1,0,,5.15E-06,0,negative,3.84E-05,4.02E-08,9.85E-05,2.66E-08,1.39E-08,9.45E-06,4.26E-05,3.11E-05,8.45E-08,0.999350268,8.20E-08,0.000428899,5.31E-07
10250,text_summarization3,232,"The scores for each model were ranked by how many times the volunteers chose a summary w.r.t each criteria , averaged by the number of participants .",Human Evaluations,Human Evaluations,text_summarization,3,7,1,0,,3.28E-05,0,negative,2.78E-05,7.10E-08,0.000210745,1.76E-08,1.93E-08,4.25E-05,0.000230601,0.000130474,1.28E-07,0.998036489,8.39E-08,0.001320028,1.02E-06
10251,text_summarization3,233,"The results are presented in , which show that our model outperformed both the seq2seq model and the pointer generator in both criteria .",Human Evaluations,Human Evaluations,text_summarization,3,8,1,0,,0.887345322,1,results,0.000392098,1.43E-08,3.76E-05,1.30E-08,7.17E-09,2.23E-06,0.001740778,8.22E-06,2.85E-09,0.039719664,3.04E-07,0.958092396,6.70E-06
10252,text_summarization3,234,"As a last step , we manually inspect the summaries generated by our model , and some examples are presented in Appendix B .",Human Evaluations,Human Evaluations,text_summarization,3,9,1,0,,0.016358722,0,negative,0.000363728,1.57E-07,0.000148529,2.69E-07,2.68E-07,1.23E-05,0.000223355,3.36E-05,1.11E-07,0.994390854,7.95E-08,0.004823491,3.27E-06
10253,text_summarization3,235,We found that the summaries were not as abstract as humanwritten summary would likely be .,Human Evaluations,Human Evaluations,text_summarization,3,10,1,0,,0.000754874,0,negative,0.00025509,5.90E-08,5.17E-05,4.15E-07,1.15E-07,5.11E-05,0.000597694,8.19E-05,8.87E-08,0.976153022,9.20E-07,0.022793179,1.47E-05
10254,text_summarization3,236,The overarching tendency of the model is still to copy segments of the source text and rearrange the phrases into a summary .,Human Evaluations,Human Evaluations,text_summarization,3,11,1,0,,5.62E-06,0,negative,0.00075109,1.03E-06,0.013585054,7.21E-07,2.92E-07,3.00E-05,0.000252282,5.58E-05,3.14E-06,0.97972732,3.08E-06,0.005560367,2.98E-05
10255,text_summarization3,237,"However , the over all approach does produce more high - level concepts with correct relations compared to the baselines , which demonstrates that our solution is a promising research direction to further pursue .",Human Evaluations,Human Evaluations,text_summarization,3,12,1,0,,0.542187002,1,results,0.002130356,1.85E-08,6.02E-05,1.22E-08,1.44E-08,1.71E-06,0.000874224,6.75E-06,4.30E-09,0.101796133,1.44E-07,0.895124956,5.43E-06
10256,text_summarization3,238,"Additionally , the generated summaries are long , fluent , and informative .",Human Evaluations,Human Evaluations,text_summarization,3,13,1,0,,0.059603122,0,results,0.000871711,1.12E-07,0.000162909,8.66E-08,1.13E-07,7.31E-06,0.002575527,3.13E-05,2.88E-08,0.320865204,4.03E-07,0.675461298,2.40E-05
10257,text_summarization3,239,Conclusion,,,text_summarization,3,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
10258,prosody_prediction0,1,title,,,prosody_prediction,0,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
10259,prosody_prediction0,2,Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,title,title,prosody_prediction,0,1,1,1,research-problem,0.99848872,1,research-problem,3.79E-08,1.65E-05,8.22E-08,3.69E-08,4.08E-08,1.16E-07,8.99E-07,2.53E-06,2.83E-06,0.002196907,0.997779714,2.93E-07,4.97E-08
10260,prosody_prediction0,3,abstract,,,prosody_prediction,0,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
10261,prosody_prediction0,4,In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text .,abstract,abstract,prosody_prediction,0,1,1,1,research-problem,0.888578425,1,research-problem,3.10E-05,0.013268299,5.52E-05,0.005436076,0.066004856,0.00012744,0.000194127,0.000109797,0.000109555,0.328503498,0.58612967,1.29E-05,1.76E-05
10262,prosody_prediction0,5,To our knowledge this will be the largest publicly available dataset with prosodic labels .,abstract,abstract,prosody_prediction,0,2,1,0,,0.031606885,0,negative,7.01E-06,0.00308486,7.42E-06,0.003217784,0.044334825,0.000158185,1.74E-05,9.38E-05,0.000121565,0.935111123,0.01384307,1.51E-06,1.47E-06
10263,prosody_prediction0,6,We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature - based classifiers to neural network systems for the prediction of discretized prosodic prominence .,abstract,abstract,prosody_prediction,0,3,1,0,,0.01305568,0,negative,2.20E-06,0.012903541,7.17E-06,0.000295011,0.017849755,6.47E-05,1.10E-05,0.000131653,0.000105905,0.94794502,0.020682459,8.48E-07,7.36E-07
10264,prosody_prediction0,7,We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10 % of the training data .,abstract,abstract,prosody_prediction,0,4,1,0,,0.261809734,0,negative,0.000153387,0.012896847,7.15E-06,5.68E-05,0.000145355,0.000106523,0.000130522,0.002490756,0.000288319,0.559216813,0.424147848,0.000348416,1.13E-05
10265,prosody_prediction0,8,Finally we discuss the dataset in light of the results and point to future research and plans for further improving both the dataset and methods of predicting prosodic prominence from text .,abstract,abstract,prosody_prediction,0,5,1,0,,0.000777825,0,negative,3.39E-06,0.000740049,4.90E-07,0.000172885,0.000345321,5.50E-05,3.28E-06,9.40E-05,6.44E-05,0.985265027,0.013255244,5.85E-07,3.26E-07
10266,prosody_prediction0,9,The dataset and the code for the models are publicly available .,abstract,abstract,prosody_prediction,0,6,1,0,,0.16272038,0,negative,2.00E-06,0.000829272,1.06E-06,0.018013476,0.006881399,0.000646648,9.77E-06,0.00034923,4.91E-05,0.967165077,0.006051692,2.31E-07,1.08E-06
10267,prosody_prediction0,10,Introduction,,,prosody_prediction,0,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
10268,prosody_prediction0,11,"Prosodic prominence , i.e. , the amount of emphasis that a speaker gives to a word , has been widely studied in phonetics and speech processing .",Introduction,Introduction,prosody_prediction,0,1,1,0,,0.377367134,0,research-problem,1.96E-06,0.000540655,7.17E-07,9.01E-06,2.80E-05,1.27E-05,1.57E-05,1.88E-05,0.000213519,0.107885338,0.891269599,2.01E-06,2.05E-06
10269,prosody_prediction0,12,"However , the research on text - based natural language processing ( NLP ) methods for predicting prosodic prominence is somewhat limited .",Introduction,Introduction,prosody_prediction,0,2,1,0,,0.945850344,1,research-problem,7.15E-07,0.000185289,2.20E-07,2.76E-06,1.04E-05,4.19E-06,7.71E-06,6.68E-06,4.52E-05,0.066247273,0.933487766,8.28E-07,9.41E-07
10270,prosody_prediction0,13,"Even in the text - to - speech synthesis domain , with many recent methodological advances , work on symbolic prosody prediction has lagged behind .",Introduction,Introduction,prosody_prediction,0,3,1,0,,0.785446204,1,research-problem,7.43E-07,0.000118964,1.79E-07,2.37E-06,5.10E-06,3.54E-06,7.14E-06,5.12E-06,4.06E-05,0.055761947,0.944052523,9.15E-07,8.27E-07
10271,prosody_prediction0,14,We believe that this is mainly due to the lack of suitable datasets .,Introduction,Introduction,prosody_prediction,0,4,1,0,,0.015687627,0,negative,1.28E-05,0.004415876,2.19E-06,3.21E-05,0.000272535,0.000168434,2.22E-05,0.000174571,0.004829146,0.927922938,0.06214054,4.20E-06,2.44E-06
10272,prosody_prediction0,15,"Existing , publicly available annotated speech corpora , are very small by current standards .",Introduction,Introduction,prosody_prediction,0,5,1,0,,0.158281432,0,negative,8.09E-06,0.002383106,4.37E-06,0.00025141,0.005863996,0.000115542,0.000106977,7.68E-05,0.000280707,0.628205223,0.362680183,9.57E-06,1.40E-05
10273,prosody_prediction0,16,"In this paper we introduce a new NLP dataset and benchmark for predicting prosodic prominence from text which is based on the recently published Libri TTS corpus , containing automatically generated prosodic prominence labels for over 260 hours or 2.8 million words of English audio books , read by 1230 different speakers .",Introduction,Introduction,prosody_prediction,0,6,1,1,dataset,0.953373821,1,dataset,8.46E-05,0.098539294,0.00019538,0.003226526,0.750441957,0.000373766,0.00075125,0.000108439,0.002391072,0.11207031,0.031653722,8.52E-05,7.84E-05
10274,prosody_prediction0,17,To our knowledge this will be the largest publicly available dataset with prosodic annotations .,Introduction,Introduction,prosody_prediction,0,7,1,1,dataset,0.517617359,1,dataset,3.75E-05,0.020777564,4.45E-05,0.003760713,0.582481579,0.000661303,0.000161929,0.000110056,0.003352442,0.385825839,0.002750908,1.79E-05,1.77E-05
10275,prosody_prediction0,18,We first give some background about prosodic prominence and related research in Section 2 .,Introduction,Introduction,prosody_prediction,0,8,1,0,,0.022992948,0,negative,1.16E-05,0.025758388,4.79E-06,0.000103248,0.001764303,0.000353844,3.91E-05,0.000491672,0.01566458,0.92855928,0.027237884,5.26E-06,6.06E-06
10276,prosody_prediction0,19,We then describe the dataset construction and annotation method in Section 3 .,Introduction,Introduction,prosody_prediction,0,9,1,0,,0.036734768,0,negative,2.92E-05,0.11683767,1.89E-05,0.000287395,0.015950846,0.000440975,6.84E-05,0.000587642,0.045165384,0.813443911,0.007147836,1.10E-05,1.08E-05
10277,prosody_prediction0,20,Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text .,Introduction,Introduction,prosody_prediction,0,10,1,1,research-problem,0.692169973,1,research-problem,1.59E-06,0.00154254,1.98E-06,1.79E-05,5.42E-05,2.93E-05,3.11E-05,4.14E-05,0.000910923,0.11324561,0.884114414,3.08E-06,5.96E-06
10278,prosody_prediction0,21,In Section 4 we explain the experiments and the experimental results using a number of different sequence labeling approaches and show that pre-trained contextualized word representations from BERT outperform our other baselines even with less than 10 % of the training data .,Introduction,Introduction,prosody_prediction,0,11,1,0,,0.277020402,0,negative,0.002782119,0.13092797,0.000102874,6.60E-05,0.006714142,0.000289548,0.003339235,0.000631214,0.009675097,0.825910491,0.014746235,0.004761568,5.35E-05
10279,prosody_prediction0,22,"Although BERT has been previously applied in various sequence labeling tasks , like named entity recognition , to the best of our knowledge , this is the first application of BERT in the task of predicting prosodic prominence .",Introduction,Introduction,prosody_prediction,0,12,1,0,,0.855459717,1,research-problem,0.000116177,0.314818753,0.000252969,0.000136502,0.006628019,0.000146801,0.000430875,0.000218755,0.028108912,0.257769236,0.391200141,0.000131988,4.09E-05
10280,prosody_prediction0,23,"We analyse the results in Section 5 , comparing BERT to a bidirectional long shortterm memory ( BiLSTM ) model and looking at the types of errors made by these selected models .",Introduction,Introduction,prosody_prediction,0,13,1,0,,0.173617295,0,negative,0.000300501,0.125437266,8.90E-05,1.11E-05,0.005021621,0.000103804,0.000250191,0.000131195,0.027526397,0.836432299,0.004463374,0.000228454,4.75E-06
10281,prosody_prediction0,24,We find that BERT outperforms the BiLSTM model across all the labels .,Introduction,Introduction,prosody_prediction,0,14,1,0,,0.516688129,1,negative,0.00339062,0.092039887,0.000100264,0.002795732,0.007716059,0.011336835,0.024980838,0.023597384,0.014357581,0.780601293,0.034354767,0.003594387,0.001134354
10282,prosody_prediction0,25,Finally in Section 6 we discuss the methods in light of the experimental results and highlight are as thatare known to negatively impact the results .,Introduction,Introduction,prosody_prediction,0,15,1,0,,0.008644491,0,negative,0.000132169,0.014239839,1.60E-05,0.001291417,0.024791639,0.000699356,9.06E-05,0.000215161,0.003237927,0.954199971,0.001060346,1.71E-05,8.52E-06
10283,prosody_prediction0,26,We also discuss the relevance of pre-training for the task of predicting prosodic prominence .,Introduction,Introduction,prosody_prediction,0,16,1,0,,0.105224644,0,negative,0.000150453,0.095254261,2.61E-05,7.33E-05,0.002193245,0.000252601,7.21E-05,0.000393126,0.06434366,0.827450805,0.009751768,3.20E-05,6.53E-06
10284,prosody_prediction0,27,We conclude by pointing to future research both in developing better methods for predicting prosodic prominence but also to further improve the quality of the dataset .,Introduction,Introduction,prosody_prediction,0,17,1,0,,0.01291926,0,negative,6.19E-05,0.00510246,6.37E-06,0.000103138,0.000740967,0.000482868,0.000152492,0.000309994,0.004630873,0.979166831,0.009206294,2.76E-05,8.23E-06
10285,prosody_prediction0,28,The dataset and the PyTorch code for the models are available on GitHub : https://github.com/,Introduction,Introduction,prosody_prediction,0,18,1,0,,0.976491081,1,code,1.24E-06,1.04E-05,4.03E-07,0.994183639,0.004466509,0.000516227,5.47E-06,3.52E-06,2.51E-06,0.000805605,1.52E-06,2.79E-08,2.89E-06
10286,prosody_prediction0,29,Helsinki - NLP / prosody .,Introduction,Introduction,prosody_prediction,0,19,1,0,,0.005921629,0,negative,5.40E-05,0.003348091,1.84E-05,0.001803996,0.019275861,0.001054858,0.000964702,0.000370925,0.000739921,0.941386403,0.030823349,7.68E-05,8.27E-05
10287,prosody_prediction0,30,Background,,,prosody_prediction,0,0,1,0,,6.12E-05,0,negative,4.94E-05,0.000177806,6.95E-06,6.36E-05,6.50E-06,0.000188754,4.78E-05,0.001033369,0.000170038,0.995496641,0.002736295,1.27E-05,1.01E-05
10288,prosody_prediction0,81,Experiments,,,prosody_prediction,0,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
10289,prosody_prediction0,82,In this section we describe the experimental setup and the results from our experiments in predicting discrete prosodic prominence labels from text using the corpus described above .,Experiments,Experiments,prosody_prediction,0,1,1,0,,0.002210691,0,negative,0.000376197,0.000634137,0.000701962,6.51E-05,0.000360968,0.001562909,0.003755583,0.006688753,4.10E-05,0.982612831,0.000289507,0.002738197,0.000172842
10290,prosody_prediction0,83,Experimental Setup,,,prosody_prediction,0,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
10291,prosody_prediction0,84,We performed experiments with the following models :,Experimental Setup,Experimental Setup,prosody_prediction,0,1,1,1,experimental-setup,0.047574307,0,negative,0.000111159,2.66E-05,0.080875081,9.44E-07,1.03E-06,0.01726185,0.002394323,0.03578609,1.87E-05,0.862371197,2.59E-05,0.001118402,8.68E-06
10292,prosody_prediction0,85,"BERT - base uncased 3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) Minitagger ( SVM ) ) + GloVe MarMoT ( CRF ) Majority class per word",Experimental Setup,Experimental Setup,prosody_prediction,0,2,1,1,experimental-setup,0.907031297,1,hyperparameters,0.000104361,3.24E-05,0.237550616,1.56E-05,2.69E-06,0.221839545,0.056443655,0.294850457,7.42E-05,0.186204217,0.000228377,0.002283265,0.00037065
10293,prosody_prediction0,86,The models were selected so that they cover a wide variety of different architectures from feature - based statistical approaches to neural networks and pre-trained language models .,Experimental Setup,Experimental Setup,prosody_prediction,0,3,1,0,,0.335433943,0,negative,6.41E-05,5.30E-05,0.001400007,1.25E-05,7.67E-06,0.138648541,0.002763916,0.279696656,3.18E-05,0.577054323,8.31E-06,0.000241804,1.75E-05
10294,prosody_prediction0,87,The models are described in more detail below .,Experimental Setup,Experimental Setup,prosody_prediction,0,4,1,0,,0.02184212,0,negative,5.11E-05,3.04E-05,0.002313179,6.31E-06,2.39E-06,0.016210923,0.000407035,0.042719352,9.71E-05,0.93800698,1.79E-05,0.000124468,1.28E-05
10295,prosody_prediction0,88,"We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library , 3 which we further fine - tune during training .",Experimental Setup,Experimental Setup,prosody_prediction,0,5,1,1,experimental-setup,0.951533192,1,experimental-setup,7.09E-05,6.31E-05,0.002874901,0.000350488,8.45E-06,0.644860099,0.011662988,0.291628368,5.24E-05,0.048247131,1.81E-05,5.76E-05,0.000105533
10296,prosody_prediction0,89,"We take the last hidden layer of BERT and train a single fully - connected classifier layer on top of it , mapping the representation of each word to the labels .",Experimental Setup,Experimental Setup,prosody_prediction,0,6,1,1,experimental-setup,0.538167052,1,negative,0.001744631,0.000496594,0.128326804,3.30E-05,1.23E-05,0.076343762,0.004524491,0.392449247,0.001501106,0.393634002,4.01E-05,0.00059844,0.000295534
10297,prosody_prediction0,90,For our experiments we use the smaller BERT - base model using the uncased alternative .,Experimental Setup,Experimental Setup,prosody_prediction,0,7,1,1,experimental-setup,0.702061134,1,negative,0.000328281,9.18E-05,0.160894416,1.56E-06,2.53E-06,0.042934406,0.008389749,0.125929081,5.05E-05,0.658657168,1.46E-05,0.002680201,2.57E-05
10298,prosody_prediction0,91,We use a batch size of 32 and fine - tune the model for 2 epochs .,Experimental Setup,Experimental Setup,prosody_prediction,0,8,1,1,experimental-setup,0.990887058,1,hyperparameters,1.69E-05,1.90E-05,0.000100646,8.13E-06,5.42E-07,0.159067114,0.002915362,0.830652596,1.19E-05,0.007129626,4.09E-06,3.26E-05,4.15E-05
10299,prosody_prediction0,92,For BiLSTM we use pre-trained 300D Glo Ve 840B word embeddings .,Experimental Setup,Experimental Setup,prosody_prediction,0,9,1,1,experimental-setup,0.994546938,1,hyperparameters,1.24E-05,2.16E-05,0.000277707,4.32E-06,4.77E-07,0.200436115,0.00360275,0.785304733,1.56E-05,0.010259154,2.45E-06,3.08E-05,3.19E-05
10300,prosody_prediction0,93,The initial word embeddings are fine - tuned during training .,Experimental Setup,Experimental Setup,prosody_prediction,0,10,1,0,,0.922941433,1,hyperparameters,3.45E-05,9.94E-05,0.000230858,2.06E-06,8.00E-07,0.034624195,0.00041261,0.891531502,8.87E-05,0.072906484,3.93E-06,4.94E-05,1.56E-05
10301,prosody_prediction0,94,"As with BERT , we add one fullyconnected classifier layer on top of the BiLSTM , mapping the representation of each word to the labels .",Experimental Setup,Experimental Setup,prosody_prediction,0,11,1,1,experimental-setup,0.810067002,1,baselines,0.00905934,0.000810171,0.517392476,4.49E-05,2.53E-05,0.038973567,0.006875386,0.127466831,0.002012771,0.294106545,3.27E-05,0.002753728,0.000446326
10302,prosody_prediction0,95,We use a dropout of 0.2 between the layers of the BiLSTM .,Experimental Setup,Experimental Setup,prosody_prediction,0,12,1,1,experimental-setup,0.994143445,1,hyperparameters,2.50E-05,2.59E-05,0.000108775,9.42E-06,6.85E-07,0.1195704,0.002290746,0.873082596,1.88E-05,0.004790767,2.23E-06,2.84E-05,4.63E-05
10303,prosody_prediction0,96,We use a batch size of 64 and train the model for 5 epochs .,Experimental Setup,Experimental Setup,prosody_prediction,0,13,1,0,,0.991213249,1,hyperparameters,1.23E-05,1.61E-05,7.35E-05,6.99E-06,5.52E-07,0.141751029,0.002874303,0.848532633,1.00E-05,0.006641579,2.73E-06,3.24E-05,4.60E-05
10304,prosody_prediction0,97,"For the SVM we use Minitagger 4 implementation by using each dimension of the pre-trained 300D Glo Ve 840B word embeddings as features , with context - size 1 , i.e. including the previous and the next word in the context .",Experimental Setup,Experimental Setup,prosody_prediction,0,14,1,1,experimental-setup,0.990448187,1,hyperparameters,1.36E-05,2.11E-05,0.000459877,1.07E-05,8.79E-07,0.285372408,0.005259696,0.695242537,1.51E-05,0.013529137,1.98E-06,2.74E-05,4.56E-05
10305,prosody_prediction0,98,For the conditional random field ( CRF ) model we use MarMot 5 by with the default configuration .,Experimental Setup,Experimental Setup,prosody_prediction,0,15,1,1,experimental-setup,0.990760117,1,hyperparameters,1.46E-05,1.18E-05,0.000568503,1.36E-05,7.95E-07,0.328182943,0.006764705,0.651069845,1.04E-05,0.013275876,1.44E-06,4.38E-05,4.16E-05
10306,prosody_prediction0,99,The model applies standard feature templates thatare used for part - ofspeech tagging such as surrounding words as well as suffix and prefix features .,Experimental Setup,Experimental Setup,prosody_prediction,0,16,1,0,,0.503669282,1,negative,8.82E-05,0.000202131,0.039084022,5.86E-06,3.00E-06,0.058551665,0.001873591,0.445714067,0.001363744,0.452883428,1.47E-05,0.000114419,0.000101181
10307,prosody_prediction0,100,We did not optimize the feature model nor any of the other hyperparameters .,Experimental Setup,Experimental Setup,prosody_prediction,0,17,1,0,,0.02331082,0,negative,0.000211071,4.66E-05,0.001020191,8.11E-05,1.60E-05,0.100106891,0.001142175,0.146603423,5.26E-05,0.75045882,3.59E-06,0.000217172,4.03E-05
10308,prosody_prediction0,101,All systems except the Minitagger and CRF are our implementations using PyTorch and are made available on GitHub : https://github.com/Helsinki - NLP / prosody .,Experimental Setup,Experimental Setup,prosody_prediction,0,18,1,1,code,0.958279445,1,experimental-setup,0.000181616,1.96E-05,0.000759852,0.276105382,0.000106333,0.637783718,0.006304025,0.027807947,3.65E-05,0.050273134,8.99E-06,6.59E-05,0.000547004
10309,prosody_prediction0,102,For the experiments we used the larger train - 360 training set .,Experimental Setup,Experimental Setup,prosody_prediction,0,19,1,0,,0.083522533,0,negative,0.00011716,1.22E-05,0.000707183,2.57E-06,7.84E-06,0.021020847,0.001625044,0.067110571,5.98E-06,0.907824179,6.95E-07,0.001552959,1.27E-05
10310,prosody_prediction0,103,We report both 2 - way and 3 - way classification results .,Experimental Setup,Experimental Setup,prosody_prediction,0,20,1,0,,0.290137271,0,negative,0.000957554,1.50E-05,0.00438958,9.35E-07,4.97E-06,0.000854249,0.002778051,0.002924936,2.29E-06,0.727275314,3.68E-06,0.260769522,2.39E-05
10311,prosody_prediction0,104,In the 2 - way classification task we take the three prominence labels and merge labels 1 and 2 into a single prominent class .,Experimental Setup,Experimental Setup,prosody_prediction,0,21,1,0,,0.072740715,0,negative,0.000528103,0.000394996,0.111856578,3.07E-06,1.88E-05,0.008696502,0.002273196,0.037364428,0.000161409,0.834739531,7.21E-06,0.003902668,5.35E-05
10312,prosody_prediction0,105,Results,,,prosody_prediction,0,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
10313,prosody_prediction0,106,All models reach over 80 % in the 2 - way classification task while 3 - way classification accuracy stays below 70 % for all of them .,Results,Results,prosody_prediction,0,1,1,1,results,0.959734002,1,results,0.013932784,1.43E-06,7.56E-05,3.15E-06,1.19E-06,1.51E-05,0.00388556,8.03E-05,6.60E-07,0.018172412,1.48E-05,0.963268923,0.000548136
10314,prosody_prediction0,107,"The BERTbased model gets the highest accuracy of 83.2 % and 68.6 % in the 2 - way and 3 - way classification tasks , respectively , demonstrating the value of a pytorch - transformers 4 https://github.com/karlstratos/",Results,Results,prosody_prediction,0,2,1,1,results,0.975402688,1,results,0.288738005,2.29E-05,0.001892399,1.44E-05,8.95E-06,6.25E-05,0.003528486,0.000250166,2.01E-05,0.069353166,2.42E-05,0.635608429,0.000476217
10315,prosody_prediction0,108,minitagger,Results,Results,prosody_prediction,0,3,1,0,,0.231122062,0,negative,0.01328526,0.000169924,0.006365264,0.000490187,6.91E-05,0.002893795,0.00693966,0.009910711,0.001076666,0.831733791,0.000601391,0.113390863,0.013073385
10316,prosody_prediction0,109,5 http://cistern.cis.lmu.de/marmot/,Results,Results,prosody_prediction,0,4,1,0,,0.223588668,0,code,0.044345601,1.32E-05,0.001632764,0.533387099,0.000628808,0.00462396,0.002820801,0.000552026,0.000128367,0.392190721,6.74E-05,0.008466822,0.011142473
10317,prosody_prediction0,110,pre-trained language model in this task .,Results,Results,prosody_prediction,0,5,1,0,,0.046999031,0,negative,0.005983105,7.66E-05,0.021987338,2.16E-05,1.45E-05,0.000478728,0.002126922,0.003449538,0.000263638,0.883088161,0.000270869,0.080157796,0.002081201
10318,prosody_prediction0,111,The 3layer BiLSTM achieves 82.1 % in the 2 - way classification and 66.4 % in the 3 - way classification task .,Results,Results,prosody_prediction,0,6,1,1,results,0.958142647,1,results,0.009224159,2.33E-06,0.000179043,2.94E-06,1.33E-06,1.99E-05,0.005973998,0.000127921,1.06E-06,0.021711769,1.18E-05,0.961847623,0.000896161
10319,prosody_prediction0,112,"The traditional feature - based classifiers perform slightly below the neural network models , with the CRF obtaining 81.8 % and 66.4 % for the two classification tasks , respectively .",Results,Results,prosody_prediction,0,7,1,1,results,0.961244819,1,results,0.030325562,1.04E-06,6.74E-05,4.87E-06,1.26E-06,1.69E-05,0.004306699,7.81E-05,6.84E-07,0.021062783,6.93E-06,0.943606856,0.000520865
10320,prosody_prediction0,113,The Minitagger SVM model 's test accuracies are slightly lower than the CRF 's with 80.8 % and 65.4 % test accuracies .,Results,Results,prosody_prediction,0,8,1,1,results,0.937289118,1,results,0.038933735,6.64E-07,8.04E-05,5.51E-06,1.91E-06,1.40E-05,0.003773341,4.37E-05,5.30E-07,0.029440348,3.74E-06,0.927237072,0.000465028
10321,prosody_prediction0,114,Finally taking a simple majority class per word gives 80.2 % for the 2 - way classification task and 62.4 % for the 3 - way classification task .,Results,Results,prosody_prediction,0,9,1,1,results,0.944373115,1,results,0.023846937,1.78E-06,9.79E-05,1.54E-06,1.49E-06,1.39E-05,0.003987263,7.58E-05,7.09E-07,0.036471236,4.44E-06,0.935202848,0.00029412
10322,prosody_prediction0,115,The results are listed in .,Results,Results,prosody_prediction,0,10,1,0,,0.010043397,0,negative,0.004491106,2.32E-06,0.000303384,1.36E-06,3.58E-06,2.00E-05,0.000418222,7.28E-05,3.27E-06,0.885274451,4.34E-06,0.109334628,7.05E-05
10323,prosody_prediction0,116,The fairly low results across the board highlight the difficulty of the task of predicting prosodic prominence from text .,Results,Results,prosody_prediction,0,11,1,0,,0.233939012,0,results,0.007100491,2.18E-06,0.000156312,4.89E-06,4.05E-06,3.58E-05,0.001786821,9.35E-05,1.93E-06,0.455996238,3.87E-05,0.534314319,0.000464748
10324,prosody_prediction0,117,"To better understand how much training data is needed in the two classification tasks , we trained selected models with different size subsets of the train - 360 training data .",Results,Results,prosody_prediction,0,12,1,0,,0.004846164,0,negative,0.011191283,2.70E-05,0.00019045,6.27E-06,1.79E-05,0.000254464,0.001042881,0.002248751,2.09E-05,0.958538403,3.33E-06,0.026305794,0.000152565
10325,prosody_prediction0,118,"The selected subsets were : 1 % , 5 % , 10 % , 50 % and 100 % of the training examples ( token - label pairs ) .",Results,Results,prosody_prediction,0,13,1,0,,0.475277921,0,negative,0.010187543,0.00011885,0.000445592,7.06E-05,4.62E-05,0.004686846,0.005399484,0.041290376,0.000134685,0.918266876,2.27E-05,0.017198239,0.002132064
10326,prosody_prediction0,119,"contain the learning curves for the 2 - way and 3 - way classification tasks , for all the models except for the majority and random baselines .",Results,Results,prosody_prediction,0,14,1,0,,0.009025432,0,negative,0.002682622,6.95E-06,0.000190854,6.68E-06,6.48E-06,0.00025091,0.001138463,0.001353869,1.55E-05,0.959810185,8.19E-06,0.03418109,0.000348198
10327,prosody_prediction0,120,For all models and for both of the classification tasks we notice that they achieve quite high test accuracy already with a very small number of training examples .,Results,Results,prosody_prediction,0,15,1,0,,0.86600911,1,results,0.005825705,1.15E-06,2.29E-05,3.10E-06,1.46E-06,2.45E-05,0.004966933,0.000148672,7.37E-07,0.050609919,9.23E-06,0.937600248,0.000785445
10328,prosody_prediction0,121,For most of the models the biggest improvement in performance is achieved when moving from 1 % of the training examples to 5 % .,Results,Results,prosody_prediction,0,16,1,1,results,0.970699642,1,results,0.031234693,1.21E-06,3.80E-05,3.07E-06,1.17E-06,1.36E-05,0.005023268,9.68E-05,7.02E-07,0.023918062,3.03E-06,0.939155996,0.000510417
10329,prosody_prediction0,122,All models have reached close to their full predictive capacity with only 10 % of the training examples .,Results,Results,prosody_prediction,0,17,1,1,results,0.88062783,1,results,0.015597822,3.23E-06,0.000109263,6.42E-06,4.00E-06,4.46E-05,0.007962911,0.000285239,2.26E-06,0.071505374,5.75E-06,0.902532935,0.001940193
10330,prosody_prediction0,123,"For example , BERT achieves 2 - way classification test accuracy of 82.6 % with 10 % of the training data , which is only - 0.6 % points lower than the accuracy with the full training set .",Results,Results,prosody_prediction,0,18,1,0,,0.515949837,1,results,0.007286837,1.05E-06,7.31E-05,7.54E-07,1.50E-06,5.86E-06,0.002275106,3.38E-05,3.87E-07,0.067657755,2.16E-06,0.922415299,0.000246408
10331,prosody_prediction0,124,"In the 3 - way classification task 10 % of the training data gives 67.1 % for BERT , which is - 1.7 % points below the accuracy with the full training set .",Results,Results,prosody_prediction,0,19,1,0,,0.84119823,1,results,0.037570712,1.87E-06,8.37E-05,2.34E-06,3.41E-06,1.06E-05,0.003369868,6.00E-05,8.51E-07,0.052712312,2.07E-06,0.90569212,0.000490117
10332,prosody_prediction0,125,"Interestingly , in the 2 - way classification task the BiLSTM model shows a slightly different learning curve , having already quite a high performance with just 1 % of the training data , but then making no improvement between 1 % and 5 % .",Results,Results,prosody_prediction,0,20,1,0,,0.60355627,1,results,0.020748187,1.12E-06,5.99E-05,1.80E-06,1.23E-06,1.04E-05,0.002605712,6.53E-05,8.13E-07,0.047847504,3.27E-06,0.928326202,0.00032854
10333,prosody_prediction0,126,"However , between 5 % and 100 % the BiLSTM model improvement is almost linear .",Results,Results,prosody_prediction,0,21,1,0,,0.851326772,1,results,0.328074672,3.24E-06,0.000213869,1.31E-05,3.70E-06,2.55E-05,0.003059212,0.000128399,4.83E-06,0.054931832,3.16E-06,0.61277894,0.000759595
10334,prosody_prediction0,127,"As the proposed dataset has been automatically generated as described in Section 3 , we also tested the best two models , BERT and BiLSTM , with a manually annotated test set from The Boston University radio news corpus . :",Results,Results,prosody_prediction,0,22,1,1,results,0.04203639,0,negative,0.008612067,6.90E-06,0.000619001,2.78E-06,1.32E-05,3.41E-05,0.001780047,0.00013754,2.68E-06,0.816149062,1.81E-06,0.172499191,0.000141617
10335,prosody_prediction0,128,Experimental results ( % ) for the 2 and 3 - way classification tasks .,Results,Results,prosody_prediction,0,23,1,0,,0.01745927,0,negative,0.0020081,8.34E-07,0.000196822,1.56E-07,3.47E-07,6.14E-06,0.001654747,3.64E-05,5.35E-07,0.505429205,4.86E-06,0.490560647,0.000101179
10336,prosody_prediction0,129,For this experiment we trained the models using the train - 360 training set ( as above ) replacing only the test set .,Results,Results,prosody_prediction,0,24,1,0,,0.009911505,0,negative,0.003129876,1.76E-05,0.000482683,4.85E-06,2.13E-05,0.000149794,0.000659473,0.001373832,1.86E-05,0.985779952,7.03E-07,0.008154891,0.000206324
10337,prosody_prediction0,130,The results of this experiment are shown in .,Results,Results,prosody_prediction,0,25,1,0,,0.017900781,0,negative,0.004277714,6.62E-07,0.000101354,2.44E-07,9.87E-07,5.26E-06,0.000642739,2.18E-05,5.43E-07,0.570741383,7.95E-07,0.42415051,5.60E-05
10338,prosody_prediction0,131,The good results 6 from this experiment provide further support for the quality of the new dataset .,Results,Results,prosody_prediction,0,26,1,1,results,0.027922792,0,results,0.010087927,7.47E-07,3.26E-05,9.61E-07,2.46E-06,8.46E-06,0.001377926,4.12E-05,5.64E-07,0.362764547,8.92E-07,0.625491104,0.000190567
10339,prosody_prediction0,132,Notice also that the difference between BERT and BiLSTM is much bigger with this test set ( + 3.9 % compared to + 1.1 % ) .,Results,Results,prosody_prediction,0,27,1,1,results,0.499626895,0,results,0.093366781,1.32E-06,6.29E-05,2.42E-06,2.30E-06,7.81E-06,0.003252681,4.74E-05,6.02E-07,0.063163422,8.63E-07,0.839743087,0.00034849
10340,prosody_prediction0,133,"This difference could be due to the genre difference between the two test sets , with the Boston University news corpus being more contemporary compared to the source for our proposed dataset .",Results,Results,prosody_prediction,0,28,1,0,,0.001986712,0,negative,0.002276688,5.51E-06,0.000166483,3.39E-06,6.36E-06,5.30E-05,0.000367342,0.000375555,1.03E-05,0.961561859,2.23E-06,0.034858133,0.000313174
10341,prosody_prediction0,134,This point will be further discussed in Section 6 .,Results,Results,prosody_prediction,0,29,1,0,,0.000377238,0,negative,0.005610025,7.68E-06,0.000140051,1.85E-05,6.77E-06,7.95E-05,0.000153925,0.000473295,4.88E-05,0.986505667,1.42E-06,0.006654203,0.000300207
10342,prosody_prediction0,135,Model vs expert vs acoustic BERT - base,Results,Results,prosody_prediction,0,30,1,0,,0.233485351,0,negative,0.022704382,2.57E-05,0.04558615,2.76E-05,1.83E-05,9.23E-05,0.005372654,0.000238144,0.00010543,0.472268241,3.95E-05,0.447330994,0.006190502
10343,prosody_prediction0,136,82.9 % 82.1 % 3 - layer BiLSTM 79.0 % 79.3 % : Test accuracies ( % ) for the Boston University radio news corpus ( 2 - way classification ) .,Results,Results,prosody_prediction,0,31,1,0,,0.061187125,0,results,0.002329791,9.47E-07,0.000194842,1.34E-06,1.20E-06,3.41E-05,0.007063014,0.000185166,1.31E-06,0.389931403,6.45E-06,0.598411905,0.001838544
10344,prosody_prediction0,137,"expert = expert annotated perceptual prominence labels , acoustic = our acoustic prominence labels",Results,Results,prosody_prediction,0,32,1,0,,0.008019374,0,negative,0.00065017,7.88E-06,0.000283513,1.09E-05,1.02E-05,0.00020562,0.000460138,0.001720912,3.47E-05,0.987686383,1.21E-06,0.007984098,0.00094428
10345,prosody_prediction0,138,Analysis,Results,,prosody_prediction,0,33,1,0,,0.000521509,0,negative,0.002135929,8.52E-06,0.000148466,2.73E-05,8.45E-06,0.000136727,0.000286429,0.000889774,4.98E-05,0.990616525,4.21E-06,0.004904862,0.000782959
10346,prosody_prediction0,139,"The experimental results show that although predicting prosodic prominence is a fairly difficult task , pre-trained contextualized word representations clearly help , as can be seen from the results for BERT .",Results,Analysis,prosody_prediction,0,34,1,0,,0.018440269,0,negative,0.044923689,0.000727875,1.60E-05,1.22E-05,0.000300069,0.000511424,0.001182631,0.001555123,4.37E-05,0.899206823,0.000157126,0.051322196,4.12E-05
10347,prosody_prediction0,140,The difference between BERT and the other models is clear if we compare the other models with BERT fine - tuned with a small fraction of the training data .,Results,Analysis,prosody_prediction,0,35,1,0,,3.78E-05,0,negative,0.000830218,8.55E-05,2.98E-06,1.10E-06,8.41E-05,7.09E-05,3.45E-05,0.000164673,5.35E-06,0.997632998,6.45E-06,0.001080455,8.21E-07
10348,prosody_prediction0,141,"In fact , BERT already outperforms the other models with just 5 % of the training examples in the 2 - way classification case and with 10 % of the training data in the 3 - way classification case .",Results,Analysis,prosody_prediction,0,36,1,0,,0.025586182,0,negative,0.018837481,0.001204175,8.34E-05,3.07E-05,0.001402038,0.000954605,0.004594358,0.002212714,4.14E-05,0.829507396,0.000117307,0.140857175,0.000157247
10349,prosody_prediction0,142,This can be seen as an indication that BERT has acquired implicit semantic or syntactic information during pre-training that is useful in the task of predicting prosodic prominence .,Results,Analysis,prosody_prediction,0,37,1,0,,3.05E-07,0,negative,1.69E-05,6.17E-05,1.17E-06,2.19E-07,2.62E-05,1.87E-05,7.60E-07,4.47E-05,3.86E-05,0.999785491,1.38E-06,4.07E-06,8.23E-08
10350,prosody_prediction0,143,"To gain a better understanding of the types of predictive errors BERT makes , we look at the confusion matrices for the two classification tasks and compare those with the confusion matrices for the BiLSTM .",Results,Analysis,prosody_prediction,0,38,1,0,,7.01E-07,0,negative,1.80E-05,0.000330099,2.28E-06,5.30E-08,3.33E-05,1.23E-05,1.41E-06,5.14E-05,2.95E-05,0.99950301,7.03E-07,1.79E-05,4.08E-08
10351,prosody_prediction0,144,The 3 - way classification confusion matrices are more informative as they allow comparison of the two models with respect to the predicted label in cases of error .,Results,Analysis,prosody_prediction,0,39,1,0,,1.86E-06,0,negative,0.000137719,0.000195608,8.00E-06,6.91E-08,2.72E-05,3.27E-05,2.66E-06,8.17E-05,7.42E-05,0.99938514,7.52E-07,5.41E-05,7.91E-08
10352,prosody_prediction0,145,contains the 3 - way classification confusion matrix for BERT and for the BiLSTM model .,Results,Analysis,prosody_prediction,0,40,1,0,,2.06E-07,0,negative,8.97E-06,8.37E-05,1.07E-06,4.64E-07,3.23E-05,0.000163016,2.67E-06,0.000440511,5.31E-05,0.999211051,5.10E-07,2.38E-06,2.73E-07
10353,prosody_prediction0,146,"In the 3 - way classification task , when the gold label is 0 ( non prominent ) BERT makes more errors with prediction being 2 ( very prominent ) compared to the BiLSTM model .",Results,Analysis,prosody_prediction,0,41,1,0,,0.000588106,0,negative,0.113087869,0.000912781,4.05E-05,5.93E-06,0.000825503,0.000228566,0.000777148,0.000732512,3.00E-05,0.854041833,9.93E-06,0.029284566,2.28E-05
10354,prosody_prediction0,147,"However , when the gold label is 2 ( very prominent )",Results,Analysis,prosody_prediction,0,42,1,0,,9.51E-08,0,negative,0.000164861,6.70E-05,1.20E-05,6.93E-08,3.40E-05,1.57E-05,2.98E-06,2.68E-05,1.51E-05,0.99958906,6.78E-07,7.16E-05,8.69E-08
10355,prosody_prediction0,148,BiLSTM makes more predictions with 0 ( non prominent ) compared to BERT .,Results,Analysis,prosody_prediction,0,43,1,0,,7.66E-05,0,negative,0.026876213,0.000428508,6.59E-05,3.56E-06,0.000741637,0.000215695,0.000519436,0.000419071,2.33E-05,0.956230318,6.39E-06,0.014457252,1.27E-05
10356,prosody_prediction0,149,"In general for 0 labels BERT seems to have higher precision and BiLSTM better recall , whereas for label 2 BERT has clearly higher recall and precision .",Results,Analysis,prosody_prediction,0,44,1,0,,8.83E-05,0,negative,0.023832927,0.00017525,1.78E-05,4.69E-06,0.000486424,0.000133938,0.000330018,0.00023283,8.04E-06,0.954727576,6.80E-06,0.020036742,6.96E-06
10357,prosody_prediction0,150,Both models have low precision and recall for the less distinctive prominence ( label 1 ) .,Results,Analysis,prosody_prediction,0,45,1,0,,9.97E-06,0,negative,0.015223988,0.000106037,1.94E-05,1.72E-06,0.000349939,9.28E-05,0.000147679,0.000137794,5.01E-06,0.975704686,2.67E-06,0.008205384,2.91E-06
10358,prosody_prediction0,151,It seems that the clearest difference between the two models is in their ability to predict high prominence ( label 2 ) .,Results,Analysis,prosody_prediction,0,46,1,0,,2.68E-06,0,negative,0.000565952,2.20E-05,2.83E-06,2.36E-07,5.36E-05,2.16E-05,1.29E-05,3.49E-05,2.40E-06,0.998812596,6.54E-07,0.000470055,2.44E-07
10359,prosody_prediction0,152,We also provide the confusion matrices for the 2 - way classification task for the two models .,Results,Analysis,prosody_prediction,0,47,1,0,,1.08E-06,0,negative,3.72E-06,8.32E-05,3.82E-07,1.54E-07,3.08E-05,3.87E-05,9.66E-07,0.000209701,2.92E-05,0.999600502,9.40E-08,2.51E-06,6.82E-08
10360,prosody_prediction0,153,contains the 2 - way classification confusion matrix for BERT and for the BiLSTM model .,Results,Analysis,prosody_prediction,0,48,1,0,,1.54E-07,0,negative,7.99E-06,6.81E-05,1.04E-06,3.15E-07,3.29E-05,0.000116004,2.45E-06,0.000330815,4.58E-05,0.999392089,1.79E-07,2.06E-06,2.51E-07
10361,prosody_prediction0,154,Here BERT has slightly higher precision and recall across both of the labels .,Results,Analysis,prosody_prediction,0,49,1,0,,8.23E-07,0,negative,0.01877578,0.000317072,8.78E-05,9.16E-06,0.004224571,0.00022759,0.001040123,0.000202657,7.38E-06,0.94020886,3.08E-06,0.034876464,1.95E-05
10362,prosody_prediction0,155,Discussion,Results,,prosody_prediction,0,50,1,0,,0.000504575,0,negative,0.002076708,3.60E-06,0.000101857,8.02E-06,3.71E-06,4.95E-05,0.000322486,0.000486616,1.76E-05,0.988126221,8.22E-07,0.007809535,0.000993372
10363,prosody_prediction0,156,We have shown above that prosodic prominence can reasonably well be predicted from text using different sequence - labelling approaches and models .,Results,Discussion,prosody_prediction,0,51,1,0,,8.32E-06,0,negative,9.10E-05,6.05E-05,1.12E-05,5.07E-08,8.47E-06,2.68E-06,2.78E-06,6.51E-06,3.35E-05,0.99964465,1.22E-05,0.000126215,2.91E-07
10364,prosody_prediction0,157,"However , the reported performance is still quite low , even for state - of - the - art systems based on large pre-trained language models such as BERT .",Results,Discussion,prosody_prediction,0,52,1,0,,4.53E-05,0,negative,9.85E-06,3.08E-05,1.47E-06,1.19E-06,1.30E-05,1.54E-05,4.91E-06,5.72E-05,8.87E-06,0.999766068,7.40E-05,1.52E-05,2.05E-06
10365,prosody_prediction0,158,We list a number of reasons for these shortcomings below and discuss their impact and potential mitigation .,Results,Discussion,prosody_prediction,0,53,1,0,,1.09E-07,0,negative,3.76E-06,3.28E-05,5.03E-07,1.03E-06,3.26E-05,6.93E-06,1.40E-07,1.76E-05,1.86E-05,0.99988551,1.69E-07,3.32E-07,7.05E-08
10366,prosody_prediction0,159,"Although the annotation method has been shown to be quite robust , errors in automatic alignment , signal processing , and quantization introduce noise to the labels .",Results,Discussion,prosody_prediction,0,54,1,0,,7.94E-07,0,negative,6.49E-05,1.76E-05,2.20E-06,3.38E-07,2.42E-05,1.65E-06,4.83E-07,3.40E-06,2.24E-06,0.999866176,5.16E-06,1.15E-05,1.33E-07
10367,prosody_prediction0,160,"This noise might not be detrimental to the training due to dataset size , but the test results are affected .",Results,Discussion,prosody_prediction,0,55,1,0,,3.03E-07,0,negative,1.09E-05,4.67E-06,4.72E-07,1.61E-08,2.54E-06,7.58E-07,8.03E-08,3.01E-06,6.00E-06,0.999970238,8.29E-08,1.26E-06,1.55E-08
10368,prosody_prediction0,161,"To measure the size of this effect , manual correction of apart of the test set could be beneficial .",Results,Discussion,prosody_prediction,0,56,1,0,,4.13E-06,0,negative,0.000146811,2.72E-05,1.29E-06,2.64E-07,1.85E-05,5.65E-06,4.63E-07,2.51E-05,2.30E-05,0.999747432,1.62E-07,3.96E-06,1.55E-07
10369,prosody_prediction0,162,"It is well known that different speakers have different accents , varying reading proficiency , and reading tempo , which all impact the consistency of the labeling as the source speech data contains in total samples from over 1200 different speakers .",Results,Discussion,prosody_prediction,0,57,1,0,,3.02E-07,0,negative,2.82E-06,1.74E-05,6.78E-07,1.76E-07,6.86E-05,2.70E-06,2.34E-07,7.99E-06,2.92E-06,0.999895112,3.07E-07,9.68E-07,9.24E-08
10370,prosody_prediction0,163,REF :,Results,Discussion,prosody_prediction,0,58,1,0,,8.48E-07,0,negative,3.25E-06,6.48E-06,1.07E-06,1.85E-07,6.21E-06,4.15E-06,2.21E-07,1.13E-05,1.61E-05,0.999950232,1.54E-07,5.06E-07,1.26E-07
10371,prosody_prediction0,164,"He is taller than the Indian , not so tall as Gilchrist .",Results,Discussion,prosody_prediction,0,59,1,0,,2.46E-07,0,negative,5.01E-06,1.37E-05,1.43E-06,7.37E-07,4.57E-05,1.50E-05,7.15E-07,2.09E-05,9.44E-06,0.99988534,3.02E-07,1.44E-06,3.51E-07
10372,prosody_prediction0,165,"BERT : He is taller than the Indian , not so tall as Gilchrist .",Results,Discussion,prosody_prediction,0,60,1,0,,9.81E-08,0,negative,4.51E-06,1.07E-05,2.57E-06,6.88E-07,2.62E-05,1.28E-05,6.63E-07,1.48E-05,1.33E-05,0.999911939,3.32E-07,1.11E-06,4.03E-07
10373,prosody_prediction0,166,"Given that inter-speaker agreement on pitch accent placement is somewhere between 80 and 90 % , we can not expect large improvements without speaker - specific modelling .",Results,Discussion,prosody_prediction,0,61,1,0,,1.61E-05,0,negative,0.000905921,1.97E-05,3.93E-06,1.26E-07,1.11E-05,4.40E-06,1.35E-05,1.82E-05,4.41E-06,0.998086592,2.17E-06,0.000929012,9.14E-07
10374,prosody_prediction0,167,The source speech data contains multitude of genres ranging from non-fiction to metric poems with fixed prominence patterns and children 's stories with high proportion of words emphasized .,Results,Discussion,prosody_prediction,0,62,1,0,,2.53E-06,0,negative,6.86E-06,0.00010251,1.07E-05,1.06E-05,0.010954188,4.22E-05,2.93E-06,3.31E-05,6.90E-06,0.988824033,1.96E-07,4.18E-06,1.44E-06
10375,prosody_prediction0,168,The difference in genres could impact the test results .,Results,Discussion,prosody_prediction,0,63,1,0,,7.04E-07,0,negative,1.16E-05,6.39E-06,4.12E-07,6.67E-08,7.23E-06,3.20E-06,3.06E-07,1.03E-05,5.30E-06,0.999952283,5.34E-08,2.79E-06,6.99E-08
10376,prosody_prediction0,169,"Moreover , the books included in the source speech data are all from pre -1923 , whereas BERT and Glo Ve are pre-trained with contemporary texts .",Results,Discussion,prosody_prediction,0,64,1,0,,2.49E-07,0,negative,3.79E-06,3.22E-05,2.92E-06,3.31E-06,0.002688363,2.30E-05,7.80E-07,2.19E-05,3.39E-06,0.997219143,1.93E-08,9.81E-07,2.06E-07
10377,prosody_prediction0,170,We expect that the difference between BERT and other models would be higher with a dataset drawn from a more contemporary source .,Results,Discussion,prosody_prediction,0,65,1,0,,4.76E-07,0,negative,1.05E-05,2.32E-05,1.87E-06,2.72E-08,7.37E-06,2.99E-06,6.85E-07,2.13E-05,2.02E-05,0.999906704,6.47E-08,5.12E-06,1.13E-07
10378,prosody_prediction0,171,"As noted in Section 3 , the difference between BERT and BiLSTM is much bigger with the The Boston University radio news corpus test set ( + 3.9 % compared to + 1.1 % with our test set ) .",Results,Discussion,prosody_prediction,0,66,1,0,,0.000125501,0,negative,0.016205749,0.000268008,5.37E-05,1.75E-06,0.000555654,3.48E-05,0.00041448,0.000143792,1.03E-05,0.968357789,8.49E-07,0.013937233,1.58E-05
10379,prosody_prediction0,172,"This could be due to the genre , with The Boston University radio news corpus being derived from a more contemporary source .",Results,Discussion,prosody_prediction,0,67,1,0,,1.25E-07,0,negative,2.54E-06,1.85E-05,1.16E-06,3.17E-08,5.35E-06,3.57E-06,3.07E-07,1.80E-05,2.41E-05,0.999925382,6.96E-08,8.99E-07,9.82E-08
10380,prosody_prediction0,173,"Overall , our results for BERT highlight the importance of pre-training of the word representations .",Results,Discussion,prosody_prediction,0,68,1,0,,0.002977693,0,negative,0.017059021,0.000569426,1.77E-05,3.99E-06,0.000322341,7.54E-05,0.000314429,0.000377765,4.25E-05,0.974904998,1.52E-06,0.006288191,2.27E-05
10381,prosody_prediction0,174,"As we noticed , already with as little as 10 % of the training data , BERT outperforms the other models when they are trained on the entire training set .",Results,Discussion,prosody_prediction,0,69,1,0,,0.010397583,0,negative,0.012715424,0.000343859,0.000109184,5.54E-06,0.000425584,0.000154443,0.001645195,0.000489776,2.54E-05,0.944596496,1.85E-06,0.039403789,8.34E-05
10382,prosody_prediction0,175,This suggests that BERT has implicitly learned syntactic or semantic information relevant for the prosody prediction task .,Results,Discussion,prosody_prediction,0,70,1,0,,1.93E-07,0,negative,1.21E-05,4.59E-05,5.10E-06,1.81E-08,6.76E-06,1.44E-06,3.89E-07,8.61E-06,9.53E-05,0.999822381,5.17E-08,1.85E-06,1.12E-07
10383,prosody_prediction0,176,Our results are inline with the earlier results by and who showed that pre-trained word embeddings improve model performance in the prominence prediction task .,Results,Discussion,prosody_prediction,0,71,1,0,,0.000193472,0,negative,0.001649397,0.000288363,9.66E-06,5.30E-07,9.77E-05,3.26E-05,0.000131351,0.000159783,4.27E-05,0.995702968,6.36E-07,0.001876553,7.73E-06
10384,prosody_prediction0,177,Table 5 lists five randomly selected examples from the test set and shows the prominence predictions by BERT compared to the reference annotation .,Results,Discussion,prosody_prediction,0,72,1,0,,4.83E-06,0,negative,3.63E-05,2.81E-05,4.00E-06,2.51E-08,0.000118568,1.47E-06,2.76E-06,4.81E-06,1.45E-06,0.99972872,9.02E-09,7.37E-05,8.53E-08
10385,prosody_prediction0,178,"These examples indicate that even if the over all accuracy of the model is not high , the predictions still look plausible in isolation .",Results,Discussion,prosody_prediction,0,73,1,0,,4.27E-07,0,negative,1.10E-05,3.17E-06,2.53E-07,3.43E-08,5.46E-06,2.36E-06,3.45E-07,5.80E-06,2.39E-06,0.999966671,1.60E-08,2.40E-06,5.58E-08
10386,prosody_prediction0,179,"Finally , the classifiers in this paper are trained on single sentences , losing any discourse - level information and relations to surrounding context .",Results,Discussion,prosody_prediction,0,74,1,0,,1.05E-06,0,negative,3.93E-05,0.00043726,2.26E-05,4.96E-07,0.000188449,6.50E-06,9.74E-07,2.02E-05,0.000109805,0.999170772,7.12E-08,3.18E-06,3.48E-07
10387,prosody_prediction0,180,"Increasing the context to contain , e.g. , also previous sentences could improve the results .",Results,Discussion,prosody_prediction,0,75,1,0,,2.28E-05,0,negative,0.000173522,4.95E-06,7.08E-07,2.48E-07,1.13E-05,1.24E-05,2.60E-06,2.54E-05,2.73E-06,0.999738005,5.52E-08,2.74E-05,6.13E-07
10388,prosody_prediction0,181,Conclusion,,,prosody_prediction,0,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
10389,question_answering4,1,title,,,question_answering,4,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
10390,question_answering4,2,Densely Connected Attention Propagation for Reading Comprehension,title,,question_answering,4,1,1,1,research-problem,0.998678202,1,research-problem,3.54E-08,8.14E-06,6.90E-08,8.62E-08,4.91E-08,1.08E-07,1.35E-06,1.66E-06,9.85E-07,0.001600565,0.998386709,1.86E-07,6.48E-08
10391,question_answering4,3,abstract,,,question_answering,4,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
10392,question_answering4,4,"We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) .",abstract,abstract,question_answering,4,1,1,1,research-problem,0.932649304,1,research-problem,1.99E-06,0.001906286,5.01E-05,8.40E-07,5.06E-06,2.11E-06,7.34E-06,1.84E-05,0.00038308,0.012043395,0.985577634,3.01E-06,7.48E-07
10393,question_answering4,5,There are two distinct characteristics of our model .,abstract,abstract,question_answering,4,2,1,0,,0.000273601,0,negative,1.57E-06,0.001080388,7.71E-07,4.34E-06,1.72E-05,7.38E-06,7.64E-07,5.38E-05,0.000264405,0.927966416,0.070602322,5.05E-07,1.09E-07
10394,question_answering4,6,"Firstly , our model densely connects all pairwise layers of the network , modeling relationships between passage and query across all hierarchical levels .",abstract,abstract,question_answering,4,3,1,0,,0.231071017,0,negative,0.000470826,0.434049028,0.000451377,0.000268594,0.003285224,0.000177773,3.46E-05,0.000826094,0.055112732,0.471900777,0.033390033,2.41E-05,8.81E-06
10395,question_answering4,7,"Secondly , the dense connectors in our network are learned via attention instead of standard residual skip - connectors .",abstract,abstract,question_answering,4,4,1,0,,0.130104726,0,negative,0.00026264,0.29694847,0.00022534,0.000123397,0.001987995,0.000133901,2.49E-05,0.000975727,0.016882415,0.657600941,0.024814173,1.59E-05,4.17E-06
10396,question_answering4,8,"To this end , we propose novel Bidirectional Attention Connectors ( BAC ) for efficiently forging connections throughout the network .",abstract,abstract,question_answering,4,5,1,0,,0.492230653,0,approach,6.12E-05,0.475408138,0.000286393,3.15E-05,0.000703897,5.58E-05,2.66E-05,0.000571877,0.104966479,0.22333388,0.194531981,1.61E-05,6.16E-06
10397,question_answering4,9,We conduct extensive experiments on four challenging RC benchmarks .,abstract,abstract,question_answering,4,6,1,1,research-problem,0.006189137,0,negative,1.24E-05,0.157827378,1.75E-05,9.35E-05,0.004386131,0.000108236,5.13E-05,0.001401311,0.000776146,0.789807557,0.04550102,1.40E-05,3.59E-06
10398,question_answering4,10,"Our proposed approach achieves state - of - the - art results on all four , outperforming existing baselines by up to 2.6 % ?",abstract,abstract,question_answering,4,7,1,0,,0.042780056,0,research-problem,5.03E-05,0.002857474,8.18E-06,5.12E-05,0.000152162,4.80E-05,0.000245075,0.000555121,6.74E-05,0.439339039,0.556023396,0.00058777,1.49E-05
10399,question_answering4,11,14.2 % in absolute F1 score .,abstract,abstract,question_answering,4,8,1,0,,0.08980342,0,negative,0.0024319,0.009268714,4.79E-05,0.000329207,0.002176112,8.32E-05,0.000178217,0.000554609,0.000335126,0.913210843,0.0708293,0.000538671,1.63E-05
10400,question_answering4,12,Introduction,,,question_answering,4,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
10401,question_answering4,13,The dominant neural architectures for reading comprehension ( RC ) typically follow a standard ' encode - interact - point ' design .,Introduction,Introduction,question_answering,4,1,1,0,,0.976481197,1,research-problem,5.13E-07,0.000168798,3.74E-07,1.32E-06,2.75E-06,1.97E-06,7.18E-06,3.50E-06,0.0001318,0.015314151,0.984365937,7.70E-07,9.36E-07
10402,question_answering4,14,"Following the embedding layer , a compositional encoder typically encodes Q ( query ) and P ( passage ) individually .",Introduction,Introduction,question_answering,4,2,1,0,,0.908185263,1,model,9.12E-05,0.112450185,0.000188486,4.72E-06,0.000241531,4.59E-05,2.31E-05,9.56E-05,0.67576397,0.186002119,0.025077464,1.27E-05,3.00E-06
10403,question_answering4,15,"Subsequently , an ( bidirectional ) attention layer is then used to model interactions between P/Q .",Introduction,Introduction,question_answering,4,3,1,0,,0.890929196,1,model,7.07E-06,0.010245387,3.81E-05,1.13E-07,1.08E-05,4.61E-06,2.65E-06,6.90E-06,0.985192273,0.00407859,0.000412491,7.34E-07,3.01E-07
10404,question_answering4,16,"Finally , these attended representations are then reasoned over to find ( point to ) the best answer span .",Introduction,Introduction,question_answering,4,4,1,0,,0.784164081,1,model,3.97E-06,0.019132885,2.99E-05,7.40E-08,1.34E-05,4.19E-06,2.46E-06,7.24E-06,0.968498738,0.011422542,0.000883449,9.32E-07,1.93E-07
10405,question_answering4,17,"While , there might be slight variants of this architecture , this over all architectural design remains consistent across many RC models .",Introduction,Introduction,question_answering,4,5,1,0,,0.033964674,0,negative,0.000108899,0.038310264,4.33E-05,1.91E-05,0.000824638,6.79E-05,4.29E-05,9.56E-05,0.038503745,0.863014213,0.058932151,3.45E-05,2.91E-06
10406,question_answering4,18,"Intuitively , the design of RC models often possess some depth , i.e. , every stage of the network easily comprises several layers .",Introduction,Introduction,question_answering,4,6,1,0,,0.337660619,0,negative,1.24E-05,0.008313633,4.33E-06,3.37E-05,0.000100732,0.000100812,2.55E-05,0.000167328,0.009768825,0.542819937,0.438642972,5.45E-06,4.45E-06
10407,question_answering4,19,"For example , the R - NET architecture adopts three BiRNN layers as the encoder and two additional BiRNN layers at the interaction layer .",Introduction,Introduction,question_answering,4,7,1,0,,0.082771345,0,model,2.68E-05,0.039356277,0.000181468,1.12E-05,0.000160303,0.000241027,8.01E-05,0.00028482,0.752796311,0.179142406,0.02770128,8.60E-06,9.43E-06
10408,question_answering4,20,"BiDAF uses two BiLSTM layers at the pointer layer , etc .",Introduction,Introduction,question_answering,4,8,1,0,,0.927640243,1,model,6.47E-05,0.091965507,0.002367428,1.10E-05,0.000439026,0.000329173,0.000227243,0.000332578,0.770351959,0.120654961,0.013214552,2.18E-05,2.01E-05
10409,question_answering4,21,"As such , RC models are often relatively deep , at the very least within the context of NLP .",Introduction,Introduction,question_answering,4,9,1,0,,0.120435893,0,negative,6.51E-06,0.002361375,3.01E-06,1.98E-05,0.00010119,8.30E-05,3.97E-05,8.86E-05,0.001251654,0.540127785,0.455908462,4.86E-06,4.15E-06
10410,question_answering4,22,"Unfortunately , the depth of a model is not without implications .",Introduction,Introduction,question_answering,4,10,1,0,,0.002028828,0,negative,7.66E-06,0.00070508,6.94E-07,1.84E-05,0.00011017,4.52E-05,9.15E-06,3.79E-05,0.000725995,0.967056076,0.031280017,2.89E-06,8.15E-07
10411,question_answering4,23,"It is well - established fact that increasing the depth may impair gradient flow and feature propagation , making networks harder to train .",Introduction,Introduction,question_answering,4,11,1,0,,0.039823676,0,negative,1.89E-05,0.001204184,2.77E-06,4.54E-05,0.000199998,8.19E-05,2.24E-05,5.02E-05,0.000848908,0.915060455,0.082457526,4.86E-06,2.55E-06
10412,question_answering4,24,"This problem is prevalent in computer vision , where mitigation strategies that rely on shortcut connections such as Residual networks , GoogLeNet and DenseNets were incepted .",Introduction,Introduction,question_answering,4,12,1,0,,0.02375605,0,research-problem,2.50E-06,0.000667479,8.73E-07,5.15E-05,6.18E-05,4.69E-05,2.02E-05,3.83E-05,0.000260298,0.391485207,0.607358968,2.27E-06,3.71E-06
10413,question_answering4,25,"Naturally , many of the existing RC models already have some built - in designs to workaround this issue by shortening the signal path in the network .",Introduction,Introduction,question_answering,4,13,1,0,,0.016358135,0,negative,1.24E-05,0.002652736,2.95E-06,7.91E-05,0.000194166,0.000144074,3.15E-05,0.000114698,0.00207786,0.825855778,0.168824384,4.81E-06,5.46E-06
10414,question_answering4,26,"Examples include attention flow , residual connections or simply the usage of highway encoders .",Introduction,Introduction,question_answering,4,14,1,0,,0.001122477,0,negative,2.01E-05,0.004872859,1.47E-05,4.99E-05,0.000290309,0.000447869,0.00010227,0.000301245,0.01182781,0.955419913,0.026632621,1.20E-05,8.41E-06
10415,question_answering4,27,"As such , we hypothesize that explicitly improving information flow can lead to further and considerable improvements in RC models .",Introduction,Introduction,question_answering,4,15,1,0,,0.414725353,0,negative,0.000191463,0.182782591,5.60E-05,2.84E-05,0.001591244,0.000126002,6.93E-05,0.00022872,0.192606478,0.609600639,0.012647892,6.48E-05,6.43E-06
10416,question_answering4,28,"A second observation is that the flow of P/Q representations across the network are often well - aligned and ' synchronous ' , i.e. , P is often only matched with Q at the same hierarchical stage ( e.g. , only after they have passed through a fixed number of encoder layers ) .",Introduction,Introduction,question_answering,4,16,1,0,,0.128448904,0,negative,2.39E-05,0.012475466,7.63E-06,2.99E-05,0.000493455,0.000152549,6.40E-05,0.000161378,0.005827322,0.942026499,0.038716716,1.65E-05,4.72E-06
10417,question_answering4,29,"To this end , we hypothesize that increasing the number of interaction interfaces , i.e. , matching in an asynchronous , cross-hierarchical fashion , can also lead to an improvement in performance .",Introduction,Introduction,question_answering,4,17,1,0,,0.056676582,0,negative,0.000111835,0.184563583,4.99E-05,1.15E-05,0.000862803,0.000118442,6.36E-05,0.000285546,0.307197895,0.498872674,0.007813859,4.34E-05,4.97E-06
10418,question_answering4,30,"Based on the above mentioned intuitions , this paper proposes a new architecture with two distinct characteristics .",Introduction,Introduction,question_answering,4,18,1,0,,0.914438334,1,model,1.88E-05,0.084393004,0.000113659,1.55E-06,0.00020706,1.86E-05,1.89E-05,2.37E-05,0.899707551,0.013656502,0.001830979,7.35E-06,2.31E-06
10419,question_answering4,31,"Firstly , our network is densely connected , connecting every layer of P with every layer of Q .",Introduction,Introduction,question_answering,4,19,1,1,model,0.571817028,1,model,0.00026143,0.290817398,0.000651735,3.95E-05,0.007371615,0.000204309,0.000130901,0.000168911,0.634814051,0.065072001,0.000431492,2.67E-05,9.89E-06
10420,question_answering4,32,This not only facilitates information flow but also increases the number of interaction interfaces between P/Q .,Introduction,Introduction,question_answering,4,20,1,0,,0.768659628,1,negative,0.000740603,0.168326048,0.000138423,8.25E-06,0.001431653,6.83E-05,0.000152147,0.000114792,0.278734901,0.54368205,0.006406424,0.000190251,6.17E-06
10421,question_answering4,33,"Secondly , our network is densely connected by attention , making it vastly different from any residual mitigation strategy in the literature .",Introduction,Introduction,question_answering,4,21,1,0,,0.620412481,1,approach,0.001696095,0.420930642,0.00056016,0.001271672,0.081875839,0.001080927,0.000638461,0.000545591,0.095473047,0.39355316,0.002081724,0.000240074,5.26E-05
10422,question_answering4,34,"To the best of our knowledge , this is the first work that explicitly considers attention as a form of skip - connector .",Introduction,Introduction,question_answering,4,22,1,0,,0.197401713,0,approach,8.51E-05,0.390035234,0.000129545,8.51E-05,0.004114022,0.000399464,0.000213659,0.000617154,0.201409674,0.389847622,0.012996578,4.73E-05,1.96E-05
10423,question_answering4,35,"Notably , models such as BiDAF incorporates a form of attention propagation ( flow ) .",Introduction,Introduction,question_answering,4,23,1,0,,0.024327349,0,negative,9.30E-05,0.016468227,7.30E-05,4.20E-05,0.000777627,0.000349654,0.000171671,0.000239827,0.0164077,0.93823718,0.027096949,3.08E-05,1.24E-05
10424,question_answering4,36,"However , this is inherently unsuitable for forging dense connections throughout the network since this would incur a massive increase in the representation size in subsequent layers .",Introduction,Introduction,question_answering,4,24,1,0,,0.137410234,0,negative,6.61E-05,0.041582144,2.11E-05,2.61E-05,0.000530614,0.000114598,6.04E-05,0.000222507,0.022483014,0.915427507,0.019426653,3.33E-05,6.07E-06
10425,question_answering4,37,"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect two sequences at arbitrary layers .",Introduction,Introduction,question_answering,4,25,1,1,model,0.962676025,1,model,1.90E-05,0.211982627,0.000112866,1.76E-06,0.000399976,2.03E-05,3.53E-05,3.48E-05,0.777964911,0.008861715,0.000556418,7.45E-06,2.89E-06
10426,question_answering4,38,"The key idea is to compress the attention outputs so that they can be small enough to propagate , yet enabling a connection between two sequences .",Introduction,Introduction,question_answering,4,26,1,1,model,0.886092272,1,model,0.000116718,0.396924204,0.000139419,2.51E-05,0.002520068,9.24E-05,4.56E-05,0.00014946,0.545692484,0.054008849,0.000267975,1.27E-05,5.06E-06
10427,question_answering4,39,"The propagated features are collectively passed into prediction layers , which effectively connect shallow layers to deeper layers .",Introduction,Introduction,question_answering,4,27,1,1,model,0.936430558,1,model,4.28E-06,0.030872071,1.57E-05,1.35E-07,3.93E-05,6.03E-06,4.36E-06,1.33E-05,0.96349494,0.005515291,3.38E-05,5.20E-07,3.11E-07
10428,question_answering4,40,"Therefore , this enables multiple bidirectional attention calls to be executed without much concern , allowing us to efficiently connect multiple layers together .",Introduction,Introduction,question_answering,4,28,1,0,,0.888782752,1,model,3.58E-05,0.193433103,5.15E-05,9.49E-07,0.000323202,1.65E-05,2.19E-05,3.48E-05,0.756236115,0.049595339,0.000240313,9.26E-06,1.20E-06
10429,question_answering4,41,"Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .",Introduction,Introduction,question_answering,4,29,1,1,model,0.935155341,1,model,0.000107556,0.192008474,0.000815936,1.51E-05,0.00133705,0.000164428,0.00076608,0.000157584,0.737362267,0.057490667,0.009599132,0.000114771,6.09E-05
10430,question_answering4,42,DECAPROP achieves a significant gain of 2.6 % ?,Introduction,Introduction,question_answering,4,30,1,0,,0.171702425,0,negative,0.00284003,0.037528522,0.000869724,2.25E-05,0.00239739,0.000346276,0.027042173,0.000336791,0.031585052,0.872795289,0.012176009,0.011779985,0.000280232
10431,question_answering4,43,"14.2 % absolute improvement in F1 score over the existing state - of - the - art on four challenging RC datasets , namely News QA , Quasar - T , Search QA and Narrative QA .",Introduction,Introduction,question_answering,4,31,1,0,,0.110990122,0,negative,0.001529891,0.081470003,0.000148076,4.62E-05,0.00311431,0.000435437,0.046514585,0.001353545,0.007828933,0.824187042,0.011347008,0.021535823,0.000489154
10432,question_answering4,44,Bidirectional Attention Connectors ( BAC ),Introduction,Introduction,question_answering,4,32,1,0,,0.804949437,1,model,7.14E-06,0.021511519,0.000166276,3.23E-07,2.63E-05,2.05E-05,6.98E-05,2.49E-05,0.933673582,0.040033387,0.004453382,9.89E-06,3.11E-06
10433,question_answering4,45,This section introduces the Bidirectional Attention Connectors ( BAC ) module which is central to our over all architecture .,Introduction,Introduction,question_answering,4,33,1,0,,0.932885751,1,model,2.20E-05,0.097863002,0.000157891,1.13E-06,0.000378547,2.25E-05,4.44E-05,2.32E-05,0.888889992,0.01238019,0.000206721,6.98E-06,3.45E-06
10434,question_answering4,46,The BAC module can bethought of as a connector component that connects two sequences / layers .,Introduction,Introduction,question_answering,4,34,1,0,,0.226290468,0,model,1.41E-05,0.023577974,6.43E-05,1.65E-06,0.000175831,4.43E-05,2.44E-05,4.37E-05,0.927716604,0.048192157,0.000139418,3.03E-06,2.53E-06
10435,question_answering4,47,"The key goals of this module are to ( 1 ) connect any two layers of P/Q in the network , returning a residual feature that can be propagated 1 to deeper layers , ( 2 ) model cross-hierarchical interactions between P/Q and ( 3 ) minimize any costs incurred to other network components such that this component maybe executed multiple times across all layers .",Introduction,Introduction,question_answering,4,35,1,0,,0.768280635,1,model,7.44E-06,0.070859326,5.05E-05,4.11E-07,0.000141801,1.02E-05,9.83E-06,1.28E-05,0.911302869,0.017493209,0.000108327,2.44E-06,8.42E-07
10436,question_answering4,48,Let P ?,Introduction,Introduction,question_answering,4,36,1,0,,0.001436882,0,negative,1.70E-05,0.005248339,5.90E-06,3.96E-06,0.000361962,0.000170728,0.000102058,0.000204525,0.009076789,0.98459712,0.000192862,1.63E-05,2.40E-06
10437,question_answering4,49,R p d and Q ?,Introduction,Introduction,question_answering,4,37,1,0,,0.007593633,0,negative,4.19E-05,0.00528686,1.61E-05,1.50E-05,0.001288418,0.000268519,0.000222539,0.000226183,0.002734698,0.989566568,0.000293379,3.47E-05,5.16E-06
10438,question_answering4,50,R q d be inputs to the BAC module .,Introduction,Introduction,question_answering,4,38,1,0,,0.004710414,0,negative,1.46E-05,0.007002356,8.56E-06,4.11E-06,0.000373588,0.000122522,6.89E-05,0.000213002,0.01313232,0.978881446,0.000164635,1.18E-05,2.11E-06
10439,question_answering4,51,The initial steps in this module remains identical to standard bi-attention in which an affinity matrix is constructed between P/Q .,Introduction,Introduction,question_answering,4,39,1,0,,0.150809542,0,model,1.27E-05,0.080613244,9.08E-05,1.21E-06,0.000285363,4.09E-05,3.27E-05,6.81E-05,0.863051129,0.055726009,7.23E-05,3.12E-06,2.38E-06
10440,question_answering4,52,"In our bi-attention module , the affinity matrix is computed via :",Introduction,Introduction,question_answering,4,40,1,0,,0.246109706,0,negative,5.69E-05,0.080174972,7.88E-05,1.09E-05,0.001398947,0.000421818,0.000155828,0.000769884,0.28580323,0.630957819,0.000146848,1.59E-05,8.12E-06
10441,question_answering4,53,where F ( . ) is a standard dense layer with ReLU activations and d is the dimensionality of the vectors .,Introduction,Introduction,question_answering,4,41,1,0,,0.009833971,0,negative,1.94E-05,0.027032313,1.29E-05,1.46E-05,0.000700026,0.000772951,0.000237806,0.002232798,0.037471411,0.931257791,0.000226072,9.95E-06,1.20E-05
10442,question_answering4,54,Note that this is the scaled dot -product attention from .,Introduction,Introduction,question_answering,4,42,1,0,,0.028074967,0,negative,4.56E-05,0.019808651,0.000110102,4.95E-06,0.000774856,0.000225431,0.000161212,0.00020385,0.11114605,0.867411326,8.84E-05,1.48E-05,4.84E-06
10443,question_answering4,55,"Next , we learn an alignment between P/Q as follows :",Introduction,Introduction,question_answering,4,43,1,0,,0.210979684,0,model,2.56E-05,0.077218931,0.000177933,5.60E-07,0.000374469,5.66E-05,7.01E-05,8.97E-05,0.608138871,0.313743925,9.12E-05,9.84E-06,2.24E-06
10444,question_answering4,56,"where A , B are the aligned representations of the query / passsage respectively .",Introduction,Introduction,question_answering,4,44,1,0,,0.00465512,0,negative,5.86E-06,0.00956778,8.84E-06,1.01E-06,0.000197982,9.22E-05,4.28E-05,0.000178887,0.033249956,0.95657442,7.42E-05,4.65E-06,1.43E-06
10445,question_answering4,57,"In many standard neural QA models , it is common to pass an augmented 2 matching vector of this attentional representation to subsequent layers .",Introduction,Introduction,question_answering,4,45,1,0,,0.169003678,0,negative,3.18E-05,0.094962899,0.000115085,5.05E-05,0.001347969,0.000707911,0.000349906,0.001260472,0.203935845,0.694686579,0.002484796,2.46E-05,4.16E-05
10446,question_answering4,58,"For this purpose , functions such as",Introduction,Introduction,question_answering,4,46,1,0,,0.006561913,0,negative,2.43E-05,0.022169482,3.55E-05,2.40E-06,0.00063949,0.000106477,0.000232689,0.000176994,0.035779926,0.940175121,0.000617085,3.57E-05,4.91E-06
10447,question_answering4,59,been used .,Introduction,Introduction,question_answering,4,47,1,0,,0.071348586,0,negative,1.89E-05,0.001105884,6.25E-06,2.23E-05,0.001259978,0.000283096,0.000116017,0.000156831,0.000561149,0.996430988,2.63E-05,8.75E-06,3.56E-06
10448,question_answering4,60,"However , simple / naive augmentation would not suffice in our use case .",Introduction,Introduction,question_answering,4,48,1,0,,0.001120561,0,negative,8.15E-05,0.001351926,4.53E-06,2.59E-05,0.001074899,0.000513884,0.000490506,0.000280917,0.000424591,0.99564553,4.87E-05,4.86E-05,8.48E-06
10449,question_answering4,61,"Even without augmentation , every call of bi-attention returns a new d dimensional vector for each element in the sequence .",Introduction,Introduction,question_answering,4,49,1,0,,0.141449497,0,model,9.04E-05,0.141731997,0.000113505,8.24E-07,0.000468804,4.57E-05,0.000146796,9.44E-05,0.51682579,0.340363494,9.02E-05,2.50E-05,3.21E-06
10450,question_answering4,62,"If the network has l layers , then connecting 3 all pairwise layers would require l 2 connectors and therefore an output dimension of l 2 d .",Introduction,Introduction,question_answering,4,50,1,0,,0.079177082,0,negative,0.00013761,0.041522381,3.89E-05,3.05E-05,0.003097141,0.002875067,0.00302367,0.00354099,0.041045578,0.904366384,0.0001597,7.08E-05,9.13E-05
10451,question_answering4,63,This is not only computationally undesirable but also require a large network at the end to reduce this vector .,Introduction,Introduction,question_answering,4,51,1,0,,0.00105107,0,negative,7.37E-05,0.003212104,7.46E-06,4.62E-05,0.001197399,0.000348981,0.000203373,0.000251736,0.001435404,0.993119713,7.10E-05,2.28E-05,1.01E-05
10452,question_answering4,64,"With augmentation , this problem is aggravated .",Introduction,Introduction,question_answering,4,52,1,0,,0.009568894,0,negative,9.39E-05,0.005262662,2.72E-05,0.000196227,0.006669242,0.000489827,0.000290177,0.000230253,0.002933169,0.983656123,0.000109876,2.58E-05,1.55E-05
10453,question_answering4,65,"Hence , standard birectional attention is not suitable here .",Introduction,Introduction,question_answering,4,53,1,0,,0.000707579,0,negative,1.27E-05,0.00163715,4.11E-06,1.90E-05,0.000503972,0.000186104,8.85E-05,0.00015871,0.000812605,0.99647721,8.97E-05,7.14E-06,3.10E-06
10454,question_answering4,66,"To overcome this limitation , we utilize a parameterized function G (. ) to compress the bi-attention vectors down to scalar .",Introduction,Introduction,question_answering,4,54,1,0,,0.890372799,1,model,0.000122492,0.356800339,0.00013425,9.42E-06,0.002032321,0.000220657,0.000287079,0.000732773,0.5575524,0.082066226,1.49E-05,1.29E-05,1.42E-05
10455,question_answering4,67,where g pi ?,Introduction,Introduction,question_answering,4,55,1,0,,0.000948634,0,negative,6.77E-06,0.001562004,3.01E-06,2.34E-06,0.00021056,0.000135162,9.44E-05,0.000154157,0.001578738,0.996227505,1.52E-05,8.45E-06,1.70E-06
10456,question_answering4,68,R 3 is the output ( for each element in P ) of the BAC module .,Introduction,Introduction,question_answering,4,56,1,0,,0.004484346,0,negative,1.90E-05,0.012687601,4.42E-05,5.49E-07,0.000330619,7.91E-05,0.000123978,0.000101366,0.059502039,0.927087602,1.12E-05,1.08E-05,1.95E-06
10457,question_answering4,69,This is done in an identical fashion for a i and q i to form g q i for each element in Q .,Introduction,Introduction,question_answering,4,57,1,0,,0.00660366,0,negative,1.81E-05,0.011925518,1.97E-05,4.89E-06,0.000675261,0.000321674,0.000195878,0.000321564,0.041582541,0.94491246,9.05E-06,8.03E-06,5.35E-06
10458,question_answering4,70,"Intuitively g * i where * = {p , q} are the learned scalar attention that is propagated to upper layers .",Introduction,Introduction,question_answering,4,58,1,0,,0.002166238,0,negative,8.74E-06,0.005758264,1.02E-05,2.50E-06,0.000364588,0.000158944,8.04E-05,0.000272187,0.016842358,0.976486204,8.07E-06,4.67E-06,2.88E-06
10459,question_answering4,71,"Since there are only three scalars , they will not cause any problems even when executed for multiple times .",Introduction,Introduction,question_answering,4,59,1,0,,0.012646455,0,negative,0.000349265,0.005472273,2.08E-05,7.74E-06,0.002681925,0.000286954,0.000922339,0.000168337,0.001621385,0.988355644,5.98E-06,0.000100385,6.96E-06
10460,question_answering4,72,"As such , the connection remains relatively lightweight .",Introduction,Introduction,question_answering,4,60,1,0,,0.070122591,0,negative,0.000229547,0.112033165,0.000195719,3.53E-05,0.008605121,0.000423583,0.000649445,0.000409052,0.117161772,0.760144512,2.25E-05,6.81E-05,2.21E-05
10461,question_answering4,73,"This compression layer can be considered as a defining trait of the BAC , differentiating it from standard bi-attention .",Introduction,Introduction,question_answering,4,61,1,0,,0.099497522,0,negative,0.000328919,0.127383103,0.000387369,4.17E-05,0.010546322,0.000349867,0.000467916,0.000319951,0.398155556,0.461933279,1.48E-05,4.99E-05,2.14E-05
10462,question_answering4,74,"Naturally , there are many potential candidates for the function G ( . ) .",Introduction,Introduction,question_answering,4,62,1,0,,0.001829766,0,negative,1.94E-06,0.001497059,1.36E-06,1.59E-06,6.53E-05,0.000114576,6.47E-05,0.000226927,0.002076743,0.995918067,2.63E-05,3.75E-06,1.68E-06
10463,question_answering4,75,One natural choice is the standard dense layer ( or multiple dense layers ) .,Introduction,Introduction,question_answering,4,63,1,0,,0.01002527,0,negative,3.03E-05,0.023685663,0.000250219,9.08E-05,0.001659223,0.003276653,0.002361476,0.002746454,0.025548583,0.940106592,0.000103211,3.21E-05,0.00010873
10464,question_answering4,76,"However , dense layers are limited as they do not compute dyadic pairwise interactions between features which inhibit its expressiveness .",Introduction,Introduction,question_answering,4,64,1,0,,0.013215087,0,negative,2.82E-05,0.004229252,2.63E-05,3.78E-05,0.00084409,0.000479783,0.000766801,0.000340897,0.001042605,0.991982664,0.000158764,3.86E-05,2.43E-05
10465,question_answering4,77,"On the other hand , factorization - based models are known to not only be expressive and efficient , but also able to model low - rank structure well .",Introduction,Introduction,question_answering,4,65,1,0,,0.006217019,0,negative,1.02E-05,0.004078698,8.62E-06,4.81E-05,0.000401933,0.000721179,0.000876833,0.000971238,0.000761887,0.991615783,0.000442107,2.37E-05,3.97E-05
10466,question_answering4,78,"To this end , we adopt factorization machines ( FM ) as G ( . ) .",Introduction,Introduction,question_answering,4,66,1,0,,0.749380377,1,model,2.64E-05,0.257475953,0.000665152,1.29E-06,0.001105035,0.000180711,0.000643286,0.000251351,0.504467528,0.235135636,1.62E-05,1.58E-05,1.56E-05
10467,question_answering4,79,The FM layer is defined as :,Introduction,Introduction,question_answering,4,67,1,0,,0.26973091,0,model,2.98E-05,0.028299649,0.000198297,1.74E-06,0.000419583,0.000186948,0.000218132,0.000262245,0.541737769,0.428618924,8.01E-06,6.98E-06,1.19E-05
10468,question_answering4,80,is a scalar .,Introduction,Introduction,question_answering,4,68,1,0,,0.343342452,0,negative,2.19E-05,0.021968326,2.05E-05,2.49E-05,0.001170273,0.002288606,0.001122024,0.009528739,0.03716951,0.926613202,9.24E-06,1.09E-05,5.19E-05
10469,question_answering4,81,"Intuitively , this layer tries to learn pairwise interactions between every x i and x j using factorized ( vector ) parameters v.",Introduction,Introduction,question_answering,4,69,1,0,,0.051118945,0,model,2.13E-05,0.044879127,0.000165033,3.41E-06,0.000955145,0.000257991,0.000241628,0.000422732,0.645410247,0.307620655,2.82E-06,3.63E-06,1.62E-05
10470,question_answering4,82,"In the context of our BAC module , the FM layer is trying to learn a low - rank structure from the ' match ' vector ( e.g.",Introduction,Introduction,question_answering,4,70,1,0,,0.03954342,0,negative,0.000150825,0.120727708,0.000596698,1.89E-06,0.00215367,0.000184433,0.000697443,0.000219309,0.403854751,0.471361984,4.21E-06,3.12E-05,1.59E-05
10471,question_answering4,83,.,Introduction,Introduction,question_answering,4,71,1,0,,0.000236294,0,negative,1.29E-06,0.000434401,7.09E-07,4.73E-07,5.69E-05,8.33E-05,4.57E-05,0.000121234,0.001621742,0.997631178,9.28E-07,1.21E-06,8.48E-07
10472,question_answering4,84,"Finally , we note that the BAC module takes inspiration from the main body of our CAFE model for entailment classification .",Introduction,Introduction,question_answering,4,72,1,0,,0.097771655,0,negative,0.000170471,0.033948093,0.000332664,1.58E-05,0.006920844,0.000377648,0.000711961,0.000168899,0.072823768,0.884468213,3.52E-06,3.66E-05,2.15E-05
10473,question_answering4,85,"However , this work demonstrates the usage and potential of the BAC as a residual connector .",Introduction,Introduction,question_answering,4,73,1,0,,0.170275133,0,negative,0.000172424,0.027306401,0.000166076,2.04E-05,0.011415474,0.000359656,0.000865053,0.000162153,0.00776035,0.951694235,5.12E-06,5.93E-05,1.33E-05
10474,question_answering4,86,Densely Connected Attention Propagation ( DECAPROP ),Introduction,Introduction,question_answering,4,74,1,0,,0.363171542,0,negative,0.000189548,0.081004274,0.001833486,9.72E-06,0.00077289,0.001661239,0.166136907,0.001399133,0.106200615,0.634605581,0.00202561,0.00214249,0.002018504
10475,question_answering4,87,"In this section , we describe our proposed model in detail .",Introduction,Introduction,question_answering,4,75,1,0,,0.006996767,0,negative,9.04E-06,0.032217273,1.86E-05,1.97E-05,0.003772544,0.000855535,0.000486544,0.001802718,0.032794455,0.927985577,2.66E-06,6.34E-06,2.90E-05
10476,question_answering4,88,depicts a high - level overview of our proposed architecture .,Introduction,Introduction,question_answering,4,76,1,0,,0.008413495,0,negative,2.86E-06,0.003741535,9.29E-06,4.40E-06,0.000670401,0.000271527,0.000175577,0.000308195,0.009066912,0.985736488,2.05E-06,2.77E-06,7.99E-06
10477,question_answering4,89,Start Pointer,Introduction,,question_answering,4,77,1,0,,0.014564894,0,negative,1.22E-05,0.004382826,5.19E-05,1.50E-05,0.000530383,0.003641866,0.003622347,0.003132753,0.026347981,0.958150214,7.60E-06,2.03E-05,8.46E-05
10478,question_answering4,90,End Pointer,Introduction,,question_answering,4,78,1,0,,0.001339838,0,negative,4.17E-06,0.002455822,2.14E-05,3.35E-06,0.000189009,0.001659765,0.001136387,0.00127693,0.01682244,0.976401902,2.97E-06,5.09E-06,2.08E-05
10479,question_answering4,91,Contextualized Input Encoder,Introduction,,question_answering,4,79,1,0,,0.036976213,0,model,2.69E-05,0.026991322,0.000483284,1.44E-06,0.000192256,0.000314305,0.003587286,0.000336546,0.580111912,0.387709353,6.17E-05,6.64E-05,0.000117223
10480,question_answering4,92,The inputs to our model are two sequences P and Q which represent passage and query respectively .,Introduction,Contextualized Input Encoder,question_answering,4,80,1,0,,1.57E-05,0,negative,3.98E-07,2.17E-06,1.29E-06,5.29E-09,1.90E-07,8.22E-07,9.89E-07,1.59E-05,3.54E-05,0.999937033,4.01E-09,5.30E-06,4.60E-07
10481,question_answering4,93,"Given Q , the task of the RC model is to select a sequence of tokens in P as the answer .",Introduction,Contextualized Input Encoder,question_answering,4,81,1,0,,7.00E-06,0,negative,6.37E-07,2.38E-06,1.49E-06,6.34E-09,1.47E-07,3.66E-07,4.79E-07,4.87E-06,2.29E-05,0.999958543,1.70E-08,7.85E-06,3.26E-07
10482,question_answering4,94,"Following many RC models , we enhance the input representations with ( 1 ) character embeddings ( passed into a BiRNN encoder ) , ( 2 ) a binary match feature which denotes if a word in the query appears in the passage ( and vice versa ) and ( 3 ) a normalized frequency score denoting how many times a word appears in the passage .",Introduction,Contextualized Input Encoder,question_answering,4,82,1,0,,0.00054507,0,negative,3.09E-05,3.89E-05,7.14E-05,1.15E-08,1.19E-06,1.60E-06,5.69E-06,1.10E-05,0.00070961,0.999100572,4.02E-09,2.80E-05,1.06E-06
10483,question_answering4,95,"The Char BiRNN of h c dimensions , along with two other binary features , is concatenated with the word embeddings w i ?",Introduction,Contextualized Input Encoder,question_answering,4,83,1,0,,3.23E-06,0,negative,2.63E-06,9.72E-07,2.92E-06,3.90E-09,2.61E-07,1.93E-06,2.41E-06,1.61E-05,1.11E-05,0.999956101,2.31E-10,5.30E-06,2.97E-07
10484,question_answering4,96,"R dw , to form the final representation of d w + h c + 2 dimensions .",Introduction,Contextualized Input Encoder,question_answering,4,84,1,0,,4.14E-06,0,negative,1.12E-05,9.23E-07,5.45E-06,4.65E-09,4.38E-07,7.37E-07,2.41E-06,2.58E-06,5.49E-06,0.999914719,5.82E-10,5.58E-05,2.40E-07
10485,question_answering4,97,Densely Connected Attention Encoder ( DECAENC ),Introduction,Contextualized Input Encoder,question_answering,4,85,1,0,,0.003852114,0,negative,7.85E-06,1.24E-05,0.000235184,2.35E-08,3.41E-07,2.62E-06,4.41E-05,1.16E-05,0.000931954,0.998491709,6.31E-07,0.000241095,2.04E-05
10486,question_answering4,98,The DECAENC accepts the inputs P and Q from the input encoder .,Introduction,Contextualized Input Encoder,question_answering,4,86,1,0,,2.39E-05,0,negative,3.50E-06,5.09E-06,5.60E-05,6.73E-09,4.34E-07,7.51E-07,2.03E-06,4.89E-06,0.000393893,0.999525278,3.42E-09,6.79E-06,1.29E-06
10487,question_answering4,99,DECAENC is a multi - layered encoder with k layers .,Introduction,Contextualized Input Encoder,question_answering,4,87,1,0,,0.031012248,0,negative,2.12E-05,7.73E-05,0.005071038,1.24E-07,6.38E-06,1.68E-05,0.000116794,5.19E-05,0.001493362,0.992959123,7.12E-08,0.000103864,8.21E-05
10488,question_answering4,100,"For each layer , we pass P/Q into a bidirectional RNN layer of h dimensions .",Introduction,Contextualized Input Encoder,question_answering,4,88,1,0,,2.28E-05,0,negative,2.35E-05,1.14E-05,1.73E-05,2.78E-08,1.35E-06,5.14E-06,1.13E-05,5.83E-05,0.000176308,0.999677161,9.55E-10,1.49E-05,3.33E-06
10489,question_answering4,101,"Next , we apply our attention connector ( BAC ) to HP /H Q ?",Introduction,Contextualized Input Encoder,question_answering,4,89,1,0,,0.000688407,0,negative,1.31E-05,1.91E-06,3.30E-05,8.65E-10,2.35E-07,1.80E-07,3.26E-06,6.79E-07,2.39E-05,0.999829949,7.92E-10,9.35E-05,2.49E-07
10490,question_answering4,102,R where H represents the hidden state outputs from the BiRNN encoder where the RNN cell can either be a GRU or LSTM encoder .,Introduction,Contextualized Input Encoder,question_answering,4,90,1,0,,1.25E-06,0,negative,1.47E-06,1.94E-07,1.88E-06,1.53E-09,7.88E-08,1.78E-07,3.35E-07,1.22E-06,2.29E-06,0.99998558,1.60E-10,6.67E-06,9.14E-08
10491,question_answering4,103,"Let d be the input dimensions of P and Q , then this encoder goes through a process of d ? h ?",Introduction,Contextualized Input Encoder,question_answering,4,91,1,0,,2.60E-06,0,negative,7.73E-06,5.08E-07,3.25E-06,6.28E-10,8.91E-08,2.04E-07,1.43E-06,1.08E-06,1.30E-05,0.99995434,2.98E-10,1.82E-05,1.31E-07
10492,question_answering4,104,h+3 ?,Introduction,Contextualized Input Encoder,question_answering,4,92,1,0,,6.75E-06,0,negative,2.74E-06,1.38E-07,8.79E-07,1.60E-08,2.29E-07,4.02E-06,7.00E-06,1.54E-05,1.26E-06,0.999939539,7.92E-10,2.69E-05,1.88E-06
10493,question_answering4,105,h in which the BiRNN at layer l + 1 consumes the propagated features from layer l .,Introduction,Contextualized Input Encoder,question_answering,4,93,1,0,,1.48E-06,0,negative,1.85E-06,1.83E-07,3.35E-06,5.06E-10,5.22E-08,6.88E-08,2.51E-07,4.28E-07,3.63E-06,0.999981039,8.73E-11,9.09E-06,5.26E-08
10494,question_answering4,106,"Intuitively , this layer models P/Q whenever they are at the same network hierarchical level .",Introduction,Contextualized Input Encoder,question_answering,4,94,1,0,,6.79E-05,0,negative,4.72E-06,3.54E-06,2.34E-05,7.17E-09,3.49E-07,5.72E-07,1.28E-06,3.97E-06,0.000391297,0.999565468,6.88E-10,4.36E-06,1.03E-06
10495,question_answering4,107,"At this point , we include ' asynchronous ' ( cross hierarchy ) connections between P and Q .",Introduction,Contextualized Input Encoder,question_answering,4,95,1,0,,2.23E-06,0,negative,7.04E-06,8.73E-07,6.84E-06,8.01E-10,1.19E-07,1.92E-07,7.30E-07,1.23E-06,4.69E-05,0.999930307,6.46E-11,5.64E-06,9.59E-08
10496,question_answering4,108,"Let P i , Q i denote the representations of P , Q at layer i .",Introduction,Contextualized Input Encoder,question_answering,4,96,1,0,,2.41E-07,0,negative,4.24E-08,5.30E-08,5.11E-08,1.72E-10,8.47E-09,7.93E-08,6.53E-08,1.51E-06,1.09E-06,0.999996666,1.45E-11,4.22E-07,1.20E-08
10497,question_answering4,109,We apply the Bidirectional Attention Connectors ( BAC ) as follows :,Introduction,Contextualized Input Encoder,question_answering,4,97,1,0,,0.000214536,0,negative,1.46E-05,1.06E-05,0.000213772,2.64E-09,5.42E-07,4.99E-07,4.47E-06,2.35E-06,0.000566376,0.999164046,1.22E-09,2.13E-05,1.39E-06
10498,question_answering4,110,This densely connects all representations of P and Q across multiple layers .,Introduction,Contextualized Input Encoder,question_answering,4,98,1,0,,1.87E-05,0,negative,5.09E-06,3.01E-06,2.70E-05,1.67E-09,2.53E-07,2.29E-07,8.64E-07,1.86E-06,0.000152681,0.999804728,1.38E-10,3.95E-06,3.73E-07
10499,question_answering4,111,Z ij * ?,Introduction,Contextualized Input Encoder,question_answering,4,99,1,0,,5.45E-07,0,negative,2.94E-07,4.08E-08,1.79E-07,2.79E-09,2.92E-08,4.37E-07,4.10E-07,3.18E-06,4.48E-07,0.999991579,8.41E-11,3.28E-06,1.14E-07
10500,question_answering4,112,R 3 represents the generated features for each ij combination of P/Q .,Introduction,Contextualized Input Encoder,question_answering,4,100,1,0,,1.35E-06,0,negative,2.88E-07,7.77E-08,2.76E-07,2.03E-10,1.53E-08,1.01E-07,1.73E-07,1.52E-06,3.32E-06,0.999993074,1.45E-11,1.11E-06,3.30E-08
10501,question_answering4,113,"In total , we obtain 3n 2 compressed attention features for each word .",Introduction,Contextualized Input Encoder,question_answering,4,101,1,0,,0.000235229,0,negative,2.09E-05,8.30E-06,4.51E-06,8.45E-08,5.05E-06,1.67E-05,3.97E-05,0.000198462,2.46E-05,0.999640599,1.62E-10,2.87E-05,1.23E-05
10502,question_answering4,114,"Intuitively , these features capture fine - grained relationships between P / Q at different stages of the network flow .",Introduction,Contextualized Input Encoder,question_answering,4,102,1,0,,3.18E-05,0,negative,2.10E-06,1.16E-06,2.39E-06,8.19E-10,1.39E-07,1.31E-07,3.35E-07,9.09E-07,3.25E-05,0.99995531,4.50E-11,4.98E-06,8.65E-08
10503,question_answering4,115,"The output of the encoder is the concatenation of all the BiRNN hidden states H 1 , H 2 H k and Z * which is a matrix of ( nh + 3 n 2 ) dimensions .",Introduction,Contextualized Input Encoder,question_answering,4,103,1,0,,8.04E-06,0,negative,5.10E-07,4.92E-07,7.28E-07,5.18E-09,1.35E-07,1.26E-06,9.88E-07,1.91E-05,1.15E-05,0.999963536,7.00E-11,1.18E-06,5.81E-07
10504,question_answering4,116,Densely Connected Core Architecture ( DECACORE ),Introduction,Contextualized Input Encoder,question_answering,4,104,1,0,,0.011931863,0,negative,1.51E-05,9.91E-06,0.000436392,3.76E-08,7.50E-07,1.29E-05,0.000290566,3.34E-05,0.000416004,0.998250942,6.44E-08,0.000377353,0.000156622
10505,question_answering4,117,This section introduces the core architecture of our proposed model .,Introduction,Contextualized Input Encoder,question_answering,4,105,1,0,,7.66E-06,0,negative,1.24E-06,1.93E-06,1.81E-06,2.07E-08,4.79E-07,1.36E-06,1.30E-06,1.46E-05,3.27E-05,0.999939266,2.29E-10,3.29E-06,2.06E-06
10506,question_answering4,118,This component corresponds to the interaction segment of standard RC model architecture .,Introduction,Contextualized Input Encoder,question_answering,4,106,1,0,,1.94E-05,0,negative,9.13E-06,2.81E-06,3.02E-05,2.41E-08,8.53E-07,1.86E-06,4.02E-06,8.16E-06,0.000221544,0.999708994,4.37E-10,5.86E-06,6.57E-06
10507,question_answering4,119,Gated Attention,Introduction,,question_answering,4,107,1,0,,0.069490637,0,negative,6.54E-05,0.009858997,0.001418812,1.51E-06,0.000529678,0.001054087,0.066211935,0.000574453,0.074793974,0.84440598,2.46E-06,0.000179894,0.000902854
10508,question_answering4,120,The outputs of the densely connected encoder are then passed into a standard gated attention layer .,Introduction,Gated Attention,question_answering,4,108,1,0,,7.41E-06,0,negative,1.37E-05,3.36E-06,3.06E-05,3.50E-09,2.85E-07,4.26E-07,2.19E-06,4.09E-06,0.000335499,0.999605092,1.58E-10,2.76E-06,1.98E-06
10509,question_answering4,121,This corresponds to the ' interact ' component in many other popular RC models that models Q/P interactions with attention .,Introduction,Gated Attention,question_answering,4,109,1,0,,7.74E-07,0,negative,2.30E-06,2.05E-07,5.52E-06,1.32E-09,5.96E-08,1.54E-07,3.68E-07,7.35E-07,1.38E-05,0.999975447,5.25E-11,1.13E-06,3.20E-07
10510,question_answering4,122,"While there are typically many choices of implementing this layer , we adopt the standard gated bi-attention layer following .",Introduction,Gated Attention,question_answering,4,110,1,0,,1.57E-06,0,negative,1.21E-05,6.78E-06,6.49E-05,3.50E-09,4.76E-07,5.68E-07,3.66E-06,3.21E-06,6.86E-05,0.999832255,1.48E-10,6.08E-06,1.41E-06
10511,question_answering4,123,where ?,Introduction,Gated Attention,question_answering,4,111,1,0,,9.65E-08,0,negative,8.55E-08,1.02E-08,7.59E-08,4.96E-10,5.76E-09,1.01E-07,9.93E-08,6.45E-07,2.39E-07,0.999998336,1.30E-11,3.61E-07,4.22E-08
10512,question_answering4,124,is the sigmoid function and F ( . ) are dense layers with ReLU activations .,Introduction,Gated Attention,question_answering,4,112,1,0,,3.69E-06,0,negative,3.83E-06,9.16E-07,1.35E-06,1.88E-08,2.42E-07,3.58E-06,4.68E-06,9.28E-05,8.00E-06,0.999879411,4.46E-11,2.49E-06,2.72E-06
10513,question_answering4,125,The output P is the query - dependent passage representation .,Introduction,Gated Attention,question_answering,4,113,1,0,,3.19E-06,0,negative,1.89E-07,9.87E-08,1.17E-06,5.55E-11,9.71E-09,2.57E-08,1.52E-07,3.41E-07,7.01E-06,0.999990098,2.27E-11,8.20E-07,8.36E-08
10514,question_answering4,126,"Gated Self - Attention Next , we employ a self - attention layer , applying Equation yet again on P , matching P against itself to form B , the output representation of the core layer .",Introduction,Gated Attention,question_answering,4,114,1,0,,0.000204983,0,negative,3.42E-05,5.50E-06,0.000691757,4.37E-09,7.62E-07,1.04E-06,2.30E-05,1.99E-06,0.000155074,0.999035493,4.12E-10,4.35E-05,7.78E-06
10515,question_answering4,127,"The key idea is that self - attention models each word in the query - dependent passsage representation against all other words , enabling each word to benefit from a wider global view of the context .",Introduction,Gated Attention,question_answering,4,115,1,0,,3.65E-05,0,negative,2.01E-05,5.64E-06,5.86E-05,3.47E-09,5.29E-07,3.18E-07,2.34E-06,1.61E-06,0.000172278,0.999731109,6.63E-11,6.08E-06,1.43E-06
10516,question_answering4,128,Dense Core,Introduction,,question_answering,4,116,1,0,,0.52428703,1,negative,0.000498392,0.003737587,0.000997339,0.000193213,0.013207222,0.023695409,0.151874366,0.005746442,0.003966687,0.792096014,4.07E-08,0.000168752,0.003818535
10517,question_answering4,129,"At this point , we note that there are two intermediate representations of P , i.e. , one after the gated bi-attention layer and one after the gated self - attention layer .",Introduction,Dense Core,question_answering,4,117,1,0,,2.17E-06,0,negative,3.11E-06,2.26E-07,1.67E-06,2.77E-10,6.05E-08,4.91E-08,1.14E-07,3.19E-07,8.64E-06,0.999984522,7.99E-12,1.19E-06,1.02E-07
10518,question_answering4,130,"We denote them as U 1 , U 2 respectively .",Introduction,Dense Core,question_answering,4,118,1,0,,2.28E-07,0,negative,9.36E-08,5.96E-08,1.01E-07,2.68E-10,2.69E-08,1.12E-07,8.15E-08,1.43E-06,9.90E-07,0.999996721,3.17E-12,2.90E-07,9.84E-08
10519,question_answering4,131,"Unlike the Densely Connected Attention Encoder , we no longer have two representations at each hierarchical level since they have already been ' fused ' .",Introduction,Dense Core,question_answering,4,119,1,0,,3.70E-06,0,negative,4.01E-05,1.74E-05,8.59E-05,7.15E-09,1.51E-06,5.84E-07,1.88E-06,3.22E-06,0.000545759,0.999292445,1.34E-10,6.76E-06,4.37E-06
10520,question_answering4,132,"Hence , we apply a one - sided BAC to all permutations of [ U 1 , U 2 ] and Q i , ?i = 1 , 2 k.",Introduction,Dense Core,question_answering,4,120,1,0,,5.70E-07,0,negative,5.96E-06,6.12E-07,3.58E-06,3.38E-10,1.33E-07,1.02E-07,3.04E-07,5.24E-07,1.25E-05,0.999974277,7.98E-12,1.78E-06,1.97E-07
10521,question_answering4,133,"Note that the one - sided BAC only outputs values for the left sequence , ignoring the right sequence .",Introduction,Dense Core,question_answering,4,121,1,0,,2.13E-06,0,negative,1.88E-06,9.39E-08,1.95E-06,1.03E-10,3.09E-08,2.92E-08,6.70E-08,1.55E-07,3.88E-06,0.999991021,3.65E-12,8.38E-07,6.32E-08
10522,question_answering4,134,where R kj ?,Introduction,Dense Core,question_answering,4,122,1,0,,1.44E-07,0,negative,4.22E-07,2.33E-08,2.39E-07,2.61E-10,2.21E-08,1.12E-07,1.83E-07,4.59E-07,2.98E-07,0.999996901,6.68E-12,1.16E-06,1.77E-07
10523,question_answering4,135,R 3 represents the connection output and F C is the one - sided BAC function .,Introduction,Dense Core,question_answering,4,123,1,0,,5.61E-07,0,negative,5.10E-07,1.34E-07,7.86E-07,3.96E-10,4.32E-08,1.45E-07,1.27E-07,1.50E-06,7.06E-06,0.99998916,6.82E-12,2.29E-07,3.01E-07
10524,question_answering4,136,"All values of R kj , ?j = 1 , 2 , ?k = 1 , 2 n are concatenated to form a matrix R of ( 2 n 6 ) , which is then concatenated with U 2 to form M ? R p ( d+12 n ) .",Introduction,Dense Core,question_answering,4,124,1,0,,4.07E-06,0,negative,5.42E-06,3.93E-07,3.32E-06,3.79E-09,7.73E-07,1.43E-06,1.91E-06,4.00E-06,3.50E-06,0.999973862,6.86E-12,2.94E-06,2.46E-06
10525,question_answering4,137,This final representation is then passed to the answer prediction layer .,Introduction,Dense Core,question_answering,4,125,1,0,,7.43E-06,0,negative,1.46E-06,1.03E-06,1.25E-05,3.31E-10,1.05E-07,1.30E-07,4.04E-07,1.04E-06,0.000206437,0.999774932,3.11E-11,5.15E-07,1.40E-06
10526,question_answering4,138,Answer Pointer and Prediction Layer,Introduction,,question_answering,4,126,1,0,,0.075496156,0,negative,4.67E-05,0.003212006,0.001601185,1.49E-05,0.002424981,0.006034249,0.052264877,0.001640598,0.027018961,0.903053002,5.51E-08,3.82E-05,0.002650283
10527,question_answering4,139,"Next , we pass M through a stacked BiRNN model with two layers and obtain two representations , H 1 and H 2 respectively .",Introduction,Answer Pointer and Prediction Layer,question_answering,4,127,1,0,,2.23E-06,0,negative,7.47E-07,6.45E-08,1.41E-06,5.09E-11,1.74E-08,2.34E-08,1.48E-07,1.64E-07,4.87E-06,0.999991457,1.11E-12,9.94E-07,1.01E-07
10528,question_answering4,140,The start and end pointers are then learned via :,Introduction,Answer Pointer and Prediction Layer,question_answering,4,128,1,0,,4.87E-07,0,negative,1.06E-06,4.26E-08,1.79E-06,2.15E-11,9.23E-09,1.48E-08,1.10E-07,8.23E-08,3.16E-06,0.999992688,5.33E-13,1.01E-06,2.72E-08
10529,question_answering4,141,"where w 1 , w 2 ?",Introduction,Answer Pointer and Prediction Layer,question_answering,4,129,1,0,,6.07E-08,0,negative,1.56E-07,1.17E-08,5.37E-08,7.02E-11,5.87E-09,5.18E-08,1.24E-07,4.32E-07,2.18E-07,0.99999822,5.20E-13,6.78E-07,4.87E-08
10530,question_answering4,142,Rd are parameters of this layer .,Introduction,Answer Pointer and Prediction Layer,question_answering,4,130,1,0,,5.80E-07,0,negative,4.25E-07,7.69E-08,6.33E-07,7.16E-10,3.02E-08,2.11E-07,4.54E-07,2.87E-06,5.28E-06,0.999989046,1.32E-12,4.60E-07,5.09E-07
10531,question_answering4,143,"To train the model , following prior work , we minimize the sum of negative log probabilities of the start and end indices :",Introduction,Answer Pointer and Prediction Layer,question_answering,4,131,1,0,,8.02E-07,0,negative,1.40E-06,1.95E-07,4.68E-07,9.27E-11,2.72E-08,8.59E-08,4.62E-07,1.36E-06,2.44E-06,0.999992168,5.82E-13,1.25E-06,1.40E-07
10532,question_answering4,144,"where N is the number of samples , y 1 i , y 2 i are the true start and end indices .",Introduction,Answer Pointer and Prediction Layer,question_answering,4,132,1,0,,1.67E-07,0,negative,1.01E-07,1.22E-08,3.96E-08,2.67E-11,3.32E-09,2.20E-08,7.24E-08,4.04E-07,2.84E-07,0.999998724,2.24E-13,3.08E-07,3.02E-08
10533,question_answering4,145,pk is the k - th value of the vector p.,Introduction,Answer Pointer and Prediction Layer,question_answering,4,133,1,0,,1.39E-07,0,negative,3.96E-08,3.40E-09,3.06E-08,1.40E-11,1.74E-09,9.26E-09,2.55E-08,1.03E-07,1.44E-07,0.99999945,1.04E-13,1.80E-07,1.32E-08
10534,question_answering4,146,"The test span is chosen by finding the maximum value of p 1 k , p 2 l where k ?",Introduction,Answer Pointer and Prediction Layer,question_answering,4,134,1,0,,3.06E-07,0,negative,9.98E-07,6.86E-08,1.76E-07,1.49E-10,3.44E-08,1.98E-07,1.43E-06,2.74E-06,4.04E-07,0.999990164,5.35E-13,3.52E-06,2.73E-07
10535,question_answering4,147,l.,Introduction,Answer Pointer and Prediction Layer,question_answering,4,135,1,0,,3.82E-07,0,negative,2.37E-07,3.17E-09,7.33E-08,5.46E-10,8.34E-09,1.06E-07,1.57E-07,4.84E-07,7.14E-08,0.999997947,2.55E-13,7.37E-07,1.76E-07
10536,question_answering4,148,Experiments,,,question_answering,4,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
10537,question_answering4,149,This section describes our experiment setup and empirical results .,Experiments,Experiments,question_answering,4,1,1,0,,0.000468277,0,negative,0.000165338,0.000247118,7.12E-05,0.000177783,0.000122917,0.00360569,0.000692182,0.023118686,5.58E-05,0.971317857,6.40E-05,0.000243831,0.000117588
10538,question_answering4,150,Datasets and Competitor Baselines,,,question_answering,4,0,1,0,,0.001130608,0,negative,5.99E-06,5.67E-05,2.83E-05,1.84E-07,6.11E-07,7.46E-05,7.86E-05,0.000976885,2.03E-05,0.997750364,0.000763667,0.000242645,1.16E-06
10539,question_answering4,151,We conduct experiments on four challenging QA datasets which are described as follows :,Datasets and Competitor Baselines,Datasets and Competitor Baselines,question_answering,4,1,1,0,,0.087141518,0,negative,3.33E-05,3.35E-05,0.410851618,2.16E-06,4.44E-06,0.000100605,0.008892265,0.001570912,2.90E-06,0.568354351,0.000101761,0.009963921,8.83E-05
10540,question_answering4,152,NewsQA,Datasets and Competitor Baselines,,question_answering,4,2,1,1,baselines,0.891510315,1,baselines,0.000163368,7.20E-06,0.669627908,6.34E-05,1.03E-05,0.001224742,0.150722256,0.004821518,4.93E-06,0.10564624,0.000212434,0.061325349,0.006170404
10541,question_answering4,153,This challenging RC dataset comprises 100 k QA pairs .,Datasets and Competitor Baselines,NewsQA,question_answering,4,3,1,0,,0.227035529,0,negative,0.000148053,1.08E-05,0.010638823,2.89E-06,2.95E-05,0.000401897,0.353827908,0.00011449,2.87E-06,0.621691569,1.59E-05,0.012454163,0.000661105
10542,question_answering4,154,Passages are relatively long at about 600 words on average .,Datasets and Competitor Baselines,NewsQA,question_answering,4,4,1,0,,0.320138008,0,negative,0.000136492,8.12E-07,0.000193019,1.10E-05,3.89E-05,0.00085707,0.163363024,9.25E-05,8.04E-07,0.828013625,5.43E-06,0.006470677,0.000816666
10543,question_answering4,155,This dataset has also been extensively used in benchmarking RC models .,Datasets and Competitor Baselines,NewsQA,question_answering,4,5,1,0,,0.017976615,0,negative,6.93E-06,1.95E-07,0.000611054,3.65E-07,1.24E-06,0.000117535,0.022110157,1.81E-05,2.96E-07,0.976283017,3.44E-06,0.000795418,5.23E-05
10544,question_answering4,156,"On this dataset , the key competitors are BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .",Datasets and Competitor Baselines,NewsQA,question_answering,4,6,1,1,baselines,0.802050876,1,negative,0.000144864,7.65E-07,0.005117769,3.77E-06,2.21E-05,0.000674547,0.239242876,3.74E-05,6.70E-07,0.745575764,9.18E-07,0.008856075,0.000322495
10545,question_answering4,157,Quasar -T,Datasets and Competitor Baselines,,question_answering,4,7,1,1,baselines,0.948480836,1,baselines,0.000282049,2.01E-05,0.446405945,1.07E-05,2.77E-06,0.000455081,0.05328545,0.004458598,1.23E-05,0.185370686,0.000959084,0.30681653,0.001920767
10546,question_answering4,158,This dataset comprises 43 k factoid - based QA pairs and is constructed using ClueWeb09 as its backbone corpus .,Datasets and Competitor Baselines,Quasar -T,question_answering,4,8,1,0,,0.052023484,0,negative,0.000322252,2.86E-05,0.037829272,0.000312389,0.000981444,0.000489266,0.016925221,0.000192748,1.52E-05,0.931964382,3.37E-05,0.005434685,0.005470849
10547,question_answering4,159,The key competitors on this dataset are BiDAF and the Reinforced Ranker - Reader ( R 3 ) .,Datasets and Competitor Baselines,Quasar -T,question_answering,4,9,1,1,baselines,0.030160241,0,negative,0.001110024,7.10E-06,0.108627118,1.58E-05,5.37E-05,0.000262745,0.023662462,0.000141664,8.07E-06,0.837772066,4.42E-06,0.027439259,0.000895526
10548,question_answering4,160,"Several variations of the ranker - reader model ( e.g. , SR , SR 2 ) , which use the Match - LSTM underneath , are also compared against .",Datasets and Competitor Baselines,Quasar -T,question_answering,4,10,1,0,,0.182633611,0,baselines,5.11E-05,7.22E-06,0.672081771,3.00E-07,4.55E-07,3.78E-05,0.004506632,6.24E-05,1.47E-05,0.321705818,1.73E-05,0.001443778,7.08E-05
10549,question_answering4,161,SearchQA,Datasets and Competitor Baselines,,question_answering,4,11,1,1,baselines,0.98535438,1,baselines,0.000298164,1.32E-05,0.700042253,0.001333782,2.55E-05,0.009744665,0.152248824,0.020671157,2.40E-05,0.095203342,5.62E-05,0.011116035,0.009222959
10550,question_answering4,162,This dataset aims to emulate the search and retrieval process in question answering applications .,Datasets and Competitor Baselines,SearchQA,question_answering,4,12,1,0,,0.080134552,0,negative,0.000284248,3.83E-06,0.040173844,0.000675432,0.000247567,0.001197806,0.049649477,9.29E-05,4.00E-06,0.852174873,5.48E-05,0.008894682,0.046546503
10551,question_answering4,163,The challenge involves reasoning over multiple documents .,Datasets and Competitor Baselines,SearchQA,question_answering,4,13,1,0,,0.045609553,0,negative,5.61E-05,2.54E-06,0.005503043,1.04E-05,1.07E-06,0.000228961,0.006088744,0.000212967,7.65E-06,0.976972914,0.000250842,0.005862507,0.004802203
10552,question_answering4,164,"In this dataset , we concatenate all documents into a single passage context and perform RC over the documents .",Datasets and Competitor Baselines,SearchQA,question_answering,4,14,1,0,,0.123273824,0,negative,0.00013507,4.49E-06,0.281011245,1.91E-06,9.18E-06,0.000150746,0.012935872,0.000103822,3.52E-06,0.701734651,1.13E-06,0.003480264,0.000428106
10553,question_answering4,165,"The competitor baselines on this dataset are Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .",Datasets and Competitor Baselines,SearchQA,question_answering,4,15,1,1,baselines,0.900650311,1,baselines,8.46E-05,6.46E-07,0.523781954,1.78E-07,5.09E-07,9.90E-05,0.060938646,5.15E-05,5.20E-07,0.398726988,1.29E-06,0.016010081,0.000304149
10554,question_answering4,166,Narrative QA ] is a recent QA dataset that involves comprehension over stories .,Datasets and Competitor Baselines,SearchQA,question_answering,4,16,1,1,baselines,0.735157291,1,experiments,0.000277606,2.10E-06,0.025757873,5.03E-05,2.18E-05,0.000420148,0.350046296,7.85E-05,8.42E-07,0.2929751,0.000364438,0.119485819,0.21051918
10555,question_answering4,167,We use the summaries setting 4 which is closer to a standard QA or reading comprehension setting .,Datasets and Competitor Baselines,SearchQA,question_answering,4,17,1,0,,0.466701426,0,negative,0.00010299,1.72E-05,0.052836261,4.45E-06,6.19E-06,0.000753724,0.078863745,0.001266886,9.63E-06,0.859840023,2.72E-06,0.004776931,0.001519277
10556,question_answering4,168,"We compare with the baselines in the original paper , namely Seq2Seq , Attention Sum Reader and BiDAF .",Datasets and Competitor Baselines,SearchQA,question_answering,4,18,1,1,baselines,0.460798296,0,negative,5.88E-05,2.38E-06,0.124406813,1.90E-07,5.25E-07,0.000114791,0.04030673,0.00013115,2.01E-06,0.829379949,7.80E-07,0.005439312,0.000156615
10557,question_answering4,169,We also compare with the recent BiAttention + MRU model .,Datasets and Competitor Baselines,SearchQA,question_answering,4,19,1,1,baselines,0.205345288,0,negative,0.000234629,6.27E-06,0.239855328,6.64E-07,9.07E-07,0.000212578,0.02628161,0.000209001,1.88E-05,0.723347602,2.58E-06,0.009403358,0.00042665
10558,question_answering4,170,"As compared to the popular SQuAD dataset , these datasets are either ( 1 ) more challenging 5 , involves more multi-sentence reasoning or is concerned with searching across multiple documents in an ' open domain ' setting ( Searc hQA / Quasar - T ) .",Datasets and Competitor Baselines,SearchQA,question_answering,4,20,1,0,,0.005505705,0,negative,2.27E-05,4.06E-07,0.005573737,2.76E-06,2.93E-06,0.000111912,0.005980134,4.39E-05,5.11E-07,0.982505122,4.52E-06,0.004495173,0.001256223
10559,question_answering4,171,"Hence , these datasets accurately reflect real world applications to a greater extent .",Datasets and Competitor Baselines,SearchQA,question_answering,4,21,1,0,,0.009182052,0,negative,1.22E-05,9.96E-08,0.001469882,4.97E-08,1.66E-07,1.10E-05,0.00068069,1.67E-05,3.42E-07,0.995812415,7.74E-08,0.001977832,1.86E-05
10560,question_answering4,172,"However , we regard the concatenated documents as a single context for performing reading comprehension .",Datasets and Competitor Baselines,SearchQA,question_answering,4,22,1,0,,0.00427789,0,negative,5.40E-05,1.05E-06,0.013785738,3.78E-07,4.54E-07,4.54E-05,0.002054358,7.60E-05,2.95E-06,0.979522243,5.15E-06,0.004169469,0.000282732
10561,question_answering4,173,The evaluation metrics are the EM ( exact match ) and F1 score .,Datasets and Competitor Baselines,SearchQA,question_answering,4,23,1,0,,0.013408394,0,negative,9.50E-06,2.37E-07,0.004964471,2.37E-08,8.03E-08,3.98E-05,0.003342279,9.86E-05,5.09E-07,0.990576631,1.38E-07,0.000930916,3.68E-05
10562,question_answering4,174,"Note that for all datasets , we compare all models solely on the RC task .",Datasets and Competitor Baselines,SearchQA,question_answering,4,24,1,0,,0.006579197,0,negative,2.70E-05,2.66E-07,0.002303847,6.49E-08,1.69E-07,3.91E-05,0.002382968,8.86E-05,5.17E-07,0.994137315,3.25E-08,0.001002975,1.72E-05
10563,question_answering4,175,"Therefore , for fair comparison we do not compare with algorithms that use a second - pass answer re-ranker .",Datasets and Competitor Baselines,SearchQA,question_answering,4,25,1,0,,0.00052516,0,negative,1.38E-05,1.97E-07,0.011680761,1.37E-08,6.37E-08,1.44E-05,0.00098644,1.64E-05,4.79E-07,0.986746294,5.59E-08,0.000533132,7.94E-06
10564,question_answering4,176,"Finally , to ensure that our model is not a failing case of SQuAD , and as requested by reviewers , we also include development set scores of our model on SQuAD .",Datasets and Competitor Baselines,SearchQA,question_answering,4,26,1,0,,0.001995927,0,negative,2.65E-05,1.13E-07,0.001099537,1.68E-07,7.34E-07,3.46E-05,0.000957124,4.12E-05,3.77E-07,0.997057315,1.44E-08,0.000757615,2.46E-05
10565,question_answering4,177,Experimental Setup,,,question_answering,4,0,1,0,,0.000999032,0,negative,7.98E-06,0.000206703,8.17E-06,1.21E-06,1.17E-06,0.000268282,7.26E-05,0.003290456,0.000133773,0.99437746,0.001522228,0.000107329,2.61E-06
10566,question_answering4,178,Our model is implemented in Tensorflow .,Experimental Setup,Experimental Setup,question_answering,4,1,1,1,experimental-setup,0.987967495,1,experimental-setup,8.85E-05,5.13E-05,0.002544668,0.000331517,4.13E-06,0.62853177,0.008282378,0.317212171,8.29E-05,0.042716766,2.10E-05,4.25E-05,9.03E-05
10567,question_answering4,179,"The sequence lengths are capped at 800/700/1500/1100 for News QA , Search QA , Quasar - T and Narrative QA respectively .",Experimental Setup,Experimental Setup,question_answering,4,2,1,1,experimental-setup,0.984597595,1,hyperparameters,6.84E-06,1.42E-05,0.000306607,1.57E-06,3.53E-07,0.180759368,0.001968882,0.780200482,1.08E-05,0.036691217,4.99E-06,2.34E-05,1.13E-05
10568,question_answering4,180,"We use Adadelta with ? = 0.5 for News QA , Adam with ? = 0.001 for Search QA , Quasar - T and Narrative QA .",Experimental Setup,Experimental Setup,question_answering,4,3,1,1,experimental-setup,0.993513581,1,hyperparameters,2.60E-05,6.18E-05,0.002299972,3.90E-06,1.03E-06,0.236935335,0.007816286,0.691936059,2.17E-05,0.060677875,2.32E-05,0.000160849,3.60E-05
10569,question_answering4,181,"The choice of the RNN encoder is tuned between GRU and LSTM cells and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .",Experimental Setup,Experimental Setup,question_answering,4,4,1,1,experimental-setup,0.991618605,1,hyperparameters,1.08E-05,1.34E-05,7.49E-05,4.16E-06,3.14E-07,0.147550316,0.001557416,0.841652572,8.03E-06,0.009083978,2.92E-06,2.55E-05,1.57E-05
10570,question_answering4,182,We use the CUDNN implementation of the RNN encoder .,Experimental Setup,Experimental Setup,question_answering,4,5,1,1,experimental-setup,0.930972782,1,experimental-setup,8.11E-05,5.37E-05,0.017751012,4.58E-05,3.72E-06,0.490974525,0.020550213,0.422858474,9.14E-05,0.047353244,1.92E-05,0.000106161,0.000111409
10571,question_answering4,183,"Batch size is tuned amongst { 16 , 32 , 64 } .",Experimental Setup,Experimental Setup,question_answering,4,6,1,1,experimental-setup,0.99108251,1,hyperparameters,7.80E-06,1.43E-05,7.46E-05,3.95E-06,3.02E-07,0.152368439,0.001501173,0.835247056,1.12E-05,0.010718373,3.77E-06,2.79E-05,2.11E-05
10572,question_answering4,184,"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } and applied to all RNN and fully - connected layers .",Experimental Setup,Experimental Setup,question_answering,4,7,1,1,experimental-setup,0.993546253,1,hyperparameters,1.04E-05,1.42E-05,5.50E-05,4.20E-06,2.63E-07,0.103023995,0.001055489,0.889999574,9.78E-06,0.005793773,1.74E-06,1.53E-05,1.63E-05
10573,question_answering4,185,We apply variational dropout in - between RNN layers .,Experimental Setup,Experimental Setup,question_answering,4,8,1,1,experimental-setup,0.9344266,1,hyperparameters,0.000148488,9.94E-05,0.001711744,8.98E-06,1.39E-06,0.105447562,0.002583739,0.866904644,0.000100345,0.022873078,3.27E-06,6.24E-05,5.50E-05
10574,question_answering4,186,We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training .,Experimental Setup,Experimental Setup,question_answering,4,9,1,1,experimental-setup,0.989399899,1,hyperparameters,7.73E-06,1.24E-05,4.78E-05,2.38E-06,2.41E-07,0.112557895,0.001045171,0.87769135,8.98E-06,0.008589885,1.76E-06,1.96E-05,1.48E-05
10575,question_answering4,187,The size of the character embeddings is set to 8 and the character RNN is set to the same as the word - level RNN encoders .,Experimental Setup,Experimental Setup,question_answering,4,10,1,1,experimental-setup,0.992461331,1,hyperparameters,4.12E-06,8.85E-06,4.90E-05,7.86E-07,9.91E-08,0.079025208,0.000928735,0.912507356,9.26E-06,0.007441413,2.02E-06,1.28E-05,1.04E-05
10576,question_answering4,188,The maximum characters per word is set to 16 .,Experimental Setup,Experimental Setup,question_answering,4,11,1,1,experimental-setup,0.991204708,1,hyperparameters,3.67E-06,7.94E-06,5.61E-05,5.94E-07,9.77E-08,0.083768213,0.001194503,0.904690257,8.30E-06,0.010235403,2.67E-06,2.06E-05,1.16E-05
10577,question_answering4,189,The number of layers in DECAENC is set to 3 and the number of factors in the factorization kernel is set to 64 .,Experimental Setup,Experimental Setup,question_answering,4,12,1,1,experimental-setup,0.99149901,1,hyperparameters,6.51E-06,9.69E-06,4.41E-05,2.09E-06,2.06E-07,0.095658742,0.001457881,0.896761859,6.37E-06,0.00600684,1.92E-06,2.41E-05,1.97E-05
10578,question_answering4,190,We use a learning rate decay factor of 2 and patience of 3 epochs whenever the EM ( or ROUGE - L ) score on the development set does not increase .,Experimental Setup,Experimental Setup,question_answering,4,13,1,1,experimental-setup,0.98274849,1,hyperparameters,1.52E-05,1.82E-05,0.000105256,2.16E-06,3.33E-07,0.089945793,0.001451334,0.898610009,1.40E-05,0.009777502,2.59E-06,3.19E-05,2.57E-05
10579,question_answering4,191,Results,,,question_answering,4,0,1,0,,0.031002109,0,negative,0.000192391,0.000125435,8.21E-06,6.67E-07,1.66E-06,2.50E-05,0.000128686,0.000413965,1.55E-05,0.991629282,0.002072435,0.005384302,2.50E-06
10580,question_answering4,192,"Overall , our results are optimistic and promising , with results indicating that DECAPROP achieves state - of - the - art performance 6 on all four datasets . 66.2 75.9 DCN + CoVE 71.3 79.9 R- NET 72.3 80.6 R - NET",Results,Results,question_answering,4,1,1,1,results,0.315566888,0,results,0.003345631,7.63E-07,9.92E-05,4.40E-06,1.29E-06,3.09E-05,0.005144143,8.60E-05,6.54E-07,0.027829067,2.40E-05,0.962623444,0.000810589
10581,question_answering4,193,( Our re-implementation ) 71.9 79.6 DECAPROP ( This paper ) 72.9 81.4 QANet 73.6 82.7 News QA reports the results on News QA .,Results,Results,question_answering,4,2,1,0,,0.02008459,0,results,0.006290197,8.03E-06,0.003257064,2.90E-05,1.77E-05,0.000148503,0.013875265,0.000166165,7.90E-06,0.359356415,0.000582829,0.611214153,0.00504679
10582,question_answering4,194,"On this dataset , DECAPROP outperforms the existing state - of - the - art , i.e. , the recent AMANDA model by ( + 4.7 % EM / + 2.6 % F1 ) .",Results,Results,question_answering,4,3,1,1,results,0.957447554,1,results,0.005278419,1.04E-06,0.000118285,1.34E-06,1.06E-06,7.13E-06,0.003123156,2.45E-05,3.32E-07,0.023075046,6.81E-06,0.968123868,0.000239045
10583,question_answering4,195,"Notably , AMANDA is a strong neural baseline that also incorporates gated self - attention layers , along with question - aware pointer layers .",Results,Results,question_answering,4,4,1,0,,0.264294262,0,baselines,0.054977421,0.000128058,0.641313224,2.36E-05,1.89E-05,0.000273159,0.002458045,0.000443094,0.000407418,0.24210618,0.000161943,0.056883867,0.000805073
10584,question_answering4,196,"Moreover , our proposed model also outperforms well - established baselines such as Match - LSTM ( + 18 % EM / + 16.3 % F1 ) and BiDAF ( + 16 % EM / + 14 % F1 ) .",Results,Results,question_answering,4,5,1,1,results,0.964479518,1,results,0.007706926,9.14E-07,4.55E-05,3.35E-06,9.95E-07,1.13E-05,0.005082121,5.38E-05,3.91E-07,0.015402423,7.16E-06,0.971207258,0.000477774
10585,question_answering4,197,reports the results on Quasar - T .,Results,Results,question_answering,4,6,1,1,results,0.391942161,0,results,0.043368515,1.55E-05,0.012381303,5.49E-05,3.31E-05,0.000110879,0.004421461,0.000130675,2.24E-05,0.44485683,0.000160977,0.492759368,0.001684127
10586,question_answering4,198,"Our model achieves state - of - the - art performance on this dataset , outperforming the state - of - the - art R 3 ( Reinforced Ranker Reader ) by a considerable margin of + 4.4 % EM / + 6 % F1 .",Results,Results,question_answering,4,7,1,1,results,0.839000793,1,results,0.00295676,9.06E-07,7.67E-05,2.10E-06,1.03E-06,9.67E-06,0.004030287,4.04E-05,4.03E-07,0.020794509,6.95E-06,0.971525393,0.000554981
10587,question_answering4,199,Performance gain over standard baselines such as BiDAF and GA are even larger ( > 15 % F1 ) .,Results,Results,question_answering,4,8,1,0,,0.829881088,1,results,0.018869062,9.56E-07,0.000103312,1.23E-06,9.57E-07,5.58E-06,0.002114422,2.84E-05,5.65E-07,0.054045446,4.21E-06,0.924625822,0.000200031
10588,question_answering4,200,Quasar - T,Results,,question_answering,4,9,1,0,,0.859473467,1,results,0.007557151,1.56E-05,0.001128458,2.47E-05,9.26E-06,0.00014427,0.009320647,0.000350512,1.57E-05,0.114953362,0.000361426,0.862889946,0.003228855
10589,question_answering4,201,Search QA and report the results 7 on Search QA .,Results,Quasar - T,question_answering,4,10,1,0,,0.043341508,0,results,0.003252404,2.24E-06,0.000673183,1.03E-05,2.48E-06,2.50E-05,0.00063744,2.39E-05,6.12E-06,0.443928305,0.000103992,0.514089223,0.037245471
10590,question_answering4,202,"On the original setting , our model outperforms AMANDA by + 15.4 % EM and + 14.2 % in terms of F1 score .",Results,Quasar - T,question_answering,4,11,1,1,results,0.931508041,1,results,0.005850588,3.29E-07,2.45E-05,3.80E-06,3.16E-07,5.72E-06,0.001180469,1.24E-05,4.99E-07,0.025827668,1.96E-06,0.957467706,0.009624067
10591,question_answering4,203,"On the over all setting , our model outperforms both AQA ( + 18.1 % EM / + 18 % F1 ) and Reinforced Reader Ranker ( + 7.8 % EM / + 8.3 % F1 ) .",Results,Quasar - T,question_answering,4,12,1,1,results,0.938009455,1,results,0.004973944,3.14E-07,2.11E-05,1.83E-06,2.25E-07,3.68E-06,0.000822818,9.84E-06,3.67E-07,0.029795077,1.30E-06,0.960497132,0.003872398
10592,question_answering4,204,Both models are reinforcement learning based extensions of existing strong baselines such as BiDAF and Match - LSTM .,Results,Quasar - T,question_answering,4,13,1,0,,0.127345264,0,negative,0.005220699,3.67E-05,0.129602041,5.46E-06,3.97E-06,5.65E-05,0.00038004,0.000103672,0.000440484,0.835142955,1.94E-05,0.026095906,0.002892079
10593,question_answering4,205,Narrative QA reports the results on Narrative QA .,Results,Quasar - T,question_answering,4,14,1,0,,0.252996058,0,results,0.002334075,1.79E-06,0.001411963,3.47E-06,1.28E-06,1.29E-05,0.001276782,1.23E-05,2.00E-06,0.247620478,0.000126168,0.71729473,0.029902133
10594,question_answering4,206,"Our proposed model outperforms all baseline systems ( Seq2Seq , ASR , BiDAF ) in the original paper .",Results,Quasar - T,question_answering,4,15,1,0,,0.651319745,1,results,0.001264833,9.93E-08,1.29E-05,5.11E-07,7.37E-08,1.90E-06,0.000561363,3.73E-06,1.05E-07,0.024280558,9.72E-07,0.971273517,0.002599404
10595,question_answering4,207,"On average , there is a ?",Results,Quasar - T,question_answering,4,16,1,0,,0.003054033,0,negative,0.002853237,8.33E-07,8.69E-05,1.80E-06,1.66E-06,1.11E-05,6.36E-05,2.91E-05,4.81E-06,0.961560086,7.04E-07,0.034658493,0.000727694
10596,question_answering4,208,+ 5 % improvement across all metrics .,Results,Quasar - T,question_answering,4,17,1,0,,0.854989753,1,results,0.00825439,2.71E-07,3.82E-05,5.82E-07,2.16E-07,2.14E-06,0.00039788,6.57E-06,4.61E-07,0.083789165,8.39E-07,0.905220796,0.002288517
10597,question_answering4,209,SQuAD reports dev scores 8 of our model against several representative models on the popular SQuAD benchmark .,Results,Quasar - T,question_answering,4,18,1,1,results,0.048133248,0,negative,0.001063965,6.73E-07,6.07E-05,4.73E-06,2.19E-06,2.24E-05,0.000588367,4.61E-05,1.51E-06,0.666828257,1.93E-06,0.320939973,0.010439223
10598,question_answering4,210,"While our model does not achieve state - of - the - art performance , our model can outperform the base R - NET ( both our implementation as well as the published score ) .",Results,Quasar - T,question_answering,4,19,1,1,results,0.600541945,1,results,0.001907012,9.13E-08,1.07E-05,5.49E-07,8.07E-08,2.14E-06,0.000379476,5.55E-06,1.70E-07,0.057455253,7.10E-07,0.937831098,0.002407221
10599,question_answering4,211,Our model achieves reasonably competitive performance .,Results,Quasar - T,question_answering,4,20,1,0,,0.20881975,0,results,0.001598396,6.78E-07,7.66E-05,3.40E-06,5.30E-07,1.61E-05,0.000832961,4.75E-05,2.43E-06,0.26291348,3.09E-06,0.721147308,0.013357542
10600,question_answering4,212,Ablation Study,,,question_answering,4,0,1,0,,0.006818876,0,negative,0.046739945,0.000279772,0.002808551,0.000142874,8.76E-05,0.000276199,0.001210224,0.00046093,0.000147782,0.934438164,0.002965146,0.010376843,6.60E-05
10601,question_answering4,213,We conduct an ablation study on the New s QA development set .,Ablation Study,Ablation Study,question_answering,4,1,1,1,ablation-analysis,0.024548483,0,ablation-analysis,0.827455565,2.86E-05,0.002497025,6.87E-06,0.000283152,3.19E-05,4.23E-05,1.09E-06,5.19E-05,0.169388637,2.30E-05,1.42E-05,0.000175776
10602,question_answering4,214,"More specifically , we report the development scores of seven ablation baselines .",Ablation Study,Ablation Study,question_answering,4,2,1,0,,0.028899109,0,negative,0.22400374,2.01E-05,0.002342312,7.05E-06,0.000620973,3.66E-05,2.19E-05,1.31E-06,5.56E-05,0.772802586,1.13E-05,7.81E-06,6.88E-05
10603,question_answering4,215,"In ( 1 ) , we removed the entire DECAPROP architecture , reverting it to an enhanced version of the original R - NET model 9 . In ( 2 ) , we removed DECACORE and passed U 2 to the answer layer instead of M .",Ablation Study,Ablation Study,question_answering,4,3,1,0,,0.021038692,0,ablation-analysis,0.60832264,9.40E-06,0.002728082,4.12E-06,3.49E-05,9.22E-05,3.18E-05,3.69E-06,0.000241386,0.388268247,1.13E-05,2.89E-06,0.000249449
10604,question_answering4,216,"In , we removed the DECAENC layer and used a 3 - layered BiRNN instead .",Ablation Study,Ablation Study,question_answering,4,4,1,0,,0.211649089,0,ablation-analysis,0.936867583,2.99E-06,0.004176709,1.75E-06,7.85E-06,3.18E-05,1.60E-05,1.20E-06,9.48E-05,0.058683178,3.68E-06,2.39E-06,0.000110176
10605,question_answering4,217,"In ( 4 ) , we kept the DECAENC but only compared layer of the same hierarchy and omitted cross hierarchical comparisons .",Ablation Study,Ablation Study,question_answering,4,5,1,0,,0.00305622,0,ablation-analysis,0.507686725,6.12E-05,0.027459552,3.55E-07,1.02E-05,4.34E-05,6.45E-05,4.23E-06,0.000882569,0.463720594,1.64E-05,6.16E-06,4.41E-05
10606,question_answering4,218,"In ( 5 ) , we removed the Gated Bi-Attention and Gated Self - Attention layers .",Ablation Study,Ablation Study,question_answering,4,6,1,0,,0.394952835,0,ablation-analysis,0.959179324,8.09E-06,0.000833834,9.60E-07,6.52E-06,2.88E-05,2.28E-05,3.21E-06,0.000178921,0.039633398,1.45E-06,2.02E-06,0.000100673
10607,question_answering4,219,Removing these layers simply allow previous layers to pass through .,Ablation Study,Ablation Study,question_answering,4,7,1,0,,0.001578347,0,negative,0.200618566,5.10E-06,0.000766722,1.48E-06,1.02E-05,4.45E-05,7.84E-06,3.27E-06,0.000317577,0.798158912,5.36E-06,1.75E-06,5.87E-05
10608,question_answering4,220,"In ( 6 - 7 ) , we varied n , the number of layers of DECAENC .",Ablation Study,Ablation Study,question_answering,4,8,1,0,,0.064294181,0,negative,0.461966565,2.37E-05,0.000612268,1.88E-06,1.80E-05,6.80E-05,6.90E-05,1.28E-05,0.000184251,0.536792612,4.12E-05,9.27E-06,0.000200491
10609,question_answering4,221,"Finally , in ( 8 - 9 ) , we varied the FM with linear and nonlinear feed - forward layers . From ( 1 ) , we observe a significant gap in performance between DECAPROP and R - NET .",Ablation Study,Ablation Study,question_answering,4,9,1,1,ablation-analysis,0.958149661,1,ablation-analysis,0.996367166,1.92E-07,5.68E-06,2.23E-08,1.59E-07,5.48E-07,1.99E-05,1.03E-07,3.72E-07,0.003583325,4.00E-07,1.14E-05,1.07E-05
10610,question_answering4,222,This demonstrates the effectiveness of our proposed architecture .,Ablation Study,Ablation Study,question_answering,4,10,1,0,,0.019444832,0,negative,0.227326343,3.23E-06,0.000106541,3.22E-07,2.48E-06,1.65E-05,2.60E-05,2.85E-06,7.74E-05,0.772289875,1.37E-05,5.94E-06,0.000128927
10611,question_answering4,223,"Overall , the key insight is that all model components are crucial to DECAPROP .",Ablation Study,Ablation Study,question_answering,4,11,1,1,ablation-analysis,0.921969917,1,ablation-analysis,0.949223361,1.01E-05,0.000218818,2.62E-06,1.15E-05,1.84E-05,3.66E-05,1.89E-06,0.000212438,0.049978736,7.69E-06,6.84E-06,0.000270967
10612,question_answering4,224,"Notably , the DECAENC seems to contribute the most to the over all performance .",Ablation Study,Ablation Study,question_answering,4,12,1,1,ablation-analysis,0.975509284,1,ablation-analysis,0.999368465,6.60E-08,4.10E-06,3.03E-07,6.41E-07,6.89E-07,9.73E-06,3.77E-08,1.65E-07,0.00058334,8.56E-08,4.75E-06,2.76E-05
10613,question_answering4,225,"Finally , shows the performance plot of the development EM metric ( New s QA ) over training .",Ablation Study,Ablation Study,question_answering,4,13,1,0,,0.026341297,0,negative,0.411163075,1.47E-06,0.000221826,3.92E-07,1.06E-05,1.41E-05,3.16E-05,1.22E-06,1.09E-05,0.588476014,2.38E-06,9.27E-06,5.72E-05
10614,question_answering4,226,We observe that the superiority of DECAPROP over R - NET is consistent and relatively stable .,Ablation Study,Ablation Study,question_answering,4,14,1,1,ablation-analysis,0.765858998,1,ablation-analysis,0.993202189,7.46E-07,1.17E-05,1.32E-07,1.15E-06,2.21E-06,6.77E-05,3.93E-07,1.17E-06,0.006628013,7.70E-07,1.69E-05,6.70E-05
10615,question_answering4,227,This is also observed across other datasets but not reported due to the lack of space .,Ablation Study,Ablation Study,question_answering,4,15,1,0,,0.000667282,0,negative,0.076906782,1.84E-06,0.000129837,3.54E-06,3.18E-05,5.22E-05,2.15E-05,3.33E-06,1.03E-05,0.922572972,6.76E-06,3.39E-06,0.000255774
10616,question_answering4,228,Related Work,,,question_answering,4,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
10617,question_answering4,251,Conclusion,,,question_answering,4,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
10618,sentiment_analysis40,1,title,,,sentiment_analysis,40,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
10619,sentiment_analysis40,2,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,title,,sentiment_analysis,40,1,1,1,research-problem,0.998367354,1,research-problem,2.88E-08,4.13E-06,5.98E-08,1.31E-07,4.73E-08,1.38E-07,1.62E-06,1.51E-06,5.51E-07,0.001470617,0.998520922,1.56E-07,8.50E-08
10620,sentiment_analysis40,3,abstract,,,sentiment_analysis,40,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
10621,sentiment_analysis40,4,We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment / review .,abstract,abstract,sentiment_analysis,40,1,1,0,,0.539447324,1,research-problem,1.34E-06,0.008221656,7.33E-06,3.05E-06,1.26E-05,3.03E-06,3.09E-06,4.64E-05,0.000718532,0.02778705,0.963194271,9.49E-07,7.58E-07
10622,sentiment_analysis40,5,"Our framework adopts multiple - attention mechanism to capture sentiment features separated by along distance , so that it is more robust against irrelevant information .",abstract,abstract,sentiment_analysis,40,2,1,0,,0.168446136,0,approach,5.26E-05,0.492455343,0.000185572,7.84E-05,0.000706228,0.000131156,2.11E-05,0.001554059,0.079963115,0.338880687,0.085957719,7.85E-06,6.12E-06
10623,sentiment_analysis40,6,"The results of multiple attentions are non-linearly combined with a recurrent neural network , which strengthens the expressive power of our model for handling more complications .",abstract,abstract,sentiment_analysis,40,3,1,0,,0.233901473,0,negative,0.000961054,0.048097281,6.39E-05,5.97E-05,0.000880139,4.83E-05,1.82E-05,0.000366851,0.005164001,0.915831176,0.028447098,6.01E-05,2.31E-06
10624,sentiment_analysis40,7,"The weightedmemory mechanism not only helps us avoid the labor - intensive feature engineering work , but also provides a tailor - made memory for different opinion targets of a sentence .",abstract,abstract,sentiment_analysis,40,4,1,0,,0.10571329,0,negative,7.58E-05,0.127733946,0.00016403,1.86E-05,0.000311783,4.03E-05,1.37E-05,0.000492426,0.065747631,0.641209179,0.164169615,2.01E-05,2.83E-06
10625,sentiment_analysis40,8,"We examine the merit of our model on four datasets : two are from Se-m Eval2014 , i.e. reviews of restaurants and laptops ; atwitter dataset , for testing its performance on social media data ; and a Chinese news comment dataset , for testing its language sensitivity .",abstract,abstract,sentiment_analysis,40,5,1,0,,0.003023487,0,negative,1.23E-05,0.090165776,1.22E-05,3.53E-05,0.001274244,5.91E-05,2.87E-05,0.001062011,0.00056631,0.847273093,0.059492872,1.63E-05,1.88E-06
10626,sentiment_analysis40,9,The experimental results show that our model consistently outperforms the state - of - the - art methods on different types of data .,abstract,abstract,sentiment_analysis,40,6,1,0,,0.007423357,0,negative,5.98E-05,0.001836404,1.44E-06,2.70E-05,7.67E-05,5.51E-05,0.000103767,0.000876732,3.46E-05,0.80881444,0.187753214,0.000356421,4.43E-06
10627,sentiment_analysis40,10,Introduction,,,sentiment_analysis,40,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
10628,sentiment_analysis40,11,"The goal of aspect sentiment analysis is to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .",Introduction,Introduction,sentiment_analysis,40,1,1,0,,0.774860189,1,research-problem,6.04E-07,0.000131545,2.35E-07,1.37E-05,2.27E-05,5.08E-06,8.13E-06,5.29E-06,3.45E-05,0.033789553,0.965986097,6.39E-07,1.99E-06
10629,sentiment_analysis40,12,"For example , in "" I bought a mobile phone , it s camera is wonderful but the battery life is short "" , there are three opinion targets , "" camera "" , "" battery life "" , and "" mobile phone "" .",Introduction,Introduction,sentiment_analysis,40,2,1,0,,0.002917622,0,negative,7.36E-06,0.001191912,3.43E-06,0.000154046,0.006782941,0.000184695,2.97E-05,4.92E-05,0.000433524,0.979205137,0.011951473,3.43E-06,3.20E-06
10630,sentiment_analysis40,13,"The reviewer has a positive sentiment on the "" camera "" , a negative sentiment on the * Corresponding author .",Introduction,Introduction,sentiment_analysis,40,3,1,0,,0.013692208,0,negative,4.93E-05,0.013441305,3.48E-05,3.00E-05,0.003001613,6.53E-05,2.50E-05,4.03E-05,0.012504653,0.954754984,0.016039591,1.09E-05,2.20E-06
10631,sentiment_analysis40,14,""" battery life "" , and a mixed sentiment on the "" mobile phone "" .",Introduction,Introduction,sentiment_analysis,40,4,1,0,,0.015685775,0,negative,0.000165594,0.009775499,3.95E-05,2.73E-05,0.001654324,0.000137763,7.69E-05,0.000105824,0.009401242,0.962994285,0.015566941,5.07E-05,4.08E-06
10632,sentiment_analysis40,15,Sentence - oriented sentiment analysis methods are not capable to capture such fine - grained sentiments on opinion targets .,Introduction,Introduction,sentiment_analysis,40,5,1,0,,0.044297394,0,research-problem,8.93E-07,0.00013003,2.00E-07,4.12E-06,9.93E-06,5.64E-06,1.13E-05,7.95E-06,3.19E-05,0.077902722,0.921892542,1.38E-06,1.40E-06
10633,sentiment_analysis40,16,"In order to identify the sentiment of an individual opinion target , one critical task is to model appropriate context features for the target in its original sentence .",Introduction,Introduction,sentiment_analysis,40,6,1,0,,0.327230775,0,research-problem,5.20E-06,0.003285912,1.08E-06,3.35E-05,0.000147045,2.37E-05,1.61E-05,5.10E-05,0.000840652,0.228366432,0.767221782,3.83E-06,3.78E-06
10634,sentiment_analysis40,17,"In simple cases , the sentiment of a target is identifiable with a syntactically nearby opinion word , e.g. "" wonderful "" for "" camera "" .",Introduction,Introduction,sentiment_analysis,40,7,1,0,,0.007123944,0,negative,1.14E-05,0.004582161,5.09E-06,0.000411981,0.008601215,0.000202543,2.68E-05,7.50E-05,0.001877373,0.96327423,0.020923042,4.06E-06,5.10E-06
10635,sentiment_analysis40,18,"However , there are many cases in which opinion words are enclosed in more complicated contexts .",Introduction,Introduction,sentiment_analysis,40,8,1,0,,0.014046721,0,negative,4.34E-06,0.000964694,6.91E-07,1.25E-05,0.00018231,2.56E-05,1.31E-05,2.82E-05,0.000221095,0.902127542,0.096415563,3.37E-06,8.97E-07
10636,sentiment_analysis40,19,"E.g. ,",Introduction,Introduction,sentiment_analysis,40,9,1,0,,0.009974167,0,negative,2.31E-05,0.007272954,5.41E-06,1.96E-05,0.000426189,0.000115668,2.55E-05,0.000171557,0.009007975,0.962254859,0.020667221,7.35E-06,2.63E-06
10637,sentiment_analysis40,20,""" It s camera is not wonderful enough "" might express a neutral sentiment on "" camera "" , but not negative .",Introduction,Introduction,sentiment_analysis,40,10,1,0,,0.002396551,0,negative,5.91E-06,0.002510863,2.46E-06,1.97E-05,0.00045125,0.000215168,2.98E-05,0.000183943,0.004389006,0.982876414,0.009308573,4.26E-06,2.66E-06
10638,sentiment_analysis40,21,Such complications usually hinder conventional approaches to aspect sentiment analysis .,Introduction,Introduction,sentiment_analysis,40,11,1,0,,0.412288502,0,research-problem,2.50E-06,0.000408474,5.51E-07,4.80E-05,6.76E-05,2.84E-05,2.47E-05,2.18E-05,8.83E-05,0.28601112,0.713292439,2.28E-06,3.90E-06
10639,sentiment_analysis40,22,"To model the sentiment of the above phraselike word sequence ( i.e. "" not wonderful enough "" ) , LSTM - based methods are proposed , such as target dependent LSTM ( TD - LSTM ) .",Introduction,Introduction,sentiment_analysis,40,12,1,0,,0.612063217,1,research-problem,5.31E-05,0.041549236,6.06E-05,2.42E-05,0.000524244,7.10E-05,0.000105398,0.000132269,0.025315044,0.423030132,0.509085009,3.87E-05,1.11E-05
10640,sentiment_analysis40,23,"TD - LSTM might suffer from the problem that after it captures a sentiment feature far from the target , it needs to propagate the feature word byword to the target , in which case it 's likely to lose this feature , such as the feature "" cost - effective "" for "" the phone "" in "" My over all feeling is that the phone , after using it for three months and considering its price , is really cost - effective "" .",Introduction,Introduction,sentiment_analysis,40,13,1,0,,0.011950435,0,negative,1.98E-05,0.001067092,4.15E-06,5.25E-05,0.000763681,0.000128662,4.30E-05,4.23E-05,0.000525765,0.971164274,0.026174283,1.05E-05,4.01E-06
10641,sentiment_analysis40,24,"1 Attention mechanism , which has been successfully used in machine translation , can enforce a model to pay more attention to the important part of a sentence .",Introduction,Introduction,sentiment_analysis,40,14,1,0,,0.058369627,0,research-problem,1.15E-05,0.008734652,1.38E-05,2.84E-05,9.99E-05,0.00014606,0.000118413,0.000271096,0.035232772,0.395310571,0.560003527,1.48E-05,1.46E-05
10642,sentiment_analysis40,25,There are already some works using attention in sentiment analysis to exploit this advantage .,Introduction,Introduction,sentiment_analysis,40,15,1,0,,0.089542253,0,negative,1.07E-05,0.002423013,5.39E-06,3.06E-05,0.000164714,0.000121775,0.0001184,0.000113214,0.000938315,0.603081891,0.392965233,1.61E-05,1.07E-05
10643,sentiment_analysis40,26,Another observation is that some types of sentence structures are particularly challenging for target sentiment analysis .,Introduction,Introduction,sentiment_analysis,40,16,1,0,,0.011134084,0,negative,1.14E-05,0.001999607,1.92E-06,3.51E-05,0.000171967,8.94E-05,9.15E-05,0.000125103,0.000515843,0.790816673,0.20611839,1.59E-05,7.10E-06
10644,sentiment_analysis40,27,"For example , in "" Except Patrick , all other actors do n't play well "" , the word "" except "" and the phrase "" do n't play well "" produce a positive sentiment on "" Patrick "" .",Introduction,Introduction,sentiment_analysis,40,17,1,0,,0.000855146,0,negative,6.47E-06,0.000797891,2.07E-06,9.13E-05,0.003831104,0.000279723,4.95E-05,0.000101168,0.000321754,0.993077406,0.001433174,4.85E-06,3.65E-06
10645,sentiment_analysis40,28,"It 's hard to synthesize these features just by LSTM , since their positions are dispersed .",Introduction,Introduction,sentiment_analysis,40,18,1,0,,0.004034206,0,negative,1.79E-05,0.002220989,3.56E-06,5.96E-05,0.000792661,0.00012548,3.79E-05,7.83E-05,0.000905434,0.980070281,0.015675684,8.61E-06,3.61E-06
10646,sentiment_analysis40,29,"Single attention based methods ( e.g. ) are also not capable to overcome such difficulty , because attending multiple words with one attention may hide the characteristic of each attended word .",Introduction,Introduction,sentiment_analysis,40,19,1,0,,0.002076437,0,negative,1.24E-05,0.001473107,2.87E-06,8.42E-05,0.000149783,0.000176815,8.86E-05,0.000148838,0.00065378,0.815894121,0.181292767,1.22E-05,1.05E-05
10647,sentiment_analysis40,30,"In this paper , we propose a novel framework to solve the above problems in target sentiment analysis .",Introduction,Introduction,sentiment_analysis,40,20,1,1,model,0.964106679,1,approach,0.000101131,0.712771438,0.000201176,3.31E-05,0.0022046,0.000110197,0.000289063,0.000267397,0.203655476,0.045627163,0.034629556,7.83E-05,3.14E-05
10648,sentiment_analysis40,31,"Specifically , our framework first adopts a bidirectional LSTM ( BLSTM ) to produce the memory ( i.e. the states of time steps generated by LSTM ) from the input , as bidirectional recurrent neural networks ( RNNs ) were found effective for a similar purpose in machine translation .",Introduction,Introduction,sentiment_analysis,40,21,1,1,model,0.949301153,1,model,4.18E-05,0.232615332,0.000200485,2.37E-06,0.000539736,3.44E-05,3.74E-05,4.50E-05,0.741648778,0.023989979,0.000830638,1.11E-05,2.89E-06
10649,sentiment_analysis40,32,"The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .",Introduction,Introduction,sentiment_analysis,40,22,1,1,model,0.829545121,1,model,2.64E-06,0.020808839,9.89E-06,4.53E-08,1.08E-05,4.27E-06,2.86E-06,1.16E-05,0.973850597,0.00523355,6.44E-05,3.39E-07,1.63E-07
10650,sentiment_analysis40,33,"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .",Introduction,Introduction,sentiment_analysis,40,23,1,1,model,0.948809051,1,model,1.97E-05,0.062127053,0.000115183,4.67E-07,0.000155263,1.14E-05,1.59E-05,1.45E-05,0.929901679,0.007557579,7.77E-05,2.66E-06,8.93E-07
10651,sentiment_analysis40,34,"Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .",Introduction,Introduction,sentiment_analysis,40,24,1,1,model,0.957690424,1,model,3.32E-05,0.060850799,9.53E-05,4.96E-07,0.000106457,2.81E-05,2.46E-05,4.78E-05,0.927721008,0.011038579,4.99E-05,2.54E-06,1.24E-06
10652,sentiment_analysis40,35,Our framework introduces a novel way of applying multiple - attention mechanism to synthesize important features in difficult sentence structures .,Introduction,Introduction,sentiment_analysis,40,25,1,1,model,0.899959892,1,model,3.71E-05,0.289522524,0.000123179,5.02E-06,0.000889366,3.64E-05,3.93E-05,5.87E-05,0.69174452,0.016849831,0.000679282,1.03E-05,4.44E-06
10653,sentiment_analysis40,36,"It 's sort of analogous to the cognition procedure of a person , who might first notice part of the important information at the beginning , then notices more as she reads through , and finally combines the information from multiple attentions to draw a conclusion .",Introduction,Introduction,sentiment_analysis,40,26,1,0,,0.00319064,0,negative,1.17E-05,0.009384736,2.23E-05,3.81E-05,0.001508763,0.000312324,8.06E-05,0.000153839,0.02640582,0.95880621,0.003261845,7.73E-06,6.06E-06
10654,sentiment_analysis40,37,"For the above sentence , our model may attend the word "" except "" first , and then attends the phrase "" do n't play well "" , finally combines them to generate a positive feature for "" Patrick "" .",Introduction,Introduction,sentiment_analysis,40,27,1,0,,0.013123494,0,negative,1.77E-05,0.00688247,1.23E-05,5.91E-05,0.00964007,0.000430483,0.000103295,0.000228827,0.006748212,0.97559603,0.000263049,1.24E-05,6.12E-06
10655,sentiment_analysis40,38,"also adopted the idea of multiple attentions , but they used the result of a previous attention to help the next attention attend more accurate information .",Introduction,Introduction,sentiment_analysis,40,28,1,0,,0.001395943,0,negative,2.08E-05,0.006403653,4.09E-05,9.34E-05,0.001594185,0.001224088,0.000269896,0.000533898,0.009217044,0.977190903,0.003373562,1.73E-05,2.03E-05
10656,sentiment_analysis40,39,"Their vector fed to softmax for classification is only from the final attention , which is essentially a linear combination of input embeddings ( they did not have a memory component ) .",Introduction,Introduction,sentiment_analysis,40,29,1,0,,0.012676502,0,negative,9.56E-05,0.081801335,0.000213768,1.02E-05,0.001655721,0.000459575,0.000181045,0.000469358,0.311637566,0.602940226,0.000507589,2.03E-05,7.76E-06
10657,sentiment_analysis40,40,"Thus , the above limitation of single attention based methods also holds for .",Introduction,Introduction,sentiment_analysis,40,30,1,0,,0.041207489,0,negative,1.78E-05,0.002494266,2.42E-06,1.29E-05,0.000200346,9.29E-05,4.59E-05,0.000131702,0.002740248,0.991837351,0.002410353,1.19E-05,1.91E-06
10658,sentiment_analysis40,41,"In contrast , our model combines the results of multiple attentions with a GRU network , which has different behaviors inherited from RNNs , such as forgetting , maintaining , and non-linearly transforming , and thus allows a better prediction accuracy .",Introduction,Introduction,sentiment_analysis,40,31,1,0,,0.940761063,1,model,2.53E-05,0.089741618,0.000133416,4.35E-07,0.000174141,1.37E-05,2.43E-05,1.82E-05,0.896430814,0.013345835,8.61E-05,4.96E-06,1.14E-06
10659,sentiment_analysis40,42,"We evaluate our approach on four datasets : the first two come from SemEval 2014 , containing reviews of restaurant domain and laptop domain ; the third one is a collection of tweets , collected by ; to examine whether our framework is language - insensitive ( since languages show differences in quite a few aspects in expressing sentiments ) , we prepared a dataset of Chinese news comments with people mentions as opinion targets .",Introduction,Introduction,sentiment_analysis,40,32,1,0,,0.167752346,0,approach,4.89E-05,0.532109489,0.000149853,0.000174006,0.294232784,0.00057826,0.000997116,0.000615506,0.007941367,0.162824328,0.000194856,9.57E-05,3.79E-05
10660,sentiment_analysis40,43,"The experimental results show that our model performs well for different types of data , and consistently outperforms the state - of - the - art methods .",Introduction,Introduction,sentiment_analysis,40,33,1,0,,0.021193183,0,negative,0.000613721,0.030201328,1.65E-05,5.81E-05,0.002429901,0.000656711,0.014071295,0.002059271,0.001974337,0.939583711,0.001988949,0.006188538,0.000157669
10661,sentiment_analysis40,44,Related Work,,,sentiment_analysis,40,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
10662,sentiment_analysis40,66,Our Model,,,sentiment_analysis,40,0,1,0,,0.003255137,0,negative,0.001374583,0.001921772,0.00034179,0.003136191,0.000112633,0.003672555,0.001461379,0.00906347,0.00912276,0.950209697,0.018500831,0.000480404,0.000601934
10663,sentiment_analysis40,67,"The architecture of our model is shown in 1 , which consists of five modules : input module , memory module , position - weighted memory module , recurrent attention module , and output module .",Our Model,Our Model,sentiment_analysis,40,1,1,0,,0.116901335,0,negative,5.62E-05,0.023356377,0.00026052,5.93E-05,0.000338307,0.001078648,0.00010789,0.001517025,0.454339932,0.516368252,0.002486431,1.08E-05,2.03E-05
10664,sentiment_analysis40,68,"Suppose the input sentence is s = {s 1 , . . . , s ? ? 1 , s ? , s ? + 1 , . . . , s T } , the goal of our model is to predict the sentiment polarity of the target s ? .",Our Model,Our Model,sentiment_analysis,40,2,1,0,,0.001840426,0,negative,4.57E-06,0.007348525,1.19E-05,4.42E-06,6.72E-05,0.000240701,1.82E-05,0.000946607,0.020767704,0.964767758,0.005817044,3.59E-06,1.72E-06
10665,sentiment_analysis40,69,"For simplicity , we notate a target as one word here , where necessary , we will elaborate how to handle phrase - form targets , e.g. "" battery life "" .",Our Model,Our Model,sentiment_analysis,40,3,1,0,,0.000229907,0,negative,5.24E-06,0.000613058,3.38E-06,1.03E-05,0.000124397,0.000284792,4.76E-06,0.000210795,0.001764075,0.996945631,3.21E-05,1.21E-06,2.38E-07
10666,sentiment_analysis40,70,Input Embedding,Our Model,,sentiment_analysis,40,4,1,0,,0.002825633,0,negative,3.51E-05,0.009631432,0.000196931,5.68E-06,3.97E-05,0.000915407,0.000109308,0.003020869,0.381180553,0.601204658,0.003631002,1.60E-05,1.34E-05
10667,sentiment_analysis40,71,Let L ?,Our Model,Input Embedding,sentiment_analysis,40,5,1,0,,7.29E-06,0,negative,3.36E-06,1.17E-05,1.79E-06,1.08E-07,1.95E-07,2.26E-05,7.75E-07,0.000842958,3.25E-05,0.999063721,8.18E-06,1.20E-05,1.35E-07
10668,sentiment_analysis40,72,"R d|V | bean embedding lookup table generated by an unsupervised method such as GloVe or CBOW , where d is the dimension of word vectors and | V | is the vocabulary size .",Our Model,Input Embedding,sentiment_analysis,40,6,1,0,,3.28E-05,0,negative,2.51E-05,0.000294586,7.72E-05,3.62E-07,2.54E-06,1.92E-05,2.17E-06,0.000903343,0.000219215,0.998342295,4.10E-05,7.25E-05,5.29E-07
10669,sentiment_analysis40,73,"The input module retrieves the word vectors from L for an input sequence and gets a list of vectors {v 1 , . . . , v t , . . . , v T } where v t ?",Our Model,Input Embedding,sentiment_analysis,40,7,1,0,,0.000113361,0,negative,1.64E-05,0.000139612,9.40E-05,6.41E-08,8.79E-07,1.00E-05,1.15E-06,0.000307714,0.000256178,0.999070506,1.69E-05,8.63E-05,1.96E-07
10670,sentiment_analysis40,74,Rd .,Our Model,,sentiment_analysis,40,8,1,0,,0.002430934,0,negative,2.28E-05,0.000319941,9.73E-06,4.48E-06,3.59E-05,0.00025529,1.82E-05,0.000247555,0.002111707,0.996740589,0.000227742,5.46E-06,6.01E-07
10671,sentiment_analysis40,75,L mayor may not be tuned in the training of our framework .,Our Model,Rd .,sentiment_analysis,40,9,1,0,,3.05E-06,0,negative,2.89E-05,8.49E-06,6.68E-06,1.16E-06,1.03E-06,0.000510184,9.73E-06,0.002827355,3.89E-05,0.996519466,1.62E-06,4.52E-05,1.30E-06
10672,sentiment_analysis40,76,"If it is not tuned , the model can utilize the words ' similarity revealed in the original embedding space .",Our Model,Rd .,sentiment_analysis,40,10,1,0,,3.11E-05,0,negative,7.13E-05,1.83E-05,2.11E-05,1.95E-07,6.93E-07,9.47E-05,5.52E-06,0.000728651,0.000133381,0.998823874,1.14E-06,0.000100541,5.85E-07
10673,sentiment_analysis40,77,"If it is tuned , we expect the model would capture some intrinsic information that is useful for the sentiment analysis task .",Our Model,Rd .,sentiment_analysis,40,11,1,0,,7.05E-06,0,negative,3.62E-05,1.75E-05,6.44E-06,2.93E-07,7.43E-07,0.000213623,7.14E-06,0.002128623,8.42E-05,0.997438758,8.03E-07,6.50E-05,7.26E-07
10674,sentiment_analysis40,78,BLSTM for Memory Building,Our Model,,sentiment_analysis,40,12,1,0,,0.413052813,0,negative,3.32E-05,0.010875949,0.000374676,2.80E-06,1.86E-05,0.000334017,0.000550652,0.000754926,0.322900703,0.446691227,0.217313319,0.000106459,4.34E-05
10675,sentiment_analysis40,79,"Mem Net simply used the sequence of word vectors as memory , which can not synthesize phrase - like features in the original sentence .",Our Model,BLSTM for Memory Building,sentiment_analysis,40,13,1,0,,4.91E-05,0,negative,9.31E-06,0.000121168,0.000227534,5.40E-07,2.65E-06,0.000116077,3.39E-05,0.000190032,0.003676825,0.99543133,0.000175689,1.31E-05,1.91E-06
10676,sentiment_analysis40,80,It is straightforward to achieve the goal with the models of RNN family .,Our Model,BLSTM for Memory Building,sentiment_analysis,40,14,1,0,,4.49E-06,0,negative,4.39E-06,8.62E-06,1.37E-06,4.29E-08,1.32E-07,8.09E-06,1.84E-06,2.45E-05,0.000133882,0.999771991,3.46E-05,1.04E-05,6.33E-08
10677,sentiment_analysis40,81,"In this paper , we use Deep Bidirectional LSTM ( DBLSTM ) to build the memory which records all information to be read in the subsequent modules .",Our Model,BLSTM for Memory Building,sentiment_analysis,40,15,1,0,,0.01791509,0,negative,9.78E-05,0.02005273,0.009329691,1.98E-06,3.34E-05,0.000257803,0.000260477,0.000627285,0.312239924,0.65618446,0.000805509,9.75E-05,1.14E-05
10678,sentiment_analysis40,82,"At each time step t , the forward LSTM not only outputs the hidden state ? ?",Our Model,BLSTM for Memory Building,sentiment_analysis,40,16,1,0,,3.26E-06,0,negative,5.46E-06,2.16E-05,7.69E-06,2.23E-08,2.73E-07,4.18E-06,1.46E-06,1.71E-05,0.000509295,0.999424199,2.39E-06,6.26E-06,3.19E-08
10679,sentiment_analysis40,83,h l tat it s layer l ( ? ? h 0 t = v t ) but also maintains a memory ? ?,Our Model,BLSTM for Memory Building,sentiment_analysis,40,17,1,0,,2.20E-06,0,negative,7.03E-05,2.13E-05,5.54E-05,5.91E-08,1.28E-06,1.51E-05,1.00E-05,2.37E-05,0.000332605,0.99940528,2.83E-06,6.21E-05,1.19E-07
10680,sentiment_analysis40,84,cl t inside it s hidden cell .,Our Model,BLSTM for Memory Building,sentiment_analysis,40,18,1,0,,2.76E-06,0,negative,1.45E-05,5.23E-06,1.22E-05,2.96E-08,5.16E-07,5.72E-06,2.03E-06,1.02E-05,0.000142804,0.999794991,6.75E-07,1.10E-05,3.94E-08
10681,sentiment_analysis40,85,The update process at time t is as follows :,Our Model,BLSTM for Memory Building,sentiment_analysis,40,19,1,0,,1.43E-06,0,negative,2.05E-06,6.38E-05,6.53E-06,1.87E-08,1.69E-07,7.28E-06,2.06E-06,4.17E-05,0.003037529,0.996824902,1.09E-05,2.92E-06,6.53E-08
10682,sentiment_analysis40,86,where ?,Our Model,BLSTM for Memory Building,sentiment_analysis,40,20,1,0,,1.05E-07,0,negative,5.83E-07,2.94E-06,3.33E-07,3.79E-08,8.18E-08,8.88E-06,8.02E-07,2.14E-05,7.42E-05,0.999888325,1.46E-06,8.78E-07,2.28E-08
10683,sentiment_analysis40,87,and tanh are sigmoid and hyperbolic tan -,Our Model,BLSTM for Memory Building,sentiment_analysis,40,21,1,0,,0.000438246,0,negative,2.55E-05,0.000290768,0.000538436,8.66E-07,4.96E-06,0.000259057,7.16E-05,0.000504711,0.014430215,0.983834965,2.24E-05,1.43E-05,2.16E-06
10684,sentiment_analysis40,88,l is the number of hidden cells at the layer l of the forward LSTM .,Our Model,BLSTM for Memory Building,sentiment_analysis,40,22,1,0,,1.79E-06,0,negative,2.13E-06,3.24E-05,1.22E-06,7.02E-08,3.62E-07,3.75E-05,4.32E-06,0.000268878,0.00058394,0.999064972,1.64E-06,2.42E-06,1.03E-07
10685,sentiment_analysis40,89,"The gates i , f , o ? R ? ?",Our Model,BLSTM for Memory Building,sentiment_analysis,40,23,1,0,,9.52E-07,0,negative,3.25E-06,4.08E-06,1.45E-06,4.99E-08,5.18E-07,1.30E-05,1.97E-06,2.44E-05,8.98E-05,0.999857736,1.90E-07,3.56E-06,2.39E-08
10686,sentiment_analysis40,90,"d l simulate binary switches that control whether to update the information from the current input , whether to forget the information in the memory cells , and whether to reveal the information in memory cells to the output , respectively .",Our Model,BLSTM for Memory Building,sentiment_analysis,40,24,1,0,,2.57E-06,0,negative,6.19E-06,1.68E-05,3.54E-05,3.50E-08,5.14E-07,1.16E-05,3.41E-06,2.78E-05,0.001009467,0.998884411,4.47E-07,3.94E-06,6.17E-08
10687,sentiment_analysis40,91,"The backward LSTM does the same thing , except that its input sequence is reversed .",Our Model,BLSTM for Memory Building,sentiment_analysis,40,25,1,0,,4.37E-07,0,negative,1.53E-05,3.10E-05,0.000481659,7.77E-09,1.97E-07,8.98E-06,9.40E-06,1.51E-05,0.002693584,0.99672644,2.01E-06,1.62E-05,1.02E-07
10688,sentiment_analysis40,92,"If there are L layers stacked in the BLSTM , the final memory generated in this",Our Model,BLSTM for Memory Building,sentiment_analysis,40,26,1,0,,3.21E-06,0,negative,3.29E-06,2.40E-05,1.96E-05,5.13E-09,1.73E-07,2.79E-06,1.54E-06,1.15E-05,0.001402489,0.998528395,1.05E-06,5.17E-06,3.61E-08
10689,sentiment_analysis40,93,"In our framework , we use 2 layers of BLSTM to build the memory , as it generally performs well in NLP tasks .",Our Model,BLSTM for Memory Building,sentiment_analysis,40,27,1,0,,0.001864086,0,negative,6.94E-05,0.002845123,7.06E-05,8.19E-06,3.11E-05,0.003178161,0.000509104,0.018083625,0.015795621,0.959340562,1.48E-05,3.65E-05,1.72E-05
10690,sentiment_analysis40,94,Position - Weighted Memory,Our Model,,sentiment_analysis,40,28,1,0,,0.032588895,0,model,2.49E-05,0.005077095,0.00099602,5.81E-07,1.69E-05,0.000122043,0.000158009,0.000160937,0.837181387,0.154478217,0.001740829,3.32E-05,9.88E-06
10691,sentiment_analysis40,95,"The memory generated in the above module is the same for multiple targets in one comment , which is not flexible enough for predicting respective sentiments of these targets .",Our Model,Position - Weighted Memory,sentiment_analysis,40,29,1,0,,2.92E-06,0,negative,4.42E-06,8.03E-05,1.53E-05,1.00E-08,1.83E-07,8.17E-07,5.56E-07,2.71E-05,0.003335715,0.996522998,1.09E-06,1.15E-05,7.46E-08
10692,sentiment_analysis40,96,"To ease this problem , we adopt an intuitive method to edit the memory to produce a tailor - made input memory for each target .",Our Model,Position - Weighted Memory,sentiment_analysis,40,30,1,0,,9.90E-06,0,negative,0.000100374,0.002208454,9.73E-05,4.48E-07,1.26E-05,3.62E-06,4.33E-06,9.84E-05,0.007041986,0.990235516,3.33E-06,0.000192967,7.18E-07
10693,sentiment_analysis40,97,"Specifically , the closer to the target a word is , the higher its memory slide is weighted .",Our Model,Position - Weighted Memory,sentiment_analysis,40,31,1,0,,9.42E-07,0,negative,4.69E-06,1.75E-05,3.39E-06,1.94E-08,2.07E-07,5.36E-07,1.96E-07,1.54E-05,0.000847359,0.99910457,7.87E-08,6.05E-06,2.99E-08
10694,sentiment_analysis40,98,We define the distance as the number of words between the word and the target .,Our Model,Position - Weighted Memory,sentiment_analysis,40,32,1,0,,1.83E-07,0,negative,7.51E-07,1.72E-05,1.93E-06,2.05E-08,1.72E-07,3.31E-06,7.59E-07,8.48E-05,0.000438691,0.999449015,2.11E-07,3.05E-06,7.19E-08
10695,sentiment_analysis40,99,"One might want to use the length of the path from the specific word to the target in the dependency tree as the distance , which is a worthwhile option to try in the future work , given the condition that dependency parsing on the input text is effective enough .",Our Model,Position - Weighted Memory,sentiment_analysis,40,33,1,0,,7.70E-07,0,negative,3.48E-06,1.21E-05,8.73E-07,4.64E-08,1.13E-07,3.79E-06,1.38E-06,8.53E-05,0.000207653,0.99966267,8.25E-07,2.16E-05,1.43E-07
10696,sentiment_analysis40,100,"Precisely , the weight for the word at position t is calculated as :",Our Model,Position - Weighted Memory,sentiment_analysis,40,34,1,0,,8.80E-08,0,negative,1.79E-06,6.18E-06,2.39E-06,4.92E-09,6.65E-08,4.11E-07,1.62E-07,8.93E-06,0.000253569,0.999721631,8.20E-08,4.76E-06,1.80E-08
10697,sentiment_analysis40,101,where t max is truncation length of the input .,Our Model,Position - Weighted Memory,sentiment_analysis,40,35,1,0,,1.80E-07,0,negative,5.42E-06,1.15E-05,1.13E-06,8.42E-08,2.91E-07,2.73E-06,1.27E-06,0.000107901,0.000160562,0.999689811,2.88E-07,1.88E-05,2.30E-07
10698,sentiment_analysis40,102,We also calculate u t = t??,Our Model,Position - Weighted Memory,sentiment_analysis,40,36,1,0,,2.08E-07,0,negative,1.16E-05,1.05E-05,3.52E-06,3.85E-08,6.86E-07,2.25E-06,1.70E-06,2.84E-05,9.52E-05,0.999797815,1.09E-07,4.81E-05,8.50E-08
10699,sentiment_analysis40,103,tmax to memorize the relative offset between each word and the target .,Our Model,Position - Weighted Memory,sentiment_analysis,40,37,1,0,,1.71E-06,0,negative,9.60E-06,2.62E-05,5.73E-05,1.87E-08,3.35E-07,1.01E-06,1.19E-06,2.48E-05,0.001158312,0.998694821,2.24E-07,2.59E-05,2.27E-07
10700,sentiment_analysis40,104,"If the target is a phrase , the distance ( i.e. t ? ? ) is calculated with it s left or right boundary index according to which side wt locates .",Our Model,Position - Weighted Memory,sentiment_analysis,40,38,1,0,,7.76E-08,0,negative,4.19E-06,2.12E-05,9.56E-06,2.29E-09,1.28E-07,3.13E-07,3.03E-07,7.76E-06,0.000308561,0.999635809,5.71E-08,1.21E-05,1.62E-08
10701,sentiment_analysis40,105,The final position - weighted memory of a target is,Our Model,Position - Weighted Memory,sentiment_analysis,40,39,1,0,,1.65E-07,0,negative,6.17E-06,2.11E-05,1.40E-05,8.03E-09,2.04E-07,5.54E-07,6.47E-07,1.19E-05,0.00093922,0.998978429,2.98E-07,2.73E-05,9.78E-08
10702,sentiment_analysis40,106,"The weighted memory is designed to up - weight nearer sentiment words , and the recurrent attention module , discussed below , attends long - distance sentiment words .",Our Model,Position - Weighted Memory,sentiment_analysis,40,40,1,0,,1.71E-05,0,negative,3.22E-05,0.000571825,0.000265928,2.50E-07,3.53E-06,5.95E-06,5.39E-06,0.000116606,0.067360142,0.931610724,1.47E-06,2.34E-05,2.57E-06
10703,sentiment_analysis40,107,"Thus , they work together to expect a better prediction accuracy .",Our Model,Position - Weighted Memory,sentiment_analysis,40,41,1,0,,3.52E-07,0,negative,3.20E-06,1.25E-05,1.60E-06,4.01E-08,2.31E-07,8.88E-07,4.46E-07,2.16E-05,0.000643924,0.999304612,1.09E-07,1.07E-05,1.30E-07
10704,sentiment_analysis40,108,Recurrent Attention on Memory,Our Model,,sentiment_analysis,40,42,1,0,,0.01922982,0,negative,7.41E-05,0.008921074,0.000435227,4.24E-06,8.25E-05,0.000747081,0.00367224,0.00124084,0.222103181,0.749731253,0.012272515,0.000568415,0.000147328
10705,sentiment_analysis40,109,"To accurately predict the sentiment of a target , it is essential to : ( 1 ) correctly distill the related information from its position - weighted memory ; and ( 2 ) appropriately manufacture such information as the input of sentiment classification .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,43,1,0,,3.98E-05,0,negative,1.81E-05,0.000136277,5.20E-06,1.07E-06,3.03E-06,1.29E-05,2.07E-05,0.000129813,0.000380414,0.99891516,8.20E-05,0.000292044,3.28E-06
10706,sentiment_analysis40,110,"We employ multiple attentions to fulfil the first aspect , and a recurrent network for the second which nonlinearly combines the attention results with GRUs ( since GRUs have less number of parameters ) .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,44,1,0,,0.00012165,0,negative,9.67E-05,0.002091274,0.000762939,2.64E-07,1.17E-05,1.90E-05,7.15E-05,0.000100707,0.042112063,0.954567835,2.00E-06,0.000160643,3.37E-06
10707,sentiment_analysis40,111,"For example , "" except "" and "" do n't play well "" in "" Except Patrick , all other actors do n't play well "" are attended by different attentions , and combined to produce a positive sentiment on "" Patrick "" .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,45,1,0,,2.41E-07,0,negative,6.61E-07,4.96E-07,5.08E-07,2.23E-08,3.53E-07,2.12E-06,9.09E-07,6.20E-06,9.00E-06,0.999972546,8.25E-09,7.14E-06,2.54E-08
10708,sentiment_analysis40,112,"Particularly , we employ a GRU to update the episode e after each attention .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,46,1,0,,4.67E-05,0,negative,4.69E-05,0.000468613,0.000218424,6.02E-08,3.05E-06,5.84E-06,1.43E-05,4.01E-05,0.014330573,0.984836668,2.96E-07,3.46E-05,6.43E-07
10709,sentiment_analysis40,113,"Let e t?1 denote the episode at the previous time and i AL t is the current information attended from the memory M , and the process of updating e t is as follows :",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,47,1,0,,7.32E-08,0,negative,6.37E-07,4.02E-06,1.67E-06,5.35E-09,8.49E-08,9.50E-07,8.19E-07,8.10E-06,0.000151497,0.999828596,5.80E-08,3.53E-06,3.15E-08
10710,sentiment_analysis40,114,", W x ? R HH , and H is the hidden size of GRU .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,48,1,0,,2.68E-07,0,negative,3.66E-06,4.20E-06,1.25E-06,1.58E-08,3.94E-07,3.66E-06,4.04E-06,3.93E-05,2.28E-05,0.999890286,2.83E-08,3.03E-05,6.11E-08
10711,sentiment_analysis40,115,"As we can see from Equations and , the state of episode e t is the interpolation of e t?1 and the candidate hidden vector ?",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,49,1,0,,1.64E-07,0,negative,5.46E-06,4.43E-06,1.93E-06,1.18E-08,3.45E-07,1.44E-06,2.58E-06,1.03E-05,3.11E-05,0.999895786,4.04E-08,4.66E-05,4.53E-08
10712,sentiment_analysis40,116,t .,Our Model,Recurrent Attention on Memory,sentiment_analysis,40,50,1,0,,2.88E-07,0,negative,1.08E-06,5.68E-07,2.17E-07,1.14E-08,6.87E-08,1.75E-06,9.56E-07,1.22E-05,1.12E-05,0.999964907,2.15E-08,7.02E-06,3.76E-08
10713,sentiment_analysis40,117,A vector of 0's is used as e 0 .,Our Model,Recurrent Attention on Memory,sentiment_analysis,40,51,1,0,,1.66E-07,0,negative,5.75E-07,4.26E-06,5.27E-07,7.42E-09,1.01E-07,1.67E-05,5.72E-06,0.000231436,8.52E-05,0.999651956,1.31E-08,3.47E-06,6.71E-08
10714,sentiment_analysis40,118,"For calculating the attended information i AL tat t , the input of an attention layer ( AL for short ) includes the memory slices m j ( 1 ? j ? T ) and the previous episode e t?1 .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,52,1,0,,2.99E-07,0,negative,3.91E-06,1.89E-05,8.67E-06,2.47E-08,7.05E-07,2.11E-06,1.95E-06,1.59E-05,0.000693883,0.99924533,6.65E-08,8.31E-06,1.60E-07
10715,sentiment_analysis40,119,We first calculate the attention score of each memory slice as follows :,Our Model,Recurrent Attention on Memory,sentiment_analysis,40,53,1,0,,1.04E-06,0,negative,2.68E-06,8.08E-06,4.37E-06,1.70E-09,1.06E-07,6.69E-07,1.16E-06,7.02E-06,0.000237901,0.999729269,1.93E-08,8.68E-06,2.91E-08
10716,sentiment_analysis40,120,"where [ , v ? ] indicates when the attention result relies on particular aspects such as those of products , we also add the target vector v ?",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,54,1,0,,1.43E-07,0,negative,4.41E-07,1.26E-06,5.91E-07,1.48E-09,4.89E-08,7.80E-07,4.91E-07,5.96E-06,3.30E-05,0.999955264,5.09E-09,2.11E-06,9.69E-09
10717,sentiment_analysis40,121,"because different product aspects have different preference on opinion words ; when the target is a person , there is no need to do so .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,55,1,0,,1.53E-07,0,negative,3.08E-06,7.12E-07,3.52E-07,1.02E-08,2.02E-07,4.39E-07,4.02E-07,3.77E-06,3.97E-06,0.999964173,9.16E-09,2.29E-05,1.59E-08
10718,sentiment_analysis40,122,"If the target is a phrase , v ? takes the average of word embeddings .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,56,1,0,,2.63E-07,0,negative,1.83E-06,5.94E-06,7.28E-06,2.06E-09,1.20E-07,2.16E-06,3.58E-06,1.99E-05,8.98E-05,0.999862769,5.80E-09,6.59E-06,4.00E-08
10719,sentiment_analysis40,123,"We utilize the previous episode for the current attention , since it can guide the model to attend different useful information . ) also adopts multiple attentions , but they do n't combine the results of different attentions .",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,57,1,0,,1.61E-06,0,negative,7.93E-06,1.12E-05,8.75E-05,7.42E-08,1.83E-06,5.85E-06,1.16E-05,1.73E-05,9.39E-05,0.999699542,9.73E-08,6.25E-05,6.24E-07
10720,sentiment_analysis40,124,Then we calculate the normalized attention score of each memory slice as :,Our Model,Recurrent Attention on Memory,sentiment_analysis,40,58,1,0,,3.21E-07,0,negative,2.78E-06,2.33E-06,3.28E-06,2.37E-09,1.41E-07,4.98E-07,8.07E-07,3.56E-06,5.37E-05,0.999921066,6.16E-09,1.18E-05,1.96E-08
10721,sentiment_analysis40,125,.,Our Model,Recurrent Attention on Memory,sentiment_analysis,40,59,1,0,,7.19E-08,0,negative,1.57E-07,1.81E-07,5.60E-08,1.20E-09,1.17E-08,3.69E-07,1.93E-07,3.33E-06,1.00E-05,0.999984468,2.92E-09,1.21E-06,7.06E-09
10722,sentiment_analysis40,126,"Finally , the inputs to a GRU ( i.e. Eqs. 8 to 11 ) at time tare the episode e t?1 at time t ?",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,60,1,0,,2.06E-07,0,negative,9.58E-06,6.56E-06,8.75E-06,2.30E-09,3.18E-07,6.45E-07,3.41E-06,4.46E-06,7.39E-05,0.999839626,6.55E-09,5.27E-05,3.19E-08
10723,sentiment_analysis40,127,"1 and the content i AL t , which is read from the memory as :",Our Model,Recurrent Attention on Memory,sentiment_analysis,40,61,1,0,,1.34E-07,0,negative,2.87E-06,2.29E-06,5.13E-06,5.96E-09,2.75E-07,1.30E-06,1.58E-06,7.43E-06,4.62E-05,0.999916675,4.88E-09,1.62E-05,4.24E-08
10724,sentiment_analysis40,128,Output and Model Training,,,sentiment_analysis,40,0,1,0,,0.089246631,0,negative,0.000125044,0.000119663,0.000111328,5.23E-06,5.82E-06,0.000159777,0.00017998,0.001407082,0.000148102,0.995240679,0.001816457,0.000667506,1.33E-05
10725,sentiment_analysis40,129,"After N -time attentions on the memory , the final episode e N serves as the feature and is fed into a softmax layer to predict the target sentiment .",Output and Model Training,Output and Model Training,sentiment_analysis,40,1,1,0,,7.86E-05,0,negative,0.000664294,2.42E-05,0.000941905,8.75E-08,5.07E-07,2.81E-05,1.86E-05,0.000126792,0.000120074,0.997977547,4.17E-06,9.19E-05,1.81E-06
10726,sentiment_analysis40,130,The model is trained by minimizing the cross entropy plus an L 2 regularization term :,Output and Model Training,Output and Model Training,sentiment_analysis,40,2,1,0,,0.000800826,0,negative,0.000394408,0.000180529,0.000833874,4.10E-07,7.21E-07,0.000638505,7.70E-05,0.006201649,0.001047359,0.990578531,1.05E-05,2.97E-05,6.83E-06
10727,sentiment_analysis40,131,"where C is the sentiment category set , Dis the collection of training data , y ?",Output and Model Training,Output and Model Training,sentiment_analysis,40,3,1,0,,6.14E-06,0,negative,4.77E-05,3.35E-06,3.26E-05,1.85E-08,1.19E-07,1.92E-05,8.78E-06,0.000105156,4.95E-06,0.999744888,2.08E-06,3.08E-05,3.48E-07
10728,sentiment_analysis40,132,"R | C| is a one - hot vector where the element for the true sentiment is 1 , f ( x ; ? ) is the predicted sentiment distribution of the model , ? is the weight of L 2 regularization term .",Output and Model Training,Output and Model Training,sentiment_analysis,40,4,1,0,,6.15E-05,0,negative,9.52E-05,3.28E-05,6.23E-05,3.24E-07,5.35E-07,0.000866216,7.12E-05,0.008688755,0.000102449,0.990054357,4.20E-06,1.68E-05,4.81E-06
10729,sentiment_analysis40,133,We also adopt dropout and early stopping to ease overfitting .,Output and Model Training,Output and Model Training,sentiment_analysis,40,5,1,0,,0.248656804,0,negative,0.010409423,0.005092003,0.003686403,7.81E-05,6.43E-05,0.016606207,0.00391167,0.125272915,0.010157079,0.823975512,3.41E-05,0.000292113,0.00042028
10730,sentiment_analysis40,134,Experiments,,,sentiment_analysis,40,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
10731,sentiment_analysis40,135,Experimental Setting,,,sentiment_analysis,40,0,1,0,,0.000550602,0,negative,6.94E-06,0.000223775,5.37E-06,1.49E-06,4.59E-07,0.00036809,4.68E-05,0.006768399,0.000329329,0.987130295,0.005063478,5.12E-05,4.36E-06
10732,sentiment_analysis40,136,"We conduct experiments on four datasets , as shown in .",Experimental Setting,Experimental Setting,sentiment_analysis,40,1,1,0,,0.197679459,0,negative,8.45E-05,0.000615954,0.000385237,9.47E-07,9.83E-06,0.000146606,0.000111777,0.010007759,1.72E-05,0.985741531,0.000190749,0.002685551,2.33E-06
10733,sentiment_analysis40,137,"The first two are from Se -m Eval 2014 , containing reviews of restaurant and laptop domains , which are widely used in previous works .",Experimental Setting,Experimental Setting,sentiment_analysis,40,2,1,0,,0.081458413,0,negative,8.43E-05,0.000355457,0.000409206,0.002960577,0.003572932,0.001139976,0.000342261,0.004015361,3.27E-05,0.986749089,0.000238446,8.14E-05,1.84E-05
10734,sentiment_analysis40,138,"The third one is a collection of tweets , collected by .",Experimental Setting,Experimental Setting,sentiment_analysis,40,3,1,0,,0.151105129,0,negative,0.000259159,0.002065459,0.007095721,0.002200377,0.005632645,0.002625555,0.001020003,0.01132584,0.000216033,0.967117488,0.000206834,0.000188921,4.60E-05
10735,sentiment_analysis40,139,"The last one is prepared by us for testing the language sensitivity of our model , which contains Chinese news comments and has politicians and entertainers as opinion targets .",Experimental Setting,Experimental Setting,sentiment_analysis,40,4,1,0,,0.151823535,0,negative,5.92E-05,0.000913399,0.000981096,0.000272072,0.001637929,0.00083288,0.000445133,0.01008458,5.39E-05,0.984433965,0.000129731,0.000143791,1.23E-05
10736,sentiment_analysis40,140,"We purposely add more negation , contrastive , and question comments to make it more challenging .",Experimental Setting,Experimental Setting,sentiment_analysis,40,5,1,0,,0.443908213,0,negative,0.003309213,0.003492949,0.002527748,0.000959448,0.001548807,0.002217752,0.000379125,0.026840984,0.000950348,0.956663195,8.27E-05,0.000974326,5.34E-05
10737,sentiment_analysis40,141,"Each comment is annotated by at least two annotators , and only if they agree with each other , the comment will be added into our dataset .",Experimental Setting,Experimental Setting,sentiment_analysis,40,6,1,0,,0.243273708,0,negative,2.38E-05,0.001002046,0.000691336,2.44E-05,0.000283077,0.000415402,5.54E-05,0.014842314,0.000155082,0.982420555,2.43E-05,5.77E-05,4.58E-06
10738,sentiment_analysis40,142,"Moreover , we replace each opinion target ( i.e. word / phrase of pronoun or person name ) with a placeholder , as did in .",Experimental Setting,Experimental Setting,sentiment_analysis,40,7,1,0,,0.220431585,0,negative,0.000375904,0.007180123,0.008115866,1.67E-05,6.68E-05,0.001245963,0.000128556,0.074735747,0.00454403,0.903310964,6.00E-05,0.00020597,1.34E-05
10739,sentiment_analysis40,143,"For the first two datasets , we removed a few examples having the "" conflict label "" , e.g. , "" Certainly not the best sushi in New York , however , it is always fresh "" .",Experimental Setting,Experimental Setting,sentiment_analysis,40,8,1,0,,0.093492939,0,negative,0.000205171,0.000441939,0.000169693,0.000152781,0.000653819,0.002003044,0.000127536,0.027440762,5.60E-05,0.968592606,1.65E-05,0.000133218,7.00E-06
10740,sentiment_analysis40,144,"We use 300 - dimension word vectors pre-trained by GloVe ( whose vocabulary size is 1.9M 2 ) for our experiments on the English datasets , as previous works did .",Experimental Setting,Experimental Setting,sentiment_analysis,40,9,1,0,,0.993340151,1,hyperparameters,1.48E-06,0.000337733,1.11E-05,1.69E-06,8.77E-07,0.006322436,0.000114149,0.970513842,5.80E-05,0.022613086,1.49E-05,5.76E-06,4.89E-06
10741,sentiment_analysis40,145,"Some works employed domain - specific training corpus to learn embeddings for better performance , such as TD - LSTM on the tweet dataset .",Experimental Setting,Experimental Setting,sentiment_analysis,40,10,1,0,,0.06683346,0,negative,1.20E-05,0.000547161,0.000121307,1.34E-05,7.46E-06,0.000788516,0.000236757,0.084173871,5.34E-05,0.903943277,0.009870097,0.000210806,2.20E-05
10742,sentiment_analysis40,146,"In contrast , we prefer to use",Experimental Setting,Experimental Setting,sentiment_analysis,40,11,1,0,,0.024957575,0,negative,0.000403615,0.001186815,0.005725788,7.37E-06,3.73E-05,0.000132798,5.85E-05,0.005680569,0.000188009,0.98545851,8.70E-05,0.001028927,4.78E-06
10743,sentiment_analysis40,147,Compared Methods,,,sentiment_analysis,40,0,1,0,,0.003363972,0,negative,1.91E-05,0.000148937,7.51E-05,4.41E-07,6.71E-07,0.00022496,0.000164342,0.00299226,8.80E-05,0.993920181,0.001856336,0.00050613,3.53E-06
10744,sentiment_analysis40,148,We compare our proposed framework of Recurrent Attention on Memory ( RAM ) with the following methods :,Compared Methods,Compared Methods,sentiment_analysis,40,1,1,0,,0.924729101,1,baselines,6.25E-06,2.58E-06,0.98633375,3.84E-08,4.28E-08,8.34E-06,9.36E-05,6.19E-05,5.25E-07,0.013282251,6.48E-06,0.000203238,1.10E-06
10745,sentiment_analysis40,149,Average Context :,Compared Methods,Compared Methods,sentiment_analysis,40,2,1,1,baselines,0.990536646,1,baselines,2.19E-05,2.13E-06,0.976681864,2.61E-08,2.11E-08,7.69E-06,0.00019615,0.000108851,5.49E-07,0.017214753,3.15E-05,0.00573077,3.78E-06
10746,sentiment_analysis40,150,There are two versions of this method .,Compared Methods,Compared Methods,sentiment_analysis,40,3,1,1,baselines,0.051312027,0,baselines,1.96E-05,5.08E-06,0.855565586,9.56E-07,2.61E-07,6.41E-05,0.000178167,0.000470681,8.01E-06,0.143458082,5.77E-05,0.000157569,1.42E-05
10747,sentiment_analysis40,151,"The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .",Compared Methods,Compared Methods,sentiment_analysis,40,4,1,1,baselines,0.997581995,1,baselines,6.64E-06,1.37E-06,0.996716088,2.52E-08,3.39E-08,2.52E-06,3.16E-05,1.91E-05,6.69E-07,0.003123745,1.53E-06,9.57E-05,9.31E-07
10748,sentiment_analysis40,152,"The second one , named AC , averages the word vectors of the full context .",Compared Methods,Compared Methods,sentiment_analysis,40,5,1,1,baselines,0.990664021,1,baselines,6.92E-06,1.36E-06,0.996054259,1.75E-08,2.66E-08,1.72E-06,1.63E-05,1.54E-05,9.81E-07,0.003844243,1.15E-06,5.70E-05,6.58E-07
10749,sentiment_analysis40,153,"SVM : The traditional state - of - the - art method using SVMs on surface features , lexicon features and parsing features , which is the best team in SemEval 2014 .",Compared Methods,Compared Methods,sentiment_analysis,40,6,1,1,baselines,0.993693199,1,baselines,3.43E-06,4.33E-06,0.965108444,4.61E-07,7.71E-08,0.000168723,0.000780739,0.001619132,2.85E-06,0.031932039,0.000204791,0.000143657,3.13E-05
10750,sentiment_analysis40,154,"Rec - NN : It firstly uses rules to transform the dependency tree and put the opinion target at the root , and then performs semantic composition with Recursive NNs for sentiment prediction .",Compared Methods,Compared Methods,sentiment_analysis,40,7,1,1,baselines,0.998400238,1,baselines,3.06E-07,6.81E-08,0.999615451,1.79E-09,1.71E-09,3.27E-07,5.96E-06,1.67E-06,7.96E-08,0.000368254,3.51E-07,7.29E-06,2.34E-07
10751,sentiment_analysis40,155,TD- LSTM : It uses a forward LSTM and a backward LSTM to abstract the information before and after the target .,Compared Methods,Compared Methods,sentiment_analysis,40,8,1,1,baselines,0.998735546,1,baselines,2.81E-07,4.09E-08,0.999726268,1.78E-09,1.12E-09,3.23E-07,5.41E-06,1.58E-06,8.63E-08,0.000261664,2.12E-07,3.89E-06,2.46E-07
10752,sentiment_analysis40,156,"Finally , it takes the hidden states of LSTM at last time step to represent the context for prediction .",Compared Methods,Compared Methods,sentiment_analysis,40,9,1,0,,0.422199426,0,baselines,1.36E-05,3.97E-06,0.948021956,4.61E-08,4.78E-08,9.61E-06,2.03E-05,0.000228038,2.03E-05,0.051637991,1.20E-06,4.07E-05,2.24E-06
10753,sentiment_analysis40,157,"We reproduce its results on the tweet dataset with our embeddings , and also run it for the other three datasets .",Compared Methods,Compared Methods,sentiment_analysis,40,10,1,0,,0.525750733,1,negative,0.000181445,3.53E-05,0.180069449,1.20E-06,2.14E-06,0.000158765,0.000842171,0.002105549,5.96E-06,0.80289255,1.22E-05,0.013674826,1.85E-05
10754,sentiment_analysis40,158,TD - LSTM - A : We developed TD - LSTM to make it have one attention on the outputs of 3 https://github.com/svn2github/word2vec,Compared Methods,Compared Methods,sentiment_analysis,40,11,1,1,baselines,0.998118597,1,baselines,2.27E-06,6.02E-07,0.997366283,6.51E-08,4.41E-08,6.84E-06,9.54E-05,3.41E-05,4.69E-07,0.002378292,1.56E-06,0.000108064,5.98E-06
10755,sentiment_analysis40,159,"forward and backward LSTMs , respectively .",Compared Methods,Compared Methods,sentiment_analysis,40,12,1,1,baselines,0.816627051,1,baselines,4.26E-05,3.78E-06,0.764579496,6.98E-07,2.52E-07,6.57E-05,0.000160338,0.000758164,3.75E-05,0.233908993,9.85E-06,0.000411222,2.14E-05
10756,sentiment_analysis40,160,"MemNet : It applies attention multiple times on the word embeddings , and the last attention 's output is fed to softmax for prediction , without combining the results of different attentions .",Compared Methods,Compared Methods,sentiment_analysis,40,13,1,1,baselines,0.991342364,1,baselines,4.28E-07,7.15E-08,0.999524584,4.38E-09,3.22E-09,7.88E-07,1.01E-05,3.62E-06,1.39E-07,0.00045286,1.37E-07,6.82E-06,4.97E-07
10757,sentiment_analysis40,161,We produce its results on all four datasets with the code released by the authors .,Compared Methods,Compared Methods,sentiment_analysis,40,14,1,0,,0.821205347,1,negative,0.000105852,1.18E-05,0.012793157,5.90E-05,1.28E-05,0.001097956,0.000697252,0.005526002,7.88E-06,0.978418647,2.96E-06,0.001234322,3.24E-05
10758,sentiment_analysis40,162,"4 For each method , the maximum number of training iterations is 100 , and the model with the minimum training error is utilized for testing .",Compared Methods,Compared Methods,sentiment_analysis,40,15,1,0,,0.965739386,1,hyperparameters,4.89E-05,5.02E-05,0.009871726,2.49E-06,1.10E-06,0.005301828,0.001537966,0.626572402,2.39E-05,0.356097706,6.27E-06,0.000430714,5.48E-05
10759,sentiment_analysis40,163,We will discuss different settings of RAM later .,Compared Methods,Compared Methods,sentiment_analysis,40,16,1,0,,0.039876248,0,negative,8.52E-05,1.35E-05,0.022540145,2.40E-06,1.26E-06,0.000218145,0.000146804,0.003696978,1.69E-05,0.972342607,3.17E-06,0.000917315,1.56E-05
10760,sentiment_analysis40,164,Main Results,,,sentiment_analysis,40,0,1,0,,0.204992025,0,negative,3.39E-05,5.69E-05,4.65E-06,1.50E-07,2.86E-07,2.47E-05,2.53E-05,0.000366261,2.49E-05,0.998372142,0.000758001,0.000332078,7.58E-07
10761,sentiment_analysis40,165,"The first evaluation metric is Accuracy , which is used in .",Main Results,Main Results,sentiment_analysis,40,1,1,0,,0.004440759,0,negative,0.001207533,4.10E-06,0.001181955,2.06E-07,7.36E-07,2.12E-05,7.37E-05,8.37E-05,1.51E-06,0.967065267,1.36E-05,0.030342463,4.04E-06
10762,sentiment_analysis40,166,"Because the datasets have unbalanced classes as shown in , Macro - averaged F- measure is also reported , as did in .",Main Results,Main Results,sentiment_analysis,40,2,1,0,,0.000172252,0,negative,0.000357803,2.70E-06,3.96E-05,1.43E-06,1.25E-06,6.05E-05,2.62E-05,0.000193895,3.08E-06,0.997485217,3.63E-06,0.001821954,2.69E-06
10763,sentiment_analysis40,167,"As shown by the results in , our RAM consistently outperforms all compared methods on these four datasets .",Main Results,Main Results,sentiment_analysis,40,3,1,1,results,0.985751277,1,results,0.001872406,1.93E-06,6.43E-05,2.54E-06,1.05E-06,3.38E-05,0.001926084,0.000129665,4.59E-07,0.025825984,3.42E-05,0.970001013,0.000106559
10764,sentiment_analysis40,168,"AC and AC - S perform poorly , because averaging context is equivalent to paying identical attention to each word which would hide the true sentiment word .",Main Results,Main Results,sentiment_analysis,40,4,1,1,results,0.985523382,1,results,0.015078346,1.94E-06,0.000366071,1.84E-06,1.31E-06,2.31E-05,0.000878319,7.27E-05,5.30E-07,0.056322187,1.90E-05,0.927187357,4.73E-05
10765,sentiment_analysis40,169,Rec - NN is better than TD - LSTM but not as good as our method .,Main Results,Main Results,sentiment_analysis,40,5,1,1,results,0.891267777,1,results,0.002800275,1.46E-06,0.000144638,3.00E-06,1.12E-06,6.11E-05,0.002492229,0.000209471,4.44E-07,0.02348196,2.83E-05,0.970615371,0.000160628
10766,sentiment_analysis40,170,The advantage of Rec - NN is that it utilizes the result of dependency parsing which might shorten the distance between the opinion target and the related opinion word .,Main Results,Main Results,sentiment_analysis,40,6,1,0,,0.408662825,0,negative,0.027160487,0.000167805,0.077355881,8.15E-06,1.67E-05,0.000118451,0.000681123,0.000255724,9.43E-05,0.639889592,0.000572872,0.253519436,0.000159457
10767,sentiment_analysis40,171,"However , dependency parsing is not guaranteed to work well on irregular texts such as tweets , which may still result in long path between the opinion word and its target , so that the opinion features would also be lost while being propagated .",Main Results,Main Results,sentiment_analysis,40,7,1,0,,0.042334836,0,negative,0.000842476,1.17E-05,0.000407298,7.03E-06,2.08E-06,7.73E-05,0.000356522,0.000235142,5.13E-06,0.950798692,0.007916945,0.039185546,0.000154059
10768,sentiment_analysis40,172,"TD - LSTM performs less competitive than our method on all the datasets , particularly on the tweet dataset , because in this dataset sentiment words are usually far from person names , for which case the multiple - attention mechanism is designed to work .",Main Results,Main Results,sentiment_analysis,40,8,1,1,results,0.977335789,1,results,0.00496017,1.91E-06,0.000181596,1.67E-06,9.76E-07,3.21E-05,0.001451996,0.000132238,4.39E-07,0.027854569,1.58E-05,0.965283134,8.34E-05
10769,sentiment_analysis40,173,"TD - LSTM - A also performs worse than our method , because it s two attentions , i.e. one for the text before the target and the other for the after , can not tackle some cases where more than one features being attended are at the same side of the target .",Main Results,Main Results,sentiment_analysis,40,9,1,1,results,0.971817081,1,results,0.009156508,1.66E-06,0.000136887,3.74E-06,1.47E-06,4.79E-05,0.001670614,0.000139379,5.15E-07,0.026989706,1.77E-05,0.961700164,0.000133818
10770,sentiment_analysis40,174,"Our method steadily performs better than Mem - Net on all four datasets , particularly on the News comment dataset , its improvement is more than 10 % .",Main Results,Main Results,sentiment_analysis,40,10,1,0,,0.973162466,1,results,0.003310291,9.49E-07,6.39E-05,1.44E-06,6.87E-07,1.43E-05,0.001124063,5.99E-05,2.49E-07,0.014986624,8.21E-06,0.980322851,0.000106556
10771,sentiment_analysis40,175,"MemNet adopts multiple attentions in order to improve the attention results , given the assumption that the result of an attention at a later hop should be better than that at the beginning .",Main Results,Main Results,sentiment_analysis,40,11,1,1,results,0.815403677,1,baselines,0.001952926,0.000232192,0.882265032,3.87E-06,9.04E-06,0.000110802,0.000359304,0.000274002,0.000336412,0.104956323,0.00016837,0.009130803,0.000200923
10772,sentiment_analysis40,176,"MemNet does n't combine the results of multiple attentions , and the vector fed to softmax is the result of the last attention , which is essentially the linear combination of word embeddings .",Main Results,Main Results,sentiment_analysis,40,12,1,0,,0.001151699,0,negative,0.006792558,0.00013221,0.344664826,1.40E-05,2.45E-05,0.000435494,0.000928639,0.000605939,0.00017859,0.612971088,0.000169253,0.032757014,0.000325874
10773,sentiment_analysis40,177,"As we described before , attending too many words in onetime may hide the characteristic of each word , moreover , the sentiment transition usually combines features in a nonlinear way .",Main Results,Main Results,sentiment_analysis,40,13,1,0,,0.015565164,0,negative,0.002590742,2.71E-05,0.001185219,4.54E-06,3.13E-06,4.67E-05,5.82E-05,0.000199393,3.68E-05,0.985477693,0.000154087,0.010167941,4.84E-05
10774,sentiment_analysis40,178,Our model overcomes this shortcoming with a GRU network to combine the results of multiple attentions .,Main Results,Main Results,sentiment_analysis,40,14,1,0,,0.161807728,0,negative,0.025834994,0.001772538,0.137694318,7.80E-05,0.000104703,0.000403208,0.000521818,0.001474841,0.006067791,0.781696679,0.000481291,0.042661143,0.001208725
10775,sentiment_analysis40,179,"The feature - based SVM , which needs labor - intensive feature engineering works and amass of extra linguistic resources , does n't display its advantage , because the features for aspect sentiment analy - sis can not be extracted as easily as for sentence or document level sentiment analysis .",Main Results,Main Results,sentiment_analysis,40,15,1,0,,0.449765286,0,results,0.004747269,5.26E-06,0.002150799,9.96E-07,1.95E-06,2.34E-05,0.000767344,5.70E-05,1.29E-06,0.214289794,0.000117524,0.777762973,7.44E-05
10776,sentiment_analysis40,180,Effects of Attention Layers,Main Results,,sentiment_analysis,40,16,1,0,,0.242298001,0,results,0.059713985,2.08E-05,0.001503773,3.07E-06,5.62E-06,4.20E-05,0.000915758,0.000178646,1.84E-05,0.301974884,2.95E-05,0.635488121,0.000105409
10777,sentiment_analysis40,181,One major setting that affects the performance of our model is the number of attention layers .,Main Results,Effects of Attention Layers,sentiment_analysis,40,17,1,0,,0.000421597,0,negative,0.004346714,2.26E-05,0.000147155,2.33E-05,6.32E-06,0.000214342,0.000128172,0.000467146,5.62E-05,0.982494147,4.07E-05,0.01195017,0.000103049
10778,sentiment_analysis40,182,"We evaluate our framework with 1 to 5 attention layers , and the results are given in , where N AL means using N attentions .",Main Results,Effects of Attention Layers,sentiment_analysis,40,18,1,0,,0.004677999,0,negative,0.001055797,4.23E-06,5.92E-05,1.90E-06,2.74E-06,0.000272023,0.000386874,0.000700332,4.42E-06,0.979604183,3.82E-06,0.017845481,5.90E-05
10779,sentiment_analysis40,183,"In general , our model with 2 or 3 attention layers works better , but the advantage is not always therefor different datasets .",Main Results,Effects of Attention Layers,sentiment_analysis,40,19,1,0,,0.293777354,0,results,0.013254689,1.71E-06,3.17E-05,1.02E-05,1.41E-06,0.000190091,0.002786915,0.000537071,1.75E-06,0.133274692,1.33E-05,0.849368331,0.000528166
10780,sentiment_analysis40,184,"For example , for the Restaurant dataset , our model with 4 attention layers performs the best .",Main Results,Effects of Attention Layers,sentiment_analysis,40,20,1,0,,0.542252442,1,results,0.008457436,2.27E-07,1.20E-05,6.63E-07,2.49E-07,1.46E-05,0.000864923,4.25E-05,1.39E-07,0.036964458,1.71E-06,0.953566201,7.49E-05
10781,sentiment_analysis40,185,"Using 1 attention is always not as good as using more , which shows that one - time attention might not be sufficient to capture the sentiment features in complicated cases .",Main Results,Effects of Attention Layers,sentiment_analysis,40,21,1,0,,0.776957389,1,results,0.028671042,1.08E-06,3.18E-05,8.70E-07,6.69E-07,3.37E-05,0.001003336,0.000110044,6.44E-07,0.08610392,4.97E-06,0.883952899,8.50E-05
10782,sentiment_analysis40,186,"One the other hand , the performance is not monotonically increasing with respect to the number of attentions .",Main Results,Effects of Attention Layers,sentiment_analysis,40,22,1,0,,0.001552074,0,negative,0.045813483,4.72E-06,0.000182444,1.75E-06,2.66E-06,1.87E-05,0.000191044,6.22E-05,4.71E-06,0.5694926,1.10E-05,0.384173878,4.08E-05
10783,sentiment_analysis40,187,"RAM - 4AL is generally not as good as RAM - 3 AL , it is because as the model 's complexity increases , the model becomes more difficult to train and less generalizable .",Main Results,Effects of Attention Layers,sentiment_analysis,40,23,1,0,,0.479448326,0,results,0.001985794,3.35E-07,4.76E-05,9.09E-07,4.54E-07,4.19E-05,0.001752622,7.83E-05,2.45E-07,0.062561089,6.21E-06,0.933319695,0.00020496
10784,sentiment_analysis40,188,Effects of Embedding Tuning,Main Results,,sentiment_analysis,40,24,1,0,,0.081190745,0,results,0.234781247,5.30E-06,0.000466025,4.10E-06,5.03E-06,2.53E-05,0.000593608,8.51E-05,4.55E-06,0.258307289,2.85E-06,0.505656481,6.31E-05
10785,sentiment_analysis40,189,The compared embedding tuning strategies are :,Main Results,Effects of Embedding Tuning,sentiment_analysis,40,25,1,0,,0.00742129,0,negative,0.00180761,2.33E-06,0.008064504,1.92E-07,4.15E-07,1.83E-05,2.16E-05,2.62E-05,7.03E-06,0.982927274,1.79E-06,0.007098532,2.43E-05
10786,sentiment_analysis40,190,RAM - 3AL - T - R :,Main Results,Effects of Embedding Tuning,sentiment_analysis,40,26,1,0,,0.010737979,0,negative,0.011703929,4.94E-06,0.07870388,3.46E-06,4.37E-06,0.000109013,0.000839648,4.98E-05,1.81E-05,0.559779845,2.72E-05,0.34508182,0.003673966
10787,sentiment_analysis40,191,"It does not pre-train word embeddings , but initializes embeddings randomly and then tunes them in the supervised From , we can see that RAM - 3AL - T - R performs very poorly , especially when the size of training data is smaller .",Main Results,Effects of Embedding Tuning,sentiment_analysis,40,27,1,0,,0.729280659,1,results,0.02063034,9.47E-07,0.000159292,6.24E-07,6.98E-07,1.48E-05,0.00036541,2.00E-05,6.69E-07,0.191210409,1.84E-06,0.787375548,0.00021942
10788,sentiment_analysis40,192,The reason could be threefold :,Main Results,Effects of Embedding Tuning,sentiment_analysis,40,28,1,0,,2.30E-05,0,negative,0.000531389,6.16E-07,3.99E-05,2.89E-07,2.56E-07,8.99E-06,3.21E-06,1.65E-05,4.59E-06,0.9981937,7.32E-07,0.001170944,2.89E-05
10789,sentiment_analysis40,193,"( 1 ) The amount of labelled samples in the four experimental datasets is too small to tune reliable embeddings from scratch for the in - vocabulary words ( i.e. existing in the training data ) ; ( 2 ) A lot of out - of - vocabulary ( OOV ) words , i.e. absent from the training data , but exist in the testing data ; ( 3 ) It increases the risk of overfitting after adding the embedding parameters to the solution space ( it requires the embeddings not only to fit model parameters , but also to capture the similarity among words ) .",Main Results,Effects of Embedding Tuning,sentiment_analysis,40,29,1,0,,0.002576797,0,negative,0.038634355,2.17E-06,0.000113074,1.63E-06,2.83E-06,2.38E-05,6.86E-05,2.29E-05,1.96E-06,0.862986958,1.89E-06,0.09800714,0.000132764
10790,sentiment_analysis40,194,"During training , we indeed observed that the training error converges too fast in RAM - 3AL - T - R. RAM - 3AL - T can utilize the embedding similarity among words at the beginning of training , but fine tuning will destroy this similarity during training .",Main Results,Effects of Embedding Tuning,sentiment_analysis,40,30,1,0,,0.027804634,0,negative,0.039644168,8.97E-07,5.43E-05,6.96E-07,1.26E-06,2.67E-05,0.000200408,4.47E-05,1.13E-06,0.663087723,1.43E-06,0.296586527,0.000350132
10791,sentiment_analysis40,195,"On the other hand , the initial embeddings of OOV words in the testing data are not tuned , so that their similarity with vocabulary words are also destroyed .",Main Results,Effects of Embedding Tuning,sentiment_analysis,40,31,1,0,,0.000540192,0,negative,0.034792419,3.65E-06,7.85E-05,9.25E-07,2.43E-06,1.43E-05,1.53E-05,4.97E-05,6.06E-06,0.94298481,2.50E-07,0.022019488,3.22E-05
10792,sentiment_analysis40,196,"In addition , RAM - 3AL - T also suffers from the risk of overfitting .",Main Results,Effects of Embedding Tuning,sentiment_analysis,40,32,1,0,,0.034994809,0,negative,0.018729182,1.73E-06,0.000276949,1.35E-06,1.35E-06,1.83E-05,0.00024502,1.61E-05,9.33E-07,0.539963134,1.10E-05,0.440068788,0.000666205
10793,sentiment_analysis40,197,"RAM - 3AL - NT performs the best on all four datasets , and we also observe that the training error converges gradually while the model parameters are being updated with the error signal from the output layer .",Main Results,Effects of Embedding Tuning,sentiment_analysis,40,33,1,0,,0.86957229,1,results,0.011115212,3.03E-07,3.84E-05,6.43E-07,3.23E-07,1.54E-05,0.000756069,2.61E-05,1.88E-07,0.099001274,5.51E-07,0.888424844,0.000620744
10794,sentiment_analysis40,198,Case Study,Main Results,,sentiment_analysis,40,34,1,0,,0.004568239,0,negative,0.000742095,2.94E-05,0.000535463,1.27E-05,1.73E-05,0.000146634,0.000287225,0.001195823,5.52E-05,0.977439138,1.54E-05,0.018897878,0.000625687
10795,sentiment_analysis40,199,We pick some testing examples from the datasets and visualize their attention results .,Main Results,Case Study,sentiment_analysis,40,35,1,0,,1.39E-06,0,negative,4.43E-06,3.49E-05,3.22E-07,3.58E-07,9.51E-05,5.35E-05,3.54E-06,5.50E-05,4.03E-06,0.999744986,2.04E-07,3.58E-06,7.39E-08
10796,sentiment_analysis40,200,"To make the visualized results comprehensible , we remove the BLSTM memory module to make the attention module directly work on the word embeddings , thus we can check whether the attention results conform with our intuition .",Main Results,Case Study,sentiment_analysis,40,36,1,0,,0.000208933,0,negative,0.002544438,0.002135991,3.92E-05,9.13E-07,0.000179731,6.60E-05,1.89E-05,9.51E-05,0.00174855,0.993064233,1.78E-06,0.000104531,6.40E-07
10797,sentiment_analysis40,201,The visualization results are shown in .,Main Results,Case Study,sentiment_analysis,40,37,1,0,,5.52E-05,0,negative,8.17E-05,6.24E-06,5.47E-07,1.40E-08,1.75E-06,7.60E-06,2.84E-05,1.07E-05,2.15E-06,0.999163761,3.11E-06,0.00069392,1.03E-07
10798,sentiment_analysis40,202,"present the differences between using two attentions and using one attention , which show that multiple attentions are useful to attend correct features .",Main Results,Case Study,sentiment_analysis,40,38,1,0,,0.000154099,0,negative,0.00024927,0.000135928,8.43E-06,7.08E-07,9.50E-05,2.36E-05,9.36E-06,1.35E-05,3.64E-05,0.999356173,6.60E-06,6.47E-05,3.00E-07
10799,sentiment_analysis40,203,"As shown in , in order to identify the sentiment of "" windows "" , the model firstly notices "" welcomed "" and secondly notices "" compared "" before the aspect target "" windows "" .",Main Results,Case Study,sentiment_analysis,40,39,1,0,,2.21E-05,0,negative,0.000134474,0.000126084,1.41E-05,8.70E-08,0.000119494,1.76E-05,8.22E-06,1.10E-05,7.81E-05,0.999427923,4.94E-07,6.23E-05,1.09E-07
10800,sentiment_analysis40,204,"Finally it combines them with the GRU network , and generates a negative sentiment because the compared item ( i.e. "" windows "" ) after a positive sentiment word ( i.e. "" welcomed "" ) is less preferred .",Main Results,Case Study,sentiment_analysis,40,40,1,0,,0.000163964,0,negative,0.001429869,0.00326137,0.001759455,7.23E-07,0.000341924,7.06E-05,6.19E-05,4.03E-05,0.007447339,0.985338825,6.90E-06,0.000237988,2.77E-06
10801,sentiment_analysis40,205,"While the attention result of the model ( a ) Example of a Chinese contrastive sentence , whose translation is "" $ T$'s quality and ability are absolutely stronger than $ PEOPLE $!!! "" .",Main Results,Case Study,sentiment_analysis,40,41,1,0,,2.39E-07,0,negative,1.15E-05,7.49E-06,2.57E-06,3.98E-07,0.000136024,2.83E-05,6.67E-06,6.92E-06,2.98E-06,0.999786522,1.69E-06,8.73E-06,2.58E-07
10802,sentiment_analysis40,206,"The target is "" $ T$ "" .",Main Results,Case Study,sentiment_analysis,40,42,1,0,,2.53E-05,0,negative,3.66E-06,8.57E-05,1.31E-06,4.39E-07,5.28E-05,0.000575287,2.80E-05,0.000935217,4.12E-05,0.998267343,9.58E-07,7.17E-06,9.48E-07
10803,sentiment_analysis40,207,( b ),Main Results,Case Study,sentiment_analysis,40,43,1,0,,8.06E-07,0,negative,5.40E-05,2.11E-05,1.12E-05,2.64E-08,7.41E-06,1.09E-05,5.89E-06,7.84E-06,3.03E-05,0.999827688,6.77E-07,2.29E-05,9.30E-08
10804,sentiment_analysis40,208,"The sentence from 3 a with a different target , i.e. "" $ PEOPLE $ 's quality and ability are absolutely stronger than $ T $!!! "" . with only one attention , as shown in , is a sort of uniform distribution and mingles too many word vectors in a linear way , which would ruin the characteristic of each word .",Main Results,Case Study,sentiment_analysis,40,44,1,0,,4.16E-07,0,negative,4.17E-06,6.91E-06,5.83E-07,1.40E-07,3.70E-05,4.00E-05,4.20E-06,1.59E-05,4.15E-06,0.999882243,4.14E-07,4.27E-06,1.21E-07
10805,sentiment_analysis40,209,"present a case that there are more than one opinion targets in a comment , which can not be analyzed with sentence - level sentiment analysis methods properly .",Main Results,Case Study,sentiment_analysis,40,45,1,0,,1.87E-06,0,negative,3.19E-05,9.27E-05,4.09E-06,1.54E-05,0.001118719,5.21E-05,6.40E-06,1.14E-05,8.85E-06,0.99863276,1.53E-05,9.24E-06,1.04E-06
10806,sentiment_analysis40,210,"Specifically , it 's a comparative sentence in which the reviewer has a positive sentiment on the first commented person , but a negative sentiment on the second person , and our model predicts both of them correctly .",Main Results,Case Study,sentiment_analysis,40,46,1,0,,1.17E-06,0,negative,2.82E-06,1.98E-05,8.96E-07,1.31E-06,0.00084051,3.19E-05,2.03E-06,1.18E-05,2.70E-06,0.999083852,2.26E-07,1.96E-06,1.86E-07
10807,sentiment_analysis40,211,"Although all useful information ( e.g. "" than "" and "" stronger "" ) is attended in both cases , the attention procedures of them show some interesting differences .",Main Results,Case Study,sentiment_analysis,40,47,1,0,,1.43E-06,0,negative,0.000129965,2.75E-05,1.96E-06,3.09E-08,1.94E-05,5.18E-06,8.86E-06,5.31E-06,5.06E-06,0.99964746,3.40E-07,0.000148889,4.70E-08
10808,sentiment_analysis40,212,They mainly attend important information after the target $ T$ in the first attention layer AL1 .,Main Results,Case Study,sentiment_analysis,40,48,1,0,,8.96E-06,0,negative,0.000270224,0.000210966,3.63E-05,5.61E-07,0.000139887,4.25E-05,1.43E-05,3.04E-05,0.000142598,0.999070521,2.15E-06,3.89E-05,6.89E-07
10809,sentiment_analysis40,213,"After that , attends more information before $ T$ in AL2 .",Main Results,Case Study,sentiment_analysis,40,49,1,0,,5.89E-07,0,negative,0.000533932,0.000117494,4.41E-05,1.50E-07,0.00016847,1.62E-05,2.35E-05,1.03E-05,5.70E-05,0.998905041,3.78E-07,0.000123074,3.21E-07
10810,sentiment_analysis40,214,"Since the same words in have different memory slices due to position weighting and augmented offset feature , as described in Section 3.3 , our model predicts opposite sentiments on the two persons .",Main Results,Case Study,sentiment_analysis,40,50,1,0,,4.68E-06,0,negative,0.000318885,0.001208469,4.66E-05,8.31E-08,6.33E-05,1.46E-05,1.12E-05,2.00E-05,0.002244285,0.995993877,6.42E-07,7.77E-05,2.89E-07
10811,sentiment_analysis40,215,"For example in , the model first attends a positive word "" stronger "" and then attends "" than "" before the target , so it reverses the sentiment and finally predicts a negative sentiment .",Main Results,Case Study,sentiment_analysis,40,51,1,0,,3.14E-07,0,negative,5.54E-06,1.07E-05,2.57E-06,3.31E-07,8.11E-05,4.40E-05,3.40E-06,1.25E-05,1.47E-05,0.99982292,1.68E-07,1.92E-06,1.71E-07
10812,sentiment_analysis40,216,Conclusions and Future Work,,,sentiment_analysis,40,0,1,0,,0.001063554,0,negative,5.43E-05,3.39E-05,5.59E-06,3.77E-07,3.51E-07,5.34E-05,6.19E-05,0.000376148,2.43E-05,0.995250656,0.003578738,0.000557487,2.86E-06
10813,natural_language_inference15,1,title,,,natural_language_inference,15,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
10814,natural_language_inference15,2,Semantic Sentence Matching with Densely - connected Recurrent and Co - attentive Information,title,title,natural_language_inference,15,1,1,1,research-problem,0.997217763,1,research-problem,2.84E-08,1.34E-05,1.45E-07,2.98E-08,3.82E-08,5.59E-08,7.07E-07,1.02E-06,1.74E-06,0.001387859,0.998594757,2.05E-07,3.63E-08
10815,natural_language_inference15,3,abstract,,,natural_language_inference,15,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
10816,natural_language_inference15,4,"Sentence matching is widely used in various natural language tasks such as natural language inference , paraphrase identification , and question answering .",abstract,abstract,natural_language_inference,15,1,1,1,research-problem,0.938217034,1,research-problem,1.11E-08,2.74E-06,8.27E-09,6.53E-07,1.80E-07,1.79E-07,3.79E-07,7.08E-07,1.54E-07,0.005948506,0.994046431,1.21E-08,3.58E-08
10817,natural_language_inference15,5,"For these tasks , understanding logical and semantic relationship between two sentences is required but it is yet challenging .",abstract,abstract,natural_language_inference,15,2,1,0,,0.423103317,0,research-problem,2.08E-08,3.13E-06,4.61E-09,3.94E-06,4.24E-07,6.29E-07,3.25E-07,1.94E-06,2.12E-07,0.032103742,0.967885574,1.05E-08,4.50E-08
10818,natural_language_inference15,6,"Although attention mechanism is useful to capture the semantic relationship and to properly align the elements of two sentences , previous methods of attention mechanism simply use a summation operation which does not retain original features enough .",abstract,abstract,natural_language_inference,15,3,1,0,,0.009187971,0,research-problem,2.27E-07,8.97E-05,1.17E-07,3.72E-06,1.95E-06,4.56E-06,1.08E-06,2.94E-05,1.36E-05,0.142714983,0.857140445,9.28E-08,1.43E-07
10819,natural_language_inference15,7,"Inspired by DenseNet , a densely connected convolutional network , we propose a densely - connected co-attentive recurrent neural network , each layer of which uses concatenated information of attentive features as well as hidden features of all the preceding recurrent layers .",abstract,abstract,natural_language_inference,15,4,1,0,,0.324302471,0,approach,7.30E-05,0.400237491,0.001021343,5.15E-05,0.000686214,0.000128542,4.45E-05,0.000990063,0.192747164,0.24178885,0.162206831,1.39E-05,1.06E-05
10820,natural_language_inference15,8,It enables preserving the original and the co-attentive feature information from the bottommost word embedding layer to the uppermost recurrent layer .,abstract,abstract,natural_language_inference,15,5,1,0,,0.14891169,0,negative,0.000175453,0.047329344,0.00016909,4.98E-05,0.00062925,4.74E-05,1.06E-05,0.000273229,0.013601168,0.884356351,0.053341598,1.44E-05,2.30E-06
10821,natural_language_inference15,9,"To alleviate the problem of an ever - increasing size of feature vectors due to dense concatenation operations , we also propose to use an autoencoder after dense concatenation .",abstract,abstract,natural_language_inference,15,6,1,0,,0.229878936,0,negative,0.000258274,0.379846448,0.000590826,8.63E-05,0.0014639,0.000132411,2.89E-05,0.00084566,0.064130319,0.500459196,0.052131831,1.94E-05,6.43E-06
10822,natural_language_inference15,10,We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching .,abstract,abstract,natural_language_inference,15,7,1,0,,0.003735821,0,negative,4.37E-06,0.062429163,8.22E-06,1.25E-05,0.000529819,4.74E-05,2.62E-05,0.000815485,0.000628388,0.828389073,0.107095593,1.25E-05,1.30E-06
10823,natural_language_inference15,11,"Experimental results show that our architecture , which retains recurrent and attentive features , achieves state - of - the - art performances for most of the tasks .",abstract,abstract,natural_language_inference,15,8,1,0,,0.008533379,0,negative,4.23E-05,0.002638523,2.04E-06,1.11E-05,5.56E-05,2.95E-05,5.31E-05,0.000561191,7.27E-05,0.803607227,0.192684456,0.000239741,2.56E-06
10824,natural_language_inference15,12,Introduction,,,natural_language_inference,15,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
10825,natural_language_inference15,13,"Semantic sentence matching , a fundamental technology in natural language processing , requires lexical and compositional semantics .",Introduction,Introduction,natural_language_inference,15,1,1,0,,0.877262382,1,research-problem,4.55E-07,7.39E-05,2.35E-07,7.38E-06,9.32E-06,2.71E-06,8.40E-06,2.42E-06,2.28E-05,0.016441831,0.983428431,5.97E-07,1.49E-06
10826,natural_language_inference15,14,"In paraphrase identification , sentence matching is utilized to identify whether two sentences have identical meaning or not .",Introduction,Introduction,natural_language_inference,15,2,1,0,,0.204606172,0,research-problem,1.14E-06,0.00019788,4.16E-07,2.08E-05,4.72E-05,8.60E-06,1.52E-05,7.55E-06,3.24E-05,0.044796126,0.954868587,1.14E-06,3.03E-06
10827,natural_language_inference15,15,"In natural language inference also known as recognizing textual entailment , it determines whether a hypothesis sentence can reasonably be inferred from a given premise sentence .",Introduction,Introduction,natural_language_inference,15,3,1,0,,0.905162851,1,research-problem,7.34E-07,0.000135456,3.79E-07,2.69E-05,2.73E-05,8.43E-06,1.08E-05,5.53E-06,3.54E-05,0.045170512,0.954575493,6.83E-07,2.46E-06
10828,natural_language_inference15,16,"In question answering , sentence matching is required to determine the degree of matching 1 ) between a query and a question for question retrieval , and 2 ) between a question and an answer for answer selection .",Introduction,Introduction,natural_language_inference,15,4,1,0,,0.894961336,1,research-problem,7.17E-07,0.000198017,3.11E-07,1.14E-05,2.35E-05,5.49E-06,9.71E-06,6.12E-06,3.88E-05,0.040304382,0.959398693,8.38E-07,2.07E-06
10829,natural_language_inference15,17,However identifying logical and semantic relationship between two sentences is not trivial due to the problem of the semantic gap .,Introduction,Introduction,natural_language_inference,15,5,1,0,,0.066143327,0,research-problem,7.30E-07,0.000101799,1.42E-07,6.47E-06,1.18E-05,6.23E-06,5.74E-06,6.65E-06,3.30E-05,0.128238399,0.87158749,6.24E-07,9.08E-07
10830,natural_language_inference15,18,Recent advances of deep neural network enable to learn textual semantics for sentence matching .,Introduction,Introduction,natural_language_inference,15,6,1,0,,0.339095912,0,research-problem,2.13E-06,0.000377869,1.03E-06,2.73E-06,9.93E-06,5.04E-06,1.72E-05,8.66E-06,0.000174015,0.065082855,0.934313338,3.40E-06,1.76E-06
10831,natural_language_inference15,19,"Large amount of annotated data such as , SNLI ( Bowman Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",Introduction,Introduction,natural_language_inference,15,7,1,0,,0.019007909,0,negative,7.96E-06,0.000777381,4.07E-06,0.000250473,0.001871485,0.000124761,6.15E-05,4.55E-05,0.000270171,0.712834633,0.283733754,6.54E-06,1.17E-05
10832,natural_language_inference15,20,"All rights reserved. , and MultiNLI have contributed significantly to learning semantics as well .",Introduction,Introduction,natural_language_inference,15,8,1,0,,0.128912295,0,negative,2.71E-05,0.009788593,2.21E-05,3.04E-05,0.000237104,0.000158446,8.30E-05,0.00017376,0.037125881,0.740229261,0.212099612,1.55E-05,9.21E-06
10833,natural_language_inference15,21,"In the conventional methods , a matching model can be trained in two different ways .",Introduction,Introduction,natural_language_inference,15,9,1,0,,0.296835319,0,negative,4.90E-05,0.134515899,5.68E-05,3.21E-05,0.00049407,0.000202136,7.31E-05,0.000493563,0.141534199,0.545155953,0.177360104,2.16E-05,1.15E-05
10834,natural_language_inference15,22,The first methods are sentence - encoding - based ones where each sentence is encoded to a fixed - sized vector in a complete isolated manner and the two vectors for the corresponding sentences are used in predicting the degree of matching .,Introduction,Introduction,natural_language_inference,15,10,1,0,,0.717883055,1,negative,3.51E-05,0.065142189,7.65E-05,0.000118286,0.00088211,0.000518,0.000212633,0.000668485,0.03145995,0.620567552,0.280268111,2.50E-05,2.61E-05
10835,natural_language_inference15,23,The others are joint methods that allow to utilize interactive features like attentive information between the sentences .,Introduction,Introduction,natural_language_inference,15,11,1,0,,0.254799408,0,negative,0.000104654,0.050696807,0.000203239,0.000103972,0.001510785,0.000577263,0.000155636,0.000360973,0.083348193,0.819784883,0.043117046,2.26E-05,1.39E-05
10836,natural_language_inference15,24,"In the former paradigm , because two sentences have no interaction , they can not utilize interactive information during the encoding procedure .",Introduction,Introduction,natural_language_inference,15,12,1,0,,0.026758687,0,negative,1.32E-05,0.018418694,9.29E-06,1.28E-05,0.000173968,7.56E-05,4.56E-05,0.000150835,0.017942078,0.668755154,0.294384331,1.47E-05,3.81E-06
10837,natural_language_inference15,25,"In our work , we adopted a joint method which enables capturing interactive information for performance improvements .",Introduction,Introduction,natural_language_inference,15,13,1,0,,0.670615739,1,approach,0.000213486,0.558252839,0.000377522,7.25E-05,0.006499688,0.000208995,0.000135717,0.000236781,0.267541242,0.152154133,0.014238606,5.39E-05,1.46E-05
10838,natural_language_inference15,26,"Furthermore , we employ a substantially deeper recurrent network for sentence matching like deep neural machine translator ( NMT ) .",Introduction,Introduction,natural_language_inference,15,14,1,0,,0.91770899,1,approach,0.000163315,0.564007239,0.001268259,4.84E-05,0.011647737,0.000244429,0.000339048,0.000180486,0.384750124,0.035384356,0.001871187,6.85E-05,2.69E-05
10839,natural_language_inference15,27,Deep recurrent models are more advantageous for learning long sequences and outperform the shallower architectures .,Introduction,Introduction,natural_language_inference,15,15,1,0,,0.096477211,0,negative,0.000987368,0.010711257,9.01E-05,2.48E-05,0.000372673,0.000201587,0.001104595,0.000279848,0.007757116,0.894409208,0.082879317,0.001152272,2.99E-05
10840,natural_language_inference15,28,"However , the attention mechanism is unstable in deeper models with the well - known vanishing gradient problem .",Introduction,Introduction,natural_language_inference,15,16,1,0,,0.094705539,0,negative,6.73E-05,0.012272645,1.07E-05,6.51E-05,0.000381033,0.000149667,8.34E-05,0.000257151,0.006527611,0.873580842,0.106570229,2.58E-05,8.61E-06
10841,natural_language_inference15,29,"Though ) uses residual connection between recurrent layers to allow better information and gradient flow , there are some limitations .",Introduction,Introduction,natural_language_inference,15,17,1,0,,0.026423949,0,negative,0.000103492,0.01939807,0.000271651,7.42E-05,0.002299346,0.000349631,0.000396423,0.000162466,0.006618328,0.870942125,0.099286136,7.26E-05,2.56E-05
10842,natural_language_inference15,30,The recurrent hidden or attentive features are not preserved intact through residual connection because the summation operation may impede the information flow in deep networks .,Introduction,Introduction,natural_language_inference,15,18,1,0,,0.066177317,0,negative,0.000561324,0.043702859,5.48E-05,1.30E-05,0.000743064,9.55E-05,8.56E-05,0.000151777,0.065377955,0.8801042,0.009012962,9.29E-05,4.05E-06
10843,natural_language_inference15,31,"Inspired by Densenet ) , we propose a densely - connected recurrent network where the recurrent hidden features are retained to the uppermost layer .",Introduction,Introduction,natural_language_inference,15,19,1,1,model,0.939867964,1,model,8.61E-05,0.245197118,0.000687162,5.87E-06,0.001056302,5.55E-05,8.01E-05,5.69E-05,0.735447221,0.016564695,0.00074068,1.66E-05,5.61E-06
10844,natural_language_inference15,32,"In addition , instead of the conventional summation operation , the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better .",Introduction,Introduction,natural_language_inference,15,20,1,1,model,0.941636231,1,model,8.46E-05,0.230199003,0.000106381,3.02E-06,0.000699804,3.64E-05,3.64E-05,7.42E-05,0.750471453,0.018066962,0.000208537,1.10E-05,2.21E-06
10845,natural_language_inference15,33,The proposed architecture shown in is called DRCN which is an abbreviation for Densely - connected Recurrent and Co -attentive neural Network .,Introduction,Introduction,natural_language_inference,15,21,1,1,model,0.740538162,1,model,9.41E-06,0.053339127,0.000150227,4.81E-06,0.000377872,7.73E-05,3.78E-05,8.22E-05,0.893265079,0.050512681,0.002133032,5.05E-06,5.44E-06
10846,natural_language_inference15,34,The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information .,Introduction,Introduction,natural_language_inference,15,22,1,1,model,0.804873465,1,model,0.000166933,0.190983837,0.00014821,6.26E-06,0.000635122,6.21E-05,8.15E-05,0.000122405,0.699539703,0.105677794,0.002518062,5.21E-05,5.96E-06
10847,natural_language_inference15,35,"Furthermore , to alleviate the problem of an ever- increasing feature vector size due to concatenation operations , we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure .",Introduction,Introduction,natural_language_inference,15,23,1,1,model,0.790502148,1,model,6.06E-05,0.124320803,0.000197913,1.18E-05,0.001314582,0.000330108,9.28E-05,0.000365355,0.788672479,0.084431343,0.000188284,7.61E-06,6.28E-06
10848,natural_language_inference15,36,"DRCN is , to our best knowledge , the first generalized version of DenseRNN which is expandable to deeper layers with the property of Dashed arrows indicate that a group of RNN - layer , concatenation and AE can be repeated multiple ( N ) times ( like a repeat mark in a music score ) .",Introduction,Introduction,natural_language_inference,15,24,1,0,,0.864299542,1,model,5.63E-05,0.271782977,0.001951975,6.09E-06,0.000997841,9.67E-05,0.000334802,0.00010006,0.639664598,0.07500954,0.009919655,6.37E-05,1.58E-05
10849,natural_language_inference15,37,"The bottleneck component denoted as AE , inserted to prevent the ever - growing size of a feature vector , is optional for each repetition .",Introduction,Introduction,natural_language_inference,15,25,1,0,,0.919699625,1,model,3.96E-05,0.076102735,4.35E-05,3.56E-06,0.000220839,0.000112801,3.42E-05,0.00035008,0.883732311,0.039192651,0.00016211,2.75E-06,2.94E-06
10850,natural_language_inference15,38,The upper right diagram is our specific architecture for experiments with 5 RNN layers ( N = 4 ) .,Introduction,Introduction,natural_language_inference,15,26,1,0,,0.032418012,0,negative,2.18E-05,0.044097322,4.40E-05,4.90E-05,0.001375965,0.001275139,0.000265781,0.001367388,0.274909776,0.67468813,0.001871758,1.42E-05,1.97E-05
10851,natural_language_inference15,39,controllable feature sizes by the use of an autoencoder .,Introduction,Introduction,natural_language_inference,15,27,1,0,,0.136459763,0,negative,0.000104543,0.042722649,7.90E-05,0.000105583,0.002799051,0.000568883,0.000265661,0.000583735,0.048768804,0.899595429,0.004322607,6.57E-05,1.83E-05
10852,natural_language_inference15,40,"We evaluate our model on three sentence matching tasks : natural language inference , paraphrase identification and answer sentence selection .",Introduction,Introduction,natural_language_inference,15,28,1,0,,0.028495177,0,approach,5.37E-05,0.56593499,0.000101092,9.57E-05,0.055479908,0.000687697,0.001616722,0.001136134,0.019208199,0.352979741,0.002431579,0.000207555,6.70E-05
10853,natural_language_inference15,41,"Experimental results on five highly competitive benchmark datasets ( SNLI , MultiNLI , QUORA , TrecQA and SelQA ) show that our model significantly outperforms the current state - of - the - art results on most of the tasks .",Introduction,Introduction,natural_language_inference,15,29,1,0,,0.026863048,0,negative,0.001393459,0.049447988,4.58E-05,3.76E-05,0.002475007,0.0003441,0.016677614,0.001016806,0.003665365,0.905144169,0.004381388,0.01520721,0.000163478
10854,natural_language_inference15,42,Related Work,,,natural_language_inference,15,0,1,0,,8.05E-05,0,negative,1.31E-05,2.42E-05,2.76E-06,5.43E-07,4.15E-07,3.05E-05,2.86E-05,0.000252678,7.83E-06,0.997124045,0.002452773,6.13E-05,1.39E-06
10855,natural_language_inference15,59,Methods,,,natural_language_inference,15,0,1,0,,2.98E-05,0,negative,4.58E-06,0.000323553,9.58E-06,8.17E-07,1.29E-06,6.39E-05,1.47E-05,0.001183839,0.000161795,0.996781926,0.001437094,1.57E-05,1.25E-06
10856,natural_language_inference15,60,"In this section , we describe our sentence matching architecture DRCN which is composed of the following three components : ( 1 ) word representation layer , ( 2 ) attentively connected RNN and ( 3 ) interaction and prediction layer .",Methods,Methods,natural_language_inference,15,1,1,0,,0.928372297,1,baselines,0.001372907,0.072441213,0.414941661,8.86E-05,0.000404539,0.001206597,0.002199267,0.012720989,0.215720407,0.243869107,0.032340013,0.001975629,0.000719111
10857,natural_language_inference15,61,"We denote two input sentences as P = {p 1 , p 2 , , p I } and Q = {q 1 , q 2 , , q J } where pi / q j is the i th /j th word of the sentence P / Q and I/ J is the word length of P / Q .",Methods,Methods,natural_language_inference,15,2,1,0,,0.065345966,0,negative,1.26E-05,0.00152617,0.000691241,3.59E-06,1.05E-05,0.000992365,0.000134087,0.031903577,0.004870888,0.959231671,0.000558489,4.87E-05,1.62E-05
10858,natural_language_inference15,62,The over all architecture of the proposed DRCN is shown in .,Methods,Methods,natural_language_inference,15,3,1,0,,0.826125849,1,negative,0.000499927,0.010133469,0.021657538,1.00E-04,0.000103985,0.002286575,0.001017396,0.043816867,0.0915909,0.818093289,0.009713638,0.000484941,0.000501522
10859,natural_language_inference15,63,Word Representation Layer,Methods,,natural_language_inference,15,4,1,0,,0.243486049,0,negative,0.001734944,0.007728902,0.250387159,0.000206296,0.00013945,0.007894898,0.009846886,0.086581213,0.153206446,0.457418206,0.018355811,0.003513534,0.002986253
10860,natural_language_inference15,64,"To construct the word representation layer , we concatenate word embedding , character representation and the exact matched flag which was used in .",Methods,Word Representation Layer,natural_language_inference,15,5,1,0,,0.000736023,0,negative,0.000152743,0.001077592,0.00065898,1.03E-06,3.32E-06,3.21E-05,9.12E-06,0.00039335,0.065284771,0.932328328,3.25E-05,2.00E-05,6.11E-06
10861,natural_language_inference15,65,"In word embedding , each word is represented as a ddimensional vector by using a pre-trained word embedding method such as GloVe or Word2vec ) .",Methods,Word Representation Layer,natural_language_inference,15,6,1,0,,0.000266374,0,negative,3.22E-05,0.001285088,0.000677568,1.76E-06,2.26E-06,2.94E-05,8.86E-06,0.000452943,0.011562926,0.985480227,0.000429869,2.80E-05,8.88E-06
10862,natural_language_inference15,66,"In our model , a word embedding vector can be updated or fixed during training .",Methods,Word Representation Layer,natural_language_inference,15,7,1,0,,0.000166304,0,negative,1.64E-05,0.000608587,2.45E-05,5.65E-07,9.22E-07,1.44E-05,2.41E-06,0.00073862,0.009215079,0.989349356,2.08E-05,7.15E-06,1.27E-06
10863,natural_language_inference15,67,The strategy whether to make the pre-trained word embedding be trainable or not is heavily task - dependent .,Methods,Word Representation Layer,natural_language_inference,15,8,1,0,,3.04E-05,0,negative,1.33E-05,1.87E-05,2.27E-06,4.09E-07,2.45E-07,2.26E-06,7.49E-07,2.60E-05,5.26E-05,0.999711189,0.000146975,2.48E-05,5.89E-07
10864,natural_language_inference15,68,Trainable word embeddings capture the characteristics of the training data well but can result in overfitting .,Methods,Word Representation Layer,natural_language_inference,15,9,1,0,,0.000121415,0,negative,3.44E-05,2.77E-05,2.19E-06,2.85E-06,5.56E-07,1.06E-05,1.69E-06,0.0001295,0.000147224,0.999460284,0.00014474,3.47E-05,3.50E-06
10865,natural_language_inference15,69,"On the other hand , fixed ( non-trainable ) word embeddings lack flexibility on task - specific data , while it can be robust for overfitting , especially for less frequent words .",Methods,Word Representation Layer,natural_language_inference,15,10,1,0,,5.25E-05,0,negative,7.86E-05,1.53E-05,2.90E-06,1.00E-06,3.52E-07,3.26E-06,1.01E-06,3.03E-05,6.02E-05,0.99966697,7.47E-05,6.43E-05,1.02E-06
10866,natural_language_inference15,70,We use both the trainable embedding e tr pi and the fixed ( non -trainable ) embedding e fix pi to let them play complementary roles in enhancing the performance of our model .,Methods,Word Representation Layer,natural_language_inference,15,11,1,0,,0.000175265,0,negative,0.000125593,0.000530469,6.24E-05,4.19E-07,1.78E-06,1.60E-05,6.56E-06,0.00051747,0.002746106,0.995943034,2.15E-06,4.72E-05,6.89E-07
10867,natural_language_inference15,71,This technique of mixing trainable and non-trainable word embeddings is simple but yet effective .,Methods,Word Representation Layer,natural_language_inference,15,12,1,0,,0.000190701,0,negative,0.000677235,0.000478236,0.000274281,2.49E-06,1.12E-05,9.33E-06,1.06E-05,4.61E-05,0.001324063,0.995829576,0.000127638,0.001203285,5.91E-06
10868,natural_language_inference15,72,The character representation c pi is calculated by feeding randomly initialized character embeddings into a convolutional neural network with the max - pooling operation .,Methods,Word Representation Layer,natural_language_inference,15,13,1,0,,1.27E-05,0,negative,1.03E-05,0.000146082,1.46E-05,1.24E-07,4.08E-07,4.75E-06,1.21E-06,0.000205675,0.003145965,0.996460234,3.55E-06,6.60E-06,4.90E-07
10869,natural_language_inference15,73,The character embeddings and convolutional weights are jointly learned during training .,Methods,Word Representation Layer,natural_language_inference,15,14,1,0,,0.000472978,0,negative,9.23E-05,0.002785086,5.83E-05,1.18E-06,5.11E-06,1.18E-05,4.55E-06,0.0006509,0.040893142,0.955458441,1.18E-05,2.47E-05,2.73E-06
10870,natural_language_inference15,74,"Like ( Gong , Luo , and Zhang 2018 ) , the exact match flag f pi is activated if the same word is found in the other sentence .",Methods,Word Representation Layer,natural_language_inference,15,15,1,0,,7.91E-06,0,negative,1.89E-05,2.95E-05,9.85E-05,2.82E-08,1.70E-07,1.10E-06,3.82E-07,1.25E-05,0.00192294,0.997905406,1.49E-06,8.80E-06,2.07E-07
10871,natural_language_inference15,75,Our final word representational feature p w i for the word pi is composed of four components as follows :,Methods,Word Representation Layer,natural_language_inference,15,16,1,0,,2.71E-06,0,negative,2.32E-05,3.18E-05,4.79E-05,8.61E-08,3.75E-07,1.52E-06,5.09E-07,1.46E-05,0.001881434,0.997985574,1.37E-06,1.13E-05,3.80E-07
10872,natural_language_inference15,76,-1,Methods,Word Representation Layer,natural_language_inference,15,17,1,0,,1.73E-06,0,negative,4.09E-06,3.97E-06,1.17E-06,7.92E-08,1.03E-07,9.07E-07,2.34E-07,1.39E-05,0.000115873,0.999855159,8.79E-07,3.47E-06,1.62E-07
10873,natural_language_inference15,77,"Here , E tr and E fix are the trainable and non-trainable ( fixed ) word embeddings respectively .",Methods,Word Representation Layer,natural_language_inference,15,18,1,0,,1.39E-06,0,negative,1.48E-06,1.37E-05,1.06E-06,2.46E-08,6.48E-08,1.47E-06,2.55E-07,4.87E-05,0.000268549,0.999662731,4.61E-07,1.46E-06,7.38E-08
10874,natural_language_inference15,78,Char - Conv is the characterlevel convolutional operation and [ ; ] is the concatenation operator .,Methods,Word Representation Layer,natural_language_inference,15,19,1,0,,3.51E-05,0,negative,4.69E-06,4.94E-05,1.26E-05,3.89E-07,4.89E-07,1.24E-05,1.74E-06,0.000245784,0.00186393,0.997803616,1.94E-06,2.14E-06,1.01E-06
10875,natural_language_inference15,79,"For each word in both sentences , the same above procedure is used to extract word features .",Methods,Word Representation Layer,natural_language_inference,15,20,1,0,,8.09E-06,0,negative,8.86E-06,3.82E-05,1.58E-05,1.29E-08,1.43E-07,7.69E-07,3.00E-07,1.49E-05,0.001238508,0.998677803,4.03E-07,4.17E-06,1.24E-07
10876,natural_language_inference15,80,Densely connected,Methods,,natural_language_inference,15,21,1,0,,0.145756176,0,negative,0.006505227,0.004606528,0.103815822,0.000285509,0.000515008,0.002086297,0.003815026,0.022059097,0.020588617,0.818272367,0.000666189,0.015435332,0.00134898
10877,natural_language_inference15,81,Recurrent Networks,Methods,,natural_language_inference,15,22,1,0,,0.207987549,0,negative,0.000342447,0.004077663,0.078294439,3.90E-05,4.54E-05,0.00145401,0.014098807,0.027072027,0.030685454,0.737630607,0.084280471,0.019792466,0.002187196
10878,natural_language_inference15,82,"The ordinal stacked RNNs ( Recurrent Neural Networks ) are composed of multiple RNN layers on top of each other , with the output sequence of previous layer forming the input sequence for the next .",Methods,Recurrent Networks,natural_language_inference,15,23,1,0,,0.002320099,0,negative,1.85E-05,4.70E-05,0.004113543,1.74E-07,4.49E-07,7.32E-06,1.61E-05,2.46E-05,0.002135704,0.99356386,2.41E-05,4.52E-05,3.42E-06
10879,natural_language_inference15,83,"More concretely , let H l be the l th RNN layer in a stacked RNN .",Methods,Recurrent Networks,natural_language_inference,15,24,1,0,,2.85E-07,0,negative,7.03E-07,5.55E-07,7.46E-07,1.54E-09,6.66E-09,1.71E-07,1.05E-07,2.69E-06,1.24E-05,0.999980885,6.61E-08,1.67E-06,6.93E-09
10880,natural_language_inference15,84,"Note that in our implementation , we employ the bidirectional LSTM ( BiLSTM ) as a base block of H l .",Methods,Recurrent Networks,natural_language_inference,15,25,1,0,,2.32E-05,0,negative,9.90E-06,3.77E-06,1.48E-05,1.56E-08,7.57E-08,1.56E-06,1.04E-06,1.40E-05,6.09E-05,0.999888687,8.00E-08,5.17E-06,6.41E-08
10881,natural_language_inference15,85,"At the time step t , an ordinal stacked RNN is expressed as follows :",Methods,Recurrent Networks,natural_language_inference,15,26,1,0,,9.81E-07,0,negative,1.78E-06,3.07E-06,1.59E-05,1.54E-09,1.03E-08,1.63E-07,2.46E-07,1.72E-06,0.000158451,0.999810811,8.78E-07,6.94E-06,2.73E-08
10882,natural_language_inference15,86,"While this architecture enables us to buildup higher level representation , deeper networks have difficulties in training due to the exploding or vanishing gradient problem .",Methods,Recurrent Networks,natural_language_inference,15,27,1,0,,4.66E-06,0,negative,8.26E-05,2.71E-06,1.12E-05,2.46E-08,9.68E-08,5.25E-07,5.31E-07,3.00E-06,2.48E-05,0.999806733,6.98E-07,6.71E-05,9.53E-08
10883,natural_language_inference15,87,"To encourage gradient to flow in the backward pass , residual connection ) is introduced which bypasses the non-linear transformations with an identity mapping .",Methods,Recurrent Networks,natural_language_inference,15,28,1,0,,0.000549533,0,negative,0.000752448,0.000262828,0.00053113,3.81E-07,2.57E-06,5.93E-06,9.47E-06,6.99E-05,0.003434321,0.994833843,1.11E-06,9.47E-05,1.37E-06
10884,natural_language_inference15,88,"Incorporating this into ( 2 ) , it becomes",Methods,Recurrent Networks,natural_language_inference,15,29,1,0,,1.86E-06,0,negative,4.28E-05,2.63E-06,3.49E-06,5.42E-08,1.02E-07,7.49E-07,8.64E-07,5.88E-06,4.04E-05,0.999857434,2.41E-07,4.52E-05,1.05E-07
10885,natural_language_inference15,89,-3,Methods,Recurrent Networks,natural_language_inference,15,30,1,0,,6.55E-06,0,negative,3.37E-06,4.31E-07,5.47E-07,1.56E-08,2.47E-08,5.43E-07,3.43E-07,6.25E-06,7.26E-06,0.999974341,7.64E-08,6.75E-06,4.72E-08
10886,natural_language_inference15,90,"However , the summation operation in the residual connection may impede the information flow in the network .",Methods,Recurrent Networks,natural_language_inference,15,31,1,0,,2.17E-05,0,negative,0.000104605,3.79E-07,3.15E-06,4.51E-09,3.92E-08,1.62E-07,3.09E-07,7.60E-07,3.21E-06,0.999743545,7.07E-08,0.000143742,1.81E-08
10887,natural_language_inference15,91,"Motivated by Densenet ) , we employ direct connections using the concatenation operation from any layer to all the subsequent layers so that the features of previous layers are not to be modified but to be retained as they are as depicted in .",Methods,Recurrent Networks,natural_language_inference,15,32,1,0,,0.000354011,0,negative,6.15E-05,3.44E-05,0.000654333,2.20E-08,3.37E-07,1.00E-06,3.25E-06,5.20E-06,0.000285943,0.998866402,8.84E-07,8.64E-05,2.73E-07
10888,natural_language_inference15,92,The densely connected recurrent neural networks can be described ash l,Methods,Recurrent Networks,natural_language_inference,15,33,1,0,,2.74E-06,0,negative,1.62E-06,7.01E-07,1.08E-05,1.19E-09,9.71E-09,2.09E-07,2.47E-07,1.73E-06,4.75E-05,0.999932654,9.01E-08,4.42E-06,2.32E-08
10889,natural_language_inference15,93,Bottleneck component,Methods,,natural_language_inference,15,34,1,0,,0.322821225,0,negative,0.003475001,0.006116435,0.074437425,0.000332741,0.000631375,0.003571402,0.007393729,0.055110825,0.082649788,0.756689551,0.000254097,0.005140345,0.004197286
10890,natural_language_inference15,94,Our network uses all layers ' outputs as a community of semantic knowledge .,Methods,Bottleneck component,natural_language_inference,15,35,1,0,,8.85E-05,0,negative,0.000172122,0.00119821,0.004071846,5.06E-07,4.36E-05,2.16E-05,9.98E-06,3.35E-05,0.158611459,0.835814911,3.96E-06,8.78E-06,9.61E-06
10891,natural_language_inference15,95,"However , this network is a structure with increasing input features as layers get deeper , and has a large number of parameters especially in the fully - connected layer .",Methods,Bottleneck component,natural_language_inference,15,36,1,0,,1.54E-05,0,negative,7.30E-05,6.19E-05,0.000261227,5.66E-07,1.35E-05,1.56E-05,4.56E-06,1.64E-05,0.000635928,0.998892976,1.05E-05,9.76E-06,4.12E-06
10892,natural_language_inference15,96,"To address this issue , we employ an autoencoder as a bottleneck component .",Methods,Bottleneck component,natural_language_inference,15,37,1,0,,0.009202839,0,negative,0.00173521,0.002855817,0.005802146,9.59E-07,6.98E-05,3.45E-05,2.35E-05,5.16E-05,0.129826128,0.859550488,4.12E-06,3.42E-05,1.15E-05
10893,natural_language_inference15,97,"Autoencoder is a compression technique that reduces the number of features while retaining the original information , which can be used as a distilled semantic knowledge in our model .",Methods,Bottleneck component,natural_language_inference,15,38,1,0,,0.001661928,0,negative,0.00043498,0.002203983,0.046378666,5.43E-06,0.000188609,0.000133287,8.27E-05,9.71E-05,0.072360801,0.877908379,3.83E-05,3.76E-05,0.000130185
10894,natural_language_inference15,98,"Furthermore , this component increased the test performance by working as a regularizer in our experiments .",Methods,Bottleneck component,natural_language_inference,15,39,1,0,,1.40E-05,0,negative,0.013353008,2.35E-05,0.000177182,3.00E-07,3.60E-05,8.87E-06,1.02E-05,4.34E-06,0.00016666,0.986017908,2.21E-07,0.000200659,1.23E-06
10895,natural_language_inference15,99,Interaction and Prediction Layer,Methods,,natural_language_inference,15,40,1,0,,0.63677934,1,negative,0.005247656,0.006897145,0.088214447,0.001317747,0.001709824,0.003546893,0.00818819,0.041977538,0.110947368,0.713152057,0.000284601,0.006674519,0.011842015
10896,natural_language_inference15,100,"To extract a proper representation for each sentence , we apply the step - wise max - pooling operation over densely connected recurrent and co-attentive features ( pooling in ) .",Methods,Interaction and Prediction Layer,natural_language_inference,15,41,1,0,,0.001775583,0,negative,0.000322281,0.00550526,0.000269607,6.98E-07,6.08E-05,4.39E-05,3.25E-05,0.000138265,0.111683027,0.881926872,1.73E-06,1.12E-05,3.80E-06
10897,natural_language_inference15,101,"More specifically , if the output of the final RNN layer is a 100d vector for a sentence with 30 words , a 30 100 matrix is obtained which is max - pooled column - wise such that the size of the resultant vector p or q is 100 .",Methods,Interaction and Prediction Layer,natural_language_inference,15,42,1,0,,1.61E-05,0,negative,5.83E-06,0.000122095,1.54E-06,1.50E-07,1.49E-05,8.18E-05,2.00E-05,0.000377152,0.000742489,0.998630126,1.98E-07,3.02E-06,7.71E-07
10898,natural_language_inference15,102,"Then , we aggregate these representations p and q for the two sentences P and Q in various ways in the interaction layer and the final feature vector v for semantic sentence matching is obtained as follows :",Methods,Interaction and Prediction Layer,natural_language_inference,15,43,1,0,,3.20E-06,0,negative,9.09E-06,5.44E-05,6.59E-06,4.19E-09,8.04E-07,6.43E-07,5.52E-07,2.28E-06,0.002189765,0.997732828,1.23E-07,2.85E-06,3.53E-08
10899,natural_language_inference15,103,"Here , the operations +, ? and | | are performed elementwise to infer the relationship between two sentences .",Methods,Interaction and Prediction Layer,natural_language_inference,15,44,1,0,,1.80E-05,0,negative,3.52E-06,7.84E-05,3.51E-06,1.05E-07,4.28E-06,4.48E-06,1.05E-06,1.56E-05,0.00205667,0.997831502,1.18E-07,6.06E-07,1.54E-07
10900,natural_language_inference15,104,The element - wise subtraction p ?,Methods,Interaction and Prediction Layer,natural_language_inference,15,45,1,0,,5.76E-06,0,negative,7.44E-06,1.57E-05,4.21E-06,4.57E-09,7.27E-07,1.39E-06,1.08E-06,3.18E-06,0.000369073,0.999594994,8.61E-08,2.13E-06,3.63E-08
10901,natural_language_inference15,105,q is an asymmetric operator for one - way type tasks such as natural language inference or answer sentence selection .,Methods,Interaction and Prediction Layer,natural_language_inference,15,46,1,0,,7.99E-06,0,negative,6.54E-06,0.000203226,1.89E-05,1.54E-07,6.65E-06,5.80E-06,3.23E-06,1.87E-05,0.002138277,0.997588048,5.22E-06,4.34E-06,8.90E-07
10902,natural_language_inference15,106,"Finally , based on previously aggregated features v , we use two fully - connected layers with ReLU activation followed by one fully - connected output layer .",Methods,Interaction and Prediction Layer,natural_language_inference,15,47,1,0,,0.001287468,0,negative,0.000305767,0.007452763,0.000481167,3.17E-06,0.000178659,0.000207617,0.000175609,0.000746365,0.185628487,0.804773873,1.61E-06,1.57E-05,2.92E-05
10903,natural_language_inference15,107,"Then , the softmax function is applied to obtain a probability distribution of each class .",Methods,Interaction and Prediction Layer,natural_language_inference,15,48,1,0,,0.000199542,0,negative,1.02E-05,0.000309237,2.37E-05,6.66E-09,1.26E-06,1.44E-06,1.20E-06,8.54E-06,0.034733714,0.964909178,1.52E-07,1.26E-06,1.05E-07
10904,natural_language_inference15,108,The model is trained end - to - end by minimizing the multi-class cross entropy loss and the reconstruction loss of autoencoders .,Methods,Interaction and Prediction Layer,natural_language_inference,15,49,1,0,,0.004728656,0,negative,3.76E-05,0.009282684,8.60E-05,1.95E-07,3.01E-05,1.56E-05,1.84E-05,0.00016241,0.183067546,0.807290624,1.06E-06,5.58E-06,2.16E-06
10905,natural_language_inference15,109,Experiments,,,natural_language_inference,15,0,1,0,,0.019539799,0,negative,4.20E-05,6.87E-05,9.63E-07,2.19E-06,1.64E-06,0.000158328,7.20E-05,0.003102128,2.70E-05,0.995415641,0.000755437,0.000350405,3.60E-06
10906,natural_language_inference15,110,We evaluate our matching model on five popular and wellstudied benchmark datasets for three challenging sentence matching tasks : ( i ) SNLI and MultiNLI for natural language inference ; ( ii ) Quora Question Pair for paraphrase identification ; and ( iii ) TrecQA and SelQA for answer sentence selection in question answering .,Experiments,Experiments,natural_language_inference,15,1,1,0,,0.090192888,0,negative,0.001087672,0.008547476,0.006660106,5.82E-05,0.00021577,0.007903421,0.01491376,0.083754297,0.000332206,0.85904906,0.001531205,0.014989828,0.000956967
10907,natural_language_inference15,111,Additional details about the above datasets can be found in the supplementary materials .,Experiments,Experiments,natural_language_inference,15,2,1,0,,0.000586101,0,negative,5.66E-05,2.50E-05,2.50E-05,4.45E-05,1.83E-05,0.002747474,0.000270257,0.01315295,1.85E-05,0.983442019,1.76E-05,0.000149079,3.26E-05
10908,natural_language_inference15,112,Implementation Details,,,natural_language_inference,15,0,1,0,,0.045796543,0,negative,4.47E-05,0.001220595,9.47E-06,3.20E-05,1.55E-05,0.000465852,0.000169332,0.007430528,0.000344669,0.986360185,0.003666023,0.000225373,1.58E-05
10909,natural_language_inference15,113,"We initialized word embedding with 300d Glo Ve vectors pre-trained from the 840B Common Crawl corpus ( Pennington , Socher , and Manning 2014 ) , while the word embeddings for the out - of - vocabulary words were initialized randomly .",Implementation Details,Implementation Details,natural_language_inference,15,1,1,1,hyperparameters,0.995090369,1,experimental-setup,6.86E-06,1.73E-05,4.89E-06,1.86E-06,4.92E-07,0.521825712,0.000503453,0.471227971,1.22E-05,0.006382244,3.99E-06,4.47E-06,8.49E-06
10910,natural_language_inference15,114,We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network .,Implementation Details,Implementation Details,natural_language_inference,15,2,1,1,hyperparameters,0.977372717,1,experimental-setup,1.52E-05,2.77E-05,4.26E-05,3.34E-06,1.45E-06,0.640106703,0.001251557,0.34674468,2.32E-05,0.011753151,3.59E-06,1.05E-05,1.63E-05
10911,natural_language_inference15,115,"For the densely - connected recurrent layers , we stacked 5 layers each of which have 100 hidden units .",Implementation Details,Implementation Details,natural_language_inference,15,3,1,1,hyperparameters,0.987973849,1,experimental-setup,3.10E-05,3.01E-05,2.07E-05,1.49E-05,2.08E-06,0.648327563,0.001771957,0.345485774,2.14E-05,0.004236741,4.60E-06,1.21E-05,4.10E-05
10912,natural_language_inference15,116,We set 1000 hidden units with respect to the fullyconnected layers .,Implementation Details,Implementation Details,natural_language_inference,15,4,1,1,hyperparameters,0.984351495,1,hyperparameters,1.77E-05,2.38E-05,8.29E-06,3.96E-06,8.34E-07,0.474428417,0.000842998,0.520645648,1.86E-05,0.003974828,4.60E-06,7.37E-06,2.30E-05
10913,natural_language_inference15,117,The dropout was applied after the word and character embedding layers with a keep rate of 0.5 .,Implementation Details,Implementation Details,natural_language_inference,15,5,1,1,hyperparameters,0.993519086,1,hyperparameters,2.46E-05,3.02E-05,8.69E-06,1.19E-05,1.34E-06,0.483602322,0.000784972,0.512679388,2.11E-05,0.002799157,2.74E-06,5.73E-06,2.78E-05
10914,natural_language_inference15,118,It was also applied before the fully - connected layers with a keep rate of 0.8 .,Implementation Details,Implementation Details,natural_language_inference,15,6,1,1,hyperparameters,0.96980664,1,experimental-setup,0.000242453,8.52E-05,0.000111195,8.71E-06,4.09E-06,0.551270366,0.001477557,0.426285188,6.68E-05,0.020340089,7.04E-06,6.33E-05,3.81E-05
10915,natural_language_inference15,119,"For the bottleneck component , we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2 .",Implementation Details,Implementation Details,natural_language_inference,15,7,1,1,hyperparameters,0.992337823,1,hyperparameters,1.67E-05,3.38E-05,9.30E-06,3.41E-06,9.14E-07,0.40262802,0.000849494,0.592709102,2.31E-05,0.003694236,3.04E-06,6.39E-06,2.26E-05
10916,natural_language_inference15,120,"The batch normalization was applied on the fully - connected layers , only for the one - way type datasets .",Implementation Details,Implementation Details,natural_language_inference,15,8,1,1,hyperparameters,0.97501145,1,hyperparameters,0.000102095,0.000108712,4.67E-05,9.57E-06,4.31E-06,0.488492468,0.000925523,0.492760606,7.70E-05,0.017419521,3.07E-06,1.90E-05,3.15E-05
10917,natural_language_inference15,121,The RMSProp optimizer with an initial learning rate of 0.001 was applied .,Implementation Details,Implementation Details,natural_language_inference,15,9,1,1,hyperparameters,0.993425203,1,experimental-setup,8.27E-06,1.53E-05,6.03E-06,3.39E-06,5.31E-07,0.507910993,0.000692391,0.488238297,1.18E-05,0.003089671,2.06E-06,4.45E-06,1.68E-05
10918,natural_language_inference15,122,The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve .,Implementation Details,Implementation Details,natural_language_inference,15,10,1,1,hyperparameters,0.969052998,1,hyperparameters,0.000104719,7.38E-05,1.45E-05,1.28E-05,4.83E-06,0.39916571,0.001015402,0.591711996,3.73E-05,0.007773989,3.84E-06,2.62E-05,5.50E-05
10919,natural_language_inference15,123,All weights except embedding matrices are constrained by L2 regularization with a regularization constant ? = 10 ?6 .,Implementation Details,Implementation Details,natural_language_inference,15,11,1,1,hyperparameters,0.993681755,1,hyperparameters,1.31E-05,2.77E-05,4.92E-06,2.01E-06,6.75E-07,0.361945996,0.000493399,0.632002492,1.66E-05,0.00547125,2.08E-06,6.71E-06,1.32E-05
10920,natural_language_inference15,124,"The sequence lengths of the sentence are all different for each dataset : 35 for SNLI , 55 for MultiNLI , 25 for Quora question pair and 50 for TrecQA .",Implementation Details,Implementation Details,natural_language_inference,15,12,1,1,hyperparameters,0.733634498,1,experimental-setup,4.16E-05,6.40E-05,2.68E-05,1.02E-05,1.76E-05,0.526454093,0.002599691,0.374708982,2.10E-05,0.095929929,5.68E-06,7.91E-05,4.12E-05
10921,natural_language_inference15,125,The learning parameters were selected based on the best performance on the dev set .,Implementation Details,Implementation Details,natural_language_inference,15,13,1,0,,0.284121698,0,hyperparameters,1.42E-05,2.90E-05,5.31E-06,1.98E-06,9.93E-07,0.375971898,0.000367386,0.603122368,2.51E-05,0.020438371,2.39E-06,1.06E-05,1.03E-05
10922,natural_language_inference15,126,We employed 8 different randomly initialized sets of parameters with the same model for our ensemble approach .,Implementation Details,Implementation Details,natural_language_inference,15,14,1,0,,0.748971771,1,experimental-setup,1.68E-05,4.25E-05,7.72E-06,4.61E-06,2.43E-06,0.495879215,0.001199126,0.490543937,1.90E-05,0.012231574,3.14E-06,2.03E-05,2.96E-05
10923,natural_language_inference15,127,Experimental Results,,,natural_language_inference,15,0,1,0,,0.010987357,0,negative,0.000353352,0.00011777,2.16E-05,3.98E-07,1.02E-06,3.15E-05,0.000609269,0.000558707,1.70E-05,0.933411653,0.009870816,0.054999552,7.39E-06
10924,natural_language_inference15,128,SNLI and MultiNLI,Experimental Results,Experimental Results,natural_language_inference,15,1,1,0,,0.755681533,1,results,0.002143192,2.93E-07,0.002405116,1.09E-07,4.66E-08,6.02E-06,0.000340331,1.66E-05,4.99E-07,0.379047056,6.66E-06,0.616001784,3.23E-05
10925,natural_language_inference15,129,We evaluated our model on the natural language inference task over SNLI and MultiNLI datasets .,Experimental Results,Experimental Results,natural_language_inference,15,2,1,0,,0.386741236,0,negative,0.001082151,3.80E-06,0.000166856,7.79E-07,6.37E-07,2.22E-05,0.000550425,8.24E-05,1.34E-06,0.752202776,1.12E-05,0.245805016,7.04E-05
10926,natural_language_inference15,130,shows the results on SNLI dataset of our model with other published models .,Experimental Results,Experimental Results,natural_language_inference,15,3,1,0,,0.490139931,0,results,0.000875782,6.21E-08,5.28E-05,7.26E-08,3.02E-08,3.84E-06,0.000241768,1.18E-05,6.25E-08,0.285796937,1.35E-06,0.71299589,1.96E-05
10927,natural_language_inference15,131,"Among them , ESIM + ELMo and LM - Transformer are the current state - of - the - art models .",Experimental Results,Experimental Results,natural_language_inference,15,4,1,0,,0.968366626,1,negative,0.001712811,5.17E-06,0.007481072,1.42E-05,1.18E-06,0.000257563,0.002968245,0.000518934,9.33E-06,0.715933644,0.000207475,0.268206543,0.002683812
10928,natural_language_inference15,132,"However , they use additional contextualized word representations from language models as an externel knowledge .",Experimental Results,Experimental Results,natural_language_inference,15,5,1,0,,0.005237644,0,negative,0.003148197,1.83E-06,0.000465976,5.08E-06,4.71E-07,5.40E-05,0.000124551,0.000110349,6.52E-06,0.952090079,2.09E-05,0.043786023,0.000186015
10929,natural_language_inference15,133,The proposed DRCN obtains an accuracy of 88.9 % which is a competitive score although we do not use any external knowledge like ESIM + ELMo and LM - Transformer .,Experimental Results,Experimental Results,natural_language_inference,15,6,1,1,results,0.975702394,1,results,0.000667611,2.45E-08,6.05E-06,6.47E-08,1.61E-08,7.52E-07,0.000359913,2.91E-06,1.16E-08,0.009384362,4.93E-07,0.98952346,5.43E-05
10930,natural_language_inference15,134,"The ensemble model achieves an accuracy of 90.1 % , which sets the new state - of the - art performance .",Experimental Results,Experimental Results,natural_language_inference,15,7,1,1,results,0.982089218,1,results,0.000705542,3.97E-08,1.05E-05,1.66E-07,2.60E-08,1.48E-06,0.000569157,5.58E-06,2.78E-08,0.00957817,6.11E-07,0.988998978,0.000129681
10931,natural_language_inference15,135,Our ensemble model with 53 m parameters ( 6.7 m 8 ) outperforms the LM - Transformer whose the number of parameters is 85 m .,Experimental Results,Experimental Results,natural_language_inference,15,8,1,1,results,0.977542769,1,results,0.001706278,1.45E-07,2.23E-05,2.91E-07,5.44E-08,4.68E-06,0.000849242,2.23E-05,7.84E-08,0.021344893,8.72E-07,0.975918419,0.000130402
10932,natural_language_inference15,136,"Furthermore , in case of the encoding - based method , we obtain the best performance of 86.5 % without the co-attention and exact match flag .",Experimental Results,Experimental Results,natural_language_inference,15,9,1,1,results,0.98889174,1,results,0.0025832,3.20E-08,6.09E-06,5.24E-08,1.30E-08,6.32E-07,0.000281292,3.70E-06,1.60E-08,0.012157298,2.36E-07,0.984943632,2.38E-05
10933,natural_language_inference15,137,shows the results on MATCHED and MISMATCHED problems of MultiNLI dataset .,Experimental Results,Experimental Results,natural_language_inference,15,10,1,1,results,0.774772947,1,results,0.000755418,2.20E-08,4.40E-05,1.75E-08,1.19E-08,5.59E-07,0.000174668,1.87E-06,1.10E-08,0.066132396,4.30E-07,0.932879597,1.10E-05
10934,natural_language_inference15,138,Our plain DRCN has a competitive performance without any contextualized knowledge .,Experimental Results,Experimental Results,natural_language_inference,15,11,1,1,results,0.767831788,1,results,0.000557436,1.30E-08,6.81E-06,1.95E-08,6.11E-09,4.26E-07,0.00023295,2.13E-06,5.74E-09,0.009799371,1.91E-07,0.989381093,1.96E-05
10935,natural_language_inference15,139,"And , by combining DRCN with the ELMo , one of the contextualized embeddings from language models , our model outperforms the LM - Transformer which has 85 m parameters with fewer parameters of 61 m .",Experimental Results,Experimental Results,natural_language_inference,15,12,1,1,results,0.981458686,1,results,0.002617816,3.68E-08,8.54E-06,2.11E-07,2.49E-08,1.52E-06,0.000517555,5.96E-06,2.96E-08,0.010189222,2.58E-07,0.986558573,0.000100244
10936,natural_language_inference15,140,"From this point of view , the combination of our model with a contextualized knowledge Models Acc .",Experimental Results,Experimental Results,natural_language_inference,15,13,1,0,,0.126619706,0,results,0.017808036,4.02E-07,0.000295007,2.21E-07,1.32E-07,3.29E-06,9.43E-05,8.69E-06,8.66E-07,0.393727414,1.75E-06,0.588032017,2.79E-05
10937,natural_language_inference15,141,|? |,Experimental Results,Experimental Results,natural_language_inference,15,14,1,0,,0.002075905,0,negative,0.000402894,1.95E-07,2.74E-05,6.75E-07,9.12E-08,1.26E-05,3.10E-05,4.11E-05,1.53E-06,0.987291558,7.36E-07,0.012155897,3.43E-05
10938,natural_language_inference15,142,Sentence encoding - based method,Experimental Results,Experimental Results,natural_language_inference,15,15,1,0,,0.953897659,1,results,0.000924483,1.65E-05,0.002301137,5.97E-06,7.51E-07,5.15E-05,0.00309507,0.000301734,2.00E-05,0.346386202,0.001928245,0.638981943,0.005986556
10939,natural_language_inference15,143,BiLSTM - Max 84.5 40 m Gumbel TreeLSTM 85.6 2.9 m CAFE 85.9 3.7 m Gumbel TreeLSTM 86.0 10 m Residual stacked 86.0 29 m Reinforced SAN 86.3 3.1 m Distance SAN 86 is a good option to enhance the performance .,Experimental Results,Experimental Results,natural_language_inference,15,16,1,0,,0.023965214,0,results,0.019879883,1.60E-07,0.000110765,7.37E-07,1.34E-07,9.21E-06,0.000393166,2.09E-05,4.05E-07,0.114758392,4.99E-07,0.864727635,9.81E-05
10940,natural_language_inference15,144,Quora Question,Experimental Results,,natural_language_inference,15,17,1,1,results,0.002269941,0,negative,0.000382587,7.69E-07,3.91E-05,4.20E-06,2.76E-07,3.44E-05,0.000142485,0.000125597,4.14E-06,0.963354433,1.59E-05,0.035415628,0.000480469
10941,natural_language_inference15,145,Pair shows our results on the Quora question pair dataset .,Experimental Results,Quora Question,natural_language_inference,15,18,1,1,results,0.328521468,0,results,0.000170228,3.35E-08,7.88E-05,1.04E-07,7.12E-08,1.36E-06,0.000322586,1.67E-06,2.58E-08,0.249057359,7.99E-07,0.750062822,0.000304183
10942,natural_language_inference15,146,BiMPM using the multiperspective matching technique between two sentences reports baseline performance of a L.D.C. network and basic multi-perspective models .,Experimental Results,Quora Question,natural_language_inference,15,19,1,0,,0.049382738,0,results,0.00031297,2.90E-08,0.000306063,1.82E-08,8.18E-09,4.01E-07,0.000161157,6.80E-07,2.01E-08,0.149255364,1.79E-06,0.849904851,5.66E-05
10943,natural_language_inference15,147,"We obtained accuracies of 90.15 % and 91.30 % in single and ensemble methods , respectively , surpassing the previous state - of - the - art model of DIIN .",Experimental Results,Quora Question,natural_language_inference,15,20,1,1,results,0.236608791,0,results,0.000226242,1.12E-08,4.41E-06,3.33E-08,8.15E-09,4.14E-07,0.000175436,1.00E-06,8.45E-09,0.046640527,1.41E-07,0.952831509,0.000120259
10944,natural_language_inference15,148,TrecQA and SelQA shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question .,Experimental Results,Quora Question,natural_language_inference,15,21,1,1,results,0.016276131,0,results,0.000132761,5.78E-08,0.00015944,1.81E-07,6.98E-08,2.15E-06,0.000434753,1.97E-06,4.39E-08,0.373509313,7.68E-06,0.624861399,0.000890179
10945,natural_language_inference15,149,Most competitive models ) also use attention methods for words alignment between question and candidate answer sentences .,Experimental Results,Quora Question,natural_language_inference,15,22,1,0,,0.00030818,0,negative,0.000111512,9.57E-08,0.000124748,2.30E-07,2.86E-08,5.97E-06,3.76E-05,1.07E-05,3.27E-07,0.986451879,2.57E-06,0.013113354,0.00014091
10946,natural_language_inference15,150,"However , the proposed DRCN using collective attentions over multiple layers , achieves the new state - of the - art performance , exceeding the current state - of - the - art performance significantly on both datasets .",Experimental Results,Quora Question,natural_language_inference,15,23,1,1,results,0.247286067,0,results,0.000748657,2.26E-08,7.79E-06,3.84E-08,9.47E-09,6.59E-07,0.000197104,2.72E-06,1.63E-08,0.071836214,1.95E-07,0.927140919,6.57E-05
10947,natural_language_inference15,151,Analysis,Experimental Results,,natural_language_inference,15,24,1,0,,0.00132444,0,negative,0.000570306,6.45E-07,2.16E-05,2.80E-06,2.07E-07,2.53E-05,4.09E-05,0.000130966,6.54E-06,0.993585912,1.02E-06,0.005514656,9.91E-05
10948,natural_language_inference15,152,Ablation study,,,natural_language_inference,15,0,1,0,,0.006818876,0,negative,0.046739945,0.000279772,0.002808551,0.000142874,8.76E-05,0.000276199,0.001210224,0.00046093,0.000147782,0.934438164,0.002965146,0.010376843,6.60E-05
10949,natural_language_inference15,153,"We conducted an ablation study on the SNLI dev set as shown in , where we aim to exam-Models Accuracy ( % ) Siamese- LSTM 82.58 MP LSTM 83.21 L.D.C. 85.55 BiMPM 88.17 pt-DecAttchar.c 88.40 DIIN 89.06 DRCN 90.15 DIIN * 89.84 DRCN * 91.30 0.750 0.811 PWIM 0.758 0.822 MP CNN 0.762 0.830 HyperQA 0.770 0.825 PR+CNN 0.780 0.834 DRCN 0.804 0.862 clean version HyperQA 0.801 0.877 PR+CNN 0.801 0.877 BiMPM : Performance for answer sentence selection on TrecQA and se l QA test set .",Ablation study,Ablation study,natural_language_inference,15,1,1,0,,0.040378078,0,ablation-analysis,0.903387134,2.09E-06,0.002017451,1.77E-07,4.89E-06,7.42E-06,6.43E-05,3.40E-07,8.64E-06,0.094386754,1.92E-05,2.99E-05,7.17E-05
10950,natural_language_inference15,154,ine the effectiveness of our word embedding technique as well as the proposed densely - connected recurrent and coattentive features .,Ablation study,Ablation study,natural_language_inference,15,2,1,0,,0.377390904,0,ablation-analysis,0.74458413,6.50E-07,4.30E-05,4.41E-07,3.66E-06,1.06E-05,3.36E-05,1.41E-06,1.97E-06,0.255195357,8.28E-06,2.12E-05,9.57E-05
10951,natural_language_inference15,155,"Firstly , we verified the effectiveness of the autoencoder as a bottleneck component in .",Ablation study,Ablation study,natural_language_inference,15,3,1,0,,0.452330023,0,ablation-analysis,0.800385055,9.84E-07,7.73E-05,5.08E-07,8.05E-06,6.17E-06,1.04E-05,3.21E-07,4.62E-06,0.199482078,2.13E-06,7.76E-06,1.46E-05
10952,natural_language_inference15,156,"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the performance was rather higher because of the regularization effect .",Ablation study,Ablation study,natural_language_inference,15,4,1,1,ablation-analysis,0.691015405,1,ablation-analysis,0.996857848,1.25E-07,2.76E-06,1.09E-07,4.07E-07,1.26E-06,9.34E-06,1.18E-07,3.88E-07,0.003105527,7.33E-07,3.20E-06,1.82E-05
10953,natural_language_inference15,157,"Secondly , we study how the technique of mixing trainable and fixed word embeddings contributes to the performance in models ( 3 - 4 ) .",Ablation study,Ablation study,natural_language_inference,15,5,1,0,,0.799913965,1,ablation-analysis,0.87266221,0.000118228,0.000755065,3.94E-06,6.39E-05,3.50E-05,6.55E-05,5.06E-06,0.000162606,0.125978091,1.38E-05,1.74E-05,0.000119287
10954,natural_language_inference15,158,"After removing E tr or E fix in eq. ( 1 ) , the performance degraded , slightly .",Ablation study,Ablation study,natural_language_inference,15,6,1,0,,0.193533514,0,ablation-analysis,0.987388889,1.16E-07,2.03E-05,1.15E-07,7.17E-07,1.14E-06,1.98E-06,4.74E-08,1.36E-06,0.012577369,5.08E-07,1.04E-06,6.39E-06
10955,natural_language_inference15,159,The trainable embedding E tr seems more effective than the fixed embedding E fix .,Ablation study,Ablation study,natural_language_inference,15,7,1,0,,0.860661293,1,ablation-analysis,0.995362788,6.06E-07,1.19E-05,1.17E-06,1.95E-06,1.31E-05,8.41E-05,1.64E-06,1.33E-06,0.004096779,1.96E-06,1.44E-05,0.000408175
10956,natural_language_inference15,160,"Next , the effectiveness of dense connections was tested in models ( 5 - 9 ) .",Ablation study,Ablation study,natural_language_inference,15,8,1,0,,0.253047557,0,ablation-analysis,0.755580203,6.63E-06,0.000850484,4.28E-07,9.78E-06,2.20E-05,4.42E-05,2.10E-06,4.80E-05,0.243389476,3.29E-06,8.25E-06,3.52E-05
10957,natural_language_inference15,161,"In ( 5 - 6 ) , we removed dense connections only over co-attentive or recurrent features , respectively .",Ablation study,Ablation study,natural_language_inference,15,9,1,0,,0.082934815,0,ablation-analysis,0.919745277,2.19E-05,0.002169814,4.06E-07,9.61E-06,2.23E-05,2.81E-05,2.95E-06,0.000179592,0.07777551,1.82E-06,3.60E-06,3.91E-05
10958,natural_language_inference15,162,The result shows that the dense connections over attentive features are more effective .,Ablation study,Ablation study,natural_language_inference,15,10,1,1,ablation-analysis,0.763043266,1,ablation-analysis,0.995284018,2.54E-07,5.24E-06,8.02E-08,4.22E-07,1.55E-06,1.97E-05,2.84E-07,1.04E-06,0.00465801,3.34E-07,6.82E-06,2.23E-05
10959,natural_language_inference15,163,"In , we removed dense connections over both co-attentive and recurrent features , and the performance degraded to 88.5 % .",Ablation study,Ablation study,natural_language_inference,15,11,1,1,ablation-analysis,0.904787896,1,ablation-analysis,0.986487024,3.44E-07,3.81E-05,1.26E-06,5.17E-06,6.67E-06,5.12E-06,2.50E-07,4.07E-06,0.013388821,2.77E-07,1.20E-06,6.16E-05
10960,natural_language_inference15,164,"In ( 8 ) , we replace dense connection with residual connection only over recurrent and co-attentive features .",Ablation study,Ablation study,natural_language_inference,15,12,1,0,,0.283539361,0,ablation-analysis,0.907052763,0.000123731,0.013720582,6.11E-07,1.30E-05,3.09E-05,5.50E-05,4.48E-06,0.002058923,0.076852234,5.12E-06,4.56E-06,7.81E-05
10961,natural_language_inference15,165,It means that only the word embedding features are densely connected to the uppermost layer while recurrent and attentive features are connected to the upper layer using the residual connection .,Ablation study,Ablation study,natural_language_inference,15,13,1,0,,0.043217466,0,ablation-analysis,0.572851429,8.21E-05,0.03907189,1.42E-06,2.66E-05,5.87E-05,4.74E-05,7.47E-06,0.003651101,0.384069206,4.30E-06,4.10E-06,0.000124239
10962,natural_language_inference15,166,"In ( 9 ) , we removed additional dense connection over word embedding features from ( 8 ) .",Ablation study,Ablation study,natural_language_inference,15,14,1,0,,0.086377592,0,ablation-analysis,0.961913156,5.45E-06,0.000623482,3.81E-07,6.79E-06,1.52E-05,1.66E-05,1.75E-06,8.69E-05,0.037283355,2.23E-07,1.68E-06,4.51E-05
10963,natural_language_inference15,167,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .",Ablation study,Ablation study,natural_language_inference,15,15,1,1,ablation-analysis,0.829860394,1,ablation-analysis,0.99313611,2.56E-07,1.03E-05,1.71E-08,2.57E-07,5.93E-07,1.34E-05,6.77E-08,9.03E-07,0.006822272,2.16E-07,7.80E-06,7.77E-06
10964,natural_language_inference15,168,The model is the basic 5 - layer RNN with attention and is the one without attention .,Ablation study,Ablation study,natural_language_inference,15,16,1,0,,0.079026627,0,baselines,0.241941169,0.000210064,0.365633781,1.55E-05,9.28E-05,0.000980081,0.00124576,0.000102919,0.01486168,0.362416397,3.51E-05,4.24E-06,0.012460433
10965,natural_language_inference15,169,The result of ( 10 ) shows that the connections among the layers are important to help gradient flow .,Ablation study,Ablation study,natural_language_inference,15,17,1,1,ablation-analysis,0.500427349,1,ablation-analysis,0.992091773,3.07E-07,9.09E-06,2.50E-08,3.26E-07,6.57E-07,7.03E-06,1.04E-07,2.03E-06,0.00787909,1.32E-07,2.90E-06,6.54E-06
10966,natural_language_inference15,170,"And , the result of ( 11 ) shows that the attentive information functioning as a soft - alignment is significantly effective in semantic sentence matching .",Ablation study,Ablation study,natural_language_inference,15,18,1,1,ablation-analysis,0.657155633,1,ablation-analysis,0.996849788,1.61E-07,6.14E-06,1.59E-08,3.06E-07,4.31E-07,1.70E-05,4.91E-08,3.58E-07,0.003107581,1.52E-07,7.49E-06,1.05E-05
10967,natural_language_inference15,171,The performances of models having different number of recurrent layers are also reported in .,Ablation study,Ablation study,natural_language_inference,15,19,1,0,,0.060934172,0,negative,0.135222204,9.36E-07,5.56E-05,5.05E-07,6.23E-06,2.44E-05,4.47E-05,2.68E-06,4.46E-06,0.86450728,2.88E-06,1.25E-05,0.000115597
10968,natural_language_inference15,172,"The models ( 5 - 9 ) which have connections between layers , are more robust to the increased depth of network , however , the performances of ( 10 - 11 ) tend to degrade as layers get deeper .",Ablation study,Ablation study,natural_language_inference,15,20,1,1,ablation-analysis,0.893899492,1,ablation-analysis,0.993041137,1.62E-07,1.66E-05,3.32E-08,3.91E-07,8.13E-07,2.23E-05,8.19E-08,3.54E-07,0.006883887,3.24E-07,1.66E-05,1.73E-05
10969,natural_language_inference15,173,"In addition , the models with dense connections rather than residual connections , have higher performance in general .",Ablation study,Ablation study,natural_language_inference,15,21,1,1,ablation-analysis,0.948813301,1,ablation-analysis,0.991959415,1.90E-07,9.33E-06,2.05E-07,1.11E-06,2.21E-06,9.00E-05,2.84E-07,2.49E-07,0.007717519,3.73E-07,6.58E-05,0.0001533
10970,natural_language_inference15,174,"shows that the connection between layers is essential , especially in deep models , endowing more representational power , and the dense connection is more effective than the residual connection .",Ablation study,Ablation study,natural_language_inference,15,22,1,1,ablation-analysis,0.89467094,1,ablation-analysis,0.997453962,1.61E-07,5.78E-06,2.70E-08,3.44E-07,5.96E-07,1.51E-05,8.76E-08,3.76E-07,0.002502205,5.62E-08,4.74E-06,1.66E-05
10971,natural_language_inference15,175,Word Alignment and Importance Our denselyconnected recurrent and co-attentive features are connected to the classification layer through the max pooling operation such that all max - valued features of every layer affect the loss function and perform a kind of deep supervision .,Ablation study,Ablation study,natural_language_inference,15,23,1,0,,0.202471218,0,ablation-analysis,0.789338701,4.22E-05,0.013187652,2.44E-06,0.000109737,6.17E-05,9.42E-05,3.50E-06,0.0004502,0.196319359,1.27E-06,9.72E-06,0.000379283
10972,natural_language_inference15,176,"Thus , we could cautiously interpret the classification results using our attentive weights and max - pooled positions .",Ablation study,Ablation study,natural_language_inference,15,24,1,0,,0.001939398,0,negative,0.113257428,1.71E-06,6.76E-05,2.46E-07,3.66E-06,1.35E-05,1.35E-05,1.64E-06,2.60E-05,0.886575153,1.16E-06,2.68E-06,3.58E-05
10973,natural_language_inference15,177,The attentive weights contain information on how two sentences are aligned and the numbers of max - pooled positions in each dimension play an important role in classification .,Ablation study,Ablation study,natural_language_inference,15,25,1,0,,0.112255472,0,negative,0.47387346,0.000153552,0.007036478,7.17E-07,2.02E-05,3.84E-05,5.48E-05,8.90E-06,0.012270051,0.506316294,4.94E-06,6.40E-06,0.000215774
10974,natural_language_inference15,178,shows the attention map (?,Ablation study,Ablation study,natural_language_inference,15,26,1,0,,0.00410069,0,negative,0.172325147,9.79E-06,0.015267653,1.02E-07,3.79E-06,1.41E-05,5.48E-05,1.09E-06,0.00062244,0.811558523,3.50E-06,6.79E-06,0.00013222
10975,natural_language_inference15,179,"i , j in eq. ( 5 ) ) on each layer of the samples in .",Ablation study,Ablation study,natural_language_inference,15,27,1,0,,0.001396427,0,negative,0.12554491,9.25E-06,0.00102835,2.61E-07,1.44E-05,9.69E-06,9.62E-06,1.37E-06,0.000165903,0.873181445,6.74E-07,3.19E-06,3.10E-05
10976,natural_language_inference15,180,The Avg ( Layers ) is the average of attentive weights over 5 layers and the gray heatmap right above the Avg ( Layers ) is the rate of max - pooled positions .,Ablation study,Ablation study,natural_language_inference,15,28,1,0,,0.001193461,0,negative,0.108601289,2.28E-05,0.001246825,6.69E-06,6.89E-05,0.000940693,0.000583772,0.000124947,0.000397291,0.883042675,2.74E-06,2.85E-06,0.004958529
10977,natural_language_inference15,181,The darker indicates the higher importance in classification .,Ablation study,Ablation study,natural_language_inference,15,29,1,0,,0.003037731,0,negative,0.023125214,6.61E-06,0.000451376,9.01E-07,9.02E-06,8.37E-05,4.56E-05,1.14E-05,0.000334387,0.975390172,1.88E-06,1.49E-06,0.000538259
10978,natural_language_inference15,182,"In the figure , we can see that tight , competing and bicycle are more important words than others in classifying the label .",Ablation study,Ablation study,natural_language_inference,15,30,1,0,,0.098690538,0,ablation-analysis,0.96262259,3.09E-07,2.13E-05,9.60E-08,3.74E-06,3.02E-06,5.51E-05,2.62E-07,7.73E-07,0.037196365,1.30E-07,1.57E-05,8.06E-05
10979,natural_language_inference15,183,The word tight clothing in the hypothesis can be inferred from spandex in the premise .,Ablation study,Ablation study,natural_language_inference,15,31,1,0,,0.000263175,0,negative,0.01065081,7.70E-07,6.06E-05,7.74E-06,8.43E-05,8.26E-05,5.38E-06,2.67E-06,1.31E-05,0.988862038,2.08E-07,2.99E-07,0.000229483
10980,natural_language_inference15,184,And competing is also inferred from race .,Ablation study,Ablation study,natural_language_inference,15,32,1,0,,0.000224151,0,negative,0.015843661,8.08E-07,0.000543845,1.23E-06,7.02E-05,3.33E-05,9.03E-06,9.34E-07,1.26E-05,0.983340318,3.05E-07,8.11E-07,0.000142962
10981,natural_language_inference15,185,"Other than that , the riding is matched with pedaling , and pair is matched with two .",Ablation study,Ablation study,natural_language_inference,15,33,1,0,,0.001271697,0,negative,0.062827441,4.46E-05,0.014721533,6.03E-07,9.28E-05,3.00E-05,4.27E-05,4.02E-06,0.000968104,0.921025282,6.61E-07,2.91E-06,0.000239338
10982,natural_language_inference15,186,"Judging by the matched terms , the model is undoubtedly able to classify the label as an entailment , correctly .",Ablation study,Ablation study,natural_language_inference,15,34,1,0,,0.062421812,0,ablation-analysis,0.58157444,1.20E-06,0.000151312,4.56E-08,3.68E-06,2.55E-06,2.06E-05,2.43E-07,1.49E-05,0.418186458,2.01E-07,1.28E-05,3.16E-05
10983,natural_language_inference15,187,"In , most of words in both the premise and the hypothesis coexist except white and gray .",Ablation study,Ablation study,natural_language_inference,15,35,1,0,,0.000482419,0,negative,0.113926307,1.19E-06,0.000176512,2.06E-06,0.000225617,3.17E-05,2.76E-05,1.29E-06,3.88E-06,0.88535592,1.62E-07,3.95E-06,0.000243782
10984,natural_language_inference15,188,"In attention map of layer 1 , the same or similar words in each sentence have a high correspondence ( gray and white are not exactly matched but have a linguistic relevance ) .",Ablation study,Ablation study,natural_language_inference,15,36,1,0,,0.004987734,0,negative,0.147317877,4.54E-05,0.024619886,9.15E-07,3.12E-05,7.12E-05,9.58E-05,1.02E-05,0.005434942,0.821033578,1.44E-06,4.57E-06,0.001333007
10985,natural_language_inference15,189,"However , as the layers get deeper , the relevance between white building and gray building is only maintained as a clue of classification ( See layer 5 ) .",Ablation study,Ablation study,natural_language_inference,15,37,1,0,,0.021961393,0,ablation-analysis,0.819009653,4.79E-06,0.001678489,9.07E-08,7.71E-06,3.99E-06,1.94E-05,3.11E-07,0.000109021,0.179112716,1.80E-07,5.94E-06,4.77E-05
10986,natural_language_inference15,190,"Because white is clearly different from gray , our model determines the label as a contradiction .",Ablation study,Ablation study,natural_language_inference,15,38,1,0,,0.000477257,0,negative,0.03222002,1.64E-06,0.000112904,7.80E-08,4.13E-06,5.95E-06,7.69E-06,9.60E-07,4.35E-05,0.967562584,2.28E-07,1.16E-06,3.91E-05
10987,natural_language_inference15,191,The densely connected recurrent and co-attentive features are well - semanticized over multiple layers as collective knowledge .,Ablation study,Ablation study,natural_language_inference,15,39,1,0,,0.400807941,0,ablation-analysis,0.59298499,0.000382252,0.008515302,2.57E-06,8.22E-05,5.88E-05,0.000171366,1.52E-05,0.018907825,0.377330965,1.11E-06,8.82E-06,0.001538496
10988,natural_language_inference15,192,And the max pooling operation selects the softpositions that may extract the clues on inference correctly .,Ablation study,Ablation study,natural_language_inference,15,40,1,0,,0.052262567,0,negative,0.218398261,1.92E-05,0.012774522,3.31E-07,1.92E-05,2.96E-05,3.57E-05,3.70E-06,0.001509267,0.766955699,2.35E-07,2.29E-06,0.000251998
10989,natural_language_inference15,193,Linguistic Error Analysis,Ablation study,,natural_language_inference,15,41,1,0,,0.154405863,0,negative,0.193776667,3.46E-05,0.001562905,1.27E-05,3.31E-05,0.000366689,0.005463204,3.60E-05,0.000261301,0.497091996,0.000467381,8.17E-05,0.300811598
10990,natural_language_inference15,194,"We conducted a linguistic error analysis on MultiNLI , and compared DRCN with the ESIM , DIIN and CAFE .",Ablation study,Linguistic Error Analysis,natural_language_inference,15,42,1,0,,1.12E-05,0,negative,0.000530906,2.24E-05,4.30E-05,1.03E-07,3.70E-05,1.53E-05,0.001266931,4.28E-06,7.94E-06,0.997761826,1.85E-07,0.00030564,4.62E-06
10991,natural_language_inference15,195,"We used annotated subset provided by the MultiNLI dataset , and each sample belongs to one of the 13 linguistic categories .",Ablation study,Linguistic Error Analysis,natural_language_inference,15,43,1,0,,1.28E-06,0,negative,1.68E-05,6.96E-07,2.83E-06,9.41E-08,6.91E-05,1.82E-05,0.000149232,2.67E-06,4.32E-07,0.999729246,3.10E-09,9.89E-06,8.16E-07
10992,natural_language_inference15,196,The results in table 7 show that our model generally has a good performance than others on most categories .,Ablation study,Linguistic Error Analysis,natural_language_inference,15,44,1,0,,0.024006779,0,negative,0.010211849,1.30E-06,2.85E-06,2.35E-07,3.27E-06,5.56E-05,0.113315804,3.15E-05,4.42E-07,0.811052732,3.95E-07,0.065215255,0.000108716
10993,natural_language_inference15,197,"Especially , we can see that ours outperforms much better on the Quantity / Time category which is one of the most difficult problems .",Ablation study,Linguistic Error Analysis,natural_language_inference,15,45,1,0,,0.003532122,0,negative,0.018003391,2.47E-06,5.03E-06,9.72E-07,5.48E-06,0.000162486,0.163608729,7.20E-05,1.38E-06,0.76659039,5.63E-07,0.051314206,0.000232883
10994,natural_language_inference15,198,"Furthermore , our DRCN shows the highest mean and the lowest stddev for both MATCHED and MISMATCHED problems , which indicates that it not only results in a competitive performance but also has a consistent performance .",Ablation study,Linguistic Error Analysis,natural_language_inference,15,46,1,0,,0.02197227,0,negative,0.065909656,2.77E-06,8.30E-06,1.70E-07,4.20E-06,2.65E-05,0.127590604,1.86E-05,7.05E-07,0.644354429,2.33E-07,0.161977627,0.000106254
10995,natural_language_inference15,199,Conclusion,,,natural_language_inference,15,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836391,0.000467808,6.66E-05,1.52E-06
10996,relation_extraction13,1,title,,,relation_extraction,13,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
10997,relation_extraction13,2,Matching the Blanks : Distributional Similarity for Relation Learning,title,,relation_extraction,13,1,1,1,research-problem,0.994930785,1,research-problem,2.35E-08,7.34E-06,3.78E-08,9.87E-08,4.23E-08,1.75E-07,1.00E-06,2.16E-06,1.03E-06,0.002448664,0.997539259,1.07E-07,5.34E-08
10998,relation_extraction13,3,abstract,,,relation_extraction,13,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
10999,relation_extraction13,4,"General purpose relation extractors , which can model arbitrary relations , are a core aspiration in information extraction .",abstract,abstract,relation_extraction,13,1,1,0,,0.780112498,1,research-problem,9.09E-09,2.76E-06,6.02E-09,2.24E-07,5.18E-08,1.00E-07,1.75E-07,6.17E-07,2.46E-07,0.005306756,0.994689019,1.10E-08,2.10E-08
11000,relation_extraction13,5,"Efforts have been made to build general purpose extractors that represent relations with their surface forms , or which jointly embed surface forms with relations from an existing knowledge graph .",abstract,abstract,relation_extraction,13,2,1,0,,0.126832693,0,research-problem,1.90E-07,2.41E-05,4.71E-08,2.01E-06,1.39E-06,1.61E-06,7.59E-07,7.15E-06,1.76E-06,0.128764153,0.871196644,8.68E-08,7.87E-08
11001,relation_extraction13,6,"However , both of these approaches are limited in their ability to generalize .",abstract,abstract,relation_extraction,13,3,1,0,,0.002118925,0,research-problem,4.06E-07,6.07E-05,1.29E-07,6.65E-06,4.94E-06,3.69E-06,1.21E-06,1.55E-05,5.67E-06,0.352959055,0.646941687,1.56E-07,1.26E-07
11002,relation_extraction13,7,"In this paper , we build on extensions of Harris ' distributional hypothesis to relations , as well as recent advances in learning text representations ( specifically , BERT ) , to build task agnostic relation representations solely from entity - linked text .",abstract,abstract,relation_extraction,13,4,1,0,,0.532938156,1,research-problem,1.75E-05,0.151818758,7.44E-05,3.44E-05,0.000548475,2.70E-05,2.35E-05,0.000323251,0.003903696,0.167870889,0.67534311,1.07E-05,4.20E-06
11003,relation_extraction13,8,We show that these representations significantly outperform previous work on exemplar based relation extraction ( FewRel ) even without using any of that task 's training data .,abstract,abstract,relation_extraction,13,5,1,0,,0.283374129,0,research-problem,0.000102736,0.024823177,3.81E-05,1.02E-05,9.10E-05,2.92E-05,6.13E-05,0.000543349,0.001116762,0.430266925,0.542609243,0.000303364,4.71E-06
11004,relation_extraction13,9,"We also show that models initialized with our task agnostic representations , and then tuned on supervised relation extraction datasets , significantly outperform the previous methods on Se - m Eval 2010 Task 8 , KBP37 , and TACRED .",abstract,abstract,relation_extraction,13,6,1,0,,0.051921266,0,negative,0.000557744,0.015474639,1.86E-05,0.000180804,0.000392562,0.000213961,0.000459339,0.003840006,0.000274966,0.642959665,0.334238316,0.001355294,3.41E-05
11005,relation_extraction13,10,Introduction,,,relation_extraction,13,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
11006,relation_extraction13,11,Reading text to identify and extract relations between entities has been along standing goal in natural language processing .,Introduction,Introduction,relation_extraction,13,1,1,1,research-problem,0.921656315,1,research-problem,9.25E-07,9.52E-05,3.11E-07,2.43E-05,4.25E-05,5.99E-06,1.69E-05,3.79E-06,1.78E-05,0.036421869,0.963366689,1.07E-06,2.69E-06
11007,relation_extraction13,12,Typically efforts in relation extraction fall into one of three groups .,Introduction,Introduction,relation_extraction,13,2,1,1,research-problem,0.008287447,0,research-problem,2.01E-06,0.000576303,7.49E-07,7.66E-06,3.59E-05,1.32E-05,1.07E-05,1.49E-05,0.000153585,0.245793632,0.753388377,1.67E-06,1.33E-06
11008,relation_extraction13,13,"In a first group , supervised , or distantly supervised relation extractors ) learn a mapping from text to relations in a limited schema .",Introduction,Introduction,relation_extraction,13,3,1,0,,0.30162763,0,research-problem,7.01E-06,0.003305489,1.09E-05,7.09E-06,7.36E-05,2.08E-05,3.05E-05,2.61E-05,0.001608163,0.152724644,0.842176976,5.43E-06,3.17E-06
11009,relation_extraction13,14,"Forming a second group , open information extraction removes the limitations of a predefined schema by instead representing relations using their surface forms , which increases scope but also leads * Work done as part of the Google AI residency .",Introduction,Introduction,relation_extraction,13,4,1,0,,0.813981743,1,negative,0.000372581,0.06554985,0.000409681,5.01E-05,0.005133778,9.10E-05,0.000247792,6.36E-05,0.021296369,0.601081647,0.305530538,0.000159205,1.39E-05
11010,relation_extraction13,15,to an associated lack of generality since many surface forms can express the same relation .,Introduction,Introduction,relation_extraction,13,5,1,0,,0.00267348,0,negative,1.22E-05,0.001103625,2.38E-06,2.94E-05,0.000248843,5.26E-05,1.37E-05,3.95E-05,0.00088149,0.955665673,0.041945374,4.09E-06,1.25E-06
11011,relation_extraction13,16,"Finally , the universal schema embraces both the diversity of text , and the concise nature of schematic relations , to build a joint representation that has been extended to arbitrary textual input , and arbitrary entity pairs .",Introduction,Introduction,relation_extraction,13,6,1,0,,0.914315426,1,model,1.74E-05,0.089290277,6.88E-05,6.33E-07,0.000128579,1.05E-05,8.11E-06,2.04E-05,0.894764501,0.014267926,0.001416933,5.11E-06,7.62E-07
11012,relation_extraction13,17,"However , like distantly supervised relation extractors , universal schema rely on large knowledge graphs ( typically Freebase ) that can be aligned to text .",Introduction,Introduction,relation_extraction,13,7,1,0,,0.056895128,0,research-problem,8.18E-06,0.001203389,2.66E-06,5.17E-05,0.000469855,3.43E-05,3.22E-05,2.06E-05,0.000160698,0.425960992,0.572045009,6.01E-06,4.38E-06
11013,relation_extraction13,18,"Building on 's extension of Harris ' distributional hypothesis to relations , as well as recent advances in learning word representations from observations of their contexts , we propose a new method of learning relation representations directly from text .",Introduction,Introduction,relation_extraction,13,8,1,0,,0.9456528,1,approach,0.000120403,0.703396625,0.000234837,7.54E-05,0.00471757,0.000122221,0.000182994,0.000283205,0.173326269,0.056607844,0.060837651,6.70E-05,2.79E-05
11014,relation_extraction13,19,"First , we study the ability of the Transformer neural network architecture to encode relations between entity pairs , and we identify a method of representation that outperforms previous work in supervised relation extraction .",Introduction,Introduction,relation_extraction,13,9,1,1,model,0.233799205,0,approach,0.000787448,0.661728985,0.000180842,0.000155215,0.022989484,0.000240923,0.000353583,0.000368219,0.058919969,0.247379735,0.006629014,0.000247764,1.88E-05
11015,relation_extraction13,20,"Then , we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks .",Introduction,Introduction,relation_extraction,13,10,1,1,model,0.938063335,1,model,4.06E-05,0.390672792,8.82E-05,3.20E-06,0.000579466,1.97E-05,1.99E-05,6.08E-05,0.566949462,0.035704049,0.00584607,1.37E-05,2.03E-06
11016,relation_extraction13,21,"[ BLANK ] , inspired by Cale 's earlier cover , recorded one of the most acclaimed versions of "" [ BLANK ] """,Introduction,Introduction,relation_extraction,13,11,1,0,,0.002388649,0,negative,4.72E-06,0.001615744,4.35E-06,0.000161511,0.003237135,0.000356669,5.98E-05,0.000130651,0.000686702,0.979768761,0.013964198,3.83E-06,5.91E-06
11017,relation_extraction13,22,"[ BLANK ] 's rendition of "" [ BLANK ] "" has been called "" one of the great songs "" by Time , and is included on Rolling Stone 's list of "" The 500 Greatest Songs of All Time "" .",Introduction,Introduction,relation_extraction,13,12,1,0,,0.021097851,0,negative,6.72E-06,0.002126811,3.46E-06,0.000143577,0.004064264,0.000319692,5.85E-05,0.000142432,0.000913618,0.982072456,0.010137804,5.12E-06,5.53E-06
11018,relation_extraction13,23,"Following , we assume access to a corpus of text in which entities have been linked to unique identifiers and we define a rela-tion statement to be a block of text containing two marked entities .",Introduction,Introduction,relation_extraction,13,13,1,0,,0.017895016,0,negative,1.38E-05,0.170596018,3.64E-05,8.83E-05,0.015219143,0.000397439,6.03E-05,0.000518302,0.140311897,0.665267305,0.00747189,9.00E-06,1.02E-05
11019,relation_extraction13,24,"From this , we create training data that contains relation statements in which the entities have been replaced with a special symbol , as illustrated in .",Introduction,Introduction,relation_extraction,13,14,1,0,,0.447194827,0,negative,3.31E-05,0.140842261,6.26E-05,0.000134456,0.186948813,0.000958777,0.000201363,0.000843266,0.024870924,0.644407921,0.000668178,1.69E-05,1.15E-05
11020,relation_extraction13,25,"Our training procedure takes in pairs of blank - containing relation statements , and has an objective that encourages relation representations to be similar if they range over the same pairs of entities .",Introduction,Introduction,relation_extraction,13,15,1,0,,0.744759359,1,approach,1.78E-05,0.679963154,5.96E-05,5.76E-06,0.003099633,3.83E-05,2.21E-05,0.000129492,0.285517336,0.030372212,0.000765876,6.46E-06,2.35E-06
11021,relation_extraction13,26,"After training , we employ learned relation representations to the recently released FewRel task in which specific relations , such as ' original language of work ' are represented with a few exemplars , such as The Crowd ( Italian : La Folla ) is a 1951 Italian film .",Introduction,Introduction,relation_extraction,13,16,1,0,,0.221416905,0,approach,5.93E-05,0.798163132,0.000333546,0.000142936,0.075173238,0.000303214,0.000490339,0.000411311,0.037899423,0.082561867,0.004329806,9.12E-05,4.07E-05
11022,relation_extraction13,27,"presented FewRel as a supervised dataset , intended to evaluate models ' ability to adapt to relations from new domains at test time .",Introduction,Introduction,relation_extraction,13,17,1,0,,0.025602192,0,negative,4.72E-05,0.012122424,8.72E-05,0.00107656,0.151596392,0.000470492,0.000167328,8.31E-05,0.001331884,0.82710274,0.00587145,2.65E-05,1.68E-05
11023,relation_extraction13,28,"We show that through training by matching the blanks , we can outperform 's top performance on FewRel , without having seen any of the FewRel training data .",Introduction,Introduction,relation_extraction,13,18,1,0,,0.485769831,0,approach,0.000783234,0.389291136,0.000187676,1.02E-05,0.001690485,9.80E-05,0.000487407,0.00025451,0.276643306,0.323329951,0.006717866,0.00049309,1.32E-05
11024,relation_extraction13,29,We also show that a model pre-trained by matching the blanks and tuned on FewRel outperforms humans on the FewRel evaluation .,Introduction,Introduction,relation_extraction,13,19,1,0,,0.193845057,0,negative,0.002200704,0.081288372,5.01E-05,8.33E-05,0.004601472,0.000520488,0.004645189,0.001406003,0.008526601,0.886663101,0.005485023,0.004460848,6.88E-05
11025,relation_extraction13,30,"Similarly , by training by matching the blanks and then tuning on labeled data , we significantly improve performance on the SemEval 2010 Task 8 , and TACRED relation extraction benchmarks .",Introduction,Introduction,relation_extraction,13,20,1,0,,0.059304184,0,negative,0.005092441,0.16287509,0.000176274,0.000331787,0.007788167,0.001674715,0.024928108,0.004358979,0.018641327,0.746155048,0.012669139,0.014666518,0.000642407
11026,relation_extraction13,31,Overview,Introduction,,relation_extraction,13,21,1,0,,0.002652444,0,negative,8.70E-06,0.008106775,6.82E-06,2.89E-05,0.000160062,0.00053346,8.46E-05,0.000585368,0.061895046,0.921199139,0.007378565,4.48E-06,8.02E-06
11027,relation_extraction13,32,Task definition,Introduction,,relation_extraction,13,22,1,0,,0.005108005,0,negative,9.63E-06,0.02650984,1.67E-05,2.03E-05,0.000994573,0.000414256,0.000418963,0.000722978,0.025231841,0.884521702,0.061063825,5.23E-05,2.30E-05
11028,relation_extraction13,33,"In this paper , we focus on learning mappings from relation statements to relation representations .",Introduction,Task definition,relation_extraction,13,23,1,0,,0.002490666,0,negative,0.000307492,0.004065374,0.000150138,5.60E-06,3.97E-05,3.31E-05,4.11E-05,0.000131173,0.001544267,0.991248425,0.001726569,0.000694684,1.24E-05
11029,relation_extraction13,34,"Formally , let x = [ x 0 . . . x n ] be a sequence of tokens , where x 0 = [ CLS ] and x n = [ SEP ] are special start and end markers .",Introduction,Task definition,relation_extraction,13,24,1,0,,2.12E-06,0,negative,4.76E-07,4.25E-06,3.95E-07,9.41E-09,8.80E-08,1.81E-06,3.62E-07,7.35E-06,0.000113149,0.999868616,2.25E-06,1.20E-06,4.37E-08
11030,relation_extraction13,35,"Let s 1 = ( i , j ) and s 2 = ( k , l ) be pairs of integers such that 0 < i < j ? 1 , j < k , k ? l ? 1 , and l ? n.",Introduction,Task definition,relation_extraction,13,25,1,0,,3.61E-06,0,negative,1.71E-06,4.33E-06,3.92E-07,4.96E-08,3.08E-07,6.44E-06,1.08E-06,2.32E-05,3.96E-05,0.999917882,9.52E-07,3.99E-06,8.37E-08
11031,relation_extraction13,36,"A relation statement is a triple r = ( x , s 1 , s 2 ) , where the indices in s 1 and s 2 delimit entity mentions in x : the sequence [ x i . . . x j?1 ] mentions an entity , and so does the sequence [ x k . . . x l?1 ] .",Introduction,Task definition,relation_extraction,13,26,1,0,,9.09E-06,0,negative,1.99E-06,2.90E-05,5.58E-06,1.53E-07,8.68E-07,9.89E-06,3.08E-06,2.75E-05,0.000383931,0.999483501,4.64E-05,6.92E-06,1.20E-06
11032,relation_extraction13,37,Our goal is to learn a function hr = f ? ( r ) that maps the relation statement to a fixed - length vector hr ?,Introduction,Task definition,relation_extraction,13,27,1,0,,1.73E-05,0,negative,3.00E-06,2.91E-05,1.65E-06,1.37E-07,6.54E-07,5.07E-06,1.09E-06,3.26E-05,0.000205202,0.999709086,7.09E-06,5.04E-06,2.52E-07
11033,relation_extraction13,38,Rd that represents the relation expressed in x between the entities marked by s 1 and s 2 .,Introduction,Task definition,relation_extraction,13,28,1,0,,3.42E-06,0,negative,3.35E-06,3.54E-06,4.85E-06,1.74E-08,3.74E-07,1.32E-06,6.06E-07,2.82E-06,5.13E-05,0.999925274,1.13E-06,5.36E-06,8.00E-08
11034,relation_extraction13,39,Contributions,Introduction,,relation_extraction,13,29,1,0,,0.000734981,0,negative,8.47E-06,0.005293965,5.78E-06,5.99E-05,0.000524138,0.000454399,6.27E-05,0.000479642,0.017145537,0.975094754,0.000861265,3.12E-06,6.31E-06
11035,relation_extraction13,40,This paper contains two main contributions .,Introduction,Contributions,relation_extraction,13,30,1,0,,0.000809116,0,negative,2.09E-06,0.005011742,2.40E-06,7.89E-05,0.00176475,0.000103358,4.81E-06,0.00010391,0.00182295,0.989796315,0.001307205,7.52E-07,7.70E-07
11036,relation_extraction13,41,"First , in Section 3.1 we investigate different architectures for the relation encoder f ? , all built on top of the widely used Transformer se-quence model .",Introduction,Contributions,relation_extraction,13,31,1,0,,0.506910271,1,approach,4.56E-05,0.508167115,0.000204835,3.59E-06,0.001960244,4.54E-05,2.80E-05,0.000115662,0.234153285,0.252432072,0.002814924,2.80E-05,1.30E-06
11037,relation_extraction13,42,We evaluate each of these architectures by applying them to a suite of relation extraction benchmarks with supervised training .,Introduction,Contributions,relation_extraction,13,32,1,0,,0.043104235,0,approach,7.04E-06,0.754751032,2.29E-05,1.44E-05,0.013877561,0.000171189,3.83E-05,0.000841372,0.016775979,0.213130565,0.000353734,1.36E-05,2.32E-06
11038,relation_extraction13,43,"Our second , more significant , contributionpresented in Section 4 - is to show that f ?",Introduction,Contributions,relation_extraction,13,33,1,0,,0.136069147,0,negative,6.14E-05,0.195613033,7.92E-05,6.30E-06,0.001995306,5.19E-05,2.26E-05,0.000123596,0.065172497,0.73224171,0.004602876,2.82E-05,1.36E-06
11039,relation_extraction13,44,can be learned from widely available distant supervision in the form of entity linked text .,Introduction,Contributions,relation_extraction,13,34,1,0,,0.077676207,0,negative,6.55E-06,0.070336289,3.02E-05,2.83E-05,0.003889577,0.00014131,1.28E-05,0.000435775,0.022320724,0.900069684,0.002720696,5.56E-06,2.44E-06
11040,relation_extraction13,45,Architectures for Relation Learning,,,relation_extraction,13,0,1,0,,0.620499267,1,research-problem,9.91E-06,8.14E-05,3.34E-05,1.46E-06,4.21E-07,2.91E-05,0.00031749,0.000268422,6.20E-05,0.084029291,0.914585479,0.000554838,2.68E-05
11041,relation_extraction13,46,The primary goal of this work is to develop models that produce relation representations directly from text .,Architectures for Relation Learning,Architectures for Relation Learning,relation_extraction,13,1,1,0,,0.003847034,0,negative,3.28E-05,6.58E-05,2.82E-05,5.01E-06,9.79E-07,2.95E-05,0.000125846,9.74E-05,6.30E-05,0.957404377,0.041743451,0.000391156,1.26E-05
11042,relation_extraction13,47,"Given the strong performance of recent deep transformers trained on variants of language modeling , we adopt 's BERT model as the basis for our work .",Architectures for Relation Learning,Architectures for Relation Learning,relation_extraction,13,2,1,0,,3.64E-05,0,negative,1.48E-05,0.000231152,8.74E-05,2.41E-06,1.48E-06,0.000138638,5.95E-05,0.000600083,0.00101983,0.997750606,6.50E-05,2.68E-05,2.31E-06
11043,relation_extraction13,48,"In this section , we explore different methods of representing relations with the Transformer model .",Architectures for Relation Learning,Architectures for Relation Learning,relation_extraction,13,3,1,0,,0.000252642,0,negative,8.62E-05,9.14E-05,7.25E-05,8.75E-08,3.69E-07,8.19E-06,1.83E-05,4.06E-05,0.000130841,0.999046265,2.41E-05,0.000480761,2.25E-07
11044,relation_extraction13,49,Relation Classification and Extraction Tasks,Architectures for Relation Learning,,relation_extraction,13,4,1,0,,0.000547715,0,negative,6.90E-05,1.26E-05,2.46E-05,4.57E-06,7.82E-07,2.21E-05,0.000650192,4.73E-05,1.56E-05,0.920158268,0.074764419,0.004183281,4.73E-05
11045,relation_extraction13,50,We evaluate the different methods of representation on a suite of supervised relation extraction benchmarks .,Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,5,1,0,,0.000417391,0,negative,6.48E-05,0.000117191,2.35E-05,1.99E-07,1.07E-06,1.59E-05,8.74E-05,6.44E-05,4.66E-05,0.998041684,2.53E-05,0.001511141,8.20E-07
11046,relation_extraction13,51,"The relation extractions tasks we use can be broadly categorized into two types : fully supervised relation extraction , and few - shot relation matching .",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,6,1,0,,0.000931857,0,negative,2.07E-05,9.55E-06,4.65E-06,1.88E-05,1.23E-06,4.51E-05,0.000128632,2.95E-05,2.05E-05,0.968834446,0.030621504,0.00023682,2.85E-05
11047,relation_extraction13,52,"For the supervised tasks , the goal is to , given a relation statement r , predict a relation type t ?",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,7,1,0,,5.33E-05,0,negative,4.88E-06,7.01E-06,1.80E-06,6.96E-07,1.41E-07,7.69E-06,7.32E-06,2.75E-05,4.47E-05,0.999074727,0.000775385,4.66E-05,1.51E-06
11048,relation_extraction13,53,T where T is a fixed dictionary of relation types and t = 0 typically denotes alack of relation between the entities in the relation statement .,Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,8,1,0,,4.03E-06,0,negative,8.94E-06,1.52E-06,1.84E-06,2.14E-08,8.31E-08,1.85E-06,2.11E-06,5.46E-06,8.10E-06,0.999931031,1.34E-06,3.77E-05,4.01E-08
11049,relation_extraction13,54,"For this type of task we evaluate on SemEval 2010 Task 8 ( Hendrickx et al. , 2009 ) , KBP - 37 and TACRED .",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,9,1,0,,0.000262161,0,negative,8.23E-05,2.17E-05,3.82E-05,6.68E-07,6.42E-06,2.56E-05,0.000332285,4.66E-05,1.50E-05,0.995646625,9.53E-06,0.003771982,3.08E-06
11050,relation_extraction13,55,"More formally ,",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,10,1,0,,5.21E-07,0,negative,2.05E-06,7.36E-07,7.62E-07,4.63E-09,1.36E-08,7.71E-07,9.27E-07,2.73E-06,1.49E-05,0.999962422,8.68E-07,1.38E-05,1.62E-08
11051,relation_extraction13,56,"In the case of few - shot relation matching , a set of candidate relation statements are ranked , and matched , according to a query relation statement .",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,11,1,0,,0.000287675,0,negative,3.22E-05,7.52E-05,5.83E-05,3.82E-07,6.59E-07,7.21E-06,2.59E-05,2.29E-05,0.00015973,0.99818424,0.000789792,0.000641138,2.36E-06
11052,relation_extraction13,57,"In this task , examples in the test and development sets typically contain relation types not present in the training set .",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,12,1,0,,2.40E-05,0,negative,4.25E-06,4.89E-06,1.49E-06,2.88E-07,6.01E-07,4.47E-06,9.09E-06,1.31E-05,9.09E-06,0.999789833,6.32E-05,9.90E-05,7.12E-07
11053,relation_extraction13,58,"For this type of task , we evaluate on the FewRel dataset .",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,13,1,0,,0.001203916,0,negative,9.12E-05,8.38E-06,2.46E-05,5.42E-08,1.26E-06,4.19E-06,7.88E-05,9.88E-06,4.91E-06,0.996672893,8.58E-07,0.003102636,3.49E-07
11054,relation_extraction13,59,"Specifically , we are given K sets of N labeled relation statements S k = { ( r 0 , t 0 ) . . . ( r N , t N ) } where ti ?",Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,14,1,0,,3.42E-06,0,negative,9.78E-06,4.81E-06,3.03E-06,3.48E-08,2.63E-07,2.38E-06,3.67E-06,7.67E-06,3.27E-05,0.999869832,1.22E-06,6.45E-05,9.72E-08
11055,relation_extraction13,60,{ 1 . . .,Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,15,1,0,,2.51E-06,0,negative,2.72E-06,3.77E-07,3.28E-07,3.22E-08,3.69E-08,2.96E-06,1.86E-06,7.12E-06,9.53E-06,0.999960288,5.14E-07,1.42E-05,4.80E-08
11056,relation_extraction13,61,K } is the corresponding relation type .,Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,16,1,0,,4.26E-06,0,negative,1.61E-06,1.16E-06,1.59E-06,1.09E-08,2.22E-08,1.54E-06,1.81E-06,5.28E-06,3.28E-05,0.999938468,2.28E-06,1.33E-05,5.99E-08
11057,relation_extraction13,62,The goal is to predict the t q ?,Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,17,1,0,,8.69E-06,0,negative,3.23E-06,1.82E-06,9.82E-07,4.17E-08,6.46E-08,2.91E-06,3.32E-06,1.12E-05,2.41E-05,0.999911092,6.63E-06,3.45E-05,1.53E-07
11058,relation_extraction13,63,{ 1 . . .,Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,18,1,0,,2.44E-06,0,negative,2.58E-06,3.43E-07,3.10E-07,2.86E-08,3.70E-08,2.69E-06,1.78E-06,6.76E-06,8.84E-06,0.99996197,3.65E-07,1.43E-05,4.88E-08
11059,relation_extraction13,64,K } for a query relation statement r q .,Architectures for Relation Learning,Relation Classification and Extraction Tasks,relation_extraction,13,19,1,0,,8.46E-06,0,negative,5.33E-06,4.16E-07,1.80E-06,4.83E-09,2.62E-08,5.80E-07,1.26E-06,1.61E-06,7.25E-06,0.999938419,3.21E-07,4.30E-05,2.80E-08
11060,relation_extraction13,65,Relation Representations from Deep Transformers,Architectures for Relation Learning,,relation_extraction,13,20,1,0,,0.00413866,0,negative,4.70E-05,3.77E-05,6.18E-05,5.91E-07,2.65E-07,1.75E-05,0.00054926,9.26E-05,0.000242729,0.983738802,0.006845241,0.008344318,2.21E-05
11061,relation_extraction13,66,Model,,,relation_extraction,13,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
11062,relation_extraction13,67,"In all experiments in this section , we start with the BERT LARGE model made available by and train towards task - specific losses .",Model,Model,relation_extraction,13,1,1,0,,0.011207661,0,negative,0.000480367,0.003790245,0.002291867,1.78E-05,9.74E-05,0.001043829,0.001517382,0.00872131,0.010907274,0.968848588,7.56E-05,0.0021636,4.47E-05
11063,relation_extraction13,68,"Since BERT has not previously been applied to the problem of relation representation , we aim to answer two primary modeling questions : ( 1 ) how do we represent entities of interest in the input to BERT , and ( 2 ) how do we extract a fixed length representation of a relation from BERT 's output .",Model,Model,relation_extraction,13,2,1,0,,0.024752148,0,negative,0.000383605,0.014955961,0.003521001,5.89E-05,0.000132022,0.000214091,0.000602138,0.001900492,0.018789084,0.894956677,0.058144617,0.006079041,0.000262379
11064,relation_extraction13,69,"We present three options for both the input encoding , and the output relation representation .",Model,Model,relation_extraction,13,3,1,0,,0.087626082,0,negative,0.000548564,0.003399932,0.000946988,2.05E-05,4.68E-05,0.000385975,0.000223845,0.004507657,0.060570949,0.928013274,0.00012796,0.001169843,3.78E-05
11065,relation_extraction13,70,Six combinations of these are illustrated in .,Model,Model,relation_extraction,13,4,1,0,,0.002140344,0,negative,0.000212234,3.13E-05,8.84E-05,4.44E-05,4.13E-05,0.000368605,0.000150122,0.000465059,0.000840157,0.997269077,1.05E-05,0.000464876,1.39E-05
11066,relation_extraction13,71,Entity span identification,Model,Model,relation_extraction,13,5,1,0,,0.050636845,0,results,0.003483732,0.001312888,0.003627783,0.001140898,0.001061906,0.001073536,0.094059543,0.002282411,0.001446541,0.19400713,0.030572426,0.64556042,0.020370785
11067,relation_extraction13,72,"Recall , from Section 2 , that the relation statement r = ( x , s 1 , s 2 ) contains the sequence of tokens x and the entity span identifiers s 1 and s 2 .",Model,Model,relation_extraction,13,6,1,0,,0.000798018,0,negative,1.29E-05,4.15E-05,4.32E-05,2.68E-07,1.12E-06,1.63E-05,1.16E-05,0.000105946,0.003174785,0.996496431,1.56E-05,7.85E-05,1.84E-06
11068,relation_extraction13,73,We present three different options forgetting information about the focus spans s 1 and s 2 into our BERT encoder .,Model,Model,relation_extraction,13,7,1,0,,0.026478254,0,negative,0.002265149,0.017269246,0.007091428,2.10E-05,0.000109308,0.000207843,0.000341102,0.002295273,0.278693743,0.683828707,0.000902402,0.006875281,9.95E-05
11069,relation_extraction13,74,Standard input,Model,,relation_extraction,13,8,1,0,,0.003296942,0,negative,0.000306385,0.000108834,0.001508754,8.80E-06,1.29E-05,0.000791821,0.0019746,0.002764759,0.00675112,0.977577994,5.29E-05,0.008033331,0.000107789
11070,relation_extraction13,75,First we experiment with a BERT model that does not have access to any explicit identification of the entity spans s 1 and s 2 .,Model,Standard input,relation_extraction,13,9,1,0,,1.49E-05,0,negative,0.000305493,0.000109201,0.023366446,9.47E-07,4.41E-06,1.71E-05,0.000199341,0.000363088,8.36E-05,0.970450434,7.61E-06,0.00508381,8.55E-06
11071,relation_extraction13,76,We refer to this choice as the STANDARD input .,Model,Standard input,relation_extraction,13,10,1,0,,3.40E-07,0,negative,1.81E-06,1.54E-06,0.000235113,4.81E-08,3.08E-08,5.18E-06,7.16E-06,0.000261847,2.40E-05,0.999433056,5.27E-07,2.92E-05,5.03E-07
11072,relation_extraction13,77,"This is an important reference point , since we believe that BERT has the ability to identify entities in x , but with the STANDARD input there is noway of knowing which two entities are in focus when x contains more than two entity mentions .",Model,Standard input,relation_extraction,13,11,1,0,,6.10E-07,0,negative,8.47E-06,4.47E-07,6.50E-05,4.76E-07,1.30E-07,4.26E-06,3.80E-06,5.00E-05,5.27E-06,0.99979292,1.21E-06,6.70E-05,1.01E-06
11073,relation_extraction13,78,Positional embeddings,Model,,relation_extraction,13,12,1,0,,0.032564284,0,negative,0.000198421,0.001260579,0.004894757,5.48E-06,6.55E-06,0.000424261,0.001488513,0.003396723,0.131120193,0.847211858,0.003461284,0.006176235,0.00035514
11074,relation_extraction13,79,"For each of the tokens in its input , BERT also adds a segmentation embedding , primarily used to add sentence segmentation information to the model .",Model,Positional embeddings,relation_extraction,13,13,1,0,,0.000227823,0,negative,0.000105894,0.000202419,0.002898616,3.17E-07,1.23E-06,1.53E-05,1.75E-05,0.000342794,0.022034406,0.974237169,7.92E-06,0.000132711,3.75E-06
11075,relation_extraction13,80,"To address the STANDARD representation 's lack of explicit entity identification , we introduce two new segmentation embeddings , one that is added to all tokens in the span s 1 , while the other is added to all tokens in the span s 2 .",Model,Positional embeddings,relation_extraction,13,14,1,0,,0.000137613,0,negative,0.000725719,0.010137121,0.006501387,9.62E-06,4.38E-05,9.17E-05,0.000180597,0.00407532,0.034671215,0.940188582,0.000126321,0.003212251,3.64E-05
11076,relation_extraction13,81,This approach is analogous to previous work where positional embeddings have been applied to relation extraction .,Model,Positional embeddings,relation_extraction,13,15,1,0,,7.62E-06,0,negative,2.24E-05,0.000169366,0.000907594,3.62E-07,1.62E-06,6.59E-06,7.51E-06,9.31E-05,0.00088191,0.997554283,4.39E-05,0.000309272,2.10E-06
11077,relation_extraction13,82,Entity marker tokens,Model,Positional embeddings,relation_extraction,13,16,1,0,,0.000529069,0,negative,2.14E-05,2.40E-05,0.000458777,3.61E-07,5.92E-07,1.17E-05,7.48E-05,0.000190471,0.000892432,0.994662353,0.000139915,0.003505858,1.73E-05
11078,relation_extraction13,83,"Finally , we augment x with four reserved word pieces to mark the begin and end of each entity mention in the relation statement .",Model,Positional embeddings,relation_extraction,13,17,1,0,,6.93E-05,0,negative,6.21E-05,6.15E-05,0.000550921,4.01E-07,2.81E-06,2.43E-05,2.44E-05,0.000541094,0.001111955,0.997461904,6.80E-07,0.000156127,1.83E-06
11079,relation_extraction13,84,"We introduce the [ E1 start ] , [ E1 end ] , [ E2 start ] and [ E2 end ] and modify x to giv ?",Model,Positional embeddings,relation_extraction,13,18,1,0,,1.50E-06,0,negative,2.31E-06,1.25E-06,5.12E-06,7.53E-08,1.04E-07,4.34E-06,1.26E-06,7.59E-05,3.83E-05,0.999854305,1.24E-07,1.68E-05,1.07E-07
11080,relation_extraction13,85,and we feed this token sequence into BERT instead of x .,Model,Positional embeddings,relation_extraction,13,19,1,0,,1.30E-06,0,negative,1.71E-05,3.59E-06,0.000103015,2.08E-08,1.48E-07,1.57E-06,1.81E-06,3.46E-05,9.94E-05,0.9996707,1.23E-07,6.78E-05,1.08E-07
11081,relation_extraction13,86,"We also update the entity indicess 1 = ( i + 1 , j + 1 ) ands 2 = ( k + 3 , l + 3 ) to account for the inserted tokens .",Model,Positional embeddings,relation_extraction,13,20,1,0,,1.59E-05,0,negative,3.05E-05,9.98E-05,0.000229141,1.01E-07,4.04E-07,1.07E-05,9.36E-06,0.000578629,0.00739327,0.991589051,1.57E-06,5.65E-05,9.82E-07
11082,relation_extraction13,87,We refer to this representation of the input as ENTITY MARKERS .,Model,Positional embeddings,relation_extraction,13,21,1,0,,5.62E-06,0,negative,1.34E-06,6.55E-06,2.26E-05,1.01E-07,2.79E-07,3.79E-06,1.70E-06,7.60E-05,0.000411266,0.999459797,6.71E-07,1.53E-05,5.31E-07
11083,relation_extraction13,88,Fixed length relation representation,Model,Positional embeddings,relation_extraction,13,22,1,0,,6.14E-05,0,negative,1.09E-05,5.85E-05,0.000848167,4.69E-07,3.57E-07,3.35E-05,0.000131442,0.000686805,0.003571076,0.992090811,0.000890911,0.001643177,3.38E-05
11084,relation_extraction13,89,We now introduce three separate methods of extracting a fixed length relation representation hr from the BERT encoder .,Model,Positional embeddings,relation_extraction,13,23,1,0,,3.50E-05,0,negative,6.07E-05,0.000361943,0.001551705,2.90E-07,2.17E-06,4.81E-06,1.51E-05,0.000120913,0.003014452,0.993596282,2.42E-05,0.001244208,3.29E-06
11085,relation_extraction13,90,"The three variants rely on extracting the last hidden layers of the transformer network , which we define as H = [ h 0 , ...h n ] for n = | x | ( or | x | if entity marker tokens are used ) .",Model,Positional embeddings,relation_extraction,13,24,1,0,,2.02E-05,0,negative,0.000107502,8.64E-05,0.001299073,8.76E-07,2.28E-06,8.26E-06,1.34E-05,0.00014681,0.000595959,0.997351065,2.16E-06,0.000383944,2.26E-06
11086,relation_extraction13,91,[ CLS ] token,Model,Positional embeddings,relation_extraction,13,25,1,0,,5.76E-05,0,negative,3.91E-06,4.33E-06,0.000146748,1.40E-07,2.67E-07,1.44E-05,2.29E-05,0.000218601,0.000383914,0.999019513,1.71E-06,0.000179477,4.14E-06
11087,relation_extraction13,92,Recall from Section 2 that each x starts with a reserved [ CLS ] token .,Model,Positional embeddings,relation_extraction,13,26,1,0,,1.30E-06,0,negative,4.66E-06,2.29E-06,3.34E-05,4.93E-09,4.51E-08,5.54E-07,7.05E-07,1.41E-05,8.41E-05,0.999809919,6.95E-08,5.01E-05,4.99E-08
11088,relation_extraction13,93,BERT 's output state that corresponds to this token is used by as a fixed length sentence representation .,Model,Positional embeddings,relation_extraction,13,27,1,0,,3.94E-06,0,negative,7.49E-06,1.57E-05,0.000403375,8.21E-08,4.80E-07,4.56E-06,5.76E-06,0.000122584,0.001998563,0.997393775,5.00E-07,4.55E-05,1.68E-06
11089,relation_extraction13,94,"We adopt the [ CLS ] output , h 0 , as our first relation representation .",Model,Positional embeddings,relation_extraction,13,28,1,0,,8.61E-06,0,negative,7.05E-07,6.17E-06,0.000140548,3.78E-09,4.12E-08,1.79E-06,2.11E-06,6.42E-05,0.000251516,0.999517933,1.35E-07,1.47E-05,1.11E-07
11090,relation_extraction13,95,Entity mention pooling,Model,Positional embeddings,relation_extraction,13,29,1,0,,0.032107063,0,negative,0.000125107,0.000350232,0.00927548,6.25E-06,4.53E-06,0.000131795,0.003638995,0.002995431,0.003911211,0.887905895,0.002919268,0.087858401,0.000877402
11091,relation_extraction13,96,"We obtain hr by maxpooling the final hidden layers corresponding to the word pieces in each entity mention , to get two vectors he 1 = MAXPOOL ( [h i ...h j?1 ]) and he 2 = MAXPOOL ( [h k ...h l?1 ] ) representing the two entity mentions .",Model,Positional embeddings,relation_extraction,13,30,1,0,,6.92E-06,0,negative,3.70E-06,9.79E-06,1.74E-05,4.89E-08,2.40E-07,4.79E-06,3.89E-06,0.000201929,0.000559218,0.999167719,2.86E-07,3.03E-05,7.33E-07
11092,relation_extraction13,97,We concatenate these two vectors to get the single representation hr = he 1 |h e 2 where a|b is the concatenation of a and b.,Model,Positional embeddings,relation_extraction,13,31,1,0,,4.23E-06,0,negative,1.81E-06,4.69E-06,4.47E-05,1.81E-08,1.14E-07,2.13E-06,2.18E-06,4.71E-05,0.000420516,0.999457851,1.48E-07,1.84E-05,4.01E-07
11093,relation_extraction13,98,We refer to this architecture as MENTION POOLING .,Model,Positional embeddings,relation_extraction,13,32,1,0,,1.87E-05,0,negative,3.98E-06,1.89E-05,0.000455818,3.89E-07,5.38E-07,1.30E-05,1.06E-05,0.000214091,0.002796324,0.996453398,2.11E-06,2.46E-05,6.38E-06
11094,relation_extraction13,99,Entity start state,Model,Positional embeddings,relation_extraction,13,33,1,0,,8.63E-05,0,negative,1.37E-05,2.20E-05,0.000286254,6.80E-07,1.06E-06,2.50E-05,6.63E-05,0.000682736,0.002440034,0.995453086,9.11E-06,0.000967817,3.23E-05
11095,relation_extraction13,100,"Finally , we propose simply representing the relation between two entities with the concatenation of the final hidden states corresponding their respective start tokens , when EN - TITY MARKERS are used .",Model,Positional embeddings,relation_extraction,13,34,1,0,,5.53E-05,0,negative,8.97E-05,0.000266416,0.000935428,2.42E-07,2.17E-06,5.72E-06,1.09E-05,0.000136899,0.011496028,0.986806525,1.54E-06,0.0002454,3.01E-06
11096,relation_extraction13,101,"Recalling that ENTITY MARKERS inserts tokens in x , creating offsets in s 1 and s 2 , our representation of the relation is r h = hi |h j+2 .",Model,Positional embeddings,relation_extraction,13,35,1,0,,6.94E-07,0,negative,7.18E-07,3.32E-06,1.22E-05,1.27E-08,8.45E-08,1.19E-06,1.01E-06,3.42E-05,0.000272569,0.999661573,1.32E-07,1.28E-05,2.20E-07
11097,relation_extraction13,102,We refer to this output representation as ENTITY START output .,Model,Positional embeddings,relation_extraction,13,36,1,0,,2.38E-05,0,negative,6.38E-07,4.38E-06,9.26E-05,1.38E-08,8.51E-08,1.68E-06,1.85E-06,3.40E-05,0.00081239,0.999032965,2.56E-07,1.86E-05,5.72E-07
11098,relation_extraction13,103,Note that this can only be applied to the ENTITY MARKERS input .,Model,Positional embeddings,relation_extraction,13,37,1,0,,2.12E-06,0,negative,5.06E-06,1.44E-06,1.07E-05,1.73E-08,6.69E-08,5.20E-07,5.68E-07,1.77E-05,5.75E-05,0.999862131,5.21E-08,4.40E-05,1.28E-07
11099,relation_extraction13,104,illustrates a few of the variants we evaluated in this section .,Model,Positional embeddings,relation_extraction,13,38,1,0,,5.37E-07,0,negative,9.07E-07,3.29E-07,2.52E-06,3.20E-08,1.00E-07,1.61E-06,1.20E-06,3.27E-05,9.51E-06,0.999925662,1.63E-08,2.53E-05,1.35E-07
11100,relation_extraction13,105,"In addition to defining the model input and output architecture , we fix the training loss used to train the models ( which is illustrated in ) .",Model,Positional embeddings,relation_extraction,13,39,1,0,,1.53E-06,0,negative,5.87E-06,1.45E-05,5.87E-06,2.29E-07,6.35E-07,7.19E-06,3.45E-06,0.000746204,0.000174887,0.999009371,2.76E-08,3.12E-05,4.75E-07
11101,relation_extraction13,106,"In all models , the output representation from the Transformer network is fed into a fully connected layer that either contains a linear activation , or ( 2 ) performs layer normalization on the representation .",Model,Positional embeddings,relation_extraction,13,40,1,0,,0.000145938,0,negative,6.24E-05,0.000163062,0.00114648,8.84E-07,3.38E-06,4.58E-05,5.35E-05,0.001570674,0.007096001,0.989769625,3.79E-07,7.88E-05,9.01E-06
11102,relation_extraction13,107,We treat the choice of post Transfomer layer as a hyper - parameter and use the best performing layer type for each task .,Model,Positional embeddings,relation_extraction,13,41,1,0,,7.35E-06,0,negative,1.37E-05,0.000229386,4.72E-05,7.62E-07,1.74E-06,8.50E-05,4.73E-05,0.017447476,0.003520533,0.978558222,3.07E-07,4.17E-05,6.62E-06
11103,relation_extraction13,108,"For the supervised tasks , we introduce a new classification layer W ?",Model,Positional embeddings,relation_extraction,13,42,1,0,,3.22E-05,0,negative,6.46E-06,2.47E-05,2.66E-05,4.82E-08,2.74E-07,4.65E-06,5.71E-06,0.000486439,0.001073071,0.998327583,1.58E-07,4.34E-05,9.73E-07
11104,relation_extraction13,109,R KxH where H is the size of the relation representation and K is the number of relation types .,Model,Positional embeddings,relation_extraction,13,43,1,0,,5.85E-07,0,negative,1.21E-05,2.45E-06,3.17E-05,2.71E-08,2.55E-07,1.54E-06,3.75E-06,5.64E-05,3.43E-05,0.999616519,7.10E-08,0.000240354,5.06E-07
11105,relation_extraction13,110,The classification loss is the standard cross entropy of the softmax of hr W T with respect to the true relation type .,Model,Positional embeddings,relation_extraction,13,44,1,0,,1.68E-05,0,negative,5.79E-06,3.26E-05,3.31E-05,3.57E-07,6.48E-07,4.98E-05,3.29E-05,0.008830117,0.000758805,0.990220793,1.09E-07,3.02E-05,4.88E-06
11106,relation_extraction13,111,"For the few - shot task , we use the dot product between relation representation of the query statement and each of the candidate statements as a similarity score .",Model,Positional embeddings,relation_extraction,13,45,1,0,,2.08E-05,0,negative,7.85E-06,0.000115087,0.00046303,1.74E-08,6.75E-07,4.42E-06,1.46E-05,0.0003113,0.000781512,0.998167492,8.41E-08,0.000133148,7.44E-07
11107,relation_extraction13,112,"In this case , we also apply across entropy loss of the softmax of similarity scores with respect to the true class .",Model,Positional embeddings,relation_extraction,13,46,1,0,,1.72E-05,0,negative,3.74E-05,3.90E-05,0.000280488,2.87E-08,4.30E-07,1.71E-06,4.29E-06,8.54E-05,0.001077669,0.998326153,6.57E-08,0.000146703,6.26E-07
11108,relation_extraction13,113,"We perform task - specific fine - tuning of the BERT model , for all variants , with the following set of hyper - parameters : shows the results of model variants on the three supervised relation extraction tasks and the 5 - way - 1 - shot variant of the few - shot relation classification task .",Model,Positional embeddings,relation_extraction,13,47,1,0,,6.44E-05,0,negative,8.26E-05,8.03E-05,6.22E-05,4.47E-07,3.44E-06,3.25E-05,0.00010159,0.002848071,0.000261555,0.995754093,5.49E-08,0.000768301,4.89E-06
11109,relation_extraction13,114,"For all four tasks , the model using the ENTITY MARKERS input representation and ENTITY START output representation achieves the best scores .",Model,Positional embeddings,relation_extraction,13,48,1,0,,0.219592777,0,results,0.000997642,3.42E-06,3.85E-05,4.43E-07,9.41E-07,1.11E-05,0.001936092,0.000498277,6.91E-06,0.128552104,2.27E-07,0.867885652,6.87E-05
11110,relation_extraction13,115,"From the results , it is clear that adding positional information in the input is critical for the model to learn useful relation representations .",Model,Positional embeddings,relation_extraction,13,49,1,0,,0.037454427,0,negative,0.020099323,4.06E-05,8.15E-05,1.39E-06,6.83E-06,2.75E-05,0.00102339,0.001207465,0.000130076,0.688684481,5.49E-07,0.288645183,5.17E-05
11111,relation_extraction13,116,"Unlike previous work that have benefited from positional embeddings , the deep Transformers benefits the most from seeing the new entity boundary word pieces ( ENTITY MARKERS ) .",Model,Positional embeddings,relation_extraction,13,50,1,0,,0.020808009,0,results,0.004818551,2.35E-05,0.000339223,1.07E-06,4.38E-06,1.30E-05,0.001034043,0.00038819,7.62E-05,0.470069718,2.58E-07,0.523169348,6.26E-05
11112,relation_extraction13,117,It is also worth noting that the best variant outperforms previous published models on all four tasks .,Model,Positional embeddings,relation_extraction,13,51,1,0,,0.083599065,0,results,0.000623144,9.53E-06,6.46E-05,5.73E-06,4.63E-06,6.84E-05,0.004673061,0.001819965,2.27E-05,0.32869863,7.98E-07,0.663689401,0.000319401
11113,relation_extraction13,118,"For the remainder of the paper , we will use this architecture when further training and evaluating our models .",Model,Positional embeddings,relation_extraction,13,52,1,0,,3.54E-07,0,negative,1.12E-06,6.39E-06,1.28E-05,5.88E-08,2.04E-07,3.03E-06,2.16E-06,0.000189339,0.000316105,0.999459804,1.97E-08,8.40E-06,6.27E-07
11114,relation_extraction13,119,Learning by Matching the Blanks,Model,,relation_extraction,13,53,1,0,,0.021312466,0,negative,5.69E-05,0.00012353,0.000300633,9.14E-07,4.35E-06,4.66E-05,0.001187189,0.000779504,0.005285086,0.984193635,4.10E-05,0.007601836,0.000378897
11115,relation_extraction13,120,"So far , we have used human labeled training data to train our relation statement encoder f ? .",Model,Learning by Matching the Blanks,relation_extraction,13,54,1,0,,6.57E-06,0,negative,1.95E-06,1.21E-06,3.21E-06,3.17E-08,1.24E-07,1.34E-06,4.60E-06,3.86E-05,1.94E-06,0.999666905,9.68E-08,0.000279434,5.19E-07
11116,relation_extraction13,121,"Inspired by open information extraction , which derives relations directly from tagged text , we now introduce a new method of training f ? without a predefined ontology , or relation - labeled training data .",Model,Learning by Matching the Blanks,relation_extraction,13,55,1,0,,0.000158245,0,negative,0.000107472,0.000338965,0.000394563,4.72E-07,5.81E-06,4.02E-06,4.73E-05,0.000121997,0.000185216,0.992460016,1.55E-06,0.006323564,9.04E-06
11117,relation_extraction13,122,"Instead , we declare that for any pair of relation statements rand r , the inner product f ? ( r ) f ? ( r ) should be high if the two relation statements , rand r , express semantically similar relations .",Model,Learning by Matching the Blanks,relation_extraction,13,56,1,0,,2.78E-07,0,negative,7.88E-07,7.04E-07,2.49E-06,1.84E-09,1.85E-08,1.99E-07,2.46E-07,1.16E-05,1.43E-05,0.999952703,2.77E-09,1.69E-05,4.15E-08
11118,relation_extraction13,123,"And , this inner product should below if the two relation statements express semantically different relations .",Model,Learning by Matching the Blanks,relation_extraction,13,57,1,0,,3.11E-07,0,negative,1.18E-06,1.14E-07,9.47E-07,1.41E-09,8.32E-09,1.13E-07,1.67E-07,3.72E-06,2.60E-06,0.999964437,1.03E-09,2.67E-05,2.20E-08
11119,relation_extraction13,124,"Unlike related work in distant supervision for information extraction ) , we do not use relation labels at training time .",Model,Learning by Matching the Blanks,relation_extraction,13,58,1,0,,2.67E-06,0,negative,2.04E-05,1.35E-05,7.35E-05,8.53E-08,5.25E-07,1.48E-06,9.32E-06,2.89E-05,1.42E-05,0.998403108,3.42E-07,0.001432954,1.62E-06
11120,relation_extraction13,125,"Instead , we observe that there is a high degree of redundancy in web text , and each relation between an arbitrary pair of entities is likely to be stated multiple times .",Model,Learning by Matching the Blanks,relation_extraction,13,59,1,0,,9.82E-06,0,negative,3.42E-05,3.75E-06,1.38E-05,3.34E-08,4.93E-07,5.24E-07,4.33E-06,9.63E-06,7.06E-06,0.997296629,5.70E-08,0.002628818,6.55E-07
11121,relation_extraction13,126,"Subsequently , r = ( x , s 1 , s 2 ) is more likely to encode the same semantic relation as r = ( x , s 1 , s 2 ) if s 1 refers to the same entity ass 1 , and s 2 refers to the same entity ass 2 .",Model,Learning by Matching the Blanks,relation_extraction,13,60,1,0,,7.91E-07,0,negative,7.88E-07,2.63E-07,1.29E-06,7.07E-10,7.63E-09,1.42E-07,4.11E-07,1.05E-05,6.74E-06,0.999947433,1.03E-09,3.24E-05,3.52E-08
11122,relation_extraction13,127,"Starting with this observation , we introduce a new method of learning f ? from entity linked text .",Model,Learning by Matching the Blanks,relation_extraction,13,61,1,0,,8.76E-05,0,negative,0.000141214,0.000343586,0.000564768,5.94E-07,5.18E-06,5.47E-06,6.37E-05,0.000155516,0.000337701,0.993029733,1.23E-06,0.005335244,1.60E-05
11123,relation_extraction13,128,We introduce this method of learning by matching the blanks ( MTB ) .,Model,Learning by Matching the Blanks,relation_extraction,13,62,1,0,,9.72E-05,0,negative,2.65E-05,6.59E-05,0.001481952,2.34E-07,2.10E-06,4.56E-06,5.73E-05,7.23E-05,0.00018217,0.996686021,1.68E-06,0.001399905,1.94E-05
11124,relation_extraction13,129,In Section 5 we show that MTB learns relation representations that can be used without any further tuning for relation extraction - even beating previous work that trained on human labeled data .,Model,Learning by Matching the Blanks,relation_extraction,13,63,1,0,,5.58E-05,0,negative,9.54E-05,1.33E-05,9.12E-05,2.44E-08,5.20E-07,6.98E-07,2.09E-05,1.54E-05,3.07E-05,0.987639931,4.95E-08,0.012090173,1.70E-06
11125,relation_extraction13,130,Learning Setup,Model,,relation_extraction,13,64,1,0,,0.179801737,0,negative,5.30E-05,0.000686857,0.000115345,6.19E-06,2.34E-05,0.000656097,0.001650212,0.070666959,0.015331495,0.909893635,3.59E-07,0.000458572,0.000457941
11126,relation_extraction13,131,Let E be a predefined set of entities .,Model,Learning Setup,relation_extraction,13,65,1,0,,0.000230511,0,negative,6.84E-07,1.23E-05,9.16E-06,3.89E-08,8.17E-07,1.79E-05,2.54E-06,0.007863771,3.40E-05,0.992019483,1.46E-08,3.85E-05,9.21E-07
11127,relation_extraction13,132,"And let D = [ ( r 0 , e 0 1 , e 0 2 ) . . . ( r N , e N 1 , e N 2 ) ] be a corpus of relation statements that have been labeled with two entities e i 1 ?",Model,Learning Setup,relation_extraction,13,66,1,0,,0.000275804,0,negative,1.59E-06,1.36E-05,4.74E-05,4.66E-08,4.84E-06,1.45E-05,6.34E-06,0.002012613,1.20E-05,0.997714088,1.98E-08,0.000171811,1.14E-06
11128,relation_extraction13,133,E and e i 2 ?,Model,Learning Setup,relation_extraction,13,67,1,0,,0.000280641,0,negative,1.19E-05,1.74E-05,2.00E-05,1.98E-06,1.05E-05,0.000367814,7.16E-05,0.048016451,1.66E-05,0.950753459,7.13E-08,0.000695773,1.64E-05
11129,relation_extraction13,134,"E. Recall , from Section 2 , that r i = ( x i , s i 1 , s i 2 ) , where s i 1 and s i 2 delimit entity mentions in xi .",Model,Learning Setup,relation_extraction,13,68,1,0,,6.93E-05,0,negative,1.81E-06,4.91E-06,3.96E-05,2.62E-08,1.39E-06,1.11E-05,2.16E-06,0.000998027,1.47E-05,0.998853294,3.41E-09,7.24E-05,6.11E-07
11130,relation_extraction13,135,"Each item in Dis created by pairing the relation statement r i with the two entities e i 1 and e i 2 corresponding to the spans s i 1 and s i 2 , respectively .",Model,Learning Setup,relation_extraction,13,69,1,0,,0.000161773,0,negative,3.04E-06,2.03E-05,9.89E-05,1.72E-07,5.33E-06,4.71E-05,1.18E-05,0.005614428,8.54E-05,0.993991817,1.65E-08,0.000116123,5.56E-06
11131,relation_extraction13,136,We aim to learn a relation statement encoder f ?,Model,Learning Setup,relation_extraction,13,70,1,0,,0.005925799,0,negative,1.38E-06,9.11E-05,6.57E-05,2.75E-07,2.55E-06,8.01E-05,2.05E-05,0.036973185,0.000193918,0.96247209,2.51E-07,8.97E-05,9.26E-06
11132,relation_extraction13,137,that we can use to determine whether or not two relation statements encode the same relation .,Model,Learning Setup,relation_extraction,13,71,1,0,,0.000199433,0,negative,2.81E-06,1.47E-05,7.02E-05,3.21E-07,9.04E-06,1.59E-05,3.57E-06,0.001227859,2.97E-05,0.998536978,2.25E-08,8.65E-05,2.30E-06
11133,relation_extraction13,138,"To do this , we define the following binary classifier",Model,Learning Setup,relation_extraction,13,72,1,0,,0.001086251,0,negative,1.01E-06,6.81E-05,0.000605253,2.31E-08,1.08E-06,3.90E-05,1.22E-05,0.007731281,0.001236124,0.990265721,6.50E-08,3.50E-05,5.16E-06
11134,relation_extraction13,139,"to assign a probability to the case that rand r encode the same relation ( l = 1 ) , or not ( l = 0 ) .",Model,Learning Setup,relation_extraction,13,73,1,0,,5.13E-05,0,negative,1.06E-06,5.94E-06,1.66E-05,3.94E-08,8.65E-07,2.23E-05,2.72E-06,0.005451268,1.81E-05,0.99444374,3.78E-09,3.63E-05,1.11E-06
11135,relation_extraction13,140,"We will then learn the parameterization off ? that rA In 1976 , e1 ( then of Bell Labs ) published e 2 , the first of his books on programming inspired by the Unix operating system .",Model,Learning Setup,relation_extraction,13,74,1,0,,0.000282123,0,negative,1.24E-06,1.16E-05,3.14E-05,1.55E-06,3.83E-06,0.000166192,2.17E-05,0.014341784,4.17E-05,0.985280588,7.46E-08,8.76E-05,1.07E-05
11136,relation_extraction13,141,rB,Model,Learning Setup,relation_extraction,13,75,1,0,,0.002760383,0,negative,2.02E-06,1.28E-05,2.43E-05,2.71E-07,2.49E-06,0.000102715,4.59E-05,0.027078592,6.12E-05,0.972388777,1.00E-07,0.000262243,1.87E-05
11137,relation_extraction13,142,"The "" e2 "" series spread the essence of "" C / Unix thinking "" with makeovers for Fortran and Pascal .",Model,Learning Setup,relation_extraction,13,76,1,0,,0.003438106,0,negative,3.80E-05,6.79E-05,0.001490983,3.39E-05,0.000391084,0.001979905,0.000621942,0.049616777,8.85E-05,0.943512458,4.28E-08,0.001836868,0.000321577
11138,relation_extraction13,143,e 1's Ratfor was eventually put in the public domain .,Model,Learning Setup,relation_extraction,13,77,1,0,,0.000189633,0,negative,1.75E-05,1.29E-05,0.000251983,3.80E-07,3.22E-05,7.27E-05,4.12E-05,0.004101276,2.15E-05,0.994693367,3.62E-09,0.00074289,1.20E-05
11139,relation_extraction13,144,r C e 1 worked at Bell Labs alongside e 3 creators Ken Thompson and Dennis Ritchie .,Model,Learning Setup,relation_extraction,13,78,1,0,,0.000621326,0,negative,2.11E-05,2.77E-05,0.00129021,8.20E-07,2.44E-05,0.000123324,0.000186481,0.00584026,6.88E-05,0.990312657,1.01E-07,0.002034937,6.92E-05
11140,relation_extraction13,145,"Mentions e 1 = Brian Kernighan ,",Model,Learning Setup,relation_extraction,13,79,1,0,,0.000202108,0,negative,6.73E-06,4.73E-06,0.00011995,1.06E-06,6.79E-06,8.54E-05,2.35E-05,0.002729567,1.30E-05,0.99673484,4.02E-08,0.000263564,1.08E-05
11141,relation_extraction13,146,"e 2 = Software Tools , e3 = Unix ( 1 ) ? e 1 ,e 1 ? e 2 ,e 2 log p ( l = 1 |r , r ) + ( 1 ? ? e 1 ,e 1 ? e 2 ,e 2 ) log ( 1 ? p ( l = 1|r , r ) )",Model,Learning Setup,relation_extraction,13,80,1,0,,0.000348423,0,negative,2.53E-06,1.55E-05,9.35E-05,1.86E-07,3.99E-06,0.000357356,7.14E-05,0.037822351,3.28E-05,0.961437134,7.23E-09,0.000150031,1.31E-05
11142,relation_extraction13,147,where ?,Model,Learning Setup,relation_extraction,13,81,1,0,,6.86E-05,0,negative,3.75E-07,2.86E-06,5.11E-06,1.52E-07,6.33E-07,4.36E-05,4.28E-06,0.006846994,1.36E-05,0.993059999,6.01E-09,2.06E-05,1.70E-06
11143,relation_extraction13,148,"e ,e is the Kronecker delta that takes the value 1 iff e = e , and 0 otherwise .",Model,Learning Setup,relation_extraction,13,82,1,0,,0.000260601,0,negative,6.66E-07,1.50E-05,2.74E-05,3.09E-08,1.09E-06,2.64E-05,4.15E-06,0.008711726,7.73E-05,0.991119772,1.29E-09,1.52E-05,1.32E-06
11144,relation_extraction13,149,Introducing Blanks,Model,,relation_extraction,13,83,1,0,,0.000242649,0,negative,1.02E-05,6.30E-07,3.22E-06,3.08E-08,3.28E-07,2.99E-06,2.48E-05,5.78E-05,5.30E-05,0.999596673,1.75E-09,0.000245885,4.48E-06
11145,relation_extraction13,150,"Readers may have noticed that the loss in Equation 1 can be minimized perfectly by the entity linking system used to create D. And , since this linking system does not have any notion of relations , it is not reasonable to assume that f ? will somehow magically build meaningful relation representations .",Model,Introducing Blanks,relation_extraction,13,84,1,0,,8.20E-08,0,negative,1.06E-05,1.80E-07,1.75E-06,8.13E-10,1.60E-07,2.23E-07,2.21E-06,6.72E-07,2.32E-06,0.999976925,1.07E-10,4.85E-06,7.75E-08
11146,relation_extraction13,151,"To avoid simply relearning the entity linking system , we introduce a modified corpus where eachr i = ( x i , s i 1 , s i 2 ) contains a relation statement in which one or both entity mentions may have been replaced by a special [ BLANK ] symbol .",Model,Introducing Blanks,relation_extraction,13,85,1,0,,1.09E-07,0,negative,3.64E-05,2.33E-05,0.000157141,4.74E-08,0.000173278,2.90E-06,4.65E-05,9.89E-06,2.79E-05,0.999499632,5.37E-10,2.02E-05,2.81E-06
11147,relation_extraction13,152,"Specifically , x contains the span defined by s 1 with probability ?.",Model,Introducing Blanks,relation_extraction,13,86,1,0,,6.86E-07,0,negative,1.40E-05,4.27E-07,1.43E-06,1.82E-10,1.03E-07,9.46E-08,2.44E-06,7.39E-07,3.38E-06,0.999970605,3.98E-11,6.68E-06,5.43E-08
11148,relation_extraction13,153,"Otherwise , the span has been replaced with a single [ BLANK ] symbol .",Model,Introducing Blanks,relation_extraction,13,87,1,0,,3.54E-07,0,negative,2.31E-06,1.62E-07,1.13E-06,9.44E-10,2.18E-07,4.11E-07,1.79E-06,2.02E-06,3.68E-06,0.999987159,3.49E-11,1.01E-06,1.15E-07
11149,relation_extraction13,154,The same is true for s 2 .,Model,Introducing Blanks,relation_extraction,13,88,1,0,,2.04E-06,0,negative,5.39E-06,2.82E-07,1.98E-06,1.52E-09,1.41E-07,1.33E-06,3.17E-05,7.01E-06,5.30E-06,0.999938207,3.71E-10,7.46E-06,1.20E-06
11150,relation_extraction13,155,Only ?,Model,Introducing Blanks,relation_extraction,13,89,1,0,,5.45E-07,0,negative,8.89E-07,4.60E-08,9.99E-07,1.60E-09,1.87E-07,6.98E-07,5.23E-06,1.78E-06,1.02E-06,0.999987293,1.03E-10,1.44E-06,4.29E-07
11151,relation_extraction13,156,2 of the relation statements in D explicitly name both of the entities that participate in the relation .,Model,Introducing Blanks,relation_extraction,13,90,1,0,,1.82E-07,0,negative,1.11E-05,5.14E-07,4.90E-06,9.46E-09,5.69E-06,1.03E-06,4.22E-06,2.69E-06,2.55E-06,0.999963776,2.83E-11,3.20E-06,3.65E-07
11152,relation_extraction13,157,"As a result , minimizing L ( D ) requires f ? to do more than simply identifying named entities in r.",Model,Introducing Blanks,relation_extraction,13,91,1,0,,4.51E-07,0,negative,7.24E-06,5.51E-07,1.34E-06,2.52E-09,2.45E-07,6.19E-07,1.59E-05,4.11E-06,2.85E-06,0.999954027,1.01E-09,1.17E-05,1.42E-06
11153,relation_extraction13,158,We hypothesize that training on D will result in a f ? that encodes the semantic relation between the two possibly elided entity spans .,Model,Introducing Blanks,relation_extraction,13,92,1,0,,3.81E-07,0,negative,4.10E-06,2.87E-06,7.92E-06,1.71E-09,5.57E-07,3.67E-07,3.74E-06,4.62E-06,5.81E-05,0.999914695,1.62E-10,2.40E-06,6.25E-07
11154,relation_extraction13,159,Results in Section 5 support this hypothesis .,Model,Introducing Blanks,relation_extraction,13,93,1,0,,2.68E-07,0,negative,4.78E-06,5.21E-08,2.62E-07,6.61E-09,7.43E-07,1.63E-06,1.68E-05,5.43E-06,2.94E-07,0.999961177,2.97E-11,7.64E-06,1.15E-06
11155,relation_extraction13,160,Matching the Blanks Training,,,relation_extraction,13,0,1,0,,0.262864566,0,negative,0.000204545,0.00021721,0.000202138,3.53E-06,4.48E-06,8.23E-05,0.000918276,0.001011136,6.05E-05,0.923795341,0.061763924,0.011704616,3.20E-05
11156,relation_extraction13,161,"To train a model with matching the blank task , we construct a training setup similar to BERT , where two losses are used concurrently : the masked language model loss and the matching the blanks loss .",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,1,1,0,,0.000687269,0,negative,0.000490367,2.53E-05,0.012188969,6.66E-07,1.20E-06,5.85E-05,0.000473855,0.000410422,4.27E-05,0.985909169,3.48E-06,0.000379947,1.55E-05
11157,relation_extraction13,162,"For generating the training corpus , we use English Wikipedia and extract text passages from the HTML paragraph blocks , ignoring lists , and tables .",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,2,1,0,,0.040218437,0,negative,0.000192902,5.88E-06,0.000521777,6.60E-06,2.91E-05,0.000174999,0.000914736,0.000466752,5.16E-06,0.9972983,2.50E-06,0.000299841,8.14E-05
11158,relation_extraction13,163,"We use an off - the - shelf entity linking system 1 to annotate text spans with a unique knowledge base identifier ( e.g. , Freebase ID or Wikipedia URL ) .",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,3,1,0,,0.01269694,0,negative,0.001394686,0.00015596,0.118036407,2.37E-05,3.27E-05,0.000478851,0.004182277,0.001089351,0.000265334,0.871102816,6.37E-05,0.002313107,0.000861153
11159,relation_extraction13,164,"The span annotations include not only proper names , but other referential entities such as common nouns and pronouns .",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,4,1,0,,0.009293085,0,negative,0.000345421,5.83E-07,0.000500282,1.15E-06,4.22E-06,8.83E-05,0.000299552,0.000107178,3.31E-06,0.998094845,3.99E-07,0.000531633,2.31E-05
11160,relation_extraction13,165,From this annotated corpus we extract relation statements where each statement contains at least two grounded entities within a fixed sized window of tokens 2 .,Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,5,1,0,,0.001506151,0,negative,0.000453814,3.59E-06,0.000918614,3.31E-05,0.000125069,0.000133493,0.000456769,0.000102574,6.58E-06,0.99735728,1.33E-06,0.000291381,0.000116409
11161,relation_extraction13,166,"To prevent a large bias towards relation statements that involve popular entities , we limit the number of relation statements that contain the same entity by randomly sampling a constant number of relation statements that contain any given entity .",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,6,1,0,,0.000782996,0,negative,0.000441517,1.84E-05,0.000703702,4.66E-07,8.47E-07,0.000146374,0.000471304,0.002357162,3.34E-05,0.995629593,7.52E-07,0.000180397,1.60E-05
11162,relation_extraction13,167,We use these statements to train model parameters to minimize L ( D ) as described in the previous section .,Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,7,1,0,,3.90E-05,0,negative,0.000110697,5.53E-07,0.00036307,2.59E-08,4.24E-08,5.92E-06,1.80E-05,4.38E-05,6.12E-06,0.999393177,4.95E-08,5.78E-05,7.92E-07
11163,relation_extraction13,168,"In practice , it is not possible to compare every pair of relation statements , as in Equation 1 , and so we use a noise - contrastive estimation .",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,8,1,0,,0.000103311,0,negative,0.000356007,1.75E-06,0.002801241,3.39E-08,9.68E-08,6.37E-06,5.97E-05,2.60E-05,6.07E-06,0.996280578,9.45E-07,0.00045915,2.00E-06
11164,relation_extraction13,169,"In this estimation , we consider all positive pairs of relation statements that contain the same entity , so there is no change to the contribution of the first term in Equation 1 - where ? e 1 ,e 1 ? e 2 ,e 2 = 1 . The approximation does , however , change the contribution of the second term .",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,9,1,0,,7.33E-05,0,negative,0.000287225,5.87E-07,0.000204016,6.87E-08,1.17E-07,7.26E-06,2.78E-05,4.46E-05,3.77E-06,0.999202781,2.98E-07,0.000219262,2.22E-06
11165,relation_extraction13,170,"Instead of summing over all pairs of relation statements that do not contain the same pair of entities , we sample a set of negatives thatare either randomly sampled uniformly from the set of all relation statement pairs , or are sampled from the set of relation statements that share just a single 5 - way 5 - way 10 - way 10 - way 1 - shot 5 - shot :",Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,10,1,0,,0.000414261,0,negative,0.000547593,5.71E-06,0.006314434,1.09E-07,4.27E-07,3.76E-05,0.000355517,0.000237456,1.86E-05,0.992209432,2.59E-07,0.00026665,6.19E-06
11166,relation_extraction13,171,Test results for FewRel few - shot relation classification task .,Matching the Blanks Training,Matching the Blanks Training,relation_extraction,13,11,1,0,,0.017629805,0,negative,0.001125055,2.61E-07,0.001048281,2.78E-08,4.55E-08,6.56E-06,0.009518313,2.50E-05,1.12E-07,0.574005546,9.40E-06,0.414214607,4.68E-05
11167,relation_extraction13,172,Proto,Matching the Blanks Training,,relation_extraction,13,12,1,0,,0.000171946,0,negative,0.000122567,1.82E-07,0.00027481,1.50E-07,9.70E-08,2.19E-05,0.000258717,5.85E-05,3.75E-06,0.998806252,7.82E-07,0.000414497,3.78E-05
11168,relation_extraction13,173,Net is the best published system from .,Matching the Blanks Training,Proto,relation_extraction,13,13,1,0,,0.001876889,0,negative,0.000192609,4.43E-07,0.02905098,7.09E-07,7.23E-07,8.73E-05,0.000442656,0.00013643,3.62E-06,0.967737797,2.26E-06,0.00189836,0.000446155
11169,relation_extraction13,174,"At the time of writing , our BERTEM + MTB model outperforms the top model on the leaderboard ( http : //www.zhuhao.me/fewrel / ) by over 10 % on the 5 - way - 1 - shot and over 15 % on the 10 - way - 1 - shot configurations .",Matching the Blanks Training,Proto,relation_extraction,13,14,1,0,,0.819555464,1,results,0.0108471,7.27E-07,0.000311958,9.80E-07,6.70E-07,1.84E-05,0.002670122,8.52E-05,5.01E-07,0.166599461,1.96E-06,0.818707966,0.000754956
11170,relation_extraction13,175,entity .,Matching the Blanks Training,Proto,relation_extraction,13,15,1,0,,1.84E-05,0,negative,5.53E-05,1.12E-06,0.00020677,1.32E-06,1.93E-07,5.98E-05,1.98E-05,0.000387581,3.89E-05,0.999093008,1.81E-06,5.47E-05,7.97E-05
11171,relation_extraction13,176,"We include the second set ' hard ' negatives to account for the fact that most randomly sampled relation statement pairs are very unlikely to be even remotely topically related , and we would like to ensure that the training procedure sees pairs of relation statements that refer to similar , but different , relations .",Matching the Blanks Training,Proto,relation_extraction,13,16,1,0,,3.55E-05,0,negative,0.001262799,1.47E-06,0.001261725,3.25E-07,2.69E-06,1.29E-05,3.39E-05,5.29E-05,2.90E-06,0.996964105,7.36E-08,0.000395036,9.21E-06
11172,relation_extraction13,177,"Finally , we probabilistically replace each entity 's mention with [ BLANK ] symbols , with a probability of ? = 0.7 , as described in Section 3.2 , to ensure that the model is not confounded by the absence of [ BLANK ] symbols in the evaluation tasks .",Matching the Blanks Training,Proto,relation_extraction,13,17,1,0,,0.000734442,0,negative,0.000906103,4.09E-06,0.001004866,3.14E-07,3.95E-07,0.000135594,0.000131748,0.002280971,2.91E-05,0.995311273,1.25E-07,0.000155761,3.97E-05
11173,relation_extraction13,178,"In total , we generate 600 million relation statement pairs from English Wikipedia , roughly split between 50 % positive and 50 % strong negative pairs .",Matching the Blanks Training,Proto,relation_extraction,13,18,1,0,,0.00246342,0,negative,0.000436322,3.67E-06,0.000471779,2.07E-05,0.00013152,9.39E-05,0.000112521,0.000126167,3.80E-06,0.997949684,3.22E-07,0.000294116,0.000355484
11174,relation_extraction13,179,Experimental Evaluation,,,relation_extraction,13,0,1,0,,0.029987675,0,negative,9.39E-06,1.31E-05,9.58E-07,1.24E-07,4.09E-07,2.84E-05,3.01E-05,0.000343042,3.40E-06,0.999152904,0.000205721,0.000211944,5.00E-07
11175,relation_extraction13,180,"In this section , we evaluate the impact of training by matching the blanks .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,1,1,0,,0.009910661,0,negative,0.006858698,3.47E-05,0.003048081,6.66E-07,2.03E-06,0.000256635,0.000836145,0.00083166,4.23E-06,0.972811786,9.35E-06,0.015292679,1.33E-05
11176,relation_extraction13,181,"We start with the best BERT based model from Section 3.3 , which we call BERT EM , and we compare this to a variant that is trained with the matching the blanks task ( BERT EM + MTB ) .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,2,1,0,,0.054634989,0,negative,0.00265057,0.000408299,0.430459418,8.72E-06,1.52E-05,0.001683939,0.01025713,0.002970849,5.24E-05,0.539250036,0.000197117,0.011853459,0.000192796
11177,relation_extraction13,182,We train the BERT EM + MTB model by initializing the Transformer weights to the weights from BERT LARGE and use the following parameters :,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,3,1,0,,0.067721611,0,negative,0.000873467,7.70E-05,0.001832224,4.23E-06,1.99E-06,0.073065191,0.004877015,0.277939454,4.89E-05,0.640628413,1.50E-05,0.00055714,7.99E-05
11178,relation_extraction13,183,"We report results on all of the tasks from Section 3.1 , using the same task - specific training methodology for both BERT EM and BERT EM + MTB .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,4,1,0,,0.165538057,0,negative,0.001412215,8.09E-06,0.000413461,6.08E-07,1.35E-06,0.00025689,0.002697369,0.000549595,5.95E-07,0.926215538,1.06E-05,0.068419852,1.38E-05
11179,relation_extraction13,184,Few - shot Relation Matching,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,5,1,0,,0.35848705,0,negative,0.001607483,5.99E-05,0.129543152,6.54E-06,3.82E-06,0.000578659,0.095225871,0.001334493,7.65E-06,0.383400907,0.02269732,0.363507398,0.002026827
11180,relation_extraction13,185,"First , we investigate the ability of BERT EM + MTB to solve the FewRel task without any task - specific training data .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,6,1,0,,0.072316935,0,negative,0.016673074,0.000333675,0.013476854,1.01E-05,2.78E-05,0.000678509,0.006176685,0.001934169,1.96E-05,0.854624279,0.000132457,0.105733689,0.000179109
11181,relation_extraction13,186,"Since FewRel is an exemplar - based approach , we can just rank each candidate rela - tion statement according to its representation 's inner product with the exemplars ' representations .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,7,1,0,,0.000684717,0,negative,0.003230101,3.53E-05,0.042220957,4.43E-07,1.25E-06,0.000238883,0.000238806,0.000527403,3.39E-05,0.947246533,1.03E-05,0.006200748,1.54E-05
11182,relation_extraction13,187,shows that the task agnostic BERT EM and BERT EM + MTB models outperform the previous published state of the art on FewRel task even when they have not seen any FewRel training data .,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,8,1,1,results,0.701449394,1,results,0.002405562,3.08E-07,0.000304236,1.84E-07,2.00E-07,2.66E-05,0.003841341,5.64E-05,3.95E-08,0.035913054,6.89E-06,0.957397918,4.72E-05
11183,relation_extraction13,188,"For BERT EM + MTB , the increase over 's supervised approach is very significant - 8.8 % on the 5 - way - 1 - shot task and 12.7 % on the 10 - way - 1 - shot task .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,9,1,1,results,0.870961083,1,results,0.014532645,1.15E-06,0.000306128,5.32E-07,9.55E-07,2.69E-05,0.006219293,0.000102328,1.40E-07,0.036495314,4.85E-06,0.94218998,0.000119778
11184,relation_extraction13,189,"BERT EM + MTB also significantly outperforms BERT EM in this unsupervised setting , which is to be expected since there is no relation - specific loss during BERT EM 's training .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,10,1,1,results,0.957345883,1,results,0.005518542,9.19E-07,0.000232595,1.00E-06,8.45E-07,7.84E-05,0.010977947,0.000191687,1.13E-07,0.025395523,6.21E-06,0.957434974,0.000161224
11185,relation_extraction13,190,"To investigate the impact of supervision on BERT EM and BERT EM + MTB , we introduce increasing amounts of FewRel 's training data .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,11,1,0,,0.017938995,0,negative,0.019928968,0.000141286,0.003765295,3.69E-06,2.21E-05,0.00061545,0.002591687,0.002493452,1.19E-05,0.946684974,4.69E-06,0.023677613,5.89E-05
11186,relation_extraction13,191,"shows the increase in performance as we either increase the number of training examples for each relation type , or we increase the number of relation types in the training data .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,12,1,0,,0.104266032,0,negative,0.007369337,1.37E-06,0.001482096,7.08E-07,1.35E-06,0.00019347,0.002403317,0.000388223,7.65E-07,0.854602176,7.86E-06,0.133492058,5.73E-05
11187,relation_extraction13,192,"When given access to all of the training data , BERT EM approaches BERT EM + MTB 's performance .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,13,1,1,results,0.833386097,1,results,0.012896732,4.12E-06,0.000649329,2.74E-06,2.27E-06,0.00025507,0.010337811,0.000789608,9.49E-07,0.082842122,2.22E-05,0.89184453,0.000352544
11188,relation_extraction13,193,"However , when we keep all relation types during training , and vary the number of types per example , BERT EM + MTB only needs 6 % of the training data to match the performance of a BERT EM model trained on all of the training data .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,14,1,0,,0.114691651,0,results,0.028769813,1.57E-05,0.00113401,4.89E-06,1.05E-05,0.00070395,0.020625459,0.002310922,2.85E-06,0.309120002,2.52E-05,0.636511465,0.000765165
11189,relation_extraction13,194,"We observe that maintaining a diversity of relation types , and reducing the number of examples per type , is the most effective way to reduce annotation effort for this task .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,15,1,0,,0.858125833,1,results,0.04067596,1.01E-05,0.000355437,2.42E-06,4.20E-06,0.000173833,0.004822127,0.000539106,1.65E-06,0.228861865,2.38E-05,0.724353645,0.000175772
11190,relation_extraction13,195,The results in show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system .,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,16,1,1,results,0.187546575,0,results,0.026891239,5.86E-06,0.000715799,9.78E-07,4.44E-06,6.02E-05,0.002157615,0.000142051,1.42E-06,0.451397516,1.28E-05,0.518532936,7.71E-05
11191,relation_extraction13,196,"Finally , we report BERT EM + MTB 's performance on all of FewRel 's fully supervised tasks in Table 3 .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,17,1,0,,0.181783988,0,results,0.001808514,5.62E-07,0.00014876,1.75E-07,6.02E-07,3.40E-05,0.004638015,7.73E-05,5.70E-08,0.167414697,2.17E-06,0.825843659,3.15E-05
11192,relation_extraction13,197,"We see that it outperforms the human upper bound reported by , and it significantly outperforms all other submissions to the FewRel leaderboard , published or unpublished .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,18,1,0,,0.75520662,1,results,0.002771957,1.95E-06,0.000164563,2.13E-06,2.38E-06,0.00016476,0.014338035,0.000418282,4.06E-07,0.085985486,6.51E-06,0.895754596,0.000388941
11193,relation_extraction13,198,outperform previously published results for these three tasks .,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,19,1,0,,0.008804182,0,negative,0.003666572,9.78E-07,0.000373391,1.56E-06,3.06E-06,0.000167445,0.003830081,0.000301579,5.08E-07,0.627477132,4.43E-06,0.364037869,0.000135398
11194,relation_extraction13,199,The additional MTB based training further increases F 1 scores for all tasks .,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,20,1,1,results,0.946612963,1,results,0.295887883,3.82E-06,0.000232131,3.87E-06,4.36E-06,8.11E-05,0.006741039,0.000320185,1.16E-06,0.054632441,1.71E-06,0.641763995,0.000326299
11195,relation_extraction13,200,We also analyzed the performance of our two models while reducing the amount of supervised task specific tuning data .,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,21,1,0,,0.004988423,0,negative,0.001190333,3.72E-06,0.000177688,1.97E-07,1.74E-06,6.97E-05,0.000158953,0.000174346,1.19E-06,0.993099153,3.47E-07,0.005117787,4.86E-06
11196,relation_extraction13,201,The results displayed in show the development set performance when tuning on a random subset of the task specific training data .,Experimental Evaluation,Experimental Evaluation,relation_extraction,13,22,1,0,,0.007258923,0,negative,0.000459157,4.28E-07,5.74E-05,7.68E-08,4.09E-07,6.53E-05,0.00038333,0.000169295,1.68E-07,0.981589114,3.42E-07,0.017269823,5.13E-06
11197,relation_extraction13,202,"For all tasks , we see that MTB based training is even more effective for low - resource cases , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,23,1,1,results,0.935111252,1,results,0.016034924,4.20E-06,0.000166141,2.43E-06,2.31E-06,0.00034687,0.015326954,0.001380951,8.37E-07,0.086842518,7.35E-06,0.879457092,0.000427422
11198,relation_extraction13,203,"This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors , and populate a knowledge base .",Experimental Evaluation,Experimental Evaluation,relation_extraction,13,24,1,1,results,0.001096505,0,negative,0.005024491,1.20E-06,0.000269967,1.98E-07,1.18E-06,4.18E-05,0.000312241,7.15E-05,6.73E-07,0.954881117,1.09E-06,0.039381128,1.34E-05
11199,relation_extraction13,204,Supervised Relation Extraction,Experimental Evaluation,,relation_extraction,13,25,1,0,,0.741180132,1,negative,0.001547329,4.01E-05,0.014351124,7.41E-06,5.08E-06,0.000437705,0.04979,0.001192063,1.49E-05,0.534718944,0.006583134,0.386545664,0.00476658
11200,relation_extraction13,205,Conclusion and Future Work,,,relation_extraction,13,0,1,0,,0.000993755,0,negative,6.55E-05,5.57E-05,6.74E-06,8.33E-07,5.66E-07,8.35E-05,7.13E-05,0.000591015,4.69E-05,0.993684537,0.004983168,0.000405376,4.84E-06
11201,sentiment_analysis9,1,title,,,sentiment_analysis,9,0,1,0,,0.000714258,0,negative,8.57E-05,0.000300427,3.36E-06,0.000105593,6.00E-06,0.000657906,5.28E-05,0.005573506,0.000349385,0.991854873,0.000977057,1.91E-05,1.43E-05
11202,sentiment_analysis9,2,Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction,title,title,sentiment_analysis,9,1,1,1,research-problem,0.978250731,1,research-problem,7.87E-08,1.20E-05,4.31E-08,6.67E-06,6.18E-07,1.69E-06,1.94E-06,6.41E-06,2.22E-06,0.015873538,0.984094547,5.82E-08,2.09E-07
11203,sentiment_analysis9,3,abstract,,,sentiment_analysis,9,0,1,0,,0.00572094,0,negative,0.000163604,0.000508067,6.11E-06,0.000175665,8.82E-06,0.001038826,0.000108262,0.00898361,0.000688488,0.985671033,0.002575566,3.93E-05,3.26E-05
11204,sentiment_analysis9,4,Proposing a model for the joint task of aspect term extraction and aspect polarity classification .,abstract,abstract,sentiment_analysis,9,1,1,0,,0.009740351,0,research-problem,5.32E-08,1.18E-05,3.72E-08,2.44E-07,1.23E-07,1.88E-07,4.15E-07,2.19E-06,1.95E-06,0.010216331,0.989766556,6.68E-08,5.38E-08
11205,sentiment_analysis9,5,"The model proposed is Chinese language - oriented and applicable to the English language , with the ability to handle both Chinese and English reviews .",abstract,abstract,sentiment_analysis,9,2,1,0,,0.071813665,0,negative,1.09E-05,0.09261106,0.000137772,1.05E-05,0.000160824,3.56E-05,8.97E-06,0.000360354,0.046513039,0.45800232,0.402143043,3.72E-06,1.93E-06
11206,sentiment_analysis9,6,The model also integrates the domain - adapted BERT model for enhancement .,abstract,abstract,sentiment_analysis,9,3,1,0,,0.39512525,0,negative,9.09E-05,0.086247402,0.001958359,2.55E-05,0.000297833,0.000167876,3.51E-05,0.000718991,0.267260043,0.515403535,0.127778344,9.06E-06,7.07E-06
11207,sentiment_analysis9,7,The model achieves state - of - the - art performance on seven ABSA datasets .,abstract,abstract,sentiment_analysis,9,4,1,0,,0.011192194,0,research-problem,4.71E-05,0.009996654,2.70E-05,1.18E-05,6.46E-05,3.74E-05,0.000100733,0.000846325,0.000391179,0.3319499,0.656191574,0.00032869,7.13E-06
11208,sentiment_analysis9,8,Aspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .,abstract,abstract,sentiment_analysis,9,5,1,1,research-problem,0.925401656,1,research-problem,2.93E-07,2.33E-05,2.60E-07,5.17E-05,1.26E-05,1.66E-06,6.80E-06,2.32E-06,5.85E-07,0.01049977,0.989399785,1.70E-07,7.73E-07
11209,sentiment_analysis9,9,Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .,abstract,abstract,sentiment_analysis,9,6,1,1,research-problem,0.089787354,0,research-problem,1.15E-07,7.01E-05,7.49E-08,2.36E-06,2.09E-06,1.38E-06,9.49E-07,9.17E-06,2.43E-06,0.074336878,0.925574253,8.59E-08,1.13E-07
11210,sentiment_analysis9,10,"Besides , the existing researches do not pay attention to the research of the Chinese - oriented ABSA task .",abstract,abstract,sentiment_analysis,9,7,1,0,,0.008525691,0,research-problem,7.67E-08,4.89E-05,2.84E-08,2.78E-06,1.69E-06,2.44E-06,9.04E-07,1.82E-05,2.17E-06,0.149063617,0.85085901,6.46E-08,8.98E-08
11211,sentiment_analysis9,11,"Based on the local context focus ( LCF ) mechanism , this paper firstly proposes a multi-task learning model for Chineseoriented aspect - based sentiment analysis , namely LCF - ATEPC .",abstract,abstract,sentiment_analysis,9,8,1,0,,0.85891493,1,research-problem,8.76E-06,0.044730647,7.43E-05,5.77E-06,6.31E-05,9.03E-06,1.88E-05,0.000152704,0.002012135,0.061874326,0.891040017,8.17E-06,2.27E-06
11212,sentiment_analysis9,12,"Compared with existing models , this model equips the capability of extracting aspect term and inferring aspect term polarity synchronously , moreover , this model is effective to analyze both Chinese and English comments simultaneously and the experiment on a multilingual mixed dataset proved its availability .",abstract,abstract,sentiment_analysis,9,9,1,0,,0.049757895,0,negative,0.000322812,0.028169645,6.97E-05,2.19E-05,0.000311166,3.95E-05,0.000112711,0.000565819,0.00184864,0.6681928,0.299790764,0.000546413,8.14E-06
11213,sentiment_analysis9,13,"By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets .",abstract,abstract,sentiment_analysis,9,10,1,1,research-problem,0.061675981,0,research-problem,0.0001947,0.037707836,8.01E-05,3.94E-05,0.000206314,0.000106743,0.00017218,0.001796034,0.004409783,0.367378871,0.587401864,0.000487704,1.85E-05
11214,sentiment_analysis9,14,"Besides , the experimental results on the most commonly used SemEval - 2014 task 4 Restaurant and Laptop datasets outperform the state - of - the - art performance on the ATE",abstract,abstract,sentiment_analysis,9,11,1,0,,0.009346607,0,negative,4.58E-05,0.001032139,3.02E-06,3.72E-05,0.000177066,4.98E-05,0.000284967,0.000511311,1.64E-05,0.722832731,0.274338175,0.000663148,8.13E-06
11215,sentiment_analysis9,15,Introduction,,,sentiment_analysis,9,0,1,0,,0.000111703,0,negative,0.000763942,0.000358669,2.54E-05,0.003741684,6.15E-05,0.001399489,0.00042687,0.002109524,0.000310948,0.982266325,0.008366291,7.03E-05,9.90E-05
11216,sentiment_analysis9,16,"Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects .",Introduction,Introduction,sentiment_analysis,9,1,1,1,research-problem,0.757828716,1,research-problem,2.02E-06,0.00040536,1.36E-06,4.39E-06,1.34E-05,3.01E-06,1.64E-05,4.82E-06,0.000107927,0.023934786,0.97550185,2.71E-06,1.95E-06
11217,sentiment_analysis9,17,"For example , given a restaurant review :",Introduction,Introduction,sentiment_analysis,9,2,1,0,,0.021915507,0,negative,1.06E-05,0.035346744,1.84E-05,8.88E-07,0.000140762,3.69E-05,3.76E-05,9.70E-05,0.073849386,0.72808087,0.16236116,1.79E-05,1.74E-06
11218,sentiment_analysis9,18,""" The dessert at this restaurant is delicious but the service is poor , "" the full - designed model for ABSA needs to extract the aspects "" dessert "" and "" service "" and correctly reason about their polarity .",Introduction,Introduction,sentiment_analysis,9,3,1,0,,0.019689737,0,negative,1.05E-05,0.001430007,3.18E-06,0.000101085,0.001829784,0.000102351,1.71E-05,3.18E-05,0.001085975,0.941193696,0.05418926,2.55E-06,2.63E-06
11219,sentiment_analysis9,19,"In this review , the consumers ' opinions on "" dessert "" and "" service "" are not consistent , with positive and negative sentiment polarity respectively .",Introduction,Introduction,sentiment_analysis,9,4,1,0,,0.027820857,0,negative,3.79E-05,0.007705329,5.20E-06,0.000106146,0.005815022,8.15E-05,5.86E-05,8.09E-05,0.000638114,0.900003346,0.085447528,1.50E-05,5.41E-06
11220,sentiment_analysis9,20,"Generally , aspects and their polarity need to be manually labeled before running the aspect polarity classification procedure in the supervised deep learning models .",Introduction,Introduction,sentiment_analysis,9,5,1,0,,0.144876466,0,negative,2.49E-05,0.011571549,3.01E-06,2.73E-05,0.000895486,3.66E-05,1.92E-05,7.78E-05,0.001834594,0.834143986,0.151354157,8.97E-06,2.44E-06
11221,sentiment_analysis9,21,"However , most of the proposed models for aspect - based sentiment analysis tasks only focus on improving the classification accuracy of aspect polarity and ignore the research of aspect term extraction .",Introduction,Introduction,sentiment_analysis,9,6,1,0,,0.528006572,1,research-problem,1.89E-06,0.000333205,3.98E-07,7.38E-06,1.95E-05,8.78E-06,1.62E-05,1.37E-05,5.93E-05,0.103761049,0.895774596,2.10E-06,1.90E-06
11222,sentiment_analysis9,22,"Therefore , when conducting transfer learning on aspect - based sentiment analysis , those proposed models often fall into the dilemma of lacking aspect extraction method on targeted tasks because there is not enough research support .",Introduction,Introduction,sentiment_analysis,9,7,1,0,,0.333977035,0,research-problem,2.41E-06,0.00061532,3.48E-07,8.67E-06,3.10E-05,8.68E-06,1.23E-05,1.67E-05,7.92E-05,0.181879339,0.817342422,2.14E-06,1.46E-06
11223,sentiment_analysis9,23,The APC task is a kind of classification problem .,Introduction,Introduction,sentiment_analysis,9,8,1,1,research-problem,0.684373273,1,research-problem,8.80E-07,0.000549623,6.33E-07,1.36E-05,2.99E-05,1.68E-05,1.72E-05,2.41E-05,0.000170283,0.111793389,0.887379214,1.47E-06,2.91E-06
11224,sentiment_analysis9,24,"The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer .",Introduction,Introduction,sentiment_analysis,9,9,1,1,research-problem,0.794135986,1,research-problem,1.20E-06,0.000634905,5.25E-07,3.26E-06,9.00E-06,1.51E-05,1.88E-05,3.15E-05,0.000339072,0.135402,0.863540975,1.95E-06,1.64E-06
11225,sentiment_analysis9,25,"The purpose of the APC task is to predict the exact sentiment polarity of different aspects in their context , rather than to fuzzily analyze the over all sentiment polarity on the sentence - level or document - level .",Introduction,Introduction,sentiment_analysis,9,10,1,0,,0.868620956,1,research-problem,2.81E-06,0.003899978,2.41E-06,4.56E-05,0.000208926,2.67E-05,3.49E-05,4.20E-05,0.000580127,0.131251593,0.863893633,4.08E-06,7.26E-06
11226,sentiment_analysis9,26,"In the APC task , the polarities are most usually classified into three categories : positive , negative , and neutral .",Introduction,Introduction,sentiment_analysis,9,11,1,0,,0.04455597,0,research-problem,3.06E-06,0.006032327,3.49E-06,5.18E-06,9.23E-05,3.18E-05,2.98E-05,7.09E-05,0.003202532,0.428656157,0.561862861,6.56E-06,3.01E-06
11227,sentiment_analysis9,27,"It is obvious that the sentiment polarity classified based on aspects can better mine the fine - grained emotional tendency in reviews or tweets , thus providing a more accurate reference for decision - makers .",Introduction,Introduction,sentiment_analysis,9,12,1,0,,0.316720865,0,negative,0.001478943,0.088986015,5.29E-05,1.46E-05,0.001170042,9.02E-05,0.000388004,0.000235503,0.04329098,0.820525452,0.043056137,0.000700086,1.11E-05
11228,sentiment_analysis9,28,"Similar to the named entity recognition ( NER ) task , the ATE task is a sequence labeling task , which aims to extract aspects from the reviews or tweet .",Introduction,Introduction,sentiment_analysis,9,13,1,0,,0.721011303,1,research-problem,3.58E-06,0.000755226,2.80E-06,0.000179738,0.000510833,5.35E-05,0.000116147,3.21E-05,7.91E-05,0.177118494,0.821124224,6.63E-06,1.76E-05
11229,sentiment_analysis9,29,"In most researches ; , the ATE task is studied independently , away from the APC task .",Introduction,Introduction,sentiment_analysis,9,14,1,0,,0.090785649,0,negative,5.14E-06,0.004159495,2.76E-06,2.40E-05,0.000149273,0.000122843,7.30E-05,0.000190115,0.001287811,0.640571776,0.353400407,7.33E-06,6.14E-06
11230,sentiment_analysis9,30,The ATE task first segments a review into separate tokens and then infers whether the tokens belong to any aspect .,Introduction,Introduction,sentiment_analysis,9,15,1,0,,0.901347417,1,research-problem,3.67E-05,0.033833528,0.000119336,0.000138341,0.003802352,9.54E-05,0.000361098,9.08E-05,0.003205216,0.299537312,0.65868366,5.71E-05,3.92E-05
11231,sentiment_analysis9,31,"The tokens maybe labeled in different forms in different studies , but most of the studies have adopted the IOB 2 label to annotate tokens .",Introduction,Introduction,sentiment_analysis,9,16,1,0,,0.071183485,0,negative,6.08E-06,0.002710831,2.85E-06,9.27E-05,0.003271318,0.00026116,6.75E-05,0.00018259,0.000466156,0.973642812,0.01928268,6.77E-06,6.55E-06
11232,sentiment_analysis9,32,"Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously , this paper proposes a multi-task learning model for aspect - based sentiment analysis .",Introduction,Introduction,sentiment_analysis,9,17,1,1,model,0.958931895,1,approach,0.00014288,0.543524607,0.000289543,5.03E-05,0.002624829,0.000114898,0.000467638,0.000255146,0.118668832,0.105206083,0.228496678,0.000112958,4.56E-05
11233,sentiment_analysis9,33,Multilingual processing is an important research orientation of natural language processing .,Introduction,Introduction,sentiment_analysis,9,18,1,0,,0.27443877,0,research-problem,3.46E-06,0.000877964,3.00E-06,9.18E-06,4.53E-05,1.77E-05,0.000121629,2.41E-05,0.000237549,0.116016313,0.882621584,1.35E-05,8.78E-06
11234,sentiment_analysis9,34,The LCF - ATEPC 3 model proposed in this paper is a novel multilingual and multi-task - oriented model .,Introduction,Introduction,sentiment_analysis,9,19,1,1,model,0.955170885,1,model,8.04E-06,0.050936391,0.000225591,2.92E-07,6.22E-05,1.16E-05,2.99E-05,1.62E-05,0.940980122,0.006371759,0.001350697,5.27E-06,1.93E-06
11235,sentiment_analysis9,35,"Apart from achieving state - of - the - art performance in commonly used SemEval - 2014 task4 datasets , the experimental results in four Chinese review datasets also validate that this model has a strong ability to expand and adapt to the needs of multilingual task .",Introduction,Introduction,sentiment_analysis,9,20,1,0,,0.042499528,0,negative,0.003336244,0.059976959,8.48E-05,2.03E-05,0.001707139,0.0002047,0.005271737,0.000571644,0.015524249,0.8949533,0.009441591,0.008846498,6.08E-05
11236,sentiment_analysis9,36,"The proposed model is based on multi-head self - attention ( MHSA ) and integrates the pre-trained and the local context focus mechanism , namely LCF - ATEPC .",Introduction,Introduction,sentiment_analysis,9,21,1,1,model,0.962748034,1,model,8.77E-06,0.067286147,0.000138428,5.94E-07,9.73E-05,1.29E-05,1.41E-05,1.82E-05,0.926579281,0.00544923,0.000391188,2.34E-06,1.47E-06
11237,sentiment_analysis9,37,"By training on a small amount of annotated data of aspect and their polarity , the model can be adapted to a large - scale dataset , automatically extracting the aspects and predicting the sentiment polarities .",Introduction,Introduction,sentiment_analysis,9,22,1,1,model,0.861684196,1,model,0.000117687,0.338606652,4.75E-05,1.35E-05,0.001554162,0.000126391,0.000136203,0.000466512,0.402601106,0.253043936,0.003205572,7.00E-05,1.08E-05
11238,sentiment_analysis9,38,"In this way , the model can discover the unknown aspects and avoids the tedious and huge cost of manually annotating all aspects and polarities .",Introduction,Introduction,sentiment_analysis,9,23,1,0,,0.810793089,1,model,5.77E-05,0.226230079,2.65E-05,8.83E-06,0.000766936,7.46E-05,5.87E-05,0.000224623,0.574090955,0.196038371,0.002389928,2.75E-05,5.27E-06
11239,sentiment_analysis9,39,It is of great significance for the field - specific aspect - based sentiment analysis .,Introduction,Introduction,sentiment_analysis,9,24,1,0,,0.038690178,0,negative,8.44E-06,0.00364911,3.50E-06,5.19E-05,0.000307738,0.000103098,0.000127067,0.000117416,0.001021634,0.619545768,0.375036531,1.61E-05,1.17E-05
11240,sentiment_analysis9,40,The main contributions of this article are as follows :,Introduction,Introduction,sentiment_analysis,9,25,1,0,,0.000944493,0,negative,1.30E-05,0.005550308,6.78E-06,0.00014401,0.00092413,0.000447506,7.18E-05,0.000331058,0.005610498,0.983150088,0.00373737,5.39E-06,8.01E-06
11241,sentiment_analysis9,41,1 .,Introduction,Introduction,sentiment_analysis,9,26,1,0,,0.001493913,0,negative,7.55E-06,0.002564261,2.03E-06,8.97E-06,0.000153892,0.00022231,2.66E-05,0.000278696,0.009766064,0.98651799,0.000447403,2.70E-06,1.55E-06
11242,sentiment_analysis9,42,"For the first time , this paper studies the multi -task model of APC subtask and ATE subtask for multilingual reviews , which provides a new idea for the research of Chinese aspect extraction .",Introduction,Introduction,sentiment_analysis,9,27,1,0,,0.910374366,1,approach,0.000204883,0.688270668,0.000521298,2.79E-05,0.005516471,0.000160653,0.000918506,0.0002573,0.168984074,0.115669287,0.019216868,0.000212672,3.94E-05
11243,sentiment_analysis9,43,"2 . This paper firstly applies self - attention and local context focus techniques to aspect word extraction task , and fully explore their potential in aspect term extraction task .",Introduction,Introduction,sentiment_analysis,9,28,1,0,,0.804461334,1,approach,0.000106723,0.505426303,0.00021081,0.000359225,0.009445768,0.001229906,0.000778439,0.001866311,0.134462925,0.332336633,0.013593108,7.80E-05,0.000105879
11244,sentiment_analysis9,44,"3 . The LCF - ATEPC model proposed in this paper integrates the pre-trained BERT model , significantly improves both the performance of ATE task and APC subtask , and achieves new state - of - the - art performance especially the F 1 score of ATE task .",Introduction,Introduction,sentiment_analysis,9,29,1,0,,0.466159974,0,negative,0.004560817,0.170369472,0.000311412,5.85E-05,0.001756428,0.000851985,0.040660528,0.002537541,0.050868773,0.69541407,0.017334372,0.014761052,0.000515001
11245,sentiment_analysis9,45,"Besides , we adopted the domain - adapted BERT model trained on the domain - related",Introduction,Introduction,sentiment_analysis,9,30,1,0,,0.839286168,1,model,2.96E-05,0.133244178,0.000424331,1.22E-06,0.000462764,9.70E-05,0.000108377,0.000112218,0.833248843,0.032147827,0.00011263,7.11E-06,3.87E-06
11246,sentiment_analysis9,46,"The labels adopted in this paper are : , ,",Introduction,Introduction,sentiment_analysis,9,31,1,0,,0.003350009,0,negative,3.34E-06,0.016076392,6.55E-06,6.07E-06,0.000460738,0.000397066,4.93E-05,0.000767551,0.074342606,0.90768462,0.000200462,2.81E-06,2.54E-06
11247,sentiment_analysis9,47,3,Introduction,Introduction,sentiment_analysis,9,32,1,0,,0.001003519,0,negative,8.32E-06,0.00194545,2.30E-06,5.32E-06,0.000199355,0.00013305,4.11E-05,0.000154971,0.008703648,0.988596676,0.00020356,4.68E-06,1.56E-06
11248,sentiment_analysis9,48,The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPC,Introduction,Introduction,sentiment_analysis,9,33,1,1,code,0.97974539,1,code,3.97E-07,1.86E-06,1.37E-07,0.998482773,0.00108578,0.00027196,2.54E-06,1.37E-06,3.96E-07,0.000150468,1.53E-07,7.81E-09,2.16E-06
11249,sentiment_analysis9,49,corpus to the ABSA joint - task learning model .,Introduction,Introduction,sentiment_analysis,9,34,1,0,,0.061299943,0,negative,2.14E-05,0.008517696,9.20E-05,4.81E-05,0.014300671,0.00051979,0.000598056,0.000204477,0.005821868,0.968783143,0.001008183,5.25E-05,3.21E-05
11250,sentiment_analysis9,50,"The experimental results show that the domain - adapted BERT model significantly promotes the performance of APC tasks on the three datasets , especially the Restaurant dataset .",Introduction,Introduction,sentiment_analysis,9,35,1,0,,0.323170988,0,negative,0.017558482,0.100978106,0.0001242,6.11E-05,0.00333813,0.000824323,0.059712215,0.002705854,0.009777222,0.777601526,0.001659464,0.025220839,0.000438499
11251,sentiment_analysis9,51,"4 . We designed and applied dual labels for the input sequence applicable for the SemEval - 2014 and Chinese review datasets of ABSA joint - task , the aspect term label , and the sentiment polarity label , respectively .",Introduction,Introduction,sentiment_analysis,9,36,1,0,,0.161412772,0,approach,9.97E-05,0.624038466,0.000125543,3.68E-05,0.007251751,0.000690811,0.000523271,0.001790259,0.247425503,0.117859534,9.80E-05,3.53E-05,2.49E-05
11252,sentiment_analysis9,52,The dual label improves the learning efficiency of the proposed model .,Introduction,Introduction,sentiment_analysis,9,37,1,0,,0.146813268,0,negative,0.018447181,0.200758139,0.000145156,0.00013007,0.006128607,0.001011728,0.009508268,0.003009719,0.046956886,0.710484022,0.000384933,0.002880442,0.000154848
11253,sentiment_analysis9,53,Related Works,,,sentiment_analysis,9,0,1,0,,0.000144394,0,negative,8.00E-06,1.03E-05,1.47E-06,1.57E-07,1.75E-07,1.81E-05,1.36E-05,0.000169567,3.75E-06,0.999096229,0.000638872,3.92E-05,4.90E-07
11254,sentiment_analysis9,64,Methodology,,,sentiment_analysis,9,0,1,0,,0.043255215,0,negative,1.91E-05,0.000159627,1.45E-05,1.37E-06,4.99E-07,0.000139902,0.000211372,0.00286638,0.000149264,0.913404109,0.082542361,0.000474296,1.72E-05
11255,sentiment_analysis9,65,"Aspect - based sentiment analysis relies on the targeted aspects , and most existing studies focus on the classification of aspect polarity , leaving the problem of aspect term extraction .",Methodology,Methodology,sentiment_analysis,9,1,1,0,,0.007887175,0,negative,3.49E-05,3.11E-05,2.96E-05,6.13E-06,1.26E-06,1.81E-05,0.000134041,9.18E-05,3.56E-06,0.73943641,0.259822925,0.000378947,1.12E-05
11256,sentiment_analysis9,66,"To propose an effective aspect - based sentiment analysis model based on multi-task learning , we adopted domain - adapted BERT model from BERT - ADA and integrated the local context focus mechanism into the proposed model .",Methodology,Methodology,sentiment_analysis,9,2,1,0,,0.010948212,0,negative,0.000478134,0.009951103,0.00662262,3.92E-06,2.49E-05,8.90E-05,0.000167973,0.001368635,0.000885193,0.973559472,0.003329545,0.003508257,1.13E-05
11257,sentiment_analysis9,67,This section introduces the architecture and methodology of LCF - ATEPC .,Methodology,Methodology,sentiment_analysis,9,3,1,0,,0.000183791,0,negative,0.000127322,0.002147129,0.000893693,1.31E-06,6.95E-06,2.76E-05,2.39E-05,0.000356863,0.001163321,0.994076046,0.000710378,0.000462059,3.39E-06
11258,sentiment_analysis9,68,"This section introduces the methodology of the APC module and the ATE module , respectively .",Methodology,Methodology,sentiment_analysis,9,4,1,0,,2.06E-05,0,negative,1.87E-05,0.000220219,5.38E-05,7.80E-08,3.78E-07,4.55E-06,1.84E-06,0.000112334,0.000190747,0.999312652,3.96E-05,4.50E-05,1.71E-07
11259,sentiment_analysis9,69,and the contents are organized by order of the network layer hierarchy .,Methodology,Methodology,sentiment_analysis,9,5,1,0,,2.55E-05,0,negative,2.32E-05,8.68E-05,0.000130776,1.00E-07,3.62E-07,6.13E-06,1.96E-06,0.000119286,0.000334182,0.999240725,1.07E-05,4.56E-05,1.90E-07
11260,sentiment_analysis9,70,Task Definition,Methodology,,sentiment_analysis,9,6,1,0,,1.79E-06,0,negative,2.28E-06,1.31E-05,6.17E-06,1.29E-07,1.51E-07,8.60E-06,1.09E-05,0.000186171,9.46E-06,0.999441411,0.000226083,9.51E-05,5.37E-07
11261,sentiment_analysis9,71,Model Architecture,,,sentiment_analysis,9,0,1,0,,0.012027743,0,negative,0.002077781,0.004304933,0.001063385,0.001680559,0.000148392,0.002327543,0.001259945,0.007127733,0.020357931,0.924862365,0.03336821,0.000759331,0.000661892
11262,sentiment_analysis9,72,"Aiming at the problem of insufficient research on aspect term extraction task , a joint deep learning model is designed in this section .",Model Architecture,Model Architecture,sentiment_analysis,9,1,1,0,,0.216146331,0,model,0.000478576,0.200209739,0.000629389,1.59E-05,0.000725742,8.28E-05,3.82E-05,0.000340367,0.618944409,0.173121778,0.005306122,9.80E-05,8.91E-06
11263,sentiment_analysis9,73,"This model combines aspect polarity classification task and aspect term extraction task , and two independent Bert layers are adopted to model the global context and the local context respectively .",Model Architecture,Model Architecture,sentiment_analysis,9,2,1,0,,0.490523256,0,model,2.21E-05,0.022962739,0.00205378,5.23E-07,2.55E-05,2.05E-05,1.19E-05,5.51E-05,0.935078883,0.038774965,0.000986759,5.50E-06,1.76E-06
11264,sentiment_analysis9,74,"For conducting multi-task training at the same time , the input sequences are tokenized into different tokens and the each token is assigned two kinds of label .",Model Architecture,Model Architecture,sentiment_analysis,9,3,1,0,,0.122456596,0,model,4.84E-05,0.126287004,0.000267689,1.57E-06,0.000267074,3.71E-05,1.22E-05,0.000367893,0.538517455,0.333637919,0.000539219,1.53E-05,1.20E-06
11265,sentiment_analysis9,75,The first label indicates whether the token belongs to an aspect ; the second label marks the polarity of the tokens belongs to the aspect .,Model Architecture,Model Architecture,sentiment_analysis,9,4,1,0,,0.028295107,0,negative,6.44E-06,0.003619473,1.48E-05,3.18E-07,1.10E-05,5.56E-05,3.26E-06,0.000280591,0.190045009,0.805850109,0.000111251,1.83E-06,3.10E-07
11266,sentiment_analysis9,76,the network architecture of LCF - ATEPC .,Model Architecture,Model Architecture,sentiment_analysis,9,5,1,0,,0.092525469,0,model,5.52E-05,0.004494176,0.000261676,2.96E-06,3.12E-05,8.99E-05,3.21E-05,0.000180832,0.637086898,0.355092928,0.002638365,2.34E-05,1.04E-05
11267,sentiment_analysis9,77,Local context feature generator ( LCFG ) unit is on the left and a global context feature generator ( GCFG ) unit is on the right .,Model Architecture,Model Architecture,sentiment_analysis,9,6,1,0,,0.401366733,0,negative,8.82E-05,0.009597097,0.000389868,3.17E-05,0.000170545,0.000358942,7.85E-05,0.000788374,0.455011485,0.532280758,0.001154064,2.63E-05,2.42E-05
11268,sentiment_analysis9,78,"Both context feature generator units contain an independent pre-trained BERT layer , and respectively .",Model Architecture,Model Architecture,sentiment_analysis,9,7,1,0,,0.311562093,0,model,5.28E-05,0.017702474,8.20E-05,1.66E-05,0.000110534,0.000479101,5.40E-05,0.002691241,0.633973133,0.344518594,0.000302298,6.17E-06,1.11E-05
11269,sentiment_analysis9,79,The LCFG unit extracts the features of the local context by a local context focus layer and a MHSA encoder .,Model Architecture,Model Architecture,sentiment_analysis,9,8,1,0,,0.58636988,1,model,3.76E-05,0.008623157,0.000441179,6.31E-07,2.23E-05,1.92E-05,8.56E-06,5.30E-05,0.944014204,0.046649836,0.000124311,3.90E-06,1.97E-06
11270,sentiment_analysis9,80,The GCFG unit deploys only one MHSA encoder to learn the global context feature .,Model Architecture,Model Architecture,sentiment_analysis,9,9,1,0,,0.690164216,1,model,8.99E-05,0.025465898,0.000495141,9.49E-07,5.46E-05,2.59E-05,1.24E-05,0.000107607,0.914067564,0.059571303,0.000100689,6.15E-06,1.88E-06
11271,sentiment_analysis9,81,The feature interactive learning ( FIL ) layer combines the learning of the interaction between local context features and global context features and predicts the sentiment polarity of aspects .,Model Architecture,Model Architecture,sentiment_analysis,9,10,1,0,,0.766034464,1,model,6.58E-05,0.013069317,0.00075847,1.18E-06,5.78E-05,2.53E-05,1.10E-05,5.64E-05,0.937484124,0.048352721,0.0001081,7.00E-06,2.78E-06
11272,sentiment_analysis9,82,The extraction of aspects based on the features of the global context .,Model Architecture,Model Architecture,sentiment_analysis,9,11,1,0,,0.282642173,0,negative,0.000109411,0.007303872,0.000205216,2.35E-06,4.03E-05,4.82E-05,1.66E-05,0.000117054,0.131387358,0.856083672,0.004627182,5.55E-05,3.27E-06
11273,sentiment_analysis9,83,BERT - Shared Layer,Model Architecture,Model Architecture,sentiment_analysis,9,12,1,0,,0.758620187,1,model,0.000207016,0.011794852,0.003032988,1.41E-05,0.00019442,0.000182368,0.000160397,0.000241513,0.866948995,0.116009706,0.001026802,0.000140657,4.62E-05
11274,sentiment_analysis9,84,"The pre-trained BERT model is designed to improve performance for most NLP tasks , and The LCF - ATEPC model deploys two independent BERT - Shared layers thatare aimed to extract local and global context features .",Model Architecture,Model Architecture,sentiment_analysis,9,13,1,0,,0.561387437,1,model,5.27E-05,0.034418241,0.000908317,1.25E-06,8.90E-05,4.37E-05,2.38E-05,0.000137843,0.916706399,0.047530657,7.90E-05,6.34E-06,2.77E-06
11275,sentiment_analysis9,85,"For pre-trained BERT , the fine - tuning learning process is indispensable .",Model Architecture,Model Architecture,sentiment_analysis,9,14,1,0,,0.705635522,1,negative,0.002748778,0.006458283,5.37E-05,1.74E-05,0.000337085,6.01E-05,1.91E-05,0.000300556,0.01396587,0.975438278,0.000353847,0.00024399,3.04E-06
11276,sentiment_analysis9,86,"Both BERT - Shared layers are regarded as embed - ded layers , and the fine - tuning process is conducted independently according to the joint loss function of multi-task learning .",Model Architecture,Model Architecture,sentiment_analysis,9,15,1,0,,0.764258223,1,model,4.39E-05,0.062857962,0.000137876,1.91E-06,0.000119207,3.31E-05,1.39E-05,0.000361811,0.865941156,0.070435739,4.67E-05,4.99E-06,1.78E-06
11277,sentiment_analysis9,87,"and are used to represent the tokenized inputs of LCFG and GCFG respectively , and we can obtain the preliminary outputs of local and global context features .",Model Architecture,Model Architecture,sentiment_analysis,9,16,1,0,,0.016349458,0,negative,1.23E-05,0.003512402,1.68E-05,4.39E-07,2.34E-05,3.56E-05,4.72E-06,0.000242859,0.124713033,0.871399033,3.43E-05,4.79E-06,3.85E-07
11278,sentiment_analysis9,88,=,Model Architecture,Model Architecture,sentiment_analysis,9,17,1,0,,0.000763354,0,negative,3.84E-06,0.000212799,1.42E-06,1.94E-07,3.30E-06,1.32E-05,1.85E-06,5.60E-05,0.007815895,0.991857825,3.03E-05,3.27E-06,1.41E-07
11279,sentiment_analysis9,89,;,Model Architecture,Model Architecture,sentiment_analysis,9,18,1,0,,0.003427375,0,negative,4.56E-06,0.000207592,2.72E-06,2.84E-07,4.85E-06,2.02E-05,2.52E-06,5.54E-05,0.009675467,0.989977484,4.56E-05,3.09E-06,2.60E-07
11280,sentiment_analysis9,90,"and are the local context features and global context features , respectively . ? ? ? 2 ? and ? ? ?",Model Architecture,Model Architecture,sentiment_analysis,9,19,1,0,,0.028640207,0,negative,6.80E-06,0.000773549,4.70E-06,1.30E-06,2.49E-05,4.62E-05,5.80E-06,0.000181643,0.010016969,0.988879606,5.32E-05,4.93E-06,4.81E-07
11281,sentiment_analysis9,91,"are the weights and bias vectors , respectively .",Model Architecture,Model Architecture,sentiment_analysis,9,20,1,0,,0.165329534,0,negative,1.49E-05,0.011809045,8.40E-06,1.18E-06,2.18E-05,0.000142681,1.60E-05,0.002874017,0.215135779,0.76987228,9.69E-05,5.39E-06,1.63E-06
11282,sentiment_analysis9,92,"To learn the features of the concatenated vectors , an MHSA encoding process is performed on the .",Model Architecture,Model Architecture,sentiment_analysis,9,21,1,0,,0.381416453,0,model,1.54E-05,0.012844357,5.89E-05,2.16E-07,1.81E-05,1.46E-05,4.03E-06,0.000118974,0.685745693,0.301129748,4.64E-05,3.07E-06,5.20E-07
11283,sentiment_analysis9,93,Multi - Head Self - Attention,Model Architecture,,sentiment_analysis,9,22,1,0,,0.621166058,1,model,5.09E-05,0.018154026,0.000361255,1.26E-06,2.49E-05,2.90E-05,7.77E-05,8.78E-05,0.887609459,0.08454755,0.008931313,0.000112807,1.20E-05
11284,sentiment_analysis9,94,"Multi - head self - attention is based on multiple scale - dot attention ( SDA ) , which can be utilized to extract deep semantic features in the context , and the features are represented in self - attention score .",Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,23,1,0,,0.383237337,0,model,5.37E-05,0.038746768,0.002520961,1.21E-06,6.55E-05,3.97E-05,0.000108976,6.78E-05,0.69967478,0.257631052,0.001006387,7.53E-05,7.89E-06
11285,sentiment_analysis9,95,The MHSA can avoids the negative influence caused by the long distance dependence of the context when learning the features .,Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,24,1,0,,0.283692163,0,negative,0.006308982,0.001339832,5.56E-05,1.82E-06,5.94E-05,1.63E-05,4.71E-05,3.61E-05,0.00861385,0.982232562,7.46E-05,0.001211919,1.87E-06
11286,sentiment_analysis9,96,Suppose is the input features learned by the LCFG .,Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,25,1,0,,0.000911917,0,negative,1.73E-06,0.000138458,9.58E-07,6.43E-08,1.25E-06,1.06E-05,3.33E-06,7.47E-05,0.005581292,0.994178607,6.87E-06,1.95E-06,1.11E-07
11287,sentiment_analysis9,97,The scale - dot attention is calculate as follows :,Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,26,1,0,,0.035910065,0,negative,2.43E-05,0.000343045,0.000177245,6.57E-08,2.76E-06,1.07E-05,9.37E-06,2.32E-05,0.055177907,0.944220897,4.22E-06,6.00E-06,2.94E-07
11288,sentiment_analysis9,98,", and are the abstract matrices packed from the input features of SDA by three weight matrices",Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,27,1,0,,0.003441714,0,negative,4.36E-06,0.000549655,4.21E-06,2.32E-07,3.98E-06,4.84E-05,1.49E-05,0.000304104,0.034688002,0.964370635,7.71E-06,3.39E-06,4.69E-07
11289,sentiment_analysis9,99,"The MHSA performs multiple scaled - dot attention in parallel and concatenate the output features , then transform the features by multiplying a vector . ?",Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,28,1,0,,0.099058174,0,negative,1.66E-05,0.000593521,0.000219577,5.03E-07,1.86E-05,2.14E-05,1.65E-05,2.49E-05,0.04944611,0.949625638,8.73E-06,6.87E-06,1.10E-06
11290,sentiment_analysis9,100,represents the number of the attention heads and equal to 12 .,Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,29,1,0,,0.145808409,0,negative,2.86E-05,0.002807717,8.98E-06,2.63E-06,2.78E-05,0.000971339,0.000360452,0.007730128,0.052830092,0.935193251,1.50E-05,1.65E-05,7.47E-06
11291,sentiment_analysis9,101,"The "" ; "" means feature concatenation of each head . ? ? ? ?",Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,30,1,0,,0.000752514,0,negative,2.77E-06,3.23E-05,2.03E-06,4.37E-07,5.13E-06,2.66E-05,6.12E-06,3.52E-05,0.001372664,0.998513603,6.77E-07,2.23E-06,1.79E-07
11292,sentiment_analysis9,102,is the parameter matrices for projection .,Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,31,1,0,,0.020842109,0,negative,9.59E-06,0.000364776,3.43E-06,7.46E-07,8.32E-06,4.99E-05,1.55E-05,0.000348336,0.009119021,0.990073074,2.33E-06,4.48E-06,5.38E-07
11293,sentiment_analysis9,103,"Additionally , we apply a tanh activation function for the MHSA learning process , which significantly enhanced featurecapture capability .",Model Architecture,Multi - Head Self - Attention,sentiment_analysis,9,32,1,0,,0.835006626,1,negative,0.003144274,0.019934808,0.000291123,2.75E-06,0.000226016,9.76E-05,0.000234421,0.000305898,0.270629599,0.704732533,6.80E-06,0.000389488,4.69E-06
11294,sentiment_analysis9,104,Local Context Focus,Model Architecture,,sentiment_analysis,9,33,1,0,,0.853078233,1,model,0.000451037,0.035983759,0.000931219,1.04E-05,0.000169575,0.000197094,0.00107919,0.000719583,0.685528961,0.266697146,0.006354831,0.001720177,0.000157018
11295,sentiment_analysis9,105,Semantic - Relative Distance,Model Architecture,,sentiment_analysis,9,34,1,0,,0.359983971,0,negative,4.03E-05,0.011670829,0.000493231,6.90E-07,3.88E-05,8.19E-05,0.000177474,0.000367949,0.490002718,0.495804247,0.001132409,0.000171513,1.79E-05
11296,sentiment_analysis9,106,"The determination of local context depends on semantic - relative distance ( SRD ) , which is proposed to determine whether the context word belongs to the local context of a targeted aspect to help the model capture the local context .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,35,1,0,,0.06815513,0,negative,0.000143909,0.000863678,0.000231855,4.09E-07,7.26E-06,7.05E-06,2.69E-05,2.16E-05,0.008636787,0.98964825,0.000100017,0.000308948,3.40E-06
11297,sentiment_analysis9,107,Local context is a new concept that can be adapted to most fine - grained NLP tasks .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,36,1,0,,0.056105409,0,negative,0.00015452,0.000694572,0.000322349,2.44E-06,1.84E-05,3.06E-05,0.000261351,6.55E-05,0.005454924,0.991054931,0.000596776,0.001306971,3.66E-05
11298,sentiment_analysis9,108,"In the ABSA field , existing models generally segment input sequences into aspect sequences and context sequences , treat aspects and context as independent segments and model their characteristics separately .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,37,1,0,,0.033019883,0,negative,3.64E-05,0.000107944,2.41E-05,1.30E-06,5.06E-06,1.51E-05,6.00E-05,2.96E-05,0.000432508,0.998778322,0.000259222,0.000242669,7.81E-06
11299,sentiment_analysis9,109,"Instead of leaving the aspect alone as part of the input , this paper mines the aspect and its local context , because the empirical result shows the local context of the target aspect contains more important information .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,38,1,0,,0.0022143,0,negative,0.000252502,0.002041099,0.000150764,1.28E-06,0.000150805,1.26E-05,3.78E-05,3.94E-05,0.003040553,0.993917236,4.79E-06,0.000348491,2.66E-06
11300,sentiment_analysis9,110,"SRD is a concept based on token - aspect pairs , describing how far a token is from the aspect .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,39,1,0,,0.013419594,0,negative,8.60E-05,0.00047957,0.000588527,1.72E-06,1.48E-05,2.41E-05,0.000226391,4.49E-05,0.003500818,0.993646652,0.000582765,0.000765933,3.78E-05
11301,sentiment_analysis9,111,It counts the number of tokens between each specific token towards a targeted aspect as the SRD of all token - aspect pairs .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,40,1,0,,0.001547577,0,negative,3.99E-05,0.000142246,0.000564814,1.46E-08,1.94E-06,2.11E-06,1.18E-05,4.38E-06,0.008106222,0.991100506,2.51E-07,2.56E-05,2.40E-07
11302,sentiment_analysis9,112,The SRD is calculated as :,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,41,1,0,,0.000109127,0,negative,6.93E-06,4.73E-06,4.04E-06,8.42E-09,1.66E-07,1.09E-06,1.83E-06,4.05E-06,0.000309207,0.999658859,1.18E-07,8.92E-06,5.93E-08
11303,sentiment_analysis9,113,Figure 3 : The simulation of the context - feature dynamic mask ( CDM ) mechanism .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,42,1,0,,0.00084918,0,negative,7.77E-06,8.60E-06,1.11E-05,1.84E-07,6.78E-07,8.30E-06,1.51E-05,1.49E-05,0.003822924,0.996091849,1.37E-06,1.46E-05,2.59E-06
11304,sentiment_analysis9,114,The arrows mean the contribution of the token in the computation of the self - attention score to arrowed positions ( POS ) .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,43,1,0,,0.000462035,0,negative,2.48E-06,7.32E-06,2.13E-06,1.75E-08,2.32E-07,2.80E-06,2.73E-06,1.09E-05,0.001728509,0.998240194,7.19E-08,2.47E-06,1.51E-07
11305,sentiment_analysis9,115,And the features of the output position that the dotted arrow points to will be masked .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,44,1,0,,1.50E-05,0,negative,1.50E-05,1.05E-06,1.58E-06,2.40E-09,1.27E-07,2.77E-07,5.78E-07,8.56E-07,8.59E-05,0.999885514,8.42E-09,9.03E-06,1.21E-08
11306,sentiment_analysis9,116,Figure 4 :,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,45,1,0,,0.000134997,0,negative,1.01E-05,2.34E-06,8.50E-06,2.07E-08,3.15E-07,1.91E-06,6.20E-06,3.06E-06,0.000556713,0.999385727,1.20E-07,2.45E-05,3.91E-07
11307,sentiment_analysis9,117,The simulation of the context - feature dynamic weighting ( CDW ) mechanism .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,46,1,0,,0.002341811,0,negative,8.77E-06,1.39E-05,5.29E-05,1.05E-08,3.00E-07,1.80E-06,1.38E-05,3.16E-06,0.002409641,0.997442682,1.95E-06,5.02E-05,8.98E-07
11308,sentiment_analysis9,118,The features of the output position ( POS ) that the dotted arrow points to will be weighted decay .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,47,1,0,,2.36E-05,0,negative,7.50E-06,1.00E-05,9.39E-06,5.92E-09,2.01E-07,1.31E-06,1.79E-06,5.66E-06,0.004095577,0.995866186,2.07E-08,2.28E-06,7.19E-08
11309,sentiment_analysis9,119,"where ( 1 < < ) is the position of the specific token , is the central position of aspect .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,48,1,0,,5.42E-05,0,negative,2.98E-06,4.34E-06,1.31E-06,4.24E-08,4.19E-07,2.34E-06,1.63E-06,1.28E-05,0.000239294,0.999731872,3.67E-08,2.83E-06,9.72E-08
11310,sentiment_analysis9,120,"is the length of targeted aspect , and represents for the SRD between the - th token and the targeted aspect .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,49,1,0,,0.000238463,0,negative,9.25E-06,7.59E-06,2.53E-06,1.31E-08,4.41E-07,1.83E-06,2.90E-06,1.31E-05,0.000347788,0.999608297,2.19E-08,6.19E-06,7.99E-08
11311,sentiment_analysis9,121,"and are two implementations of the local context focus mechanism , the context - feature dynamic mask ( CDM ) layer and context - feature dynamic weighting ( CDW ) layer , respectively .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,50,1,0,,0.00447943,0,negative,3.58E-05,0.000101479,0.000844498,5.54E-08,2.96E-06,3.04E-06,1.58E-05,5.04E-06,0.019505058,0.979454754,4.97E-07,2.95E-05,1.49E-06
11312,sentiment_analysis9,122,The bottom and top of the figures represent the feature input and output positions ( POS ) corresponding to each token .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,51,1,0,,0.000224836,0,negative,1.02E-06,3.46E-06,1.07E-06,1.87E-08,3.07E-07,2.96E-06,2.37E-06,1.13E-05,0.000451992,0.999523251,1.42E-08,2.12E-06,1.15E-07
11313,sentiment_analysis9,123,"The self - attention mechanism treats all tokens equally , so that each token can generate the self - attention score with other tokens through parallel matrix operation .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,52,1,0,,0.07172387,0,negative,5.49E-05,0.000591826,0.000595779,2.53E-08,3.02E-06,3.83E-06,2.15E-05,1.22E-05,0.137309139,0.861390652,2.37E-07,1.60E-05,9.44E-07
11314,sentiment_analysis9,124,"According to the definition of MHSA , the features of the output position corresponding to each token are more closely related to itself .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,53,1,0,,0.0013642,0,negative,8.80E-06,2.76E-05,9.18E-06,2.82E-09,3.03E-07,3.10E-07,1.14E-06,1.60E-06,0.001802551,0.9981341,5.02E-08,1.43E-05,4.55E-08
11315,sentiment_analysis9,125,"After calculating the output of all tokens by MHSA encoder , the output features of each output position will be masked or attenuated , except that the local context will be retained intact .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,54,1,0,,0.002073737,0,negative,4.23E-05,2.01E-05,1.59E-05,3.94E-09,4.47E-07,5.92E-07,2.48E-06,3.16E-06,0.00201384,0.997888405,7.79E-09,1.27E-05,6.68E-08
11316,sentiment_analysis9,126,Context - features Dynamic Mask,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,55,1,0,,0.056560888,0,negative,0.00023466,0.000108117,0.001026569,4.98E-07,1.16E-05,2.82E-05,0.000221428,3.96E-05,0.022661901,0.975349233,6.97E-07,0.000287616,2.99E-05
11317,sentiment_analysis9,127,"Apart from to the features of the local context , the CDM layer will mask non-local context 's features learned by the layer .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,56,1,0,,0.003415699,0,negative,2.38E-05,5.19E-05,5.94E-05,1.32E-08,9.97E-07,1.31E-06,4.08E-06,5.06E-06,0.015020168,0.984824556,3.51E-08,8.38E-06,3.63E-07
11318,sentiment_analysis9,128,"Although it is easy to directly mask the non-local context words in the input sequence , it is inevitable to discard the features of non-local context words .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,57,1,0,,0.001035613,0,negative,2.59E-05,3.78E-06,1.45E-06,1.68E-08,3.50E-07,5.35E-07,9.15E-07,1.90E-06,6.56E-05,0.999876274,4.20E-08,2.32E-05,7.97E-08
11319,sentiment_analysis9,129,"As the CDM layer is deployed , only a relatively small amount of the semantic context itself will be masked at the corresponding output position .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,58,1,0,,0.000294015,0,negative,0.000472759,2.12E-05,1.39E-05,2.10E-08,2.10E-06,1.01E-06,5.86E-06,3.47E-06,0.000641602,0.998759669,1.68E-08,7.82E-05,2.01E-07
11320,sentiment_analysis9,130,The relative representation of context words and aspects with relatively few semantics is preserved in the corresponding output position .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,59,1,0,,0.014622481,0,negative,0.000253339,7.97E-05,1.33E-05,2.92E-08,1.14E-06,1.69E-06,3.42E-05,1.14E-05,0.004101921,0.995150361,1.37E-07,0.000351594,1.19E-06
11321,sentiment_analysis9,131,"According to the CDM implementation , the features on all the positions of non-local context words will beset to zero vectors .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,60,1,0,,0.001234891,0,negative,2.51E-06,6.00E-06,1.23E-06,1.53E-09,1.28E-07,8.73E-07,1.30E-06,6.97E-06,0.00084559,0.999133625,4.04E-09,1.72E-06,4.08E-08
11322,sentiment_analysis9,132,"In order to avoid the unbalanced distribution of features after the CDM operation , an MHSA encoder is utilized to learn and rebalance the masked local context features .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,61,1,0,,0.029311176,0,negative,0.00019621,0.001225293,0.000284919,9.12E-08,1.26E-05,6.04E-06,3.29E-05,2.61E-05,0.103285514,0.894888169,1.25E-07,3.97E-05,2.44E-06
11323,sentiment_analysis9,133,"Suppose that the is the preliminary output features of , then we get the local context feature output as follows ,",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,62,1,0,,5.62E-05,0,negative,1.06E-06,1.14E-06,1.44E-06,1.85E-09,1.03E-07,2.71E-07,5.34E-07,1.38E-06,0.00016258,0.999829591,2.60E-09,1.87E-06,3.25E-08
11324,sentiment_analysis9,134,"To mask the features of non-local context , we defines a feature masking matrix , and is the mask vectors for each token in the input sequence .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,63,1,0,,0.000447437,0,negative,1.09E-05,4.78E-05,3.73E-06,1.29E-07,2.37E-06,1.28E-05,1.28E-05,0.000175389,0.003139864,0.996588012,1.88E-08,5.03E-06,1.15E-06
11325,sentiment_analysis9,135,is the SRD threshold and is the length of input sequence including aspect .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,64,1,0,,0.000841363,0,negative,4.23E-06,6.56E-06,1.13E-06,3.22E-08,7.33E-07,3.95E-06,5.29E-06,4.16E-05,0.000220007,0.999711675,9.32E-09,4.45E-06,3.24E-07
11326,sentiment_analysis9,136,Tokens whose SRD regarding to the targeted aspect is less than the threshold are the local contexts .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,65,1,0,,0.000724457,0,negative,1.43E-06,9.42E-06,5.66E-06,3.01E-09,5.49E-07,1.33E-06,3.90E-06,7.80E-06,0.000466619,0.999500472,2.61E-09,2.66E-06,1.52E-07
11327,sentiment_analysis9,137,The ? ? ?,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,66,1,0,,0.000184947,0,negative,7.78E-07,2.50E-07,2.31E-07,6.69E-09,1.03E-07,1.48E-06,2.00E-06,3.23E-06,2.06E-05,0.99996921,1.88E-09,1.99E-06,7.05E-08
11328,sentiment_analysis9,138,represents the ones vector and ? ? ?,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,67,1,0,,4.37E-05,0,negative,4.28E-07,1.43E-06,3.96E-07,4.86E-09,1.09E-07,1.90E-06,2.19E-06,1.23E-05,0.000131768,0.999848234,2.52E-09,1.18E-06,7.75E-08
11329,sentiment_analysis9,139,"is the zeros vectors . "" . "" denotes the dot -product operation of the vectors .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,68,1,0,,0.000752644,0,negative,6.05E-07,3.13E-06,1.29E-06,1.47E-08,2.37E-07,2.50E-06,2.71E-06,1.48E-05,0.000310429,0.999662902,4.32E-09,1.21E-06,2.08E-07
11330,sentiment_analysis9,140,Context - features Dynamic Weighting,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,69,1,0,,0.02972605,0,negative,4.01E-05,3.41E-05,9.74E-05,3.89E-08,1.29E-06,7.63E-06,8.02E-05,2.68E-05,0.009977236,0.989672584,6.39E-08,5.59E-05,6.66E-06
11331,sentiment_analysis9,141,"Although empirical results show that the CDM has achieved excellent performance compared with existing models , we design the CDW to explore the potential of LCF mechanism .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,70,1,0,,0.020482314,0,negative,0.000105598,0.000713703,0.000199841,6.74E-08,1.36E-05,7.15E-06,7.69E-05,2.03E-05,0.009462497,0.989221949,1.19E-07,0.000175221,2.99E-06
11332,sentiment_analysis9,142,"The CDW is another implementation of the LCF mechanism , takes a more modest strategy compared to the CDM layer , which simply drops the features of the non-local context completely .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,71,1,0,,0.006898345,0,negative,0.000179688,0.000135117,0.004339554,6.18E-08,1.52E-05,7.75E-06,0.00015718,7.26E-06,0.007859277,0.987081177,5.06E-08,0.000209395,8.26E-06
11333,sentiment_analysis9,143,"While the features of local context retained intact , the features of the non-local context words will be weighted decay according to their SRD concerning a targeted aspect .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,72,1,0,,0.000586307,0,negative,1.57E-05,2.89E-05,1.75E-05,1.20E-09,3.32E-07,4.47E-07,3.24E-06,2.25E-06,0.006107103,0.993818467,2.37E-09,6.11E-06,7.81E-08
11334,sentiment_analysis9,144,where is the constructed weight matrix and is the weight vector for each non-local context words .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,73,1,0,,6.15E-05,0,negative,1.48E-06,2.12E-06,5.94E-07,6.45E-09,1.75E-07,1.12E-06,1.54E-06,1.10E-05,0.000177002,0.999803109,1.94E-09,1.72E-06,1.16E-07
11335,sentiment_analysis9,145,"Consistently with CDM , is the SRD between the i - th context token and a targeted aspect .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,74,1,0,,0.000116498,0,negative,2.06E-06,2.04E-05,5.87E-06,3.94E-09,2.50E-07,4.57E-07,2.39E-06,3.06E-06,0.000504978,0.999450716,3.49E-08,9.58E-06,2.00E-07
11336,sentiment_analysis9,146,is the length of the input sequence .,Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,75,1,0,,0.002920021,0,negative,1.03E-06,2.45E-06,4.52E-07,5.60E-09,1.44E-07,2.82E-06,6.87E-06,3.20E-05,0.000130317,0.99982073,4.69E-09,2.81E-06,3.63E-07
11337,sentiment_analysis9,147,"is the SRD threshold . "" . "" denotes the vector dot -product operation .",Model Architecture,Semantic - Relative Distance,sentiment_analysis,9,76,1,0,,0.000634708,0,negative,7.47E-07,5.47E-06,1.10E-06,2.20E-08,3.43E-07,4.50E-06,6.14E-06,4.81E-05,0.000345539,0.999586012,3.93E-09,1.40E-06,6.60E-07
11338,sentiment_analysis9,148,Feature Interactive,Model Architecture,,sentiment_analysis,9,77,1,0,,0.030158013,0,negative,0.000248921,0.000623443,0.000757877,6.94E-05,0.002024319,0.000881153,0.000730866,0.000516179,0.01792436,0.975893947,3.54E-07,0.000194629,0.00013457
11339,sentiment_analysis9,149,Learning,Model Architecture,,sentiment_analysis,9,78,1,0,,0.096627149,0,negative,3.29E-05,0.001524381,8.93E-05,3.30E-06,0.000658494,0.000202673,0.000827056,0.001154203,0.00812645,0.987060072,1.12E-06,0.000212809,0.000107247
11340,sentiment_analysis9,150,"LCF - ATEPC does not only rely on local context features for sentiment polarity classification , but combines and learns the local context features and the global context features to conduct polarity classification .",Model Architecture,Learning,sentiment_analysis,9,79,1,0,,0.905499093,1,negative,9.13E-05,0.075119285,0.019447289,1.78E-06,0.002839441,0.000259449,0.002881413,0.000409324,0.143065157,0.75511161,4.40E-06,0.000568004,0.000201601
11341,sentiment_analysis9,151,Aspect Polarity Classifier,Model Architecture,,sentiment_analysis,9,80,1,0,,0.9093715,1,negative,2.06E-05,0.002810843,0.000367633,3.78E-07,5.47E-05,0.000123573,0.001616194,0.000596189,0.102227552,0.891662658,1.90E-05,0.000303564,0.000197085
11342,sentiment_analysis9,152,Aspect polarity classifier performs a head - pooling on the learned concatenated context features .,Model Architecture,Aspect Polarity Classifier,sentiment_analysis,9,81,1,0,,0.882811209,1,negative,8.33E-05,0.005872336,0.006112344,8.79E-07,0.000607066,0.000282887,0.001085426,8.35E-05,0.150204967,0.835589532,7.98E-07,2.53E-05,5.17E-05
11343,sentiment_analysis9,153,Head - pooling is to extract the hidden states on the corresponding position of the first token in the input sequence .,Model Architecture,Aspect Polarity Classifier,sentiment_analysis,9,82,1,0,,0.570472045,1,negative,1.17E-05,0.001819991,0.000409207,2.60E-06,0.000248636,0.000438858,0.000389934,0.000172214,0.055192349,0.941273432,7.53E-07,4.38E-06,3.59E-05
11344,sentiment_analysis9,154,then a Softmax operation is applied to predict the sentiment polarity .,Model Architecture,Aspect Polarity Classifier,sentiment_analysis,9,83,1,0,,0.799328696,1,negative,2.60E-05,0.002110682,0.000100034,4.54E-08,5.77E-05,3.61E-05,7.49E-05,4.30E-05,0.235392522,0.762155551,2.17E-08,1.55E-06,1.92E-06
11345,sentiment_analysis9,155,"where is the number of sentiment categories , and represents the polarity predicted by aspect polarity classifier .",Model Architecture,Aspect Polarity Classifier,sentiment_analysis,9,84,1,0,,0.258067961,0,negative,1.08E-06,7.12E-05,1.15E-06,1.69E-08,8.96E-06,2.46E-05,1.53E-05,4.34E-05,0.001722764,0.998110769,8.19E-09,3.95E-07,2.63E-07
11346,sentiment_analysis9,156,Aspect Term Extractor,Model Architecture,,sentiment_analysis,9,85,1,0,,0.584580104,1,negative,0.000228848,0.003606016,0.001843914,1.29E-05,0.001076023,0.00110787,0.02366749,0.001911379,0.029504256,0.928910509,1.63E-05,0.004163779,0.003950769
11347,sentiment_analysis9,157,"Aspect term extractor first performs the token - level classification for each token , suppose is the features on the corresponding position of token ,",Model Architecture,Aspect Term Extractor,sentiment_analysis,9,86,1,0,,0.428754581,0,negative,4.30E-05,0.001387613,0.001851386,8.25E-08,9.68E-05,3.22E-05,0.000265924,2.55E-05,0.097897038,0.89836317,7.35E-08,2.47E-05,1.25E-05
11348,sentiment_analysis9,158,"where is the number of token categories , and represents the token category inferred by aspect polarity classifier .",Model Architecture,Aspect Term Extractor,sentiment_analysis,9,87,1,0,,0.031272009,0,negative,8.93E-07,1.89E-05,7.46E-07,7.40E-09,2.15E-06,7.18E-06,6.32E-06,2.65E-05,0.000584448,0.99935213,2.45E-09,6.16E-07,1.47E-07
11349,sentiment_analysis9,159,Training Details,,,sentiment_analysis,9,0,1,0,,0.002340091,0,negative,6.05E-05,0.001338931,1.43E-05,0.000123468,5.30E-05,0.00076714,0.000215992,0.00645336,0.000307534,0.988848599,0.00167076,0.000130888,1.56E-05
11350,sentiment_analysis9,160,"The LCFG and the GCFG are based on the BERT - BASE and BERT - SPC models , respectively .",Training Details,Training Details,sentiment_analysis,9,1,1,0,,0.961768978,1,experimental-setup,0.000303871,0.001284886,0.144720225,5.61E-06,9.10E-06,0.403350468,0.007835184,0.216045239,0.005722037,0.219997325,0.000371823,0.000246321,0.000107903
11351,sentiment_analysis9,161,And the BERT - SPC significantly improved the performance of APC tasks .,Training Details,Training Details,sentiment_analysis,9,2,1,0,,0.899847239,1,results,0.031901448,6.63E-05,0.00081842,4.81E-05,8.32E-05,0.085406709,0.064710671,0.050431127,1.68E-05,0.309849623,0.000459223,0.45530809,0.000900242
11352,sentiment_analysis9,162,"In LCF - ATEPC , BERT - SPC only refactored the input sequence form compared with BERT - BASE model .",Training Details,Training Details,sentiment_analysis,9,3,1,0,,0.951350613,1,negative,0.016126931,0.00075507,0.044610694,0.000144578,0.000356051,0.290236884,0.068985092,0.092553757,0.000247368,0.428765575,0.002408967,0.053412787,0.001396248
11353,sentiment_analysis9,163,"The input sequence of BERT - BASE is formed in "" [ CLS ] "" + sequence + "" [ SEP ] "" , while it is formed in "" [ CLS ] "" + sequence + "" [ SEP ] "" + aspect + "" [ SEP ] "" for BERT - SPC .",Training Details,Training Details,sentiment_analysis,9,4,1,0,,0.251403574,0,experimental-setup,3.16E-05,7.12E-05,0.000358845,3.64E-06,1.34E-05,0.62888402,0.003448308,0.241539126,6.78E-05,0.125466393,1.51E-05,7.42E-05,2.64E-05
11354,sentiment_analysis9,164,"Since LCF - ATEPC is a multi- task learning model , we redesigned the form of data input and adopted dual labels of sentiment polarity and token category .",Training Details,Training Details,sentiment_analysis,9,5,1,0,,0.019512555,0,experimental-setup,0.001548109,0.000880368,0.000972711,6.89E-05,0.000241225,0.500081998,0.002523469,0.156905603,0.000636087,0.335383073,5.96E-05,0.000592204,0.000106601
11355,sentiment_analysis9,165,"The are the input samples of BERT - BASE and BERT - SPC model , respectively .",Training Details,Training Details,sentiment_analysis,9,6,1,0,,0.015133059,0,experimental-setup,9.94E-06,2.49E-05,9.00E-06,8.08E-07,1.66E-06,0.481749892,0.000453799,0.346590426,3.11E-05,0.171106634,6.48E-06,1.13E-05,4.08E-06
11356,sentiment_analysis9,166,"The cross - entropy loss is adopted for APC and ATE subtask and the 2 regularization is applied in LCF - ATEPC , here is the loss function for APC task ,",Training Details,Training Details,sentiment_analysis,9,7,1,0,,0.405894202,0,experimental-setup,1.43E-05,4.95E-05,4.34E-05,1.22E-06,1.03E-06,0.525451846,0.000562881,0.453913106,6.10E-05,0.019882865,4.28E-06,5.77E-06,8.91E-06
11357,sentiment_analysis9,167,"where is the number of polarity categories , is the 2 regularization parameter , and ?",Training Details,Training Details,sentiment_analysis,9,8,1,0,,0.028032309,0,experimental-setup,2.97E-05,2.43E-05,1.14E-05,3.85E-06,3.78E-06,0.616536895,0.000672126,0.277171326,2.21E-05,0.105476272,1.05E-05,2.65E-05,1.13E-05
11358,sentiment_analysis9,168,is the parameter - set of the LCF - ATEPC .,Training Details,Training Details,sentiment_analysis,9,9,1,0,,0.079556085,0,experimental-setup,3.25E-05,4.74E-05,2.26E-05,4.08E-06,5.77E-06,0.504696919,0.000741036,0.289417038,4.94E-05,0.204914009,1.44E-05,3.96E-05,1.53E-05
11359,sentiment_analysis9,169,The loss function for ATE task is,Training Details,Training Details,sentiment_analysis,9,10,1,0,,0.033369112,0,hyperparameters,5.16E-05,0.000138545,8.74E-05,3.10E-06,4.54E-06,0.410121739,0.000821192,0.410131517,0.000224154,0.178289061,5.16E-05,5.09E-05,2.47E-05
11360,sentiment_analysis9,170,where is the number of token classes and is the sum of the tokens in each input sequence .,Training Details,Training Details,sentiment_analysis,9,11,1,0,,0.019491536,0,negative,7.19E-05,6.29E-05,4.20E-05,6.24E-06,8.04E-06,0.37444976,0.000643405,0.201486696,0.000154127,0.422967867,3.39E-05,5.27E-05,2.04E-05
11361,sentiment_analysis9,171,"Accordingly , the loss function of LCF - ATEPC is as follows :",Training Details,Training Details,sentiment_analysis,9,12,1,0,,0.085423261,0,negative,0.000245744,0.000530121,0.001070679,6.67E-06,1.55E-05,0.260737094,0.001787178,0.239060303,0.001221332,0.494704419,0.000230589,0.000327043,6.33E-05
11362,sentiment_analysis9,172,4 . Experiments,Training Details,Training Details,sentiment_analysis,9,13,1,0,,0.033803025,0,negative,0.00020041,4.21E-05,7.60E-05,7.69E-06,3.49E-05,0.232085487,0.005426827,0.042869515,1.20E-05,0.717683714,1.98E-05,0.001509987,3.15E-05
11363,sentiment_analysis9,173,Datasets and Hyperparameters Setting,,,sentiment_analysis,9,0,1,0,,0.000508597,0,negative,8.32E-06,0.000277671,6.47E-06,9.73E-07,6.88E-07,0.000456916,4.06E-05,0.011290757,0.000123424,0.987164137,0.000585569,4.24E-05,2.12E-06
11364,sentiment_analysis9,174,"To comprehensive evaluate the performance of the proposed model , the experiments were conducted in three most commonly used ABSA datasets , the Laptops and Restaurant datasets of SemEval - 2014 Task4 subtask2 and an ACL Twitter social dataset .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,1,1,0,,0.087314987,0,negative,6.75E-05,0.000272837,0.003943672,1.78E-06,1.17E-05,0.001007788,0.00029707,0.028499737,1.05E-05,0.965088589,3.59E-05,0.000755731,7.30E-06
11365,sentiment_analysis9,175,"To evaluate our model capability with processing the Chinese language , we also tested the performance of LCF - ATEPC on four Chinese comment datasets ; Zhao , Pan , Du and Zheng ( 2015 b ) ; ( Car , Phone , Notebook , Camera ) .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,2,1,0,,0.142243858,0,negative,0.000205621,0.000211488,0.020955421,2.37E-06,1.24E-05,0.002600215,0.001436141,0.040180278,1.15E-05,0.928743278,7.04E-05,0.005549311,2.16E-05
11366,sentiment_analysis9,176,We preprocessed the seven datasets .,Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,3,1,0,,0.116229879,0,negative,4.92E-05,3.77E-05,0.000533574,2.52E-05,2.85E-05,0.021636325,0.000517304,0.138183689,1.18E-05,0.838819144,1.01E-05,0.000129142,1.83E-05
11367,sentiment_analysis9,177,"We reformatted the origin dataset and annotated each sample with the IOB labels for ATE task and polarity labels for APC tasks , respectively .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,4,1,0,,0.251297263,0,negative,0.00010515,7.26E-05,0.003614265,2.59E-05,7.89E-05,0.00811451,0.000385557,0.062394617,1.57E-05,0.925048599,3.34E-06,0.000127792,1.31E-05
11368,sentiment_analysis9,178,"The polarity of each aspect on the Laptops , Restaurants and datasets maybe positive , neutral , and negative , and the conflicting labels of polarity are not considered .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,5,1,0,,0.027861273,0,negative,2.26E-05,2.12E-05,0.000672764,7.21E-07,1.77E-06,0.001535697,3.00E-05,0.058120122,9.80E-06,0.939501867,2.78E-06,7.80E-05,2.59E-06
11369,sentiment_analysis9,179,"The reviews in the four Chinese datasets have been purged , with each aspect maybe positive or negative binary polarity .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,6,1,0,,0.041308144,0,negative,2.43E-05,8.31E-05,0.001409528,4.70E-06,3.54E-05,0.004724781,0.000253405,0.110767476,1.24E-05,0.882599435,4.19E-06,7.36E-05,7.69E-06
11370,sentiment_analysis9,180,"To verify the effectiveness and performance of LCF - ATEPC models on multilingual datasets , we built a multilingual dataset by mixing the 7 datasets .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,7,1,0,,0.173525289,0,negative,0.000136964,0.000127831,0.010539074,1.95E-06,2.15E-05,0.001513027,0.000534221,0.03033488,1.06E-05,0.955710125,6.65E-06,0.001053206,1.00E-05
11371,sentiment_analysis9,181,We adopt this dataset to conduct multilingual - oriented ATE and APC experiments .,Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,8,1,0,,0.300671466,0,negative,0.000200252,0.000791958,0.022189394,0.000172165,0.001299478,0.004415569,0.001885199,0.03892674,5.30E-05,0.928858226,4.15E-05,0.001054987,0.000111498
11372,sentiment_analysis9,182,The table demonstrates the details of these datasets,Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,9,1,0,,0.043966739,0,negative,7.50E-06,5.38E-06,0.000116532,3.59E-06,3.96E-06,0.001598124,6.61E-05,0.020485335,3.01E-06,0.977616736,2.93E-06,8.68E-05,4.02E-06
11373,sentiment_analysis9,183,4 .,Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,10,1,0,,0.169605288,0,negative,3.37E-05,1.58E-05,0.000270306,1.66E-06,7.84E-07,0.00475711,6.67E-05,0.113533153,2.32E-05,0.881222975,5.90E-06,5.98E-05,8.98E-06
11374,sentiment_analysis9,184,The samples distribution of those datasets is not balanced .,Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,11,1,0,,0.052026488,0,negative,2.67E-05,7.05E-06,0.000304681,1.35E-06,2.87E-06,0.000637933,7.86E-05,0.016935377,1.40E-06,0.981273828,9.39E-06,0.000716044,4.76E-06
11375,sentiment_analysis9,185,"For example , most samples in the restaurant dataset are positive , while the neutral samples in the Twitter dataset account for the majority .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,12,1,0,,0.025931025,0,negative,0.000250268,1.30E-05,0.001165701,1.18E-06,7.36E-06,0.00062922,0.000101346,0.016568897,2.24E-06,0.978901196,2.97E-06,0.002350308,6.33E-06
11376,sentiment_analysis9,186,"Apart from some hyperparameters setting referred to previous researches , we also conducted the controlled trials and analyzed the experimental results to optimize the hyperparameters setting .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,13,1,0,,0.116252859,0,negative,8.37E-05,0.000155973,0.000174201,2.80E-05,2.64E-05,0.006807633,0.000205257,0.266056002,2.56E-05,0.726207475,4.10E-06,0.000205563,2.00E-05
11377,sentiment_analysis9,187,The superior hyperparameters are listed in .,Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,14,1,0,,0.238551587,0,negative,2.13E-05,1.76E-05,5.37E-05,2.71E-06,1.55E-06,0.007396497,6.19E-05,0.424714993,1.19E-05,0.567660588,1.08E-06,5.11E-05,5.16E-06
11378,sentiment_analysis9,188,"The default SRD setting for all experiments is 5 , with additional instructions for experiments with different SRD .",Datasets and Hyperparameters Setting,Datasets and Hyperparameters Setting,sentiment_analysis,9,15,1,0,,0.990769369,1,hyperparameters,2.20E-06,1.67E-05,4.11E-05,1.20E-06,3.51E-07,0.020158203,0.00024815,0.961656507,5.09E-06,0.017851486,7.10E-07,1.05E-05,7.83E-06
11379,sentiment_analysis9,189,Compared Methods,,,sentiment_analysis,9,0,1,0,,0.003363972,0,negative,1.91E-05,0.000148937,7.51E-05,4.41E-07,6.71E-07,0.00022496,0.000164342,0.00299226,8.80E-05,0.993920181,0.001856336,0.00050613,3.53E-06
11380,sentiment_analysis9,190,We compare the LCF - ATEPC model to current state - of - the - art methods .,Compared Methods,Compared Methods,sentiment_analysis,9,1,1,0,,0.701518385,1,baselines,0.000115202,0.000167364,0.679771762,1.52E-06,1.65E-06,0.000224875,0.000735714,0.003854315,1.56E-05,0.311464923,0.000105195,0.003524896,1.70E-05
11381,sentiment_analysis9,191,Experimental results show that the proposed model achieves state - of - the - art performance both in the ATE and APC tasks .,Compared Methods,Compared Methods,sentiment_analysis,9,2,1,0,,0.74422546,1,results,0.001381884,8.23E-06,0.038206829,6.29E-07,4.87E-07,5.56E-05,0.00142321,0.001012199,8.55E-07,0.201063045,9.02E-05,0.756723467,3.34E-05
11382,sentiment_analysis9,192,"ATAE - LSTM is a classical LSTM - based network for the APC task , which applies the attention mechanism to focus on the important words in the context .",Compared Methods,Compared Methods,sentiment_analysis,9,3,1,1,baselines,0.995347937,1,baselines,9.13E-07,6.45E-07,0.997226719,1.08E-07,1.80E-08,1.15E-05,0.000118437,7.46E-05,6.62E-07,0.002518487,1.95E-05,2.17E-05,6.66E-06
11383,sentiment_analysis9,193,"Besides , ATAE - LSTM appends aspect embedding and the learned features to make full use of the aspect features .",Compared Methods,Compared Methods,sentiment_analysis,9,4,1,0,,0.99102216,1,baselines,5.67E-05,3.92E-06,0.988446224,6.98E-08,9.24E-08,5.72E-06,2.39E-05,6.80E-05,5.88E-06,0.01124898,8.97E-07,0.000137808,1.79E-06
11384,sentiment_analysis9,194,The ATAE - LSTM can be adapted to the Chinese review datasets .,Compared Methods,Compared Methods,sentiment_analysis,9,5,1,0,,0.969642985,1,baselines,7.11E-05,1.83E-05,0.882014072,2.87E-07,1.97E-07,5.60E-05,0.000287997,0.00109678,9.71E-06,0.111039355,7.39E-05,0.005316966,1.53E-05
11385,sentiment_analysis9,195,ATSM -S Peng et al.,Compared Methods,Compared Methods,sentiment_analysis,9,6,1,1,baselines,0.608670984,1,baselines,5.96E-05,7.19E-07,0.984171794,9.60E-08,8.29E-08,6.83E-06,0.000120072,3.70E-05,5.00E-07,0.013203703,1.05E-05,0.002383501,5.59E-06
11386,sentiment_analysis9,196,is a baseline model of the ATSM variations for Chinese language - oriented ABSA task .,Compared Methods,Compared Methods,sentiment_analysis,9,7,1,1,baselines,0.881587634,1,baselines,2.06E-06,5.46E-07,0.994110038,2.93E-08,1.99E-08,3.60E-06,5.19E-05,3.48E-05,3.26E-07,0.005682075,7.67E-06,0.000104944,1.96E-06
11387,sentiment_analysis9,197,This model learns the sentence and aspect terms at three perspectives of granularity .,Compared Methods,Compared Methods,sentiment_analysis,9,8,1,0,,0.977907824,1,baselines,4.24E-06,4.00E-06,0.986342018,8.80E-08,3.59E-08,1.09E-05,2.98E-05,0.000212542,4.32E-05,0.013315423,9.75E-06,2.19E-05,6.07E-06
11388,sentiment_analysis9,198,GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs .,Compared Methods,Compared Methods,sentiment_analysis,9,9,1,1,baselines,0.98312956,1,baselines,6.46E-06,2.33E-06,0.994226409,6.42E-07,8.73E-08,2.23E-05,0.000296733,0.000192183,3.59E-06,0.004981685,0.000117081,0.000104991,4.55E-05
11389,sentiment_analysis9,199,The GANN applied the Gate Truncation RNN ( GTR ) to learn informative aspect - dependent sentiment clue representations .,Compared Methods,Compared Methods,sentiment_analysis,9,10,1,0,,0.987476928,1,baselines,4.56E-06,2.74E-06,0.99499039,6.23E-08,5.33E-08,4.75E-06,2.12E-05,6.78E-05,5.27E-06,0.004876339,1.06E-06,2.32E-05,2.53E-06
11390,sentiment_analysis9,200,GANN obtained the state - of - the - art APC performance on the Chinese review datasets .,Compared Methods,Compared Methods,sentiment_analysis,9,11,1,0,,0.925346459,1,baselines,0.000171002,8.15E-06,0.649650803,1.74E-06,7.26E-07,0.000231574,0.010065117,0.003181954,1.96E-06,0.150414904,0.000407402,0.185529909,0.000334758
11391,sentiment_analysis9,201,"AEN - is an attentional encoder network based on the pretrained BERT model , which aims to solve the aspect polarity classification .",Compared Methods,Compared Methods,sentiment_analysis,9,12,1,1,baselines,0.976971502,1,baselines,2.35E-06,4.90E-07,0.999210413,2.89E-08,1.93E-08,1.30E-06,1.68E-05,7.89E-06,1.03E-06,0.000739019,1.36E-06,1.68E-05,2.49E-06
11392,sentiment_analysis9,202,"BERT - is a BERT - adapted model for Review Reading Comprehension ( RRC ) task , a task inspired by machine reading comprehension ( MRC ) , it could be adapted to aspect - level sentiment classification task .",Compared Methods,Compared Methods,sentiment_analysis,9,13,1,1,baselines,0.986302694,1,baselines,7.33E-06,3.05E-06,0.996305103,2.61E-07,1.27E-07,6.60E-06,0.000245663,3.65E-05,8.77E-07,0.002796052,8.41E-05,0.000486853,2.74E-05
11393,sentiment_analysis9,203,BERT - BASE is the basic pretrained BERT model .,Compared Methods,Compared Methods,sentiment_analysis,9,14,1,1,baselines,0.994069794,1,baselines,6.36E-06,3.55E-06,0.99113409,8.29E-07,2.13E-07,8.88E-05,0.000776675,0.000918491,6.30E-06,0.006858227,6.03E-06,0.000141282,5.92E-05
11394,sentiment_analysis9,204,"We adapt it to ABSA multi-task learning , which equips the same ability to automatically extract aspect terms and classify aspects polarity as LCF - ATEPC model .",Compared Methods,Compared Methods,sentiment_analysis,9,15,1,1,baselines,0.880707677,1,baselines,5.03E-05,0.000149021,0.945078946,7.09E-07,1.34E-06,4.38E-05,0.000188373,0.001139454,5.13E-05,0.052762084,1.33E-05,0.000501384,1.99E-05
11395,sentiment_analysis9,205,is a pretrained BERT model designed for the sentence - pair classification task .,Compared Methods,Compared Methods,sentiment_analysis,9,16,1,0,,0.839278313,1,baselines,2.24E-05,1.54E-05,0.937983643,1.02E-06,5.76E-07,7.19E-05,0.000163783,0.002368969,3.97E-05,0.059177211,5.16E-06,0.000109286,4.10E-05
11396,sentiment_analysis9,206,"Consistent with the basic BERT model , we implemented this model for ABSA multitasking .",Compared Methods,Compared Methods,sentiment_analysis,9,17,1,0,,0.5260192,1,baselines,3.50E-05,2.56E-05,0.929285671,4.52E-07,5.92E-07,5.05E-05,0.000193137,0.000773029,2.98E-05,0.068924652,6.33E-06,0.000658433,1.68E-05
11397,sentiment_analysis9,207,BERT - ADA,Compared Methods,Compared Methods,sentiment_analysis,9,18,1,1,baselines,0.824801903,1,baselines,8.08E-05,2.46E-06,0.954445643,3.39E-06,1.29E-06,0.000178755,0.00666327,0.000904838,2.49E-06,0.019081379,1.21E-05,0.018179848,0.000443686
11398,sentiment_analysis9,208,Rietzler et al.,Compared Methods,Compared Methods,sentiment_analysis,9,19,1,1,baselines,0.485255141,0,baselines,0.001274787,3.25E-06,0.657660586,2.07E-06,1.64E-06,6.18E-05,0.000339429,0.000336696,6.92E-06,0.330281992,3.54E-06,0.009991723,3.56E-05
11399,sentiment_analysis9,209,"is a domain - adapted BERT - based model proposed for the APC task , which finetuned the BERT - BASE model on task - related corpus .",Compared Methods,Compared Methods,sentiment_analysis,9,20,1,1,baselines,0.869444159,1,baselines,1.01E-05,1.16E-06,0.996430295,8.72E-08,1.04E-07,2.35E-06,3.51E-05,1.45E-05,1.12E-06,0.003392546,1.43E-06,0.000106145,5.09E-06
11400,sentiment_analysis9,210,This model obtained state - of - the - art accuracy on the Laptops dataset .,Compared Methods,Compared Methods,sentiment_analysis,9,21,1,0,,0.669917422,1,baselines,0.000263493,3.67E-06,0.771369001,7.44E-07,9.12E-07,6.16E-05,0.002061756,0.000673647,1.81E-06,0.104250526,9.57E-06,0.121163433,0.000139864
11401,sentiment_analysis9,211,"LCF - ATEPC 5 is the multi -task learning model for the ATE and APC tasks , which is based on the the BERT - SPC model and local context focus mechanism .",Compared Methods,Compared Methods,sentiment_analysis,9,22,1,1,baselines,0.99288515,1,baselines,6.90E-06,6.11E-06,0.991374397,5.01E-07,2.97E-07,4.10E-05,0.000427559,0.000629047,6.70E-06,0.007336429,2.38E-06,0.000125059,4.36E-05
11402,sentiment_analysis9,212,LCF - ATE are the variations of the LCF - ATEPC model which only optimize for the ATE task .,Compared Methods,Compared Methods,sentiment_analysis,9,23,1,1,baselines,0.984137646,1,baselines,1.60E-06,5.37E-07,0.997853367,1.34E-08,1.78E-08,1.50E-06,2.61E-05,1.06E-05,7.58E-07,0.002056763,7.81E-07,4.57E-05,2.20E-06
11403,sentiment_analysis9,213,LCF - APC are the variations of LCF - ATEPC and it only optimize for the APC task during training process .,Compared Methods,Compared Methods,sentiment_analysis,9,24,1,1,baselines,0.953812961,1,baselines,2.82E-06,1.08E-06,0.994967067,3.86E-08,5.28E-08,3.12E-06,5.48E-05,3.35E-05,9.81E-07,0.00482666,1.20E-06,0.000103254,5.40E-06
11404,sentiment_analysis9,214,Results Analysis,,,sentiment_analysis,9,0,1,0,,0.0058658,0,negative,4.02E-05,4.85E-05,2.35E-06,6.46E-07,5.82E-07,4.48E-05,9.67E-05,0.000744578,1.32E-05,0.99399188,0.003964117,0.001049784,2.66E-06
11405,sentiment_analysis9,215,The experiments are conducted in several segments .,Results Analysis,Results Analysis,sentiment_analysis,9,1,1,0,,0.001681284,0,negative,0.000733104,4.47E-06,5.46E-05,8.12E-07,2.78E-06,3.44E-05,7.67E-05,0.00010451,2.35E-06,0.994577265,8.17E-06,0.004398258,2.55E-06
11406,sentiment_analysis9,216,"First , the baseline performance of LCF - ATEPC on all Chinese and English data sets was tested , and then the effectiveness of multi-task learning was demonstrated .",Results Analysis,Results Analysis,sentiment_analysis,9,2,1,0,,0.147153784,0,negative,0.007186364,0.000107789,0.000396584,1.21E-06,8.31E-06,3.77E-05,0.000632499,0.000251118,7.65E-06,0.833625587,5.25E-05,0.157681469,1.12E-05
11407,sentiment_analysis9,217,"Finally , the assistance of domain - adapted BERT model in improving performance was evaluated and the sensitivity of different datasets to SRD was studied .",Results Analysis,Results Analysis,sentiment_analysis,9,3,1,0,,0.469671838,0,negative,0.057743128,3.85E-05,0.00031061,1.14E-06,3.66E-06,3.72E-05,0.000229924,0.000151684,1.18E-05,0.75779806,3.33E-05,0.183632849,8.10E-06
11408,sentiment_analysis9,218,are the experimental results of LCF - ATEPC models on four Chinese review datasets .,Results Analysis,Results Analysis,sentiment_analysis,9,4,1,0,,0.313667071,0,negative,0.001353982,3.51E-05,0.001054016,2.26E-06,3.83E-06,6.47E-05,0.002196828,0.000338833,3.46E-06,0.711233553,0.002379703,0.281256337,7.74E-05
11409,sentiment_analysis9,219,"The experimental results ( % ) of LCF - ATEPC models on four Chinese datasets . "" - "" represents the result is not unreported .",Results Analysis,Results Analysis,sentiment_analysis,9,5,1,0,,0.006332994,0,negative,0.000425475,3.97E-06,7.10E-05,7.16E-07,6.00E-07,0.000177962,0.000581207,0.000887304,2.68E-06,0.970465561,0.000186812,0.02717459,2.21E-05
11410,sentiment_analysis9,220,The optimal performance is in bold .,Results Analysis,Results Analysis,sentiment_analysis,9,6,1,0,,0.000598535,0,negative,0.000221874,7.48E-06,4.16E-05,1.07E-06,6.37E-07,0.000183152,7.70E-05,0.001011966,1.28E-05,0.996497858,3.03E-05,0.001908398,5.88E-06
11411,sentiment_analysis9,221,Performance on Chinese Review Datasets,Results Analysis,,sentiment_analysis,9,7,1,0,,0.550584529,1,results,0.000585596,2.55E-06,0.000255149,1.77E-07,2.19E-07,8.24E-06,0.001966345,3.77E-05,3.23E-07,0.080888124,0.000327095,0.915906233,2.23E-05
11412,sentiment_analysis9,222,Model,,,sentiment_analysis,9,0,1,0,,0.001639822,0,negative,0.000381652,0.003010384,0.001698777,4.51E-05,3.35E-05,0.000613342,0.000837708,0.006011634,0.008891553,0.923081376,0.053568581,0.001626411,0.0002
11413,sentiment_analysis9,223,Laptop Restaurant,Model,,sentiment_analysis,9,1,1,0,,0.022809118,0,negative,0.00299104,0.00111435,0.002202224,0.008618838,0.002156425,0.015278662,0.028129768,0.013092057,0.011784278,0.863993426,0.001572928,0.042774699,0.006291305
11414,sentiment_analysis9,224,"Twitter learning can not achieve as the best effect as single - task learning does , which is also the compromise of the multi-task learning model when dealing with multiple tasks .",Model,Laptop Restaurant,sentiment_analysis,9,2,1,0,,0.050983658,0,negative,0.001055091,1.96E-06,7.90E-05,1.70E-06,8.13E-07,0.000849197,0.006244968,0.000565071,3.38E-06,0.904865187,7.75E-05,0.086183075,7.30E-05
11415,sentiment_analysis9,225,"The BERT - BASE model is trained on a large - scale general corpus , so the fine - tuning during process during training process is significant and inevitable for BERT - based models .",Model,Laptop Restaurant,sentiment_analysis,9,3,1,0,,0.034021619,0,negative,0.000207747,3.61E-05,0.001215975,4.16E-06,1.03E-05,0.003242731,0.009852267,0.001905098,3.23E-05,0.978413236,3.28E-05,0.004979638,6.76E-05
11416,sentiment_analysis9,226,"Meanwhile , the ABSA datasets commonly benchmarked are generally small with the domain - specific characteristic , the effect of BERT - BASE model on the most ABSA datasets can be further improved through domain - adaption .",Model,Laptop Restaurant,sentiment_analysis,9,4,1,0,,0.053537535,0,negative,0.009461298,2.46E-05,0.00018579,1.01E-05,7.67E-06,0.001347529,0.012636826,0.001547366,2.43E-05,0.671512482,9.58E-05,0.302878969,0.00026737
11417,sentiment_analysis9,227,Domain adaption is a effective technique while integrating the pre-trained BERT - BASE model .,Model,Laptop Restaurant,sentiment_analysis,9,5,1,0,,0.604032033,1,negative,0.003425871,2.89E-05,0.001491386,3.74E-05,1.37E-05,0.002311335,0.012259911,0.001048164,6.12E-05,0.899172144,0.000587612,0.078841983,0.000720355
11418,sentiment_analysis9,228,"By further training the BERT - BASE model in a domain - related corpus similar to or homologous to the target ABSA dataset , then domain - related pretrained BERT model can be obtained .",Model,Laptop Restaurant,sentiment_analysis,9,6,1,0,,0.015568187,0,negative,0.00019895,1.81E-05,0.000191332,8.15E-07,3.20E-06,0.00034289,0.001015392,0.000497356,8.09E-05,0.991745724,3.96E-06,0.005888444,1.30E-05
11419,sentiment_analysis9,229,We adopted the method proposed in to obtain the domain - adapted pre-trained BERT model based on the corpus of Yelp Dataset Challenge reviews 7 and the amazon Laptops review dataset,Model,Laptop Restaurant,sentiment_analysis,9,7,1,0,,0.037291752,0,negative,0.000176065,0.00014483,0.001128098,4.37E-06,2.33E-05,0.005538377,0.018943257,0.005676534,0.000118531,0.962304543,1.67E-05,0.005813883,0.000111512
11420,sentiment_analysis9,230,He and McAuley ( 2016 ) .,Model,Laptop Restaurant,sentiment_analysis,9,8,1,0,,0.000877278,0,negative,0.000399885,2.35E-06,0.000553043,1.73E-06,2.27E-06,0.000733381,0.001618727,0.000193917,3.50E-05,0.993416127,2.96E-06,0.003018823,2.18E-05
11421,sentiment_analysis9,231,shows that the performance of APC task significantly improved by domain - adapted BERT model .,Model,Laptop Restaurant,sentiment_analysis,9,9,1,0,,0.523738454,1,results,0.012696376,2.81E-06,0.000132632,2.22E-06,3.54E-06,0.000322964,0.026250214,0.000303037,3.65E-06,0.13961681,4.55E-06,0.820448744,0.00021245
11422,sentiment_analysis9,232,"The accuracy benchmark in the classical Restaurant achieving more than 90 % , which means that the LCF - ATEPC is the first ABSAoriented model obtained up to 90 % accuracy on the Restaurant dataset .",Model,Laptop Restaurant,sentiment_analysis,9,10,1,0,,0.738939332,1,results,0.00057414,7.13E-07,5.24E-05,5.48E-07,9.59E-07,0.000165964,0.039987952,0.000152347,4.67E-07,0.057571284,2.97E-06,0.901293281,0.000197009
11423,sentiment_analysis9,233,"In addition , experimental result on the Laptop dataset also prove the effectiveness of domain - adaption in multi-task learning .",Model,Laptop Restaurant,sentiment_analysis,9,11,1,0,,0.071894602,0,negative,0.002362394,2.66E-06,3.34E-05,7.50E-07,1.90E-06,0.000393523,0.016429441,0.000377925,2.34E-06,0.530482364,3.19E-06,0.449847155,6.30E-05
11424,sentiment_analysis9,234,"Besides , the experimental results on the laptop dataset also validate the effectiveness of domain - adapted BERT model for ABSA multi-task learning .",Model,Laptop Restaurant,sentiment_analysis,9,12,1,0,,0.28027398,0,results,0.003757806,3.85E-06,2.87E-05,1.77E-06,3.30E-06,0.000736559,0.036165894,0.000752067,3.13E-06,0.286294137,4.16E-06,0.672092597,0.000156009
11425,sentiment_analysis9,235,Overall Performance Analysis,Model,,sentiment_analysis,9,13,1,0,,0.013303408,0,negative,8.51E-05,4.99E-05,6.03E-05,3.80E-07,1.03E-06,6.86E-05,0.000346515,0.000806103,0.001468567,0.992953528,5.28E-05,0.004091092,1.60E-05
11426,sentiment_analysis9,236,"Many models for ABSA tasks do not take into account the ATE subtask , but there are still some joint models based on the traditional neural network architecture to conduct the APC and ATE tasks simultaneously .",Model,Overall Performance Analysis,sentiment_analysis,9,14,1,0,,0.002882319,0,negative,2.04E-05,1.32E-05,1.98E-05,3.30E-07,2.01E-07,6.62E-06,1.17E-05,6.34E-05,3.92E-05,0.998646816,0.001081324,9.43E-05,2.65E-06
11427,sentiment_analysis9,237,"Benefit from the joint training process , the two ABSA subtasks of APC and ATE can promote each other and improve the performance .",Model,Overall Performance Analysis,sentiment_analysis,9,15,1,0,,0.009074138,0,negative,0.002729663,1.62E-05,2.57E-05,3.99E-07,1.13E-06,3.61E-06,9.91E-06,2.62E-05,9.51E-05,0.992576104,6.18E-06,0.004508203,1.60E-06
11428,sentiment_analysis9,238,"The CDM layer works better on twitter dataset because there are a lot of non-standard grammar usage and language abbreviations within it , and the local context focus techniques can promote to infer the polarity of terms .",Model,Overall Performance Analysis,sentiment_analysis,9,16,1,1,results,0.815227138,1,results,0.039907145,7.97E-06,8.72E-05,8.38E-07,1.57E-06,2.18E-05,0.000677744,0.000128108,8.04E-06,0.22729393,1.19E-05,0.731827667,2.61E-05
11429,sentiment_analysis9,239,"Surprisingly , for the Laptop and Restaurant datasets , guests occasionally have a unified "" global "" view in a specific review .",Model,Overall Performance Analysis,sentiment_analysis,9,17,1,0,,0.000331888,0,negative,0.000563374,2.41E-06,1.83E-05,3.68E-08,1.03E-06,1.30E-06,4.16E-06,5.07E-06,2.95E-06,0.99796372,2.64E-07,0.001437283,9.99E-08
11430,sentiment_analysis9,240,"That is , if the customer is not satisfied with one aspect , it is likely to criticize the other .",Model,Overall Performance Analysis,sentiment_analysis,9,18,1,0,,1.26E-05,0,negative,2.04E-05,7.03E-07,8.75E-07,1.73E-08,7.03E-08,6.17E-07,2.05E-07,4.92E-06,7.32E-06,0.999955928,1.45E-07,8.81E-06,2.21E-08
11431,sentiment_analysis9,241,"Things will be the same if a customer prefers a restaurant he would be tolerant of some small dis amenity , so the CDW mechanism performs better because it does not completely mask the local context of the other aspect .",Model,Overall Performance Analysis,sentiment_analysis,9,19,1,0,,0.012866382,0,negative,0.004182878,1.86E-06,8.81E-06,5.35E-08,3.48E-07,2.49E-06,1.57E-05,1.79E-05,4.04E-06,0.982562869,7.67E-07,0.013201892,3.64E-07
11432,sentiment_analysis9,242,"In the multi-task learning process , the convergence rate of APC and ATE tasks is different , so the model does not achieve the optimal effect at the same time .",Model,Overall Performance Analysis,sentiment_analysis,9,20,1,0,,0.000875443,0,negative,0.000123145,1.06E-05,4.47E-06,5.19E-08,2.20E-07,1.20E-06,1.50E-06,2.91E-05,3.31E-05,0.999639694,3.15E-06,0.000153545,2.10E-07
11433,sentiment_analysis9,243,We build a joint model for the multi-task of ATE and APC based on the BERT - BASE model .,Model,Overall Performance Analysis,sentiment_analysis,9,21,1,0,,0.001275988,0,negative,0.000311399,0.000710488,0.00129799,2.66E-07,2.03E-06,6.47E-06,1.30E-05,6.28E-05,0.024165882,0.973050029,6.65E-05,0.00030859,4.58E-06
11434,sentiment_analysis9,244,"After optimizing the model parameters according to the empirical result , the joint model based on BERT - BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets , such as BERT - PT , AEN - BERT , SDGCN - BERT , and soon .",Model,Overall Performance Analysis,sentiment_analysis,9,22,1,1,results,0.647413133,1,negative,0.011301172,2.02E-05,3.47E-05,9.55E-07,2.33E-06,2.57E-05,0.000630971,0.000230529,2.41E-05,0.642664503,1.34E-05,0.345025494,2.59E-05
11435,sentiment_analysis9,245,"Meanwhile , we implement the joint - task model based on BERT - SPC .",Model,Overall Performance Analysis,sentiment_analysis,9,23,1,0,,0.023363521,0,negative,0.000419891,0.000563316,0.002201502,1.02E-07,2.06E-06,4.86E-06,1.52E-05,5.28E-05,0.003453021,0.992844444,5.38E-06,0.000436114,1.36E-06
11436,sentiment_analysis9,246,"Compared with the BERT - BASE model , BERT - SPC significantly improves the accuracy and F 1 score of aspect polarity classification .",Model,Overall Performance Analysis,sentiment_analysis,9,24,1,1,results,0.808925167,1,results,0.014593729,6.48E-06,8.65E-05,2.19E-07,1.03E-06,4.19E-06,0.000509943,2.88E-05,4.76E-06,0.235980738,7.96E-06,0.748760008,1.56E-05
11437,sentiment_analysis9,247,"In addition , for the first time , BERT - SPC has increased the F 1 score of ATE subtask on three datasets up to 99 % .",Model,Overall Performance Analysis,sentiment_analysis,9,25,1,1,results,0.403303355,0,results,0.006140098,1.42E-05,0.00026541,1.92E-07,2.29E-06,5.39E-06,0.000588633,3.36E-05,1.05E-05,0.481754748,6.43E-06,0.511162683,1.59E-05
11438,sentiment_analysis9,248,"ATEPC - Fusion is a supplementary scheme of LCF mechanism , and it adopts a moderate approach to generate local context features .",Model,Overall Performance Analysis,sentiment_analysis,9,26,1,1,results,0.007345496,0,negative,0.000701069,0.000256859,0.028450348,4.03E-07,6.97E-06,9.23E-06,4.32E-05,3.67E-05,0.001927794,0.967803161,9.53E-06,0.000746768,8.02E-06
11439,sentiment_analysis9,249,The experimental results show that its performance is also better than the existing BERT - based models .,Model,Overall Performance Analysis,sentiment_analysis,9,27,1,1,results,0.106359976,0,negative,0.004503641,4.75E-06,2.73E-05,8.08E-08,5.12E-07,3.68E-06,0.000189614,3.40E-05,6.77E-06,0.777952883,2.61E-06,0.217269907,4.18E-06
11440,sentiment_analysis9,250,Effectiveness of Multi-task Learning,Model,,sentiment_analysis,9,28,1,0,,0.058821998,0,negative,0.002598527,0.000111671,0.000592314,1.02E-05,3.86E-05,9.64E-05,0.00655361,0.000677912,0.000432739,0.603436757,0.000111633,0.384800263,0.000539398
11441,sentiment_analysis9,251,"Keeping the main architecture of the LCF - ATEPC model unchanged , we tried to only optimize parameters for a single task in the multi-task model to explore the difference between the optimal performance of a single task and the multi -task learning model 6 .",Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,29,1,0,,0.000688454,0,negative,7.24E-05,3.17E-06,6.97E-06,9.98E-08,2.06E-07,6.91E-05,3.86E-05,0.000608739,1.48E-05,0.998960277,6.92E-08,0.000224454,1.15E-06
11442,sentiment_analysis9,252,The depicts the performance of the LCF - ATEPC model when performing an single APC or ATE task .,Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,30,1,0,,0.000111556,0,negative,6.44E-06,9.21E-08,2.75E-06,5.03E-09,1.08E-08,3.05E-06,1.23E-05,2.35E-05,8.48E-07,0.999561263,5.92E-08,0.000389399,2.65E-07
11443,sentiment_analysis9,253,Experimental results show that on some datasets the LCF - ATEPC model performs better concerning APC or ATE single task than conducting ABSA multi-task on some datasets .,Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,31,1,0,,0.021585997,0,results,0.001032735,2.45E-07,4.78E-06,3.25E-08,6.79E-08,3.54E-06,0.000347875,3.22E-05,2.18E-07,0.345581889,1.97E-07,0.652992705,3.55E-06
11444,sentiment_analysis9,254,"In general , the proposed model LCF - ATEPC proposed in this paper is still superior to other ABSA - oriented multi-task models and even the single - task models aim to APC or ATE .",Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,32,1,0,,0.142531855,0,results,0.000711382,5.60E-07,1.29E-05,1.14E-07,1.43E-07,9.66E-06,0.000973449,7.75E-05,5.74E-07,0.266959726,6.25E-07,0.731239842,1.35E-05
11445,sentiment_analysis9,255,"When optimizing the model parameters for through back - propagation of multiple tasks , the multi-task learning model needs to take into account multiple loss functions of the different subtasks .",Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,33,1,0,,0.000127113,0,negative,1.72E-05,2.54E-06,1.89E-06,6.42E-08,5.78E-08,4.55E-06,4.97E-06,8.41E-05,1.71E-05,0.99963105,6.19E-07,0.000234986,9.12E-07
11446,sentiment_analysis9,256,So sometimes the multi-task,Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,34,1,0,,2.18E-06,0,negative,7.77E-07,4.14E-08,9.91E-07,3.65E-09,3.65E-09,8.82E-07,3.43E-06,5.17E-06,3.40E-07,0.999878558,6.30E-07,0.000108996,1.79E-07
11447,sentiment_analysis9,257,The empirical performance comparison between multi-task and single - task learning .,Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,35,1,0,,9.69E-06,0,negative,7.15E-06,7.71E-08,8.46E-06,1.43E-09,7.52E-09,3.14E-07,5.60E-06,1.92E-06,2.20E-07,0.998642675,3.12E-07,0.001333074,1.86E-07
11448,sentiment_analysis9,258,"The "" - "" indicates that the statistics are not important during single - task learning optimization and not listed in the table .",Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,36,1,0,,4.83E-06,0,negative,1.47E-06,9.15E-08,4.16E-07,3.09E-08,2.70E-08,7.30E-06,2.84E-06,5.43E-05,8.33E-07,0.999896431,9.74E-09,3.61E-05,1.44E-07
11449,sentiment_analysis9,259,"The optimal performance is in bold . "" * "" indicates the real performance is almost up to 100 % .",Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,37,1,0,,3.37E-05,0,negative,9.01E-07,6.57E-08,4.81E-07,4.14E-09,4.40E-09,7.31E-06,8.50E-06,8.62E-05,9.25E-07,0.999807878,2.89E-08,8.75E-05,2.14E-07
11450,sentiment_analysis9,260,Domain - adaption for LCF - ATEPC,Model,Effectiveness of Multi-task Learning,sentiment_analysis,9,38,1,0,,0.011281567,0,negative,3.06E-05,1.14E-05,0.000320657,7.33E-08,1.12E-07,7.98E-06,0.000283766,8.19E-05,7.00E-05,0.977121372,4.75E-05,0.022001076,2.35E-05
11451,sentiment_analysis9,261,SRD Sensitivity on Different Datasets,Model,,sentiment_analysis,9,39,1,0,,0.343305312,0,negative,0.000338761,0.000125827,0.00017679,6.79E-07,3.18E-06,4.19E-05,0.003312101,0.000874084,0.001155003,0.764682147,6.41E-05,0.229057759,0.000167604
11452,sentiment_analysis9,262,"We tested the sensitivity of SRD threshold on the typical Chinese and English ABSA datasets : the Phone dataset and The Restaurant dataset , respectively .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,40,1,0,,0.001519541,0,negative,6.08E-05,3.85E-05,1.40E-05,4.25E-08,6.40E-07,2.50E-06,2.06E-05,0.000110145,1.21E-05,0.99227283,7.58E-07,0.007466283,6.68E-07
11453,sentiment_analysis9,263,"Besides , for the evaluation of the restaurant dataset , we adopted the domainadapted BERT model as the underlying architecture of the LCF - ATEPC model .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,41,1,0,,0.002742175,0,negative,4.69E-05,3.23E-05,0.000142505,6.16E-08,7.73E-07,1.04E-05,2.31E-05,0.000309492,6.50E-05,0.998983786,4.37E-08,0.000385105,5.47E-07
11454,sentiment_analysis9,264,"The experimental result of , 7 are evaluated in multi-task learning process .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,42,1,0,,0.000149297,0,negative,2.80E-06,2.71E-07,7.40E-07,1.70E-09,3.32E-08,4.28E-07,1.14E-06,2.00E-05,5.29E-07,0.999649267,3.24E-09,0.000324792,2.37E-08
11455,sentiment_analysis9,265,"For the Chinese Phone dataset , the LCF - ATEPC - CDM model can achieve the best APC accuracy and F 1 score when the SRD threshold is about 4 - 5 , while the best ATE task performance reaches the highest when the SRD threshold is about 1 - 3 .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,43,1,0,,0.05945563,0,results,0.000218775,6.79E-07,1.11E-05,2.84E-08,1.12E-07,1.45E-06,0.000230442,4.19E-05,4.12E-07,0.147383332,2.12E-07,0.852106742,4.85E-06
11456,sentiment_analysis9,266,"The LCF - ATEPC - CDW model obtains the best APC performance on the Phone dataset when the SRD threshold is 5 , while the best ATE F1 score is approximately obtained when the SRD threshold is 7 .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,44,1,0,,0.108360952,0,results,0.000505698,1.12E-06,1.51E-05,5.80E-08,2.20E-07,2.39E-06,0.000352866,7.18E-05,6.90E-07,0.147440457,1.36E-07,0.851602119,7.33E-06
11457,sentiment_analysis9,267,"For the Restaurant dataset , the optimal APC accuracy and F1 score achieved by LCF - ATEPC - CDM while the SRD threshold is approximately between 4 and 6 .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,45,1,0,,0.007132076,0,negative,0.00035212,3.45E-06,9.66E-06,5.90E-08,4.06E-07,4.09E-06,0.000154345,0.000221261,2.24E-06,0.725951552,1.35E-07,0.273297061,3.63E-06
11458,sentiment_analysis9,268,"While the SRD threshold for the LCF - ATEPC - CDW is set to 8 , the model achieves the optimal aspect classification accuracy and F1 score .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,46,1,0,,0.040223425,0,negative,0.003278799,8.29E-05,2.58E-05,3.41E-06,4.97E-06,0.000140432,0.000994928,0.016684846,7.15E-05,0.868483403,6.58E-07,0.110164891,6.35E-05
11459,sentiment_analysis9,269,"However , the F1 score of the ATE task is less sensitive to the SRD threshold , indicating that aspect polarity classification task has less assistance on it during the joint learning process .",Model,SRD Sensitivity on Different Datasets,sentiment_analysis,9,47,1,0,,0.01126111,0,negative,0.004304292,1.85E-06,8.85E-06,1.01E-07,4.81E-07,2.21E-06,0.000121316,7.41E-05,1.37E-06,0.504334008,1.53E-07,0.491147855,3.38E-06
11460,sentiment_analysis9,270,Conclusion,,,sentiment_analysis,9,0,1,0,,0.000132834,0,negative,5.93E-05,7.65E-05,2.26E-06,1.26E-06,9.63E-07,3.41E-05,1.31E-05,0.000374953,6.52E-05,0.998836395,0.000467806,6.66E-05,1.52E-06
