2	0	41	Semi-supervised Word Sense Disambiguation
4	50	83	word sense disambiguation ( WSD )
5	58	70	word vectors
5	71	85	extracted from
5	133	136	WSD
20	28	52	two novel WSD algorithms
21	13	21	based on
21	24	55	Long Short Term Memory ( LSTM )
22	57	61	when
22	79	87	performs
22	88	108	significantly better
22	109	113	than
22	117	126	algorithm
22	127	135	based on
22	138	181	continuous bag of words model ( Word2 vec )
22	184	197	especially on
22	198	203	verbs
23	8	15	present
23	18	43	semi-supervised algorithm
23	50	54	uses
23	55	72	label propagation
23	73	81	to label
23	82	101	unlabeled sentences
23	102	110	based on
23	117	127	similarity
23	128	130	to
23	131	143	labeled ones
130	0	14	Sem Eval Tasks
136	0	23	Our proposed algorithms
136	24	31	achieve
136	36	66	highest all - words F 1 scores
136	67	77	except for
136	78	93	Sem - Eval 2013
138	0	11	Unified WSD
138	20	37	highest F 1 score
138	38	40	on
138	41	72	Nouns ( Sem - Eval - 7 Coarse )
138	79	93	our algorithms
138	94	104	outperform
138	105	116	Unified WSD
138	117	119	on
138	120	149	other part - of - speech tags
148	0	17	Word2 Vec vectors
150	3	11	performs
150	12	22	similar to
150	23	52	IMS + Word2 Vec ( T: SemCor )
150	57	79	SVM - based classifier
151	0	5	shows
151	15	30	LSTM classifier
151	31	42	outperforms
151	47	67	Word2 Vec classifier
152	0	17	Sem Cor Vs. OMSTI
153	42	57	LSTM classifier
153	58	70	trained with
153	71	76	OMSTI
153	77	85	performs
153	86	91	worse
153	92	96	than
153	102	114	trained with
153	115	121	SemCor
155	94	132	our naive nearest neighbor classifiers
155	133	144	do not have
155	147	160	learned model
155	165	169	deal
155	180	184	with
155	185	197	noisy labels
160	0	9	NOAD Eval
178	0	15	LSTM classifier
180	0	19	Most frequent sense
184	0	4	LSTM
184	5	16	outperforms
184	17	25	Word2Vec
184	26	28	by
184	29	57	more than 10 % overall words
186	0	23	Change of training data
191	4	41	SemCor ( or MASC ) trained classifier
191	45	58	on a par with
191	63	86	NOAD trained classifier
191	87	89	on
191	90	98	F1 score
193	0	33	Change of language model capacity
196	0	10	To balance
196	15	42	accuracy and resource usage
196	48	51	use
196	56	103	second best LSTM model ( h = 2048 and p = 512 )
197	0	19	Semi-supervised WSD
201	26	28	LP
201	29	42	did not yield
201	43	57	clear benefits
201	58	68	when using
201	73	97	Word2 Vec language model
202	7	10	see
202	11	35	significant improvements
202	38	52	6.3 % increase
202	53	55	on
202	56	62	SemCor
202	67	81	7.3 % increase
202	82	84	on
202	85	89	MASC
202	92	97	using
202	98	100	LP
202	101	105	with
202	110	129	LSTM language model
204	0	19	Change of seed data
205	20	22	LP
205	23	45	substantially improves
205	46	59	classifier F1
205	60	64	when
205	69	86	training datasets
205	87	90	are
205	91	104	SemCor + NOAD
205	108	119	MASC + NOAD
208	0	23	Change of graph density
209	16	25	construct
209	30	38	LP graph
209	39	52	by connecting
209	53	62	two nodes
209	63	65	if
209	72	80	affinity
209	81	83	is
209	84	89	above
209	90	105	95 % percentile
212	4	13	F1 scores
212	14	17	are
212	18	35	relatively stable
212	36	40	when
212	45	55	percentile
212	56	62	ranges
212	63	70	between
212	71	79	85 to 98
212	86	94	decrease
212	95	99	when
212	104	114	percentile
212	115	123	drops to
212	124	126	80
