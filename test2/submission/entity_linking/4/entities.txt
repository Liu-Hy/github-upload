26	19	26	propose
26	27	64	Neural Text - Entity Encoder ( NTEE )
26	69	89	neural network model
26	90	106	to jointly learn
26	107	134	distributed representations
26	135	137	of
26	184	195	KB entities
27	0	3	For
27	4	14	every text
27	15	17	in
27	22	24	KB
27	37	52	aims to predict
27	57	74	relevant entities
27	81	87	places
27	92	122	text and the relevant entities
27	123	131	close to
27	132	142	each other
27	143	145	in
27	148	171	continuous vector space
28	3	6	use
28	7	37	humanedited entity annotations
28	38	51	obtained from
28	52	61	Wikipedia
28	70	72	as
28	73	88	supervised data
28	89	91	of
28	92	109	relevant entities
28	110	112	to
28	117	122	texts
28	123	133	containing
29	12	23	KB entities
29	63	72	semantics
29	73	75	of
29	76	81	texts
30	28	62	Explicit Semantic Analysis ( ESA )
30	71	81	represents
30	106	111	using
30	114	133	sparse vector space
31	14	17	ESA
31	18	23	shows
31	29	33	text
35	36	43	placing
35	44	62	texts and entities
35	63	67	into
35	72	89	same vector space
35	90	97	enables
35	104	118	easily compute
35	123	133	similarity
35	134	141	between
35	142	160	texts and entities
54	59	97	https://github.com/ studio-ousia /ntee
125	0	3	BOW
125	4	6	is
125	9	30	conventional approach
125	31	36	using
125	39	76	logistic regression ( LR ) classifier
125	77	89	trained with
125	90	109	binary BOW features
125	110	120	to predict
125	125	139	correct answer
126	0	8	BOW - DT
126	12	20	based on
126	25	37	BOW baseline
126	38	52	augmented with
126	57	68	feature set
126	69	73	with
126	74	104	dependency relation indicators
127	0	5	QANTA
127	6	8	is
127	21	29	based on
127	32	56	recursive neural network
127	57	66	to derive
129	0	10	FTS - BRNN
129	14	22	based on
129	27	73	bidirectional recurrent neural network ( RNN )
129	74	78	with
129	79	108	gated recurrent units ( GRU )
137	88	99	compared to
137	169	179	our method
137	180	200	clearly outperformed
140	17	26	indicated
140	32	42	our method
140	43	59	mostly performed
140	60	67	perfect
140	68	90	in terms of predicting
