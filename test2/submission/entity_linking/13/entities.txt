2	0	25	Word Sense Disambiguation
14	111	144	word sense disambiguation ( WSD )
19	0	12	Improved WSD
23	43	51	modeling
23	56	73	sequence of words
23	74	85	surrounding
23	90	101	target word
23	193	202	represent
23	207	212	words
23	213	218	using
23	219	252	real valued vector representation
23	260	275	word embeddings
83	18	35	implemented using
83	36	46	TensorFlow
87	4	14	embeddings
87	19	36	initialized using
87	46	79	freely available 2 Glo Ve vectors
87	80	90	trained on
87	91	113	Wikipedia and Gigaword
88	0	5	Words
88	6	21	not included in
88	35	51	initialized from
88	52	65	N ( 0 , 0.1 )
106	0	18	Our proposed model
106	19	27	achieves
106	32	41	top score
106	42	44	on
106	45	48	SE2
106	57	66	tied with
106	67	83	IMS + adapted CW
106	84	86	on
106	87	90	SE3
107	14	22	see that
107	23	31	dropword
107	32	53	consistently improves
107	58	65	results
107	66	68	on
107	74	85	SE2 and SE3
108	0	11	Randomizing
108	29	40	input words
108	41	47	yields
108	50	76	substantially worse result
109	8	16	see that
109	21	27	system
109	28	52	effectively makes use of
109	57	68	information
109	69	71	in
109	76	103	pre-trained word embeddings
