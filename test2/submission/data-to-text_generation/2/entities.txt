2	38	72	Structured Data to Text Generation
22	54	69	text generation
22	75	77	as
22	80	109	sequenceto - sequence problem
132	0	11	WebNLG task
134	22	27	model
134	28	32	with
134	33	44	GCN encoder
134	45	56	outperforms
134	59	74	strong baseline
134	80	87	employs
134	92	104	LSTM encoder
134	107	111	with
134	112	128	.009 BLEU points
135	4	13	GCN model
135	14	16	is
135	22	33	more stable
135	34	38	than
135	43	51	baseline
135	52	56	with
135	59	77	standard deviation
135	78	80	of
137	4	16	GCN EC model
137	17	28	outperforms
137	29	38	PKUWRITER
137	44	48	uses
137	52	72	ensemble of 7 models
137	79	114	further reinforcement learning step
137	115	117	by
137	118	134	.047 BLEU points
137	141	150	MELBOURNE
137	151	153	by
137	154	170	.014 BLEU points
139	0	14	SR11 Deep task
150	8	15	compare
150	20	33	neural models
150	34	38	with
150	39	58	upper bound results
150	59	61	on
150	66	78	same dataset
150	79	81	by
150	86	100	pipeline model
150	108	134	STUMBA - D and TBDIL model
150	135	142	obtains
150	178	191	outperforming
150	196	213	GCN - based model
163	33	43	importance
163	44	46	of
163	47	63	skip connections
163	64	71	between
163	72	82	GCN layers
164	0	30	Residual and dense connections
164	31	38	lead to
164	39	54	similar results
165	0	17	Dense connections
165	39	46	produce
165	47	53	models
165	92	96	than
165	97	117	residual connections
