2	33	70	SEMI - SUPERVISED TEXT CLASSIFICATION
18	84	104	image classification
19	42	44	to
19	45	70	text classification tasks
20	0	25	Adversarial perturbations
20	54	73	small modifications
20	74	76	to
21	4	23	text classification
21	30	35	input
21	36	38	is
21	39	47	discrete
21	62	76	represented as
22	103	109	define
22	114	126	perturbation
22	127	129	on
22	130	156	continuous word embeddings
22	157	167	instead of
22	168	188	discrete word inputs
25	8	15	propose
25	71	86	text classifier
25	87	101	by stabilizing
25	106	129	classification function
100	16	20	used
100	21	31	TensorFlow
100	32	34	on
100	35	39	GPUs
101	26	92	https://github.com/tensorflow/models/tree/master/adversarial_text.
112	3	14	trained for
112	15	28	100,000 steps
113	3	10	applied
113	11	28	gradient clipping
113	29	33	with
113	34	38	norm
113	39	45	set to
113	46	49	1.0
113	50	52	on
113	53	71	all the parameters
113	72	78	except
113	79	94	word embeddings
115	0	3	For
115	4	18	regularization
115	19	21	of
115	26	50	recurrent language model
115	56	63	applied
115	64	71	dropout
115	72	74	on
115	79	99	word embedding layer
115	100	104	with
115	105	121	0.5 dropout rate
116	0	3	For
116	8	32	bidirectional LSTM model
116	38	42	used
116	43	64	512 hidden units LSTM
116	65	68	for
116	78	121	standard order and reversed order sequences
116	131	135	used
116	136	167	256 dimensional word embeddings
116	178	189	shared with
163	3	11	saw that
163	12	27	cosine distance
163	28	30	on
163	31	91	adversarial and virtual adversarial training ( 0.159-0.331 )
163	92	96	were
163	97	109	much smaller
163	110	114	than
163	120	122	on
163	127	182	baseline and random perturbation method ( 0.244-0.399 )
165	10	26	test performance
165	27	29	on
165	34	56	Elec and RCV1 datasets
166	31	39	improved
166	40	56	test performance
166	57	59	on
166	64	79	baseline method
166	84	92	achieved
166	93	121	state of the art performance
166	122	124	on
166	125	138	both datasets
167	0	29	Our unidirectional LSTM model
167	30	41	improves on
167	46	69	state of the art method
167	74	84	our method
167	85	89	with
167	92	110	bidirectional LSTM
167	119	127	improves
167	128	135	results
167	136	138	on
167	139	143	RCV1
169	23	25	on
169	30	53	Rotten Tomatoes dataset
170	0	20	Adversarial training
170	25	32	able to
170	33	40	improve
170	41	45	over
170	50	65	baseline method
170	72	76	with
170	82	122	adversarial and virtual adversarial cost
170	125	133	achieved
170	134	161	almost the same performance
170	162	164	as
170	169	200	current state of the art method
171	12	28	test performance
171	29	31	of
171	32	65	only virtual adversarial training
171	66	69	was
171	70	80	worse than
171	85	93	baseline
