2	18	47	Efficient Text Classification
14	18	25	explore
14	56	58	to
14	59	76	very large corpus
14	77	81	with
14	84	102	large output space
14	123	142	text classification
15	75	79	show
15	85	98	linear models
15	99	103	with
15	106	121	rank constraint
15	128	151	fast loss approximation
15	156	164	train on
15	167	180	billion words
15	181	187	within
15	188	199	ten minutes
15	208	217	achieving
15	218	229	performance
15	230	241	on par with
15	246	268	state - of - the - art
53	0	18	Sentiment analysis
60	3	6	use
60	7	22	10 hidden units
60	27	30	run
60	31	39	fastText
60	40	43	for
60	44	52	5 epochs
60	53	57	with
60	60	73	learning rate
60	74	85	selected on
60	88	102	validation set
60	103	107	from
60	108	135	{ 0.05 , 0.1 , 0.25 , 0.5 }
61	15	21	adding
61	22	40	bigram information
61	41	49	improves
61	54	65	performance
61	66	68	by
61	69	76	1 - 4 %
62	8	20	our accuracy
62	24	44	slightly better than
62	45	71	char - CNN and char - CRNN
62	80	94	bit worse than
62	95	100	VDCNN
63	17	25	increase
63	30	38	accuracy
63	39	47	slightly
63	48	56	by using
63	57	69	more n-grams
63	89	97	trigrams
63	104	115	performance
63	116	118	on
63	119	124	Sogou
63	125	135	goes up to
63	136	142	97.1 %
65	3	7	tune
65	12	27	hyperparameters
65	28	30	on
65	35	49	validation set
65	54	61	observe
65	67	72	using
65	73	80	n-grams
65	81	88	up to 5
65	89	97	leads to
65	102	118	best performance
69	0	14	Tag prediction
79	3	11	consider
79	14	40	frequency - based baseline
79	47	55	predicts
79	60	77	most frequent tag
80	8	20	compare with
80	21	29	Tagspace
80	63	65	is
80	68	88	tag prediction model
80	111	119	based on
80	124	136	Wsabie model
83	0	11	Both models
83	12	19	achieve
83	22	41	similar performance
83	42	46	with
83	49	67	small hidden layer
83	74	80	adding
83	81	88	bigrams
83	89	97	gives us
83	100	117	significant boost
83	118	120	in
83	121	129	accuracy
