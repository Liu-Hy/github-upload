2	33	52	Text Classification
22	18	25	propose
22	28	64	new graph neural networkbased method
22	65	68	for
22	69	88	text classification
23	3	12	construct
23	15	33	single large graph
23	34	38	from
23	42	55	entire corpus
23	64	72	contains
23	73	92	words and documents
23	93	95	as
23	96	101	nodes
24	3	8	model
24	13	18	graph
24	19	23	with
24	26	61	Graph Convolutional Network ( GCN )
24	66	107	simple and effective graph neural network
24	113	121	captures
24	122	158	high order neighborhoods information
25	4	8	edge
25	9	16	between
25	17	31	two word nodes
25	35	73	built byword co-occurrence information
25	82	86	edge
25	87	94	between
25	97	124	word node and document node
25	128	139	built using
25	140	154	word frequency
25	159	185	word 's document frequency
26	8	12	turn
26	13	40	text classification problem
26	41	45	into
26	46	74	anode classification problem
28	32	71	https://github. com/yao8839836/text_gcn
114	0	9	Baselines
115	3	10	compare
115	15	23	Text GCN
116	0	13	TF - IDF + LR
116	16	38	bag - of - words model
116	39	43	with
116	44	94	term frequencyinverse document frequency weighting
117	0	19	Logistic Regression
117	23	30	used as
117	35	45	classifier
118	0	3	CNN
118	6	34	Convolutional Neural Network
119	3	11	explored
119	12	21	CNN -rand
119	28	32	uses
119	33	69	randomly initialized word embeddings
119	74	91	CNN - non- static
119	98	102	uses
119	103	130	pre-trained word embeddings
120	0	4	LSTM
120	39	43	uses
120	48	65	last hidden state
120	66	68	as
120	73	87	representation
120	88	90	of
120	95	105	whole text
122	0	8	Bi- LSTM
122	13	32	bi-directional LSTM
124	0	9	PV - DBOW
124	14	36	paragraph vector model
124	55	70	orders of words
124	71	73	in
124	74	78	text
124	79	82	are
124	83	90	ignored
125	3	7	used
125	8	27	Logistic Regression
125	28	30	as
125	35	45	classifier
126	0	7	PV - DM
126	12	34	paragraph vector model
126	55	64	considers
126	69	79	word order
127	3	7	used
127	8	27	Logistic Regression
127	28	30	as
127	35	45	classifier
128	0	3	PTE
128	6	31	predictive text embedding
128	40	54	firstly learns
128	55	69	word embedding
128	70	78	based on
128	79	105	heterogeneous text network
128	106	116	containing
128	117	145	words , documents and labels
128	146	148	as
128	149	154	nodes
128	162	170	averages
128	171	186	word embeddings
128	187	189	as
128	190	209	document embeddings
128	210	213	for
128	214	233	text classification
129	0	9	fast Text
129	14	61	simple and efficient text classification method
129	70	76	treats
129	81	118	average of word / n- grams embeddings
129	119	121	as
129	122	141	document embeddings
129	149	154	feeds
129	155	174	document embeddings
129	175	179	into
129	182	199	linear classifier
131	0	4	SWEM
131	7	35	simple word embedding models
131	44	51	employs
131	52	77	simple pooling strategies
131	78	91	operated over
131	92	107	word embeddings
132	0	4	LEAM
132	7	41	label - embedding attentive models
132	50	56	embeds
132	61	77	words and labels
132	78	80	in
132	85	101	same joint space
132	102	105	for
132	106	125	text classification
133	3	11	utilizes
133	12	30	label descriptions
134	0	15	Graph - CNN - C
134	20	35	graph CNN model
134	41	49	operates
134	50	62	convolutions
134	63	67	over
134	68	100	word embedding similarity graphs
134	162	178	Chebyshev filter
135	0	15	Graph - CNN - S
135	30	45	Graph - CNN - C
135	50	55	using
135	56	69	Spline filter
136	0	15	Graph - CNN - F
136	22	29	same as
136	30	45	Graph - CNN - C
136	50	55	using
136	56	70	Fourier filter
155	0	3	For
155	4	12	Text GCN
155	18	21	set
155	26	40	embedding size
155	41	43	of
155	48	71	first convolution layer
155	72	74	as
155	75	78	200
155	83	86	set
155	91	102	window size
155	103	105	as
155	106	108	20
157	3	8	tuned
157	9	25	other parameters
157	30	33	set
157	38	51	learning rate
157	52	54	as
157	55	59	0.02
157	62	69	dropout
157	100	126	default parameter settings
158	0	3	For
158	4	19	baseline models
158	20	25	using
158	26	53	pre-trained word embeddings
158	59	63	used
158	64	102	300 dimensional Glo Ve word embeddings
161	0	8	Text GCN
161	9	17	performs
161	22	26	best
161	31	56	significantly outperforms
161	57	76	all baseline models
161	115	117	on
161	118	131	four datasets
163	0	4	When
163	5	39	pre-trained Glo Ve word embeddings
163	44	52	provided
163	55	58	CNN
163	59	67	performs
163	68	79	much better
163	82	95	especially on
163	96	113	Ohsumed and 20 NG
165	12	31	LSTM - based models
165	37	44	rely on
165	45	72	pre-trained word embeddings
165	77	84	tend to
165	85	92	perform
165	93	99	better
165	100	104	when
165	105	114	documents
165	115	118	are
165	119	126	shorter
166	0	9	PV - DBOW
166	10	18	achieves
166	19	37	comparable results
166	38	40	to
166	41	57	strong baselines
166	58	60	on
166	61	78	20 NG and Ohsumed
166	97	99	on
166	100	112	shorter text
166	137	143	others
168	0	7	PV - DM
168	8	16	performs
168	17	22	worse
168	23	27	than
168	28	37	PV - DBOW
168	75	77	MR
172	0	18	Graph - CNN models
172	24	28	show
172	29	53	competitive performances
