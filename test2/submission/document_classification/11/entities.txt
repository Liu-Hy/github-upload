2	35	54	Text Classification
30	19	26	propose
30	29	66	task - oriented word embedding method
38	20	71	words ' contextual information and task information
38	76	94	inherently jointed
38	95	107	to construct
38	112	127	word embeddings
40	0	8	To model
40	13	29	task information
40	35	45	regularize
40	50	62	distribution
40	63	65	of
40	70	83	salient words
40	84	91	to have
40	94	123	clear classification boundary
40	135	141	adjust
40	146	158	distribution
40	159	161	of
40	166	177	other words
40	178	180	in
40	185	200	embedding space
146	73	83	BOW method
146	87	98	employed as
146	101	115	basic baseline
147	3	13	represents
147	14	27	each document
147	28	30	as
147	33	45	bag of words
147	54	70	weighting scheme
147	71	73	is
147	74	79	TFIDF
149	10	26	Word2 Vec method
149	27	29	is
149	32	62	neural network language method
149	63	75	which learns
149	76	91	word embeddings
149	92	105	by maximizing
149	110	133	conditional probability
149	134	144	leveraging
149	145	167	contextual information
183	6	16	Our method
183	17	25	performs
183	26	32	better
183	33	37	than
183	42	55	other methods
184	20	36	ToWE - SG method
184	37	62	significantly outperforms
184	67	82	other baselines
184	83	85	on
184	90	104	20 New s Group
184	107	125	5 Abstract s Group
184	132	134	MR
186	10	32	word embedding methods
186	33	43	outperform
186	48	78	basic bag - of - words methods
186	79	81	in
186	82	92	most cases
186	95	105	indicating
186	110	121	superiority
186	122	124	of
186	157	161	over
190	0	10	Our method
190	11	19	achieves
190	20	38	better performance
190	39	43	over
190	44	59	Retrofit method
193	0	10	Our method
193	11	22	outperforms
193	27	37	TWE method
193	38	40	on
193	50	93	document - level and sentence - level tasks
