2	27	66	CROSS - LINGUAL DOCUMENT CLASSIFICATION
5	23	60	cross - lingual understanding ( XLU )
8	3	10	combine
8	11	57	state - of - the - art cross - lingual methods
8	89	92	for
8	93	119	weakly supervised learning
8	120	127	such as
8	128	153	unsupervised pre-training
8	158	188	unsupervised data augmentation
8	222	234	language gap
8	243	253	domain gap
8	254	256	in
8	257	260	XLU
38	43	46	for
38	47	64	domain adaptation
39	20	28	based on
39	29	71	masked language model ( MLM ) pre-training
39	82	87	using
39	88	121	unlabeled target language corpora
41	21	61	unsupervised data augmentation ( UDA ) )
41	64	69	where
41	70	91	synthetic paraphrases
41	96	110	generated from
41	115	131	unlabeled corpus
41	142	147	model
41	151	161	trained on
41	164	186	label consistency loss
152	0	16	Fine-tune ( Ft )
152	19	32	Fine - tuning
152	37	54	pre-trained model
152	55	59	with
152	64	92	source - domain training set
154	0	28	Fine - tune with UDA ( UDA )
155	12	20	utilizes
155	25	39	unlabeled data
155	40	44	from
155	49	62	target domain
155	63	76	by optimizing
155	81	98	UDA loss function
156	0	15	Self - training
156	16	24	based on
156	29	53	UDA model ( UDA + Self )
157	9	14	train
157	19	41	Ft model and UDA model
157	48	54	choose
157	59	69	better one
157	70	72	as
157	77	90	teacher model
158	4	17	teacher model
158	37	52	new XLM student
158	53	58	using
158	59	84	only unlabeled data U tgt
158	85	87	in
158	92	105	target domain
171	11	29	Ft ( XLM ) results
171	49	68	without the help of
171	69	83	unlabeled data
171	84	88	from
171	93	106	target domain
171	130	145	substantial gap
171	146	153	between
171	158	175	model performance
171	176	178	of
171	215	236	monolingual baselines
171	244	254	when using
171	255	320	state - of - the - art pre-trained cross -lingual representations
172	9	43	UDA algorithm and MLM pre-training
172	48	53	offer
172	54	78	significant improvements
172	79	91	by utilizing
172	96	110	unlabeled data
173	0	2	In
173	7	36	sentiment classification task
173	39	44	where
173	49	68	unlabeled data size
173	69	71	is
173	72	78	larger
173	81	123	Ft ( XLM ft ) model usnig MLM pre-training
173	124	145	consistently provides
173	146	165	larger improvements
173	166	179	compared with
173	184	194	UDA method
174	24	34	MLM method
174	35	37	is
174	38	72	relatively more resource intensive
174	77	82	takes
174	83	89	longer
174	90	101	to converge
175	14	16	in
175	21	34	MLdoc dataset
175	37	41	when
175	46	50	size
175	51	53	of
175	58	75	unlabeled samples
175	76	78	is
175	79	86	limited
175	93	103	UDA method
175	104	106	is
175	107	119	more helpful
178	0	2	In
178	7	36	sentiment classification task
178	42	49	observe
178	54	79	self - training technique
178	80	101	consistently improves
178	102	106	over
178	111	124	teacher model
179	3	9	offers
179	10	22	best results
179	23	25	in
179	31	63	XLM and XLM ft based classifiers
181	0	2	In
181	7	20	MLdoc dataset
181	23	38	self - training
181	44	52	achieves
181	57	69	best results
183	10	24	comparing with
183	29	57	best cross - lingual results
183	62	94	monolingual fine - tune baseline
183	104	111	able to
183	112	128	completely close
183	133	148	performance gap
183	149	161	by utilizing
183	162	176	unlabeled data
183	177	179	in
183	184	199	target language
184	14	27	our framework
184	28	35	reaches
184	36	70	new state - of - the - art results
184	73	87	improving over
184	88	109	vanilla XLM baselines
184	110	112	by
184	113	117	44 %
214	0	10	Leveraging
214	15	29	unlabeled data
214	30	34	from
214	35	48	other domains
214	49	63	does not offer
214	64	86	consistent improvement
214	101	108	provide
214	109	125	additional value
214	126	128	in
214	129	143	isolated cases
