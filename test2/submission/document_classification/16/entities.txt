2	30	49	Text Classification
29	19	28	introduce
29	31	47	new architecture
29	48	57	short for
29	58	66	C - LSTM
29	67	79	by combining
29	80	92	CNN and LSTM
29	93	101	to model
29	102	111	sentences
30	56	62	design
30	65	109	simple end - to - end , unified architecture
30	110	120	by feeding
30	125	131	output
30	132	134	of
30	137	152	one - layer CNN
30	153	157	into
30	158	162	LSTM
31	4	7	CNN
31	11	32	constructed on top of
31	37	61	pre-trained word vectors
31	62	66	from
31	67	94	massive unlabeled text data
31	95	103	to learn
31	104	132	higher - level representions
31	133	135	of
31	136	143	n-grams
32	5	13	to learn
32	14	37	sequential correlations
32	38	42	from
32	43	81	higher - level suqence representations
32	88	100	feature maps
32	101	103	of
32	104	107	CNN
32	112	124	organized as
32	125	151	sequential window features
32	152	163	to serve as
32	168	173	input
32	174	176	of
32	177	181	LSTM
34	3	9	choose
34	10	32	sequence - based input
34	33	43	other than
34	44	54	relying on
34	59	80	syntactic parse trees
34	81	98	before feeding in
34	103	117	neural network
142	3	12	implement
142	13	22	our model
142	23	31	based on
142	32	38	Theano
142	45	59	python library
142	68	76	supports
142	77	111	efficient symbolic differentiation
142	116	134	transparent use of
142	137	140	GPU
143	31	33	of
143	55	57	of
143	62	69	tensors
143	75	80	train
143	85	90	model
143	91	93	on
143	96	99	GPU
147	32	35	use
147	36	78	one convolutional layer and one LSTM layer
147	79	82	for
147	83	93	both tasks
167	0	3	For
167	4	8	TREC
167	15	32	number of filters
167	36	45	set to be
167	46	49	300
167	58	74	memory dimension
167	78	87	set to be
167	88	91	300
168	4	40	word vector layer and the LSTM layer
168	45	60	dropped outwith
168	63	74	probability
168	75	77	of
168	78	81	0.5
169	8	11	add
169	12	29	L2 regularization
169	30	34	with
169	37	43	factor
169	44	46	of
169	47	52	0.001
169	53	55	to
169	60	67	weights
169	68	70	in
169	75	88	softmax layer
169	89	92	for
169	93	103	both tasks
173	0	24	Sentiment Classification
184	34	41	achieve
184	46	74	fourth best published result
184	75	78	for
184	83	112	5 - class classification task
185	0	3	For
185	8	34	binary classification task
185	40	47	achieve
185	48	66	comparable results
185	67	82	with respect to
185	87	114	state - of - the - art ones
191	0	28	Question Type Classification
198	17	41	consistently outperforms
198	42	78	all published neural baseline models
199	20	28	close to
199	34	36	of
199	41	67	state - of - the - art SVM
199	73	83	depends on
199	84	110	highly engineered features
206	43	69	single convolutional layer
206	70	74	with
206	75	90	filter length 3
206	91	109	always outperforms
206	114	125	other cases
210	6	11	shown
210	17	43	single convolutional layer
210	44	48	with
210	49	64	filter length 3
210	65	73	performs
210	74	78	best
210	79	84	among
210	85	110	all filter configurations
211	0	3	For
211	16	45	multiple convolutional layers
211	46	57	in parallel
211	66	76	shown that
211	77	98	filter configurations
211	99	103	with
211	104	119	filter length 3
211	120	128	performs
211	129	135	better
211	136	140	that
211	147	154	without
211	155	171	tri-gram filters
