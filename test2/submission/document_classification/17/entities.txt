2	37	56	Text Classification
35	34	52	deep architectures
35	53	55	of
35	56	81	many convolutional layers
35	106	111	using
35	112	127	up to 29 layers
196	4	14	dictionary
196	15	26	consists of
196	31	51	following characters
196	54	87	abcdefghijklmnopqrstuvwxyz0123456
196	132	136	plus
196	139	154	special padding
196	157	180	space and unknown token
196	187	196	add up to
196	199	207	total of
196	208	217	69 tokens
197	4	14	input text
197	18	27	padded to
197	30	40	fixed size
197	41	43	of
197	44	48	1014
197	51	62	larger text
197	67	76	truncated
198	4	23	character embedding
198	27	29	of
198	30	34	size
198	35	37	16
199	0	8	Training
199	12	26	performed with
199	27	30	SGD
199	33	38	using
199	41	51	mini-batch
199	52	59	of size
199	60	63	128
199	69	90	initial learning rate
199	91	93	of
199	94	98	0.01
199	103	111	momentum
199	112	114	of
199	115	118	0.9
204	4	18	implementation
204	22	32	done using
204	33	38	Torch
204	33	40	Torch 7
205	20	32	performed on
205	35	56	single NVidia K40 GPU
206	73	76	use
206	77	96	temporal batch norm
206	97	104	without
206	105	112	dropout
210	4	21	deep architecture
210	22	27	works
210	28	32	well
210	33	35	on
210	36	49	big data sets
210	66	74	even for
210	75	87	small depths
212	0	3	For
212	8	22	smallest depth
212	26	29	use
212	62	70	see that
212	71	80	our model
212	89	97	performs
212	105	109	than
212	110	142	Zhang 's convolutional baselines
212	218	220	on
212	225	242	biggest data sets
214	4	27	most important decrease
214	28	30	in
214	31	51	classification error
214	59	70	observed on
214	75	103	largest data set Amazon Full
214	114	150	more than 3 Million training samples
217	8	15	observe
217	21	24	for
217	27	38	small depth
217	41	63	temporal max - pooling
217	64	69	works
217	70	74	best
217	75	77	on
217	78	91	all data sets
