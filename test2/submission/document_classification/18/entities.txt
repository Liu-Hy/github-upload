2	45	64	Text Classification
18	27	35	treating
18	36	40	text
18	41	43	as
18	46	64	kind of raw signal
18	65	67	at
18	68	83	character level
18	90	98	applying
18	99	136	temporal ( one-dimensional ) ConvNets
103	0	19	Traditional Methods
106	0	16	Bag - of - words
106	25	30	TFIDF
107	0	3	For
107	4	16	each dataset
107	23	45	bag - of - words model
107	49	73	constructed by selecting
107	74	100	50,000 most frequent words
107	101	105	from
107	110	125	training subset
112	0	17	Bag - of - ngrams
112	26	31	TFIDF
113	4	28	bag - of - ngrams models
113	33	57	constructed by selecting
113	62	111	500,000 most frequent n-grams ( up to 5 - grams )
113	112	116	from
113	121	136	training subset
113	137	140	for
113	141	153	each dataset
115	0	16	Bag - of - means
115	17	19	on
115	20	34	word embedding
116	16	34	experimental model
116	40	44	uses
116	45	52	k-means
116	53	55	on
116	56	64	word2vec
116	65	76	learnt from
116	81	96	training subset
116	97	99	of
116	100	112	each dataset
116	124	127	use
116	134	146	learnt means
116	147	149	as
116	150	165	representatives
116	166	168	of
116	173	188	clustered words
121	0	21	Deep Learning Methods
124	0	21	Word - based ConvNets
131	0	24	Long - short term memory
194	51	53	is
194	59	85	character - level ConvNets
194	92	100	work for
194	101	120	text classification
194	121	128	without
194	133	141	need for
194	142	147	words
199	0	19	Traditional methods
199	20	24	like
199	25	38	n-grams TFIDF
199	39	45	remain
199	46	63	strong candidates
199	64	67	for
199	68	75	dataset
199	76	83	of size
199	84	89	up to
199	90	119	several hundreds of thousands
199	193	200	observe
199	206	232	character - level ConvNets
199	233	244	start to do
199	245	251	better
200	0	9	Conv Nets
200	14	18	work
200	19	23	well
200	24	27	for
200	28	49	user - generated data
207	0	18	Choice of alphabet
207	19	24	makes
207	27	37	difference
211	13	18	tasks
