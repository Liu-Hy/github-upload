4	62	82	constituency parsing
10	15	42	neural constituency parsing
35	99	158	two state - of - the - art generative neural parsing models
35	165	222	Recurrent Neural Network Grammar generative parser ( RG )
35	236	283	LSTM language modeling generative parser ( LM )
36	19	26	present
36	31	34	use
36	37	66	beam - based search procedure
36	67	71	with
36	75	96	augmented state space
36	106	124	search directly in
36	129	146	generative models
43	46	52	taking
43	55	71	weighted average
43	72	74	of
43	79	85	scores
43	86	88	of
43	89	100	both models
43	101	115	when selecting
43	118	123	parse
43	124	128	from
43	133	162	base parser 's candidate list
43	163	171	improves
43	192	197	score
43	198	200	of
43	205	221	generative model
90	0	10	Augmenting
90	15	28	candidate set
98	0	2	RG
98	3	12	decreases
98	13	24	performance
98	25	29	from
98	30	38	93.45 F1
98	39	41	to
98	42	50	92.78 F1
98	51	53	on
98	58	73	development set
107	0	17	Score combination
111	13	22	combining
111	27	33	scores
111	34	36	of
111	37	48	both models
111	49	60	improves on
111	61	66	using
111	71	76	score
111	77	79	of
111	80	98	either model alone
119	0	31	Strengthening model combination
123	2	11	Combining
123	12	33	candidates and scores
123	34	38	from
123	39	55	all three models
123	71	77	obtain
123	78	88	93.94 F1 .
123	78	86	93.94 F1
131	0	10	Ensembling
131	24	34	compare to
131	84	94	ensembling
131	95	113	multiple instances
131	121	136	same model type
131	137	149	trained from
131	150	182	different random initializations
134	0	11	Performance
134	12	22	when using
134	32	51	ensembled RD models
134	63	65	is
134	66	71	lower
134	72	76	than
134	77	86	rescoring
134	89	104	single RD model
134	105	109	with
134	110	128	score combinations
134	132	145	single models
134	155	162	RD + RG
134	176	183	RD + LM
135	0	2	In
135	7	18	PTB setting
135	21	31	ensembling
135	32	36	with
135	37	54	score combination
135	55	63	achieves
135	68	88	best over all result
135	89	91	of
135	92	97	94.25
