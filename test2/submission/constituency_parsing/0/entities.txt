2	0	33	Recurrent Neural Network Grammars
12	19	28	introduce
12	29	70	recurrent neural network grammars ( RNNGs
12	116	118	of
12	119	128	sentences
12	152	187	nested , hierarchical relationships
13	0	5	RNNGs
13	6	17	operate via
13	20	47	recursive syntactic process
13	63	110	probabilistic context - free grammar generation
13	117	126	decisions
13	131	150	parameterized using
13	151	155	RNNs
13	161	173	condition on
13	178	213	entire syntactic derivation history
15	3	7	give
15	8	20	two variants
15	21	23	of
15	28	37	algorithm
15	44	47	for
15	48	55	parsing
15	128	138	generation
24	4	24	discriminative model
24	42	59	ancestor sampling
24	60	69	to obtain
24	70	77	samples
24	78	80	of
24	81	92	parse trees
24	93	96	for
24	97	106	sentences
24	169	174	RNNGs
24	177	190	approximating
24	195	227	marginal likelihood and MAP tree
24	242	247	under
24	252	268	generative model
25	3	10	present
25	13	49	simple importance sampling algorithm
25	56	60	uses
25	61	68	samples
25	69	73	from
25	78	99	discriminative parser
25	100	108	to solve
25	109	127	inference problems
25	128	130	in
25	135	151	generative model
160	0	3	For
160	8	28	discriminative model
160	34	38	used
160	39	56	hidden dimensions
160	57	59	of
160	60	83	128 and 2 - layer LSTMs
161	0	3	For
161	8	24	generative model
161	30	34	used
161	35	49	256 dimensions
161	54	67	2 layer LSTMs
162	21	26	tuned
162	31	43	dropout rate
162	44	55	to maximize
162	56	81	validation set likelihood
162	84	93	obtaining
162	94	107	optimal rates
162	108	110	of
163	0	3	For
163	8	32	sequential LSTM baseline
163	33	36	for
163	41	55	language model
163	66	71	found
163	75	95	optimal dropout rate
163	96	98	of
163	99	102	0.3
164	0	3	For
164	4	12	training
164	16	20	used
164	21	48	stochastic gradient descent
164	49	53	with
164	56	69	learning rate
164	70	72	of
164	73	76	0.1
