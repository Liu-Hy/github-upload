(single attention model||gets to||88.3)
(ensemble of 5 LSTM + A+D models||achieves||90.5)
(90.5||matching||single - model BerkeleyParser)
(single - model BerkeleyParser||on||WSJ)
(single attention model||has||88.3)
(single LSTM + A model||achieves||92.5)
(large high - confidence corpus||has||single LSTM + A model)
(outperforms||has||best single model)
(Results||trained on||large high - confidence corpus)
(ensemble||of||5 LSTM+ A models)
(5 LSTM+ A models||improves||score)
(score||to||92.8)
(LSTM + A model||trained on||WSJ dataset)
(WSJ dataset||produced||malformed trees)
(malformed trees||for||25 of the 1700 sentences)
(25 of the 1700 sentences||in||our development set)
(full high - confidence dataset||for||14 sentences ( 0.8 % ))
(Results||has||LSTM + A model)
(difference||between||F 1 score)
(F 1 score||on||sentences)
(1.3||for||BerkeleyParser)
(1.3||for||baseline LSTM)
(1.3||for||0.7)
(1.7||for||baseline LSTM)
(0.7||for||LSTM + A)
(Results||has||difference)
(LSTM + A||trained on||high - confidence corpus)
(high - confidence corpus||achieved||F 1 score)
(F 1 score||of||95.7)
(F 1 score||of||84.6)
(95.7||on||QTB)
(95.7||on||84.6)
(84.6||on||WEB)
(Results||has||LSTM + A)
(Contribution||has||Results)
