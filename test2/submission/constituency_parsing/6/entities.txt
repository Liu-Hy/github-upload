2	0	7	Parsing
4	3	9	recast
4	10	27	syntactic parsing
4	157	191	constituency Penn Treebank parsing
4	219	224	using
4	232	234	as
4	235	243	training
4	252	263	development
4	271	283	tri-training
8	71	78	present
8	81	108	neural - net parse reranker
8	114	122	achieves
8	123	140	very good results
8	143	151	93.8 F 1
8	154	158	with
8	161	194	comparatively simple architecture
51	14	31	three LSTM layers
51	32	36	with
51	37	48	1,500 units
51	58	70	trained with
51	71	96	truncated backpropagation
51	97	104	through
51	105	109	time
51	110	114	with
51	115	133	mini-batch size 20
51	138	150	step size 50
52	3	13	initialize
52	14	29	starting states
52	30	34	with
52	35	75	previous minibatch 's last hidden states
53	4	20	forget gate bias
53	24	41	initialized to be
53	42	45	one
53	54	78	rest of model parameters
53	83	95	sampled from
53	96	115	U ( ? 0.05 , 0.05 )
54	0	7	Dropout
54	11	21	applied to
54	22	47	non-recurrent connections
54	52	61	gradients
54	66	73	clipped
54	74	78	when
54	85	89	norm
54	93	104	bigger than
54	105	107	20
55	4	17	learning rate
55	18	20	is
55	21	34	0.25 0.85 max
55	47	59	epoch number
56	20	23	use
56	24	39	vanilla softmax
56	40	44	over
56	48	65	entire vocabulary
56	69	79	opposed to
56	80	100	hierarchical softmax
56	104	132	noise contrastive estimation
77	2	25	single LSTM - LM ( GS )
77	26	39	together with
77	40	55	Charniak ( GS )
77	56	63	reaches
77	64	68	93.6
77	112	116	with
77	117	132	Charniak ( GS )
77	133	141	achieves
77	144	164	new state of the art
77	167	175	93.8 F 1
78	5	10	trees
78	15	27	converted to
78	28	49	Stanford dependencies
78	54	65	UAS and LAS
78	66	69	are
78	70	87	95.9 % and 94.1 %
78	131	165	state of the art dependency parser
