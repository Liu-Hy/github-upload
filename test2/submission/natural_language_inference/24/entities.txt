2	28	49	Open-Domain Questions
9	70	91	open - domain setting
9	92	97	using
9	98	107	Wikipedia
9	108	110	as
14	49	57	retrieve
14	62	83	few relevant articles
14	84	89	among
14	90	115	more than 5 million items
14	127	131	scan
14	137	146	carefully
14	147	158	to identify
14	163	169	answer
15	23	55	machine reading at scale ( MRS )
16	9	15	treats
16	16	25	Wikipedia
16	26	28	as
16	31	53	collection of articles
17	14	26	our approach
17	27	29	is
17	30	37	generic
17	42	50	could be
17	51	62	switched to
17	63	93	other collections of documents
17	112	136	daily updated newspapers
20	0	6	Having
20	9	32	single knowledge source
20	33	39	forces
20	44	49	model
20	50	55	to be
20	56	68	very precise
20	69	74	while
20	75	88	searching for
20	92	98	answer
174	3	6	use
174	7	36	3 - layer bidirectional LSTMs
174	37	41	with
174	42	62	h = 128 hidden units
174	63	66	for
174	72	103	paragraph and question encoding
175	3	8	apply
175	13	37	Stanford CoreNLP toolkit
175	38	41	for
175	42	54	tokenization
175	64	74	generating
175	75	80	lemma
175	83	98	partof - speech
175	105	122	named entity tags
176	17	34	training examples
176	39	48	sorted by
176	53	59	length
176	60	62	of
176	63	72	paragraph
176	77	89	divided into
176	90	101	minibatches
176	102	104	of
176	105	116	32 examples
177	3	6	use
177	7	13	Adamax
177	14	17	for
177	18	30	optimization
178	0	7	Dropout
178	8	12	with
178	13	20	p = 0.3
178	24	34	applied to
178	35	50	word embeddings
178	55	75	all the hidden units
178	76	78	of
178	79	84	LSTMs
182	0	27	Our system ( single model )
182	40	58	70.0 % exact match
182	63	80	79.0 % F 1 scores
182	81	83	on
182	88	96	test set
182	105	114	surpasses
182	115	140	all the published results
182	149	154	match
182	159	174	top performance
182	175	177	on
182	182	199	SQuAD leaderboard
186	0	7	Without
186	12	46	aligned question embedding feature
186	99	109	our system
186	119	134	able to achieve
186	135	137	F1
186	138	142	over
186	143	147	77 %
187	27	33	remove
187	34	38	both
187	39	66	f aligned and f exact match
187	73	84	performance
187	85	90	drops
187	91	103	dramatically
