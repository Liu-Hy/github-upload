2	13	48	Large - Scale Reading Comprehension
4	0	29	Machine reading comprehension
6	19	26	propose
6	29	67	novel deep neural network architecture
6	68	77	to handle
6	80	103	long - range dependency
6	104	106	in
6	107	115	RC tasks
13	0	28	Reading comprehension ( RC )
23	28	35	propose
23	36	56	two novel strategies
23	57	69	that improve
23	74	102	memory - handling capability
23	103	119	while mitigating
23	124	146	information distortion
24	3	9	extend
24	14	31	memory controller
24	32	36	with
24	39	58	residual connection
24	59	71	to alleviate
24	76	98	information distortion
25	8	14	expand
25	19	47	gated recurrent unit ( GRU )
25	48	52	with
25	55	71	dense connection
25	77	84	conveys
25	85	102	enriched features
25	103	105	to
25	110	120	next layer
25	121	131	containing
25	136	183	original as well as the transformed information
120	25	33	to build
120	38	43	model
120	48	54	Sonnet
120	57	69	to implement
120	74	90	memory interface
121	0	4	NLTK
121	8	16	used for
121	17	27	tokenizing
121	17	33	tokenizing words
122	0	2	In
122	7	24	memory controller
122	30	33	use
122	34	49	four read heads
122	54	68	one write head
122	79	90	memory size
122	94	100	set to
122	101	107	100 36
122	119	133	initialized as
122	134	135	0
123	4	29	hidden vector dimension l
123	33	39	set to
123	40	43	200
124	3	6	use
124	7	33	AdaDelta ( Zeiler , 2012 )
124	34	36	as
124	40	49	optimizer
124	50	54	with
124	57	70	learning rate
124	71	73	of
124	74	77	0.5
125	4	14	batch size
125	18	24	set to
125	25	27	20
125	28	31	for
125	32	40	TriviaQA
125	45	47	30
125	48	51	for
125	52	72	SQuAD and QUASAR - T
126	3	6	use
126	10	36	exponential moving average
126	37	39	of
126	40	47	weights
126	48	52	with
126	55	70	decaying factor
126	71	73	of
126	74	79	0.001
127	15	22	require
127	23	34	more memory
127	35	39	than
127	40	56	existing methods
127	65	75	single GPU
127	135	141	within
131	10	12	in
131	13	37	lengthy - document cases
131	38	45	such as
131	73	82	our model
131	83	94	outperforms
131	95	120	all the published results
131	157	159	in
131	164	185	short - document case
131	186	193	such as
131	194	199	SQuAD
131	224	236	best results
164	3	9	assume
164	19	32	concatenation
164	33	35	of
164	40	53	layer outputs
164	54	56	in
164	57	61	DEBS
164	62	67	helps
164	72	89	memory controller
164	90	95	store
164	96	122	contextual representations
166	20	25	using
166	26	30	DEBS
166	31	33	in
166	34	48	all the places
166	49	57	improves
166	62	78	performance most
166	103	120	memory controller
166	121	125	with
166	126	130	DEBS
166	131	136	gives
166	141	167	largest performance margin
