2	0	18	Text Understanding
5	81	96	well suited for
65	10	16	called
65	21	55	Attention Sum Reader ( AS Reader )
65	61	74	tailor - made
65	75	86	to leverage
65	91	95	fact
65	105	111	answer
65	112	114	is
65	117	121	word
65	122	126	from
65	131	147	context document
70	3	10	compute
70	13	29	vector embedding
70	30	32	of
70	37	42	query
72	3	10	compute
72	13	29	vector embedding
72	30	32	of
72	33	53	each individual word
72	54	56	in
72	61	68	context
72	69	71	of
72	76	90	whole document
74	0	5	Using
74	8	19	dot product
74	20	27	between
74	32	79	question embedding and the contextual embedding
74	80	82	of
74	83	98	each occurrence
74	99	101	of
74	104	120	candidate answer
74	121	123	in
74	128	136	document
74	142	148	select
74	153	171	most likely answer
172	13	18	model
172	22	26	used
172	27	54	stochastic gradient descent
172	55	59	with
172	64	80	ADAM update rule
172	85	98	learning rate
172	99	101	of
172	102	117	0.001 or 0.0005
178	0	7	Weights
178	8	10	in
178	15	27	GRU networks
178	33	47	initialized by
178	48	74	random orthogonal matrices
178	79	85	biases
178	91	105	initialized to
178	106	110	zero
179	8	12	used
179	15	42	gradient clipping threshold
179	43	45	of
179	46	48	10
179	53	60	batches
179	61	68	of size
179	69	71	32
212	0	5	mance
212	6	8	of
212	9	25	our single model
212	26	28	is
212	48	52	than
212	53	64	performance
212	65	67	of
212	68	99	simultaneously published models
214	10	32	ensemble of our models
214	33	44	outperforms
215	0	2	On
215	7	18	CNN dataset
215	36	40	with
215	41	65	best validation accuracy
215	66	74	achieves
215	77	90	test accuracy
215	91	93	of
215	94	100	69.5 %
216	4	23	average performance
216	24	26	of
216	31	46	top 20 % models
216	47	59	according to
216	60	79	validation accuracy
216	80	82	is
216	83	89	69.9 %
216	126	156	single best - validation model
218	0	6	Fusing
218	7	22	multiple models
218	28	33	gives
218	36	64	significant further increase
218	65	67	in
218	68	76	accuracy
218	77	79	on
218	85	112	CNN and Daily Mail datasets
220	0	2	In
220	3	26	named entity prediction
220	31	48	best single model
220	49	53	with
220	54	62	accuracy
220	63	65	of
220	66	72	68.6 %
220	73	81	performs
220	82	101	2 % absolute better
220	102	106	than
220	111	116	MemNN
220	117	121	with
220	122	138	self supervision
220	145	163	averaging ensemble
220	164	172	performs
220	173	192	4 % absolute better
220	193	197	than
220	202	222	best previous result
221	0	2	In
221	3	25	common noun prediction
221	26	43	our single models
221	44	46	is
221	47	73	0.4 % absolute better than
221	74	80	Mem NN
221	93	101	ensemble
221	102	110	improves
221	115	126	performance
221	127	129	to
221	130	134	69 %
221	169	174	MemNN
