2	74	94	TEXT BASED REASONING
4	13	48	text - based question and answering
11	34	62	https://github.com/juung/RMN
42	23	56	Relation Memory Network " ( RMN )
42	62	74	able to find
42	75	91	complex relation
42	97	101	when
42	104	122	lot of information
42	126	131	given
43	3	7	uses
43	8	11	MLP
43	12	23	to find out
43	24	44	relevant information
43	45	49	with
43	52	70	new generalization
43	94	105	information
44	17	20	RMN
44	21	29	inherits
44	30	66	RN 's MLP - based output feature map
44	67	69	on
44	70	97	Memory Network architecture
125	0	29	bAbI story - based QA dataset
129	0	19	Embedding component
129	36	41	where
129	42	60	story and question
129	65	81	embedded through
129	82	97	different LSTMs
129	100	132	32 unit word - lookup embeddings
129	135	147	32 unit LSTM
129	148	151	for
129	152	170	story and question
130	0	3	For
130	4	23	attention component
130	32	35	use
130	36	45	2 hop RMN
130	58	63	g 1 ?
132	0	3	For
132	4	18	regularization
132	24	27	use
132	28	47	batch normalization
132	48	51	for
132	52	60	all MLPs
133	4	18	softmax output
133	23	37	optimized with
133	40	69	cross - entropy loss function
133	70	75	using
133	80	94	Adam optimizer
133	95	99	with
133	102	115	learning rate
133	116	118	of
133	119	125	2 e ?4
134	0	19	bAbI dialog dataset
135	3	13	trained on
135	14	33	full dialog scripts
135	34	38	with
135	39	59	every model response
135	60	62	as
135	63	69	answer
135	72	99	all previous dialog history
135	100	102	as
135	103	112	sentences
135	113	128	to be memorized
135	139	158	last user utterance
135	159	161	as
135	162	170	question
136	6	13	selects
136	18	40	most probable response
136	41	45	from
136	46	62	4,212 candidates
136	73	84	ranked from
136	87	90	set
136	91	93	of
136	94	112	all bot utterances
136	113	125	appearing in
136	126	179	training , validation and test sets ( plain and OOV )
136	180	183	for
136	184	193	all tasks
154	0	11	BABI DIALOG
156	0	7	Without
156	25	35	RN and RMN
156	36	46	outperform
156	47	81	previous memory - augmented models
156	82	84	on
156	90	110	normal and OOV tasks
160	3	12	converted
160	13	39	RMN 's attention component
160	40	42	to
160	43	72	inner product based attention
160	91	99	revealed
160	104	114	error rate
160	115	127	increased to
160	128	134	11.3 %
162	4	38	number of unnecessary object pairs
162	39	49	created by
162	54	56	RN
162	80	95	processing time
162	105	114	decreases
162	119	127	accuracy
163	0	4	With
163	9	27	match type feature
163	30	40	all models
163	41	51	other than
163	52	55	RMN
163	61	83	significantly improved
163	90	101	performance
163	102	112	except for
163	113	119	task 3
163	120	131	compared to
163	136	151	plain condition
171	29	32	RMN
171	33	39	yields
171	44	59	same error rate
171	60	66	25.1 %
171	67	71	with
171	72	92	MemN2N and GMe m N2N
