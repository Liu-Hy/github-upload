(Results||on||Stanford Question Answering Dataset ( SQuAD ))
(SQuAD dataset||consists of||more than 100,000 questions)
(more than 100,000 questions||annotated by||crowdsourcing workers)
(Results||has||SQuAD dataset)
(Our model||achieves||68.73 % EM score)
(Our model||achieves||77.39 % F1 score)
(77.39 % F1 score||ranked among||state of the art single models)
(Results||has||Our model)
(baseline model||using||no Q- code)
(no Q- code||achieved||68.00 % and 77.36 % EM and F 1 scores)
(explicit question type T - code||into||baseline model)
(improved slightly||to||68.16 % ( EM ))
(improved slightly||to||77.58 % ( F1 ))
(performance||has||improved slightly)
(Results||added||explicit question type T - code)
(TreeLSTM||introduce||syntactic parses)
(syntactic parses||for||question representation and understanding)
(TreeLSTM||has||syntactic parses)
(Results||used||TreeLSTM)
(78.38 % F1 score||on||whole development set)
(78.38 % F1 score||separated into||two parts)
(Contribution||has||Results)
