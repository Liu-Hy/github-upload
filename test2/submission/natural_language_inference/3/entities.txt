4	98	126	machine comprehension ( MC )
4	131	156	question answering ( QA )
13	41	59	modeling questions
13	60	62	in
13	71	110	end - to - end neural network framework
14	9	19	introduced
14	20	41	syntactic information
14	42	49	to help
14	57	66	questions
15	8	27	viewed and modelled
15	28	56	different types of questions
15	65	76	information
15	95	97	as
15	101	119	adaptation problem
15	124	132	proposed
15	133	150	adaptation models
163	19	21	on
163	22	67	Stanford Question Answering Dataset ( SQuAD )
164	4	17	SQuAD dataset
164	18	29	consists of
164	30	57	more than 100,000 questions
164	58	70	annotated by
164	71	92	crowdsourcing workers
164	93	95	on
168	3	6	use
168	7	45	pre-trained 300 - D Glove 840B vectors
168	46	59	to initialize
168	60	79	our word embeddings
169	0	35	Out - of - vocabulary ( OOV ) words
169	40	51	initialized
169	52	60	randomly
169	61	65	with
169	66	82	Gaussian samples
170	0	21	CharCNN filter length
170	22	24	is
170	25	34	1 , 3 , 5
170	45	58	50 dimensions
172	4	20	cluster number K
172	21	23	in
172	24	44	discriminative block
172	45	47	is
172	48	51	100
173	4	15	Adam method
173	19	27	used for
173	28	40	optimization
174	8	22	first momentum
174	26	35	set to be
174	36	39	0.9
174	48	54	second
174	55	60	0.999
175	4	25	initial learning rate
175	26	28	is
175	29	35	0.0004
175	44	54	batch size
175	55	57	is
175	58	60	32
178	4	17	hidden states
178	18	20	of
178	21	41	GRUs , and TreeLSTMs
178	42	45	are
178	46	60	500 dimensions
178	69	95	word - level embedding d w
178	96	98	is
178	99	113	300 dimensions
179	3	6	set
179	7	17	max length
179	18	20	of
179	21	29	document
179	30	32	to
179	33	36	500
179	43	47	drop
179	52	77	question - document pairs
179	78	84	beyond
179	90	92	on
179	93	105	training set
180	0	39	Explicit question - type dimension d ET
180	40	42	is
180	43	45	50
181	3	8	apply
181	9	16	dropout
181	17	19	to
181	24	59	Encoder layer and aggregation layer
181	60	64	with
181	67	79	dropout rate
181	80	82	of
181	83	86	0.5
192	0	9	Our model
192	10	18	achieves
192	21	37	68.73 % EM score
192	42	58	77.39 % F1 score
192	70	82	ranked among
192	87	117	state of the art single models
194	4	18	baseline model
194	19	24	using
194	25	35	no Q- code
194	36	44	achieved
194	47	84	68.00 % and 77.36 % EM and F 1 scores
195	8	13	added
195	18	49	explicit question type T - code
195	50	54	into
195	59	73	baseline model
195	80	91	performance
195	96	113	improved slightly
195	114	116	to
195	117	131	68.16 % ( EM )
195	136	150	77.58 % ( F1 )
196	8	12	used
196	13	21	TreeLSTM
196	22	31	introduce
196	32	48	syntactic parses
196	49	52	for
196	53	94	question representation and understanding
204	39	47	observed
204	50	66	78.38 % F1 score
204	67	69	on
204	74	95	whole development set
204	111	125	separated into
204	126	135	two parts
204	160	169	equals to
