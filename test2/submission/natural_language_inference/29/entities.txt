2	53	74	MACHINE COMPREHENSION
19	62	71	introduce
19	74	101	general framework PhaseCond
19	102	105	for
19	117	142	multiple attention layers
21	124	147	multi-hops architecture
21	159	181	alternatively captures
21	182	222	question - aware passage representations
21	227	234	refines
21	239	246	results
21	247	255	by using
21	258	280	self - attention model
24	98	133	question - passage attention models
24	134	137	for
24	183	192	calculate
24	197	213	alignment matrix
24	214	230	corresponding to
24	231	266	all question and passage word pairs
126	4	13	EM result
126	14	16	of
126	17	47	our baseline Iterative Aligner
126	48	50	is
126	51	61	lower than
126	62	66	RNET
127	29	33	RNET
127	34	38	uses
127	41	62	different feature set
127	127	151	different encoding steps
127	254	258	uses
127	261	286	different ensemble method
128	0	5	shows
128	10	21	performance
128	22	26	with
128	27	53	different number of layers
128	54	57	for
128	63	97	question - passage attention phase
128	102	124	self - attention phase
130	0	3	For
130	8	42	question - passage attention phase
130	45	50	using
130	51	63	single layer
130	64	80	does n't degrade
130	85	96	performance
130	97	110	significantly
130	111	115	from
130	120	135	default setting
130	136	138	of
130	139	149	two layers
132	14	38	multiple stacking layers
132	43	49	needed
132	50	58	to allow
132	63	71	evidence
132	72	96	fully propagated through
132	101	108	passage
