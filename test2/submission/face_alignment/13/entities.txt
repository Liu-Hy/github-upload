12	3	14	demonstrate
12	19	48	superior representation power
12	49	51	of
12	56	71	nonlinear 3 DMM
12	72	76	over
12	81	99	linear counterpart
12	110	122	contribution
12	123	125	to
12	126	162	face alignment and 3D reconstruction
45	11	18	utilize
45	19	39	two network decoders
45	42	52	instead of
45	53	67	two PCA spaces
45	70	72	as
45	77	111	shape and texture model components
46	50	56	design
46	57	75	different networks
46	76	79	for
46	80	97	shape and texture
46	104	136	multi - layer perceptron ( MLP )
46	137	140	for
46	141	146	shape
46	151	187	convolutional neural network ( CNN )
46	188	191	for
46	192	199	texture
47	0	12	Each decoder
47	18	22	take
47	25	56	shape or texture representation
47	57	59	as
47	60	65	input
47	70	76	output
47	101	113	face texture
49	13	18	learn
49	23	40	fitting algorithm
49	41	43	to
49	48	63	nonlinear 3 DMM
49	75	88	formulated as
49	91	102	CNN encoder
50	4	11	encoder
50	12	17	takes
50	20	34	2 D face image
50	35	37	as
50	38	43	input
50	48	57	generates
50	62	90	shape and texture parameters
50	104	116	two decoders
50	117	125	estimate
50	130	149	3D face and texture
51	4	24	3 D face and texture
51	31	52	perfectly reconstruct
51	57	67	input face
51	70	72	if
51	109	120	well learnt
52	15	21	design
52	24	54	differentiable rendering layer
52	55	66	to generate
52	69	87	reconstructed face
52	88	97	by fusing
52	102	158	3D face , texture , and the camera projection parameters
52	159	171	estimated by
52	176	183	encoder
53	14	41	endto - end learning scheme
53	45	56	constructed
53	57	62	where
53	67	91	encoder and two decoders
53	96	102	learnt
53	103	110	jointly
53	111	122	to minimize
53	127	137	difference
53	138	145	between
53	150	187	reconstructed face and the input face
54	0	16	Jointly learning
54	21	56	3 DMM and the model fitting encoder
54	70	78	leverage
54	83	126	large collection of unconstrained 2D images
54	127	145	without relying on
54	146	154	3D scans
55	3	7	show
55	8	30	significantly improved
55	31	69	shape and texture representation power
55	70	74	over
55	79	91	linear 3 DMM
208	13	28	optimized using
208	29	43	Adam optimizer
208	44	48	with
208	52	73	initial learning rate
208	74	76	of
208	77	82	0.001
208	83	98	when minimizing
208	99	102	L 0
208	109	115	0.0002
208	116	131	when minimizing
208	132	134	L.
209	3	6	set
209	34	35	Q
209	38	46	53 , 215
209	49	60	U = V = 128
209	63	78	l S = l T = 160
209	106	112	losses
209	113	120	to have
209	121	139	similar magnitudes
226	0	20	Representation Power
230	23	31	minimize
230	36	56	reconstruction error
230	57	59	in
230	64	75	image space
230	78	85	through
230	90	105	rendering layer
230	106	110	with
230	115	134	groundtruth S and m
233	12	29	nonlinear texture
233	33	42	closer to
233	47	58	groundtruth
233	59	63	than
233	68	80	linear model
233	83	97	especially for
233	98	120	in - the - wild images
235	21	36	nonlinear model
235	41	85	significantly lower L 1 reconstruction error
235	86	90	than
235	95	98	lin
235	109	116	compare
235	157	172	in representing
235	173	194	real - world 3D scans
247	4	19	nonlinear model
247	26	68	significantly smaller reconstruction error
247	69	73	than
247	78	90	linear model
247	93	110	0.0196 vs. 0.0241
264	0	44	Quantitative evaluation of 3D reconstruction
265	3	9	obtain
265	12	21	low error
265	30	43	comparable to
265	44	72	optimization - based methods
271	0	22	3D Face Reconstruction
280	3	10	achieve
280	11	27	on - par results
280	28	32	with
280	33	47	Garrido et al.
280	53	80	offline optimization method
280	89	99	surpassing
280	100	128	all other regression methods
284	0	5	Using
284	8	42	global image - based discriminator
284	43	45	is
284	46	55	redundant
284	63	79	global structure
284	83	96	guaranteed by
284	101	116	rendering layer
285	32	37	using
285	38	72	global image - based discriminator
285	83	99	severe artifacts
285	100	102	in
285	107	124	resultant texture
287	10	18	patchGAN
287	19	25	offers
287	26	40	higher realism
287	45	60	fewer artifacts
