(DeCaFA models||use||1 to 4 stages)
(12 3 3 convolutional layers||with||64 ? 64 ? 128 ? 128 ? 256 ? 256 channels)
(64 ? 64 ? 128 ? 128 ? 256 ? 256 channels||for||downsampling portion)
(Hyperparameters||has||DeCaFA models)
(input images||resized to||128 128 grayscale images)
(128 128 grayscale images||processed by||network)
(Hyperparameters||has||input images)
(Each convolution||followed by||batch normalization layer)
(batch normalization layer||with||ReLU activation)
(Hyperparameters||has||Each convolution)
(smooth feature maps||do not use||transposed convolution)
(bilinear image upsampling||followed with||3 3 convolutional layers)
(Hyperparameters||to generate||smooth feature maps)
(whole architecture||trained using||ADAM optimizer)
(ADAM optimizer||with||5e ? 4 learning rate)
(ADAM optimizer||with||learning rate)
(ADAM optimizer||with||learning rate annealing)
(5e ? 4 learning rate||with||momentum 0.9)
(learning rate annealing||with||power 0.9)
(annealing||with||power 0.9)
(5e ? 4 learning rate||with||momentum 0.9)
(5e ? 4 learning rate||with||learning rate)
(5e ? 4 learning rate||with||learning rate annealing)
(learning rate annealing||with||power 0.9)
(annealing||with||power 0.9)
(learning rate||has||learning rate annealing)
(learning rate annealing||has||annealing)
(Hyperparameters||has||whole architecture)
(400000 updates||with||batch size)
(400000 updates||with||batch size 8)
(400000 updates||with||alternating updates)
(batch size||for||each database)
(batch size 8||for||each database)
(8||for||each database)
(400000 updates||with||alternating updates)
(alternating updates||between||databases)
(batch size||has||batch size 8)
(Hyperparameters||apply||400000 updates)
(Contribution||has||Hyperparameters)
