2	67	106	End - to - End Co -reference Resolution
4	18	25	present
4	28	48	word embedding model
4	61	88	cross - sentence dependency
4	89	102	for improving
4	103	154	end - to - end co-reference resolution ( E2E - CR )
5	37	46	generates
5	47	67	word representations
5	68	78	by running
5	139	141	on
5	142	155	each sentence
5	156	158	of
5	208	215	propose
5	244	279	attentional sentence linking models
5	280	288	to learn
5	289	313	crosssentence dependency
10	0	23	Co-reference resolution
24	267	274	propose
24	277	301	cross - sentence encoder
24	302	305	for
24	306	346	end - to - end co-reference ( E2E - CR )
25	58	79	external memory block
25	80	90	containing
25	91	125	syntactic and semantic information
25	126	130	from
25	131	148	context sentences
25	152	160	added to
25	165	184	standard LSTM model
26	0	4	With
26	10	30	context memory block
26	37	51	proposed model
26	55	62	able to
26	63	69	encode
26	70	85	input sentences
26	86	88	as
26	91	96	batch
26	108	117	calculate
26	122	137	representations
26	138	140	of
26	141	152	input words
26	153	162	by taking
26	168	206	target sentences and context sentences
26	207	225	into consideration
87	18	30	LSTM modules
87	31	41	applied in
87	52	56	have
87	57	73	200 output units
88	3	6	ASL
88	12	21	calculate
88	22	49	cross - sentence dependency
88	50	55	using
88	58	79	multilayer perceptron
88	80	84	with
88	85	101	one hidden layer
88	102	115	consisting of
88	116	132	150 hidden units
89	4	25	initial learning rate
89	29	35	set as
89	36	41	0.001
89	46	52	decays
89	53	60	0.001 %
89	61	66	every
89	67	76	100 steps
90	4	9	model
90	13	27	optimized with
90	32	46	Adam algorithm
91	3	18	randomly select
91	19	48	up to 40 continuous sentences
91	49	52	for
91	53	61	training
91	62	64	if
91	69	74	input
91	75	77	is
91	78	86	too long
97	0	14	Comparing with
97	19	33	baseline model
97	48	63	67.2 % F1 score
97	70	79	ASL model
97	80	88	improved
97	93	104	performance
97	105	107	by
97	108	113	0.6 %
97	118	126	achieved
97	127	144	67.8 % average F1
108	0	4	show
108	14	20	models
108	26	34	consider
108	35	62	cross - sentence dependency
108	63	87	significantly outperform
108	92	106	baseline model
108	115	122	encodes
108	123	136	each sentence
108	137	141	from
108	146	160	input document
109	17	26	indicated
109	36	45	ASL model
109	50	68	better performance
109	69	73	than
109	78	87	LSL model
