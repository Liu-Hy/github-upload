2	29	51	Coreference Resolution
4	34	56	coreference prediction
4	63	75	benefit from
4	76	84	modeling
4	85	103	global information
4	104	109	about
4	110	127	entity - clusters
12	18	23	posit
12	29	43	global context
12	54	67	necessary for
12	68	88	further improvements
12	89	91	in
12	92	114	coreference resolution
13	42	57	representations
13	58	60	of
13	61	77	mention clusters
13	78	95	by embedding them
13	96	108	sequentially
13	109	114	using
13	117	141	recurrent neural network
14	14	16	no
14	17	50	manually defined cluster features
14	65	71	learns
14	74	95	global representation
14	96	100	from
14	105	124	individual mentions
14	125	135	present in
14	136	148	each cluster
15	3	14	incorporate
15	37	41	into
15	44	86	mention - ranking style coreference system
16	19	28	including
16	33	57	recurrent neural network
16	66	94	mention - ranking sub-system
16	100	107	trained
16	108	122	end - to - end
16	123	125	on
16	130	146	coreference task
17	3	8	train
17	13	18	model
17	19	21	as
17	24	40	local classifier
17	41	45	with
17	46	59	fixed context
181	0	3	For
181	4	12	training
181	18	21	use
181	22	49	document - size minibatches
181	58	68	allows for
181	69	94	efficient pre-computation
181	95	97	of
181	98	108	RNN states
181	118	126	minimize
181	131	135	loss
181	159	163	with
181	164	171	AdaGrad
181	174	188	after clipping
181	189	203	LSTM gradients
182	3	7	find
182	17	38	initial learning rate
182	39	49	chosen for
182	50	57	AdaGrad
182	64	82	significant impact
182	83	85	on
182	86	93	results
182	103	109	choose
182	110	124	learning rates
182	125	128	for
182	129	139	each layer
182	140	146	out of
182	147	184	{ 0.1 , 0.02 , 0.01 , 0.002 , 0.001 }
183	20	23	set
183	24	62	ha ( x n ) , h c ( x n ) , and h ( m )
183	63	68	to be
183	69	76	? R 200
183	83	96	hp ( x n , y)
184	3	6	use
184	9	28	single - layer LSTM
184	31	38	without
184	74	88	implemented in
184	93	114	element - rnn library
185	0	3	For
185	4	18	regularization
185	24	29	apply
185	30	37	Dropout
185	38	42	with
185	45	49	rate
185	50	52	of
185	53	56	0.4
185	57	72	before applying
185	77	93	linear weights u
185	108	113	apply
185	114	121	Dropout
185	122	126	with
185	129	133	rate
185	134	136	of
185	137	140	0.3
185	141	143	to
185	148	159	LSTM states
185	160	174	before forming
185	179	198	dot -product scores
189	36	74	https : //github.com/swiseman/nn_coref
190	11	23	makes use of
190	26	29	GPU
190	30	33	for
190	34	42	training
190	49	58	trains in
190	59	74	about two hours
193	3	6	see
193	9	46	statistically significant improvement
193	47	49	of
193	50	72	over 0.8 Co NLL points
193	73	77	over
193	82	107	previous state of the art
203	19	22	RNN
203	23	31	improves
203	32	43	performance
203	44	52	over all
203	55	59	with
203	64	93	most dramatic improve - ments
203	94	96	on
203	97	119	non-anaphoric pronouns
205	18	33	RNN performance
205	34	36	is
205	37	57	significantly better
205	58	62	than
205	75	87	Avg baseline
205	96	116	barely improves over
205	117	134	mention - ranking
205	147	161	oracle history
