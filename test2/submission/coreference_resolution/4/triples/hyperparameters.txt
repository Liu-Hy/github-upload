(Glo Ve word embeddings||pre-trained on||Gigaword and Wikipedia)
(Hyperparameters||has||Glo Ve word embeddings)
(Vocabulary||built from||words)
(words||in||training data)
(frequency||in||{ 3 , U ( 1 , 10 ) })
(words||with||frequency)
(training data||with||frequency)
(frequency||in||{ 3 , U ( 1 , 10 ) })
(OOV words||replaced with||UNK token)
(Hyperparameters||has||Vocabulary)
(size||of||LSTMs hidden states)
(LSTMs hidden states||set to||{ 100 , qlog - U ( 30 , 150 ) })
(Hyperparameters||has||size)
(weight matrices||of||LSTMs)
(LSTMs||with||random orthogonal matrices)
(Hyperparameters||initialized||weight matrices)
(first feed - forward layer size||set to||value)
(value||in||Optimization)
(first feed - forward layer size||has||value)
(Hyperparameters||has||first feed - forward layer size)
(our model||in||minibatches)
(minibatches||using||Adam ( Kingma and Ba , 2015 ))
(Adam ( Kingma and Ba , 2015 )||with||learning rate)
(Adam ( Kingma and Ba , 2015 )||with||maximal batch size)
(learning rate||of||10 ? 4)
(learning rate||has||10 ? 4)
(maximal batch size||has||64)
(Hyperparameters||trained||our model)
(gradients||by||global norm)
(global norm||with||clipping value)
(clipping value||in||{ 1.0 , U ( 1 , 100 ) })
(Hyperparameters||clip||gradients)
(model||performs||best)
(best||on||devset)
(Hyperparameters||train for||10 epochs)
(Hyperparameters||used||l 2 - regularization)
(input||with||k p ? U (0.8 , 1.0 ))
(outputs||of||LSTMs)
(input||with||k p ? U (0.8 , 1.0 ))
(Hyperparameters||has||Dropout)
(Contribution||has||Hyperparameters)
