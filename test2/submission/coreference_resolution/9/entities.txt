2	9	31	Coreference Resolution
11	3	14	fine - tune
11	15	19	BERT
11	20	22	to
11	23	45	coreference resolution
11	48	57	achieving
11	58	77	strong improvements
12	3	10	present
12	11	19	two ways
12	23	32	extending
12	37	55	c 2f - coref model
16	2	40	https://github.com/mandarjoshi90/coref
63	3	9	extend
63	14	49	original Tensorflow implementations
63	50	52	of
63	53	65	c 2f - coref
63	72	76	BERT
64	3	12	fine tune
64	13	23	all models
64	24	26	on
64	31	53	OntoNotes English data
64	54	57	for
64	58	67	20 epochs
64	68	73	using
64	76	83	dropout
64	84	86	of
64	87	90	0.3
64	97	111	learning rates
64	112	114	of
64	115	135	1 10 ?5 and 2 10 ? 4
64	136	140	with
64	141	153	linear decay
64	154	157	for
64	162	201	BERT parameters and the task parameters
66	3	10	trained
66	11	26	separate models
66	27	31	with
66	32	47	max segment len
66	48	50	of
66	51	76	128 , 256 , 384 , and 512
66	83	89	models
66	90	100	trained on
66	101	124	128 and 384 word pieces
66	125	134	performed
66	139	143	best
66	144	147	for
66	148	176	BERT - base and BERT - large
70	0	21	Paragraph Level : GAP
76	8	13	shows
76	19	23	BERT
76	24	32	improves
76	33	46	c 2 f - coref
76	47	49	by
76	50	64	9 % and 11.5 %
76	65	68	for
76	73	94	base and large models
78	0	26	Document Level : OntoNotes
83	0	5	shows
83	11	22	BERT - base
83	23	29	offers
83	33	44	improvement
83	45	47	of
83	48	53	0.9 %
83	54	58	over
83	63	91	ELMo - based c2 fcoref model
87	0	12	BERT - large
87	25	33	improves
87	34	47	c 2 f - coref
87	48	50	by
87	55	73	much larger margin
87	74	76	of
87	77	82	3.9 %
88	8	15	observe
88	25	40	overlap variant
88	41	47	offers
88	48	62	no improvement
88	63	67	over
88	68	79	independent
