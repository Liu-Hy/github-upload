Unnamed: 0,Unnamed: 0.1,Unnamed: 0.1.1,idx,text,main_heading,heading,topic,paper_idx,BIO,BIO_1,BIO_2,offset1,pro1,offset2,pro2,offset3,pro3,mask,labels,title,paper
0,0,0,2,Recurrent Neural Network Grammars,title,title,constituency_parsing,0,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",1,0,1,0.004484305,1,0,1,research-problem,title,constituency_parsing0
1,1,1,12,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .",Introduction,Introduction,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 12), (21, 22), (22, 23), (26, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.043478261,11,0.049327354,4,0.2,1,model,Introduction,constituency_parsing0
2,2,2,13,"RNNGs operate via a recursive syntactic process reminiscent of probabilistic context - free grammar generation , but decisions are parameterized using RNNs that condition on the entire syntactic derivation history , greatly relaxing context - free independence assumptions .",Introduction,Introduction,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 7), (9, 15), (17, 18), (19, 21), (21, 22), (23, 25), (26, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.054347826,12,0.053811659,5,0.25,1,model,Introduction,constituency_parsing0
3,3,3,15,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .",Introduction,Introduction,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 7), (9, 10), (10, 11), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.076086957,14,0.062780269,7,0.35,1,model,Introduction,constituency_parsing0
4,4,4,24,"The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences , and this is used to solve a second practical challenge with RNNGs : approximating the marginal likelihood and MAP tree of a sentence under the generative model .",Introduction,Refer to for an example .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (7, 9), (9, 11), (11, 12), (12, 13), (13, 15), (15, 16), (16, 17), (29, 30), (31, 32), (33, 38), (41, 42), (43, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.173913043,23,0.103139013,16,0.8,1,model,Introduction: Refer to for an example .,constituency_parsing0
5,5,5,25,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,Introduction,Refer to for an example .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (8, 9), (9, 10), (10, 11), (12, 14), (14, 16), (16, 18), (18, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.184782609,24,0.107623318,17,0.85,1,model,Introduction: Refer to for an example .,constituency_parsing0
6,6,6,160,"For the discriminative model , we used hidden dimensions of 128 and 2 - layer LSTMs ( larger numbers of dimensions reduced validation set performance ) .",Experiments,Model and training parameters .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (7, 9), (9, 10), (10, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.375,159,0.713004484,3,0.1875,1,hyperparameters,Experiments: Model and training parameters .,constituency_parsing0
7,7,7,161,"For the generative model , we used 256 dimensions and 2 layer LSTMs .",Experiments,Model and training parameters .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (7, 9), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5,160,0.717488789,4,0.25,1,hyperparameters,Experiments: Model and training parameters .,constituency_parsing0
8,8,8,162,"For both models , we tuned the dropout rate to maximize validation set likelihood , obtaining optimal rates of 0.2 ( discriminative ) and 0.3 ( generative ) .",Experiments,Model and training parameters .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 11), (11, 14), (15, 16), (16, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,161,0.721973094,5,0.3125,1,hyperparameters,Experiments: Model and training parameters .,constituency_parsing0
9,9,9,163,"For the sequential LSTM baseline for the language model , we also found an optimal dropout rate of 0.3 .",Experiments,Model and training parameters .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (5, 6), (7, 9), (12, 13), (14, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.75,162,0.726457399,6,0.375,1,hyperparameters,Experiments: Model and training parameters .,constituency_parsing0
10,10,10,164,For training we used stochastic gradient descent with a learning rate of 0.1 .,Experiments,Model and training parameters .,constituency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 4), (4, 7), (7, 8), (9, 11), (11, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.875,163,0.730941704,7,0.4375,1,hyperparameters,Experiments: Model and training parameters .,constituency_parsing0
11,11,11,2,Cloze - driven Pretraining of Self - attention Networks,title,,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004901961,1,0,1,research-problem,title,constituency_parsing1
12,12,12,4,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .,abstract,abstract,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 11), (12, 13), (13, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.014705882,1,0.25,1,research-problem,abstract,constituency_parsing1
13,13,13,9,Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems .,Introduction,Introduction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.058823529,8,0.039215686,1,0.058823529,1,research-problem,Introduction,constituency_parsing1
14,14,14,11,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .",Introduction,Introduction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (12, 14), (14, 16), (16, 18), (18, 19), (20, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.176470588,10,0.049019608,3,0.176470588,1,model,Introduction,constituency_parsing1
15,15,15,12,Our bi-directional transformer architecture predicts every token in the training data ( ) .,Introduction,Introduction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 7), (7, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.235294118,11,0.053921569,4,0.235294118,1,model,Introduction,constituency_parsing1
16,16,16,13,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,Introduction,Introduction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 11), (11, 12), (13, 14), (17, 19), (19, 20), (20, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.294117647,12,0.058823529,5,0.294117647,1,model,Introduction,constituency_parsing1
17,17,17,14,Our model separately computes both forward and backward states with * Equal contribution . :,Introduction,Introduction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.352941176,13,0.06372549,6,0.352941176,1,model,Introduction,constituency_parsing1
18,18,18,110,Experimental setup,,,constituency_parsing,1,"['O', 'O']",[],"['O', 'O']",0,0,109,0.534313725,0,0,1,experiments,,constituency_parsing1
19,19,19,128,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .",Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 6), (6, 7), (8, 9), (11, 12), (12, 13), (14, 18), (18, 19), (19, 20), (20, 21), (22, 24), (25, 28), (28, 29), (29, 30), (30, 31), (32, 33), (34, 35), (35, 36), (36, 37), (39, 40), (40, 41), (41, 42), (43, 44), (44, 45), (45, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.454545455,127,0.62254902,17,0.739130435,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
20,20,20,129,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (7, 8), (8, 13), (13, 14), (14, 17), (19, 21), (22, 26), (26, 27), (28, 30), (30, 31), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.545454545,128,0.62745098,18,0.782608696,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
21,21,21,130,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 8), (8, 9), (9, 13), (16, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,129,0.632352941,19,0.826086957,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
22,22,22,131,We also use the NCCL2 library and the torch .,Pretraining hyper -parameters,Pretraining hyper -parameters,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.727272727,130,0.637254902,20,0.869565217,1,experimental-setup,Pretraining hyper -parameters,constituency_parsing1
23,23,23,135,Results,,,constituency_parsing,1,['O'],[],['O'],0,0,134,0.656862745,0,0,1,experiments,,constituency_parsing1
24,24,24,136,GLUE,Results,,constituency_parsing,1,['O'],"[(0, 1)]",['O'],1,0.026315789,135,0.661764706,0,0,1,experiments,Results,constituency_parsing1
25,25,25,151,"All our models outperform the uni-directional transformer ( OpenAI GPT ) of , however , our model is about 50 % larger than their model .",Results,More information can be found in .,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.421052632,150,0.735294118,15,0.681818182,1,results,Results: More information can be found in .,constituency_parsing1
26,26,26,153,"Our CNN base model performs as well as STILTs in aggregate , however , on some tasks involving sentence - pairs , STILTs performs much better ( MRPC , RTE ) ; there is a similar trend for BERT .",Results,More information can be found in .,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (5, 8), (8, 9), (9, 10), (10, 11), (14, 15), (22, 23), (23, 24), (24, 26), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.473684211,152,0.745098039,17,0.772727273,1,results,Results: More information can be found in .,constituency_parsing1
27,27,27,163,Named Entity Recognition,Results,Structured Prediction,constituency_parsing,1,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",28,0.736842105,162,0.794117647,0,0,1,results,Results: Structured Prediction,constituency_parsing1
28,28,28,167,"shows the results , with comparison to previous published ELMo BASE results the art , but fine tuning gives the biggest gain .",Results,Structured Prediction,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (16, 18), (18, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.842105263,166,0.81372549,4,1,1,results,Results: Structured Prediction,constituency_parsing1
29,29,29,168,Constituency Parsing,Results,,constituency_parsing,1,"['O', 'O']","[(0, 2)]","['O', 'O']",33,0.868421053,167,0.818627451,0,0,1,results,Results,constituency_parsing1
30,30,30,180,This results in the bilm loss dominating the triplet loss and we found that scaling the bilm term by a factor of 0.15 results in better performance .,Objective functions for pretraining,Objective functions for pretraining,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (8, 10), (14, 15), (16, 18), (18, 19), (20, 21), (21, 22), (22, 23), (23, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.260869565,179,0.87745098,6,0.75,1,ablation-analysis,Objective functions for pretraining,constituency_parsing1
31,31,31,181,shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself .,Objective functions for pretraining,Objective functions for pretraining,constituency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 5), (5, 6), (6, 8), (8, 9), (10, 12), (14, 15), (16, 19), (22, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.304347826,180,0.882352941,7,0.875,1,results,Objective functions for pretraining,constituency_parsing1
32,32,32,2,An Empirical Study of Building a Strong Baseline for Constituency Parsing,title,title,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.007407407,1,0,1,research-problem,title,constituency_parsing2
33,33,33,13,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,Introduction,Introduction,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 8), (10, 11), (12, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.092105263,12,0.088888889,7,0.583333333,1,model,Introduction,constituency_parsing2
34,34,34,111,( 1 ) Smaller mini-batch size M and gradient clipping G provided the better performance .,Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7), (8, 11), (11, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.487179487,110,0.814814815,19,0.59375,1,results,Experiments,constituency_parsing2
35,35,35,113,"( 2 ) Larger layer size , hidden state dimension , and beam size have little impact on the performance ; our setting , L = 2 , H = 200 , and B = 5 looks adequate in terms of speed / performance trade - off .",Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 14), (14, 15), (15, 17), (17, 18), (19, 20), (24, 36), (36, 37), (37, 38), (38, 41), (41, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.538461538,112,0.82962963,21,0.65625,1,results,Experiments,constituency_parsing2
36,36,36,115,"As often demonstrated in the NMT literature , using subword split as input token unit instead of standard tokenized word unit has potential to improve the performance .",Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 12), (12, 15), (15, 17), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.58974359,114,0.844444444,23,0.71875,1,results,Experiments,constituency_parsing2
37,37,37,119,"Thus , using subword information as features is one promising approach for leveraging subword information into constituency parsing .",Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 6), (6, 7), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.692307692,118,0.874074074,27,0.84375,1,baselines,Experiments,constituency_parsing2
38,38,38,126,Our Seq2seq approach successfully achieved the competitive level as the current top - notch methods : RNNG and its variants .,Experiments,Experiments,constituency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (6, 8), (8, 9), (10, 15), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.871794872,125,0.925925926,1,0.166666667,1,results,Experiments,constituency_parsing2
39,39,39,4,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .,abstract,abstract,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.014285714,1,0.166666667,1,research-problem,abstract,constituency_parsing3
40,40,40,21,"We found this model to work poorly when we trained it on standard human - annotated parsing datasets ( 1M tokens ) , so we constructed an artificial dataset by labelling a large corpus with the BerkeleyParser .",Introduction,"Recently , Sutskever et al .",constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (25, 26), (27, 29), (29, 31), (32, 34), (34, 35), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.157142857,20,0.095238095,11,0.323529412,1,model,"Introduction: Recently , Sutskever et al .",constituency_parsing3
41,41,41,25,"We trained a sequence - to - sequence model with attention on the small human - annotated parsing dataset and were able to achieve an F 1 score of 88.3 on section 23 of the WSJ without the use of an ensemble and 90.5 with an ensemble , which matches the performance of the BerkeleyParser ( 90.4 ) when trained on the same data .",Introduction,"Recently , Sutskever et al .",constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (9, 10), (10, 11), (11, 12), (13, 19), (21, 24), (25, 28), (28, 29), (29, 30), (30, 31), (35, 36), (36, 37), (41, 42), (43, 44), (44, 45), (46, 47), (49, 50), (51, 52)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.214285714,24,0.114285714,15,0.441176471,1,experiments,"Introduction: Recently , Sutskever et al .",constituency_parsing3
42,42,42,26,"Finally , we constructed a second artificial dataset consisting of only high - confidence parse trees , as measured by the agreement of two parsers .",Introduction,"Recently , Sutskever et al .",constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 10), (11, 16), (18, 20), (21, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.228571429,25,0.119047619,16,0.470588235,1,experiments,"Introduction: Recently , Sutskever et al .",constituency_parsing3
43,43,43,27,We trained a sequence - to - sequence model with attention on this data and achieved an F 1 score of 92.5 on section 23 of the WSJ - a new state - of - the - art .,Introduction,"Recently , Sutskever et al .",constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (9, 10), (10, 11), (15, 16), (17, 20), (20, 21), (21, 22), (22, 23), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.242857143,26,0.123809524,17,0.5,1,experiments,"Introduction: Recently , Sutskever et al .",constituency_parsing3
44,44,44,67,"In our experiments we used a model with 3 LSTM layers and 256 units in each layer , which we call LSTM + A .",Introduction,Sizes .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 7), (7, 8), (8, 11), (12, 14), (14, 15), (15, 17), (20, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.814285714,66,0.314285714,2,0.08,1,model,Introduction: Sizes .,constituency_parsing3
45,45,45,70,"Training on a small dataset we additionally used 2 dropout layers , one between LSTM 1 and LSTM 2 , and one between LSTM 2 and LSTM 3 .",Introduction,Dropout .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (8, 11), (13, 14), (14, 19), (22, 23), (23, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.857142857,69,0.328571429,5,0.2,1,hyperparameters,Introduction: Dropout .,constituency_parsing3
46,46,46,82,The embedding layer for our 90K vocabulary can be initialized randomly or using pre-trained word - vector embeddings .,Pre-training word vectors .,Pre-training word vectors .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (7, 9), (9, 11), (12, 13), (13, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.111111111,81,0.385714286,17,0.68,1,hyperparameters,Pre-training word vectors .,constituency_parsing3
47,47,47,83,We pre-trained skip - gram embeddings of size 512 using word2vec [ 6 ] on a 10B - word corpus .,Pre-training word vectors .,Pre-training word vectors .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 8), (8, 9), (9, 10), (10, 11), (14, 15), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.222222222,82,0.39047619,18,0.72,1,hyperparameters,Pre-training word vectors .,constituency_parsing3
48,48,48,118,But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,Evaluation,Evaluation,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (5, 7), (7, 8), (10, 17), (17, 18), (18, 19), (19, 20), (21, 25), (25, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.264705882,117,0.557142857,9,0.157894737,1,results,Evaluation,constituency_parsing3
49,49,49,119,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .",Evaluation,Evaluation,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 9), (11, 16), (16, 17), (17, 18), (20, 21), (24, 27), (31, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.294117647,118,0.561904762,10,0.175438596,1,results,Evaluation,constituency_parsing3
50,50,50,120,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,Evaluation,Evaluation,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 7), (8, 9), (10, 11), (11, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.323529412,119,0.566666667,11,0.192982456,1,results,Evaluation,constituency_parsing3
51,51,51,122,"The LSTM + A model trained on WSJ dataset only produced malformed trees for 25 of the 1700 sentences in our development set ( 1.5 % of all cases ) , and the model trained on full high - confidence dataset did this for 14 sentences ( 0.8 % ) .",Evaluation,Generating well - formed trees .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 7), (7, 9), (10, 11), (11, 13), (13, 14), (14, 19), (19, 20), (20, 23), (34, 36), (36, 41), (43, 44), (44, 50)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.382352941,121,0.576190476,13,0.228070175,1,results,Evaluation: Generating well - formed trees .,constituency_parsing3
52,52,52,130,"The difference between the F 1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser , 1.7 for the baseline LSTM , and 0.7 for LSTM + A .",Evaluation,"The results , presented in , are surprising .",constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 7), (7, 8), (8, 9), (9, 11), (17, 18), (18, 19), (19, 20), (21, 22), (23, 24), (24, 25), (26, 28), (30, 31), (31, 32), (32, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.617647059,129,0.614285714,21,0.368421053,1,results,"Evaluation: The results , presented in , are surprising .",constituency_parsing3
53,53,53,157,LSTM + A trained on the high - confidence corpus ( which only includes text from news ) achieved an F 1 score of 95.7 on QTB and 84.6 on WEB .,Pre-training influence .,Performance on other datasets .,constituency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 5), (6, 10), (18, 19), (20, 23), (23, 24), (24, 25), (25, 26), (26, 27), (28, 29), (29, 30), (30, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.382352941,156,0.742857143,48,0.842105263,1,results,Pre-training influence .: Performance on other datasets .,constituency_parsing3
54,54,54,4,Recent work has proposed several generative neural models for constituency parsing that achieve state - of - the - art results .,abstract,abstract,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.020833333,1,0.2,1,research-problem,abstract,constituency_parsing4
55,55,55,10,Recent work on neural constituency parsing has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler .,Introduction,Introduction,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.076923077,9,0.0625,1,0.028571429,1,research-problem,Introduction,constituency_parsing4
56,56,56,35,"In this paper , we present experiments to isolate the degree to which each gain occurs for each of two state - of - the - art generative neural parsing models : the Recurrent Neural Network Grammar generative parser ( RG ) of , and the LSTM language modeling generative parser ( LM ) of .",Perhaps generative models,A as a reranking gain .,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(19, 31), (33, 42), (46, 54)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.226415094,34,0.236111111,26,0.742857143,1,ablation-analysis,Perhaps generative models: A as a reranking gain .,constituency_parsing4
57,57,57,36,"In particular , we present and use a beam - based search procedure with an augmented state space that can search directly in the generative models , allowing us to explore A ?",Perhaps generative models,A as a reranking gain .,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 7), (8, 13), (13, 14), (15, 18), (20, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.245283019,35,0.243055556,27,0.771428571,1,approach,Perhaps generative models: A as a reranking gain .,constituency_parsing4
58,58,58,43,"We find that this is indeed the case : simply taking a weighted average of the scores of both models when selecting a parse from the base parser 's candidate list improves over using only the score of the generative model , in many cases substantially ( Section 3.2 ) .",Perhaps generative models,A as a reranking gain .,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 14), (14, 15), (16, 17), (17, 18), (18, 20), (20, 22), (23, 24), (24, 25), (26, 31), (31, 32), (36, 37), (37, 38), (39, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.377358491,42,0.291666667,34,0.971428571,1,results,Perhaps generative models: A as a reranking gain .,constituency_parsing4
59,59,59,90,Augmenting the candidate set,Experiments,,constituency_parsing,4,"['O', 'O', 'O', 'O']","[(0, 1), (2, 4)]","['O', 'O', 'O', 'O']",13,0.317073171,89,0.618055556,0,0,1,experiments,Experiments,constituency_parsing4
60,60,60,98,RG decreases performance from 93.45 F1 to 92.78 F1 on the development set .,Experiments,Augmenting the candidate set,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 3), (3, 4), (4, 6), (6, 7), (7, 9), (9, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.512195122,97,0.673611111,8,0.5,1,ablation-analysis,Experiments: Augmenting the candidate set,constituency_parsing4
61,61,61,107,Score combination,Experiments,,constituency_parsing,4,"['O', 'O']","[(0, 2)]","['O', 'O']",30,0.731707317,106,0.736111111,0,0,1,experiments,Experiments,constituency_parsing4
62,62,62,111,"We find that combining the scores of both models improves on using the score of either model alone , regardless of the source of candidates .",Experiments,Score combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 6), (6, 7), (7, 9), (9, 11), (11, 12), (13, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.829268293,110,0.763888889,4,0.363636364,1,results,Experiments: Score combination,constituency_parsing4
63,63,63,119,Strengthening model combination,,,constituency_parsing,4,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,118,0.819444444,0,0,1,experiments,,constituency_parsing4
64,64,64,123,"2 Combining candidates and scores from all three models ( row 9 ) , we obtain 93.94 F1 . :",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 9), (15, 16), (16, 19), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.16,122,0.847222222,4,0.235294118,1,ablation-analysis,Strengthening model combination,constituency_parsing4
65,65,65,131,"Ensembling Finally , we compare to another commonly used model combination method : ensembling multiple instances of the same model type trained from different random initializations .",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (4, 6), (13, 14), (14, 16), (18, 21), (21, 23), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.48,130,0.902777778,12,0.705882353,1,baselines,Strengthening model combination,constituency_parsing4
66,66,66,134,"Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 8), (12, 13), (13, 14), (14, 15), (15, 16), (17, 20), (20, 21), (21, 23), (24, 26), (28, 31), (36, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6,133,0.923611111,15,0.882352941,1,ablation-analysis,Strengthening model combination,constituency_parsing4
67,67,67,135,"In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .",Strengthening model combination,Strengthening model combination,constituency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 6), (6, 7), (7, 9), (9, 10), (11, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.64,134,0.930555556,16,0.941176471,1,results,Strengthening model combination,constituency_parsing4
68,68,68,2,In- Order Transition - based Constituent Parsing,title,title,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004694836,1,0,1,research-problem,title,constituency_parsing5
69,69,69,4,Both bottom - up and top - down strategies have been used for neural transition - based constituent parsing .,abstract,abstract,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.014084507,1,0.166666667,1,research-problem,abstract,constituency_parsing5
70,70,70,32,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .",Introduction,Introduction,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 11), (11, 13), (14, 15), (31, 36), (37, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.488888889,31,0.145539906,22,0.285714286,1,model,Introduction,constituency_parsing5
71,71,71,49,We release our code at https://github.com/LeonCrashCode/InOrderParser .,Introduction,,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.866666667,48,0.225352113,39,0.506493506,1,code,Introduction,constituency_parsing5
72,72,72,131,Reranking experiments,,,constituency_parsing,5,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,130,0.610328638,0,0,1,experiments,,constituency_parsing5
73,73,73,133,The bottom - up system performs slightly better than the top - down system .,Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 8), (8, 9), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,132,0.61971831,2,0.2,1,results,Reranking experiments,constituency_parsing5
74,74,74,134,The inorder system outperforms both the bottom - up and the top - down system .,Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (6, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,133,0.624413146,3,0.3,1,results,Reranking experiments,constituency_parsing5
75,75,75,136,"We find that the bottom - up parser and the top - down parser have similar results under the greedy setting , and the in - order parser outperforms both of them .",Reranking experiments,Reranking experiments,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 14), (14, 15), (15, 17), (17, 18), (19, 21), (24, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.833333333,135,0.633802817,5,0.5,1,results,Reranking experiments,constituency_parsing5
76,76,76,138,English constituent results,,,constituency_parsing,5,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,137,0.643192488,7,0.7,1,results,,constituency_parsing5
77,77,77,140,"With the fully - supervise setting 5 , the inorder parser outperforms the state - of - the - art discrete parser , the state - of - the - art neural parsers",English constituent results,English constituent results,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (9, 11), (11, 12), (13, 22), (24, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.666666667,139,0.65258216,9,0.9,1,results,English constituent results,constituency_parsing5
78,78,78,145,Chinese dependency results,,,constituency_parsing,5,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,144,0.676056338,0,0,1,results,,constituency_parsing5
79,79,79,146,"As shown in , by converting the results to dependencies 6 , our final model achieves the best results among transitionbased parsing , and obtains comparable results to the state - of - the - art graph - based models . 85.5 84.0 87.7 86.2",Chinese dependency results,Chinese dependency results,constituency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 15), (15, 16), (17, 19), (19, 20), (20, 22), (24, 25), (25, 27), (27, 28), (29, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.019607843,145,0.680751174,1,0,1,results,Chinese dependency results,constituency_parsing5
80,80,80,2,Parsing as Language Modeling,,,constituency_parsing,6,"['O', 'O', 'O', 'O']","[(0, 1)]","['O', 'O', 'O', 'O']",0,0,1,0.011111111,0,0,1,experiments,,constituency_parsing6
81,81,81,4,"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .",abstract,abstract,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (27, 31), (39, 40), (40, 43), (43, 44), (44, 45), (48, 49), (51, 52)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.5,3,0.033333333,1,0.5,1,research-problem,abstract,constituency_parsing6
82,82,82,7,"Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 .",Introduction,Introduction,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,6,0.066666667,1,0.142857143,1,research-problem,Introduction,constituency_parsing6
83,83,83,8,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .",Introduction,Introduction,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(14, 15), (16, 21), (22, 23), (23, 26), (27, 30), (31, 32), (33, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.285714286,7,0.077777778,2,0.285714286,1,model,Introduction,constituency_parsing6
84,84,84,51,"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50 .",Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (7, 9), (10, 11), (11, 13), (13, 15), (15, 16), (16, 17), (17, 18), (18, 21), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.545454545,50,0.555555556,1,0.166666667,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
85,85,85,52,We initialize starting states with previous minibatch 's last hidden states .,Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,51,0.566666667,2,0.333333333,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
86,86,86,53,"The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .",Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 8), (8, 9), (11, 15), (16, 18), (18, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.727272727,52,0.577777778,3,0.5,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
87,87,87,54,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 6), (7, 8), (9, 10), (10, 11), (12, 13), (14, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.818181818,53,0.588888889,4,0.666666667,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
88,88,88,55,The learning rate is 0.25 0.85 max where is an epoch number .,Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.909090909,54,0.6,5,0.833333333,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
89,89,89,56,"For simplicity , we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax or noise contrastive estimation ( Gutmann and Hyvrinen , 2012 ) .",Model,Hyper-parameters,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 7), (7, 8), (9, 11), (14, 16), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1,55,0.611111111,6,1,1,hyperparameters,Model: Hyper-parameters,constituency_parsing6
90,90,90,72,Results,,,constituency_parsing,6,['O'],[],['O'],0,0,71,0.788888889,0,0,1,experiments,,constituency_parsing6
91,91,91,77,"A single LSTM - LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .",Results,Improved Semi-supervision,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 8), (8, 10), (10, 14), (14, 15), (15, 16), (27, 28), (28, 32), (32, 33), (34, 39), (40, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.714285714,76,0.844444444,4,0.666666667,1,results,Results: Improved Semi-supervision,constituency_parsing6
92,92,92,78,"When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .",Results,Improved Semi-supervision,constituency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (9, 12), (12, 13), (13, 18), (29, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.857142857,77,0.855555556,5,0.833333333,1,results,Results: Improved Semi-supervision,constituency_parsing6
93,93,93,2,What Do Recurrent Neural Network Grammars Learn About Syntax ?,title,title,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004347826,1,0,1,research-problem,title,constituency_parsing7
94,94,94,4,Recurrent neural network grammars ( RNNG ) area recently proposed probabilistic generative modeling family for natural language .,abstract,abstract,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.013043478,1,0.166666667,1,research-problem,abstract,constituency_parsing7
95,95,95,12,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 8), (8, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.037735849,11,0.047826087,2,0.105263158,1,approach,Introduction,constituency_parsing7
96,96,96,21,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 8), (8, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.20754717,20,0.086956522,11,0.578947368,1,approach,Introduction,constituency_parsing7
97,97,97,22,We begin with an ablation study to discover the importance of the composition function in 3 .,Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (6, 8), (9, 10), (10, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.226415094,21,0.091304348,12,0.631578947,1,approach,Introduction,constituency_parsing7
98,98,98,23,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 11), (11, 12), (13, 17), (18, 20), (21, 24), (25, 27), (27, 29), (29, 30), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.245283019,22,0.095652174,13,0.684210526,1,approach,Introduction,constituency_parsing7
99,99,99,24,"Using the GA - RNNG , we proceed by investigating the role that individual heads play in phrasal representation ( 5 ) and the role that nonterminal category labels play ( 6 ) .",Introduction,Introduction,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (9, 10), (11, 12), (13, 15), (15, 16), (16, 17), (17, 19), (26, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.264150943,23,0.1,14,0.736842105,1,approach,Introduction,constituency_parsing7
100,100,100,88,"The RNNG with only a stack is the strongest of the ablations , and it even outperforms the "" full "" RNNG with all three data structures .",Experimental results .,Discussion .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (6, 7), (11, 12), (16, 17), (18, 22), (22, 23), (23, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.12244898,87,0.37826087,23,0.766666667,1,results,Experimental results .: Discussion .,constituency_parsing7
101,101,101,89,Ablating the stack gives the worst among the new results .,Experimental results .,Discussion .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (5, 6), (6, 7), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.142857143,88,0.382608696,24,0.8,1,results,Experimental results .: Discussion .,constituency_parsing7
102,102,102,93,"A similar performance degradation is seen in language modeling : the stack - only RNNG achieves the best performance , and ablating the stack is most harmful .",Experimental results .,Discussion .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (11, 15), (15, 16), (17, 19), (21, 22), (23, 24), (24, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.224489796,92,0.4,28,0.933333333,1,results,Experimental results .: Discussion .,constituency_parsing7
103,103,103,94,"Indeed , modeling syntax without explicit composition ( the stackablated RNNG ) provides little benefit over a sequential LSTM language model .",Experimental results .,Discussion .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 7), (12, 13), (13, 15), (15, 16), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.244897959,93,0.404347826,29,0.966666667,1,results,Experimental results .: Discussion .,constituency_parsing7
104,104,104,95,We remark that the stack - only results are the best published PTB results for both phrasestructure and dependency parsing among supervised models .,Experimental results .,Discussion .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 8), (8, 9), (10, 14), (14, 15), (16, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.265306122,94,0.408695652,30,1,1,results,Experimental results .: Discussion .,constituency_parsing7
105,105,105,96,Gated Attention RNNG,Experimental results .,Discussion .,constituency_parsing,7,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",14,0.285714286,95,0.413043478,0,0,1,results,Experimental results .: Discussion .,constituency_parsing7
106,106,106,134,"It is clear that the model outperforms the baseline RNNG with all three structures present and achieves competitive performance with the strongest , stack - only , RNNG variant .",Experimental results .,Experimental results .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (8, 10), (10, 11), (11, 14), (16, 17), (17, 19), (19, 20), (21, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.051282051,133,0.57826087,24,1,1,results,Experimental results .,constituency_parsing7
107,107,107,135,Headedness in Phrases,Experimental results .,,constituency_parsing,7,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",3,0.076923077,134,0.582608696,0,0,1,results,Experimental results .,constituency_parsing7
108,108,108,173,The model has a higher overlap with the conversion using Collins head rules ( 49.8 UAS ) rather than the Stanford head rules ( 40.4 UAS ) .,Results .,Results .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (8, 9), (9, 10), (10, 17), (17, 19), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.058823529,172,0.747826087,7,0.4375,1,results,Results .,constituency_parsing7
109,109,109,175,"In general , the attention - based tree output has a high error rate ( ? 90 % ) when the dependent is a verb , since the constituent with the highest attention weight in a verb phrase is often the noun phrase instead of the verb , as discussed above .",Results .,Results .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 9), (11, 19), (19, 20), (21, 22), (22, 23), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.176470588,174,0.756521739,9,0.5625,1,results,Results .,constituency_parsing7
110,110,110,176,"The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .",Results .,Results .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 13), (15, 17), (15, 18), (17, 18), (18, 29), (29, 32), (33, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.235294118,175,0.760869565,10,0.625,1,results,Results .,constituency_parsing7
111,111,111,183,The Role of Nonterminal Labels,Results .,Discussion .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O']",11,0.647058824,182,0.791304348,0,0,1,ablation-analysis,Results .: Discussion .,constituency_parsing7
112,112,112,192,"On test data ( with the usual split ) , the GA - RNNG achieves 94.2 % , while the U - GA - RNNG achieves 93.5 % .",Experiments .,Experiments .,constituency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 5), (6, 8), (11, 14), (14, 15), (15, 17), (20, 25), (25, 26), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.095238095,191,0.830434783,9,0.321428571,1,results,Experiments .,constituency_parsing7
113,113,113,2,Constituency Parsing with a Self - Attentive Encoder,title,,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004926108,1,0,1,research-problem,title,constituency_parsing8
114,114,114,16,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .",Introduction,Introduction,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (9, 10), (11, 12), (12, 14), (21, 22), (23, 24), (24, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.285714286,15,0.073891626,6,0.285714286,1,model,Introduction,constituency_parsing8
115,115,115,24,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .",Introduction,Introduction,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (9, 10), (11, 13), (15, 16), (16, 17), (17, 18), (18, 21), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.666666667,23,0.113300493,14,0.666666667,1,model,Introduction,constituency_parsing8
116,116,116,163,6 Results,Models with Subword Features,External Embeddings,constituency_parsing,8,"['O', 'O']",[],"['O', 'O']",28,0.595744681,162,0.798029557,8,0.296296296,1,experiments,Models with Subword Features: External Embeddings,constituency_parsing8
117,117,117,164,English ( WSJ ),Models with Subword Features,External Embeddings,constituency_parsing,8,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",29,0.617021277,163,0.802955665,9,0.333333333,1,experiments,Models with Subword Features: External Embeddings,constituency_parsing8
118,118,118,168,"The test score of 93.55 F1 for our CharLSTM parser exceeds the previous best numbers for single - system parsers trained on the Penn Treebank ( without the use of any external data , such as pre-trained word embeddings ) .",Models with Subword Features,External Embeddings,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (6, 7), (7, 10), (10, 11), (12, 15), (15, 16), (16, 20), (20, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.70212766,167,0.822660099,13,0.481481481,1,experiments,Models with Subword Features: External Embeddings,constituency_parsing8
119,119,119,169,"When our parser is augmented with ELMo word representations , it achieves a new state - of - the - art score of 95.13 F1 on the WSJ test set .",Models with Subword Features,External Embeddings,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 9), (11, 12), (13, 22), (22, 23), (23, 25), (25, 26), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.723404255,168,0.827586207,14,0.518518519,1,experiments,Models with Subword Features: External Embeddings,constituency_parsing8
120,120,120,171,Multilingual ( SPMRL ),Models with Subword Features,External Embeddings,constituency_parsing,8,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",36,0.765957447,170,0.837438424,16,0.592592593,1,experiments,Models with Subword Features: External Embeddings,constituency_parsing8
121,121,121,178,"Development set results show that the addition of word embeddings to a model that uses a character LSTM has a mixed effect : it improves performance for some languages , but hurts for others .",Models with Subword Features,External Embeddings,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (6, 8), (8, 10), (10, 11), (12, 13), (13, 15), (16, 18), (20, 22), (24, 25), (25, 26), (26, 27), (27, 29), (31, 32), (32, 33), (33, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.914893617,177,0.871921182,23,0.851851852,1,experiments,Models with Subword Features: External Embeddings,constituency_parsing8
122,122,122,180,"On 8 of the 9 languages , our test set result exceeds the previous best - published numbers from any system we are aware of .",Models with Subword Features,External Embeddings,constituency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 6), (8, 11), (11, 12), (13, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.957446809,179,0.881773399,25,0.925925926,1,experiments,Models with Subword Features: External Embeddings,constituency_parsing8
123,123,123,2,Improving Coreference Resolution by Learning Entity - Level Distributed Representations,title,title,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00390625,1,0,1,research-problem,title,coreference_resolution0
124,124,124,16,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 11), (11, 13), (13, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,15,0.05859375,7,0.35,1,model,Introduction,coreference_resolution0
125,125,125,17,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (8, 15), (15, 17), (18, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4,16,0.0625,8,0.4,1,model,Introduction,coreference_resolution0
126,126,126,18,"Using the cluster - pair representations , our network learns when combining two coreference clusters is desirable .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (7, 9), (9, 10), (12, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,17,0.06640625,9,0.45,1,model,Introduction,coreference_resolution0
127,127,127,19,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .",Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 6), (6, 8), (8, 9), (10, 12), (12, 14), (14, 15), (16, 18), (20, 21), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5,18,0.0703125,10,0.5,1,model,Introduction,coreference_resolution0
128,128,128,20,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (6, 14), (15, 16), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,19,0.07421875,11,0.55,1,model,Introduction,coreference_resolution0
129,129,129,22,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 12), (12, 14), (14, 15), (15, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.65,21,0.08203125,13,0.65,1,model,Introduction,coreference_resolution0
130,130,130,23,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,Introduction,Introduction,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 7), (14, 16), (17, 19), (21, 25), (31, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7,22,0.0859375,14,0.7,1,model,Introduction,coreference_resolution0
131,131,131,229,Our mention - ranking model surpasses all previous systems .,Final System Performance,Final System Performance,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.222222222,228,0.890625,2,0.222222222,1,results,Final System Performance,coreference_resolution0
132,132,132,231,"The cluster - ranking model improves results further across both languages and all evaluation metrics , demonstrating the utility of incorporating entity - level information .",Final System Performance,Final System Performance,coreference_resolution,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.444444444,230,0.8984375,4,0.444444444,1,results,Final System Performance,coreference_resolution0
133,133,133,2,End - to - end Deep Reinforcement Learning Based Coreference Resolution,title,title,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.006802721,1,0,1,research-problem,title,coreference_resolution1
134,134,134,23,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 17), (17, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.666666667,22,0.149659864,12,0.666666667,1,model,Introduction,coreference_resolution1
135,135,135,24,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (7, 8), (10, 12), (14, 15), (15, 16), (16, 18), (19, 20), (20, 23), (25, 26), (27, 29), (29, 30), (34, 35), (35, 36), (37, 39), (39, 40), (41, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.722222222,23,0.156462585,13,0.722222222,1,model,Introduction,coreference_resolution1
136,136,136,25,"Once a sequence of linking actions are made , our reward function is used to measure how good the generated coreference clusters are , which is directly related to coreference evaluation metrics .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (4, 6), (7, 8), (10, 12), (14, 16), (16, 18), (19, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.777777778,24,0.163265306,14,0.777777778,1,model,Introduction,coreference_resolution1
137,137,137,26,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 10), (10, 11), (12, 13), (14, 15), (15, 16), (16, 19), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.833333333,25,0.170068027,15,0.833333333,1,model,Introduction,coreference_resolution1
138,138,138,27,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .",Introduction,Introduction,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 9), (9, 11), (12, 13), (13, 15), (15, 19), (22, 24), (25, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.888888889,26,0.176870748,16,0.888888889,1,model,Introduction,coreference_resolution1
139,139,139,101,Experiments,,,coreference_resolution,1,['O'],"[(0, 1)]",['O'],0,0,100,0.680272109,0,0,1,experiments,,coreference_resolution1
140,140,140,104,"First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .",Experiments,Experiments,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (6, 7), (11, 12), (12, 16), (17, 18), (19, 21), (21, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.428571429,103,0.700680272,3,0.428571429,1,hyperparameters,Experiments,coreference_resolution1
141,141,141,105,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .",Experiments,Experiments,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 11), (11, 12), (12, 13), (14, 15), (16, 18), (20, 21), (21, 36), (37, 40), (40, 43), (43, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.571428571,104,0.707482993,4,0.571428571,1,hyperparameters,Experiments,coreference_resolution1
142,142,142,109,Results,,,coreference_resolution,1,['O'],[],['O'],0,0,108,0.734693878,0,0,1,experiments,,coreference_resolution1
143,143,143,115,"Built on top of the model in but excluding ELMo , our base reinforced model improves the average F 1 score around 2 points ( statistical significant t- test with p < 0.05 ) compared with .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 6), (6, 7), (8, 9), (9, 10), (11, 15), (15, 16), (17, 21), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.545454545,114,0.775510204,6,0.545454545,1,results,Results,coreference_resolution1
144,144,144,117,"Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 7), (7, 9), (9, 10), (11, 12), (13, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.727272727,116,0.789115646,8,0.727272727,1,results,Results,coreference_resolution1
145,145,145,118,"Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9), (9, 10), (10, 13), (14, 16), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.818181818,117,0.795918367,9,0.818181818,1,results,Results,coreference_resolution1
146,146,146,120,"Overall , our full model achieves the state - of the - art performance of 73.8 % F1 - score when using ELMo and entropy regularization ( compared to models marked with * in , and our approach simultaneously obtains the best F1 -score of 70.5 % when using fixed word embedding only .",Results,Results,coreference_resolution,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (5, 6), (7, 14), (14, 15), (15, 20), (20, 22), (22, 26), (36, 38), (41, 44), (44, 45), (45, 47), (47, 49), (49, 53)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1,119,0.80952381,11,1,1,results,Results,coreference_resolution1
147,147,147,4,Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning .,abstract,abstract,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (0, 2), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.02,1,0.2,1,research-problem,abstract,coreference_resolution2
148,148,148,15,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .",Introduction,Introduction,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 12), (12, 15), (16, 18), (18, 19), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.065217391,14,0.093333333,6,0.157894737,1,approach,Introduction,coreference_resolution2
149,149,149,16,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .",Introduction,Introduction,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 9), (11, 13), (14, 15), (15, 17), (17, 20), (20, 21), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.076086957,15,0.1,7,0.184210526,1,approach,Introduction,coreference_resolution2
150,150,150,17,We also test the REINFORCE policy gradient algorithm .,Introduction,Introduction,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.086956522,16,0.106666667,8,0.210526316,1,experiments,Introduction,coreference_resolution2
151,151,151,18,Our model is a neural mention - ranking model .,Introduction,Introduction,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.097826087,17,0.113333333,9,0.236842105,1,approach,Introduction,coreference_resolution2
152,152,152,108,Results,,,coreference_resolution,2,['O'],[],['O'],0,0,107,0.713333333,0,0,1,experiments,,coreference_resolution2
153,153,153,110,"We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .",Results,Results,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 7), (7, 8), (9, 11), (13, 15), (15, 16), (16, 18), (18, 19), (19, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.060606061,109,0.726666667,2,0.285714286,1,results,Results,coreference_resolution2
154,154,154,115,"The reward - rescaled max - margin loss combines the best of both worlds , resulting in superior performance .",Results,Results,coreference_resolution,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 8), (8, 9), (10, 14), (15, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.212121212,114,0.76,7,1,1,results,Results,coreference_resolution2
155,155,155,2,Higher - order Coreference Resolution with Coarse - to - fine Inference,title,title,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.007462687,1,0,1,research-problem,title,coreference_resolution3
156,156,156,15,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 9), (10, 11), (12, 16), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,14,0.104477612,6,0.375,1,approach,Introduction,coreference_resolution3
157,157,157,16,"At each iteration , the antecedent distribution is used as an attention mechanism to optionally update existing span representations , enabling later corefer - Speaker 1 : U m and think that is what 's - Go ahead Linda .",Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 7), (8, 10), (11, 13), (13, 16), (16, 19), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,15,0.111940299,7,0.4375,1,approach,Introduction,coreference_resolution3
158,158,158,19,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .",Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 14), (15, 19), (21, 23), (24, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,18,0.134328358,10,0.625,1,approach,Introduction,coreference_resolution3
159,159,159,20,We introduce a less accurate but more efficient coarse factor in the pairwise scoring function .,Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 10), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,19,0.141791045,11,0.6875,1,approach,Introduction,coreference_resolution3
160,160,160,21,This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor .,Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 9), (9, 10), (11, 12), (13, 16), (16, 18), (19, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.75,20,0.149253731,12,0.75,1,approach,Introduction,coreference_resolution3
161,161,161,22,"Intuitively , the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function .",Introduction,Introduction,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 9), (9, 10), (10, 12), (12, 14), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,21,0.156716418,13,0.8125,1,approach,Introduction,coreference_resolution3
162,162,162,112,Results,,,coreference_resolution,3,['O'],[],['O'],0,0,111,0.828358209,0,0,1,experiments,,coreference_resolution3
163,163,163,118,"The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .",Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 12), (13, 15), (16, 20), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.461538462,117,0.873134328,6,0.461538462,1,results,Results,coreference_resolution3
164,164,164,119,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .",Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 6), (7, 8), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.538461538,118,0.880597015,7,0.538461538,1,results,Results,coreference_resolution3
165,165,165,121,"Despite using far less computation , it outperforms the baseline because the coarse scores s c ( i , j ) can be computed for all antecedents , enabling the model to potentially predict a coreference link between any two spans in the document .",Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.692307692,120,0.895522388,9,0.692307692,1,results,Results,coreference_resolution3
166,166,166,122,"As a result , we observe a much higher recall when adopting the coarse - to - fine approach .",Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 12), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.769230769,121,0.902985075,10,0.769230769,1,results,Results,coreference_resolution3
167,167,167,123,We also observe further improvement by including the second - order inference ( Section 3 ) .,Results,Results,coreference_resolution,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 6), (6, 7), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.846153846,122,0.910447761,11,0.846153846,1,results,Results,coreference_resolution3
168,168,168,2,A Mention - Ranking Model for Abstract Anaphora Resolution,title,title,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004166667,1,0,1,research-problem,title,coreference_resolution4
169,169,169,15,"Current research in anaphora ( or coreference ) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real Leo Born , Juri Opitz and Anette Frank contributed equally to this work .",Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037,14,0.058333333,1,0.037037037,1,research-problem,Introduction,coreference_resolution4
170,170,170,28,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .",Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 10), (10, 11), (11, 13), (14, 17), (18, 20), (21, 23), (23, 24), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.518518519,27,0.1125,14,0.518518519,1,model,Introduction,coreference_resolution4
171,171,171,29,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .",Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (13, 15), (20, 22), (38, 42), (42, 43), (43, 44), (44, 45), (46, 51), (51, 52), (53, 55)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.555555556,28,0.116666667,15,0.555555556,1,model,Introduction,coreference_resolution4
172,172,172,30,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 8), (8, 11), (12, 13), (14, 15), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.592592593,29,0.120833333,16,0.592592593,1,model,Introduction,coreference_resolution4
173,173,173,31,The learned score is used to select the highest - scoring antecedent candidate for the given anaphoric sentence and hence its anaphor .,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (8, 13), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.62962963,30,0.125,17,0.62962963,1,model,Introduction,coreference_resolution4
174,174,174,32,We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL .,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 7), (8, 9), (10, 11), (11, 12), (13, 14), (14, 15), (16, 17), (19, 20), (20, 21), (22, 27), (27, 28), (29, 30), (30, 32), (32, 35), (46, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.666666667,31,0.129166667,18,0.666666667,1,model,Introduction,coreference_resolution4
175,175,175,33,With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent .,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (7, 8), (8, 9), (10, 11), (11, 12), (13, 14), (14, 15), (16, 18), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.703703704,32,0.133333333,19,0.703703704,1,model,Introduction,coreference_resolution4
176,176,176,36,It produces large amounts of instances and is easily adaptable to other languages .,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (8, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.814814815,35,0.145833333,22,0.814814815,1,model,Introduction,coreference_resolution4
177,177,177,41,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora.,Introduction,Introduction,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,1,40,0.166666667,27,1,1,code,Introduction,coreference_resolution4
178,178,178,156,Baselines and evaluation metrics,,,coreference_resolution,4,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",0,0,155,0.645833333,0,0,1,experiments,,coreference_resolution4
179,179,179,158,"Additionally , we report the preceding sentence baseline ( PS BL ) that chooses the previous sentence for the antecedent and TAGbaseline ( TAG BL ) that randomly chooses a candidate with the constituent tag label in {S , VP , ROOT , SBAR } .",Baselines and evaluation metrics,Baselines and evaluation metrics,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 12), (13, 14), (15, 17), (17, 18), (19, 20), (21, 26), (27, 29), (30, 31), (31, 32), (33, 36), (36, 37), (37, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,157,0.654166667,2,0.5,1,baselines,Baselines and evaluation metrics,coreference_resolution4
180,180,180,172,"Glo Ve word embeddings pre-trained on the Gigaword and Wikipedia , and did not fine - tune them .",Hyperparameters tuning .,Input representation .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 6), (7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.192307692,171,0.7125,11,0.297297297,1,hyperparameters,Hyperparameters tuning .: Input representation .,coreference_resolution4
181,181,181,173,"Vocabulary was built from the words in the training data with frequency in { 3 , U ( 1 , 10 ) } , and OOV words were replaced with an UNK token .",Hyperparameters tuning .,Input representation .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 6), (6, 7), (8, 10), (10, 11), (11, 12), (12, 13), (13, 23), (25, 27), (28, 30), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.211538462,172,0.716666667,12,0.324324324,1,hyperparameters,Hyperparameters tuning .: Input representation .,coreference_resolution4
182,182,182,177,"The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 7), (8, 10), (10, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.288461538,176,0.733333333,16,0.432432432,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
183,183,183,178,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 8), (8, 9), (9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.307692308,177,0.7375,17,0.459459459,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
184,184,184,179,The first feed - forward layer size is set to a value in Optimization .,Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (8, 10), (11, 12), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.326923077,178,0.741666667,18,0.486486486,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
185,185,185,180,"We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 6), (6, 7), (7, 15), (15, 16), (17, 19), (19, 20), (20, 23), (24, 27), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.346153846,179,0.745833333,19,0.513513514,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
186,186,186,181,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .",Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6), (7, 8), (9, 11), (11, 12), (12, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.365384615,180,0.75,20,0.540540541,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
187,187,187,182,We train for 10 epochs and choose the model that performs best on the devset .,Hyperparameters tuning .,Weights initialization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (6, 7), (8, 9), (10, 11), (11, 12), (12, 13), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.384615385,181,0.754166667,21,0.567567568,1,hyperparameters,Hyperparameters tuning .: Weights initialization .,coreference_resolution4
188,188,188,184,"We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) }.",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.423076923,183,0.7625,23,0.621621622,1,hyperparameters,Hyperparameters tuning .: Regularization .,coreference_resolution4
189,189,189,185,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (18, 20), (21, 22), (22, 23), (24, 25), (27, 31), (32, 34), (35, 36), (36, 37), (37, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.442307692,184,0.766666667,24,0.648648649,1,hyperparameters,Hyperparameters tuning .: Regularization .,coreference_resolution4
190,190,190,188,"In terms of s@1 score , MR - LSTM outperforms both KZH13 's results and TAG BL without even necessitating HP tuning .",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 5), (6, 9), (9, 10), (11, 14), (15, 17), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5,187,0.779166667,27,0.72972973,1,results,Hyperparameters tuning .: Regularization .,coreference_resolution4
191,191,191,190,"From we observe : ( 1 ) with HPs tuned on ARRAU - AA , we obtain results well beyond KZH13 , ( 2 ) all ablated model variants perform worse than the full model , ( 3 ) a large performance drop when omitting syntactic information ( tag , cut ) suggests that the model makes good use of it .",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 9), (9, 11), (11, 14), (16, 17), (17, 18), (18, 20), (20, 21), (25, 29), (29, 30), (30, 31), (31, 32), (33, 35), (40, 43), (43, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.538461538,189,0.7875,29,0.783783784,1,results,Hyperparameters tuning .: Regularization .,coreference_resolution4
192,192,192,195,"Performance of 68.10 s@1 score indicates that the model is able to learn without syntactic guidance , contrary to the 19.68 s@1 score before tuning .",Hyperparameters tuning .,Regularization .,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 5), (5, 6), (8, 9), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.634615385,194,0.808333333,34,0.918918919,1,results,Hyperparameters tuning .: Regularization .,coreference_resolution4
193,193,193,199,Results on the ARRAU corpus,Hyperparameters tuning .,,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O']",37,0.711538462,198,0.825,0,0,1,results,Hyperparameters tuning .,coreference_resolution4
194,194,194,200,"The MR - LSTM is more successful in resolving nominal than pronominal anaphors , although the training data provides only pronominal ones .",Hyperparameters tuning .,Results on the ARRAU corpus,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.730769231,199,0.829166667,1,0.066666667,1,results,Hyperparameters tuning .: Results on the ARRAU corpus,coreference_resolution4
195,195,195,202,"Moreover , for shell noun resolution in KZH13 's dataset , the MR - LSTM achieved s@1 scores in the range 76.09-93.14 , while the best variant of the model achieves 51.89 s@1 score for nominal anaphors in ARRAU - AA .",Hyperparameters tuning .,Results on the ARRAU corpus,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (6, 7), (7, 10), (12, 15), (15, 16), (16, 18), (18, 19), (20, 21), (21, 22), (30, 31), (31, 34), (34, 35), (35, 37), (37, 38), (38, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.769230769,201,0.8375,3,0.2,1,results,Hyperparameters tuning .: Results on the ARRAU corpus,coreference_resolution4
196,196,196,209,"This is what we can observe from row 2 vs. row 6 in Table 5 : the MR - LSTM without context embedding ( ctx ) achieves a comparable s@ 2 score with the variant that omits syntactic information , but better s@3 - 4 scores .",Hyperparameters tuning .,Results on the ARRAU corpus,coreference_resolution,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 26), (26, 27), (28, 32), (32, 33), (34, 35), (35, 37), (37, 39), (41, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.903846154,208,0.866666667,10,0.666666667,1,results,Hyperparameters tuning .: Results on the ARRAU corpus,coreference_resolution4
197,197,197,2,Learning Global Features for Coreference Resolution,title,,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O']","[(4, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004255319,1,0,1,research-problem,title,coreference_resolution5
198,198,198,4,There is compelling evidence that coreference prediction would benefit from modeling global information about entity - clusters .,abstract,abstract,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 10), (10, 11), (11, 13), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.012765957,1,0.166666667,1,research-problem,abstract,coreference_resolution5
199,199,199,12,"In this work , we posit that global context is indeed necessary for further improvements in coreference resolution , but argue that informative cluster , rather than mention , level features are very difficult to devise , limiting their effectiveness .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (11, 13), (13, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.181818182,11,0.046808511,2,0.181818182,1,approach,Introduction,coreference_resolution5
200,200,200,13,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 9), (9, 11), (11, 14), (14, 15), (15, 16), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.272727273,12,0.05106383,3,0.272727273,1,approach,Introduction,coreference_resolution5
201,201,201,14,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 8), (11, 12), (13, 15), (15, 16), (17, 19), (19, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.363636364,13,0.055319149,4,0.363636364,1,approach,Introduction,coreference_resolution5
202,202,202,15,We incorporate these representations into a mention - ranking style coreference system .,Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (6, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.454545455,14,0.059574468,5,0.454545455,1,approach,Introduction,coreference_resolution5
203,203,203,16,"The entire model , including the recurrent neural network and the mention - ranking sub-system , is trained end - to - end on the coreference task .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 9), (11, 15), (17, 18), (18, 23), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.545454545,15,0.063829787,6,0.545454545,1,approach,Introduction,coreference_resolution5
204,204,204,17,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .",Introduction,Introduction,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,16,0.068085106,7,0.636363636,1,approach,Introduction,coreference_resolution5
205,205,205,181,"For training , we use document - size minibatches , which allows for efficient pre-computation of RNN states , and we minimize the loss described in Section 5 with AdaGrad ( after clipping LSTM gradients to lie ( elementwise ) in ( ?10 , 10 ) ) .",Methods,Features,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 9), (11, 13), (13, 15), (15, 16), (16, 18), (21, 22), (23, 24), (28, 29), (29, 30), (31, 33), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.678571429,180,0.765957447,19,0.678571429,1,experimental-setup,Methods: Features,coreference_resolution5
206,206,206,182,"We find that the initial learning rate chosen for AdaGrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .",Methods,Features,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 7), (7, 9), (9, 10), (12, 14), (14, 15), (15, 16), (19, 20), (20, 22), (22, 23), (23, 25), (25, 27), (27, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.714285714,181,0.770212766,20,0.714285714,1,experimental-setup,Methods: Features,coreference_resolution5
207,207,207,183,"In experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? R 200 , and hp ( x n , y) ? R 700 .",Methods,Features,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 23), (23, 25), (25, 28), (30, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.75,182,0.774468085,21,0.75,1,experimental-setup,Methods: Features,coreference_resolution5
208,208,208,184,"We use a single - layer LSTM ( without "" peep - hole "" connections ) , as implemented in the element - rnn library .",Methods,Features,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (8, 9), (18, 20), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.785714286,183,0.778723404,22,0.785714286,1,experimental-setup,Methods: Features,coreference_resolution5
209,209,209,185,"For regularization , we apply Dropout with a rate of 0.4 before applying the linear weights u , and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot -product scores .",Methods,Features,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 6), (6, 7), (8, 9), (9, 10), (10, 11), (11, 13), (14, 17), (21, 22), (22, 23), (23, 24), (25, 26), (26, 27), (27, 28), (28, 29), (30, 32), (32, 34), (35, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.821428571,184,0.782978723,23,0.821428571,1,experimental-setup,Methods: Features,coreference_resolution5
210,210,210,189,Code for our system is available at https : //github.com/swiseman/nn_coref .,Methods,Features,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.964285714,188,0.8,27,0.964285714,1,code,Methods: Features,coreference_resolution5
211,211,211,190,"The system makes use of a GPU for training , and trains in about two hours .",Methods,Features,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (6, 7), (7, 8), (8, 9), (11, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,1,189,0.804255319,28,1,1,experimental-setup,Methods: Features,coreference_resolution5
212,212,212,191,Results,,,coreference_resolution,5,['O'],[],['O'],0,0,190,0.808510638,0,0,1,experiments,,coreference_resolution5
213,213,213,193,"We see a statistically significant improvement of over 0.8 Co NLL points over the previous state of the art , and the highest F 1 scores to date on all three CoNLL metrics .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 12), (12, 13), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.058823529,192,0.817021277,2,0.125,1,results,Results,coreference_resolution5
214,214,214,203,"In we see that the RNN improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 8), (8, 10), (11, 12), (13, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.352941176,202,0.859574468,12,0.75,1,results,Results,coreference_resolution5
215,215,215,205,"Importantly , the RNN performance is significantly better than that of the Avg baseline , which barely improves over mention - ranking , even with oracle history .",Results,Results,coreference_resolution,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 8), (8, 9), (12, 14), (16, 19), (19, 22), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.411764706,204,0.868085106,14,0.875,1,results,Results,coreference_resolution5
216,216,216,2,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution,title,title,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.007874016,1,0,1,research-problem,title,coreference_resolution6
217,217,217,4,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .",abstract,abstract,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (11, 12), (12, 16), (16, 18), (18, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.023622047,1,0.2,1,research-problem,abstract,coreference_resolution6
218,218,218,5,"While the traditional E2E - CR model generates word representations by running long short - term memory ( LSTM ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .",abstract,abstract,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12), (35, 36), (40, 44), (44, 46), (46, 48)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,4,0.031496063,2,0.4,1,research-problem,abstract,coreference_resolution6
219,219,219,10,Co-reference resolution requires models to cluster mentions that refer to the same physical entities .,Introduction,Introduction,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.014084507,9,0.070866142,1,0.034482759,1,model,Introduction,coreference_resolution6
220,220,220,24,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .",Introduction,Introduction,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(46, 47), (48, 52), (52, 53), (53, 64)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.211267606,23,0.181102362,15,0.517241379,1,model,Introduction,coreference_resolution6
221,221,221,25,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .",Introduction,Introduction,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 14), (14, 15), (15, 19), (19, 20), (20, 22), (23, 25), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.225352113,24,0.188976378,16,0.551724138,1,model,Introduction,coreference_resolution6
222,222,222,26,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .",Introduction,Introduction,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 9), (10, 13), (13, 15), (15, 16), (17, 18), (21, 22), (23, 24), (24, 25), (25, 27), (27, 29), (30, 35), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.23943662,25,0.196850394,17,0.586206897,1,model,Introduction,coreference_resolution6
223,223,223,87,"In practice , the LSTM modules applied in our model have 200 output units .",Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 8), (10, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,86,0.677165354,1,0.166666667,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
224,224,224,88,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .",Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 9), (9, 10), (11, 13), (13, 14), (14, 17), (17, 19), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,87,0.68503937,2,0.333333333,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
225,225,225,89,The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 8), (9, 10), (10, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,88,0.692913386,3,0.5,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
226,226,226,90,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .",Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,89,0.700787402,4,0.666666667,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
227,227,227,91,We randomly select up to 40 continuous sentences for training if the input is too long .,Model and Hyperparameter Setup,Model and Hyperparameter Setup,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 8), (8, 9), (9, 10), (10, 11), (12, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.833333333,90,0.708661417,5,0.833333333,1,hyperparameters,Model and Hyperparameter Setup,coreference_resolution6
228,228,228,93,Experiment Results and Discussion,,,coreference_resolution,6,"['O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O']",0,0,92,0.724409449,0,0,1,results,,coreference_resolution6
229,229,229,97,"Comparing with the baseline model that achieved 67.2 % F1 score , the ASL model improved the performance by 0.6 % and achieved 67.8 % average F1 .",Experiment Results and Discussion,Experiment Results and Discussion,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 7), (7, 11), (13, 15), (15, 16), (17, 18), (18, 19), (19, 21), (22, 23), (23, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.133333333,96,0.755905512,4,0.133333333,1,results,Experiment Results and Discussion,coreference_resolution6
230,230,230,108,"show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .",Experiment Results and Discussion,Experiment Results and Discussion,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 4), (5, 6), (6, 10), (10, 12), (13, 15), (17, 18), (18, 20), (20, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5,107,0.842519685,15,0.5,1,results,Experiment Results and Discussion,coreference_resolution6
231,231,231,109,"Experiments also indicated that the ASL model has better performance than the LSL model , since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence - level embeddings .",Experiment Results and Discussion,Experiment Results and Discussion,coreference_resolution,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 7), (8, 10), (10, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.533333333,108,0.850393701,16,0.533333333,1,results,Experiment Results and Discussion,coreference_resolution6
232,232,232,2,Coreference Resolution with Entity Equalization,title,,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O']",1,0,1,0.007751938,1,0,1,research-problem,title,coreference_resolution7
233,233,233,18,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (6, 7), (8, 12), (12, 13), (14, 18), (21, 22), (22, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.380952381,17,0.131782946,8,0.380952381,1,approach,Introduction,coreference_resolution7
234,234,234,19,"Our "" Entity Equalization "" approach posits that each entity should be represented via the sum of its corresponding mention representations .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 10), (12, 14), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.428571429,18,0.139534884,9,0.428571429,1,approach,Introduction,coreference_resolution7
235,235,235,22,"Similar to recent coreference models , our approach uses contextual embeddings as input mention representations .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.571428571,21,0.162790698,12,0.571428571,1,approach,Introduction,coreference_resolution7
236,236,236,23,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .",Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.619047619,22,0.170542636,13,0.619047619,1,approach,Introduction,coreference_resolution7
237,237,237,25,We show that this can be done by using BERT in a fully convolutional manner .,Introduction,Introduction,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 10), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.714285714,24,0.186046512,15,0.714285714,1,approach,Introduction,coreference_resolution7
238,238,238,115,Results,,,coreference_resolution,7,['O'],[],['O'],0,0,114,0.88372093,0,0,1,experiments,,coreference_resolution7
239,239,239,119,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg.",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8), (8, 10), (10, 13), (14, 19), (21, 22), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5,118,0.914728682,4,0.5,1,results,Results,coreference_resolution7
240,240,240,120,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 9), (9, 10), (10, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,119,0.92248062,5,0.625,1,results,Results,coreference_resolution7
241,241,241,121,"Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 8), (8, 10), (10, 12), (12, 13), (13, 16), (17, 18), (18, 23), (23, 24), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.75,120,0.930232558,6,0.75,1,results,Results,coreference_resolution7
242,242,242,122,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (4, 5), (5, 7), (7, 8), (8, 13), (16, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.875,121,0.937984496,7,0.875,1,results,Results,coreference_resolution7
243,243,243,123,"Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .",Results,Results,coreference_resolution,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9), (9, 10), (10, 12), (13, 14), (15, 20), (20, 21), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1,122,0.945736434,8,1,1,results,Results,coreference_resolution7
244,244,244,2,End - to - end Neural Coreference Resolution,title,title,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004032258,1,0,1,research-problem,title,coreference_resolution8
245,245,245,4,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .,abstract,abstract,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 12), (13, 14), (16, 18), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.012096774,1,0.2,1,research-problem,abstract,coreference_resolution8
246,246,246,10,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .,Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (17, 18), (18, 21), (21, 22), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,9,0.036290323,1,0.0625,1,model,Introduction,coreference_resolution8
247,247,247,12,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .",Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (16, 17), (19, 20), (24, 26), (27, 34), (35, 37), (40, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,11,0.044354839,3,0.1875,1,model,Introduction,coreference_resolution8
248,248,248,13,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 6), (6, 7), (7, 9), (9, 11), (12, 14), (15, 17), (18, 20), (20, 21), (21, 23), (23, 24), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,12,0.048387097,4,0.25,1,model,Introduction,coreference_resolution8
249,249,249,14,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .",Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (8, 9), (10, 11), (11, 13), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,13,0.052419355,5,0.3125,1,model,Introduction,coreference_resolution8
250,250,250,15,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .",Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 9), (9, 10), (10, 11), (13, 14), (15, 16), (18, 19), (19, 24), (24, 25), (26, 31), (31, 32), (33, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,14,0.056451613,6,0.375,1,model,Introduction,coreference_resolution8
251,251,251,16,"The attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .",Introduction,Introduction,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 14), (14, 15), (15, 17), (20, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,15,0.060483871,7,0.4375,1,model,Introduction,coreference_resolution8
252,252,252,116,Hyperparameters,,,coreference_resolution,8,['O'],"[(0, 1)]",['O'],0,0,115,0.463709677,0,0,1,experiments,,coreference_resolution8
253,253,253,118,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .",Hyperparameters,Word representations,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (6, 7), (7, 12), (13, 17), (20, 21), (21, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.066666667,117,0.471774194,2,0.25,1,hyperparameters,Hyperparameters: Word representations,coreference_resolution8
254,254,254,119,Outof - vocabulary words are represented by a vector of zeros .,Hyperparameters,Word representations,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 7), (8, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1,118,0.475806452,3,0.375,1,hyperparameters,Hyperparameters: Word representations,coreference_resolution8
255,255,255,120,"In the character CNN , characters are represented as learned 8 - dimensional embeddings .",Hyperparameters,Word representations,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 6), (7, 9), (9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.133333333,119,0.47983871,4,0.5,1,hyperparameters,Hyperparameters: Word representations,coreference_resolution8
256,256,256,121,"The convolutions have window sizes of 3 , 4 , and 5 characters , each consisting of 50 filters .",Hyperparameters,Word representations,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (6, 13), (15, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.166666667,120,0.483870968,5,0.625,1,hyperparameters,Hyperparameters: Word representations,coreference_resolution8
257,257,257,123,The hidden states in the LSTMs have 200 dimensions .,Hyperparameters,Hidden dimensions,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 6), (6, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.233333333,122,0.491935484,7,0.875,1,hyperparameters,Hyperparameters: Hidden dimensions,coreference_resolution8
258,258,258,124,Each feedforward neural network consists of two hidden layers with 150 dimensions and rectified linear units .,Hyperparameters,Hidden dimensions,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 6), (6, 9), (9, 10), (10, 12), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.266666667,123,0.495967742,8,1,1,hyperparameters,Hyperparameters: Hidden dimensions,coreference_resolution8
259,259,259,133,We use ADAM for learning with a minibatch size of 1 .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (7, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.566666667,132,0.532258065,1,0.125,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
260,260,260,134,The LSTM weights are initialized with random orthonormal matrices as described in .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6,133,0.536290323,2,0.25,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
261,261,261,135,We apply 0.5 dropout to the word embeddings and character CNN outputs .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.633333333,134,0.540322581,3,0.375,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
262,262,262,136,We apply 0.2 dropout to all hidden layers and feature embeddings .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.666666667,135,0.544354839,4,0.5,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
263,263,263,137,Dropout masks are shared across timesteps to preserve long - distance information as described in .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 6), (6, 8), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.7,136,0.548387097,5,0.625,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
264,264,264,138,The learning rate is decayed by 0.1 % every 100 steps .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.733333333,137,0.552419355,6,0.75,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
265,265,265,139,"The model is trained for up to 150 epochs , with early stopping based on the development set .",Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 9), (10, 11), (11, 13), (13, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.766666667,138,0.556451613,7,0.875,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
266,266,266,140,All code is implemented in Tensor - Flow and is publicly available .,Hyperparameters,Learning,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.8,139,0.560483871,8,1,1,hyperparameters,Hyperparameters: Learning,coreference_resolution8
267,267,267,147,Results,,,coreference_resolution,8,['O'],[],['O'],0,0,146,0.588709677,0,0,1,experiments,,coreference_resolution8
268,268,268,151,Coreference Results,,,coreference_resolution,8,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,150,0.60483871,0,0,1,results,,coreference_resolution8
269,269,269,153,We outperform previous systems in all metrics .,Coreference Results,Coreference Results,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.285714286,152,0.612903226,2,0.285714286,1,results,Coreference Results,coreference_resolution8
270,270,270,154,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .",Coreference Results,Coreference Results,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (8, 17), (17, 18), (18, 19), (21, 26), (26, 27), (28, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.428571429,153,0.616935484,3,0.428571429,1,results,Coreference Results,coreference_resolution8
271,271,271,155,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .",Coreference Results,Coreference Results,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 6), (6, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.571428571,154,0.620967742,4,0.571428571,1,results,Coreference Results,coreference_resolution8
272,272,272,162,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .",Ablations,Features,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 9), (9, 10), (10, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.036144578,161,0.649193548,3,0.166666667,1,ablation-analysis,Ablations: Features,coreference_resolution8
273,273,273,163,They contribute 3.8 F1 to the final result .,Ablations,Features,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.048192771,162,0.653225806,4,0.222222222,1,ablation-analysis,Ablations: Features,coreference_resolution8
274,274,274,169,"Since coreference decisions often involve rare named entities , we see a contribution of 0.9 F1 from character - level modeling .",Ablations,Word representations,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 13), (13, 14), (14, 16), (16, 17), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.120481928,168,0.677419355,10,0.555555556,1,ablation-analysis,Ablations: Word representations,coreference_resolution8
275,275,275,173,Ablations also show a 1.3 F1 degradation in performance without the attention mechanism for finding task - specific heads .,Ablations,Word representations,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (8, 9), (9, 10), (11, 13), (13, 15), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.168674699,172,0.693548387,14,0.777777778,1,ablation-analysis,Ablations: Word representations,coreference_resolution8
276,276,276,181,"As shown in , keeping mention candidates detected by the rule - based system over predicted parse trees ( Raghunathan et al. , 2010 ) degrades performance by 1 F1 .",Ablations,Comparing Span Pruning Strategies,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 7), (7, 9), (10, 14), (14, 15), (15, 18), (25, 26), (26, 27), (27, 28), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.265060241,180,0.725806452,3,0.6,1,ablation-analysis,Ablations: Comparing Span Pruning Strategies,coreference_resolution8
277,277,277,183,"With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .",Ablations,Comparing Span Pruning Strategies,coreference_resolution,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (7, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.289156627,182,0.733870968,5,1,1,ablation-analysis,Ablations: Comparing Span Pruning Strategies,coreference_resolution8
278,278,278,2,BERT for Coreference Resolution : Baselines and Analysis,title,,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.008474576,1,0,1,research-problem,title,coreference_resolution9
279,279,279,11,"We fine - tune BERT to coreference resolution , achieving strong improvements on the GAP and benchmarks .",Introduction,Introduction,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (6, 8), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.166666667,10,0.084745763,3,0.166666667,1,model,Introduction,coreference_resolution9
280,280,280,12,We present two ways of extending the c 2f - coref model in .,Introduction,Introduction,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.222222222,11,0.093220339,4,0.222222222,1,model,Introduction,coreference_resolution9
281,281,281,16,1 https://github.com/mandarjoshi90/coref,Introduction,Introduction,coreference_resolution,9,"['O', 'O']","[(1, 2)]","['O', 'O']",8,0.444444444,15,0.127118644,8,0.444444444,1,code,Introduction,coreference_resolution9
282,282,282,63,We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 11), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02,62,0.525423729,1,0.034482759,1,hyperparameters,Implementation and Hyperparameters,coreference_resolution9
283,283,283,64,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 6), (7, 10), (10, 11), (11, 13), (13, 14), (15, 16), (16, 17), (17, 18), (20, 22), (22, 23), (23, 31), (31, 32), (32, 34), (34, 35), (36, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.04,63,0.533898305,2,0.068965517,1,hyperparameters,Implementation and Hyperparameters,coreference_resolution9
284,284,284,66,"We trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for BERT - base and BERT - large respectively .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 8), (8, 9), (9, 17), (19, 20), (20, 22), (22, 27), (27, 28), (29, 30), (30, 31), (31, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.08,65,0.550847458,4,0.137931034,1,hyperparameters,Implementation and Hyperparameters,coreference_resolution9
285,285,285,70,Paragraph Level : GAP,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",8,0.16,69,0.584745763,8,0.275862069,1,results,Implementation and Hyperparameters,coreference_resolution9
286,286,286,76,Table 2 shows that BERT improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (6, 11), (11, 12), (12, 17), (17, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.28,75,0.63559322,14,0.482758621,1,results,Implementation and Hyperparameters,coreference_resolution9
287,287,287,78,Document Level : OntoNotes,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O']","[(3, 4)]","['O', 'O', 'O', 'O']",16,0.32,77,0.652542373,16,0.551724138,1,experiments,Implementation and Hyperparameters,coreference_resolution9
288,288,288,83,shows that BERT - base offers an improvement of 0.9 % over the ELMo - based c2 fcoref model .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (5, 6), (7, 8), (8, 9), (9, 11), (11, 12), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.42,82,0.694915254,21,0.724137931,1,results,Implementation and Hyperparameters,coreference_resolution9
289,289,289,87,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .",Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (6, 7), (7, 12), (12, 13), (14, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5,86,0.728813559,25,0.862068966,1,results,Implementation and Hyperparameters,coreference_resolution9
290,290,290,88,We also observe that the overlap variant offers no improvement over independent .,Implementation and Hyperparameters,Implementation and Hyperparameters,coreference_resolution,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 7), (7, 8), (8, 10), (10, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.52,87,0.737288136,26,0.896551724,1,results,Implementation and Hyperparameters,coreference_resolution9
291,291,291,2,A Hierarchical Model for Data - to - Text Generation,title,title,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003649635,1,0,1,research-problem,title,data-to-text_generation0
292,292,292,4,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .",abstract,abstract,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.010948905,1,0.166666667,1,research-problem,abstract,data-to-text_generation0
293,293,293,36,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .",Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 13), (13, 14), (13, 15), (15, 16), (16, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.742857143,35,0.127737226,26,0.742857143,1,model,Introduction,data-to-text_generation0
294,294,294,37,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .",Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 6), (6, 7), (8, 11), (14, 15), (16, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.771428571,36,0.131386861,27,0.771428571,1,model,Introduction,data-to-text_generation0
295,295,295,39,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .",Introduction,Introduction,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (8, 9), (9, 10), (11, 15), (16, 18), (18, 20), (20, 24), (25, 26), (28, 29), (30, 32), (32, 36), (37, 38), (41, 42), (43, 45), (45, 46), (46, 52), (52, 54), (54, 56), (56, 57), (57, 61), (61, 64), (64, 66), (75, 76), (77, 80), (80, 82), (83, 85), (85, 87), (88, 89)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.828571429,38,0.138686131,29,0.828571429,1,model,Introduction,data-to-text_generation0
296,296,296,188,Baselines,,,data-to-text_generation,0,['O'],"[(0, 1)]",['O'],0,0,187,0.682481752,0,0,1,experiments,,data-to-text_generation0
297,297,297,191,Wiseman is a standard encoder - decoder system with copy mechanism .,Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.375,190,0.693430657,3,0.375,1,baselines,Baselines,data-to-text_generation0
298,298,298,192,"Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .",Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 7), (7, 8), (9, 12), (13, 14), (15, 18), (18, 19), (22, 24), (24, 26), (26, 28), (29, 30), (30, 31), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5,191,0.697080292,4,0.5,1,baselines,Baselines,data-to-text_generation0
299,299,299,193,"Puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .",Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 5), (5, 7), (9, 14), (14, 15), (16, 17), (22, 24), (26, 27), (29, 34), (34, 35), (35, 36), (36, 37), (38, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,192,0.700729927,5,0.625,1,baselines,Baselines,data-to-text_generation0
300,300,300,194,Puduppully - updt .,Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",6,0.75,193,0.704379562,6,0.75,1,baselines,Baselines,data-to-text_generation0
301,301,301,195,"It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .",Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 8), (9, 10), (11, 13), (13, 16), (16, 18), (18, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.875,194,0.708029197,7,0.875,1,baselines,Baselines,data-to-text_generation0
302,302,302,196,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .",Baselines,Baselines,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 9), (9, 10), (11, 12), (12, 14), (14, 15), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1,195,0.711678832,8,1,1,baselines,Baselines,data-to-text_generation0
303,303,303,208,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 10), (10, 11), (12, 14), (16, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.363636364,207,0.755474453,4,0.363636364,1,hyperparameters,Implementation details,data-to-text_generation0
304,304,304,209,We use dropout at rate 0.5 .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.454545455,208,0.759124088,5,0.454545455,1,hyperparameters,Implementation details,data-to-text_generation0
305,305,305,210,The models are trained with a batch size of 64 .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (8, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.545454545,209,0.762773723,6,0.545454545,1,hyperparameters,Implementation details,data-to-text_generation0
306,306,306,211,"We follow the training procedure in and train the model for a fixed number of 25 K updates , and average the weights of the last 5 checkpoints ( at every 1 K updates ) to ensure more stability across runs .",Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (7, 8), (9, 10), (10, 11), (12, 18), (20, 21), (22, 23), (23, 24), (25, 28), (35, 37), (37, 39), (39, 40), (40, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,210,0.766423358,7,0.636363636,1,hyperparameters,Implementation details,data-to-text_generation0
307,307,307,212,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .",Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (10, 13), (13, 14), (14, 15), (18, 20), (20, 21), (21, 22), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.727272727,211,0.770072993,8,0.727272727,1,hyperparameters,Implementation details,data-to-text_generation0
308,308,308,213,We used beam search with beam size of 5 during inference .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 7), (7, 8), (8, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.818181818,212,0.773722628,9,0.818181818,1,hyperparameters,Implementation details,data-to-text_generation0
309,309,309,214,All the models are implemented in Open NMT - py .,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.909090909,213,0.777372263,10,0.909090909,1,hyperparameters,Implementation details,data-to-text_generation0
310,310,310,215,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,Implementation details,Implementation details,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O']","[(5, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",11,1,214,0.781021898,11,1,1,code,Implementation details,data-to-text_generation0
311,311,311,226,"As shown in , we can see the lower results obtained by the Flat scenario compared to the other scenarios ( e.g. BLEU 16.7 vs. 17.5 for resp .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 10), (10, 12), (13, 15), (15, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.04,225,0.821167883,10,0.208333333,1,ablation-analysis,Ablation studies,data-to-text_generation0
312,312,312,229,"Second , the comparison between scenario Hierarchical - kv and Hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted CS - R% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 12), (12, 13), (14, 15), (15, 16), (17, 18), (18, 19), (20, 22), (22, 23), (24, 26), (26, 27), (27, 29), (33, 34), (34, 36), (36, 37), (37, 39), (39, 40), (40, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1,228,0.832116788,13,0.270833333,1,ablation-analysis,Ablation studies,data-to-text_generation0
313,313,313,244,"Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman , reinforcing the crucial role of structure in data semantics and saliency .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 7), (7, 8), (8, 10), (11, 13), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4,243,0.886861314,28,0.583333333,1,ablation-analysis,Ablation studies,data-to-text_generation0
314,314,314,251,Results shows that our Flat scenario obtains a significant higher BLEU score ( 16.7 vs. 14.5 ) and generates fluent descriptions with accurate mentions ( RG - P % ) thatare also included in the gold descriptions ( CS - R% ) .,Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 6), (6, 7), (8, 13), (13, 17), (18, 19), (19, 21), (21, 22), (22, 30), (32, 34), (35, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.54,250,0.912408759,35,0.729166667,1,ablation-analysis,Ablation studies,data-to-text_generation0
315,315,315,253,"Our hierarchical models outperform the two - step decoders of Li and Puduppully - plan on both BLEU and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .",Ablation studies,Ablation studies,data-to-text_generation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (5, 9), (9, 10), (10, 15), (15, 16), (17, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.58,252,0.919708029,37,0.770833333,1,ablation-analysis,Ablation studies,data-to-text_generation0
316,316,316,2,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation,title,title,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004424779,1,0,1,research-problem,title,data-to-text_generation1
317,317,317,4,Natural language generation lies at the core of generative dialogue systems and conversational agents .,abstract,abstract,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.013274336,1,0.25,1,research-problem,abstract,data-to-text_generation1
318,318,318,28,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .",Introduction,Located near,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9), (12, 16), (16, 20), (20, 21), (22, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.833333333,27,0.119469027,20,0.833333333,1,model,Introduction: Located near,data-to-text_generation1
319,319,319,29,"We explore novel ways to represent the MR inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .",Introduction,Located near,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 6), (7, 9), (10, 11), (11, 13), (13, 15), (20, 23), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.875,28,0.123893805,21,0.875,1,model,Introduction: Located near,data-to-text_generation1
320,320,320,158,We built our ensemble model using the seq2seq framework for TensorFlow .,Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.083333333,157,0.694690265,1,0.142857143,1,experimental-setup,Experimental Setup,data-to-text_generation1
321,321,321,159,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .",Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (6, 9), (9, 10), (10, 14), (17, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.166666667,158,0.699115044,2,0.285714286,1,experimental-setup,Experimental Setup,data-to-text_generation1
322,322,322,160,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,Experimental Setup,Experimental Setup,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (7, 12), (12, 13), (13, 18), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,159,0.703539823,3,0.428571429,1,experimental-setup,Experimental Setup,data-to-text_generation1
323,323,323,165,Experiments on the E2E Dataset,Experimental Setup,,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O']",8,0.666666667,164,0.725663717,0,0,1,experiments,Experimental Setup,data-to-text_generation1
324,324,324,170,Automatic Metric Evaluation,,,data-to-text_generation,1,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,169,0.747787611,0,0,1,experiments,,data-to-text_generation1
325,325,325,172,The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo - samples in the training set .,Automatic Metric Evaluation,Automatic Metric Evaluation,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (7, 12), (13, 15), (15, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,171,0.756637168,2,0.2,1,results,Automatic Metric Evaluation,data-to-text_generation1
326,326,326,180,"On the official E2E test set , our ensemble model performs comparably to the baseline model , TGen , in terms of automatic metrics ) .",Automatic Metric Evaluation,Automatic Metric Evaluation,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (7, 10), (10, 11), (11, 12), (12, 13), (14, 16), (17, 18), (19, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1,179,0.792035398,10,1,1,results,Automatic Metric Evaluation,data-to-text_generation1
327,327,327,210,Experiments on TV and Laptop Datasets,Experiments with Data Selection,,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O']","[(2, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",14,0.7,209,0.924778761,0,0,1,experiments,Experiments with Data Selection,data-to-text_generation1
328,328,328,212,"As shows , our ensemble model performs competitively with the baseline on the TV dataset , and it outperforms it on the Laptop dataset by a wide margin .",Experiments with Data Selection,Experiments on TV and Laptop Datasets,data-to-text_generation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (7, 8), (8, 9), (10, 11), (11, 12), (13, 15), (18, 19), (20, 21), (22, 24), (24, 25), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,211,0.933628319,2,0.333333333,1,results,Experiments with Data Selection: Experiments on TV and Laptop Datasets,data-to-text_generation1
329,329,329,2,Deep Graph Convolutional Encoders for Structured Data to Text Generation,title,title,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005235602,1,0,1,research-problem,title,data-to-text_generation2
330,330,330,4,Most previous work on neural text generation from graph - structured data relies on standard sequence - to - sequence methods .,abstract,abstract,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.015706806,1,0.2,1,research-problem,abstract,data-to-text_generation2
331,331,331,22,Most previous work casts the graph structured data to text generation task as a sequenceto - sequence problem .,Introduction,Introduction,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11), (12, 13), (14, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.149425287,21,0.109947644,13,0.419354839,1,research-problem,Introduction,data-to-text_generation2
332,332,332,131,Results,,,data-to-text_generation,2,['O'],[],['O'],0,0,130,0.680628272,0,0,1,experiments,,data-to-text_generation2
333,333,333,132,WebNLG task,Results,Results,data-to-text_generation,2,"['O', 'O']","[(0, 2)]","['O', 'O']",1,0.034482759,131,0.685863874,1,0.045454545,1,results,Results,data-to-text_generation2
334,334,334,134,"In this setting , the model with GCN encoder outperforms a strong baseline that employs the LSTM encoder , with .009 BLEU points .",Results,Results,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 9), (9, 10), (11, 13), (14, 15), (16, 18), (19, 20), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.103448276,133,0.696335079,3,0.136363636,1,results,Results,data-to-text_generation2
335,335,335,135,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,Results,Results,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (7, 8), (9, 10), (10, 11), (12, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.137931034,134,0.701570681,4,0.181818182,1,results,Results,data-to-text_generation2
336,336,336,137,The GCN EC model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points ; and MELBOURNE by .014 BLEU points .,Results,Results,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (7, 8), (9, 13), (15, 19), (19, 20), (20, 23), (25, 26), (26, 27), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.206896552,136,0.712041885,6,0.272727273,1,results,Results,data-to-text_generation2
337,337,337,139,SR11 Deep task,Results,Results,data-to-text_generation,2,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",8,0.275862069,138,0.722513089,8,0.363636364,1,results,Results,data-to-text_generation2
338,338,338,150,"We also compare the neural models with upper bound results on the same dataset by the pipeline model of The STUMBA - D and TBDIL model obtains respectively .794 and . 805 BLUE , outperforming the GCN - based model .",Results,Gold,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 10), (10, 11), (12, 14), (14, 15), (16, 18), (20, 26), (26, 27), (34, 35), (36, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.655172414,149,0.780104712,19,0.863636364,1,results,Results: Gold,data-to-text_generation2
339,339,339,163,The first thing we notice is the importance of skip connections between GCN layers .,Ablation Study,Ablation Study,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 9), (9, 11), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,162,0.848167539,2,0.4,1,ablation-analysis,Ablation Study,data-to-text_generation2
340,340,340,164,Residual and dense connections lead to similar results .,Ablation Study,Ablation Study,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,163,0.853403141,3,0.6,1,ablation-analysis,Ablation Study,data-to-text_generation2
341,341,341,165,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .",Ablation Study,Ablation Study,data-to-text_generation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (9, 10), (10, 11), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,164,0.858638743,4,0.8,1,ablation-analysis,Ablation Study,data-to-text_generation2
342,342,342,2,Pragmatically Informative Text Generation,title,,data-to-text_generation,3,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",1,0,1,0.007936508,1,0,1,research-problem,title,data-to-text_generation3
343,343,343,10,Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures .,Introduction,Introduction,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.023809524,9,0.071428571,1,0.04,1,research-problem,Introduction,data-to-text_generation3
344,344,344,13,"Our work builds on a line of learned Rational Speech Acts ( RSA ) models , in which generated strings are selected to optimize the behav - Human - written A cheap coffee shop in riverside with a 5 out of 5 customer rating is Fitzbillies .",Introduction,Introduction,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (7, 15), (18, 20), (21, 22), (25, 26), (36, 37), (38, 44), (45, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.095238095,12,0.095238095,4,0.16,1,approach,Introduction,data-to-text_generation3
345,345,345,26,"The canonical presentation of the RSA framework ( Frank and Goodman , 2012 ) is grounded in reference resolution : models of speakers attempt to describe referents in the presence of distractors , and models of listeners attempt to resolve descriptors to referents .",Introduction,Introduction,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (15, 17), (17, 19), (26, 27), (31, 32), (40, 41), (42, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.404761905,25,0.198412698,17,0.68,1,approach,Introduction,data-to-text_generation3
346,346,346,89,Abstractive Summarization,,,data-to-text_generation,3,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,88,0.698412698,0,0,1,experiments,,data-to-text_generation3
347,347,347,94,"The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .",Abstractive Summarization,Abstractive Summarization,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 10), (11, 13), (13, 14), (15, 19), (20, 21), (22, 28), (28, 29), (30, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.25,93,0.738095238,5,0.625,1,experiments,Abstractive Summarization,data-to-text_generation3
348,348,348,95,"SD 1 is strong across all metrics , obtaining results competitive to the best previous abstractive systems . ( b ) Coverage ratios by attribute type ( columns ) for the base model S0 , and for the pragmatic system SD 1 when constructing the distractor by masking the specified attribute ( rows ) .",Abstractive Summarization,Abstractive Summarization,data-to-text_generation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 4), (4, 5), (5, 7), (8, 9), (9, 10), (10, 12), (13, 17), (21, 23), (24, 26), (29, 30), (31, 34), (36, 37), (38, 42), (42, 44), (45, 46), (46, 48), (49, 51)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3,94,0.746031746,6,0.75,1,experiments,Abstractive Summarization,data-to-text_generation3
349,349,349,2,Data - to - Text Generation with Content Selection and Planning,title,title,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003322259,1,0,1,research-problem,title,data-to-text_generation4
350,350,350,20,Our model learns a content plan from the input and conditions on the content plan in order to generate the output document ( see for an illustration ) .,Introduction,Introduction,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (8, 11), (11, 12), (13, 15), (17, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.44,19,0.063122924,11,0.44,1,model,Introduction,data-to-text_generation4
351,351,351,22,"We train our model end - to - end using neural networks and evaluate its performance on ROTOWIRE , a recently released dataset which contains statistics of NBA basketball games paired with human - written summaries ( see ) .",Introduction,Introduction,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 9), (9, 10), (10, 12), (13, 14), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.52,21,0.069767442,13,0.52,1,model,Introduction,data-to-text_generation4
352,352,352,158,"We used one - layer pointer networks during content planning , and two - layer LSTMs during text generation .",Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 7), (7, 8), (8, 10), (12, 16), (16, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.333333333,157,0.521594684,16,0.727272727,1,hyperparameters,Training Configuration,data-to-text_generation4
353,353,353,159,Input feeding was employed for the text decoder .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.444444444,158,0.524916944,17,0.772727273,1,hyperparameters,Training Configuration,data-to-text_generation4
354,354,354,160,We applied dropout ) at a rate of 0.3 .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 5), (6, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.555555556,159,0.528239203,18,0.818181818,1,hyperparameters,Training Configuration,data-to-text_generation4
355,355,355,161,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .",Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 6), (6, 7), (8, 10), (12, 15), (15, 16), (16, 17), (18, 21), (22, 24), (24, 29), (31, 33), (34, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.666666667,160,0.531561462,19,0.863636364,1,hyperparameters,Training Configuration,data-to-text_generation4
356,356,356,162,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .",Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 8), (8, 9), (11, 12), (13, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.777777778,161,0.534883721,20,0.909090909,1,hyperparameters,Training Configuration,data-to-text_generation4
357,357,357,163,We set the beam size to 5 during inference .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.888888889,162,0.53820598,21,0.954545455,1,hyperparameters,Training Configuration,data-to-text_generation4
358,358,358,164,All models are implemented in Open NMT - py .,Training Configuration,Training Configuration,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,1,163,0.541528239,22,1,1,hyperparameters,Training Configuration,data-to-text_generation4
359,359,359,165,Results,,,data-to-text_generation,4,['O'],[],['O'],0,0,164,0.544850498,0,0,1,experiments,,data-to-text_generation4
360,360,360,179,"As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 8), (8, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.40625,178,0.591362126,14,0.179487179,1,results,Automatic Evaluation,data-to-text_generation4
361,361,361,180,"In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (5, 7), (7, 8), (9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4375,179,0.594684385,15,0.192307692,1,results,Automatic Evaluation,data-to-text_generation4
362,362,362,181,"Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (5, 6), (6, 13), (13, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.46875,180,0.598006645,16,0.205128205,1,results,Automatic Evaluation,data-to-text_generation4
363,363,363,182,"Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (12, 13), (14, 16), (16, 17), (17, 20), (18, 20), (20, 23), (23, 25), (26, 29), (30, 31), (30, 32), (31, 32), (32, 34), (35, 36), (36, 37), (37, 39), (40, 42), (42, 43), (44, 46), (48, 49), (49, 50), (50, 52)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5,181,0.601328904,17,0.217948718,1,results,Automatic Evaluation,data-to-text_generation4
364,364,364,184,"As far as the template - based system is concerned , we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics .",Automatic Evaluation,Automatic Evaluation,data-to-text_generation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (15, 16), (16, 21), (22, 23), (24, 25), (25, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5625,183,0.607973422,19,0.243589744,1,results,Automatic Evaluation,data-to-text_generation4
365,365,365,2,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation,title,title,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.5,1,0.003134796,1,0.5,1,research-problem,title,data-to-text_generation5
366,366,366,5,"Data - to - text generation can be conceptually divided into two parts : ordering and structuring the information ( planning ) , and generating fluent language describing the information ( realization ) .",abstract,abstract,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6), (14, 17), (20, 21), (24, 25), (25, 27), (27, 28), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.090909091,4,0.012539185,1,0.090909091,1,research-problem,abstract,data-to-text_generation5
367,367,367,49,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .",Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 11), (13, 14), (15, 17), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.804878049,48,0.150470219,33,0.804878049,1,model,Introduction,data-to-text_generation5
368,368,368,50,The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (8, 10), (10, 11), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.829268293,49,0.153605016,34,0.829268293,1,model,Introduction,data-to-text_generation5
369,369,369,52,"Once the plan is determined , 2 a neural generation system is used to transform it into fluent , natural language text .",Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (8, 11), (13, 15), (17, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.87804878,51,0.159874608,36,0.87804878,1,model,Introduction,data-to-text_generation5
370,370,370,57,We release our code and the corpus extended with matching plans in https://github.com/AmitMY/ chimera .,Introduction,Introduction,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,1,56,0.175548589,41,1,1,code,Introduction,data-to-text_generation5
371,371,371,208,"We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .",Compared Systems,Compared Systems,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10), (11, 12), (14, 20), (21, 22), (22, 23), (23, 24), (24, 26), (26, 27), (28, 30), (32, 35), (37, 43), (44, 45), (45, 46), (46, 47), (48, 50)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,207,0.648902821,2,0.285714286,1,baselines,Compared Systems,data-to-text_generation5
372,372,372,209,"Additionally , we developed an end - to - end neural baseline which outperforms the WebNLG neural systems .",Compared Systems,Compared Systems,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 12), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,208,0.652037618,3,0.428571429,1,baselines,Compared Systems,data-to-text_generation5
373,373,373,210,"It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .",Compared Systems,Compared Systems,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (7, 9), (9, 10), (10, 11), (13, 17), (19, 22), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3,209,0.655172414,4,0.571428571,1,baselines,Compared Systems,data-to-text_generation5
374,374,374,213,6 Experiments and Results,Compared Systems,Compared Systems,data-to-text_generation,5,"['O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O']",6,0.6,212,0.664576803,7,1,1,results,Compared Systems,data-to-text_generation5
375,375,375,217,Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics,Compared Systems,Automatic Metrics,data-to-text_generation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 7), (7, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1,216,0.677115987,3,1,1,results,Compared Systems: Automatic Metrics,data-to-text_generation5
376,376,376,2,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation,title,title,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005586592,1,0,1,research-problem,title,data-to-text_generation6
377,377,377,4,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .",abstract,abstract,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.016759777,1,0.125,1,research-problem,abstract,data-to-text_generation6
378,378,378,8,"Moreover , since characters constitute the common "" building blocks "" of every text , it also allows a more general approach to text generation , enabling the possibility to exploit transfer learning for training .",abstract,abstract,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (17, 18), (19, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,7,0.039106145,5,0.625,1,model,abstract,data-to-text_generation6
379,379,379,23,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 16), (17, 26), (26, 27), (27, 29), (30, 32), (33, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.611111111,22,0.122905028,11,0.611111111,1,model,Introduction,data-to-text_generation6
380,380,380,24,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 14), (15, 16), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.666666667,23,0.12849162,12,0.666666667,1,model,Introduction,data-to-text_generation6
381,381,381,25,"As we will see , such an approach achieves rather interesting performance results and produces a vocabulary - free model that is inherently more general , as it does not depend on a specific domain 's set of terms , but rather on a general alphabet .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 13), (14, 15), (16, 20), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.722222222,24,0.134078212,13,0.722222222,1,model,Introduction,data-to-text_generation6
382,382,382,27,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .",Introduction,Introduction,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 9), (14, 20), (27, 32), (33, 35), (36, 38), (38, 39), (39, 43), (47, 48), (48, 50), (50, 56), (62, 65), (67, 68), (69, 72), (73, 74), (74, 75), (77, 79), (80, 81), (82, 86)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.833333333,26,0.145251397,15,0.833333333,1,model,Introduction,data-to-text_generation6
383,383,383,105,"We developed our system using the PyTorch framework 2 , release 0.4.1 3 .",Implementation Details,Implementation Details,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.076923077,104,0.581005587,1,0.076923077,1,experimental-setup,Implementation Details,data-to-text_generation6
384,384,384,108,"We minimize the negative log - likelihood loss using teacher forcing and Adam , the latter being an optimizer that computes individual adaptive learning rates .",Implementation Details,Implementation Details,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 8), (8, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.307692308,107,0.597765363,4,0.307692308,1,experimental-setup,Implementation Details,data-to-text_generation6
385,385,385,118,Results and Discussion,,,data-to-text_generation,6,"['O', 'O', 'O']",[],"['O', 'O', 'O']",0,0,117,0.653631285,0,0,1,experiments,,data-to-text_generation6
386,386,386,137,"A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (11, 14), (14, 17), (17, 18), (18, 19), (20, 24), (26, 33), (33, 34), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.38,136,0.759776536,19,0.38,1,results,Results and Discussion,data-to-text_generation6
387,387,387,138,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 8), (9, 10), (10, 11), (11, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4,137,0.765363128,20,0.4,1,results,Results and Discussion,data-to-text_generation6
388,388,388,140,"A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (10, 13), (13, 15), (15, 18), (18, 20), (20, 21), (22, 24), (24, 25), (26, 30), (38, 39), (39, 40), (43, 44), (44, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.44,139,0.776536313,22,0.44,1,results,Results and Discussion,data-to-text_generation6
389,389,389,141,"Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (6, 8), (8, 9), (9, 13), (13, 16), (16, 19), (20, 22), (23, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.46,140,0.782122905,23,0.46,1,results,Results and Discussion,data-to-text_generation6
390,390,390,142,"Finally , the baseline model , EDA , is largely outperformed by all other examined methods .",Results and Discussion,Results and Discussion,data-to-text_generation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 7), (9, 12), (12, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.48,141,0.787709497,24,0.48,1,results,Results and Discussion,data-to-text_generation6
391,391,391,2,An improved neural network model for joint POS tagging and dependency parsing,title,title,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005882353,1,0,1,research-problem,title,dependency_parsing0
392,392,392,4,We propose a novel neural network model for joint part - of - speech ( POS ) tagging and dependency parsing .,abstract,abstract,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 8), (8, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.017647059,1,0.166666667,1,research-problem,abstract,dependency_parsing0
393,393,393,9,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,abstract,abstract,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,8,0.047058824,6,1,1,code,abstract,dependency_parsing0
394,394,394,11,"Dependency parsing - a key research topic in natural language processing ( NLP ) in the last decade ) - has also been demonstrated to be extremely useful in many applications such as relation extraction , semantic parsing and machine translation ) .",Introduction,Introduction,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.076923077,10,0.058823529,1,0.076923077,1,research-problem,Introduction,dependency_parsing0
395,395,395,18,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .",Introduction,Introduction,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13), (13, 14), (14, 16), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.615384615,17,0.1,8,0.615384615,1,model,Introduction,dependency_parsing0
396,396,396,19,Our joint model extends the well - known BIST graph - based dependency parser with an additional lower - level BiLSTM - based tagging component .,Introduction,Introduction,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 14), (14, 15), (16, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.692307692,18,0.105882353,9,0.692307692,1,model,Introduction,dependency_parsing0
397,397,397,75,Our jPTDP v 2.0 is implemented using DYNET v2.0 with a fixed random seed .,Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 9), (9, 10), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.033333333,74,0.435294118,3,0.176470588,1,experimental-setup,Implementation details,dependency_parsing0
398,398,398,77,"Word embeddings are initialized either randomly or by pre-trained word vectors , while character and POS tag embeddings are randomly initialized .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (8, 11), (13, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.055555556,76,0.447058824,5,0.294117647,1,experimental-setup,Implementation details,dependency_parsing0
399,399,399,79,We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 9), (9, 10), (11, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.077777778,78,0.458823529,7,0.411764706,1,experimental-setup,Implementation details,dependency_parsing0
400,400,400,80,"Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 8), (8, 10), (11, 12), (12, 13), (13, 15), (17, 18), (18, 22), (28, 29), (30, 32), (32, 33), (34, 39), (39, 40), (40, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.088888889,79,0.464705882,8,0.470588235,1,experimental-setup,Implementation details,dependency_parsing0
401,401,401,82,"We optimize the objective loss using Adam ( Kingma and Ba , 2014 ) with an initial learning rate at 0.001 and no mini-batches .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 14), (14, 15), (16, 19), (19, 20), (20, 21), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.111111111,81,0.476470588,10,0.588235294,1,experimental-setup,Implementation details,dependency_parsing0
402,402,402,86,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .",Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (10, 15), (16, 21), (22, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.155555556,85,0.5,14,0.823529412,1,experimental-setup,Implementation details,dependency_parsing0
403,403,403,87,We also fix the number of hidden nodes in MLPs at 100 .,Implementation details,Implementation details,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8), (8, 9), (9, 10), (10, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.166666667,86,0.505882353,15,0.882352941,1,experimental-setup,Implementation details,dependency_parsing0
404,404,404,99,"Clearly , our model produces very competitive parsing results .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3,98,0.576470588,9,0.18,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
405,405,405,100,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 10), (10, 12), (14, 16), (16, 17), (17, 19), (29, 30), (30, 34), (34, 35), (36, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.311111111,99,0.582352941,10,0.2,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
406,406,406,102,We achieve 0.9 % lower parsing scores than the state - of - the - art dependency parser of .,Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 7), (7, 8), (9, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.333333333,101,0.594117647,12,0.24,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
407,407,407,106,"We also obtain a state - of - the - art POS tagging accuracy at 97.97 % on the test Section 23 , which is about 0.4 + % higher than those by , and .",Implementation details,Experiments on English Penn treebank,dependency_parsing,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 14), (14, 15), (15, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.377777778,105,0.617647059,16,0.32,1,experiments,Implementation details: Experiments on English Penn treebank,dependency_parsing0
408,408,408,2,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,title,title,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003194888,1,0,1,research-problem,title,dependency_parsing1
409,409,409,24,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (9, 10), (12, 14), (14, 15), (17, 18), (18, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.576923077,23,0.073482428,15,0.576923077,1,approach,Introduction,dependency_parsing1
410,410,410,26,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (10, 11), (12, 13), (13, 14), (15, 18), (21, 22), (23, 25), (29, 31), (32, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.653846154,25,0.079872204,17,0.653846154,1,approach,Introduction,dependency_parsing1
411,411,411,27,"Crucially , the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (8, 12), (14, 16), (17, 20), (20, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.692307692,26,0.083067093,18,0.692307692,1,approach,Introduction,dependency_parsing1
412,412,412,29,"We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures , transition - based ( Section 4 ) as well as a graph - based ( Section 5 ) .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (10, 13), (13, 14), (14, 17), (18, 21), (29, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.769230769,28,0.089456869,20,0.769230769,1,approach,Introduction,dependency_parsing1
413,413,413,30,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .",Introduction,Introduction,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 8), (9, 13), (13, 16), (17, 18), (19, 20), (20, 21), (21, 22), (23, 25), (31, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.807692308,29,0.092651757,21,0.807692308,1,approach,Introduction,dependency_parsing1
414,414,414,281,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .",Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 8), (9, 11), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.27027027,280,0.89456869,10,0.27027027,1,hyperparameters,Experiments and Results,dependency_parsing1
415,415,415,282,The code is available at the github repository https://github.com/elikip / bist -parser .,Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.297297297,281,0.897763578,11,0.297297297,1,code,Experiments and Results,dependency_parsing1
416,416,416,283,"We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .",Experiments and Results,Experiments and Results,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (7, 8), (10, 11), (10, 12), (11, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.324324324,282,0.900958466,12,0.324324324,1,hyperparameters,Experiments and Results,dependency_parsing1
417,417,417,299,"It is clear that our parsers are very competitive , despite using very simple parsing architectures and minimal feature extractors .",Experiments and Results,Hyperparameter Tuning,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 6), (6, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.756756757,298,0.952076677,28,0.756756757,1,results,Experiments and Results: Hyperparameter Tuning,dependency_parsing1
418,418,418,300,"When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .",Experiments and Results,Hyperparameter Tuning,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (7, 14), (14, 15), (15, 17), (17, 18), (18, 21), (21, 24), (24, 26), (27, 28), (29, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.783783784,299,0.955271565,29,0.783783784,1,results,Experiments and Results: Hyperparameter Tuning,dependency_parsing1
419,419,419,301,"The greedy transition based parser with 4 features also matches or outperforms most other parsers , including the beam - based transition parser with heavily engineered features of and the Stack - LSTM parser of , as well as the same parser when trained using a dynamic oracle .",Experiments and Results,Hyperparameter Tuning,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 8), (9, 12), (12, 15), (16, 17), (18, 23), (23, 24), (24, 27), (30, 34), (40, 42), (43, 45), (46, 48)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.810810811,300,0.958466454,30,0.810810811,1,results,Experiments and Results: Hyperparameter Tuning,dependency_parsing1
420,420,420,302,Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,Experiments and Results,Hyperparameter Tuning,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 8), (8, 9), (10, 17), (17, 19), (19, 21), (21, 22), (22, 23), (23, 24), (25, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.837837838,301,0.961661342,31,0.837837838,1,results,Experiments and Results: Hyperparameter Tuning,dependency_parsing1
421,421,421,303,"Interestingly , when adding external word embeddings the accuracy of the graph - based parser degrades .",Experiments and Results,Hyperparameter Tuning,dependency_parsing,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 7), (8, 9), (9, 10), (11, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.864864865,302,0.96485623,32,0.864864865,1,results,Experiments and Results: Hyperparameter Tuning,dependency_parsing1
422,422,422,14,"We give a probabilistic interpretation to the ensemble parser ( with a minor modification ) , viewing it as an instance of minimum Bayes risk inference .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 9), (16, 19), (20, 21), (21, 22), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.075471698,13,0.060185185,4,0.285714286,1,model,Introduction,dependency_parsing2
423,423,423,18,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 10), (10, 11), (12, 15), (15, 16), (16, 18), (18, 20), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.150943396,17,0.078703704,8,0.571428571,1,model,Introduction,dependency_parsing2
424,424,424,19,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 9), (9, 12), (12, 13), (14, 19), (21, 22), (23, 24), (24, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.169811321,18,0.083333333,9,0.642857143,1,model,Introduction,dependency_parsing2
425,425,425,22,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .",Introduction,Introduction,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 12), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.226415094,21,0.097222222,12,0.857142857,1,research-problem,Introduction,dependency_parsing2
426,426,426,176,"First , consider the neural FOG parser trained with Hamming cost ( C H in the second - to - last row ) .",Hyperparameters .,All scores are shown in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.485714286,175,0.810185185,24,0.571428571,1,ablation-analysis,Hyperparameters .: All scores are shown in .,dependency_parsing2
427,427,427,177,"This is a very strong benchmark , outperforming many higherorder graph - based and neural network models on all three datasets .",Hyperparameters .,All scores are shown in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (7, 8), (8, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.514285714,176,0.814814815,25,0.595238095,1,results,Hyperparameters .: All scores are shown in .,dependency_parsing2
428,428,428,178,"Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .",Hyperparameters .,All scores are shown in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 9), (9, 10), (10, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.542857143,177,0.819444444,26,0.619047619,1,results,Hyperparameters .: All scores are shown in .,dependency_parsing2
429,429,429,179,"For English , we see that this model comes close to the slower ensemble it was trained to simulate .",Hyperparameters .,All scores are shown in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 6), (8, 11), (12, 14), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.571428571,178,0.824074074,27,0.642857143,1,results,Hyperparameters .: All scores are shown in .,dependency_parsing2
430,430,430,180,"For Chinese , it achieves the best published scores , for German the best published UAS scores , and just after Bohnet and Nivre ( 2012 ) for LAS .",Hyperparameters .,All scores are shown in .,dependency_parsing,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (6, 9), (10, 11), (11, 12), (13, 17), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6,179,0.828703704,28,0.666666667,1,results,Hyperparameters .: All scores are shown in .,dependency_parsing2
431,431,431,2,From POS tagging to dependency parsing for biomedical event extraction,title,title,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003846154,1,0,1,research-problem,title,dependency_parsing3
432,432,432,12,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.,abstract,,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,1,11,0.042307692,9,0.375,1,code,abstract,dependency_parsing3
433,433,433,31,"In this paper , we therefore investigate current stateof - the - art ( SOTA ) approaches to dependency parsing as applied to biomedical texts .",Background,Retrained parser,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.857142857,30,0.115384615,3,0.5,0,research-problem,Background: Retrained parser,dependency_parsing3
434,434,434,33,"Finally , we study the impact of parser choice on biomedical event extraction , following the structure of the extrinsic parser evaluation shared task ( EPE 2017 ) for biomedical event extraction .",Background,Retrained parser,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (9, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.952380952,32,0.123076923,5,0.833333333,0,model,Background: Retrained parser,dependency_parsing3
435,435,435,97,"For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 9), (10, 13), (14, 15), (16, 19), (20, 21), (21, 24), (26, 27), (27, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.24,96,0.369230769,21,0.525,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
436,436,436,99,"For the traditional feature - based models MarMoT , NLP4J - POS and NLP4J - dep , we use their original pure Java implementations with default hyperparameter settings .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7), (7, 8), (9, 12), (13, 16), (18, 19), (20, 24), (24, 25), (25, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.32,98,0.376923077,23,0.575,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
437,437,437,100,"For the BiLSTM - CRF - based models , we use default hyper - parameters provided in with the following exceptions : for training , we use Nadam and run for 50 epochs .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 8), (10, 11), (11, 15), (22, 23), (23, 24), (26, 27), (27, 28), (29, 31), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.36,99,0.380769231,24,0.6,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
438,438,438,103,"For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 7), (8, 10), (10, 11), (11, 16), (21, 23), (23, 24), (24, 39), (40, 41), (41, 43), (43, 44), (45, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.48,102,0.392307692,27,0.675,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
439,439,439,104,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 10), (11, 12), (13, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.52,103,0.396153846,28,0.7,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
440,440,440,105,"We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .",Implementation details,The metric for POS tagging is the accuracy .,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8), (8, 9), (9, 10), (11, 12), (13, 17), (17, 18), (18, 20), (20, 21), (21, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.56,104,0.4,29,0.725,1,experimental-setup,Implementation details: The metric for POS tagging is the accuracy .,dependency_parsing3
441,441,441,118,POS tagging results,,,dependency_parsing,3,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,117,0.45,0,0,1,results,,dependency_parsing3
442,442,442,120,"BiLSTM - CRF and Mar - MoT obtain the lowest scores on GENIA and CRAFT , respectively .",POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7), (7, 8), (9, 11), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.095238095,119,0.457692308,2,0.095238095,1,results,POS tagging results,dependency_parsing3
443,443,443,121,jPTDP obtains a similar score to Mar - MoT on GENIA and similar score to BiLSTM - CRF on CRAFT .,POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 5), (5, 6), (6, 9), (9, 10), (10, 11), (12, 14), (14, 15), (15, 18), (18, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.142857143,120,0.461538462,3,0.142857143,1,results,POS tagging results,dependency_parsing3
444,444,444,122,"In particular , MarMoT obtains accuracy results at 98.61 % and 97.07 % on GENIA and CRAFT , which are about 0.2 % and 0.4 % absolute lower than NLP4J - POS , respectively .",POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (5, 7), (7, 8), (8, 13), (13, 14), (14, 17), (28, 29), (29, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.19047619,121,0.465384615,4,0.19047619,1,results,POS tagging results,dependency_parsing3
445,445,445,124,BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .,POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 5), (5, 6), (6, 8), (8, 9), (9, 12), (13, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.285714286,123,0.473076923,6,0.285714286,1,results,POS tagging results,dependency_parsing3
446,446,446,130,"Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .",POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 13), (14, 15), (16, 19), (19, 20), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.571428571,129,0.496153846,12,0.571428571,1,results,POS tagging results,dependency_parsing3
447,447,447,135,"On both GENIA and CRAFT , BiLSTM - CRF with character - level word embeddings obtains the highest accuracy scores .",POS tagging results,POS tagging results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (6, 9), (9, 10), (10, 15), (15, 16), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.80952381,134,0.515384615,17,0.80952381,1,results,POS tagging results,dependency_parsing3
448,448,448,140,Overall dependency parsing results,,,dependency_parsing,3,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",0,0,139,0.534615385,0,0,1,results,,dependency_parsing3
449,449,449,148,"On GENIA , among pre-trained models , BLLIP obtains highest results .",Overall dependency parsing results,Overall dependency parsing results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 4), (4, 6), (7, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.615384615,147,0.565384615,8,0.615384615,1,results,Overall dependency parsing results,dependency_parsing3
450,450,450,150,The pre-trained Stanford - Biaffine ( v1 ) model produces lower scores than the pre-trained Stanford - NNdep model on GENIA .,Overall dependency parsing results,Overall dependency parsing results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 9), (9, 10), (10, 12), (12, 13), (14, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.769230769,149,0.573076923,10,0.769230769,1,results,Overall dependency parsing results,dependency_parsing3
451,451,451,152,Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .,Overall dependency parsing results,Overall dependency parsing results,dependency_parsing,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 8), (8, 10), (10, 14), (14, 16), (17, 21), (24, 27), (27, 28), (28, 30), (32, 37), (37, 38), (38, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.923076923,151,0.580769231,12,0.923076923,1,results,Overall dependency parsing results,dependency_parsing3
452,452,452,2,Stack - Pointer Networks for Dependency Parsing,title,,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004255319,1,0,1,research-problem,title,dependency_parsing4
453,453,453,25,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 12), (12, 14), (15, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.652173913,24,0.10212766,15,0.652173913,1,model,Introduction,dependency_parsing4
454,454,454,26,"STACKPTR is a transition - based architecture , with the corresponding asymptotic efficiency , but still maintains a global view of the sentence that proves essential for achieving competitive accuracy .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 7), (8, 9), (11, 13), (16, 17), (18, 20), (20, 21), (22, 23), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.695652174,25,0.106382979,16,0.695652174,1,model,Introduction,dependency_parsing4
455,455,455,27,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (7, 8), (9, 10), (13, 15), (16, 18), (18, 20), (21, 22), (22, 23), (23, 25), (25, 26), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.739130435,26,0.110638298,17,0.739130435,1,model,Introduction,dependency_parsing4
456,456,456,28,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .",Introduction,Introduction,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (7, 15), (16, 17), (17, 19), (21, 22), (23, 24), (24, 26), (27, 28), (28, 29), (30, 31), (31, 32), (33, 35), (36, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608696,27,0.114893617,18,0.782608696,1,model,Introduction,dependency_parsing4
457,457,457,158,"For fair comparison of the parsing performance , we re-implemented the graph - based Deep Biaffine ( BIAF ) parser , which achieved state - of - the - art results on a wide range of languages .",Baseline,Baseline,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.5,157,0.668085106,12,0.923076923,1,baselines,Baseline,dependency_parsing4
458,458,458,160,Main Results,,,dependency_parsing,4,"['O', 'O']",[],"['O', 'O']",0,0,159,0.676595745,0,0,1,experiments,,dependency_parsing4
459,459,459,165,"On UAS and LAS , the Full variation of STACKPTR with decoding beam size 10 outperforms BIAF on Chinese , and obtains competitive performance on English and German .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 10), (10, 11), (11, 15), (15, 16), (16, 17), (17, 18), (18, 19), (21, 22), (22, 24), (24, 25), (25, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.098039216,164,0.69787234,5,0.294117647,1,results,Main Results,dependency_parsing4
460,460,460,166,"An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 9), (10, 12), (12, 13), (13, 16), (18, 19), (19, 21), (21, 22), (22, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.117647059,165,0.70212766,6,0.352941176,1,results,Main Results,dependency_parsing4
461,461,461,168,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (5, 6), (6, 8), (8, 9), (9, 10), (10, 12), (13, 14), (15, 16), (19, 20), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.156862745,167,0.710638298,8,0.470588235,1,results,Main Results,dependency_parsing4
462,462,462,169,The results of our parser on RA are slightly worse than BIAF .,Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.176470588,168,0.714893617,9,0.529411765,1,results,Main Results,dependency_parsing4
463,463,463,175,"re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 4), (4, 6), (6, 7), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.294117647,174,0.740425532,15,0.882352941,1,results,Main Results,dependency_parsing4
464,464,464,176,"Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 11), (11, 12), (13, 16), (16, 17), (17, 18), (20, 22), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.31372549,175,0.744680851,16,0.941176471,1,results,Main Results,dependency_parsing4
465,465,465,177,"On German , the performance is competitive with BIAF , and significantly better than other models .",Main Results,Main Results,dependency_parsing,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 6), (6, 8), (8, 9), (11, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.333333333,176,0.74893617,17,1,1,results,Main Results,dependency_parsing4
466,466,466,2,Structured Training for Neural Network Transition - Based Parsing,title,,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003508772,1,0,1,research-problem,title,dependency_parsing5
467,467,467,4,We present structured perceptron training for neural network transition - based dependency parsing .,abstract,abstract,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.010526316,1,0.2,1,research-problem,abstract,dependency_parsing5
468,468,468,11,"Lately , dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages and the efficiency of dependency parsers .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.043478261,10,0.035087719,2,0.043478261,1,research-problem,Introduction,dependency_parsing5
469,469,469,13,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 7), (8, 10), (11, 16), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.086956522,12,0.042105263,4,0.086956522,1,model,Introduction,dependency_parsing5
470,470,470,24,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 10), (10, 12), (12, 13), (14, 16), (16, 18), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.326086957,23,0.080701754,15,0.326086957,1,model,Introduction,dependency_parsing5
471,471,471,29,"As in prior work , we train the neural network to model the probability of individual parse actions .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 10), (10, 12), (13, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.434782609,28,0.098245614,20,0.434782609,1,model,Introduction,dependency_parsing5
472,472,472,31,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 6), (6, 7), (7, 9), (9, 10), (11, 13), (13, 14), (15, 16), (16, 17), (18, 21), (23, 25), (25, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.47826087,30,0.105263158,22,0.47826087,1,model,Introduction,dependency_parsing5
473,473,473,35,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (9, 14), (14, 16), (16, 18), (18, 19), (19, 22), (23, 24), (26, 27), (30, 32), (32, 33), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.565217391,34,0.119298246,26,0.565217391,1,model,Introduction,dependency_parsing5
474,474,474,36,"This approach is known as "" tri-training "" and we show that it benefits our neural network parser significantly more than other approaches .",Introduction,Introduction,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (6, 7), (13, 14), (14, 18), (18, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.586956522,35,0.122807018,27,0.586956522,1,model,Introduction,dependency_parsing5
475,475,475,200,We used the publicly available word2vec 2 tool to learn CBOW embeddings following the sample configuration provided with the tool .,Model Initialization & Hyperparameters,Model Initialization & Hyperparameters,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 8), (8, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.25,199,0.698245614,6,0.25,1,hyperparameters,Model Initialization & Hyperparameters,dependency_parsing5
476,476,476,219,Results,,,dependency_parsing,5,['O'],[],['O'],0,0,218,0.764912281,0,0,1,experiments,,dependency_parsing5
477,477,477,222,"The highest of these is , with a reported accuracy of 94.22 % UAS .",Results,Results,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 10), (10, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.06,221,0.775438596,3,0.75,1,results,Results,dependency_parsing5
478,478,478,223,"Even though the UAS is not directly comparable , it is typically similar , and this suggests that our model is competitive with some of the highest reported accuries for dependencies on WSJ .",Results,Results,dependency_parsing,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(18, 20), (21, 22), (22, 23), (23, 29), (29, 30), (30, 32), (32, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.08,222,0.778947368,4,1,1,results,Results,dependency_parsing5
479,479,479,2,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING,title,title,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00877193,1,0,1,research-problem,title,dependency_parsing6
480,480,480,13,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .",INTRODUCTION,INTRODUCTION,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (13, 15), (15, 17), (19, 20), (21, 22), (22, 23), (24, 25), (26, 27), (27, 29), (31, 32), (33, 39), (40, 43), (43, 44), (44, 46), (65, 66), (68, 70), (73, 74)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.043956044,12,0.105263158,4,0.666666667,1,model,INTRODUCTION,dependency_parsing6
481,481,481,96,"We choose to optimize with Adam , which ( among other things ) keeps a moving average of the L 2 norm of the gradient for each parameter throughout training and divides the gradient for each parameter by this moving average , ensuring that the magnitude of the gradients will on average be close to one .",INTRODUCTION,OPTIMIZER,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (3, 4), (5, 6), (13, 14), (15, 17), (17, 18), (19, 22), (22, 23), (24, 25), (25, 26), (26, 28), (28, 29), (29, 30), (31, 32), (33, 34), (34, 35), (35, 37), (37, 38), (39, 41), (42, 43), (45, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.956043956,95,0.833333333,1,0.2,1,hyperparameters,INTRODUCTION: OPTIMIZER,dependency_parsing6
482,482,482,102,"Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .",RESULTS,RESULTS,dependency_parsing,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 8), (8, 9), (9, 13), (13, 14), (15, 18), (28, 29), (29, 32), (32, 33), (33, 36), (39, 41), (41, 42), (42, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,101,0.885964912,1,0.125,1,results,RESULTS,dependency_parsing6
483,483,483,5,"This form of training , which accounts for model predictions at training time , improves parsing accuracies .",abstract,abstract,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (10, 11), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.666666667,4,0.038834951,2,0.666666667,1,research-problem,abstract,dependency_parsing7
484,484,484,8,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures ; this formalization is known as transitionbased parsing , and is often coupled with a greedy search procedure .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.043478261,7,0.067961165,1,0.076923077,1,research-problem,Introduction,dependency_parsing7
485,485,485,9,"The literature on transition - based parsing is vast , but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7), (17, 19), (20, 23), (23, 24), (24, 25), (26, 29), (31, 32), (33, 35), (37, 39), (40, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.086956522,8,0.077669903,2,0.153846154,1,research-problem,Introduction,dependency_parsing7
486,486,486,17,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (11, 13), (13, 15), (15, 16), (20, 22), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.434782609,16,0.155339806,10,0.769230769,1,model,Introduction,dependency_parsing7
487,487,487,18,"To do so , we use the method of to dynamically chose an optimal ( relative to the final attachment accuracy ) action given an imperfect history .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (9, 12), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.47826087,17,0.165048544,11,0.846153846,1,model,Introduction,dependency_parsing7
488,488,488,19,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .",Introduction,Introduction,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 7), (8, 9), (11, 13), (14, 16), (17, 20), (20, 21), (21, 23), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.52173913,18,0.174757282,12,0.923076923,1,model,Introduction,dependency_parsing7
489,489,489,76,Experiments,,,dependency_parsing,7,['O'],"[(0, 1)]",['O'],0,0,75,0.72815534,0,0,1,experiments,,dependency_parsing7
490,490,490,78,The score achieved by the dynamic oracle for English is 93.56 UAS .,Experiments,Experiments,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 7), (7, 8), (8, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.133333333,77,0.747572816,2,0.133333333,1,results,Experiments,dependency_parsing7
491,491,491,80,"Moreover , the Chinese score establishes the state - of - the - art , using the same settings as Chen and Manning ( 2014 ) .",Experiments,Experiments,dependency_parsing,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (7, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.266666667,79,0.766990291,4,0.266666667,1,results,Experiments,dependency_parsing7
492,492,492,2,Globally Normalized Transition - Based Neural Networks,title,,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004219409,1,0,1,research-problem,title,dependency_parsing8
493,493,493,4,"We introduce a globally normalized transition - based neural network model that achieves state - of - the - art part - ofspeech tagging , dependency parsing and sentence compression results .",abstract,abstract,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 11), (12, 13), (13, 24), (25, 27), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.333333333,3,0.012658228,1,0.333333333,1,research-problem,abstract,dependency_parsing8
494,494,494,11,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (4, 6), (6, 11), (11, 12), (12, 14), (14, 16), (16, 20), (20, 21), (21, 22), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2,10,0.042194093,4,0.2,1,model,Introduction,dependency_parsing8
495,495,495,12,"Our model , described in detail in Section 2 , uses a transition system and feature embeddings as introduced by * On leave from Columbia University ..",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.25,11,0.046413502,5,0.25,1,model,Introduction,dependency_parsing8
496,496,496,13,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 13), (13, 15), (16, 17), (17, 19), (19, 20), (21, 28), (28, 30), (31, 34), (35, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3,12,0.050632911,6,0.3,1,model,Introduction,dependency_parsing8
497,497,497,14,"Since we use beam inference , we approximate the partition function by summing over the elements in the beam , and use early updates .",Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (7, 8), (9, 11), (11, 12), (12, 14), (15, 16), (16, 17), (18, 19), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,13,0.054852321,7,0.35,1,model,Introduction,dependency_parsing8
498,498,498,15,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,Introduction,Introduction,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (6, 9), (10, 11), (11, 14), (14, 15), (15, 19), (19, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4,14,0.05907173,8,0.4,1,model,Introduction,dependency_parsing8
499,499,499,149,Experiments,,,dependency_parsing,8,['O'],"[(0, 1)]",['O'],0,0,148,0.624472574,0,0,1,experiments,,dependency_parsing8
500,500,500,158,Part of Speech Tagging,Experiments,,dependency_parsing,8,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",9,0.6,157,0.662447257,0,0,1,experiments,Experiments,dependency_parsing8
501,501,501,169,Results .,,,dependency_parsing,8,"['O', 'O']",[],"['O', 'O']",0,0,168,0.708860759,11,0.366666667,1,experiments,,dependency_parsing8
502,502,502,171,Our globally normalized model again significantly outperforms the local model .,Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,170,0.717299578,13,0.433333333,1,results,Results .,dependency_parsing8
503,503,503,172,Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5 .,Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 7), (7, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,171,0.721518987,14,0.466666667,1,results,Results .,dependency_parsing8
504,504,504,186,"Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .",The Results .,The Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4), (5, 8), (8, 9), (10, 11), (13, 14), (14, 16), (17, 19), (20, 23), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,185,0.780590717,28,0.933333333,1,results,The Results .,dependency_parsing8
505,505,505,187,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .",The Results .,The Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 6), (6, 7), (7, 9), (10, 11), (11, 13), (13, 14), (15, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,186,0.784810127,29,0.966666667,1,results,The Results .,dependency_parsing8
506,506,506,189,Dependency Parsing,The Results .,,dependency_parsing,8,"['O', 'O']","[(0, 2)]","['O', 'O']",9,0.9,188,0.793248945,0,0,1,experiments,The Results .,dependency_parsing8
507,507,507,191,Results .,,,dependency_parsing,8,"['O', 'O']",[],"['O', 'O']",0,0,190,0.801687764,2,0.4,1,experiments,,dependency_parsing8
508,508,508,192,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .",Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10), (12, 13), (14, 21), (23, 24), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.111111111,191,0.805907173,3,0.6,1,results,Results .,dependency_parsing8
509,509,509,194,Our results also significantly outperform the LSTM - based approaches of .,Results .,Results .,dependency_parsing,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.333333333,193,0.814345992,5,1,1,results,Results .,dependency_parsing8
510,510,510,2,Bag of Tricks for Efficient Text Classification,title,title,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.010752688,1,0,1,research-problem,title,document_classification0
511,511,511,14,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",Introduction,Introduction,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (8, 9), (11, 12), (12, 15), (15, 16), (17, 20), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.777777778,13,0.139784946,7,0.777777778,1,model,Introduction,document_classification0
512,512,512,15,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",Introduction,Introduction,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (14, 16), (16, 17), (18, 20), (22, 25), (26, 28), (29, 31), (31, 32), (32, 34), (36, 37), (37, 38), (38, 41), (42, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.888888889,14,0.150537634,8,0.888888889,1,research-problem,Introduction,document_classification0
513,513,513,53,Sentiment analysis,Experiments,,document_classification,0,"['O', 'O']","[(0, 2)]","['O', 'O']",5,1,52,0.559139785,0,0,1,experiments,Experiments,document_classification0
514,514,514,60,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (6, 7), (7, 8), (8, 9), (9, 11), (11, 12), (13, 15), (15, 17), (18, 20), (20, 21), (21, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.181818182,59,0.634408602,7,0.466666667,1,results,Results .: We present the results in .,document_classification0
515,515,515,61,"On this task , adding bigram information improves the performance by 1 - 4 % .",Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 7), (7, 8), (9, 10), (10, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.272727273,60,0.64516129,8,0.533333333,1,results,Results .: We present the results in .,document_classification0
516,516,516,62,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (7, 14), (17, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.363636364,61,0.655913978,9,0.6,1,results,Results .: We present the results in .,document_classification0
517,517,517,63,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 7), (7, 8), (8, 10), (10, 12), (13, 16), (16, 17), (19, 20), (20, 21), (21, 22), (22, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.454545455,62,0.666666667,10,0.666666667,1,results,Results .: We present the results in .,document_classification0
518,518,518,65,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,Results .,We present the results in .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 8), (9, 10), (11, 12), (12, 13), (13, 15), (15, 16), (16, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,64,0.688172043,12,0.8,1,results,Results .: We present the results in .,document_classification0
519,519,519,69,Tag prediction,Results .,,document_classification,0,"['O', 'O']","[(0, 2)]","['O', 'O']",11,1,68,0.731182796,0,0,1,experiments,Results .,document_classification0
520,520,520,79,We consider a frequency - based baseline which predicts the most frequent tag .,Dataset and baselines .,We report precision at 1 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (8, 9), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.529411765,78,0.838709677,10,0.555555556,1,baselines,Dataset and baselines .: We report precision at 1 .,document_classification0
521,521,521,80,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",Dataset and baselines .,We report precision at 1 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (14, 15), (16, 19), (24, 26), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.588235294,79,0.849462366,11,0.611111111,1,baselines,Dataset and baselines .: We report precision at 1 .,document_classification0
522,522,522,82,Results and training time . and 200 .,Dataset and baselines .,,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.705882353,81,0.870967742,13,0.722222222,1,experiments,Dataset and baselines .,document_classification0
523,523,523,83,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",Dataset and baselines .,Results and training time . and 200 .,document_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 6), (6, 7), (8, 11), (13, 14), (14, 15), (15, 17), (18, 20), (20, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.764705882,82,0.88172043,14,0.777777778,1,experiments,Dataset and baselines .: Results and training time . and 200 .,document_classification0
524,524,524,2,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION,,,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.166666667,1,0.003521127,1,0,1,research-problem,,document_classification1
525,525,525,5,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",,,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.416666667,4,0.014084507,2,0.222222222,1,research-problem,,document_classification1
526,526,526,8,We combine state - of - the - art cross - lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU .,,,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 13), (17, 18), (18, 21), (21, 23), (23, 25), (26, 29), (34, 36), (38, 40), (40, 41), (41, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.666666667,7,0.024647887,5,0.555555556,1,baselines,,document_classification1
527,527,527,38,"In particular , we focus on two approaches for domain adaptation .",INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.735294118,37,0.13028169,25,0.735294118,1,model,INTRODUCTION,document_classification1
528,528,528,39,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 13), (17, 18), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.764705882,38,0.133802817,26,0.764705882,1,model,INTRODUCTION,document_classification1
529,529,529,41,"The second method is unsupervised data augmentation ( UDA ) ) , where synthetic paraphrases are generated from the unlabeled corpus , and the model is trained on a label consistency loss .",INTRODUCTION,INTRODUCTION,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 11), (12, 13), (13, 15), (16, 18), (19, 21), (24, 25), (26, 28), (29, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.823529412,40,0.14084507,28,0.823529412,1,model,INTRODUCTION,document_classification1
530,530,530,152,Fine-tune ( Ft ) : Fine - tuning the pre-trained model with the source - domain training set .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 8), (9, 11), (11, 12), (13, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.030769231,151,0.531690141,2,0.054054054,1,baselines,MAIN RESULTS,document_classification1
531,531,531,154,Fine - tune with UDA ( UDA ) :,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.061538462,153,0.538732394,4,0.108108108,1,baselines,MAIN RESULTS,document_classification1
532,532,532,155,This method utilizes the unlabeled data from the target domain by optimizing the UDA loss function ( Eq. ) .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (8, 10), (10, 12), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.076923077,154,0.542253521,5,0.135135135,1,baselines,MAIN RESULTS,document_classification1
533,533,533,156,Self - training based on the UDA model ( UDA + Self ) :,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 5), (6, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.092307692,155,0.545774648,6,0.162162162,1,baselines,MAIN RESULTS,document_classification1
534,534,534,157,"We first train the Ft model and UDA model , and choose the better one as the teacher model .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9), (11, 12), (13, 15), (15, 16), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.107692308,156,0.549295775,7,0.189189189,1,baselines,MAIN RESULTS,document_classification1
535,535,535,158,"The teacher model is used to train a new XLM student using only unlabeled data U tgt in the target domain , as described above .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (8, 11), (11, 12), (12, 17), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.123076923,157,0.552816901,8,0.216216216,1,baselines,MAIN RESULTS,document_classification1
536,536,536,171,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 7), (12, 16), (16, 18), (18, 19), (20, 22), (27, 29), (29, 30), (31, 33), (33, 34), (40, 42), (44, 46), (46, 57)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.323076923,170,0.598591549,21,0.567567568,1,results,MAIN RESULTS,document_classification1
537,537,537,172,Both the UDA algorithm and MLM pre-training can offer significant improvements by utilizing the unlabeled data .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 7), (8, 9), (9, 11), (11, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.338461538,171,0.602112676,22,0.594594595,1,results,MAIN RESULTS,document_classification1
538,538,538,173,"In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (6, 7), (8, 11), (11, 12), (12, 13), (14, 23), (23, 25), (25, 27), (27, 29), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.353846154,172,0.605633803,23,0.621621622,1,results,MAIN RESULTS,document_classification1
539,539,539,174,"On the other hand , the MLM method is relatively more resource intensive and takes longer to converge ( see Appendix A.5 ) .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 9), (9, 13), (14, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.369230769,173,0.60915493,24,0.648648649,1,results,MAIN RESULTS,document_classification1
540,540,540,175,"In contrast , in the MLdoc dataset , when the size of the unlabeled samples is limited , the UDA method is more helpful .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (8, 9), (10, 11), (11, 12), (13, 15), (15, 16), (16, 17), (19, 21), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.384615385,174,0.612676056,25,0.675675676,1,results,MAIN RESULTS,document_classification1
541,541,541,178,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (9, 13), (13, 15), (15, 16), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.430769231,177,0.623239437,28,0.756756757,1,results,MAIN RESULTS,document_classification1
542,542,542,179,It offers best results in both XLM and XLM ft based classifiers .,MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.446153846,178,0.626760563,29,0.783783784,1,results,MAIN RESULTS,document_classification1
543,543,543,181,"In the MLdoc dataset , self - training also achieves the best results over all , however the gains are less clear .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 8), (9, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.476923077,180,0.633802817,31,0.837837838,1,results,MAIN RESULTS,document_classification1
544,544,544,183,"Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 10), (11, 16), (19, 21), (21, 23), (24, 26), (26, 28), (28, 30), (30, 31), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.507692308,182,0.64084507,33,0.891891892,1,results,MAIN RESULTS,document_classification1
545,545,545,184,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .",MAIN RESULTS,MAIN RESULTS,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 14), (15, 17), (17, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.523076923,183,0.644366197,34,0.918918919,1,results,MAIN RESULTS,document_classification1
546,546,546,214,"Leveraging the unlabeled data from other domains does not offer consistent improvement , however can provide additional value in isolated cases .",MAIN RESULTS,ABLATION STUDY : AUGMENTATION STRATEGIES,document_classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (5, 7), (7, 10), (10, 12), (15, 16), (16, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.984615385,213,0.75,26,0.962962963,1,ablation-analysis,MAIN RESULTS: ABLATION STUDY : AUGMENTATION STRATEGIES,document_classification1
547,547,547,2,Neural Attentive Bag - of - Entities Model for Text Classification,title,title,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.006097561,1,0,1,research-problem,title,document_classification10
548,548,548,9,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,abstract,abstract,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,8,0.048780488,6,1,1,code,abstract,document_classification10
549,549,549,21,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .",Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 13), (15, 16), (17, 20), (21, 22), (23, 26), (26, 28), (29, 30), (30, 31), (32, 34), (34, 35), (35, 36), (36, 37), (38, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.611111111,20,0.12195122,11,0.611111111,1,model,Introduction,document_classification10
550,550,550,22,"For each entity name in a document ( e.g. , "" Apple "" ) , our model first detects entities that maybe referred to by this name ( e.g. , Apple Inc. , Apple ( food ) ) , and then represents the document using the weighted average of the embeddings of these entities .",Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (4, 5), (6, 7), (17, 19), (19, 20), (22, 25), (41, 42), (43, 44), (44, 45), (46, 48), (48, 49), (50, 51), (51, 52), (53, 54)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.666666667,21,0.12804878,12,0.666666667,1,model,Introduction,document_classification10
551,551,551,23,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 10), (11, 12), (13, 14), (14, 17), (18, 20), (20, 21), (22, 23), (23, 25), (25, 27), (27, 28), (28, 29), (30, 33), (34, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.722222222,22,0.134146341,13,0.722222222,1,model,Introduction,document_classification10
552,552,552,24,"In other words , the attention mechanism is designed to compute weights by jointly addressing entity linking and entity salience detection tasks .",Introduction,Introduction,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 11), (11, 12), (12, 15), (15, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.777777778,23,0.140243902,14,0.777777778,1,model,Introduction,document_classification10
553,553,553,81,The model was trained using mini-batch SGD with its learning rate controlled by Adam and its mini-batch size set to 32 .,Experimental Setup,Setup,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 7), (7, 8), (9, 11), (11, 13), (13, 14), (16, 18), (18, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.833333333,80,0.487804878,10,0.714285714,1,hyperparameters,Experimental Setup: Setup,document_classification10
554,554,554,83,The size of the embeddings of words and entities was set to d = 300 .,Experimental Setup,Setup,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 5), (5, 6), (6, 9), (10, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.916666667,82,0.5,12,0.857142857,1,hyperparameters,Experimental Setup: Setup,document_classification10
555,555,555,86,Baselines,,,document_classification,10,['O'],"[(0, 1)]",['O'],0,0,85,0.518292683,0,0,1,experiments,,document_classification10
556,556,556,88,BoW,Baselines,,document_classification,10,['O'],"[(0, 1)]",['O'],2,0.153846154,87,0.530487805,2,0.153846154,1,baselines,Baselines,document_classification10
557,557,557,89,This model is based on a logistic regression classifier with conventional binary BoW features .,Baselines,BoW,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 9), (9, 10), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.230769231,88,0.536585366,3,0.230769231,1,baselines,Baselines: BoW,document_classification10
558,558,558,90,FTS- BRNN,Baselines,BoW,document_classification,10,"['O', 'O']","[(0, 2)]","['O', 'O']",4,0.307692308,89,0.542682927,4,0.307692308,1,baselines,Baselines: BoW,document_classification10
559,559,559,91,This model is based on a bidirectional RNN with gated recurrent units ( GRU ) .,Baselines,BoW,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (8, 9), (9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.384615385,90,0.548780488,5,0.384615385,1,baselines,Baselines: BoW,document_classification10
560,560,560,93,NTEE This model is a state - of - the - art model that uses a multi - layer perceptron classifier with the features computed using the embeddings of words and entities trained on Wikipedia using the neural network model proposed in their paper .,Baselines,BoW,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 4), (5, 13), (14, 15), (16, 21), (21, 22), (23, 24), (24, 26), (29, 32), (32, 34), (34, 35), (35, 36), (37, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.538461538,92,0.56097561,7,0.538461538,1,baselines,Baselines: BoW,document_classification10
561,561,561,100,Results,,,document_classification,10,['O'],[],['O'],0,0,99,0.603658537,0,0,1,experiments,,document_classification10
562,562,562,101,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .",Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (5, 7), (7, 8), (8, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.027027027,100,0.609756098,1,0.111111111,1,results,Results,document_classification10
563,563,563,102,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 9), (9, 12), (12, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.054054054,101,0.615853659,2,0.222222222,1,results,Results,document_classification10
564,564,564,103,"Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .",Results,Results,document_classification,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 10), (10, 13), (13, 15), (15, 16), (17, 19), (22, 25), (25, 26), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.081081081,102,0.62195122,3,0.333333333,1,results,Results,document_classification10
565,565,565,2,Task - oriented Word Embedding for Text Classification,title,,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004761905,1,0,1,research-problem,title,document_classification11
566,566,566,30,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .",Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.393939394,29,0.138095238,13,0.393939394,1,model,Introduction,document_classification11
567,567,567,38,"In our method , the words ' contextual information and task information are inherently jointed to construct the word embeddings .",Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 12), (13, 15), (15, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.636363636,37,0.176190476,21,0.636363636,1,model,Introduction,document_classification11
568,568,568,40,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .",Introduction,Introduction,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (7, 8), (9, 10), (10, 11), (12, 14), (14, 16), (17, 20), (23, 24), (25, 26), (26, 27), (28, 30), (30, 31), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.696969697,39,0.185714286,23,0.696969697,1,model,Introduction,document_classification11
569,569,569,146,"To evaluate our method , we consider the following baselines : ( 1 ) the BOW method is employed as a basic baseline .",Baseline Methods,Baseline Methods,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 17), (18, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,145,0.69047619,1,0.166666667,1,baselines,Baseline Methods,document_classification11
570,570,570,147,It represents each document as a bag of words and the weighting scheme is TFIDF .,Baseline Methods,Baseline Methods,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 9), (11, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,146,0.695238095,2,0.333333333,1,baselines,Baseline Methods,document_classification11
571,571,571,149,( 2 ) the Word2 Vec method is a neural network language method which learns word embeddings by maximizing the conditional probability leveraging contextual information .,Baseline Methods,Baseline Methods,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 8), (9, 13), (13, 15), (15, 17), (17, 19), (20, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,148,0.704761905,4,0.666666667,1,baselines,Baseline Methods,document_classification11
572,572,572,183,"( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 7), (7, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.596153846,182,0.866666667,4,0.285714286,1,results,Experimental Settings: Overall Performance,document_classification11
573,573,573,184,"In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 8), (8, 10), (11, 13), (13, 14), (15, 19), (20, 24), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.615384615,183,0.871428571,5,0.357142857,1,results,Experimental Settings: Overall Performance,document_classification11
574,574,574,186,"( 2 ) The word embedding methods outperform the basic bag - of - words methods in most cases , indicating the superiority of distributed word representation over the one - hot representation .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 8), (9, 16), (16, 17), (17, 19), (20, 21), (22, 23), (23, 24), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.653846154,185,0.880952381,7,0.5,1,results,Experimental Settings: Overall Performance,document_classification11
575,575,575,190,"Our method achieves better performance over Retrofit method , indicating that the task - specific features could be more effective compared with general semantic relations constructed by humans in the knowledge bases .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 5), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.730769231,189,0.9,11,0.785714286,1,results,Experimental Settings: Overall Performance,document_classification11
576,576,576,193,"Our method outperforms the TWE method on both the document - level and sentence - level tasks , which shows the stability and reliability of modeling taskspecific features in real - world applications .",Experimental Settings,Overall Performance,document_classification,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 6), (6, 7), (9, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.788461538,192,0.914285714,14,1,1,results,Experimental Settings: Overall Performance,document_classification11
577,577,577,2,Graph Convolutional Networks for Text Classification,title,,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O']","[(4, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004504505,1,0,1,research-problem,title,document_classification12
578,578,578,22,"In this work , we propose a new graph neural networkbased method for text classification .",Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.094736842,21,0.094594595,9,0.45,1,model,Introduction,document_classification12
579,579,579,23,"We construct a single large graph from an entire corpus , which contains words and documents as nodes .",Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (8, 10), (12, 13), (13, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.105263158,22,0.099099099,10,0.5,1,model,Introduction,document_classification12
580,580,580,24,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .",Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 12), (14, 20), (21, 22), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.115789474,23,0.103603604,11,0.55,1,model,Introduction,document_classification12
581,581,581,25,The edge between two word nodes is built byword co-occurrence information and the edge between a word node and document node is built using word frequency and word 's document frequency .,Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (7, 11), (13, 14), (14, 15), (16, 21), (22, 24), (24, 26), (27, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.126315789,24,0.108108108,12,0.6,1,model,Introduction,document_classification12
582,582,582,26,We then turn text classification problem into anode classification problem .,Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (6, 7), (7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.136842105,25,0.112612613,13,0.65,1,model,Introduction,document_classification12
583,583,583,28,Our source code is available at https://github. com/yao8839836/text_gcn .,Introduction,Introduction,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.157894737,27,0.121621622,15,0.75,1,code,Introduction,document_classification12
584,584,584,114,Baselines .,,,document_classification,12,"['O', 'O']","[(0, 1)]","['O', 'O']",0,0,113,0.509009009,5,0.046728972,1,experiments,,document_classification12
585,585,585,115,We compare our Text GCN with multiple stateof - the - art text classification and embedding methods as follows :,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.009803922,114,0.513513514,6,0.056074766,1,baselines,Baselines .,document_classification12
586,586,586,116,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (6, 12), (12, 13), (13, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.019607843,115,0.518018018,7,0.065420561,1,baselines,Baselines .,document_classification12
587,587,587,117,Logistic Regression is used as the classifier .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.029411765,116,0.522522523,8,0.074766355,1,baselines,Baselines .,document_classification12
588,588,588,118,CNN : Convolutional Neural Network ( Kim 2014 ) .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.039215686,117,0.527027027,9,0.08411215,1,baselines,Baselines .,document_classification12
589,589,589,119,We explored CNN -rand which uses randomly initialized word embeddings and CNN - non- static which uses pre-trained word embeddings .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 6), (6, 10), (11, 15), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.049019608,118,0.531531532,10,0.093457944,1,baselines,Baselines .,document_classification12
590,590,590,120,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (8, 9), (10, 13), (13, 14), (15, 16), (16, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.058823529,119,0.536036036,11,0.102803738,1,baselines,Baselines .,document_classification12
591,591,591,122,"Bi- LSTM : a bi-directional LSTM , commonly used in text classification .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.078431373,121,0.545045045,13,0.121495327,1,baselines,Baselines .,document_classification12
592,592,592,124,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (5, 8), (12, 13), (12, 15), (15, 16), (16, 17), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.098039216,123,0.554054054,15,0.140186916,1,baselines,Baselines .,document_classification12
593,593,593,125,We used Logistic Regression as the classifier .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.107843137,124,0.558558559,16,0.14953271,1,baselines,Baselines .,document_classification12
594,594,594,126,"PV - DM : a paragraph vector model proposed by , which considers the word order .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (5, 8), (12, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.117647059,125,0.563063063,17,0.158878505,1,baselines,Baselines .,document_classification12
595,595,595,127,We used Logistic Regression as the classifier .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.12745098,126,0.567567568,18,0.168224299,1,baselines,Baselines .,document_classification12
596,596,596,128,"PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 9), (9, 11), (11, 13), (13, 16), (16, 17), (17, 22), (22, 23), (23, 24), (26, 27), (27, 29), (29, 30), (30, 32), (32, 33), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.137254902,127,0.572072072,19,0.177570093,1,baselines,Baselines .,document_classification12
597,597,597,129,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (12, 13), (14, 21), (21, 22), (22, 24), (26, 27), (27, 29), (29, 30), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.147058824,128,0.576576577,20,0.186915888,1,baselines,Baselines .,document_classification12
598,598,598,131,"SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (8, 9), (9, 12), (12, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.166666667,130,0.585585586,22,0.205607477,1,baselines,Baselines .,document_classification12
599,599,599,132,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7), (9, 10), (11, 14), (14, 15), (16, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.176470588,131,0.59009009,23,0.214953271,1,baselines,Baselines .,document_classification12
600,600,600,133,It utilizes label descriptions .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4)]","['O', 'O', 'O', 'O', 'O']",19,0.18627451,132,0.594594595,24,0.224299065,1,baselines,Baselines .,document_classification12
601,601,601,134,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .",Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (7, 10), (11, 12), (12, 13), (13, 14), (14, 18), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.196078431,133,0.599099099,25,0.23364486,1,baselines,Baselines .,document_classification12
602,602,602,135,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) . ,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.205882353,134,0.603603604,26,0.242990654,1,baselines,Baselines .,document_classification12
603,603,603,136,Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,Baselines .,Baselines .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (7, 9), (9, 14), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.215686275,135,0.608108108,27,0.252336449,1,baselines,Baselines .,document_classification12
604,604,604,155,"For Text GCN , we set the embedding size of the first convolution layer as 200 and set the window size as 20 .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (7, 9), (9, 10), (11, 14), (14, 15), (15, 16), (17, 18), (19, 21), (21, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.401960784,154,0.693693694,46,0.429906542,1,hyperparameters,Baselines .: Settings .,document_classification12
605,605,605,157,"We tuned other parameters and set the learning rate as 0.02 , dropout For baseline models , we used default parameter settings as in their original papers or implementations .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 6), (7, 9), (9, 10), (10, 11), (12, 13), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.421568627,156,0.702702703,48,0.448598131,1,hyperparameters,Baselines .: Settings .,document_classification12
606,606,606,158,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4), (4, 7), (9, 10), (10, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.431372549,157,0.707207207,49,0.457943925,1,hyperparameters,Baselines .: Settings .,document_classification12
607,607,607,161,"Text GCN performs the best and significantly outperforms all baseline models ( p < 0.05 based on student t- test ) on four datasets , which showcases the effectiveness of the proposed method on long text datasets .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 5), (6, 8), (8, 11), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.460784314,160,0.720720721,52,0.485981308,1,results,Baselines .: Settings .,document_classification12
608,608,608,163,"When pre-trained Glo Ve word embeddings are provided , CNN performs much better , especially on Ohsumed and 20 NG .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 6), (7, 8), (9, 10), (10, 11), (11, 13), (14, 16), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.480392157,162,0.72972973,54,0.504672897,1,results,Baselines .: Settings .,document_classification12
609,609,609,165,"Similarly , LSTM - based models also rely on pre-trained word embeddings and tend to perform better when documents are shorter .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (7, 9), (9, 12), (13, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.5,164,0.738738739,56,0.523364486,1,results,Baselines .: Settings .,document_classification12
610,610,610,166,"PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 6), (6, 7), (7, 9), (9, 10), (10, 14), (18, 19), (19, 21), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.509803922,165,0.743243243,57,0.53271028,1,results,Baselines .: Settings .,document_classification12
611,611,611,168,"PV - DM performs worse than PV - DBOW , the only comparable results are on MR , where word orders are more essential .",Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 5), (5, 6), (6, 9), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.529411765,167,0.752252252,59,0.551401869,1,results,Baselines .: Settings .,document_classification12
612,612,612,172,Graph - CNN models also show competitive performances .,Baselines .,Settings .,document_classification,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.568627451,171,0.77027027,63,0.588785047,1,results,Baselines .: Settings .,document_classification12
613,613,613,2,Deep Pyramid Convolutional Neural Networks for Text Categorization,title,title,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004424779,1,0,1,research-problem,title,document_classification13
614,614,614,29,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .",Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (10, 11), (12, 16), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.514285714,28,0.123893805,18,0.514285714,1,model,Introduction,document_classification13
615,615,615,30,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .",Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (4, 5), (5, 7), (9, 11), (12, 13), (14, 16), (18, 20), (20, 23), (23, 24), (25, 27), (28, 30), (30, 32), (32, 35), (42, 44), (45, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.542857143,29,0.128318584,19,0.542857143,1,model,Introduction,document_classification13
616,616,616,31,The network depth can be treated as a meta-parameter .,Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 7), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.571428571,30,0.132743363,20,0.571428571,1,model,Introduction,document_classification13
617,617,617,32,The computational complexity of this network is bounded to be no more than twice that of one convolution block .,Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 6), (7, 10), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6,31,0.137168142,21,0.6,1,model,Introduction,document_classification13
618,618,618,36,We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification ..,Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 8), (8, 9), (10, 13), (13, 14), (14, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.714285714,35,0.154867257,25,0.714285714,1,model,Introduction,document_classification13
619,619,619,37,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .",Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (9, 10), (10, 14), (14, 15), (16, 17), (17, 18), (18, 20), (20, 21), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.742857143,36,0.159292035,26,0.742857143,1,model,Introduction,document_classification13
620,620,620,38,It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .,Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 6), (16, 18), (18, 20), (20, 21), (21, 23), (23, 24), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.771428571,37,0.163716814,27,0.771428571,1,model,Introduction,document_classification13
621,621,621,39,The final pooling layer aggregates internal data for each document into one vector .,Introduction,Introduction,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 7), (7, 8), (8, 10), (10, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.8,38,0.168141593,28,0.8,1,model,Introduction,document_classification13
622,622,622,40,We use max pooling for all pooling layers .,Introduction,,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.828571429,39,0.172566372,29,0.828571429,1,hyperparameters,Introduction,document_classification13
623,623,623,150,Results,,,document_classification,13,['O'],[],['O'],0,0,149,0.659292035,0,0,1,experiments,,document_classification13
624,624,624,155,Large data results,,,document_classification,13,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,154,0.681415929,1,0.023255814,1,results,,document_classification13
625,625,625,159,"On all the five datasets , DPCNN outperforms all of the previous results , which validates the effectiveness of our approach .",Large data results,Large data results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 5), (6, 7), (7, 8), (8, 13), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.114285714,158,0.699115044,5,0.11627907,1,results,Large data results,document_classification13
626,626,626,191,Small data results,,,document_classification,13,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,190,0.840707965,37,0.860465116,1,results,,document_classification13
627,627,627,194,"For these small datasets , the DPCNN performances with 100 - dim unsupervised embed - dings are shown , which turned out to be as good as those with 300 - dim unsupervised embeddings .",Small data results,Small data results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 9), (9, 16), (20, 24), (24, 27), (28, 29), (29, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.096774194,193,0.853982301,40,0.930232558,1,results,Small data results,document_classification13
628,628,628,196,"ShallowCNN ( row 2 ) rivals DPCNN ( row 1 ) , and Zhang et al. 's best linear model ( row 3 ) moved up from the worst performer to the third best performer .",Small data results,Small data results,document_classification,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (5, 6), (6, 7), (13, 14), (17, 20), (24, 26), (26, 27), (28, 30), (30, 31), (32, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.161290323,195,0.862831858,42,0.976744186,1,results,Small data results,document_classification13
629,629,629,2,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,title,title,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00390625,1,0,1,research-problem,title,document_classification14
630,630,630,4,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",abstract,abstract,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.01171875,1,0.125,1,research-problem,abstract,document_classification14
631,631,631,31,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",Introduction,JZ15 proposed variations to alleviate these issues .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 10), (10, 11), (12, 16), (18, 19), (20, 24), (24, 25), (25, 33), (41, 42), (43, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.171171171,30,0.1171875,19,0.558823529,1,model,Introduction: JZ15 proposed variations to alleviate these issues .,document_classification14
632,632,632,35,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,Introduction,LSTM ) is a recurrent neural network .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 12), (12, 13), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.207207207,34,0.1328125,23,0.676470588,1,model,Introduction: LSTM ) is a recurrent neural network .,document_classification14
633,633,633,36,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",Introduction,LSTM ) is a recurrent neural network .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (10, 12), (12, 13), (13, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.216216216,35,0.13671875,24,0.705882353,1,model,Introduction: LSTM ) is a recurrent neural network .,document_classification14
634,634,634,38,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",Introduction,LSTM ) is a recurrent neural network .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (6, 7), (12, 13), (13, 14), (14, 15), (16, 19), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.234234234,37,0.14453125,26,0.764705882,1,model,Introduction: LSTM ) is a recurrent neural network .,document_classification14
635,635,635,46,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,Introduction,Our findings are threefold .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.306306306,45,0.17578125,34,1,1,code,Introduction: Our findings are threefold .,document_classification14
636,636,636,124,Experiments ( supervised ),,,document_classification,14,"['O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O']",0,0,123,0.48046875,33,0.464788732,1,experiments,,document_classification14
637,637,637,135,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,Experiments ( supervised ),The datasets are summarized in .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (5, 6), (6, 8), (8, 11), (11, 12), (12, 13), (14, 15), (15, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.157142857,134,0.5234375,44,0.61971831,1,experiments,Experiments ( supervised ): The datasets are summarized in .,document_classification14
638,638,638,139,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",Experiments ( supervised ),The datasets are summarized in .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11), (11, 25), (25, 26), (26, 35), (35, 36), (36, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.214285714,138,0.5390625,48,0.676056338,1,experiments,Experiments ( supervised ): The datasets are summarized in .,document_classification14
639,639,639,140,Now we review the non -LSTM baseline methods .,Experiments ( supervised ),,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.228571429,139,0.54296875,49,0.690140845,1,experiments,Experiments ( supervised ),document_classification14
640,640,640,143,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",Experiments ( supervised ),Now we review the non -LSTM baseline methods .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 9), (10, 14), (14, 15), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.271428571,142,0.5546875,52,0.732394366,1,results,Experiments ( supervised ): Now we review the non -LSTM baseline methods .,document_classification14
641,641,641,147,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 10), (10, 16), (20, 21), (22, 25), (25, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.328571429,146,0.5703125,56,0.788732394,1,results,"Experiments ( supervised ): However , on RCV1 , it underperforms both .",document_classification14
642,642,642,158,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 7), (7, 9), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.485714286,157,0.61328125,67,0.943661972,1,results,"Experiments ( supervised ): However , on RCV1 , it underperforms both .",document_classification14
643,643,643,160,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (6, 7), (7, 8), (15, 16), (16, 17), (18, 20), (20, 24), (24, 25), (25, 27), (27, 28), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.514285714,159,0.62109375,69,0.971830986,1,results,Experiments ( supervised ): Comparison with the previous best results on 20 NG,document_classification14
644,644,644,161,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 7), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.528571429,160,0.625,70,0.985915493,1,results,Experiments ( supervised ): Comparison with the previous best results on 20 NG,document_classification14
645,645,645,195,Semi-supervised experiments,,,document_classification,14,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,194,0.7578125,0,0,1,experiments,,document_classification14
646,646,646,210,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .",Semi-supervised experiments,Other details followed the supervised experiments .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (9, 13), (15, 16), (17, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.294117647,209,0.81640625,15,0.535714286,1,results,Semi-supervised experiments: Other details followed the supervised experiments .,document_classification14
647,647,647,216,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .",Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 8), (8, 9), (10, 13), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.411764706,215,0.83984375,21,0.75,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
648,648,648,220,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .",Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (6, 10), (10, 11), (11, 17), (24, 26), (26, 28), (40, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.490196078,219,0.85546875,25,0.892857143,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
649,649,649,221,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (7, 10), (11, 12), (17, 18), (18, 21), (22, 23), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.509803922,220,0.859375,26,0.928571429,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
650,650,650,222,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",Semi-supervised experiments,Two types of word vectors were tested .,document_classification,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (4, 6), (6, 7), (7, 10), (10, 11), (11, 12), (14, 15), (15, 16), (20, 23), (23, 24), (24, 25), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.529411765,221,0.86328125,27,0.964285714,1,results,Semi-supervised experiments: Two types of word vectors were tested .,document_classification14
651,651,651,2,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,,,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,1,0.004926108,1,0,1,research-problem,,document_classification15
652,652,652,18,Previous work has primarily applied adversarial and virtual adversarial training to image classification tasks .,INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.391304348,17,0.083743842,9,0.391304348,1,research-problem,INTRODUCTION,document_classification15
653,653,653,19,"In this work , we extend these techniques to text classification tasks and sequence models .",INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.434782609,18,0.088669951,10,0.434782609,1,model,INTRODUCTION,document_classification15
654,654,654,20,Adversarial perturbations typically consist of making small modifications to very many real - valued inputs .,INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.47826087,19,0.093596059,11,0.47826087,1,model,INTRODUCTION,document_classification15
655,655,655,21,"For text classification , the input is discrete , and usually represented as a series of highdimensional one - hot vectors .",INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 6), (6, 7), (7, 8), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.52173913,20,0.098522167,12,0.52173913,1,research-problem,INTRODUCTION,document_classification15
656,656,656,22,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .",INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(18, 19), (20, 21), (21, 22), (22, 25), (25, 27), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.565217391,21,0.103448276,13,0.565217391,1,model,INTRODUCTION,document_classification15
657,657,657,25,We thus propose this approach exclusively as a means of regularizing a text classifier by stabilizing the classification function .,INTRODUCTION,INTRODUCTION,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (12, 14), (14, 16), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.695652174,24,0.118226601,16,0.695652174,1,model,INTRODUCTION,document_classification15
658,658,658,100,All experiments used TensorFlow on GPUs .,EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.05,99,0.487684729,1,0.05,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
659,659,659,101,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text.,EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O']","[(5, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",2,0.1,100,0.492610837,2,0.1,1,code,EXPERIMENTAL SETTINGS,document_classification15
660,660,660,112,"We trained for 100,000 steps .",EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5)]","['O', 'O', 'O', 'O', 'O', 'O']",13,0.65,111,0.54679803,13,0.65,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
661,661,661,113,We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 6), (6, 8), (8, 9), (9, 10), (10, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7,112,0.551724138,14,0.7,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
662,662,662,115,"For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .",EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 3), (4, 7), (9, 10), (10, 11), (11, 12), (13, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,114,0.561576355,16,0.8,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
663,663,663,116,"For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .",EXPERIMENTAL SETTINGS,EXPERIMENTAL SETTINGS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (8, 12), (12, 13), (15, 21), (24, 25), (25, 29), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.85,115,0.566502463,17,0.85,1,experimental-setup,EXPERIMENTAL SETTINGS,document_classification15
664,664,664,163,We saw that cosine distance on adversarial and virtual adversarial training ( 0.159-0.331 ) were much smaller than ones on the baseline and random perturbation method ( 0.244-0.399 ) .,RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 6), (6, 14), (14, 15), (15, 17), (17, 18), (19, 20), (21, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4375,162,0.798029557,13,0.433333333,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
665,665,665,165,shows the test performance on the Elec and RCV1 datasets .,RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5,164,0.807881773,15,0.5,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
666,666,666,166,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .",RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 9), (9, 10), (11, 13), (14, 15), (15, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.53125,165,0.812807882,16,0.533333333,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
667,667,667,167,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 6), (7, 12), (13, 15), (15, 16), (17, 19), (20, 21), (21, 22), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5625,166,0.81773399,17,0.566666667,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
668,668,668,169,shows test performance on the Rotten Tomatoes dataset .,RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4), (5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.625,168,0.827586207,19,0.633333333,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
669,669,669,170,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .",RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 6), (6, 7), (8, 10), (12, 13), (14, 19), (20, 21), (21, 25), (25, 26), (27, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.65625,169,0.832512315,20,0.666666667,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
670,670,670,171,However the test performance of only virtual adversarial training was worse than the baseline .,RESULTS,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 9), (9, 10), (10, 12), (10, 11), (11, 12), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6875,170,0.837438424,21,0.7,1,results,RESULTS: TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,document_classification15
671,671,671,2,A C - LSTM Neural Network for Text Classification,title,,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004587156,1,0,1,research-problem,title,document_classification16
672,672,672,29,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 11), (11, 14), (14, 16), (16, 19), (19, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.692307692,28,0.128440367,18,0.692307692,1,model,Introduction,document_classification16
673,673,673,30,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (14, 23), (23, 25), (26, 27), (27, 28), (29, 33), (33, 34), (34, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.730769231,29,0.133027523,19,0.730769231,1,model,Introduction,document_classification16
674,674,674,31,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (8, 11), (11, 12), (12, 16), (16, 18), (18, 22), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.769230769,30,0.137614679,20,0.769230769,1,model,Introduction,document_classification16
675,675,675,32,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 6), (6, 11), (13, 15), (15, 16), (16, 17), (18, 20), (20, 23), (23, 26), (27, 28), (28, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.807692308,31,0.142201835,21,0.807692308,1,model,Introduction,document_classification16
676,676,676,34,"We choose sequence - based input other than relying on the syntactic parse trees before feeding in the neural network , thus our model does n't rely on any external language knowledge and complicated pre-processing .",Introduction,Introduction,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 8), (8, 10), (11, 14), (14, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.884615385,33,0.151376147,23,0.884615385,1,model,Introduction,document_classification16
677,677,677,142,"We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 6), (6, 7), (10, 12), (14, 15), (15, 18), (19, 22), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.035714286,141,0.646788991,1,0.035714286,1,experimental-setup,Experimental Settings,document_classification16
678,678,678,143,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (8, 9), (10, 11), (13, 14), (15, 16), (16, 17), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.071428571,142,0.651376147,2,0.071428571,1,experimental-setup,Experimental Settings,document_classification16
679,679,679,147,"In our final settings , we only use one convolutional layer and one LSTM layer for both tasks .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.214285714,146,0.669724771,6,0.214285714,1,experimental-setup,Experimental Settings,document_classification16
680,680,680,167,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .",Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 7), (8, 11), (11, 12), (14, 16), (17, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.928571429,166,0.76146789,26,0.928571429,1,experimental-setup,Experimental Settings,document_classification16
681,681,681,168,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 8), (9, 11), (12, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.964285714,167,0.766055046,27,0.964285714,1,experimental-setup,Experimental Settings,document_classification16
682,682,682,169,We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,Experimental Settings,Experimental Settings,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 6), (7, 8), (8, 9), (9, 10), (10, 11), (12, 13), (13, 14), (15, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,1,168,0.770642202,28,1,1,experimental-setup,Experimental Settings,document_classification16
683,683,683,170,Results and Model Analysis,,,document_classification,16,"['O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O']",0,0,169,0.775229358,0,0,1,experimental-setup,,document_classification16
684,684,684,173,Sentiment Classification,Results and Model Analysis,,document_classification,16,"['O', 'O']","[(0, 2)]","['O', 'O']",3,0.096774194,172,0.788990826,0,0,1,results,Results and Model Analysis,document_classification16
685,685,685,184,"To the best of our knowledge , we achieve the fourth best published result for the 5 - class classification task on this dataset .",Results and Model Analysis,Sentiment Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (10, 14), (14, 15), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.451612903,183,0.839449541,11,0.647058824,1,results,Results and Model Analysis: Sentiment Classification,document_classification16
686,686,686,185,"For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .",Results and Model Analysis,Sentiment Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (8, 10), (10, 13), (14, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.483870968,184,0.844036697,12,0.705882353,1,results,Results and Model Analysis: Sentiment Classification,document_classification16
687,687,687,191,Question Type Classification,Results and Model Analysis,,document_classification,16,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",21,0.677419355,190,0.871559633,0,0,1,results,Results and Model Analysis,document_classification16
688,688,688,198,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .",Results and Model Analysis,Question Type Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.903225806,197,0.903669725,7,0.7,1,results,Results and Model Analysis: Question Type Classification,document_classification16
689,689,689,199,( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,Results and Model Analysis,Question Type Classification,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (9, 10), (11, 19), (20, 22), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.935483871,198,0.908256881,8,0.8,1,results,Results and Model Analysis: Question Type Classification,document_classification16
690,690,690,206,"However , we found in our experiments that single convolutional layer with filter length 3 always outperforms the other cases .",Model Analysis,Model Analysis,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 11), (11, 12), (12, 15), (15, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,205,0.940366972,4,0.4,1,ablation-analysis,Model Analysis,document_classification16
691,691,691,210,It it shown that single convolutional layer with filter length 3 performs best among all filter configurations .,Model Analysis,Model Analysis,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (8, 11), (11, 12), (12, 13), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,209,0.958715596,8,0.8,1,ablation-analysis,Model Analysis,document_classification16
692,692,692,211,"For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .",Model Analysis,Model Analysis,document_classification,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (4, 7), (7, 9), (12, 14), (14, 16), (16, 17), (17, 20), (20, 21), (21, 22), (22, 23), (24, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,210,0.963302752,9,0.9,1,ablation-analysis,Model Analysis,document_classification16
693,693,693,2,Very Deep Convolutional Networks for Text Classification,title,title,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004329004,1,0,1,research-problem,title,document_classification17
694,694,694,34,"We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences , jointly with the task .",Introduction,We propose the following analogy .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.714285714,33,0.142857143,25,0.714285714,1,research-problem,Introduction: We propose the following analogy .,document_classification17
695,695,695,35,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .",Introduction,We propose the following analogy .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10), (10, 11), (11, 14), (19, 20), (20, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.742857143,34,0.147186147,26,0.742857143,1,model,Introduction: We propose the following analogy .,document_classification17
696,696,696,196,"The dictionary consists of the following characters "" abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:'"" / | # $ % & *' +=<>( ) [ ]{} "" plus a special padding , space and unknown token which add up to a total of 69 tokens .",Common model settings,Common model settings,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 7), (8, 9), (22, 23), (24, 26), (27, 31), (32, 35), (36, 38), (38, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.285714286,195,0.844155844,4,0.285714286,1,experimental-setup,Common model settings,document_classification17
697,697,697,197,"The input text is padded to a fixed size of 1014 , larger text are truncated .",Common model settings,Common model settings,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 9), (9, 10), (10, 11), (12, 14), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.357142857,196,0.848484848,5,0.357142857,1,experimental-setup,Common model settings,document_classification17
698,698,698,198,The character embedding is of size 16 .,Common model settings,,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 5), (5, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.428571429,197,0.852813853,6,0.428571429,1,experimental-setup,Common model settings,document_classification17
699,699,699,199,"Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .",Common model settings,The character embedding is of size 16 .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (6, 7), (8, 9), (9, 11), (11, 12), (14, 17), (17, 18), (18, 19), (20, 21), (21, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.5,198,0.857142857,7,0.5,1,experimental-setup,Common model settings: The character embedding is of size 16 .,document_classification17
700,700,700,204,The implementation is done using Torch 7 .,Common model settings,,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.857142857,203,0.878787879,12,0.857142857,1,experimental-setup,Common model settings,document_classification17
701,701,701,205,All experiments are performed on a single NVidia K40 GPU .,Common model settings,The implementation is done using Torch 7 .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.928571429,204,0.883116883,13,0.928571429,1,experimental-setup,Common model settings: The implementation is done using Torch 7 .,document_classification17
702,702,702,206,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .",Common model settings,The implementation is done using Torch 7 .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 14), (14, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,1,205,0.887445887,14,1,1,experimental-setup,Common model settings: The implementation is done using Torch 7 .,document_classification17
703,703,703,210,"Our deep architecture works well on big data sets in particular , even for small depths .",Experimental results,Experimental results,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (6, 9), (12, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3,209,0.904761905,3,0.3,1,results,Experimental results,document_classification17
704,704,704,212,"For the smallest depth we use ( 9 convolutional layers ) , we see that our model already performs better than Zhang 's convolutional baselines ( which includes 6 convolutional layers and has a different architecture ) on the biggest data sets :",Experimental results,Experimental results,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 6), (13, 15), (15, 17), (18, 19), (20, 21), (21, 25), (37, 38), (39, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,211,0.913419913,5,0.5,1,results,Experimental results,document_classification17
705,705,705,214,The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples . :,Experimental results,Experimental results,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 7), (9, 11), (12, 17), (19, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,213,0.922077922,7,0.7,1,results,Experimental results,document_classification17
706,706,706,217,"We also observe that for a small depth , temporal max - pooling works best on all data sets .",Experimental results,Best published results from previous work .,document_classification,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (6, 8), (9, 13), (13, 14), (14, 15), (15, 16), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1,216,0.935064935,10,1,1,results,Experimental results: Best published results from previous work .,document_classification17
707,707,707,2,Character - level Convolutional Networks for Text Classification,title,,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.5,1,0.004424779,1,0.5,1,research-problem,title,document_classification18
708,708,708,18,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",Introduction,Introduction,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 8), (9, 13), (13, 14), (14, 16), (18, 19), (19, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.12244898,17,0.075221239,6,0.333333333,1,model,Introduction,document_classification18
709,709,709,103,Traditional Methods,,,document_classification,18,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,102,0.451327434,0,0,1,experiments,,document_classification18
710,710,710,106,Bag - of - words and its TFIDF .,Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.176470588,105,0.46460177,3,0.176470588,1,baselines,Traditional Methods,document_classification18
711,711,711,107,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 11), (12, 15), (15, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.235294118,106,0.469026549,4,0.235294118,1,hyperparameters,Traditional Methods,document_classification18
712,712,712,112,Bag - of - ngrams and its TFIDF .,Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.529411765,111,0.491150442,9,0.529411765,1,baselines,Traditional Methods,document_classification18
713,713,713,113,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (8, 11), (12, 23), (23, 24), (25, 27), (27, 28), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.588235294,112,0.495575221,10,0.588235294,1,hyperparameters,Traditional Methods,document_classification18
714,714,714,115,Bag - of - means on word embedding .,Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.705882353,114,0.504424779,12,0.705882353,1,baselines,Traditional Methods,document_classification18
715,715,715,116,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",Traditional Methods,Traditional Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 8), (8, 9), (9, 10), (10, 11), (11, 13), (14, 16), (16, 17), (17, 19), (22, 23), (24, 26), (26, 27), (27, 28), (28, 29), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.764705882,115,0.508849558,13,0.764705882,1,baselines,Traditional Methods,document_classification18
716,716,716,121,Deep Learning Methods,,,document_classification,18,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,120,0.530973451,0,0,1,experiments,,document_classification18
717,717,717,124,Word - based ConvNets .,Deep Learning Methods,Deep Learning Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O']",3,0.03030303,123,0.544247788,3,0.1875,1,baselines,Deep Learning Methods,document_classification18
718,718,718,131,Long - short term memory .,Deep Learning Methods,Deep Learning Methods,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O']",10,0.101010101,130,0.575221239,10,0.625,1,experiments,Deep Learning Methods,document_classification18
719,719,719,194,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,Deep Learning Methods,We obtained Yahoo!,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 13), (14, 16), (16, 18), (18, 19), (20, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.737373737,193,0.853982301,55,0.679012346,1,results,Deep Learning Methods: We obtained Yahoo!,document_classification18
720,720,720,199,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",Deep Learning Methods,We obtained Yahoo!,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 5), (5, 6), (6, 8), (8, 9), (9, 10), (10, 12), (12, 14), (14, 18), (33, 34), (35, 39), (39, 42), (42, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.787878788,198,0.876106195,60,0.740740741,1,results,Deep Learning Methods: We obtained Yahoo!,document_classification18
721,721,721,200,Conv Nets may work well for user - generated data .,Deep Learning Methods,We obtained Yahoo!,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (4, 5), (5, 6), (6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.797979798,199,0.880530973,61,0.75308642,1,results,Deep Learning Methods: We obtained Yahoo!,document_classification18
722,722,722,207,Choice of alphabet makes a difference .,Deep Learning Methods,Answers .,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.868686869,206,0.911504425,68,0.839506173,1,experiments,Deep Learning Methods: Answers .,document_classification18
723,723,723,211,Semantics of tasks may not matter .,Deep Learning Methods,Answers .,document_classification,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.909090909,210,0.92920354,72,0.888888889,1,experiments,Deep Learning Methods: Answers .,document_classification18
724,724,724,2,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling,title,title,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004132231,1,0,1,research-problem,title,document_classification19
725,725,725,31,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .",Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 24), (24, 26), (26, 27), (27, 28), (30, 34), (36, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.484848485,30,0.123966942,16,0.484848485,1,model,Introduction,document_classification19
726,726,726,32,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 13), (13, 15), (16, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.515151515,31,0.128099174,17,0.515151515,1,model,Introduction,document_classification19
727,727,727,33,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (7, 8), (8, 10), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.545454545,32,0.132231405,18,0.545454545,1,model,Introduction,document_classification19
728,728,728,34,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,Introduction,Introduction,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 11), (11, 13), (13, 16), (16, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.575757576,33,0.136363636,19,0.575757576,1,model,Introduction,document_classification19
729,729,729,170,"The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .",Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (9, 11), (11, 12), (12, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.838709677,169,0.698347107,4,0.444444444,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
730,730,730,171,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .",Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (6, 7), (7, 9), (9, 10), (10, 15), (16, 19), (19, 20), (20, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.870967742,170,0.702479339,5,0.555555556,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
731,731,731,172,We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 7), (9, 11), (11, 12), (12, 13), (13, 14), (15, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.903225806,171,0.70661157,6,0.666666667,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
732,732,732,173,"For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .",Experimental Setup,Hyper-parameter Settings,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 7), (7, 8), (8, 10), (10, 11), (11, 12), (12, 13), (14, 16), (17, 18), (18, 19), (20, 22), (23, 24), (24, 25), (26, 28), (31, 32), (32, 35), (35, 36), (36, 40), (40, 41), (42, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.935483871,172,0.710743802,7,0.777777778,1,hyperparameters,Experimental Setup: Hyper-parameter Settings,document_classification19
733,733,733,176,Results,,,document_classification,19,['O'],[],['O'],0,0,175,0.723140496,0,0,1,experiments,,document_classification19
734,734,734,179,The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 8), (8, 9), (9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.051724138,178,0.73553719,2,0.046511628,1,results,Results: Overall Performance,document_classification19
735,735,735,180,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 11), (11, 12), (12, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.068965517,179,0.739669421,3,0.069767442,1,results,Results: Overall Performance,document_classification19
736,736,736,181,BLSTM - 2DPooling performs worse than the state - of - the - art models .,Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 5), (5, 6), (7, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.086206897,180,0.743801653,4,0.093023256,1,results,Results: Overall Performance,document_classification19
737,737,737,183,"BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 6), (6, 7), (7, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.120689655,182,0.752066116,6,0.139534884,1,results,Results: Overall Performance,document_classification19
738,738,738,184,"As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (7, 10), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.137931034,183,0.756198347,7,0.162790698,1,results,Results: Overall Performance,document_classification19
739,739,739,188,"Compared with RCNN , BLSTM - 2DCNN achieves a comparable result .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 7), (7, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.206896552,187,0.772727273,11,0.255813953,1,results,Results: Overall Performance,document_classification19
740,740,740,190,"Compared with ReNN , the proposed two models do not depend on external language - specific features such as dependency parse trees .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (12, 17), (17, 19), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.24137931,189,0.780991736,13,0.302325581,1,results,Results: Overall Performance,document_classification19
741,741,741,194,"Compared with DSCNN , BLSTM - 2DCNN outperforms it on five datasets .",Results,Overall Performance,document_classification,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 7), (7, 8), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.310344828,193,0.797520661,17,0.395348837,1,results,Results: Overall Performance,document_classification19
742,742,742,2,Rethinking Complex Neural Network Architectures for Document Classification,title,,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.0078125,1,0,1,research-problem,title,document_classification2
743,743,743,21,"Like the papers cited above , we question the need for overly complex neural architectures , focusing on the problem of document classification .",Introduction,Introduction,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 15), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.733333333,20,0.15625,11,0.733333333,1,approach,Introduction,document_classification2
744,744,744,22,"Starting with a large - scale reproducibility study of several recent neural models , we find that a simple bi-directional LSTM ( BiLSTM ) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .",Introduction,Introduction,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 16), (18, 25), (25, 26), (26, 28), (28, 29), (29, 33), (36, 39), (40, 44), (44, 45), (45, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8,21,0.1640625,12,0.8,1,approach,Introduction,document_classification2
745,745,745,60,"We conduct a large - scale reproducibility study involving HAN , XML - CNN , KimCNN , and SGM .",Experimental Setup,Experimental Setup,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 8), (8, 9), (9, 10), (11, 14), (15, 16), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.05,59,0.4609375,1,0.111111111,1,baselines,Experimental Setup,document_classification2
746,746,746,64,"In addition , we compare the neural approaches to logistic regression ( LR ) and support vector machines ( SVMs ) .",Experimental Setup,Experimental Setup,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 8), (8, 9), (9, 14), (15, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.25,63,0.4921875,5,0.555555556,1,experimental-setup,Experimental Setup,document_classification2
747,747,747,65,"The LR model is trained using a one - vs - rest multi-label objective , while the SVM is trained with a linear kernel .",Experimental Setup,Experimental Setup,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 14), (17, 18), (19, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3,64,0.5,6,0.666666667,1,experimental-setup,Experimental Setup,document_classification2
748,748,748,67,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .",Experimental Setup,Experimental Setup,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 15), (16, 17), (17, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4,66,0.515625,8,0.888888889,1,experimental-setup,Experimental Setup,document_classification2
749,749,749,68,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,Experimental Setup,Experimental Setup,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 6), (7, 11), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,67,0.5234375,9,1,1,experimental-setup,Experimental Setup,document_classification2
750,750,750,111,"We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 8), (8, 9), (9, 13), (13, 14), (14, 17), (26, 27), (27, 29), (29, 30), (30, 33), (33, 34), (34, 39), (39, 40), (41, 43), (43, 44), (44, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.083333333,110,0.859375,10,0.476190476,1,results,Baseline Comparison .,document_classification2
751,751,751,113,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 8), (9, 10), (10, 11), (11, 13), (13, 14), (14, 18), (31, 32), (32, 33), (33, 34), (34, 38), (38, 39), (39, 44)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,112,0.875,12,0.571428571,1,results,Baseline Comparison .,document_classification2
752,752,752,114,A few of our LSTM reg runs attain state - of - theart test F 1 scores on AAPD .,Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.333333333,113,0.8828125,13,0.619047619,1,results,Baseline Comparison .,document_classification2
753,753,753,119,"Interestingly , the non-neural LR and SVM baselines perform remarkably well .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,118,0.921875,18,0.857142857,1,results,Baseline Comparison .,document_classification2
754,754,754,120,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (7, 8), (8, 9), (9, 12), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.833333333,119,0.9296875,19,0.904761905,1,results,Baseline Comparison .,document_classification2
755,755,755,121,"On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (6, 9), (10, 12), (13, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.916666667,120,0.9375,20,0.952380952,1,results,Baseline Comparison .,document_classification2
756,756,756,122,"Compared to the SVM , the LR baseline appears better suited for the single - label datasets IMDB and Yelp 2014 , where it achieves better accuracy than the SVM does .",Baseline Comparison .,Baseline Comparison .,document_classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (6, 8), (8, 9), (9, 12), (13, 17), (17, 21), (24, 25), (25, 27), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1,121,0.9453125,21,1,1,results,Baseline Comparison .,document_classification2
757,757,757,2,Practical Text Classification With Large Pre-Trained Language Models,title,,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004672897,1,0,1,research-problem,title,document_classification20
758,758,758,17,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .",Introduction,Introduction,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 12), (12, 13), (14, 19), (21, 22), (24, 25), (25, 29), (39, 42), (42, 44), (45, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.333333333,16,0.074766355,5,0.333333333,1,approach,Introduction,document_classification20
759,759,759,139,Results,,,document_classification,20,['O'],[],['O'],0,0,138,0.644859813,0,0,1,experiments,,document_classification20
760,760,760,140,Binary Sentiment Tweets,Results,,document_classification,20,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",1,0.014705882,139,0.64953271,0,0,1,results,Results,document_classification20
761,761,761,143,"While the Transformer gets close but does not exceed the state of the art on the SST dataset , it exceeds both the mL - STM and ELMo baseline as well as both Watson and Google Sentiment APIs on the company tweets .",Results,See.,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (6, 9), (10, 14), (14, 15), (16, 18), (20, 21), (23, 29), (33, 38), (38, 39), (40, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.058823529,142,0.663551402,3,0.15,1,results,Results: See.,document_classification20
762,762,762,145,Multi - Label Emotion Tweets,Results,,document_classification,20,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",6,0.088235294,144,0.672897196,5,0.25,1,results,Results,document_classification20
763,763,763,148,We find that our models outperform Watson on every emotion category .,Results,Multi - Label Emotion Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.132352941,147,0.686915888,8,0.4,1,results,Results: Multi - Label Emotion Tweets,document_classification20
764,764,764,149,Sem Eval Tweets,Results,,document_classification,20,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",10,0.147058824,148,0.691588785,9,0.45,1,results,Results,document_classification20
765,765,765,152,"Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .",Results,Sem Eval Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 8), (8, 9), (9, 11), (12, 13), (13, 17), (17, 18), (19, 22), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.191176471,151,0.705607477,12,0.6,1,results,Results: Sem Eval Tweets,document_classification20
766,766,766,154,We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .,Results,Sem Eval Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (9, 13), (18, 19), (21, 22), (22, 23), (24, 26), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.220588235,153,0.714953271,14,0.7,1,results,Results: Sem Eval Tweets,document_classification20
767,767,767,162,Our models gets lower F 1 scores on the company tweets dataset than on equivalent Se -m Eval categories .,Results,Plutchik on Company Tweets,document_classification,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 7), (7, 8), (9, 12), (12, 14), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.338235294,161,0.752336449,1,0.166666667,1,results,Results: Plutchik on Company Tweets,document_classification20
768,768,768,2,Squeezed Very Deep Convolutional Neural Networks for Text Classification,title,,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005235602,1,0,1,research-problem,title,document_classification3
769,769,769,30,"In this paper , we investigate modifications on the network proposed by Conneau et al. with the aim of reducing its number of parameters , storage size and latency with minimal performance degradation .",I. INTRODUCTION,I. INTRODUCTION,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 8), (9, 10), (19, 20), (21, 24), (29, 30), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.165354331,29,0.151832461,27,0.143617021,1,model,I. INTRODUCTION,document_classification3
770,770,770,31,To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques .,I. INTRODUCTION,I. INTRODUCTION,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 10), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.173228346,30,0.157068063,28,0.14893617,1,model,I. INTRODUCTION,document_classification3
771,771,771,32,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .",I. INTRODUCTION,I. INTRODUCTION,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 18), (20, 23), (24, 25), (25, 28), (28, 30), (31, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.181102362,31,0.162303665,29,0.154255319,1,model,I. INTRODUCTION,document_classification3
772,772,772,141,"For SVDCNN and Char - CNN , we calculated the abovementioned number from the network architecture implemented in PyTorch .",EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 6), (8, 9), (12, 13), (14, 16), (16, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.210526316,140,0.732984293,138,0.734042553,1,experimental-setup,EXPERIMENTS,document_classification3
773,773,773,146,"The SVDCNN experimental settings are similar to the original VDCNN paper , using the same dictionary and the same embedding size of 16 .",EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (12, 13), (14, 16), (18, 21), (21, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.473684211,145,0.759162304,143,0.760638298,1,experimental-setup,EXPERIMENTS,document_classification3
774,774,774,147,"The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .",EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 6), (6, 7), (8, 9), (9, 11), (11, 12), (12, 13), (14, 15), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.526315789,146,0.764397906,144,0.765957447,1,experimental-setup,EXPERIMENTS,document_classification3
775,775,775,148,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .",EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (7, 8), (10, 11), (11, 12), (12, 13), (15, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.578947368,147,0.769633508,145,0.771276596,1,experimental-setup,EXPERIMENTS,document_classification3
776,776,776,149,All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,EXPERIMENTS,EXPERIMENTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 11), (12, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947,148,0.77486911,146,0.776595745,1,experimental-setup,EXPERIMENTS,document_classification3
777,777,777,158,The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks .,RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (6, 7), (7, 8), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037,157,0.821989529,155,0.824468085,1,results,RESULTS,document_classification3
778,778,778,159,"Considering a dataset with four target classes , and comparing SVDCNN with VDCNN , the number of parameters of the FC layers has passed from 12.59 to 0.02 million parameters , representing a reduction of 99.84 % .",RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (4, 7), (9, 10), (10, 11), (11, 12), (12, 13), (15, 18), (18, 19), (20, 22), (24, 25), (25, 30), (31, 32), (33, 34), (34, 35), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.074074074,158,0.827225131,156,0.829787234,1,results,RESULTS,document_classification3
779,779,779,160,"Following with the same comparison , but to Char - CNN , the proposed model is 99.82 % smaller , 0.02 against 11.36 million of FC parameters .",RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 11), (13, 15), (15, 16), (21, 22), (22, 24), (24, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.111111111,159,0.832460733,157,0.835106383,1,results,RESULTS,document_classification3
780,780,780,162,"While our most in - depth model ( 29 ) occupies only 6 MB , VDCNN with the same depth occupies 64. 16 MB of storage .",RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 7), (10, 11), (15, 16), (16, 17), (18, 20), (20, 21), (21, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.185185185,161,0.842931937,159,0.845744681,1,results,RESULTS,document_classification3
781,781,781,166,"Regarding accuracy results , usually , a model with such parameter reduction should present some loss of accuracy in comparison to the original model .",RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (18, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.333333333,165,0.863874346,163,0.867021277,1,results,RESULTS,document_classification3
782,782,782,167,"Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .",RESULTS,RESULTS,document_classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 10), (10, 12), (12, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.37037037,166,0.869109948,164,0.872340426,1,results,RESULTS,document_classification3
783,783,783,2,Joint Embedding of Words and Labels for Text Classification,title,,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003773585,1,0,1,research-problem,title,document_classification4
784,784,784,35,"Our primary contribution is therefore to propose such a solution by making use of the label embedding framework , and propose the Label - Embedding Attentive Model ( LEAM ) to improve text classification .",Introduction,Our Contribution,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 18), (20, 21), (22, 30), (30, 32), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.724137931,34,0.128301887,1,0.111111111,1,model,Introduction: Our Contribution,document_classification4
785,785,785,37,"The proposed LEAM is implemented by jointly embedding the word and label in the same latent space , and the text representations are constructed directly using the text - label compatibility .",Introduction,Our Contribution,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 8), (9, 12), (12, 13), (14, 17), (20, 22), (23, 26), (27, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.793103448,36,0.135849057,3,0.333333333,1,model,Introduction: Our Contribution,document_classification4
786,786,786,38,"Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .",Introduction,Our Contribution,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (13, 18), (18, 19), (19, 20), (20, 21), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.827586207,37,0.139622642,4,0.444444444,1,model,Introduction: Our Contribution,document_classification4
787,787,787,39,"( ii ) The LEAM learning procedure only involves a series of basic algebraic operations , and hence it retains the interpretability of simple models , especially when the label description is available .",Introduction,Our Contribution,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (8, 9), (10, 15), (19, 20), (21, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.862068966,38,0.143396226,5,0.555555556,1,model,Introduction: Our Contribution,document_classification4
788,788,788,199,Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model .,Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 10), (10, 11), (11, 12), (12, 13), (13, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.025,198,0.747169811,1,0.125,1,experimental-setup,Experimental Results,document_classification4
789,789,789,200,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 9), (10, 12), (13, 15), (15, 16), (16, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.05,199,0.750943396,2,0.25,1,experimental-setup,Experimental Results,document_classification4
790,790,790,201,The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task .,Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 9), (9, 11), (12, 16), (16, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.075,200,0.754716981,3,0.375,1,baselines,Experimental Results,document_classification4
791,791,791,202,"We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .",Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (6, 7), (8, 10), (18, 19), (20, 23), (23, 24), (24, 25), (28, 30), (30, 31), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1,201,0.758490566,4,0.5,1,experimental-setup,Experimental Results,document_classification4
792,792,792,203,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .",Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 9), (10, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.125,202,0.762264151,5,0.625,1,experimental-setup,Experimental Results,document_classification4
793,793,793,204,The model is implemented using Tensorflow and is trained on GPU Titan X.,Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (8, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.15,203,0.766037736,6,0.75,1,experimental-setup,Experimental Results,document_classification4
794,794,794,205,The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,Experimental Results,Experimental Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.175,204,0.769811321,7,0.875,1,code,Experimental Results,document_classification4
795,795,795,249,"We compare against the three baselines : a logistic regression model with bag - ofwords , a bidirectional gated recurrent unit ( Bi - GRU ) and a single - layer 1 D convolutional network .",Results,Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (8, 11), (11, 12), (12, 15), (17, 26), (28, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.090909091,248,0.935849057,1,0.090909091,1,baselines,Results,document_classification4
796,796,796,250,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .",Results,Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 13), (14, 15), (15, 23), (24, 26), (27, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.181818182,249,0.939622642,2,0.181818182,1,baselines,Results,document_classification4
797,797,797,256,"LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .",Results,Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 6), (8, 13), (13, 14), (14, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.727272727,255,0.962264151,8,0.727272727,1,results,Results,document_classification4
798,798,798,257,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .",Results,Results,document_classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 9), (12, 15), (15, 16), (16, 17), (17, 18), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.818181818,256,0.966037736,9,0.818181818,1,results,Results,document_classification4
799,799,799,2,HDLTex : Hierarchical Deep Learning for Text Classification,title,,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003787879,1,0,1,research-problem,title,document_classification5
800,800,800,5,"Central to these information processing methods is document classification , which has become an important application for supervised learning .",abstract,abstract,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.285714286,4,0.015151515,2,0.011428571,1,research-problem,abstract,document_classification5
801,801,801,9,Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,abstract,abstract,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (9, 10), (10, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.857142857,8,0.03030303,6,0.034285714,1,model,abstract,document_classification5
802,802,802,24,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 10), (12, 13), (13, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.073446328,23,0.087121212,21,0.12,1,model,I. INTRODUCTION,document_classification5
803,803,803,25,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,I. INTRODUCTION,I. INTRODUCTION,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 5), (5, 7), (8, 13), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.079096045,24,0.090909091,22,0.125714286,1,model,I. INTRODUCTION,document_classification5
804,804,804,231,The following results were obtained using a combination of central processing units ( CPUs ) and graphical processing units ( GPUs ) .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 15), (16, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.055555556,230,0.871212121,38,0.690909091,1,results,B. Hardware and Implementation,document_classification5
805,805,805,232,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 14), (14, 15), (15, 17), (18, 20), (23, 25), (25, 26), (26, 30), (31, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.111111111,231,0.875,39,0.709090909,1,experimental-setup,B. Hardware and Implementation,document_classification5
806,806,806,233,"We implemented our approaches in Python using the Compute Unified Device Architecture ( CUDA ) , which is a parallel computing platform and Application Programming Interface ( API ) model created by N vidia .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 6), (6, 7), (8, 15), (17, 18), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.166666667,232,0.878787879,40,0.727272727,1,experimental-setup,B. Hardware and Implementation,document_classification5
807,807,807,234,"We also used Keras and Tensor Flow libraries for creating the neural networks , .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 8), (8, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.222222222,233,0.882575758,41,0.745454545,1,experimental-setup,B. Hardware and Implementation,document_classification5
808,808,808,236,"The baseline tests compare three conventional document classification approaches ( nave Bayes and two versions of SVM ) and stacking SVM with three deep learning approaches ( DNN , RNN , and CNN ) .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 9), (19, 21), (21, 22), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.333333333,235,0.890151515,43,0.781818182,1,baselines,B. Hardware and Implementation,document_classification5
809,809,809,237,In this set of tests the RNN outperforms the others for all three W OS data sets .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 8), (9, 10), (10, 11), (11, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.388888889,236,0.893939394,44,0.8,1,results,B. Hardware and Implementation,document_classification5
810,810,810,238,CNN performs secondbest for three data sets .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 3), (3, 4), (4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.444444444,237,0.897727273,45,0.818181818,1,results,B. Hardware and Implementation,document_classification5
811,811,811,239,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 6), (6, 7), (8, 11), (13, 15), (17, 18), (18, 20), (20, 21), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5,238,0.901515152,46,0.836363636,1,results,B. Hardware and Implementation,document_classification5
812,812,812,242,"Overall , nave Bayes does much worse than the other methods throughout these tests .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 7), (7, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.666666667,241,0.912878788,49,0.890909091,1,results,B. Hardware and Implementation,document_classification5
813,813,813,243,"As for the tests of classifying these documents within a hierarchy , the HDLTex approaches with stacked , deep learning architectures clearly provide superior performance .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 15), (15, 16), (16, 21), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.722222222,242,0.916666667,50,0.909090909,1,results,B. Hardware and Implementation,document_classification5
814,814,814,244,"For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 7), (9, 11), (12, 14), (15, 17), (17, 18), (19, 23), (24, 25), (25, 26), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.777777778,243,0.920454545,51,0.927272727,1,results,B. Hardware and Implementation,document_classification5
815,815,815,245,"This gives accuracies of 94 % for the first level , 92 % for the second level and 86 % over all .",B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6), (6, 7), (8, 10), (11, 13), (13, 14), (15, 17), (18, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.833333333,244,0.924242424,52,0.945454545,1,results,B. Hardware and Implementation,document_classification5
816,816,816,247,For data set W OS ? 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2 .,B. Hardware and Implementation,B. Hardware and Implementation,document_classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 7), (8, 10), (12, 14), (14, 15), (15, 16), (16, 18), (22, 23), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.944444444,246,0.931818182,54,0.981818182,1,results,B. Hardware and Implementation,document_classification5
817,817,817,2,Explicit Interaction Model towards Text Classification,title,,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O']","[(4, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003968254,1,0,1,research-problem,title,document_classification6
818,818,818,31,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 11), (21, 24), (25, 30), (30, 31), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.316666667,30,0.119047619,19,0.575757576,1,model,Introduction,document_classification6
819,819,819,35,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 12), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.383333333,34,0.134920635,23,0.696969697,1,model,Introduction,document_classification6
820,820,820,36,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 7), (7, 10), (11, 15), (16, 18), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.4,35,0.138888889,24,0.727272727,1,model,Introduction,document_classification6
821,821,821,37,The word - level encoder projects the textual contents into the word - level representations .,Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (7, 9), (9, 10), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.416666667,36,0.142857143,25,0.757575758,1,model,Introduction,document_classification6
822,822,822,38,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (7, 9), (9, 10), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.433333333,37,0.146825397,26,0.787878788,1,model,Introduction,document_classification6
823,823,823,39,"Then , the last layer aggregates those matching scores into predictions over each class , respectively .",Introduction,Introduction,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (7, 9), (9, 10), (10, 11), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.45,38,0.150793651,27,0.818181818,1,model,Introduction,document_classification6
824,824,824,129,Experiments,,,document_classification,6,['O'],"[(0, 1)]",['O'],0,0,128,0.507936508,0,0,1,experiments,,document_classification6
825,825,825,130,Multi - Class Classification,Experiments,,document_classification,6,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",1,0.1,129,0.511904762,1,0.010526316,1,results,Experiments,document_classification6
826,826,826,141,"For the multi -class task , we chose region embedding as the Encoder in EXAM .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (8, 10), (10, 11), (12, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,140,0.555555556,12,0.126315789,1,experimental-setup,Hyperparameters,document_classification6
827,827,827,142,The region size is 7 and embedding size is 128 .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (6, 8), (8, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,141,0.55952381,13,0.136842105,1,experimental-setup,Hyperparameters,document_classification6
828,828,828,143,We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 9), (9, 10), (11, 12), (12, 13), (14, 17), (17, 18), (20, 22), (23, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,142,0.563492063,14,0.147368421,1,experimental-setup,Hyperparameters,document_classification6
829,829,829,144,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (7, 8), (9, 10), (10, 11), (12, 14), (14, 15), (15, 17), (15, 20), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,143,0.567460317,15,0.157894737,1,experimental-setup,Hyperparameters,document_classification6
830,830,830,145,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7), (7, 13), (13, 14), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,1,144,0.571428571,16,0.168421053,1,experimental-setup,Hyperparameters,document_classification6
831,831,831,146,Baselines,,,document_classification,6,['O'],"[(0, 1)]",['O'],0,0,145,0.575396825,17,0.178947368,1,experiments,,document_classification6
832,832,832,148,The baselines are mainly in three variants :,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.045454545,147,0.583333333,19,0.2,1,baselines,Baselines,document_classification6
833,833,833,149,1 ) models based on feature engineering ;,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.068181818,148,0.587301587,20,0.210526316,1,baselines,Baselines,document_classification6
834,834,834,150,"2 ) Char - based deep models , and 3 ) Word - based deep models .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 7), (11, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.090909091,149,0.591269841,21,0.221052632,1,baselines,Baselines,document_classification6
835,835,835,154,Models based on feature engineering get the worst results on all the five datasets compared to the other methods .,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 5), (5, 6), (7, 9), (9, 10), (10, 14), (14, 16), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.181818182,153,0.607142857,25,0.263157895,1,results,Baselines,document_classification6
836,836,836,156,Char - based models get the highest over all scores on the two Amazon datasets .,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (6, 10), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.227272727,155,0.615079365,27,0.284210526,1,results,Baselines,document_classification6
837,837,837,160,Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .,Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (6, 8), (8, 9), (9, 11), (12, 13), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.318181818,159,0.630952381,31,0.326315789,1,results,Baselines,document_classification6
838,838,838,162,"For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 8), (8, 9), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.363636364,161,0.638888889,33,0.347368421,1,results,Baselines,document_classification6
839,839,839,163,"It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 8), (9, 11), (11, 12), (13, 15), (16, 17), (18, 20), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.386363636,162,0.642857143,34,0.357894737,1,results,Baselines,document_classification6
840,840,840,164,"For the Yah.A. , EXAM improves the best performance by 1.1 % .",Baselines,Baselines,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (4, 5), (5, 6), (7, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.409090909,163,0.646825397,35,0.368421053,1,results,Baselines,document_classification6
841,841,841,176,Multi - Label Classification,Baselines,,document_classification,6,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",30,0.681818182,175,0.694444444,47,0.494736842,1,baselines,Baselines,document_classification6
842,842,842,194,We implemented the baseline models and EXAM by MXNet .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.032258065,193,0.765873016,65,0.684210526,1,experimental-setup,Hyperparameters,document_classification6
843,843,843,195,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 6), (6, 7), (7, 9), (10, 12), (15, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.064516129,194,0.76984127,66,0.694736842,1,experimental-setup,Hyperparameters,document_classification6
844,844,844,196,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .",Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 6), (8, 11), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.096774194,195,0.773809524,67,0.705263158,1,experimental-setup,Hyperparameters,document_classification6
845,845,845,197,The accumulated MLP has 60 hidden units .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.129032258,196,0.777777778,68,0.715789474,1,experimental-setup,Hyperparameters,document_classification6
846,846,846,198,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 11), (11, 12), (13, 15), (15, 16), (16, 17), (19, 22), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.161290323,197,0.781746032,69,0.726315789,1,experimental-setup,Hyperparameters,document_classification6
847,847,847,199,The validation set is applied for early - stopping to avoid overfitting .,Hyperparameters,Hyperparameters,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 9), (9, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.193548387,198,0.785714286,70,0.736842105,1,experimental-setup,Hyperparameters,document_classification6
848,848,848,210,Word - based models are better than char - based models in Kanshan - Cup dataset .,Hyperparameters,Metrics,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (5, 7), (7, 11), (11, 12), (12, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.548387097,209,0.829365079,81,0.852631579,1,results,Hyperparameters: Metrics,document_classification6
849,849,849,213,Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .,Hyperparameters,Metrics,document_classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.64516129,212,0.841269841,84,0.884210526,1,results,Hyperparameters: Metrics,document_classification6
850,850,850,2,A Corpus for Multilingual Document Classification in Eight Languages,title,,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.007194245,1,0,1,research-problem,title,document_classification7
851,851,851,4,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .,abstract,abstract,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.021582734,1,0.1,1,research-problem,abstract,document_classification7
852,852,852,31,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .",Introduction,Introduction,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 9), (14, 16), (23, 24), (24, 28), (30, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.303571429,30,0.215827338,17,0.80952381,1,research-problem,Introduction,document_classification7
853,853,853,32,"For each language , we define a train , development and test corpus .",Introduction,Introduction,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (7, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.321428571,31,0.223021583,18,0.857142857,1,model,Introduction,document_classification7
854,854,854,114,Zero - shot cross - lingual document classification,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.682539683,113,0.81294964,10,0.4,1,results,Baseline results,document_classification7
855,855,855,116,The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 7), (7, 8), (8, 10), (10, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.714285714,115,0.827338129,12,0.48,1,results,Baseline results,document_classification7
856,856,856,117,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 7), (7, 9), (9, 10), (10, 12), (13, 15), (17, 18), (18, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.73015873,116,0.834532374,13,0.52,1,results,Baseline results,document_classification7
857,857,857,119,"However , the transfer accuracies are quite low when training the classifiers on other languages than English , in particular for Russian , Chinese and Japanese .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 8), (8, 10), (11, 12), (12, 13), (13, 15), (15, 16), (16, 17), (21, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.761904762,118,0.848920863,15,0.6,1,results,Baseline results,document_classification7
858,858,858,120,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.777777778,119,0.856115108,16,0.64,1,results,Baseline results,document_classification7
859,859,859,122,Training on German or French actually leads to better transfer performance than training on English .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 5), (6, 8), (8, 11), (11, 12), (12, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.80952381,121,0.870503597,18,0.72,1,results,Baseline results,document_classification7
860,860,860,123,Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 6), (6, 7), (7, 10), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.825396825,122,0.877697842,19,0.76,1,results,Baseline results,document_classification7
861,861,861,130,Joint multilingual document classification,Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",59,0.936507937,129,0.928057554,0,0,1,results,Baseline results,document_classification7
862,862,862,134,"This leads to important improvement for all languages , in comparison to zero - shot or targeted transfer learning .",Baseline results,Baseline results,document_classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 6), (6, 8), (9, 12), (12, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,1,133,0.956834532,4,1,1,results,Baseline results,document_classification7
863,863,863,2,Disconnected Recurrent Neural Networks for Text Categorization,title,title,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003875969,1,0,1,research-problem,title,document_classification8
864,864,864,23,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .",Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 8), (8, 9), (10, 11), (12, 14), (14, 15), (15, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.371428571,22,0.085271318,13,0.371428571,1,model,Introduction,document_classification8
865,865,865,27,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .",Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (8, 9), (9, 11), (11, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.485714286,26,0.100775194,17,0.485714286,1,model,Introduction,document_classification8
866,866,866,28,Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units .,Introduction,Introduction,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (9, 12), (12, 13), (13, 15), (16, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.514285714,27,0.104651163,18,0.514285714,1,model,Introduction,document_classification8
867,867,867,156,"We utilize the 300D Glo Ve 840B vectors ( Pennington et al. , 2014 ) as our pre-trained word embeddings .",Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 8), (15, 16), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,155,0.600775194,5,0.3125,1,hyperparameters,Implementation Details,document_classification8
868,868,868,159,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .",Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 8), (8, 10), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5,158,0.612403101,8,0.5,1,hyperparameters,Implementation Details,document_classification8
869,869,869,160,The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 7), (14, 18), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,159,0.61627907,9,0.5625,1,hyperparameters,Implementation Details,document_classification8
870,870,870,161,"To avoid the gradient explosion problem , we apply gradient norm clipping .",Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (8, 9), (9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,160,0.620155039,10,0.625,1,hyperparameters,Implementation Details,document_classification8
871,871,871,162,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,Implementation Details,Implementation Details,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (8, 11), (11, 12), (12, 16), (16, 17), (18, 21), (21, 23), (23, 27), (27, 28), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,161,0.624031008,11,0.6875,1,experiments,Implementation Details,document_classification8
872,872,872,172,We can see that very deep CNN ( VDCNN ) performs well in large datasets .,Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 10), (10, 11), (11, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.047058824,171,0.662790698,4,0.333333333,1,results,Experimental Results,document_classification8
873,873,873,180,shows that our model achieves 10 - 50 % relative error reduction compared with char - CRNN in these datasets .,Experimental Results,Experimental Results,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (5, 12), (12, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.141176471,179,0.69379845,12,1,1,results,Experimental Results,document_classification8
874,874,874,186,shows that DRNN performs far better than CNN .,Experimental Results,Comparison with RNN and CNN,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (4, 6), (6, 7), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.211764706,185,0.717054264,5,0.416666667,1,results,Experimental Results: Comparison with RNN and CNN,document_classification8
875,875,875,193,Our model DRNN achieves much better performance than GRU and LSTM .,Experimental Results,The experimental results are shown in .,document_classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 7), (7, 8), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.294117647,192,0.744186047,12,1,1,results,Experimental Results: The experimental results are shown in .,document_classification8
876,876,876,2,Investigating Capsule Networks with Dynamic Routing for Text Classification,title,,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004115226,1,0,1,research-problem,title,document_classification9
877,877,877,10,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,abstract,abstract,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,1,9,0.037037037,7,1,1,code,abstract,document_classification9
878,878,878,31,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,Introduction,Introduction,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.740740741,30,0.12345679,20,0.740740741,1,model,Introduction,document_classification9
879,879,879,32,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,Introduction,Introduction,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 8), (9, 11), (11, 12), (12, 13), (13, 14), (14, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.777777778,31,0.127572016,21,0.777777778,1,model,Introduction,document_classification9
880,880,880,36,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words thatare unrelated to specific categories .",Introduction,Introduction,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (4, 6), (7, 10), (10, 12), (13, 14), (14, 15), (15, 18), (21, 25), (25, 27), (27, 29), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.925925926,35,0.144032922,25,0.925925926,1,model,Introduction,document_classification9
881,881,881,139,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",Implementation Details,Implementation Details,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 11), (11, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,138,0.567901235,1,0.25,1,hyperparameters,Implementation Details,document_classification9
882,882,882,140,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,Implementation Details,Implementation Details,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 10), (11, 12), (12, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,139,0.572016461,2,0.5,1,hyperparameters,Implementation Details,document_classification9
883,883,883,141,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,Implementation Details,Implementation Details,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 11), (11, 13), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,140,0.576131687,3,0.75,1,hyperparameters,Implementation Details,document_classification9
884,884,884,144,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .",Baseline methods,Baseline methods,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 22), (23, 32), (36, 43), (44, 53), (59, 68), (70, 80)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,143,0.588477366,1,0,1,baselines,Baseline methods,document_classification9
885,885,885,145,Experimental Results,,,document_classification,9,"['O', 'O']",[],"['O', 'O']",0,0,144,0.592592593,0,0,1,experiments,,document_classification9
886,886,886,149,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",Quantitative Evaluation,Quantitative Evaluation,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (8, 10), (10, 11), (11, 13), (13, 14), (14, 19), (21, 22), (23, 24), (24, 25), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,148,0.609053498,3,0.75,1,results,Quantitative Evaluation,document_classification9
887,887,887,156,Single - Label to Multi - Label Text Classification,Ablation Study,,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.090909091,155,0.637860082,5,0.178571429,1,ablation-analysis,Ablation Study,document_classification9
888,888,888,163,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .",Ablation Study,Single - Label to Multi - Label Text Classification,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 9), (9, 11), (11, 12), (12, 15), (15, 17), (19, 23), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.218181818,162,0.666666667,12,0.428571429,1,ablation-analysis,Ablation Study: Single - Label to Multi - Label Text Classification,document_classification9
889,889,889,175,"From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .",Ablation Study,Single - Label to Multi - Label Text Classification,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (6, 7), (9, 11), (11, 12), (12, 16), (16, 19), (19, 23), (23, 24), (25, 28), (28, 29), (30, 32), (32, 33), (38, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.436363636,174,0.716049383,24,0.857142857,1,ablation-analysis,Ablation Study: Single - Label to Multi - Label Text Classification,document_classification9
890,890,890,176,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .",Ablation Study,Single - Label to Multi - Label Text Classification,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (8, 14), (18, 20), (20, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.454545455,175,0.720164609,25,0.892857143,1,ablation-analysis,Ablation Study: Single - Label to Multi - Label Text Classification,document_classification9
891,891,891,178,The capsule network has much stronger transferring capability than the conventional deep neural networks .,Ablation Study,Single - Label to Multi - Label Text Classification,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 8), (8, 9), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.490909091,177,0.728395062,27,0.964285714,1,ablation-analysis,Ablation Study: Single - Label to Multi - Label Text Classification,document_classification9
892,892,892,179,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",Ablation Study,Single - Label to Multi - Label Text Classification,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 10), (11, 12), (14, 16), (17, 19), (19, 20), (20, 21), (21, 22), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.509090909,178,0.732510288,28,1,1,ablation-analysis,Ablation Study: Single - Label to Multi - Label Text Classification,document_classification9
893,893,893,180,Connection Strength Visualization,Ablation Study,,document_classification,9,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",29,0.527272727,179,0.736625514,0,0,1,ablation-analysis,Ablation Study,document_classification9
894,894,894,188,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .",Ablation Study,Connection Strength Visualization,document_classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (5, 6), (7, 9), (10, 14), (15, 17), (17, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.672727273,187,0.769547325,8,0.666666667,1,ablation-analysis,Ablation Study: Connection Strength Visualization,document_classification9
895,895,895,2,Deep Joint Entity Disambiguation with Local Neural Attention,title,,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003891051,1,0,1,research-problem,title,entity_linking0
896,896,896,9,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .,Introduction,Introduction,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.090909091,8,0.031128405,1,0.090909091,1,research-problem,Introduction,entity_linking0
897,897,897,12,"ED research has largely focused on two types of contextual information for disambiguation : local information based on words that occur in a context window around an entity mention , and , global information , exploiting document - level coherence of the referenced entities .",Introduction,Introduction,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (14, 16), (16, 18), (18, 19), (23, 25), (25, 26), (32, 34), (35, 36), (36, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.363636364,11,0.042801556,4,0.363636364,1,research-problem,Introduction,entity_linking0
898,898,898,18,The explicit goal of our work is to use deep learning in order to learn basic features and their combinations from scratch .,Introduction,Introduction,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11), (13, 15), (15, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.909090909,17,0.06614786,10,0.909090909,1,model,Introduction,entity_linking0
899,899,899,179,All models are implemented in the Torch framework .,Experiments,Training Details and ( Hyper ) Parameters,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.243902439,178,0.692607004,9,0.225,1,experimental-setup,Experiments: Training Details and ( Hyper ) Parameters,entity_linking0
900,900,900,181,"For entity embeddings only , we use Wikipedia ( Feb 2014 ) corpus for training .",Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 7), (7, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.292682927,180,0.700389105,11,0.275,1,experimental-setup,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
901,901,901,182,Entity vectors are initialized randomly from a 0 mean normal distribution with standard deviation 1 .,Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (5, 6), (7, 11), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.317073171,181,0.704280156,12,0.3,1,experiments,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
902,902,902,183,We first train each entity vector on the entity 's Wikipedia canonical description page ( title words included ) for 400 iterations .,Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (6, 7), (8, 14), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.341463415,182,0.708171206,13,0.325,1,experiments,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
903,903,903,186,We use Adagrad with a learning rate of 0.3 .,Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.414634146,185,0.719844358,16,0.4,1,experiments,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
904,904,904,187,"We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .",Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 7), (8, 16), (16, 25), (26, 28), (28, 29), (29, 30), (30, 31), (32, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.43902439,186,0.723735409,17,0.425,1,experiments,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
905,905,905,190,Training of those takes 20 hours on a single TitanX GPU with 12 GB of memory .,Experiments,Entity Vectors Training & Relatedness Evaluation .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 4), (4, 6), (6, 7), (8, 11), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.512195122,189,0.73540856,20,0.5,1,experimental-setup,Experiments: Entity Vectors Training & Relatedness Evaluation .,entity_linking0
906,906,906,199,"Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6), (7, 9), (9, 16), (17, 19), (19, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.731707317,198,0.770428016,29,0.725,1,experimental-setup,Experiments: Local and Global Model Training .,entity_linking0
907,907,907,200,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6), (6, 7), (7, 10), (10, 11), (11, 13), (13, 14), (14, 16), (18, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.756097561,199,0.774319066,30,0.75,1,experimental-setup,Experiments: Local and Global Model Training .,entity_linking0
908,908,908,206,"To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 7), (10, 11), (11, 12), (12, 13), (14, 16), (16, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.902439024,205,0.79766537,36,0.9,1,experimental-setup,Experiments: Local and Global Model Training .,entity_linking0
909,909,909,207,"Training on a single GPU takes , on average , 2 ms per mention , or 16 hours for 1250 epochs over AIDA - train .",Experiments,Local and Global Model Training .,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 6), (7, 9), (10, 12), (12, 13), (13, 14), (16, 18), (18, 19), (19, 21), (21, 22), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.926829268,206,0.80155642,37,0.925,1,experimental-setup,Experiments: Local and Global Model Training .,entity_linking0
910,910,910,228,We obtain state of the art accuracy on AIDA which is the largest and hardest ( by the accuracy of thep ( e |m ) baseline ) manually created ED dataset .,Entity Similarity Results,Entity Similarity Results,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 7), (7, 8), (8, 9), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.435897436,227,0.883268482,8,0.533333333,1,results,Entity Similarity Results,entity_linking0
911,911,911,232,"To gain further insight , we analyzed the accuracy on the AIDA - B dataset for situations where gold entities have low frequency or mention prior .",Entity Similarity Results,Entity Similarity Results,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 9), (9, 10), (11, 15), (15, 16), (18, 20), (20, 21), (21, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.538461538,231,0.898832685,12,0.8,1,results,Entity Similarity Results,entity_linking0
912,912,912,233,shows that our method performs well in these harder cases . :,Entity Similarity Results,Entity Similarity Results,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (5, 6), (6, 7), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.564102564,232,0.902723735,13,0.866666667,1,results,Entity Similarity Results,entity_linking0
913,913,913,257,Our code and data are publicly available : http://github.com/dalab/deep-ed,Conclusion,Conclusion,entity_linking,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,256,0.996108949,6,1,0,code,Conclusion,entity_linking0
914,914,914,2,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation,title,title,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.007874016,1,0,1,research-problem,title,entity_linking1
915,915,915,5,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .",abstract,abstract,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (11, 12), (12, 15), (15, 16), (16, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.285714286,4,0.031496063,2,0.285714286,1,research-problem,abstract,entity_linking1
916,916,916,9,We evaluated our model by addressing NED using a simple NED model based on the trained contextualized embeddings .,abstract,abstract,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 8), (9, 12), (12, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.857142857,8,0.062992126,6,0.857142857,1,model,abstract,entity_linking1
917,917,917,18,"In this paper , we describe a new contextualized embedding model for words and entities for NED .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 12), (12, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.466666667,17,0.133858268,7,0.466666667,1,model,Introduction,entity_linking1
918,918,918,19,"Following , the proposed model is based on the bidirectional transformer encoder .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.533333333,18,0.141732283,8,0.533333333,1,model,Introduction,entity_linking1
919,919,919,20,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 8), (8, 9), (10, 12), (14, 15), (16, 18), (18, 19), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6,19,0.149606299,9,0.6,1,model,Introduction,entity_linking1
920,920,920,21,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .",Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.666666667,20,0.157480315,10,0.666666667,1,model,Introduction,entity_linking1
921,921,921,22,We trained the model using texts and their entity annotations retrieved from Wikipedia .,Introduction,Introduction,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 10), (10, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.733333333,21,0.165354331,11,0.733333333,1,model,Introduction,entity_linking1
922,922,922,113,"As shown , our models outperformed all previously proposed models .",Results,Results,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,112,0.881889764,3,0.75,1,results,Results,entity_linking1
923,923,923,114,"Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .",Results,Results,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (6, 7), (8, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,1,113,0.88976378,4,1,1,results,Results,entity_linking1
924,924,924,118,"Our models achieved new state - of - the - art results on four of the five datasets , namely MSNBC , AQUAINT , ACE2004 , and WNED - WIKI , and performed competitive on the WNED - CLUEWEB dataset .",Methods,Methods,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 12), (12, 13), (13, 18), (19, 20), (20, 21), (22, 23), (24, 25), (27, 30), (32, 33), (33, 34), (34, 35), (36, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,117,0.921259843,3,0.5,1,results,Methods,entity_linking1
925,925,925,119,"Furthermore , using pseudo entity annotations improved the performance on the AQUAINT and ACE2004 datasets .",Methods,Methods,entity_linking,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (6, 7), (8, 9), (9, 10), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,118,0.929133858,4,0.666666667,1,results,Methods,entity_linking1
926,926,926,2,Deep contextualized word representations,title,,entity_linking,10,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",1,0,1,0.004672897,1,0,1,research-problem,title,entity_linking10
927,927,927,4,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",abstract,abstract,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.014018692,1,0.25,1,research-problem,abstract,entity_linking10
928,928,928,9,Pre-trained word representations are a key component in many neural language understanding models .,Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.026315789,8,0.037383178,1,0.027027027,1,research-problem,Introduction,entity_linking10
929,929,929,12,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.105263158,11,0.051401869,4,0.108108108,1,model,Introduction,entity_linking10
930,930,930,13,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 8), (10, 12), (13, 14), (15, 16), (19, 21), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.131578947,12,0.056074766,5,0.135135135,1,model,Introduction,entity_linking10
931,931,931,14,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (6, 8), (10, 12), (13, 22), (22, 23), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.157894737,13,0.060747664,6,0.162162162,1,model,Introduction,entity_linking10
932,932,932,15,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.184210526,14,0.065420561,7,0.189189189,1,model,Introduction,entity_linking10
933,933,933,16,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11), (11, 12), (12, 13), (21, 23), (28, 29), (30, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.210526316,15,0.070093458,8,0.216216216,1,model,Introduction,entity_linking10
934,934,934,17,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 11), (10, 11), (11, 13), (13, 16), (16, 17), (17, 20), (22, 24), (24, 25), (25, 26), (29, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.236842105,16,0.074766355,9,0.243243243,1,model,Introduction,entity_linking10
935,935,935,19,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense disambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",Introduction,Introduction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 6), (8, 13), (13, 14), (14, 18), (18, 19), (19, 21), (41, 43), (43, 44), (44, 45), (46, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.289473684,18,0.08411215,11,0.297297297,1,model,Introduction,entity_linking10
936,936,936,103,Question answering,Evaluation,,entity_linking,10,"['O', 'O']","[(0, 2)]","['O', 'O']",1,0.009174312,102,0.476635514,1,0.034482759,1,experiments,Evaluation,entity_linking10
937,937,937,107,"After adding ELMo to the baseline model , test set F 1 improved by 4.7 % from 81.1 % to 85.8 % , a 24.9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1.4 % .",Evaluation,Question answering,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 7), (8, 12), (12, 13), (13, 14), (14, 16), (16, 17), (17, 22), (24, 29), (29, 30), (31, 32), (34, 35), (36, 46), (46, 47), (47, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.04587156,106,0.495327103,5,0.172413793,1,experiments,Evaluation: Question answering,entity_linking10
938,938,938,109,The increase of 4.7 % with ELMo is also significantly larger then the 1.8 % improvement from adding CoVe to a baseline model .,Evaluation,Question answering,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (9, 11), (11, 12), (13, 16), (16, 18), (18, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.064220183,108,0.504672897,7,0.24137931,1,experiments,Evaluation: Question answering,entity_linking10
939,939,939,112,Textual entailment,Evaluation,,entity_linking,10,"['O', 'O']","[(0, 2)]","['O', 'O']",10,0.091743119,111,0.518691589,10,0.344827586,1,experiments,Evaluation,entity_linking10
940,940,940,116,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",Evaluation,Textual entailment,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (6, 8), (8, 9), (9, 10), (10, 11), (12, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.128440367,115,0.537383178,14,0.482758621,1,experiments,Evaluation: Textual entailment,entity_linking10
941,941,941,118,Semantic role labeling,Evaluation,,entity_linking,10,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",16,0.146788991,117,0.546728972,16,0.551724138,1,experiments,Evaluation,entity_linking10
942,942,942,124,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",Evaluation,Semantic role labeling,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 9), (10, 14), (15, 16), (16, 17), (17, 18), (19, 22), (22, 23), (23, 25), (25, 26), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.201834862,123,0.574766355,22,0.75862069,1,experiments,Evaluation: Semantic role labeling,entity_linking10
943,943,943,125,Named entity extraction,Evaluation,,entity_linking,10,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",23,0.211009174,124,0.579439252,23,0.793103448,1,experiments,Evaluation,entity_linking10
944,944,944,128,"As shown in , our ELMo enhanced biLSTM - CRF achieves 92. 22 % F 1 averaged over five runs .",Evaluation,Named entity extraction,entity_linking,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 10), (10, 11), (11, 16), (16, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.23853211,127,0.593457944,26,0.896551724,1,experiments,Evaluation: Named entity extraction,entity_linking10
945,945,945,2,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,title,title,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005181347,1,0,1,research-problem,title,entity_linking11
946,946,946,5,"We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision .",abstract,abstract,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (6, 8), (9, 10), (10, 11), (11, 14), (15, 16), (19, 20), (21, 22), (22, 23), (23, 26), (28, 30), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.666666667,4,0.020725389,2,0.666666667,1,approach,abstract,entity_linking11
947,947,947,6,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .",abstract,abstract,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 11), (12, 14), (14, 18), (20, 22), (24, 26), (27, 31), (31, 32), (32, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,1,5,0.025906736,3,1,1,approach,abstract,entity_linking11
948,948,948,8,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",Introduction,Introduction,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.05,7,0.03626943,1,0.05,1,research-problem,Introduction,entity_linking11
949,949,949,17,"In this work , the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet , such as the hypernymy , the hyponymy , the meronymy , the antonymy , etc .",Introduction,Introduction,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 15), (16, 18), (18, 19), (19, 20), (20, 22), (22, 23), (24, 26), (27, 28), (30, 31), (33, 34), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5,16,0.082901554,10,0.5,1,approach,Introduction,entity_linking11
950,950,950,18,"Our method is based on the observation that a sense and its closest related senses ( it s hypernym or it s hyponyms for instance ) all share a common idea or concept , and so a word can sometimes be disambiguated using only related concepts .",Introduction,Introduction,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 7), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,17,0.088082902,11,0.55,1,approach,Introduction,entity_linking11
951,951,951,132,"For BERT , we used the model named "" bert - largecased "" of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .",Implementation details,Implementation details,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (6, 7), (7, 8), (9, 12), (13, 14), (15, 17), (20, 22), (22, 23), (23, 24), (25, 26), (27, 29), (29, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,131,0.678756477,1,0.25,1,experimental-setup,Implementation details,entity_linking11
952,952,952,134,"For the Transformer encoder layers , we used the same parameters as the "" base "" model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .",Implementation details,Implementation details,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (9, 11), (11, 12), (13, 17), (21, 23), (23, 24), (24, 27), (29, 31), (31, 32), (32, 33), (36, 37), (37, 38), (38, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,133,0.689119171,3,0.75,1,experimental-setup,Implementation details,entity_linking11
953,953,953,164,"In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .",Evaluation,Evaluation,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 11), (11, 13), (14, 17), (17, 18), (21, 23), (23, 24), (24, 25), (27, 29), (29, 30), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.705882353,163,0.844559585,12,0.705882353,1,results,Evaluation,entity_linking11
954,954,954,169,"In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .",Evaluation,Evaluation,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (10, 14), (14, 16), (21, 23), (23, 24), (24, 25), (25, 27), (29, 30), (30, 31), (32, 36), (36, 37), (37, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,1,168,0.870466321,17,1,1,results,Evaluation,entity_linking11
955,955,955,180,"As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 13), (17, 19), (19, 20), (20, 21), (21, 23), (26, 28), (28, 29), (32, 34), (34, 35), (35, 36), (37, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,179,0.92746114,10,0.625,1,ablation-analysis,Ablation Study,entity_linking11
956,956,956,181,"Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 4), (4, 8), (8, 9), (11, 12), (12, 13), (13, 18), (19, 21), (23, 24), (25, 26), (26, 27), (28, 30), (30, 31), (32, 33), (33, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,180,0.932642487,11,0.6875,1,ablation-analysis,Ablation Study,entity_linking11
957,957,957,182,"Finally , using ensembles adds roughly another 1 point to the final F1 score .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 9), (9, 10), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.75,181,0.937823834,12,0.75,1,ablation-analysis,Ablation Study,entity_linking11
958,958,958,186,"However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .",Ablation Study,Ablation Study,entity_linking,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 8), (8, 10), (10, 12), (13, 14), (14, 15), (15, 17), (18, 20), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,1,185,0.958549223,16,1,1,ablation-analysis,Ablation Study,entity_linking11
959,959,959,2,Incorporating Glosses into Neural Word Sense Disambiguation,title,title,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003759398,0,0,1,research-problem,title,entity_linking12
960,960,960,4,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,abstract,abstract,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.011278195,1,0.125,1,research-problem,abstract,entity_linking12
961,961,961,5,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,abstract,abstract,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 4), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,4,0.015037594,2,0.25,1,research-problem,abstract,entity_linking12
962,962,962,30,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (12, 18), (21, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.666666667,29,0.109022556,18,0.666666667,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
963,963,963,31,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 7), (7, 8), (9, 11), (12, 13), (14, 16), (16, 17), (18, 21), (21, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.703703704,30,0.112781955,19,0.703703704,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
964,964,964,32,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",Introduction,There are several lines of research on WSD .,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (8, 11), (15, 16), (16, 19), (19, 20), (21, 22), (22, 23), (24, 26), (27, 28), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.740740741,31,0.116541353,20,0.740740741,1,model,Introduction: There are several lines of research on WSD .,entity_linking12
965,965,965,65,Incorporating Glosses into Neural Word Sense Disambiguation,Related Work,Related Work,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.925925926,64,0.240601504,0,0,0,experiments,Related Work,entity_linking12
966,966,966,195,"We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 8), (13, 14), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.222222222,194,0.729323308,2,0.222222222,1,hyperparameters,Implementation Details,entity_linking12
967,967,967,196,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.333333333,195,0.733082707,3,0.333333333,1,hyperparameters,Implementation Details,entity_linking12
968,968,968,197,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 6), (6, 7), (7, 8), (9, 12), (12, 13), (13, 20), (21, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.444444444,196,0.736842105,4,0.444444444,1,hyperparameters,Implementation Details,entity_linking12
969,969,969,198,We assign gloss expansion depth K the value of 4 .,Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (7, 8), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.555555556,197,0.740601504,5,0.555555556,1,hyperparameters,Implementation Details,entity_linking12
970,970,970,199,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 12), (12, 13), (13, 16), (16, 17), (17, 19), (20, 21), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.666666667,198,0.744360902,6,0.666666667,1,hyperparameters,Implementation Details,entity_linking12
971,971,971,200,We use Adam optimizer in the training process with 0.001 initial learning rate .,Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (8, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.777777778,199,0.748120301,7,0.777777778,1,hyperparameters,Implementation Details,entity_linking12
972,972,972,201,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (7, 8), (8, 10), (11, 12), (12, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.888888889,200,0.751879699,8,0.888888889,1,hyperparameters,Implementation Details,entity_linking12
973,973,973,202,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,Implementation Details,Implementation Details,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 7), (7, 8), (8, 10), (10, 11), (12, 14), (14, 17), (17, 18), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,1,201,0.755639098,9,1,1,hyperparameters,Implementation Details,entity_linking12
974,974,974,210,Knowledge - based Systems,,,entity_linking,12,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",0,0,209,0.785714286,7,0.777777778,1,experiments,,entity_linking12
975,975,975,212,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,Knowledge - based Systems,Knowledge - based Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (4, 7), (7, 8), (8, 9), (10, 11), (12, 17), (17, 18), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,1,211,0.793233083,9,1,1,baselines,Knowledge - based Systems,entity_linking12
976,976,976,213,Supervised Systems,,,entity_linking,12,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,212,0.796992481,0,0,1,experiments,,entity_linking12
977,977,977,215,"IMS : Zhi and Ng ( 2010 ) selects a linear Support Vector Machine ( SVM ) as its classifier and makes use of a set of features surrounding the target word within a limited window , such as POS tags , local words and local collocations .",Supervised Systems,Supervised Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (8, 9), (10, 17), (17, 18), (19, 20), (21, 24), (25, 28), (28, 29), (30, 32), (32, 33), (34, 36), (37, 39), (39, 41), (42, 44), (45, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.666666667,214,0.804511278,2,0.2,1,baselines,Supervised Systems,entity_linking12
978,978,978,216,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,Supervised Systems,Supervised Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (4, 5), (5, 6), (7, 9), (10, 13), (13, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,1,215,0.808270677,3,0.3,1,baselines,Supervised Systems,entity_linking12
979,979,979,217,Neural - based Systems,,,entity_linking,12,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",0,0,216,0.812030075,4,0.4,1,experiments,,entity_linking12
980,980,980,219,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,Neural - based Systems,Neural - based Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (5, 8), (9, 10), (10, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,218,0.819548872,6,0.6,1,baselines,Neural - based Systems,entity_linking12
981,981,981,221,"Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS : transfers WSD into a sequence learning task and propose a multi - task learning framework for WSD , POS tagging and coarse - grained semantic labels ( LEX ) .",Neural - based Systems,Neural - based Systems,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 14), (14, 15), (15, 16), (17, 20), (21, 22), (23, 28), (28, 29), (29, 30), (31, 33), (34, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,220,0.827067669,8,0.8,1,baselines,Neural - based Systems,entity_linking12
982,982,982,225,English all - words results,Results and Discussion,,entity_linking,12,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",1,0.029411765,224,0.842105263,1,0.058823529,1,results,Results and Discussion,entity_linking12
983,983,983,230,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (6, 12), (12, 13), (14, 16), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.176470588,229,0.860902256,6,0.352941176,1,results,Results and Discussion: English all - words results,entity_linking12
984,984,984,231,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(18, 20), (20, 22), (22, 23), (23, 27), (27, 28), (29, 31), (31, 32), (32, 33), (34, 36), (37, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.205882353,230,0.864661654,7,0.411764706,1,results,Results and Discussion: English all - words results,entity_linking12
985,985,985,234,It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (8, 9), (10, 11), (14, 15), (15, 18), (18, 19), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.294117647,233,0.87593985,10,0.588235294,1,results,Results and Discussion: English all - words results,entity_linking12
986,986,986,242,Multiple Passes Analysis,Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",18,0.529411765,241,0.906015038,0,0,1,results,Results and Discussion: English all - words results,entity_linking12
987,987,987,252,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (1, 2), (3, 6), (6, 7), (7, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.823529412,251,0.943609023,10,0.625,1,results,Results and Discussion: English all - words results,entity_linking12
988,988,988,256,"In Table 3 , with the increasing number of passes , the F1 - score increases .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 10), (12, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.941176471,255,0.958646617,14,0.875,1,results,Results and Discussion: English all - words results,entity_linking12
989,989,989,257,"However , when the number of passes is larger than 3 , the F1- score stops increasing or even decreases due to over-fitting .",Results and Discussion,English all - words results,entity_linking,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (8, 10), (10, 11), (13, 15), (15, 16), (16, 20), (20, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.970588235,256,0.962406015,15,0.9375,1,results,Results and Discussion: English all - words results,entity_linking12
990,990,990,2,Word Sense Disambiguation using a Bidirectional LSTM,title,title,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00862069,1,0,1,research-problem,title,entity_linking13
991,991,991,14,"The task of assigning a word token in a text , e.g. rock , to a well defined word sense in a lexicon is called word sense disambiguation ( WSD ) .",Introduction,Introduction,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(25, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.157894737,13,0.112068966,3,0.157894737,1,research-problem,Introduction,entity_linking13
992,992,992,19,"Improved WSD would be beneficial to many natural language processing ( NLP ) problems , e.g. machine translation , information Retrieval , information Extraction , and sense aware word representations .",Introduction,Introduction,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.421052632,18,0.155172414,8,0.421052632,1,research-problem,Introduction,entity_linking13
993,993,993,23,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .",Introduction,Introduction,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 15), (15, 16), (17, 19), (36, 37), (38, 39), (39, 40), (40, 44), (46, 48)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947,22,0.189655172,12,0.631578947,1,model,Introduction,entity_linking13
994,994,994,83,"The source code , implemented using TensorFlow , has been released as open source 1 .",Experimental settings,Experimental settings,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.133333333,82,0.706896552,2,1,1,experimental-setup,Experimental settings,entity_linking13
995,995,995,87,The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,Experimental settings,Embeddings,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (8, 14), (14, 16), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,86,0.74137931,3,0.428571429,1,experimental-setup,Experimental settings: Embeddings,entity_linking13
996,996,996,88,"Words not included in this set are initialized from N ( 0 , 0.1 ) .",Experimental settings,Embeddings,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (7, 9), (9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.466666667,87,0.75,4,0.571428571,1,experimental-setup,Experimental settings: Embeddings,entity_linking13
997,997,997,106,Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (5, 7), (7, 8), (8, 9), (11, 13), (13, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.692307692,105,0.905172414,9,0.75,1,results,Results,entity_linking13
998,998,998,107,"Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .",Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 8), (9, 10), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.769230769,106,0.913793103,10,0.833333333,1,results,Results,entity_linking13
999,999,999,108,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .",Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (5, 7), (7, 8), (9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.846153846,107,0.922413793,11,0.916666667,1,results,Results,entity_linking13
1000,1000,1000,109,We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these datasets .,Results,Results,entity_linking,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 6), (6, 10), (11, 12), (12, 13), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.923076923,108,0.931034483,12,1,1,results,Results,entity_linking13
1001,1001,1001,2,Knowledge - based Word Sense Disambiguation using Topic Models,title,title,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005128205,1,0,1,research-problem,title,entity_linking14
1002,1002,1002,4,Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data .,abstract,abstract,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,3,0.015384615,1,0.142857143,1,research-problem,abstract,entity_linking14
1003,1003,1003,5,Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context .,abstract,abstract,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (5, 6), (12, 13), (14, 16), (16, 17), (18, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.285714286,4,0.020512821,2,0.285714286,1,research-problem,abstract,entity_linking14
1004,1004,1004,12,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .,Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.055555556,11,0.056410256,1,0.055555556,1,research-problem,Introduction,entity_linking14
1005,1005,1005,22,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .",Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13), (13, 14), (15, 20), (22, 23), (24, 26), (26, 27), (28, 29), (29, 30), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.611111111,21,0.107692308,11,0.611111111,1,model,Introduction,entity_linking14
1006,1006,1006,23,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .",Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 9), (11, 12), (13, 14), (14, 15), (15, 17), (18, 19), (19, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.666666667,22,0.112820513,12,0.666666667,1,model,Introduction,entity_linking14
1007,1007,1007,24,Our method is a variant of LDA in which the topic proportions for a document are replaced by synset proportions for a document .,Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 9), (10, 12), (12, 13), (14, 15), (16, 18), (18, 20), (20, 21), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.722222222,23,0.117948718,13,0.722222222,1,model,Introduction,entity_linking14
1008,1008,1008,25,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 9), (9, 10), (10, 11), (11, 13), (14, 17), (17, 18), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.777777778,24,0.123076923,14,0.777777778,1,model,Introduction,entity_linking14
1009,1009,1009,26,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .",Introduction,Introduction,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 7), (7, 8), (8, 9), (9, 11), (12, 14), (14, 16), (17, 19), (19, 20), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.833333333,25,0.128205128,15,0.833333333,1,model,Introduction,entity_linking14
1010,1010,1010,169,"The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .",Experiments & Results,Experiments & Results,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (6, 9), (19, 20), (21, 30), (30, 31), (32, 39), (39, 41), (42, 46), (46, 47), (47, 48), (49, 51), (51, 54), (54, 55), (55, 56)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.307692308,168,0.861538462,8,0.666666667,1,results,Experiments & Results,entity_linking14
1011,1011,1011,170,"We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .",Experiments & Results,Experiments & Results,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 6), (6, 7), (8, 10), (10, 11), (11, 14), (14, 15), (16, 19), (20, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.346153846,169,0.866666667,9,0.75,1,results,Experiments & Results,entity_linking14
1012,1012,1012,172,The proposed system outperforms all previous knowledgebased systems overall parts of speech .,Experiments & Results,Experiments & Results,entity_linking,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 8), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.423076923,171,0.876923077,11,0.916666667,1,results,Experiments & Results,entity_linking14
1013,1013,1013,2,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,title,title,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003861004,1,0,1,research-problem,title,entity_linking15
1014,1014,1014,11,"The first stage for every QA approach is entity linking ( EL ) , that is the identification of entity mentions in the question and linking them to entities in KB .",Introduction,Introduction,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.025316456,10,0.038610039,2,0.181818182,1,research-problem,Introduction,entity_linking15
1015,1015,1015,14,The state - of - the - art QA systems usually rely on off - the - shelf EL systems to extract entities from the question .,Introduction,Introduction,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.063291139,13,0.05019305,5,0.454545455,1,research-problem,Introduction,entity_linking15
1016,1016,1016,27,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .",Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(16, 17), (17, 23), (23, 24), (24, 25), (26, 29), (30, 31), (32, 34), (34, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.227848101,26,0.1003861,5,0.075757576,1,model,Introduction: PERFORMER,entity_linking15
1017,1017,1017,29,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .",Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (9, 11), (11, 12), (12, 13), (14, 17), (17, 18), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.253164557,28,0.108108108,7,0.106060606,1,model,Introduction: PERFORMER,entity_linking15
1018,1018,1018,30,Each level of granularity is handled by a separate component of the model .,Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 7), (8, 10), (10, 11), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.265822785,29,0.111969112,8,0.121212121,1,model,Introduction: PERFORMER,entity_linking15
1019,1019,1019,31,"A token - level component extracts higher - level features from the whole question context , whereas a character - level component builds lower - level features for the candidate n-gram .",Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 10), (10, 11), (12, 15), (18, 22), (22, 23), (23, 27), (27, 28), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.278481013,30,0.115830116,9,0.136363636,1,model,Introduction: PERFORMER,entity_linking15
1020,1020,1020,32,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .",Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (5, 6), (7, 10), (10, 11), (12, 14), (15, 19), (20, 22), (23, 25), (26, 30), (31, 32), (32, 34), (35, 36), (36, 37), (38, 40), (40, 41), (42, 44)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.291139241,31,0.11969112,10,0.151515152,1,model,Introduction: PERFORMER,entity_linking15
1021,1021,1021,33,This information is aggregated and used to predict whether the n-gram is an entity mention and to what entity it should be linked .,Introduction,PERFORMER,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (10, 11), (11, 12), (13, 15), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.303797468,32,0.123552124,11,0.166666667,1,model,Introduction: PERFORMER,entity_linking15
1022,1022,1022,42,The complete code as well as the scripts that produce the evaluation data can be found here : https://github.com/UKPLab/ starsem2018-entity-linking.,Introduction,Code and datasets,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.417721519,41,0.158301158,20,0.303030303,1,code,Introduction: Code and datasets,entity_linking15
1023,1023,1023,212,Existing systems,,,entity_linking,15,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,211,0.814671815,1,0.090909091,1,experiments,,entity_linking15
1024,1024,1024,213,"In our experiments , we compare to DBPedia Spotlight that was used in several QA systems and represents a strong baseline for entity linking 4 .",Existing systems,Existing systems,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 9), (11, 13), (17, 18), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.045454545,212,0.818532819,2,0.181818182,1,baselines,Existing systems,entity_linking15
1025,1025,1025,214,"In addition , we are able to compare to the state - of - the - art S - MART system , since their output on the WebQSP datasets was publicly released 5 .",Existing systems,Existing systems,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (10, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.090909091,213,0.822393822,3,0.272727273,1,results,Existing systems,entity_linking15
1026,1026,1026,216,We also include a heuristics baseline that ranks candidate entities according to their frequency in Wikipedia .,Existing systems,Existing systems,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (7, 8), (8, 10), (10, 12), (13, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.181818182,215,0.83011583,5,0.454545455,1,baselines,Existing systems,entity_linking15
1027,1027,1027,218,Simplified VCG,Existing systems,,entity_linking,15,"['O', 'O']","[(0, 2)]","['O', 'O']",6,0.272727273,217,0.837837838,7,0.636363636,1,baselines,Existing systems,entity_linking15
1028,1028,1028,220,"In particular , we employ features that cover ( 1 ) frequency of the entity in Wikipedia , ( 2 ) edit distance between the label of the entity and the token n-gram , ( 3 ) number of entities and relations immediately connected to the entity in the KB , ( 4 ) word overlap between the input question and the labels of the connected entities and relations , ( 5 ) length of the n-gram .",Existing systems,Simplified VCG,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (7, 8), (11, 12), (12, 13), (14, 15), (15, 16), (16, 17), (21, 23), (23, 24), (25, 26), (26, 27), (31, 33), (37, 42), (42, 45), (46, 47), (47, 48), (49, 50), (54, 56), (56, 57), (58, 60), (62, 63), (63, 64), (65, 69), (73, 74), (74, 75), (76, 77)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.363636364,219,0.845559846,9,0.818181818,1,hyperparameters,Existing systems: Simplified VCG,entity_linking15
1029,1029,1029,238,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,Results,Results,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 9), (11, 13), (14, 17), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.176470588,237,0.915057915,3,0.176470588,1,results,Results,entity_linking15
1030,1030,1030,239,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,Results,Results,entity_linking,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (8, 11), (12, 14), (14, 16), (17, 20), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.235294118,238,0.918918919,4,0.235294118,1,results,Results,entity_linking15
1031,1031,1031,2,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,title,title,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.006369427,1,0,1,research-problem,title,entity_linking16
1032,1032,1032,5,"To mine these data properly , attributable to their innate ambiguity , a Word Sense Disambiguation ( WSD ) algorithm can avoid numbers of difficulties in Natural Language Processing ( NLP ) pipeline .",abstract,abstract,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,4,0.025477707,2,0.333333333,1,research-problem,abstract,entity_linking16
1033,1033,1033,6,"However , considering a large number of ambiguous words in one language or technical domain , we may encounter limiting constraints for proper deployment of existing WSD models .",abstract,abstract,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,5,0.031847134,3,0.5,1,research-problem,abstract,entity_linking16
1034,1034,1034,16,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .",Introduction,Introduction,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (11, 12), (13, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.461538462,15,0.095541401,6,0.461538462,1,model,Introduction,entity_linking16
1035,1035,1035,17,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .",Introduction,Introduction,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 12), (15, 17), (17, 19), (21, 22), (22, 30), (33, 35), (36, 40), (40, 41), (49, 50), (51, 53)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.538461538,16,0.101910828,7,0.538461538,1,model,Introduction,entity_linking16
1036,1036,1036,110,Between - all - models comparisons,Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0.025,109,0.694267516,1,0.034482759,1,results,Results,entity_linking16
1037,1037,1037,114,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .",Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (5, 7), (6, 7), (8, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.125,113,0.719745223,5,0.172413793,1,results,Results,entity_linking16
1038,1038,1038,115,shows the results of the top - performing and low - performing supervised algorithms .,Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (5, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.15,114,0.72611465,6,0.206896552,1,results,Results,entity_linking16
1039,1039,1039,128,Within - our - model comparisons,Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",19,0.475,127,0.808917197,19,0.655172414,1,results,Results,entity_linking16
1040,1040,1040,136,"We observe if reverse the sequential follow of information into our Bidirectional LSTM , we shuffle the order of the context words , or even replace our Bidirectional LSTMs with two different fully - connected networks of the same size 50 ( the size of the LSTMs outputs ) , the achieved results were notably less than 72.5 % .",Results,Results,entity_linking,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (5, 7), (7, 8), (8, 9), (9, 10), (10, 13), (15, 16), (17, 18), (18, 19), (20, 22), (25, 26), (26, 29), (29, 30), (30, 36), (36, 37), (38, 41), (51, 53), (57, 59)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.675,135,0.859872611,27,0.931034483,1,results,Results,entity_linking16
1041,1041,1041,2,Neural Sequence Learning Models for Word Sense Disambiguation,title,,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004926108,1,0,1,research-problem,title,entity_linking2
1042,1042,1042,9,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .",Introduction,Introduction,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(16, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.066666667,8,0.039408867,1,0.066666667,1,research-problem,Introduction,entity_linking2
1043,1043,1043,18,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .",Introduction,Introduction,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (42, 44), (51, 54), (55, 58)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.666666667,17,0.083743842,10,0.666666667,1,model,Introduction,entity_linking2
1044,1044,1044,20,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .",Introduction,Introduction,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 15), (15, 16), (19, 20), (22, 29), (31, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8,19,0.093596059,12,0.8,1,model,Introduction,entity_linking2
1045,1045,1045,21,"Each architecture reflects a particular way of modeling the disambiguation problem , but they all share some key features that set them apart from previous supervised approaches to WSD : they are trained end - to - end from sense - annotated text to sense labels , and learn a single all - words model from the training data , without fine tuning or explicit engineering of local features .",Introduction,Introduction,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (9, 11), (28, 29), (32, 33), (38, 39), (39, 43), (43, 44), (44, 46), (48, 49), (50, 55), (55, 56), (57, 59), (60, 61)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.866666667,20,0.098522167,13,0.866666667,1,model,Introduction,entity_linking2
1046,1046,1046,144,"To set a level playing field with comparison systems on English all - words WSD , we followed and , for all our models , we used a layer of word embeddings pre-trained 8 on the English uk WaC corpus as initialization , and kept them fixed during the training process .",Architecture Details .,Architecture Details .,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (10, 15), (26, 27), (28, 32), (32, 33), (36, 40), (40, 41), (41, 42), (46, 47), (49, 51)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.333333333,143,0.704433498,10,0.555555556,1,hyperparameters,Architecture Details .,entity_linking2
1047,1047,1047,145,For all architectures we then employed 2 layers of bidirectional LSTM with 2048 hidden units ( 1024 units per direction ) .,Architecture Details .,Architecture Details .,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (6, 8), (8, 9), (9, 11), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.666666667,144,0.709359606,11,0.611111111,1,hyperparameters,Architecture Details .,entity_linking2
1048,1048,1048,155,"We report the F1 - score on each in - dividual test set , as well as the F1- score obtained on the concatenation of all four test sets , divided by part - of - speech tag .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 13), (18, 20), (20, 22), (23, 24), (24, 25), (25, 29), (30, 32), (32, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.048780488,154,0.75862069,2,0.074074074,1,results,Experimental Results,entity_linking2
1049,1049,1049,157,"As supervised systems , we considered Context2 Vec and It Makes Sense , both the original implementation and the best configuration reported by , which also integrates word embeddings using exponential decay .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (6, 8), (26, 27), (27, 29), (29, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.097560976,156,0.768472906,4,0.148148148,1,hyperparameters,Experimental Results,entity_linking2
1050,1050,1050,161,"11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 8), (8, 9), (12, 19), (31, 32), (33, 36), (40, 41), (41, 44), (44, 46), (46, 48), (48, 51)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.195121951,160,0.78817734,8,0.296296296,1,results,Experimental Results,entity_linking2
1051,1051,1051,162,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (7, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.219512195,161,0.793103448,9,0.333333333,1,results,Experimental Results,entity_linking2
1052,1052,1052,164,English All - words WSD,Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",11,0.268292683,163,0.802955665,11,0.407407407,1,results,Experimental Results,entity_linking2
1053,1053,1053,169,"It is worth noting that RNN - based architectures outperformed classical supervised approaches when dealing with verbs , which are shown to be highly ambiguous .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 9), (9, 10), (10, 13), (13, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.390243902,168,0.827586207,16,0.592592593,1,results,Experimental Results,entity_linking2
1054,1054,1054,171,"Both BLSTM and Seq2Seq outperformed UKB and IMS trained on SemCor , as well as recent supervised approaches based on distributional semantics and neural architectures .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 8), (8, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.43902439,170,0.837438424,18,0.666666667,1,results,Experimental Results,entity_linking2
1055,1055,1055,172,Multilingual All - words WSD,Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",19,0.463414634,171,0.842364532,19,0.703703704,1,results,Experimental Results,entity_linking2
1056,1056,1056,179,"F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 10), (19, 21), (22, 24), (25, 26), (29, 30), (32, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.634146341,178,0.876847291,26,0.962962963,1,results,Experimental Results,entity_linking2
1057,1057,1057,180,"We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .",Experimental Results,Experimental Results,entity_linking,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 9), (18, 19), (19, 21), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.658536585,179,0.881773399,27,1,1,results,Experimental Results,entity_linking2
1058,1058,1058,2,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings,title,title,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004545455,1,0,1,research-problem,title,entity_linking3
1059,1059,1059,7,"Since vectors of the same word type can vary depending on the respective context , they implicitly provide a model for word sense disambiguation ( WSD ) .",abstract,abstract,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(21, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.076923077,6,0.027272727,4,0.076923077,1,research-problem,abstract,entity_linking3
1060,1060,1060,8,We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs .,abstract,abstract,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (7, 8), (8, 9), (9, 10), (11, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.096153846,7,0.031818182,5,0.096153846,1,research-problem,abstract,entity_linking3
1061,1061,1061,49,We show that CWEs can be utilized directly to approach the WSD task due to their nature of providing distinct vector representations for the same token depending on its context .,abstract,abstract,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (6, 8), (8, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.884615385,48,0.218181818,46,0.884615385,1,research-problem,abstract,entity_linking3
1062,1062,1062,50,"To learn the semantic capabilities of CWEs , we employ a simple , yet interpretable approach to WSD using a k -nearest neighbor classification ( kNN ) approach .",abstract,abstract,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 7), (9, 10), (11, 16), (16, 17), (17, 18), (18, 19), (20, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.903846154,49,0.222727273,47,0.903846154,1,model,abstract,entity_linking3
1063,1063,1063,134,Contextualized Embeddings,Experimental Results,,entity_linking,3,"['O', 'O']","[(0, 2)]","['O', 'O']",5,0.090909091,133,0.604545455,0,0,1,results,Experimental Results,entity_linking3
1064,1064,1064,137,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,Experimental Results,Contextualized Embeddings,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 5), (5, 8), (8, 10), (10, 11), (12, 16), (16, 17), (18, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.145454545,136,0.618181818,3,0.1875,1,results,Experimental Results: Contextualized Embeddings,entity_linking3
1065,1065,1065,138,BERT also outperforms all others on the SE - 3 task .,Experimental Results,Contextualized Embeddings,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 5), (5, 6), (7, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.163636364,137,0.622727273,4,0.25,1,results,Experimental Results: Contextualized Embeddings,entity_linking3
1066,1066,1066,139,"However , we observe a major performance drop of our approach for the two all - words WSD tasks in which no training data is provided along with the test set .",Experimental Results,Contextualized Embeddings,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 9), (9, 11), (11, 12), (13, 19), (25, 26), (26, 28), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.181818182,138,0.627272727,5,0.3125,1,results,Experimental Results: Contextualized Embeddings,entity_linking3
1067,1067,1067,141,"In fact , similarity of contextualized embeddings largely relies on semantically and structurally similar sentence contexts of polysemic target words .",Experimental Results,Contextualized Embeddings,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (10, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.218181818,140,0.636363636,7,0.4375,1,results,Experimental Results: Contextualized Embeddings,entity_linking3
1068,1068,1068,146,"As can be seen in , the SE - 2 and SE - 3 training datasets provide more CWEs for each word and sense , and our approach performs better with a growing number of CWEs , even with a higher average number of senses per word as is the casein SE - 3 .",Experimental Results,UFSAC 4 BERT performed best in experiment one .,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 16), (16, 17), (17, 19), (19, 20), (20, 24), (26, 28), (28, 29), (29, 30), (30, 31), (32, 36), (40, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.309090909,145,0.659090909,12,0.75,1,results,Experimental Results: UFSAC 4 BERT performed best in experiment one .,entity_linking3
1069,1069,1069,149,"Moreover , CWEs actually do not organize well in spherically shaped form in the embedding space .",Experimental Results,UFSAC 4 BERT performed best in experiment one .,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (8, 9), (9, 12), (12, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.363636364,148,0.672727273,15,0.9375,1,results,Experimental Results: UFSAC 4 BERT performed best in experiment one .,entity_linking3
1070,1070,1070,151,Nearest Neighbors,Experimental Results,,entity_linking,3,"['O', 'O']","[(0, 2)]","['O', 'O']",22,0.4,150,0.681818182,0,0,1,results,Experimental Results,entity_linking3
1071,1071,1071,152,K- Optimization :,Experimental Results,Nearest Neighbors,entity_linking,3,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",23,0.418181818,151,0.686363636,1,0.024390244,1,results,Experimental Results: Nearest Neighbors,entity_linking3
1072,1072,1072,155,"For SensEval - 2 and SensEval - 3 , we achieve a new state - of - the - art result .",Experimental Results,Nearest Neighbors,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 8), (10, 11), (12, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.472727273,154,0.7,4,0.097560976,1,results,Experimental Results: Nearest Neighbors,entity_linking3
1073,1073,1073,157,"For the S7 - T * , we also achieve minor improvements with a higher k , but still drastically lack behind the state of the art .",Experimental Results,Nearest Neighbors,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (9, 10), (10, 12), (12, 13), (14, 16), (23, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.509090909,156,0.709090909,6,0.146341463,1,results,Experimental Results: Nearest Neighbors,entity_linking3
1074,1074,1074,158,Senses in CWE space :,Experimental Results,Nearest Neighbors,entity_linking,3,"['O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O']",29,0.527272727,157,0.713636364,7,0.170731707,1,results,Experimental Results: Nearest Neighbors,entity_linking3
1075,1075,1075,159,We investigate how well different CWE models encode information such as distinguishable senses in their vector space .,Experimental Results,Nearest Neighbors,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 7), (7, 8), (8, 9), (9, 11), (11, 13), (13, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.545454545,158,0.718181818,8,0.195121951,1,results,Experimental Results: Nearest Neighbors,entity_linking3
1076,1076,1076,162,The Flair embeddings hardly allow to distinguish any clusters as most senses are scattered across the entire plot .,Experimental Results,Nearest Neighbors,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 7), (7, 9), (10, 12), (13, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.6,161,0.731818182,11,0.268292683,1,results,Experimental Results: Nearest Neighbors,entity_linking3
1077,1077,1077,163,"In the ELMo embedding space , the major senses are slightly more separated in different regions of the point cloud .",Experimental Results,Nearest Neighbors,entity_linking,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 9), (9, 10), (10, 13), (13, 14), (14, 16), (16, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.618181818,162,0.736363636,12,0.292682927,1,results,Experimental Results: Nearest Neighbors,entity_linking3
1078,1078,1078,2,Learning Distributed Representations of Texts and Entities from Knowledge Base,title,title,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004366812,1,0,1,research-problem,title,entity_linking4
1079,1079,1079,26,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 13), (15, 18), (18, 21), (21, 23), (23, 24), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2,25,0.109170306,7,0.2,1,model,Introduction,entity_linking4
1080,1080,1080,27,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4), (5, 6), (9, 12), (13, 15), (17, 18), (19, 24), (24, 26), (26, 28), (28, 29), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.228571429,26,0.113537118,8,0.228571429,1,model,Introduction,entity_linking4
1081,1081,1081,28,We use humanedited entity annotations obtained from Wikipedia ( see ) as supervised data of relevant entities to the texts containing these annotations .,Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 7), (7, 8), (11, 12), (12, 14), (14, 15), (15, 17), (17, 18), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.257142857,27,0.11790393,9,0.257142857,1,model,Introduction,entity_linking4
1082,1082,1082,29,"Note that , KB entities have been conventionally used to model semantics of texts .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (11, 12), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.285714286,28,0.122270742,10,0.285714286,1,model,Introduction,entity_linking4
1083,1083,1083,30,"A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 10), (12, 13), (14, 15), (18, 19), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.314285714,29,0.126637555,11,0.314285714,1,research-problem,Introduction,entity_linking4
1084,1084,1084,31,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 6), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.342857143,30,0.131004367,12,0.342857143,1,research-problem,Introduction,entity_linking4
1085,1085,1085,35,"Furthermore , we also consider that placing texts and entities into the same vector space enables us to easily compute the similarity between texts and entities , which can be beneficial for various KB - related tasks .",Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 10), (10, 11), (12, 15), (15, 16), (18, 20), (21, 22), (22, 23), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.457142857,34,0.148471616,16,0.457142857,1,model,Introduction,entity_linking4
1086,1086,1086,54,We release our code and trained models to the community at https://github.com/ studio-ousia /ntee to facilitate further academic research .,Introduction,Introduction,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,1,53,0.231441048,35,1,1,code,Introduction,entity_linking4
1087,1087,1087,125,BOW is a conventional approach using a logistic regression ( LR ) classifier trained with binary BOW features to predict the correct answer .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 5), (5, 6), (7, 13), (13, 15), (15, 18), (18, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.272727273,124,0.541484716,3,0.272727273,1,baselines,Baselines,entity_linking4
1088,1088,1088,126,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 6), (7, 9), (9, 11), (12, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.363636364,125,0.545851528,4,0.363636364,1,baselines,Baselines,entity_linking4
1089,1089,1089,127,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (4, 6), (7, 10), (10, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.454545455,126,0.550218341,5,0.454545455,1,baselines,Baselines,entity_linking4
1090,1090,1090,129,FTS - BRNN is based on the bidirectional recurrent neural network ( RNN ) with gated recurrent units ( GRU ) .,Baselines,Baselines,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 6), (7, 14), (14, 15), (15, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,128,0.558951965,7,0.636363636,1,baselines,Baselines,entity_linking4
1091,1091,1091,137,"In particular , despite the simplicity of the neural network architecture of our method compared to the state - of - the - art methods ( i.e. , QANTA and FTS - BRNN ) , our method clearly outperformed these methods .",Results,Results,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12), (14, 16), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.230769231,136,0.593886463,3,0.428571429,1,results,Results,entity_linking4
1092,1092,1092,140,"Our observations indicated that our method mostly performed perfect in terms of predicting the types of target answers ( e.g. , locations , events , and people ) .",Results,Results,entity_linking,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 8), (8, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.461538462,139,0.6069869,6,0.857142857,1,results,Results,entity_linking4
1093,1093,1093,2,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,title,title,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005102041,1,0,1,research-problem,title,entity_linking5
1094,1094,1094,4,"In Word Sense Disambiguation ( WSD ) , the predominant approach generally involves a supervised system trained on sense annotated corpora .",abstract,abstract,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.015306122,1,0.2,1,research-problem,abstract,entity_linking5
1095,1095,1095,7,"Our method leads to state of the art results on most WSD evaluation tasks , while improving the coverage of supervised systems , reducing the training time and the size of the models , without additional training data .",abstract,abstract,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 9), (9, 10), (10, 14), (16, 17), (18, 19), (19, 20), (20, 22), (23, 24), (34, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,6,0.030612245,4,0.8,1,research-problem,abstract,entity_linking5
1096,1096,1096,27,We propose a method for reducing the vocabulary of senses of Word Net by selecting the minimal set of senses required for differentiating the meaning of every word .,Introduction,Introduction,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 6), (7, 10), (10, 11), (11, 13), (13, 15), (16, 20), (20, 23), (24, 25), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.75,26,0.132653061,18,0.818181818,1,approach,Introduction,entity_linking5
1097,1097,1097,31,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,Introduction,Introduction,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.916666667,30,0.153061224,22,1,1,code,Introduction,entity_linking5
1098,1098,1098,181,"In subsection 3.2 , we showed that our vocabulary reduction method improves the coverage of supervised systems overall WordNet vocabulary .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 11), (11, 12), (13, 14), (14, 15), (15, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.210526316,180,0.918367347,4,0.210526316,1,results,Results,entity_linking5
1099,1099,1099,182,"In , we can see that this coverage improvement holds true on the evaluation tasks , for both training sets .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 9), (9, 11), (11, 12), (13, 15), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.263157895,181,0.923469388,5,0.263157895,1,results,Results,entity_linking5
1100,1100,1100,185,"Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 13), (13, 15), (15, 17), (17, 18), (19, 24), (19, 22), (25, 26), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.421052632,184,0.93877551,8,0.421052632,1,results,Results,entity_linking5
1101,1101,1101,186,"However we can notice a very large gap on the SemEval 2013 task , especially when the SemCor is used alone for training .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 9), (10, 13), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.473684211,185,0.943877551,9,0.473684211,1,results,Results,entity_linking5
1102,1102,1102,190,"When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (9, 11), (14, 15), (15, 21), (21, 22), (22, 24), (24, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.684210526,189,0.964285714,13,0.684210526,1,results,Results,entity_linking5
1103,1103,1103,191,"Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 13), (14, 15), (15, 16), (16, 18), (30, 31), (32, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.736842105,190,0.969387755,14,0.736842105,1,results,Results,entity_linking5
1104,1104,1104,193,"As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (8, 11), (11, 12), (12, 13), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.842105263,192,0.979591837,16,0.842105263,1,results,Results,entity_linking5
1105,1105,1105,194,"Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .",Results,Results,entity_linking,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (6, 7), (7, 8), (8, 10), (10, 12), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.894736842,193,0.984693878,17,0.894736842,1,results,Results,entity_linking5
1106,1106,1106,2,Incorporating Glosses into Neural Word Sense Disambiguation,title,title,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003717472,0,0,1,research-problem,title,entity_linking6
1107,1107,1107,4,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,abstract,abstract,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.011152416,1,0.125,1,research-problem,abstract,entity_linking6
1108,1108,1108,5,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,abstract,abstract,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 4), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,4,0.014869888,2,0.25,1,research-problem,abstract,entity_linking6
1109,1109,1109,31,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (12, 18), (21, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.678571429,30,0.111524164,19,0.678571429,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1110,1110,1110,32,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 7), (7, 8), (9, 11), (12, 13), (14, 16), (16, 17), (18, 21), (21, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.714285714,31,0.115241636,20,0.714285714,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1111,1111,1111,33,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",Introduction,There are several lines of research on WSD .,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (8, 11), (15, 16), (16, 19), (19, 20), (21, 22), (22, 23), (24, 26), (27, 28), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.75,32,0.118959108,21,0.75,1,model,Introduction: There are several lines of research on WSD .,entity_linking6
1112,1112,1112,196,"We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 8), (13, 14), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.222222222,195,0.724907063,2,0.222222222,1,hyperparameters,Implementation Details,entity_linking6
1113,1113,1113,197,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.333333333,196,0.728624535,3,0.333333333,1,hyperparameters,Implementation Details,entity_linking6
1114,1114,1114,198,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 6), (6, 7), (7, 8), (9, 12), (12, 13), (13, 20), (21, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.444444444,197,0.732342007,4,0.444444444,1,hyperparameters,Implementation Details,entity_linking6
1115,1115,1115,199,We assign gloss expansion depth K the value of 4 .,Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (7, 8), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.555555556,198,0.73605948,5,0.555555556,1,hyperparameters,Implementation Details,entity_linking6
1116,1116,1116,200,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 12), (12, 13), (13, 16), (16, 17), (17, 19), (20, 21), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.666666667,199,0.739776952,6,0.666666667,1,hyperparameters,Implementation Details,entity_linking6
1117,1117,1117,201,We use Adam optimizer in the training process with 0.001 initial learning rate .,Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (8, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.777777778,200,0.743494424,7,0.777777778,1,hyperparameters,Implementation Details,entity_linking6
1118,1118,1118,202,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (7, 8), (8, 10), (11, 12), (12, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.888888889,201,0.747211896,8,0.888888889,1,hyperparameters,Implementation Details,entity_linking6
1119,1119,1119,203,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,Implementation Details,Implementation Details,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 7), (7, 8), (8, 10), (10, 11), (12, 14), (14, 17), (17, 18), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,1,202,0.750929368,9,1,1,hyperparameters,Implementation Details,entity_linking6
1120,1120,1120,228,English all - words results,Results and Discussion,,entity_linking,6,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",1,0.029411765,227,0.843866171,1,0.055555556,1,results,Results and Discussion,entity_linking6
1121,1121,1121,233,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (6, 12), (12, 13), (14, 16), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.176470588,232,0.862453532,6,0.333333333,1,results,Results and Discussion: English all - words results,entity_linking6
1122,1122,1122,237,". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 9), (13, 15), (15, 17), (17, 18), (18, 22), (22, 23), (24, 26), (26, 27), (27, 28), (29, 31), (32, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.294117647,236,0.87732342,10,0.555555556,1,results,Results and Discussion: English all - words results,entity_linking6
1123,1123,1123,238,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 8), (15, 17), (17, 20), (20, 21), (22, 27), (27, 28), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.323529412,237,0.881040892,11,0.611111111,1,results,Results and Discussion: English all - words results,entity_linking6
1124,1124,1124,240,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 11), (11, 12), (13, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.382352941,239,0.888475836,13,0.722222222,1,results,Results and Discussion: English all - words results,entity_linking6
1125,1125,1125,241,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 3), (3, 5), (6, 8), (9, 10), (11, 12), (13, 15), (16, 18), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.411764706,240,0.892193309,14,0.777777778,1,results,Results and Discussion: English all - words results,entity_linking6
1126,1126,1126,242,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 7), (10, 12), (13, 16), (16, 18), (19, 21), (21, 22), (22, 27), (27, 31), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.441176471,241,0.895910781,15,0.833333333,1,results,Results and Discussion: English all - words results,entity_linking6
1127,1127,1127,243,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .",Results and Discussion,English all - words results,entity_linking,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 6), (9, 11), (11, 12), (13, 15), (16, 17), (19, 21), (22, 23), (23, 27), (28, 30), (30, 31), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.470588235,242,0.899628253,16,0.888888889,1,results,Results and Discussion: English all - words results,entity_linking6
1128,1128,1128,2,Semi-supervised Word Sense Disambiguation with Neural Models,title,title,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004464286,1,0,1,research-problem,title,entity_linking7
1129,1129,1129,4,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,abstract,abstract,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.013392857,1,0.125,1,research-problem,abstract,entity_linking7
1130,1130,1130,5,"Recently , researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms .",abstract,abstract,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10), (10, 12), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,4,0.017857143,2,0.25,1,research-problem,abstract,entity_linking7
1131,1131,1131,20,"In this paper , we describe two novel WSD algorithms .",Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.533333333,19,0.084821429,8,0.533333333,1,model,Introduction,entity_linking7
1132,1132,1132,21,The first is based on a Long Short Term Memory ( LSTM ) ) .,Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6,20,0.089285714,9,0.6,1,model,Introduction,entity_linking7
1133,1133,1133,22,"Since this model is able to take into account word order when classifying , it performs significantly better than an algorithm based on a continuous bag of words model ( Word2 vec ) , especially on verbs .",Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12), (15, 16), (16, 18), (18, 19), (20, 21), (21, 23), (24, 33), (34, 36), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.666666667,21,0.09375,10,0.666666667,1,experiments,Introduction,entity_linking7
1134,1134,1134,23,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,Introduction,Introduction,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (7, 8), (8, 10), (10, 12), (12, 14), (14, 16), (17, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.733333333,22,0.098214286,11,0.733333333,1,model,Introduction,entity_linking7
1135,1135,1135,130,Sem Eval Tasks,Experiments,Experiments,entity_linking,7,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",4,0.06779661,129,0.575892857,4,0.137931034,1,experiments,Experiments,entity_linking7
1136,1136,1136,136,Our proposed algorithms achieve the highest all - words F 1 scores except for Sem - Eval 2013 .,Experiments,Experiments,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (5, 12), (12, 14), (14, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.169491525,135,0.602678571,10,0.344827586,1,results,Experiments,entity_linking7
1137,1137,1137,138,"Unified WSD has the highest F 1 score on Nouns ( Sem - Eval - 7 Coarse ) , but our algorithms outperform Unified WSD on other part - of - speech tags .",Experiments,Experiments,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (4, 8), (8, 9), (9, 18), (20, 22), (22, 23), (23, 25), (25, 26), (26, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.203389831,137,0.611607143,12,0.413793103,1,results,Experiments,entity_linking7
1138,1138,1138,148,Word2 Vec vectors Vs. LSTM,Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O']",22,0.372881356,147,0.65625,22,0.75862069,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1139,1139,1139,150,"It performs similar to IMS + Word2 Vec ( T: SemCor ) , a SVM - based classifier studied in .",Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 12), (14, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.406779661,149,0.665178571,24,0.827586207,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1140,1140,1140,151,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 5), (5, 6), (7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.423728814,150,0.669642857,25,0.862068966,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1141,1141,1141,152,Sem Cor Vs. OMSTI,Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",26,0.440677966,151,0.674107143,26,0.896551724,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1142,1142,1142,153,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .",Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10), (10, 12), (12, 13), (13, 14), (14, 15), (15, 16), (17, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.457627119,152,0.678571429,27,0.931034483,1,results,Experiments: The learning rate is 0.1 .,entity_linking7
1143,1143,1143,155,"While the SVM classifier studied in maybe able to learn a model which copes with this noise , our naive nearest neighbor classifiers do not have a learned model and deal less well with noisy labels .",Experiments,The learning rate is 0.1 .,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(18, 23), (23, 26), (27, 29), (30, 31), (33, 34), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.491525424,154,0.6875,29,1,1,baselines,Experiments: The learning rate is 0.1 .,entity_linking7
1144,1144,1144,160,NOAD Eval,Experiments,We use the implementation of DIST EXPANDER .,entity_linking,7,"['O', 'O']","[(0, 2)]","['O', 'O']",34,0.576271186,159,0.709821429,0,0,1,experiments,Experiments: We use the implementation of DIST EXPANDER .,entity_linking7
1145,1145,1145,178,LSTM classifier,Experiments,,entity_linking,7,"['O', 'O']","[(0, 2)]","['O', 'O']",52,0.881355932,177,0.790178571,0,0,1,experiments,Experiments,entity_linking7
1146,1146,1146,180,Most frequent sense :,Experiments,LSTM classifier,entity_linking,7,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",54,0.915254237,179,0.799107143,2,0.111111111,1,baselines,Experiments: LSTM classifier,entity_linking7
1147,1147,1147,184,"LSTM outperforms Word2Vec by more than 10 % overall words , where most of the gains are from verbs and adverbs .",Experiments,LSTM classifier,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 3), (3, 4), (4, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.983050847,183,0.816964286,6,0.333333333,1,results,Experiments: LSTM classifier,entity_linking7
1148,1148,1148,186,Change of training data,,,entity_linking,7,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",0,0,185,0.825892857,8,0.444444444,1,experiments,,entity_linking7
1149,1149,1149,191,The SemCor ( or MASC ) trained classifier is on a par with the NOAD trained classifier on F1 score .,Change of training data,Change of training data,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 8), (9, 13), (14, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.172413793,190,0.848214286,13,0.722222222,1,results,Change of training data,entity_linking7
1150,1150,1150,193,Change of language model capacity,Change of training data,,entity_linking,7,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",7,0.24137931,192,0.857142857,15,0.833333333,1,ablation-analysis,Change of training data,entity_linking7
1151,1151,1151,196,"To balance the accuracy and resource usage , we use the second best LSTM model ( h = 2048 and p = 512 ) by default .",Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 7), (9, 10), (11, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.344827586,195,0.870535714,18,1,1,hyperparameters,Change of training data: Change of language model capacity,entity_linking7
1152,1152,1152,197,Semi-supervised WSD,Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O']","[(0, 2)]","['O', 'O']",11,0.379310345,196,0.875,0,0,1,ablation-analysis,Change of training data: Change of language model capacity,entity_linking7
1153,1153,1153,201,"As can be observed from , LP did not yield clear benefits when using the Word2 Vec language model .",Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 10), (10, 12), (12, 14), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.517241379,200,0.892857143,4,0.222222222,1,results,Change of training data: Change of language model capacity,entity_linking7
1154,1154,1154,202,"We did see significant improvements , 6.3 % increase on SemCor and 7.3 % increase on MASC , using LP with the LSTM language model .",Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (6, 9), (9, 10), (10, 11), (12, 15), (15, 16), (16, 17), (18, 19), (19, 20), (20, 21), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.551724138,201,0.897321429,5,0.277777778,1,results,Change of training data: Change of language model capacity,entity_linking7
1155,1155,1155,204,Change of seed data :,Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O']",18,0.620689655,203,0.90625,7,0.388888889,1,ablation-analysis,Change of training data: Change of language model capacity,entity_linking7
1156,1156,1156,205,"As can be seen in , LP substantially improves classifier F1 when the training datasets are SemCor + NOAD or MASC + NOAD .",Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 9), (9, 11), (11, 12), (13, 15), (15, 16), (16, 19), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.655172414,204,0.910714286,8,0.444444444,1,ablation-analysis,Change of training data: Change of language model capacity,entity_linking7
1157,1157,1157,208,Change of graph density :,Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O']",22,0.75862069,207,0.924107143,11,0.611111111,1,ablation-analysis,Change of training data: Change of language model capacity,entity_linking7
1158,1158,1158,209,"By default , we construct the LP graph by connecting two nodes if their affinity is above 95 % percentile .",Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 8), (8, 10), (10, 12), (12, 13), (14, 15), (15, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.793103448,208,0.928571429,12,0.666666667,1,hyperparameters,Change of training data: Change of language model capacity,entity_linking7
1159,1159,1159,212,"The F1 scores are relatively stable when the percentile ranges between 85 to 98 , but decrease when the percentile drops to 80 .",Change of training data,Change of language model capacity,entity_linking,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (6, 7), (8, 9), (9, 10), (10, 11), (11, 14), (16, 17), (17, 18), (19, 20), (20, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.896551724,211,0.941964286,15,0.833333333,1,ablation-analysis,Change of training data: Change of language model capacity,entity_linking7
1160,1160,1160,5,"Language modeling tasks , in which words , or word - pieces , are predicted on the basis of a local context , have been very effective for learning word embeddings and context dependent representations of phrases .",abstract,abstract,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,4,0.01980198,1,0.25,1,research-problem,abstract,entity_linking8
1161,1161,1161,21,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .",INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 11), (15, 18), (21, 24), (24, 28), (28, 29), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.125,20,0.099009901,12,0.48,1,approach,INTRODUCTION,entity_linking8
1162,1162,1162,22,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .",INTRODUCTION,INTRODUCTION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6), (22, 24), (38, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.135416667,21,0.103960396,13,0.52,1,approach,INTRODUCTION,entity_linking8
1163,1163,1163,98,"To train RELIC , we obtain data from the 2018 - 10 - 22 dump of English Wikipedia .",INTRODUCTION,Entity linking,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (5, 6), (6, 7), (7, 8), (9, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.927083333,97,0.48019802,1,0.125,1,experiments,INTRODUCTION: Entity linking,entity_linking8
1164,1164,1164,101,We limit each context sentence to 128 tokens .,INTRODUCTION,Entity linking,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.958333333,100,0.495049505,4,0.5,1,hyperparameters,INTRODUCTION: Entity linking,entity_linking8
1165,1165,1165,104,We set the entity embedding size to d = 300 .,INTRODUCTION,Entity linking,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,0.989583333,103,0.50990099,7,0.875,1,hyperparameters,INTRODUCTION: Entity linking,entity_linking8
1166,1166,1166,105,We train the model using Tensor Flow,INTRODUCTION,Entity linking,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",96,1,104,0.514851485,8,1,1,approach,INTRODUCTION: Entity linking,entity_linking8
1167,1167,1167,113,ENTITY LINKING,EVALUATION,EVALUATION,entity_linking,8,"['O', 'O']","[(0, 2)]","['O', 'O']",7,0.318181818,112,0.554455446,0,0,1,experiments,EVALUATION,entity_linking8
1168,1168,1168,124,"However , when we do adopt the standard CoNLL - Aida training set and alias table , RELIC matches the state of the art on this benchmark , despite using far fewer hand engineered resources use the large Wikidata knowledge base to create entity representations ) .",EVALUATION,EVALUATION,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13), (17, 18), (18, 19), (20, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.818181818,123,0.608910891,11,0.733333333,1,results,EVALUATION,entity_linking8
1169,1169,1169,134,"Finally , we believe that RELIC 's entity linking performance could be boosted even higher through the adoption of commonly used entity linking features .",System,System,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 10), (10, 12), (12, 15), (15, 16), (17, 19), (19, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.084745763,133,0.658415842,5,0.714285714,1,results,System,entity_linking8
1170,1170,1170,141,"RELIC outperforms prior work , even with only 5 % of the training data .",System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 4), (10, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.203389831,140,0.693069307,4,0.307692308,1,results,System: F1,entity_linking8
1171,1171,1171,142,ENTITY - LEVEL FINE TYPING,System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",13,0.220338983,141,0.698019802,5,0.384615385,1,results,System: F1,entity_linking8
1172,1172,1172,146,show that RELIC significantly outperforms prior results on both datasets .,System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 5), (5, 7), (7, 8), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.288135593,145,0.717821782,9,0.692307692,1,results,System: F1,entity_linking8
1173,1173,1173,148,"For TypeNet , aggregate mention - level types and train with a structured loss based on the TypeNet hierarchy , but is still outperformed by our flat classifier of binary labels .",System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 8), (3, 4), (4, 8), (9, 11), (12, 14), (14, 16), (17, 19), (23, 25), (25, 28), (28, 29), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.322033898,147,0.727722772,11,0.846153846,1,results,System: F1,entity_linking8
1174,1174,1174,151,EFFECT OF MASKING,System,F1,entity_linking,8,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",22,0.372881356,150,0.742574257,0,0,1,results,System: F1,entity_linking8
1175,1175,1175,154,"It is clear that masking mentions during training is beneficial for entity typing tasks , but detrimental for entity linking .",System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 8), (9, 11), (11, 14), (16, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.423728814,153,0.757425743,3,0.103448276,1,results,System: F1,entity_linking8
1176,1176,1176,159,"A higher mask rate leads to better performance , both in low and high - data situations .",System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 6), (6, 8), (11, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.508474576,158,0.782178218,8,0.275862069,1,results,System: F1,entity_linking8
1177,1177,1177,162,"However , we would like to point out that that a mask rate of 10 % , RELIC nears optimum performance on most tasks .",System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 13), (13, 14), (14, 16), (17, 18), (18, 19), (19, 21), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.559322034,161,0.797029703,11,0.379310345,1,results,System: F1,entity_linking8
1178,1178,1178,164,FEW - SHOT CATEGORY COMPLETION,System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",35,0.593220339,163,0.806930693,13,0.448275862,1,results,System: F1,entity_linking8
1179,1179,1179,166,"Furthermore , due to the incompleteness of the the FIGMENT and TypeNet type systems , we also believe that RELIC 's performance is approaching the upper bound on both of these supervised tasks .",System,F1,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 19), (19, 22), (23, 24), (25, 27), (27, 28), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.627118644,165,0.816831683,15,0.517241379,1,results,System: F1,entity_linking8
1180,1180,1180,181,TRIVIA QUESTION ANSWERING,System,F1,entity_linking,8,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",52,0.881355932,180,0.891089109,0,0,1,results,System: F1,entity_linking8
1181,1181,1181,196,We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,Results,Trivia,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 10), (10, 12), (12, 13), (13, 14), (15, 19), (19, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.538461538,195,0.965346535,7,0.538461538,1,results,Results: Trivia,entity_linking8
1182,1182,1182,197,"However , ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that RELIC 's much faster nearest neighbor lookup captures 80 % of ORQA 's performance .",Results,Trivia,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 10), (10, 11), (11, 14), (14, 15), (31, 32), (32, 34), (34, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.615384615,196,0.97029703,8,0.615384615,1,results,Results: Trivia,entity_linking8
1183,1183,1183,198,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .",Results,Trivia,entity_linking,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 6), (6, 7), (8, 11), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.692307692,197,0.975247525,9,0.692307692,1,results,Results: Trivia,entity_linking8
1184,1184,1184,2,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,title,title,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003690037,1,0,1,research-problem,title,entity_linking9
1185,1185,1185,4,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",abstract,abstract,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.011070111,1,0.166666667,1,research-problem,abstract,entity_linking9
1186,1186,1186,5,"In this paper , we propose a novel embedding method specifically designed for NED .",abstract,abstract,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,4,0.014760148,2,0.333333333,1,research-problem,abstract,entity_linking9
1187,1187,1187,21,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 10), (11, 13), (14, 16), (16, 19), (19, 20), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.25,20,0.073800738,11,0.44,1,model,Introduction,entity_linking9
1188,1188,1188,22,"In this model , similar words and entities are placed close to one another in a vector space .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 8), (9, 10), (10, 12), (12, 14), (14, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.272727273,21,0.077490775,12,0.48,1,model,Introduction,entity_linking9
1189,1189,1189,23,"Hence , we can measure the similarity between any pair of items ( i.e. , words , entities , and a word and an entity ) by simply computing their cosine similarity .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 7), (7, 8), (8, 12), (26, 27), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.295454545,22,0.081180812,13,0.52,1,model,Introduction,entity_linking9
1190,1190,1190,25,"Our model is based on the skip - gram model , a recently proposed embedding model that learns to predict each context word given the target word .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 10), (20, 23), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.340909091,24,0.088560886,15,0.6,1,model,Introduction,entity_linking9
1191,1191,1191,26,Our model consists of the following three models based on the skip - gram model :,Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (8, 10), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.363636364,25,0.092250923,16,0.64,1,model,Introduction,entity_linking9
1192,1192,1192,27,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 8), (9, 12), (12, 14), (14, 15), (16, 18), (18, 19), (19, 21), (25, 28), (29, 32), (32, 34), (34, 35), (36, 38), (38, 39), (40, 42), (42, 43), (44, 45), (50, 53), (54, 57), (57, 59), (59, 60), (61, 63), (63, 64), (64, 69), (69, 70), (71, 72)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.386363636,26,0.095940959,17,0.68,1,model,Introduction,entity_linking9
1193,1193,1193,29,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (10, 13), (14, 15), (15, 17), (17, 18), (19, 21), (22, 25), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.431818182,28,0.103321033,19,0.76,1,model,Introduction,entity_linking9
1194,1194,1194,32,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .",Introduction,Introduction,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 6), (6, 7), (7, 10), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5,31,0.114391144,22,0.88,1,model,Introduction,entity_linking9
1195,1195,1195,215,Our method successfully achieved enhanced performance on both the CoNLL and the TAC 2010 datasets .,Entity Relatedness,CoNLL,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (4, 6), (6, 7), (9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.94,214,0.789667897,18,0.857142857,0,results,Entity Relatedness: CoNLL,entity_linking9
1196,1196,1196,216,"Moreover , we found that the choice of candidate generation method considerably affected performance on the CoNLL dataset .",Entity Relatedness,CoNLL,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 11), (11, 13), (13, 14), (14, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.96,215,0.793357934,19,0.904761905,0,results,Entity Relatedness: CoNLL,entity_linking9
1197,1197,1197,218,Our method outperformed all the state - of - the - art methods on both datasets .,Entity Relatedness,CoNLL,entity_linking,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,1,217,0.800738007,21,1,0,results,Entity Relatedness: CoNLL,entity_linking9
1198,1198,1198,4,"3D Morphable Models ( 3 DMMs ) are powerful statistical models of 3D facial shape and texture , and among the stateof - the - art methods for reconstructing facial shape from single images .",abstract,abstract,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.012658228,1,0.1,1,research-problem,abstract,face_alignment0
1199,1199,1199,18,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.105263158,17,0.071729958,4,0.105263158,1,research-problem,Introduction,face_alignment0
1200,1200,1200,41,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 6), (7, 10), (10, 11), (11, 19), (27, 30), (33, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.710526316,40,0.168776371,27,0.710526316,1,model,Introduction,face_alignment0
1201,1201,1201,42,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(27, 30), (30, 31), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.736842105,41,0.172995781,28,0.736842105,1,model,Introduction,face_alignment0
1202,1202,1202,43,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .",Introduction,Introduction,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 7), (8, 19), (19, 20), (22, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.763157895,42,0.17721519,29,0.763157895,1,model,Introduction,face_alignment0
1203,1203,1203,210,3D Shape Recovery,Experiments,Experiments,face_alignment,0,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",5,0.192307692,209,0.88185654,0,0,1,experiments,Experiments,face_alignment0
1204,1204,1204,222,"The Classic model struggles to fit to the "" in - the -wild "" conditions present in the test set , and performs the worst .",Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (6, 7), (8, 15), (15, 17), (18, 20), (22, 23), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.653846154,221,0.932489451,12,0.8,1,results,Experiments,face_alignment0
1205,1205,1205,223,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .",Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (6, 7), (7, 8), (11, 13), (14, 18), (19, 21), (21, 23), (24, 27), (27, 28), (29, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.692307692,222,0.936708861,13,0.866666667,1,results,Experiments,face_alignment0
1206,1206,1206,225,"We note that in a wide variety of expression , identity , lighting and occlusion conditions our model is able to robustly reconstruct a realistic 3 D facial shape that stands up to scrutiny .",Experiments,Experiments,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (5, 16), (19, 21), (21, 23), (24, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.769230769,224,0.945147679,15,1,1,results,Experiments,face_alignment0
1207,1207,1207,226,Quantitative Normal Recovery,Experiments,,face_alignment,0,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",21,0.807692308,225,0.949367089,0,0,1,results,Experiments,face_alignment0
1208,1208,1208,231,ITW slightly outperforms IMM even though both IMM and PS - NL use all four available images of each subject .,Experiments,Quantitative Normal Recovery,face_alignment,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,1,230,0.970464135,5,1,1,experiments,Experiments: Quantitative Normal Recovery,face_alignment0
1209,1209,1209,2,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,title,title,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00456621,1,0,1,research-problem,title,face_alignment1
1210,1210,1210,5,Abstract 3 D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty .,abstract,abstract,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.222222222,4,0.01826484,2,0.222222222,1,research-problem,abstract,face_alignment1
1211,1211,1211,33,"We describe a very simple approach which bypasses many of the difficulties encountered in 3D face reconstruction by using a novel volumetric representation of the 3D facial geometry , and an appropriate CNN architecture that is trained to regress directly from a 2 D facial image to the corresponding 3D volume .",Introduction,Main contributions,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (14, 17), (17, 19), (20, 23), (23, 24), (31, 34), (36, 38), (38, 39), (39, 41), (42, 46), (46, 47), (48, 51)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.666666667,32,0.146118721,1,0.090909091,1,model,Introduction: Main contributions,face_alignment1
1212,1212,1212,147,"Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .",Training,Training,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 11), (11, 12), (12, 13), (13, 14), (15, 18), (18, 19), (19, 22), (25, 26), (26, 27), (27, 29), (29, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,146,0.666666667,1,0.2,1,hyperparameters,Training,face_alignment1
1213,1213,1213,148,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .",Training,Training,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 5), (6, 8), (8, 15), (17, 23), (25, 26), (56, 58)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,147,0.671232877,2,0.4,1,hyperparameters,Training,face_alignment1
1214,1214,1214,149,"In 20 % of cases , the input and target were flipped horizontally .",Training,Training,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (7, 10), (11, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,148,0.675799087,3,0.6,1,hyperparameters,Training,face_alignment1
1215,1215,1215,150,"Finally , the input samples were adjusted with some colour scaling on each RGB channel .",Training,Training,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (8, 11), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,149,0.680365297,4,0.8,1,hyperparameters,Training,face_alignment1
1216,1216,1216,151,"In the case of the VRN - Guided , the landmark detection module was trained to regress Gaussians with standard deviation of approximately 3 pixels (? = 1 ) .",Training,Training,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (10, 13), (14, 17), (17, 18), (18, 19), (19, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,1,150,0.684931507,5,1,1,hyperparameters,Training,face_alignment1
1217,1217,1217,159,Volumetric Regression Networks largely outperform,Results,,face_alignment,1,"['O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 5)]","['O', 'O', 'O', 'O', 'O']",7,0.159090909,158,0.721461187,0,0,1,results,Results,face_alignment1
1218,1218,1218,160,"3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.181818182,159,0.726027397,1,0.034482759,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1219,1219,1219,161,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 6), (6, 7), (8, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.204545455,160,0.730593607,2,0.068965517,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1220,1220,1220,163,"3 . The best performing VRN is the one guided by detected landmarks , however at the cost of higher computational complexity :",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (9, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.25,162,0.739726027,4,0.137931034,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1221,1221,1221,165,"4 . VRN - Multitask does not always perform particularly better than the plain VRN ( in fact on BU - 4 DFE it performs worse ) , not justifying the increase of network complexity .",Results,Volumetric Regression Networks largely outperform,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (9, 11), (11, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.295454545,164,0.748858447,6,0.206896552,1,results,Results: Volumetric Regression Networks largely outperform,face_alignment1
1222,1222,1222,200,Effect of pose .,Ablation studies,,face_alignment,1,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",3,0.176470588,199,0.908675799,3,0.176470588,1,ablation-analysis,Ablation studies,face_alignment1
1223,1223,1223,202,"As shown in , the performance of our method decreases as the pose increases .",Ablation studies,Effect of pose .,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 9), (9, 10), (10, 11), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.294117647,201,0.917808219,5,0.294117647,1,ablation-analysis,Ablation studies: Effect of pose .,face_alignment1
1224,1224,1224,205,Effect of expression .,Ablation studies,,face_alignment,1,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",8,0.470588235,204,0.931506849,8,0.470588235,1,ablation-analysis,Ablation studies,face_alignment1
1225,1225,1225,208,"This kind of extreme acted facial expressions generally do not occur in the training set , yet as shown in , the performance variation across different expressions is quite minor .",Ablation studies,Effect of expression .,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 15), (22, 24), (24, 25), (27, 28), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.647058824,207,0.945205479,11,0.647058824,1,ablation-analysis,Ablation studies: Effect of expression .,face_alignment1
1226,1226,1226,209,Effect of Gaussian size for guidance .,Ablation studies,,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.705882353,208,0.949771689,12,0.705882353,1,ablation-analysis,Ablation studies,face_alignment1
1227,1227,1227,211,"The performance of the 3D reconstruction dropped by a negligible amount , suggesting that as long as the Gaussians are of a sensible size , guidance will always help .",Ablation studies,Effect of Gaussian size for guidance .,face_alignment,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 6), (6, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.823529412,210,0.95890411,14,0.823529412,1,ablation-analysis,Ablation studies: Effect of Gaussian size for guidance .,face_alignment1
1228,1228,1228,2,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,title,title,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003424658,1,0,1,research-problem,title,face_alignment10
1229,1229,1229,14,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .",Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.033333333,13,0.044520548,1,0.033333333,1,research-problem,Introduction,face_alignment10
1230,1230,1230,34,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .",Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 6), (7, 9), (12, 15), (16, 21), (21, 22), (22, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.7,33,0.113013699,21,0.7,1,model,Introduction,face_alignment10
1231,1231,1231,35,"a data augmentation strategy , i.e. pose - based data balancing , that compensates the low frequency of occurrence of samples with large out - of - plane head rotations in the training set .",Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 6), (6, 11), (13, 14), (15, 19), (19, 20), (20, 21), (21, 22), (22, 30), (30, 31), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.733333333,34,0.116438356,22,0.733333333,1,model,Introduction,face_alignment10
1232,1232,1232,36,a two - stage facial landmark localisation framework for performance boosting .,Introduction,Introduction,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.766666667,35,0.119863014,23,0.766666667,1,research-problem,Introduction,face_alignment10
1233,1233,1233,209,"In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 8), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.013513514,208,0.712328767,1,0.04,1,experimental-setup,Implementation details,face_alignment10
1234,1234,1234,210,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 7), (8, 10), (11, 12), (12, 13), (13, 15), (15, 16), (16, 22), (23, 26), (27, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.027027027,209,0.715753425,2,0.08,1,experimental-setup,Implementation details,face_alignment10
1235,1235,1235,211,Note that we only use one GPU card for measuring the run time .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 8), (8, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.040540541,210,0.719178082,3,0.12,1,experimental-setup,Implementation details,face_alignment10
1236,1236,1236,212,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 10), (11, 12), (12, 13), (13, 14), (15, 17), (17, 18), (18, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.054054054,211,0.72260274,4,0.16,1,experimental-setup,Implementation details,face_alignment10
1237,1237,1237,213,Each model was trained for 120 k iterations .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.067567568,212,0.726027397,5,0.2,1,experimental-setup,Implementation details,face_alignment10
1238,1238,1238,215,"The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 9), (11, 13), (13, 14), (15, 17), (17, 18), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.094594595,214,0.732876712,7,0.28,1,experimental-setup,Implementation details,face_alignment10
1239,1239,1239,216,"For the convolutional layer , we used 3 3 kernels with the stride of 1 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (7, 10), (10, 11), (12, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.108108108,215,0.73630137,8,0.32,1,experimental-setup,Implementation details,face_alignment10
1240,1240,1240,218,"For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 11), (12, 14), (14, 15), (15, 16), (16, 17), (18, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.135135135,217,0.743150685,10,0.4,1,experimental-setup,Implementation details,face_alignment10
1241,1241,1241,219,"For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 9), (9, 10), (10, 13), (15, 16), (17, 19), (19, 20), (20, 24), (24, 25), (25, 28), (28, 29), (30, 32), (34, 35), (35, 43), (43, 44), (45, 48)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.148648649,218,0.746575342,11,0.44,1,experimental-setup,Implementation details,face_alignment10
1242,1242,1242,220,"The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 6), (7, 9), (15, 16), (16, 19), (21, 24), (24, 25), (25, 28), (30, 31), (32, 34), (34, 35), (35, 43), (43, 44), (45, 47), (49, 50), (50, 59), (59, 60), (61, 64)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.162162162,219,0.75,12,0.48,1,experimental-setup,Implementation details,face_alignment10
1243,1243,1243,222,"To perform data augmentation , we randomly rotated each training image between [ ? 30 , 30 ] degrees for CNN - 6 and between [ ? 10 , 10 ] degrees for CNN - 7 .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (6, 8), (8, 11), (11, 12), (12, 19), (19, 20), (20, 23), (24, 25), (25, 32), (32, 33), (33, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.189189189,221,0.756849315,14,0.56,1,experimental-setup,Implementation details,face_alignment10
1244,1244,1244,224,"For bounding box perturbation , we applied random translations to the upper-left and bottom - right corners of the face bounding box within 5 % of the bounding .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 7), (7, 9), (9, 10), (11, 17), (17, 18), (19, 22), (22, 23), (23, 25), (25, 26), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.216216216,223,0.76369863,16,0.64,1,experimental-setup,Implementation details,face_alignment10
1245,1245,1245,226,"We compare our method with a set of state - of - the - art approaches , including SDM , ERT , RCPR , CFSS , LBF , GRF , CCL , DAC - CSR and TR - DRN. box size .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 18), (18, 19), (20, 21), (22, 23), (24, 25), (26, 27), (28, 29), (30, 31), (32, 35), (36, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.243243243,225,0.770547945,18,0.72,1,experimental-setup,Implementation details,face_alignment10
1246,1246,1246,227,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 11), (11, 12), (12, 15), (15, 16), (17, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.256756757,226,0.773972603,19,0.76,1,experimental-setup,Implementation details,face_alignment10
1247,1247,1247,234,Comparison with state of the art 7.2.1 AFLW,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.351351351,233,0.797945205,0,0,1,results,Implementation details,face_alignment10
1248,1248,1248,246,"As shown in , our CNN - 6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function ( magenta solid line ) .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 9), (9, 10), (10, 14), (16, 18), (19, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.513513514,245,0.839041096,12,0.413793103,1,results,Implementation details,face_alignment10
1249,1249,1249,248,"Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (6, 8), (8, 9), (9, 10), (10, 11), (11, 12), (13, 15), (17, 18), (18, 19), (19, 21), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.540540541,247,0.845890411,14,0.482758621,1,results,Implementation details,face_alignment10
1250,1250,1250,249,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 11), (11, 13), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.554054054,248,0.849315068,15,0.517241379,1,results,Implementation details,face_alignment10
1251,1251,1251,251,300 W,Implementation details,Implementation details,face_alignment,10,"['O', 'O']","[(0, 2)]","['O', 'O']",43,0.581081081,250,0.856164384,17,0.586206897,1,experimental-setup,Implementation details,face_alignment10
1252,1252,1252,262,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .",Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 11), (11, 12), (13, 15), (17, 22), (22, 23), (23, 32), (32, 33), (34, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.72972973,261,0.893835616,28,0.965517241,1,results,Implementation details,face_alignment10
1253,1253,1253,263,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,Implementation details,Implementation details,face_alignment,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 6), (6, 9), (10, 12), (13, 16), (16, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.743243243,262,0.897260274,29,1,1,results,Implementation details,face_alignment10
1254,1254,1254,2,Unsupervised Training for 3D Morphable Model Regression,title,,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003759398,1,0,1,research-problem,title,face_alignment11
1255,1255,1255,20,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 10), (11, 12), (14, 16), (16, 19), (21, 23), (23, 25), (25, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,19,0.071428571,12,0.6,1,model,Introduction,face_alignment11
1256,1256,1256,21,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (8, 9), (9, 11), (12, 15), (15, 17), (18, 21), (21, 23), (23, 26), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.65,20,0.07518797,13,0.65,1,model,Introduction,face_alignment11
1257,1257,1257,23,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 6), (7, 8), (9, 10), (11, 13), (13, 14), (15, 17), (19, 21), (21, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,22,0.082706767,15,0.75,1,model,Introduction,face_alignment11
1258,1258,1258,26,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (7, 10), (12, 15), (15, 17), (18, 19), (19, 20), (20, 23), (23, 24), (25, 26), (26, 27), (28, 30), (32, 34), (34, 36), (37, 39), (40, 42), (43, 45), (48, 51), (53, 54), (54, 55), (55, 59), (59, 60), (61, 63)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9,25,0.093984962,18,0.9,1,model,Introduction,face_alignment11
1259,1259,1259,27,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13), (13, 14), (16, 19), (21, 24), (29, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.95,26,0.097744361,19,0.95,1,model,Introduction,face_alignment11
1260,1260,1260,28,"We show that despite learning from unlabeled photographs , the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects .",Introduction,Introduction,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 6), (6, 8), (10, 13), (13, 15), (16, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1,27,0.101503759,20,1,1,experiments,Introduction,face_alignment11
1261,1261,1261,184,Neutral Pose Reconstruction on MICC,Experiments,,face_alignment,11,"['O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O']",19,0.633333333,183,0.687969925,0,0,1,results,Experiments,face_alignment11
1262,1262,1262,194,"Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .",Experiments,Neutral Pose Reconstruction on MICC,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (6, 7), (7, 9), (9, 10), (11, 13), (13, 14), (14, 18), (23, 25), (25, 26), (30, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.966666667,193,0.72556391,10,0.909090909,1,results,Experiments: Neutral Pose Reconstruction on MICC,face_alignment11
1263,1263,1263,195,"We are also more stable across changing environments , with similar results for all three test sets .",Experiments,Neutral Pose Reconstruction on MICC,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,1,194,0.729323308,11,1,1,results,Experiments: Neutral Pose Reconstruction on MICC,face_alignment11
1264,1264,1264,196,Face Recognition Results,,,face_alignment,11,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,195,0.733082707,0,0,1,results,,face_alignment11
1265,1265,1265,202,Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .,Face Recognition Results,Face Recognition Results,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 6), (6, 7), (7, 10), (10, 11), (11, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.352941176,201,0.755639098,6,0.230769231,1,results,Face Recognition Results,face_alignment11
1266,1266,1266,205,"Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .",Face Recognition Results,Face Recognition Results,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 7), (8, 12), (12, 13), (14, 16), (16, 17), (17, 19), (27, 29), (30, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.529411765,204,0.766917293,9,0.346153846,1,results,Face Recognition Results,face_alignment11
1267,1267,1267,211,"Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .",Face Recognition Results,Face Recognition Results,face_alignment,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (6, 15), (15, 16), (16, 18), (19, 20), (20, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.882352941,210,0.789473684,15,0.576923077,1,results,Face Recognition Results,face_alignment11
1268,1268,1268,2,Dense Face Alignment,title,,face_alignment,12,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",1,0,1,0.003690037,0,0,1,research-problem,title,face_alignment12
1269,1269,1269,4,Face alignment is a classic problem in the computer vision field .,abstract,abstract,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,3,0.011070111,1,0.142857143,1,research-problem,abstract,face_alignment12
1270,1270,1270,38,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 12), (12, 14), (15, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.675,37,0.136531365,27,0.675,1,model,Introduction,face_alignment12
1271,1271,1271,41,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.75,40,0.147601476,30,0.75,1,model,Introduction,face_alignment12
1272,1272,1272,42,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 7), (7, 8), (9, 11), (11, 13), (14, 19), (21, 23), (23, 24), (25, 28), (28, 30), (41, 43), (43, 44), (45, 48)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.775,41,0.151291513,31,0.775,1,model,Introduction,face_alignment12
1273,1273,1273,43,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (8, 9), (9, 13), (16, 22), (22, 24), (25, 27), (27, 28), (28, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.8,42,0.15498155,32,0.8,1,model,Introduction,face_alignment12
1274,1274,1274,44,"For the second challenge of leveraging multiple datasets , the 3D face model fitting approach has the inherent advantage in handling multiple training databases .",Introduction,Introduction,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (10, 15), (17, 19), (19, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.825,43,0.158671587,33,0.825,1,model,Introduction,face_alignment12
1275,1275,1275,83,Dense Face Alignment,Related Work,,face_alignment,12,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",31,0.58490566,82,0.302583026,0,0,0,experiments,Related Work,face_alignment12
1276,1276,1276,203,"To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (6, 7), (7, 14), (14, 15), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.35483871,202,0.745387454,11,0.35483871,1,hyperparameters,Experimental setup,face_alignment12
1277,1277,1277,204,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 8), (8, 12), (14, 15), (16, 18), (18, 19), (20, 23), (23, 24), (25, 27), (27, 28), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.387096774,203,0.749077491,12,0.387096774,1,hyperparameters,Experimental setup,face_alignment12
1278,1278,1278,205,"The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .",Experimental setup,Experimental setup,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (6, 8), (8, 9), (9, 10), (13, 15), (15, 16), (16, 18), (18, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.419354839,204,0.752767528,13,0.419354839,1,hyperparameters,Experimental setup,face_alignment12
1279,1279,1279,218,"For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (5, 7), (7, 8), (9, 11), (11, 12), (13, 15), (15, 16), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.838709677,217,0.800738007,26,0.838709677,1,results,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1280,1280,1280,219,"For AFLW2000 - 3D , our method also shows a large improvement .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (5, 7), (8, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.870967742,218,0.804428044,27,0.870967742,1,results,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1281,1281,1281,220,"Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 7), (7, 8), (8, 13), (14, 16), (16, 17), (18, 19), (19, 20), (20, 22), (23, 24), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.903225806,219,0.808118081,28,0.903225806,1,results,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1282,1282,1282,221,"For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .",Experimental setup,Experiments on Large - pose Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (23, 25), (26, 27), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.935483871,220,0.811808118,29,0.935483871,1,results,Experimental setup: Experiments on Large - pose Datasets,face_alignment12
1283,1283,1283,225,"Even though the proposed method can handle largepose alignment , to show its performance on the near- frontal datasets , we evaluate our method on the 300W dataset .",Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(24, 25), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,224,0.826568266,1,0.125,1,results,Experiments on Near-frontal Datasets,face_alignment12
1284,1284,1284,228,Our method is the second best method on the challenging set .,Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 7), (7, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5,227,0.837638376,4,0.5,1,results,Experiments on Near-frontal Datasets,face_alignment12
1285,1285,1285,229,"In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .",Experiments on Near-frontal Datasets,Experiments on Near-frontal Datasets,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (6, 8), (9, 11), (11, 13), (15, 17), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,228,0.841328413,5,0.625,1,results,Experiments on Near-frontal Datasets,face_alignment12
1286,1286,1286,238,The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (7, 10), (10, 12), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.166666667,237,0.874538745,5,0.166666667,1,ablation-analysis,Ablation Study,face_alignment12
1287,1287,1287,239,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .",Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (7, 9), (9, 10), (10, 17), (17, 19), (20, 21), (21, 22), (23, 28), (28, 29), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2,238,0.878228782,6,0.2,1,ablation-analysis,Ablation Study,face_alignment12
1288,1288,1288,240,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .",Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (7, 11), (15, 19), (20, 21), (21, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.233333333,239,0.881918819,7,0.233333333,1,ablation-analysis,Ablation Study,face_alignment12
1289,1289,1289,248,Comparing LFC + SPC and LFC + CFC performances shows that the CFC is more helpful than the SPC .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 9), (9, 10), (12, 13), (13, 14), (14, 16), (16, 17), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5,247,0.911439114,15,0.5,1,ablation-analysis,Ablation Study,face_alignment12
1290,1290,1290,250,Using all constraints achieves the best performance .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.566666667,249,0.918819188,17,0.566666667,1,ablation-analysis,Ablation Study,face_alignment12
1291,1291,1291,254,This result shows that for the images with NME - lp between 5 % and 15 % the SPC is helpful .,Ablation Study,Ablation Study,face_alignment,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (6, 7), (7, 8), (8, 11), (11, 12), (12, 17), (18, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.7,253,0.933579336,21,0.7,1,ablation-analysis,Ablation Study,face_alignment12
1292,1292,1292,12,"We demonstrate the superior representation power of our nonlinear 3 DMM over its linear counterpart , and its contribution to face alignment and 3D reconstruction .",abstract,abstract,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (8, 11), (11, 12), (13, 15), (18, 19), (19, 20), (20, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,11,0.037037037,9,0.9,1,research-problem,abstract,face_alignment13
1293,1293,1293,45,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 7), (8, 10), (10, 13), (14, 15), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.3875,44,0.148148148,31,0.659574468,1,model,Introduction,face_alignment13
1294,1294,1294,46,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 12), (12, 15), (17, 24), (24, 25), (25, 26), (27, 33), (33, 34), (34, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4,45,0.151515152,32,0.680851064,1,model,Introduction,face_alignment13
1295,1295,1295,47,Each decoder will take a shape or texture representation as input and output the dense 3 D face or a face texture .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (5, 9), (9, 10), (10, 11), (12, 13), (14, 22), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4125,46,0.154882155,33,0.70212766,1,model,Introduction,face_alignment13
1296,1296,1296,49,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (7, 8), (9, 12), (15, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.4375,48,0.161616162,35,0.744680851,1,model,Introduction,face_alignment13
1297,1297,1297,50,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 8), (8, 9), (9, 10), (11, 12), (13, 17), (18, 19), (20, 22), (22, 23), (24, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.45,49,0.164983165,36,0.765957447,1,model,Introduction,face_alignment13
1298,1298,1298,51,"The 3 D face and texture would perfectly reconstruct the input face , if the fitting algorithm and 3 DMM are well learnt .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (7, 9), (10, 12), (13, 14), (15, 17), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.4625,50,0.168350168,37,0.787234043,1,model,Introduction,face_alignment13
1299,1299,1299,52,"Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 10), (11, 13), (13, 15), (16, 26), (26, 28), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.475,51,0.171717172,38,0.808510638,1,model,Introduction,face_alignment13
1300,1300,1300,53,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .",Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 8), (9, 10), (10, 11), (12, 16), (17, 19), (19, 21), (22, 23), (23, 24), (25, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.4875,52,0.175084175,39,0.829787234,1,model,Introduction,face_alignment13
1301,1301,1301,54,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 10), (15, 21), (21, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.5,53,0.178451178,40,0.85106383,1,model,Introduction,face_alignment13
1302,1302,1302,55,We show significantly improved shape and texture representation power over the linear 3 DMM .,Introduction,Introduction,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 9), (9, 10), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5125,54,0.181818182,41,0.872340426,1,experiments,Introduction,face_alignment13
1303,1303,1303,208,"The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.",Experimental Results,Experimental Results,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (7, 8), (9, 12), (12, 13), (13, 14), (14, 16), (16, 18), (20, 21), (21, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.068181818,207,0.696969697,3,0.75,1,hyperparameters,Experimental Results,face_alignment13
1304,1304,1304,209,"We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .",Experimental Results,Experimental Results,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (12, 17), (18, 25), (32, 33), (33, 35), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.090909091,208,0.7003367,4,1,1,hyperparameters,Experimental Results,face_alignment13
1305,1305,1305,226,Representation Power,Experimental Results,,face_alignment,13,"['O', 'O']","[(0, 2)]","['O', 'O']",21,0.477272727,225,0.757575758,0,0,1,results,Experimental Results,face_alignment13
1306,1306,1306,230,"Alternatively , we can minimize the reconstruction error in the image space , through the rendering layer with the groundtruth S and m .",Experimental Results,Texture .,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 8), (8, 9), (10, 12), (13, 14), (15, 17), (17, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.568181818,229,0.771043771,4,0.173913043,1,baselines,Experimental Results: Texture .,face_alignment13
1307,1307,1307,233,"As in , our nonlinear texture is closer to the groundtruth than the linear model , especially for in - the - wild images ( the first two rows ) .",Experimental Results,Texture .,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 9), (10, 11), (11, 12), (13, 15), (16, 18), (18, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.636363636,232,0.781144781,7,0.304347826,1,results,Experimental Results: Texture .,face_alignment13
1308,1308,1308,235,"Quantitatively , our nonlinear model has significantly lower L 1 reconstruction error than the lin - We also compare the power of nonlinear and linear 3 DMM in representing real - world 3D scans .",Experimental Results,Texture .,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 12), (12, 13), (14, 15), (18, 19), (20, 21), (22, 27), (27, 29), (29, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.681818182,234,0.787878788,9,0.391304348,1,results,Experimental Results: Texture .,face_alignment13
1309,1309,1309,247,"Our nonlinear model has a significantly smaller reconstruction error than the linear model , 0.0196 vs. 0.0241 ( Tab. 3 ) .",Experimental Results,Texture .,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 9), (9, 10), (11, 13), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.954545455,246,0.828282828,21,0.913043478,1,results,Experimental Results: Texture .,face_alignment13
1310,1310,1310,264,Quantitative evaluation of 3D reconstruction .,Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (3, 5)]","['O', 'O', 'O', 'O', 'O', 'O']",14,0.466666667,263,0.885521886,14,0.466666667,1,results,Applications,face_alignment13
1311,1311,1311,265,We obtain a low error that is comparable to optimization - based methods .,Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (7, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5,264,0.888888889,15,0.5,1,results,Applications,face_alignment13
1312,1312,1312,271,3D Face Reconstruction .,Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",21,0.7,270,0.909090909,21,0.7,1,experiments,Applications,face_alignment13
1313,1313,1313,280,"We achieve on - par results with Garrido et al. , an offline optimization method , while surpassing all other regression methods ] .",Applications,Applications,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (7, 10), (12, 15), (17, 18), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,1,279,0.939393939,30,1,1,results,Applications,face_alignment13
1314,1314,1314,284,Using a global image - based discriminator is redundant as the global structure is guaranteed by the rendering layer .,Ablation on Texture Learning,Ablation on Texture Learning,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7), (7, 8), (8, 9), (11, 13), (14, 16), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,283,0.952861953,3,0.5,1,ablation-analysis,Ablation on Texture Learning,face_alignment13
1315,1315,1315,285,"Also , we empirically find that using global image - based discriminator can cause severe artifacts in the resultant texture .",Ablation on Texture Learning,Ablation on Texture Learning,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 7), (7, 12), (12, 14), (14, 16), (16, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,284,0.956228956,4,0.666666667,1,ablation-analysis,Ablation on Texture Learning,face_alignment13
1316,1316,1316,287,"Clearly , patchGAN offers higher realism and fewer artifacts .",Ablation on Texture Learning,Ablation on Texture Learning,face_alignment,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 6), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,286,0.962962963,6,1,1,ablation-analysis,Ablation on Texture Learning,face_alignment13
1317,1317,1317,2,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses,title,title,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003773585,1,0,1,research-problem,title,face_alignment14
1318,1318,1318,14,Robust face recognition and analysis are contingent upon accurate localization of facial features .,Introduction,Introduction,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037,13,0.049056604,1,0.043478261,1,research-problem,Introduction,face_alignment14
1319,1319,1319,33,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .",Introduction,Introduction,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 16), (18, 21), (22, 25), (25, 27), (28, 30), (30, 31), (32, 36), (36, 38), (38, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.740740741,32,0.120754717,20,0.869565217,1,model,Introduction,face_alignment14
1320,1320,1320,203,Our network is implemented in the Caffe framework .,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.047619048,202,0.762264151,1,0.047619048,1,experimental-setup,Implementation Details,face_alignment14
1321,1321,1321,204,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 5), (5, 7), (8, 12), (14, 17), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.095238095,203,0.766037736,2,0.095238095,1,experimental-setup,Implementation Details,face_alignment14
1322,1322,1322,206,"We adopt two architectures , AlexNet and VGG - 16 , as the pre-trained models for our shared feature extraction networks in , i.e. we use the convolution layers from the pre-trained models to initialize ours .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 10), (11, 12), (13, 15), (15, 16), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.19047619,205,0.773584906,4,0.19047619,1,experimental-setup,Implementation Details,face_alignment14
1323,1323,1323,208,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .",Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (8, 10), (11, 12), (13, 17), (19, 22), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.285714286,207,0.781132075,6,0.285714286,1,experimental-setup,Implementation Details,face_alignment14
1324,1324,1324,209,The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,Implementation Details,Implementation Details,face_alignment,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 8), (8, 10), (10, 13), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.333333333,208,0.78490566,7,0.333333333,1,experimental-setup,Implementation Details,face_alignment14
1325,1325,1325,2,Face Alignment Across Large Poses : A 3D Solution,title,,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003496503,1,0,1,research-problem,title,face_alignment15
1326,1326,1326,4,"Face alignment , which fits a face model to an image and extracts the semantic meanings of facial pixels , has been an important topic in CV community .",abstract,abstract,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.111111111,3,0.01048951,1,0.111111111,1,research-problem,abstract,face_alignment15
1327,1327,1327,45,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 13), (14, 18), (18, 19), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.695652174,44,0.153846154,1,0.066666667,1,model,Introduction,face_alignment15
1328,1328,1328,47,We call this method 3D Dense Face Alignment ( 3DDFA ) .,Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.739130435,46,0.160839161,3,0.2,1,model,Introduction,face_alignment15
1329,1329,1329,50,"To resolve the fitting process in 3 DDFA , we propose a cascaded convolutional neutral network ( CNN ) based regression method .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 8), (10, 11), (12, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.804347826,49,0.171328671,6,0.4,1,model,Introduction,face_alignment15
1330,1330,1330,52,"In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 9), (10, 13), (13, 14), (15, 18), (19, 20), (20, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.847826087,51,0.178321678,8,0.533333333,1,model,Introduction,face_alignment15
1331,1331,1331,53,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 9), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.869565217,52,0.181818182,9,0.6,1,model,Introduction,face_alignment15
1332,1332,1332,54,"To the best of our knowledge , this is the first attempt to solve the 3D face alignment with CNN .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (10, 12), (12, 14), (15, 18), (18, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.891304348,53,0.185314685,10,0.666666667,1,model,Introduction,face_alignment15
1333,1333,1333,56,"To enable the training of the 3DDFA , we construct a face database containing pairs of 2D face images and 3D face models .",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (4, 5), (6, 7), (9, 10), (11, 13), (13, 14), (14, 15), (15, 16), (16, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.934782609,55,0.192307692,12,0.8,1,model,Introduction,face_alignment15
1334,1334,1334,57,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 9), (9, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.956521739,56,0.195804196,13,0.866666667,1,model,Introduction,face_alignment15
1335,1335,1335,58,The synthesized samples well simulate the face appearances in large poses and boost the performance of both prior and our proposed face alignment algorithms .,Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (6, 8), (8, 9), (9, 11), (12, 13), (14, 15), (15, 16), (17, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.97826087,57,0.199300699,14,0.933333333,1,experiments,Introduction,face_alignment15
1336,1336,1336,59,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/.",Introduction,Introduction,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,1,58,0.202797203,15,1,1,code,Introduction,face_alignment15
1337,1337,1337,221,Error Reduction in Cascade :,Experiments,Performance Analysis,face_alignment,15,"['O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O']",22,0.578947368,220,0.769230769,1,0.058823529,1,ablation-analysis,Experiments: Performance Analysis,face_alignment15
1338,1338,1338,225,"As observed , the testing error is reduced due to initialization regeneration .",Experiments,Performance Analysis,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 8), (8, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.684210526,224,0.783216783,5,0.294117647,1,ablation-analysis,Experiments: Performance Analysis,face_alignment15
1339,1339,1339,226,In the generic cascade process the training and testing errors converge fast after 2 iterations .,Experiments,Performance Analysis,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (6, 10), (10, 11), (11, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.710526316,225,0.786713287,6,0.352941176,1,ablation-analysis,Experiments: Performance Analysis,face_alignment15
1340,1340,1340,227,"While with initialization regeneration , the training error is updated at the beginning of each iteration and the testing error continues to descend .",Experiments,Performance Analysis,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (6, 8), (9, 10), (10, 11), (12, 14), (14, 16), (18, 20), (20, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.736842105,226,0.79020979,7,0.411764706,1,hyperparameters,Experiments: Performance Analysis,face_alignment15
1341,1341,1341,231,Performance with Different Costs :,Experiments,Performance Analysis,face_alignment,15,"['O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O']",32,0.842105263,230,0.804195804,11,0.647058824,1,results,Experiments: Performance Analysis,face_alignment15
1342,1342,1342,235,It is shown that PDC can not well model the fitting error and converges to an unsatisfied result .,Experiments,Performance Analysis,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 9), (10, 12), (13, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.947368421,234,0.818181818,15,0.882352941,1,ablation-analysis,Experiments: Performance Analysis,face_alignment15
1343,1343,1343,237,"WPDC explicitly models the priority of each parameter and adaptively optimizes them with the parameter weights , leading to the best result .",Experiments,Performance Analysis,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 5), (5, 6), (6, 8), (9, 11), (12, 13), (14, 16), (17, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,1,236,0.825174825,17,1,1,baselines,Experiments: Performance Analysis,face_alignment15
1344,1344,1344,240,Large Pose Face Alignment in AFLW Protocol :,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.051282051,239,0.835664336,2,0.076923077,1,baselines,Comparison Experiments,face_alignment15
1345,1345,1345,258,"Firstly , the results indicate that all the methods benefits substantially from face profiling when dealing with large poses .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 9), (9, 12), (12, 14), (14, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.512820513,257,0.898601399,20,0.769230769,1,results,Comparison Experiments,face_alignment15
1346,1346,1346,262,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 9), (9, 10), (10, 14), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.615384615,261,0.912587413,24,0.923076923,1,results,Comparison Experiments,face_alignment15
1347,1347,1347,263,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (7, 8), (9, 10), (10, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.641025641,262,0.916083916,25,0.961538462,1,results,Comparison Experiments,face_alignment15
1348,1348,1348,264,"Finally , the performance of 3DDFA can be further improved with the SDM landmark refinement in Section 5.2 .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (5, 6), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.666666667,263,0.91958042,26,1,1,results,Comparison Experiments,face_alignment15
1349,1349,1349,265,3D Face Alignment in AFLW2000-3D,Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 5)]","['O', 'O', 'O', 'O', 'O']",27,0.692307692,264,0.923076923,0,0,1,baselines,Comparison Experiments,face_alignment15
1350,1350,1350,270,"For all the methods , despite with ground truth bounding boxes the performance in [ 60 , 90 ] and the standard deviation are obviously reduced when considering all the landmarks .",Comparison Experiments,Comparison Experiments,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (5, 7), (12, 13), (13, 14), (14, 19), (21, 23), (25, 26), (26, 28), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.820512821,269,0.940559441,5,0.833333333,1,results,Comparison Experiments,face_alignment15
1351,1351,1351,279,Common Challenging Full TSPM,Method,,face_alignment,15,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",1,0.5,278,0.972027972,1,0.5,1,results,Method,face_alignment15
1352,1352,1352,280,"8 that even as a generic face alignment algorithm , 3 DDFA still demonstrates competitive performance on the common set and state - of - the - art performance on the challenging set .",Method,Common Challenging Full TSPM,face_alignment,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12), (13, 14), (14, 16), (16, 17), (18, 20), (21, 29), (29, 30), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,1,279,0.975524476,2,1,1,results,Method: Common Challenging Full TSPM,face_alignment15
1353,1353,1353,2,Deep Multi- Center Learning for Face Alignment,title,,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003039514,1,0,1,research-problem,title,face_alignment16
1354,1354,1354,11,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,abstract,abstract,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.666666667,10,0.030395137,8,0.166666667,1,code,abstract,face_alignment16
1355,1355,1355,35,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 12), (12, 13), (13, 20), (20, 22), (23, 25), (25, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.218390805,34,0.103343465,32,0.666666667,1,model,I. INTRODUCTION,face_alignment16
1356,1356,1356,36,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 10), (10, 12), (13, 14), (14, 15), (15, 16), (18, 22), (22, 24), (25, 26), (26, 27), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.229885057,35,0.106382979,33,0.6875,1,model,I. INTRODUCTION,face_alignment16
1357,1357,1357,37,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 7), (8, 10), (10, 11), (11, 12), (12, 13), (15, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.24137931,36,0.109422492,34,0.708333333,1,model,I. INTRODUCTION,face_alignment16
1358,1358,1358,38,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 7), (9, 10), (11, 14), (14, 16), (16, 20), (20, 21), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.252873563,37,0.112462006,35,0.729166667,1,model,I. INTRODUCTION,face_alignment16
1359,1359,1359,39,The entire framework reinforces the learning process of each landmark with a low model complexity .,I. INTRODUCTION,I. INTRODUCTION,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (7, 8), (8, 10), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.264367816,38,0.11550152,36,0.75,1,model,I. INTRODUCTION,face_alignment16
1360,1360,1360,210,"Uniform scaling and translation with different extents on face bounding boxes are further conducted , in which each newly generated face bounding box is used to crop the face .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (5, 7), (7, 8), (8, 11), (17, 23), (26, 27), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.706666667,209,0.635258359,75,0.465838509,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1361,1361,1361,211,Finally training samples are augmented through horizontal flip and JPEG compression .,A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.713333333,210,0.638297872,76,0.472049689,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1362,1362,1362,213,We train our MCL using an open source deep learning framework Caffe .,A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",109,0.726666667,212,0.6443769,78,0.48447205,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1363,1363,1363,214,"The input face patch is a 50 50 grayscale image , and each pixel value is normalized to [ ?1 , 1 ) by subtracting 128 and multiplying 0.0078125 .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (6, 10), (12, 15), (16, 18), (18, 23), (23, 24), (24, 25), (25, 26), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",110,0.733333333,213,0.647416413,79,0.49068323,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1364,1364,1364,215,"A more complex model is needed for a labeling pattern with more facial landmarks , so Dis set to be 512/512/1 , 024 for 5/29/68 facial landmarks .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (8, 10), (10, 11), (11, 14), (20, 23), (23, 24), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",111,0.74,214,0.650455927,80,0.49689441,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1365,1365,1365,216,"The type of solver is SGD with a mini-batch size of 64 , a momentum of 0.9 , and a weight decay of 0.0005 .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (6, 7), (8, 10), (10, 11), (11, 12), (14, 15), (15, 16), (16, 17), (20, 22), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",112,0.746666667,215,0.653495441,81,0.50310559,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1366,1366,1366,217,"The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 10), (10, 11), (11, 16), (20, 23), (23, 24), (24, 31), (31, 32), (32, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",113,0.753333333,216,0.656534954,82,0.50931677,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1367,1367,1367,219,"The learning rate is multiplied by a factor of 0.3 at every 3 10 4 iterations , and the remaining parameter ?",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 8), (8, 9), (9, 10), (10, 11), (11, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",115,0.766666667,218,0.662613982,84,0.52173913,1,experimental-setup,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1368,1368,1368,230,"We compare our work MCL against state - of - the - art methods including ESR , SDM , Cascaded CNN , RCPR , CFAN , LBF , c GPRT , CFSS , TCDCN , , ALR , CFT , RFLD , RecNet , RAR , and FLD + PDE .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 6), (6, 14), (14, 15), (15, 16), (17, 18), (19, 21), (22, 23), (24, 25), (26, 27), (28, 30), (31, 32), (33, 34), (36, 37), (38, 39), (40, 41), (42, 43), (44, 45), (47, 50)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",126,0.84,229,0.696048632,95,0.590062112,1,baselines,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1369,1369,1369,237,"Our method MCL outperforms most of the state - of - the - art methods , especially on AFLW dataset where a relative error reduction of 3.93 % is achieved compared to RecNet .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 15), (17, 18), (18, 20), (30, 32), (32, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",133,0.886666667,236,0.717325228,102,0.633540373,1,results,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1370,1370,1370,239,We compare with other methods on several challenging images from AFLW and COFW respectively in .,A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 9), (9, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",135,0.9,238,0.723404255,104,0.645962733,1,results,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1371,1371,1371,242,"MCL demonstrates a superior capability of handling severe occlusions and complex variations of pose , expression , illumination .",A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",138,0.92,241,0.732522796,107,0.664596273,1,results,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1372,1372,1372,244,It is observed that MCL achieves competitive performance on all three benchmarks .,A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (6, 8), (8, 9), (9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",140,0.933333333,243,0.738601824,109,0.677018634,1,results,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1373,1373,1373,245,The average running speed of deep learning methods for detecting 68 facial landmarks are presented in .,A. Network Architecture,EXPERIMENTS A. Datasets and Settings,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 8), (8, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",141,0.94,244,0.741641337,110,0.683229814,1,results,A. Network Architecture: EXPERIMENTS A. Datasets and Settings,face_alignment16
1374,1374,1374,256,1 ) Global Average Pooling vs. Full Connection :,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.015384615,255,0.775075988,121,0.751552795,1,ablation-analysis,C. Ablation Study,face_alignment16
1375,1375,1375,259,It can be seen that BM performs better on IBUG and COFW but worse on AFLW than pre-BM .,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (8, 9), (9, 12), (13, 15), (15, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.061538462,258,0.784194529,124,0.770186335,1,ablation-analysis,C. Ablation Study,face_alignment16
1376,1376,1376,260,It demonstrates that Global Average Pooling is more advantageous for more complex problems with more facial landmarks .,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 9), (9, 10), (10, 13), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.076923077,259,0.787234043,125,0.776397516,1,ablation-analysis,C. Ablation Study,face_alignment16
1377,1377,1377,266,2 ) Robustness of Weighting :,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O']","[(2, 5)]","['O', 'O', 'O', 'O', 'O', 'O']",11,0.169230769,265,0.805471125,131,0.813664596,1,ablation-analysis,C. Ablation Study,face_alignment16
1378,1378,1378,273,"When ? is 0.4 , WM can still achieves good performance .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (5, 6), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.276923077,272,0.82674772,138,0.857142857,1,ablation-analysis,C. Ablation Study,face_alignment16
1379,1379,1379,276,3 ) Analysis of Shape Prediction Layers :,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.323076923,275,0.835866261,141,0.875776398,1,ablation-analysis,C. Ablation Study,face_alignment16
1380,1380,1380,279,"Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (5, 13), (14, 15), (16, 18), (18, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.369230769,278,0.844984802,144,0.894409938,1,ablation-analysis,C. Ablation Study,face_alignment16
1381,1381,1381,280,"As a result , the assembled AM can improve the detection accuracy of landmarks of the left eye and the right eye on the basis of WM .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 9), (10, 12), (12, 13), (13, 14), (14, 15), (16, 22), (22, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.384615385,279,0.848024316,145,0.900621118,1,ablation-analysis,C. Ablation Study,face_alignment16
1382,1382,1382,281,Note that the two models also improve the localization precision of other clusters .,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 7), (8, 10), (10, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.4,280,0.85106383,146,0.906832298,1,ablation-analysis,C. Ablation Study,face_alignment16
1383,1383,1383,285,4 ) Integration of Weighting Fine - Tuning and Multi - Center Fine - Tuning :,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.461538462,284,0.863221884,150,0.931677019,1,ablation-analysis,C. Ablation Study,face_alignment16
1384,1384,1384,288,"The accuracy of AM is superior to that of Simplified AM especially on challenging IBUG , which is attributed to the integration of two stages .",C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 7), (9, 11), (11, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.507692308,287,0.872340426,153,0.950310559,1,ablation-analysis,C. Ablation Study,face_alignment16
1385,1385,1385,290,It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG .,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (8, 10), (10, 11), (11, 12), (13, 16), (17, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.538461538,289,0.878419453,155,0.962732919,1,ablation-analysis,C. Ablation Study,face_alignment16
1386,1386,1386,293,It can be observed that AM has higher accuracy and stronger robustness than BM and WM .,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 6), (7, 9), (10, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.584615385,292,0.887537994,158,0.98136646,1,ablation-analysis,C. Ablation Study,face_alignment16
1387,1387,1387,295,The localization accuracy of facial landmarks from each cluster is improved in the details .,C. Ablation Study,C. Ablation Study,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (6, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.615384615,294,0.893617021,160,0.99378882,1,ablation-analysis,C. Ablation Study,face_alignment16
1388,1388,1388,297,D. MCL for Partially Occluded Faces,C. Ablation Study,,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",42,0.646153846,296,0.899696049,0,0,1,ablation-analysis,C. Ablation Study,face_alignment16
1389,1389,1389,298,The correlations among different facial parts are very useful for face alignment especially for partially occluded faces .,C. Ablation Study,D. MCL for Partially Occluded Faces,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (10, 12), (12, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.661538462,297,0.902735562,1,0.03125,1,ablation-analysis,C. Ablation Study: D. MCL for Partially Occluded Faces,face_alignment16
1390,1390,1390,303,"After processing testing faces with occlusions , the mean error results of both WM and AM increase .",C. Ablation Study,D. MCL for Partially Occluded Faces,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (4, 5), (5, 6), (8, 11), (11, 12), (13, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.738461538,302,0.917933131,6,0.1875,1,ablation-analysis,C. Ablation Study: D. MCL for Partially Occluded Faces,face_alignment16
1391,1391,1391,304,"Besides the results of landmarks from the left eye cluster , the results of remaining landmarks from other clusters become worse slightly .",C. Ablation Study,D. MCL for Partially Occluded Faces,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 6), (7, 10), (12, 13), (13, 14), (14, 16), (16, 17), (17, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.753846154,303,0.920972644,7,0.21875,1,ablation-analysis,C. Ablation Study: D. MCL for Partially Occluded Faces,face_alignment16
1392,1392,1392,306,"Note that WM and AM still perform well on occluded left eyes with the mean error of 6.60 and 6.50 respectively , due to the following reasons .",C. Ablation Study,D. MCL for Partially Occluded Faces,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (6, 7), (7, 8), (8, 9), (9, 12), (12, 13), (14, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.784615385,305,0.927051672,9,0.28125,1,ablation-analysis,C. Ablation Study: D. MCL for Partially Occluded Faces,face_alignment16
1393,1393,1393,310,E. Weighting Fine - Tuning for State - of - the - Art Frameworks,C. Ablation Study,D. MCL for Partially Occluded Faces,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.846153846,309,0.939209726,13,0.40625,1,ablation-analysis,C. Ablation Study: D. MCL for Partially Occluded Faces,face_alignment16
1394,1394,1394,317,It can be seen that the mean error of re -DAN is reduced from 7.97 to 7.81 after using our proposed weighting fine - tuning .,C. Ablation Study,D. MCL for Partially Occluded Faces,face_alignment,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 9), (9, 11), (12, 14), (14, 17), (17, 19), (19, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.953846154,316,0.960486322,20,0.625,1,ablation-analysis,C. Ablation Study: D. MCL for Partially Occluded Faces,face_alignment16
1395,1395,1395,5,"Facial landmark detection , or face alignment , is a fundamental task that has been extensively studied .",abstract,abstract,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,4,0.014336918,1,0.142857143,1,research-problem,abstract,face_alignment17
1396,1396,1396,11,The code is made publicly available at https://github.com/thesouthfrog/stylealign.,abstract,abstract,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,1,10,0.035842294,7,1,1,code,abstract,face_alignment17
1397,1397,1397,30,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 11), (11, 12), (12, 13), (13, 16), (16, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.514285714,29,0.103942652,18,0.514285714,1,model,Introduction,face_alignment17
1398,1398,1398,31,"Instead of directly generating images , we first map face images into the space of structure and style .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 12), (13, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.542857143,30,0.107526882,19,0.542857143,1,model,Introduction,face_alignment17
1399,1399,1399,32,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 18), (19, 21), (21, 32), (33, 35), (35, 37), (37, 38), (38, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.571428571,31,0.111111111,20,0.571428571,1,model,Introduction,face_alignment17
1400,1400,1400,33,"By factoring these features , we perform visual style translation between existing facial geometry .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (6, 7), (7, 10), (10, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6,32,0.114695341,21,0.6,1,model,Introduction,face_alignment17
1401,1401,1401,34,"Given existing facial structure , faces with glasses , of poor quality , under blur or strong lighting are rerendered from corresponding style , which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry .",Introduction,Introduction,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (5, 6), (7, 8), (9, 10), (10, 12), (13, 14), (14, 18), (19, 21), (21, 23), (28, 30), (31, 34), (34, 35), (36, 41), (41, 43), (43, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.628571429,33,0.11827957,22,0.628571429,1,model,Introduction,face_alignment17
1402,1402,1402,154,"Implementation Details Before training , all images are cropped and resized to 256 256 using provided bounding boxes .",Evaluation Metrics,Evaluation Metrics,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 7), (8, 12), (12, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.147058824,153,0.548387097,6,0.171428571,1,hyperparameters,Evaluation Metrics,face_alignment17
1403,1403,1403,156,"We use 6 residual encoder blocks for downsampling the input feature maps , where batch normalization is removed for better synthetic results .",Evaluation Metrics,Evaluation Metrics,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (6, 8), (7, 8), (9, 12), (13, 14), (14, 16), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.205882353,155,0.555555556,8,0.228571429,1,hyperparameters,Evaluation Metrics,face_alignment17
1404,1404,1404,158,"For training of the disentangling step , we use Adam with an initial learning rate of 0.01 , which descends linearly to 0.0001 with no augmentation .",Evaluation Metrics,Evaluation Metrics,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 3), (4, 6), (8, 9), (9, 10), (10, 11), (12, 15), (15, 16), (16, 17), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.264705882,157,0.562724014,10,0.285714286,1,hyperparameters,Evaluation Metrics,face_alignment17
1405,1405,1405,159,"For training of detectors , we first augment each landmark map with k random styles sampled from other face images .",Evaluation Metrics,Evaluation Metrics,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (7, 8), (8, 11), (11, 12), (12, 15), (15, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.294117647,158,0.566308244,11,0.314285714,1,hyperparameters,Evaluation Metrics,face_alignment17
1406,1406,1406,161,"For the detector architecture , a simple baseline network based on ResNet - 18 is chosen by changing the output dimension of the last FC layers to landmark 2 to demonstrate the increase brought by style translation .",Evaluation Metrics,Evaluation Metrics,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 9), (9, 11), (11, 14), (15, 18), (19, 21), (21, 22), (23, 26), (26, 27), (29, 31), (32, 33), (33, 35), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.352941176,160,0.573476703,13,0.371428571,1,hyperparameters,Evaluation Metrics,face_alignment17
1407,1407,1407,164,WFLW,Evaluation Metrics,,face_alignment,17,['O'],"[(0, 1)]",['O'],15,0.441176471,163,0.584229391,16,0.457142857,1,baselines,Evaluation Metrics,face_alignment17
1408,1408,1408,170,The light - weight Res - 18 is improved by 13.8 % .,Evaluation Metrics,WFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (8, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.617647059,169,0.605734767,22,0.628571429,1,results,Evaluation Metrics: WFLW,face_alignment17
1409,1409,1409,171,"By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .",Evaluation Metrics,WFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 8), (8, 9), (9, 12), (12, 13), (13, 17), (18, 19), (19, 26), (26, 27), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.647058824,170,0.609318996,23,0.657142857,1,results,Evaluation Metrics: WFLW,face_alignment17
1410,1410,1410,172,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .",Evaluation Metrics,WFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (8, 10), (11, 12), (12, 15), (15, 16), (16, 18), (20, 23), (23, 24), (24, 25), (25, 26), (26, 29), (29, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.676470588,171,0.612903226,24,0.685714286,1,results,Evaluation Metrics: WFLW,face_alignment17
1411,1411,1411,175,"With additional "" style- augmented "" synthetic training samples , our model based on a simple backbone outperforms previous state - of - the - art methods .",Evaluation Metrics,WFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 9), (10, 12), (12, 14), (15, 17), (17, 18), (18, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.764705882,174,0.623655914,27,0.771428571,1,results,Evaluation Metrics: WFLW,face_alignment17
1412,1412,1412,177,300W,Evaluation Metrics,WFLW,face_alignment,17,['O'],"[(0, 1)]",['O'],28,0.823529412,176,0.630824373,29,0.828571429,1,baselines,Evaluation Metrics: WFLW,face_alignment17
1413,1413,1413,183,"However , our model still yields 1.8 % and 3.1 % improvement on LAB and SAN respectively , which manifest the consistent benefit when using the "" style - augmented "" strategy .",Evaluation Metrics,In,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 6), (6, 12), (12, 13), (13, 16), (19, 20), (21, 23), (23, 25), (26, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,1,182,0.652329749,35,1,1,results,Evaluation Metrics: In,face_alignment17
1414,1414,1414,185,Common Cross - dataset Evaluation on COFW,Method,Method,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.071428571,184,0.659498208,1,0.071428571,1,results,Method,face_alignment17
1415,1415,1415,188,"Our model performs the best with 4.43 % mean error and 2.82 % failure rate , which indicates high robustness to occlusion due to our proper utilization of style translation .",Method,Method,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 5), (5, 6), (6, 10), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.285714286,187,0.670250896,4,0.285714286,1,results,Method,face_alignment17
1416,1416,1416,189,AFLW,Method,,face_alignment,17,['O'],"[(0, 1)]",['O'],5,0.357142857,188,0.673835125,5,0.357142857,1,baselines,Method,face_alignment17
1417,1417,1417,196,"Exploiting style information also boosts landmark detectors with a large - scale training set ( 25 , 000 images in AFLW ) .",Method,AFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 5), (5, 7), (7, 8), (9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.857142857,195,0.698924731,12,0.857142857,1,experiments,Method: AFLW,face_alignment17
1418,1418,1418,197,"Interestingly , our method improves SAN baseline in terms of NME on Full set from 6.94 % to 6.01 % , which indicates that augmenting in style level brings promising improvement on solving large pose variation .",Method,AFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 7), (7, 10), (10, 11), (11, 12), (12, 14), (14, 15), (15, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.928571429,196,0.702508961,13,0.928571429,1,results,Method: AFLW,face_alignment17
1419,1419,1419,198,The visual comparison in shows hidden face part is better modeled with our strategy .,Method,AFLW,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 8), (9, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,1,197,0.70609319,14,1,1,results,Method: AFLW,face_alignment17
1420,1420,1420,201,Disentanglement of style and structure is the key that influences quality of style - augmented samples .,Ablation Study,Improvement on Limited Data,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.064516129,200,0.716845878,1,0.058823529,1,ablation-analysis,Ablation Study: Improvement on Limited Data,face_alignment17
1421,1421,1421,210,"Style - augmented synthetic images improve detectors ' performance by a large margin , while the improvement is even larger when the number of training images is quite small .",Ablation Study,Improvement on Limited Data,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (5, 6), (6, 9), (9, 10), (11, 13), (16, 17), (18, 20), (20, 21), (22, 26), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.35483871,209,0.749103943,10,0.588235294,1,ablation-analysis,Ablation Study: Improvement on Limited Data,face_alignment17
1422,1422,1422,225,We evaluate our method by adding the number of random sampled styles k of each annotated landmarks on a ResNet - 50 baseline .,Ablation Study,Estimating the Upper-bound,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13), (13, 14), (14, 17), (17, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.838709677,224,0.802867384,7,0.583333333,1,ablation-analysis,Ablation Study: Estimating the Upper-bound,face_alignment17
1423,1423,1423,227,"By adding a number of augmented styles , the model continue gaining improvement .",Ablation Study,Estimating the Upper-bound,face_alignment,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (9, 10), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.903225806,226,0.810035842,9,0.75,1,ablation-analysis,Ablation Study: Estimating the Upper-bound,face_alignment17
1424,1424,1424,2,Deep Alignment Network : A convolutional neural network for robust face alignment,title,title,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004115226,1,0,1,research-problem,title,face_alignment18
1425,1425,1425,12,"The goal of face alignment is to localize a set of predefined facial landmarks ( eye corners , mouth corners etc. ) in an image of a face .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.028571429,11,0.04526749,1,0.028571429,1,research-problem,Introduction,face_alignment18
1426,1426,1426,19,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 16), (18, 19), (19, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.228571429,18,0.074074074,8,0.228571429,1,model,Introduction,face_alignment18
1427,1427,1427,20,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 8), (8, 9), (9, 11), (11, 12), (13, 15), (15, 17), (18, 20), (21, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.257142857,19,0.0781893,9,0.257142857,1,model,Introduction,face_alignment18
1428,1428,1428,21,The input to each stage of our algorithm ( except the first stage ) area face image normalized to a canonical pose and an image learned from the dense layer of the previous stage .,Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 6), (6, 8), (14, 15), (15, 17), (17, 19), (20, 22), (24, 25), (25, 27), (28, 30), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.285714286,20,0.082304527,10,0.285714286,1,model,Introduction,face_alignment18
1429,1429,1429,22,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .",Introduction,Introduction,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (8, 9), (10, 11), (11, 12), (12, 14), (17, 19), (19, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.314285714,21,0.086419753,11,0.314285714,1,model,Introduction,face_alignment18
1430,1430,1430,193,During data augmentation a total of 10 images are created from each input image in the training set .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 8), (9, 11), (11, 14), (14, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.076923077,192,0.790123457,42,0.677419355,1,experimental-setup,Implementation,face_alignment18
1431,1431,1431,195,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 6), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.128205128,194,0.798353909,44,0.709677419,1,experimental-setup,Implementation,face_alignment18
1432,1432,1432,196,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 4), (4, 7), (7, 8), (9, 12), (12, 13), (13, 14), (15, 18), (18, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.153846154,195,0.802469136,45,0.725806452,1,experimental-setup,Implementation,face_alignment18
1433,1433,1433,197,For validation we use a random subset of 100 images from the training set .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 4), (5, 7), (7, 8), (8, 10), (10, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.179487179,196,0.806584362,46,0.741935484,1,experimental-setup,Implementation,face_alignment18
1434,1434,1434,198,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 7), (7, 8), (8, 9), (9, 11), (11, 12), (14, 16), (16, 17), (17, 18), (20, 21), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.205128205,197,0.810699588,47,0.758064516,1,experimental-setup,Implementation,face_alignment18
1435,1435,1435,210,"a failure rate reduction of 60 % on the 300 W private test set ,",Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 7), (7, 8), (9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.512820513,209,0.860082305,59,0.951612903,1,results,Implementation,face_alignment18
1436,1436,1436,211,"a failure rate reduction of 72 % on the 300W public test set ,",Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 7), (7, 8), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.538461538,210,0.864197531,60,0.967741935,1,results,Implementation,face_alignment18
1437,1437,1437,212,a 9 % improvement of the mean error on the challenging subset .,Implementation,Implementation,face_alignment,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (6, 8), (8, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.564102564,211,0.868312757,61,0.983870968,1,results,Implementation,face_alignment18
1438,1438,1438,2,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild,title,title,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004854369,1,0,1,research-problem,title,face_alignment2
1439,1439,1439,4,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .",abstract,abstract,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.014563107,1,0.125,1,research-problem,abstract,face_alignment2
1440,1440,1440,22,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (11, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.526315789,21,0.101941748,10,0.526315789,1,model,Introduction,face_alignment2
1441,1441,1441,23,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 6), (9, 14), (15, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.578947368,22,0.106796117,11,0.578947368,1,model,Introduction,face_alignment2
1442,1442,1442,25,"It illustrates how these attention maps are refined through the successive stages , and how the different prediction tasks can benefit from each other .",Introduction,Introduction,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 6), (7, 8), (8, 9), (10, 12), (16, 19), (20, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.684210526,24,0.116504854,13,0.684210526,1,model,Introduction,face_alignment2
1443,1443,1443,130,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .",Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (8, 9), (9, 13), (16, 21), (21, 22), (22, 34), (34, 35), (36, 38), (43, 44), (45, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,129,0.626213592,1,0.166666667,1,hyperparameters,Implementation details,face_alignment2
1444,1444,1444,131,The input images are resized to 128 128 grayscale images prior to being processed by the network .,Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 10), (13, 15), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,130,0.631067961,2,0.333333333,1,hyperparameters,Implementation details,face_alignment2
1445,1445,1445,132,Each convolution is followed by a batch normalization layer with ReLU activation .,Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,131,0.63592233,3,0.5,1,hyperparameters,Implementation details,face_alignment2
1446,1446,1446,133,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 7), (8, 11), (11, 13), (14, 17), (17, 19), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,132,0.640776699,4,0.666666667,1,hyperparameters,Implementation details,face_alignment2
1447,1447,1447,134,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 8), (8, 9), (10, 15), (15, 16), (16, 18), (19, 21), (19, 22), (21, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.833333333,133,0.645631068,5,0.833333333,1,hyperparameters,Implementation details,face_alignment2
1448,1448,1448,135,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .",Implementation details,Implementation details,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 7), (5, 8), (7, 8), (8, 9), (9, 11), (12, 13), (13, 15), (15, 16), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,134,0.650485437,6,1,1,hyperparameters,Implementation details,face_alignment2
1449,1449,1449,153,"The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (6, 7), (7, 9), (11, 12), (12, 13), (14, 15), (15, 16), (16, 19), (42, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.073170732,152,0.737864078,3,0.090909091,1,ablation-analysis,Ablation study,face_alignment2
1450,1450,1450,156,"Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7), (7, 9), (10, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.146341463,155,0.752427184,6,0.181818182,1,ablation-analysis,Ablation study,face_alignment2
1451,1451,1451,160,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 18), (18, 20), (21, 22), (22, 23), (23, 25), (25, 27), (27, 31), (32, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.243902439,159,0.77184466,10,0.303030303,1,ablation-analysis,Ablation study,face_alignment2
1452,1452,1452,161,F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 16), (16, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.268292683,160,0.776699029,11,0.333333333,1,ablation-analysis,Ablation study,face_alignment2
1453,1453,1453,162,"Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 7), (9, 10), (10, 14), (14, 15), (16, 17), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.292682927,161,0.781553398,12,0.363636364,1,ablation-analysis,Ablation study,face_alignment2
1454,1454,1454,163,"Furthermore , chaining the transfer layers is better than using independant transfer layers : likewise , in such a case , the first transfer layer benefits from the gradients from the subsequents layer at train time .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 10), (10, 13), (22, 25), (25, 27), (28, 29), (29, 30), (31, 33), (33, 34), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.317073171,162,0.786407767,13,0.393939394,1,ablation-analysis,Ablation study,face_alignment2
1455,1455,1455,176,"Finally , shows a comparison of our method and state - of - the - art approaches on Celeb A .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.634146341,175,0.849514563,26,0.787878788,1,ablation-analysis,Ablation study,face_alignment2
1456,1456,1456,181,"Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .",Ablation study,Ablation study,face_alignment,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 13), (13, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.756097561,180,0.873786408,31,0.939393939,1,ablation-analysis,Ablation study,face_alignment2
1457,1457,1457,2,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,title,title,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00295858,1,0,1,research-problem,title,face_alignment3
1458,1458,1458,13,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.,abstract,,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1,12,0.035502959,10,1,1,code,abstract,face_alignment3
1459,1459,1459,15,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037,14,0.041420118,1,0.037037037,1,research-problem,Introduction,face_alignment3
1460,1460,1460,30,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (8, 9), (10, 13), (18, 20), (20, 22), (25, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.592592593,29,0.085798817,16,0.592592593,1,model,Introduction,face_alignment3
1461,1461,1461,31,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 10), (10, 19), (19, 21), (21, 26), (28, 29), (33, 35), (42, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.62962963,30,0.088757396,17,0.62962963,1,model,Introduction,face_alignment3
1462,1462,1462,32,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(14, 16), (16, 18), (19, 22), (24, 25), (27, 28), (28, 30), (31, 34), (34, 35), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.666666667,31,0.091715976,18,0.666666667,1,model,Introduction,face_alignment3
1463,1463,1463,34,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .",Introduction,Introduction,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (7, 8), (9, 10), (10, 11), (11, 13), (13, 15), (16, 19), (19, 20), (21, 24), (26, 29), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.740740741,33,0.097633136,20,0.740740741,1,model,Introduction,face_alignment3
1464,1464,1464,231,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 8), (8, 9), (10, 13), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.333333333,230,0.680473373,6,0.333333333,1,hyperparameters,Implementation details,face_alignment3
1465,1465,1465,232,We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 6), (6, 7), (13, 15), (15, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.388888889,231,0.683431953,7,0.388888889,1,hyperparameters,Implementation details,face_alignment3
1466,1466,1466,233,"We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (8, 10), (11, 13), (13, 21), (21, 22), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.444444444,232,0.686390533,8,0.444444444,1,hyperparameters,Implementation details,face_alignment3
1467,1467,1467,234,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 7), (11, 12), (17, 18), (24, 25), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5,233,0.689349112,9,0.5,1,hyperparameters,Implementation details,face_alignment3
1468,1468,1468,235,"Random Gaussian blur , noise and occlusion are also used .",Implementation details,Implementation details,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.555555556,234,0.692307692,10,0.555555556,1,hyperparameters,Implementation details,face_alignment3
1469,1469,1469,244,Evaluation on 300W,,,face_alignment,3,"['O', 'O', 'O']","[(0, 2), (2, 3)]","['O', 'O', 'O']",0,0,243,0.718934911,0,0,1,results,,face_alignment3
1470,1470,1470,245,"Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .",Evaluation on 300W,Evaluation on 300W,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (7, 15), (15, 16), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,244,0.721893491,1,0.2,1,results,Evaluation on 300W,face_alignment3
1471,1471,1471,246,"For the challenge subset ( iBug dataset ) , we are able to outperform",Evaluation on 300W,Evaluation on 300W,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 8), (11, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,245,0.724852071,2,0.4,1,results,Evaluation on 300W,face_alignment3
1472,1472,1472,247,"Wing by a significant margin , which also proves the robustness of our approach against occlusion and large pose variation .",Evaluation on 300W,Evaluation on 300W,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (8, 9), (10, 11), (11, 12), (12, 14), (14, 15), (15, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,246,0.727810651,3,0.6,1,results,Evaluation on 300W,face_alignment3
1473,1473,1473,248,"Furthermore , on the 300 W private test dataset ) , we again outperform the previous state - of - theart on variant metrics including NME , AUC and FR measured with either 8 % NME and 10 % NME .",Evaluation on 300W,Evaluation on 300W,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9), (13, 14), (15, 21), (21, 22), (22, 24), (24, 25), (25, 26), (30, 32), (33, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,247,0.730769231,4,0.8,1,results,Evaluation on 300W,face_alignment3
1474,1474,1474,250,Evaluation on WFLW,,,face_alignment,3,"['O', 'O', 'O']","[(2, 3)]","['O', 'O', 'O']",0,0,249,0.736686391,0,0,1,results,,face_alignment3
1475,1475,1475,251,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .",Evaluation on WFLW,Evaluation on WFLW,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (5, 7), (7, 8), (9, 11), (15, 18), (18, 19), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,250,0.73964497,1,0.055555556,1,results,Evaluation on WFLW,face_alignment3
1476,1476,1476,252,On every subset we outperform the previous state - of - the - art ap - :,Evaluation on WFLW,Evaluation on WFLW,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 5), (6, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,251,0.74260355,2,0.111111111,1,results,Evaluation on WFLW,face_alignment3
1477,1477,1477,256,"All in all , our approach fails on only 2.84 % of all images , more than a two times improvement compared with 7.6 .",Evaluation on WFLW,Evaluation on WFLW,face_alignment,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 8), (9, 11), (11, 12), (12, 14), (18, 21), (21, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,255,0.75443787,6,0.333333333,1,results,Evaluation on WFLW,face_alignment3
1478,1478,1478,2,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network,title,title,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004739336,1,0,1,research-problem,title,face_alignment4
1479,1479,1479,31,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.642857143,30,0.142180095,18,0.642857143,1,model,Introduction,face_alignment4
1480,1480,1480,32,"By means of the powerful representation of Convolutional Neural Network ( CNN ) , we only train one regressor to learn the descent directions in coarse and fine stages together .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 6), (6, 7), (7, 13), (16, 17), (17, 19), (19, 21), (22, 24), (24, 25), (25, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.678571429,31,0.146919431,19,0.678571429,1,model,Introduction,face_alignment4
1481,1481,1481,36,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 7), (9, 10), (11, 18), (20, 22), (23, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.821428571,35,0.165876777,23,0.821428571,1,model,Introduction,face_alignment4
1482,1482,1482,37,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .",Introduction,Introduction,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 8), (10, 11), (12, 14), (16, 18), (19, 20), (20, 21), (22, 25), (27, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.857142857,36,0.170616114,24,0.857142857,1,model,Introduction,face_alignment4
1483,1483,1483,167,"We perform the experiments based on a machine with Core i7 - 5930 k CPU , 32 GB memory and GTX 1080 GPU with 8G video memory .",Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 8), (8, 9), (9, 15), (16, 19), (20, 23), (23, 24), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.235294118,166,0.786729858,12,0.461538462,1,experimental-setup,Experiments: Metrics .,face_alignment4
1484,1484,1484,168,The detected faces are resized into 256 256 and the location patch size is 57 57 .,Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 8), (10, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.254901961,167,0.791469194,13,0.5,1,experimental-setup,Experiments: Metrics .,face_alignment4
1485,1485,1485,169,"For CNN structure , the Rectified Linear Unit ( ReLU ) is adopted as the activation function , and the optimizer is the Adadelta ( Zeiler 2012 ) approach , learning rate is set to 0.1 and weight decay is set to 1 e ?",Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 11), (12, 14), (15, 17), (20, 21), (21, 22), (23, 29), (30, 32), (33, 35), (35, 36), (37, 39), (40, 42), (42, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.274509804,168,0.796208531,14,0.538461538,1,experimental-setup,Experiments: Metrics .,face_alignment4
1486,1486,1486,170,4 . Training the CNN requires around 2 days .,Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.294117647,169,0.800947867,15,0.576923077,1,experimental-setup,Experiments: Metrics .,face_alignment4
1487,1487,1487,175,The NME results shows that SIR performs comparatively with RAR ) and outperform other existing methods .,Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 6), (6, 7), (7, 9), (9, 10), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.392156863,174,0.82464455,20,0.769230769,1,results,Experiments: Metrics .,face_alignment4
1488,1488,1488,177,"In the more challenging IBUG subset , our method achieves robust performance in large pose , expression and illumination environment .",Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (7, 9), (9, 10), (10, 12), (12, 13), (13, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.431372549,176,0.834123223,22,0.846153846,1,results,Experiments: Metrics .,face_alignment4
1489,1489,1489,179,"As shown in , the SIR method outperform the state - of - the - art methods according to the CED curve .",Experiments,Metrics .,face_alignment,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (9, 17), (17, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.470588235,178,0.843601896,24,0.923076923,1,results,Experiments: Metrics .,face_alignment4
1490,1490,1490,2,Look at Boundary : A Boundary - Aware Face Alignment Algorithm,title,title,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003508772,1,0,1,research-problem,title,face_alignment5
1491,1491,1491,14,Dataset and model will be publicly available at https://wywu.github.io/projects/LAB/LAB.html,abstract,,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1,13,0.045614035,11,1,1,code,abstract,face_alignment5
1492,1492,1492,16,"Face alignment , which refers to facial landmark detection in this work , serves as a key step for many face applications , e.g. , face recognition , face verification and face frontalisation .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.027777778,15,0.052631579,1,0.027777778,1,research-problem,Introduction,face_alignment5
1493,1493,1493,28,"To this end , we use well - defined facial boundaries to represent the geometric structure of the human face .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 11), (11, 13), (14, 16), (16, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.361111111,27,0.094736842,13,0.361111111,1,approach,Introduction,face_alignment5
1494,1494,1494,30,"In this work , we represent facial structure using 13 boundary lines .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 8), (8, 9), (9, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.416666667,29,0.101754386,15,0.416666667,1,approach,Introduction,face_alignment5
1495,1495,1495,32,Our boundary - aware face alignment algorithm contains two stages .,Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (7, 8), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.472222222,31,0.10877193,17,0.472222222,1,approach,Introduction,face_alignment5
1496,1496,1496,33,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (8, 9), (9, 10), (10, 11), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5,32,0.112280702,18,0.5,1,approach,Introduction,face_alignment5
1497,1497,1497,35,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12), (12, 15), (15, 17), (18, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.555555556,34,0.119298246,20,0.555555556,1,approach,Introduction,face_alignment5
1498,1498,1498,37,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .",Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (9, 12), (14, 16), (16, 17), (18, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.611111111,36,0.126315789,22,0.611111111,1,approach,Introduction,face_alignment5
1499,1499,1499,38,We used stacked hourglass structure to estimate facial boundary heatmap and model the structure between facial boundaries through message passing to increase its robustness to occlusion .,Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 7), (7, 10), (11, 12), (13, 14), (14, 15), (15, 17), (17, 18), (18, 20), (20, 22), (23, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.638888889,37,0.129824561,23,0.638888889,1,approach,Introduction,face_alignment5
1500,1500,1500,40,The boundary heatmaps serve as structure cue to guide feature learning for the landmark regressor .,Introduction,Introduction,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 7), (7, 9), (9, 11), (11, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.694444444,39,0.136842105,25,0.694444444,1,approach,Introduction,face_alignment5
1501,1501,1501,224,All our models are trained with Caffe [ 24 ] on 4 Titan X GPUs .,Evaluation metric .,Evaluation metric .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (10, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.470588235,223,0.78245614,27,0.931034483,1,experimental-setup,Evaluation metric .,face_alignment5
1502,1502,1502,227,Comparison with existing approaches 4.1.1 Evaluation on 300W,Evaluation metric .,,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.647058824,226,0.792982456,0,0,1,results,Evaluation metric .,face_alignment5
1503,1503,1503,231,Our method performs best among all of the state - of - the - art methods .,Evaluation metric .,Comparison with existing approaches 4.1.1 Evaluation on 300W,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 4), (4, 5), (5, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.882352941,230,0.807017544,4,0.666666667,1,results,Evaluation metric .: Comparison with existing approaches 4.1.1 Evaluation on 300W,face_alignment5
1504,1504,1504,234,Evaluation on WFLW,,,face_alignment,5,"['O', 'O', 'O']","[(2, 3)]","['O', 'O', 'O']",0,0,233,0.81754386,0,0,1,results,,face_alignment5
1505,1505,1505,237,"Though reasonable performance is obtained , there is illustrated to be still a lot of room for improvement for the extreme diversity of samples on WFLW , e.g. , large pose , exaggerated expressions and heavy occlusion .",Evaluation on WFLW,Evaluation on WFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 18), (18, 19), (20, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.176470588,236,0.828070175,3,0.176470588,1,results,Evaluation on WFLW,face_alignment5
1506,1506,1506,238,Cross - dataset evaluation on COFW and AFLW,Evaluation on WFLW,,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.235294118,237,0.831578947,4,0.235294118,1,results,Evaluation on WFLW,face_alignment5
1507,1507,1507,241,Our model outperforms previous results with a large margin .,Evaluation on WFLW,Cross - dataset evaluation on COFW and AFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 5), (5, 6), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.411764706,240,0.842105263,7,0.411764706,1,results,Evaluation on WFLW: Cross - dataset evaluation on COFW and AFLW,face_alignment5
1508,1508,1508,242,We achieve 4.62 % mean error with 2.17 % failure rate .,Evaluation on WFLW,Cross - dataset evaluation on COFW and AFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (7, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.470588235,241,0.845614035,8,0.470588235,1,results,Evaluation on WFLW: Cross - dataset evaluation on COFW and AFLW,face_alignment5
1509,1509,1509,243,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .",Evaluation on WFLW,Cross - dataset evaluation on COFW and AFLW,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.529411765,242,0.849122807,9,0.529411765,1,results,Evaluation on WFLW: Cross - dataset evaluation on COFW and AFLW,face_alignment5
1510,1510,1510,249,"Moreover , our method uses boundary information achieves 29 % , 32 % and 29 % relative performance improve- ment over the baseline method ( "" LAB without boundary "" ) on COFW - 29 , AFLW - Full and AFLW - Frontal respectively .",Evaluation on WFLW,The results are reported in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 7), (7, 8), (8, 20), (20, 21), (22, 24), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.882352941,248,0.870175439,15,0.882352941,1,results,Evaluation on WFLW: The results are reported in .,face_alignment5
1511,1511,1511,260,"It can be observed easily that boundary map ( "" BM "" ) is the most effective one .",Ablation study,The overall results are shown in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (6, 13), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.615384615,259,0.90877193,8,0.615384615,1,ablation-analysis,Ablation study: The overall results are shown in .,face_alignment5
1512,1512,1512,261,Boundary information fusion is one of the key steps in our algorithm .,Ablation study,The overall results are shown in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.692307692,260,0.912280702,9,0.692307692,1,ablation-analysis,Ablation study: The overall results are shown in .,face_alignment5
1513,1513,1513,263,"As indicated in , our final model that fuses boundary information in all four levels improves mean error from 7.12 % to 6.13 % .",Ablation study,The overall results are shown in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 12), (12, 15), (15, 16), (16, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.846153846,262,0.919298246,11,0.846153846,1,ablation-analysis,Ablation study: The overall results are shown in .,face_alignment5
1514,1514,1514,265,It can be observed that performance is improved consistently by fusing boundary heatmaps at more levels .,Ablation study,The overall results are shown in .,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (7, 9), (9, 11), (11, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1,264,0.926315789,13,1,1,ablation-analysis,Ablation study: The overall results are shown in .,face_alignment5
1515,1515,1515,268,"The comparison between "" BL + HG "" and "" BL + HG/ B "" indicates the effectiveness of boundary information fusion rather than network structure changes .",Method,Method,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (15, 16), (17, 18), (18, 19), (19, 22), (22, 24), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.105263158,267,0.936842105,2,0.166666667,1,results,Method,face_alignment5
1516,1516,1516,269,"The comparison between "" BL + HG "" and "" BL + CL "" indicates the effectiveness of the using hourglass structure design .",Method,Method,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (14, 15), (16, 17), (17, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.157894737,268,0.940350877,3,0.25,1,results,Method,face_alignment5
1517,1517,1517,270,Message passing plays a vital role for heatmap quality improvement when severe occlusions happen .,Method,Method,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.210526316,269,0.943859649,4,0.333333333,1,research-problem,Method,face_alignment5
1518,1518,1518,272,Adversarial learning further improves the quality and effectiveness of boundary heatmaps .,Method,Method,face_alignment,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (5, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.315789474,271,0.950877193,6,0.5,1,results,Method,face_alignment5
1519,1519,1519,2,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees,title,title,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.002849003,1,0,1,research-problem,title,face_alignment6
1520,1520,1520,23,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 16), (18, 24), (24, 26), (27, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.434782609,22,0.062678063,10,0.434782609,1,model,Introduction,face_alignment6
1521,1521,1521,25,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 6), (7, 11), (11, 12), (13, 15), (15, 17), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.52173913,24,0.068376068,12,0.52173913,1,model,Introduction,face_alignment6
1522,1522,1522,27,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 9), (10, 13), (13, 14), (15, 16), (17, 18), (19, 20), (20, 21), (21, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.608695652,26,0.074074074,14,0.608695652,1,model,Introduction,face_alignment6
1523,1523,1523,28,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .",Introduction,Introduction,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 9), (9, 10), (11, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.652173913,27,0.076923077,15,0.652173913,1,model,Introduction,face_alignment6
1524,1524,1524,31,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,Introduction,A preliminary version of our work appeared in .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 7), (8, 12), (13, 14), (15, 16), (16, 17), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608696,30,0.085470085,18,0.782608696,1,model,Introduction: A preliminary version of our work appeared in .,face_alignment6
1525,1525,1525,32,We have also introduced early stopping and better data augmentation techniques for increasing the regularization when training both the ERT and the CNN .,Introduction,A preliminary version of our work appeared in .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (4, 11), (11, 13), (14, 15), (15, 17), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.826086957,31,0.088319088,19,0.826086957,1,model,Introduction: A preliminary version of our work appeared in .,face_alignment6
1526,1526,1526,210,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .",Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.217391304,209,0.595441595,5,0.217391304,1,experimental-setup,Implementation,face_alignment6
1527,1527,1527,211,We train until convergence with an initial learning rate ? = 0.001 .,Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (3, 4), (4, 5), (6, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.260869565,210,0.598290598,6,0.260869565,1,experimental-setup,Implementation,face_alignment6
1528,1528,1528,212,"When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .",Implementation,Implementation,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 6), (6, 8), (10, 11), (12, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.304347826,211,0.601139601,7,0.304347826,1,experimental-setup,Implementation,face_alignment6
1529,1529,1529,214,We apply batch normalization after each convolution .,Implementation,,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.391304348,213,0.606837607,9,0.391304348,1,experimental-setup,Implementation,face_alignment6
1530,1530,1530,215,All layers contain 68 filters to describe the required landmark features .,Implementation,We apply batch normalization after each convolution .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 5), (5, 7), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.434782609,214,0.60968661,10,0.434782609,1,experimental-setup,Implementation: We apply batch normalization after each convolution .,face_alignment6
1531,1531,1531,216,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .",Implementation,We apply batch normalization after each convolution .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 9), (9, 10), (11, 14), (14, 16), (17, 18), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.47826087,215,0.612535613,11,0.47826087,1,experimental-setup,Implementation: We apply batch normalization after each convolution .,face_alignment6
1532,1532,1532,217,We train the coarse - to - fine ERT with the Gradient Boosting algorithm ) .,Implementation,We apply batch normalization after each convolution .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (9, 10), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.52173913,216,0.615384615,12,0.52173913,1,experimental-setup,Implementation: We apply batch normalization after each convolution .,face_alignment6
1533,1533,1533,218,It requires a maximum of T = 20 stages of K = 50 regression trees per stage .,Implementation,We apply batch normalization after each convolution .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (9, 10), (10, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.565217391,217,0.618233618,13,0.565217391,1,experimental-setup,Implementation: We apply batch normalization after each convolution .,face_alignment6
1534,1534,1534,219,The depth of trees is set to 4 .,Implementation,,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 7), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.608695652,218,0.621082621,14,0.608695652,1,experimental-setup,Implementation,face_alignment6
1535,1535,1535,220,"The number of tests to choose the best split parameters , ? , is set to 200 .",Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 6), (7, 10), (14, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.652173913,219,0.623931624,15,0.652173913,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1536,1536,1536,221,We resize each image to set the face size to 160160 pixels .,Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 6), (7, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.695652174,220,0.626780627,16,0.695652174,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1537,1537,1537,223,We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (8, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608696,222,0.632478632,18,0.782608696,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1538,1538,1538,224,"We augment the shapes of each face training image to create a set , SA , of at least N A = 60000 samples to train the cascade .",Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 9), (9, 11), (12, 13), (14, 15), (16, 17), (17, 24), (24, 26), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.826086957,223,0.635327635,19,0.826086957,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1539,1539,1539,225,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 5), (12, 17), (17, 18), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.869565217,224,0.638176638,20,0.869565217,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1540,1540,1540,227,"Training the CNN and the coarse - to - fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20 GHz ( 210 cores / 20 threads , 128 GB of RAM ) with a batch size of 32 images .",Implementation,The depth of trees is set to 4 .,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 13), (13, 14), (14, 16), (16, 17), (18, 28), (30, 36), (36, 37), (37, 39), (51, 52), (53, 55), (55, 56), (56, 58)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.956521739,226,0.643874644,22,0.956521739,1,experimental-setup,Implementation: The depth of trees is set to 4 .,face_alignment6
1541,1541,1541,237,"Overall , 3 DDE is better than any other providing a public implementation in the literature .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 7), (7, 9), (9, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.615384615,236,0.672364672,8,0.615384615,1,results,Experiments using public code,face_alignment6
1542,1542,1542,239,"In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 8), (9, 11), (11, 14), (14, 15), (15, 16), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.769230769,238,0.678062678,10,0.769230769,1,results,Experiments using public code,face_alignment6
1543,1543,1543,240,"We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.846153846,239,0.680911681,11,0.846153846,1,results,Experiments using public code,face_alignment6
1544,1544,1544,241,"Even DAN and LAB , that implement a cascade of CNN regressors , can not compete with the regularization obtained by using the cascade of ERT in 3 DDE ( see ) .",Experiments using public code,Experiments using public code,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (6, 7), (8, 12), (18, 19), (19, 22), (23, 26), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.923076923,240,0.683760684,12,0.923076923,1,results,Experiments using public code,face_alignment6
1545,1545,1545,247,Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition ( see ) and in the full subset of the 300W public test set ( see ) .,Experiments using published results,Experiments using published results,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 7), (7, 8), (9, 13), (13, 14), (15, 17), (21, 22), (23, 25), (25, 26), (27, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.097560976,246,0.700854701,4,0.097560976,1,results,Experiments using published results,face_alignment6
1546,1546,1546,249,"In the challenging subset of the 300W public competition , SHN gets better results than 3DDE .",Experiments using published results,Experiments using published results,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (6, 9), (10, 11), (11, 12), (12, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.146341463,248,0.706552707,6,0.146341463,1,results,Experiments using published results,face_alignment6
1547,1547,1547,296,"When combined with the cascaded ERT , the 3D initialization is key to achieve top overall performance , see CNN + MS + DE vs CNN + 3D + DE in the full subset .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (8, 10), (11, 12), (12, 14), (14, 17), (18, 19), (19, 24), (25, 30), (30, 31), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.523809524,295,0.84045584,11,0.523809524,1,ablation-analysis,Ablation study,face_alignment6
1548,1548,1548,299,"Of course , the 3D initialization is fundamental to achieve good performance in presence of large face rotations .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (10, 12), (12, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.666666667,298,0.849002849,14,0.666666667,1,ablation-analysis,Ablation study,face_alignment6
1549,1549,1549,302,"The large receptive fields of CNNs are specially helpful in challenging situations , specifically those in the pose and occlusion subsets .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (7, 9), (9, 10), (10, 12), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.80952381,301,0.857549858,17,0.80952381,1,ablation-analysis,Ablation study,face_alignment6
1550,1550,1550,303,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .",Ablation study,Ablation study,face_alignment,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (7, 8), (11, 12), (12, 15), (15, 16), (16, 18), (19, 20), (20, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.857142857,302,0.86039886,18,0.857142857,1,ablation-analysis,Ablation study,face_alignment6
1551,1551,1551,2,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,title,title,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004672897,1,0,1,research-problem,title,face_alignment7
1552,1552,1552,10,Code is available at https://github.com/YadiraF/PRNet.,abstract,abstract,face_alignment,7,"['O', 'O', 'O', 'O', 'O']","[(4, 5)]","['O', 'O', 'O', 'O', 'O']",7,1,9,0.042056075,7,1,1,code,abstract,face_alignment7
1553,1553,1553,12,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .,Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.034482759,11,0.051401869,1,0.034482759,1,research-problem,Introduction,face_alignment7
1554,1554,1554,27,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13), (13, 14), (14, 21), (21, 24), (24, 26), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.551724138,26,0.121495327,16,0.551724138,1,model,Introduction,face_alignment7
1555,1555,1555,31,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .",Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (10, 11), (12, 14), (14, 15), (16, 18), (18, 19), (20, 24), (30, 31), (32, 34), (34, 35), (35, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.689655172,30,0.140186916,20,0.689655172,1,model,Introduction,face_alignment7
1556,1556,1556,32,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,Introduction,Introduction,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9), (9, 10), (11, 13), (14, 15), (14, 17), (15, 17), (17, 19), (19, 21), (22, 25), (25, 26), (27, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.724137931,31,0.144859813,21,0.724137931,1,model,Introduction,face_alignment7
1557,1557,1557,146,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .",Training Details,Training Details,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 7), (7, 8), (9, 11), (11, 13), (13, 14), (15, 16), (16, 17), (17, 18), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.846153846,145,0.677570093,11,0.846153846,1,experimental-setup,Training Details,face_alignment7
1558,1558,1558,147,The batch size is set as 16 .,Training Details,Training Details,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.923076923,146,0.682242991,12,0.923076923,1,experimental-setup,Training Details,face_alignment7
1559,1559,1559,148,All of our training codes are implemented with TensorFlow .,Training Details,Training Details,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1,147,0.686915888,13,1,1,experimental-setup,Training Details,face_alignment7
1560,1560,1560,168,3D Face Alignment,Experimental Results,Test Dataset,face_alignment,7,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",19,0.358490566,167,0.780373832,0,0,1,results,Experimental Results: Test Dataset,face_alignment7
1561,1561,1561,172,"As shown in figure 5 , our result slightly outperforms the state - of - the - art method 3D - FAN when calculating per distance with 2D coordinates .",Experimental Results,Test Dataset,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 10), (11, 22), (22, 24), (24, 26), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.433962264,171,0.799065421,4,0.166666667,1,results,Experimental Results: Test Dataset,face_alignment7
1562,1562,1562,173,"When considering the depth value , the performance discrepancy between our method and 3D - FAN increases .",Experimental Results,Test Dataset,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (7, 9), (9, 10), (10, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.452830189,172,0.803738318,5,0.208333333,1,results,Experimental Results: Test Dataset,face_alignment7
1563,1563,1563,178,The result shows that our method is robust to the change of pose and datasets .,Experimental Results,Test Dataset,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 6), (7, 9), (10, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.547169811,177,0.827102804,10,0.416666667,1,results,Experimental Results: Test Dataset,face_alignment7
1564,1564,1564,184,Examples from AFLW2000 - 3 D dataset show that our predictions are more accurate than ground truth in some cases .,Experimental Results,Test Dataset,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 7), (7, 8), (9, 11), (11, 12), (12, 14), (14, 15), (15, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.660377358,183,0.855140187,16,0.666666667,1,results,Experimental Results: Test Dataset,face_alignment7
1565,1565,1565,189,"As shown in figure 7 , our method outperforms the best methods with a large margin of more than 27 % on both 2 D and 3D coordinates . : CED curves on AFLW2000 - 3D .",Experimental Results,Test Dataset,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 9), (10, 12), (12, 13), (14, 16), (16, 17), (17, 21), (21, 22), (23, 28), (33, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.754716981,188,0.878504673,21,0.875,1,results,Experimental Results: Test Dataset,face_alignment7
1566,1566,1566,207,Network trained without using weight mask has worst performance compared with other two settings .,Ablation Study,Ablation Study,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (0, 1), (1, 4), (4, 6), (7, 9), (9, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,206,0.962616822,4,0.8,1,ablation-analysis,Ablation Study,face_alignment7
1567,1567,1567,208,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .",Ablation Study,Ablation Study,face_alignment,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6), (6, 8), (8, 11), (12, 15), (16, 18), (19, 20), (20, 22), (22, 23), (23, 26), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,1,207,0.96728972,5,1,1,ablation-analysis,Ablation Study,face_alignment7
1568,1568,1568,2,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning,title,title,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003521127,1,0,1,research-problem,title,face_alignment8
1569,1569,1569,9,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,abstract,Best viewed in color .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.333333333,8,0.028169014,6,0.333333333,1,research-problem,abstract: Best viewed in color .,face_alignment8
1570,1570,1570,23,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,Abstract,Abstract,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,22,0.077464789,1,0.142857143,1,research-problem,Abstract,face_alignment8
1571,1571,1571,31,3 D face reconstruction is an important task in the field of computer vision and graphics .,Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.03125,30,0.105633803,1,0.03125,1,research-problem,Introduction,face_alignment8
1572,1572,1572,41,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(16, 17), (18, 21), (22, 23), (23, 33), (33, 38), (39, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.34375,40,0.14084507,11,0.34375,1,model,Introduction,face_alignment8
1573,1573,1573,45,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (11, 14), (15, 19), (19, 20), (20, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.46875,44,0.154929577,15,0.46875,1,model,Introduction,face_alignment8
1574,1574,1574,47,The model should be able to recover 2D landmarks from predicted 3D ones via direct 3D - to - 2D projection .,Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 9), (9, 10), (10, 13), (13, 14), (14, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.53125,46,0.161971831,17,0.53125,1,model,Introduction,face_alignment8
1575,1575,1575,49,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 10), (10, 11), (12, 15), (18, 19), (20, 23), (33, 35), (45, 47), (47, 48), (49, 51)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.59375,48,0.169014085,19,0.59375,1,model,Introduction,face_alignment8
1576,1576,1576,51,"To facilitate the overall learning procedure , our method also exploits self - critic learning .",Introduction,Introduction,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (10, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.65625,50,0.176056338,21,0.65625,1,model,Introduction,face_alignment8
1577,1577,1577,205,Our proposed 2 DASL is implemented with Pytorch .,Training details and datasets,,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.018867925,204,0.718309859,1,0.045454545,1,experimental-setup,Training details and datasets,face_alignment8
1578,1578,1578,206,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .",Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (8, 9), (10, 12), (12, 14), (14, 17), (18, 19), (19, 20), (22, 23), (23, 24), (25, 28), (28, 29), (30, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.037735849,205,0.721830986,2,0.090909091,1,experimental-setup,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1579,1579,1579,209,We use a two - stage strategy to train our model .,Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.094339623,208,0.732394366,5,0.227272727,1,experimental-setup,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1580,1580,1580,210,"In the first stage , we train the model using the overall loss L.",Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (6, 7), (8, 9), (9, 10), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.113207547,209,0.735915493,6,0.272727273,1,experimental-setup,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1581,1581,1581,211,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .",Training details and datasets,Our proposed 2 DASL is implemented with Pytorch .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 11), (11, 12), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.132075472,210,0.73943662,7,0.318181818,1,experimental-setup,Training details and datasets: Our proposed 2 DASL is implemented with Pytorch .,face_alignment8
1582,1582,1582,227,Dense face alignment,Training details and datasets,,face_alignment,8,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",23,0.433962264,226,0.795774648,0,0,1,experiments,Training details and datasets,face_alignment8
1583,1583,1583,239,"The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .",Training details and datasets,Dense face alignment,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 13), (13, 14), (15, 20), (20, 21), (25, 30), (30, 31), (31, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.660377358,238,0.838028169,12,0.4,1,experiments,Training details and datasets: Dense face alignment,face_alignment8
1584,1584,1584,240,"For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .",Training details and datasets,Dense face alignment,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 6), (7, 11), (12, 14), (14, 15), (16, 17), (18, 20), (20, 21), (23, 26), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.679245283,239,0.841549296,13,0.433333333,1,experiments,Training details and datasets: Dense face alignment,face_alignment8
1585,1585,1585,245,"As can be observed , our method achieves the lowest mean NME on both of the two datasets , and the lowest NME across all poses on AFLW2000 - 3D .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (9, 12), (12, 13), (13, 18), (21, 23), (23, 24), (24, 26), (26, 27), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.773584906,244,0.85915493,18,0.6,1,results,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1586,1586,1586,246,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (4, 5), (5, 6), (6, 7), (8, 9), (9, 10), (10, 11), (11, 14), (14, 15), (15, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.79245283,245,0.862676056,19,0.633333333,1,results,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1587,1587,1587,247,"Es - pecially on large poses ( from 60 to 90 ) , 2 DASL achieves 0.2 lower NME than PRNet .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (13, 15), (15, 16), (16, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.811320755,246,0.866197183,20,0.666666667,1,results,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1588,1588,1588,249,3 D face reconstruction,Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",45,0.849056604,248,0.873239437,22,0.733333333,1,experiments,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1589,1589,1589,255,"As can be seen , the 3D reconstruction results of 2 DASL outperforms 3 DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .",Training details and datasets,The comparison results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 10), (10, 12), (12, 13), (13, 15), (15, 16), (16, 17), (20, 21), (21, 22), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.962264151,254,0.894366197,28,0.933333333,1,results,Training details and datasets: The comparison results are shown in Tab .,face_alignment8
1590,1590,1590,265,"2 . Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages , respectively .",Ablation study,The ablation study results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 7), (7, 8), (9, 11), (11, 12), (13, 14), (14, 15), (15, 16), (17, 18), (18, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,264,0.929577465,7,0.35,1,ablation-analysis,Ablation study: The ablation study results are shown in Tab .,face_alignment8
1591,1591,1591,267,"If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .",Ablation study,The ablation study results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 7), (7, 9), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,266,0.936619718,9,0.45,1,ablation-analysis,Ablation study: The ablation study results are shown in Tab .,face_alignment8
1592,1592,1592,268,"While the self - supervision scheme reduce NME by 0.1 when the weight mask is used , and 0.23 if the weight mask is removed , no significant improvement is observed .",Ablation study,The ablation study results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (12, 14), (15, 16), (18, 19), (19, 20), (21, 23), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5,267,0.940140845,10,0.5,1,ablation-analysis,Ablation study: The ablation study results are shown in Tab .,face_alignment8
1593,1593,1593,270,"Moreover , in our experiments , we found taking the FLMs as input can accelerate the convergence of training process .",Ablation study,The ablation study results are shown in Tab .,face_alignment,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 9), (10, 11), (11, 12), (12, 13), (14, 15), (16, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,269,0.947183099,12,0.6,1,ablation-analysis,Ablation study: The ablation study results are shown in Tab .,face_alignment8
1594,1594,1594,2,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection,title,title,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003389831,1,0,1,research-problem,title,face_alignment9
1595,1595,1595,30,"In this paper , we propose a novel Semantic Alignment method which reduces the ' semantic ambiguity ' intrinsi-cally .",Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (12, 13), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.666666667,29,0.098305085,13,0.619047619,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1596,1596,1596,32,"We model the ' real ' ground - truth as a latent variable to optimize , and the optimized ' real ' ground - truth then supervises the landmark detection network training .",Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (9, 10), (11, 13), (13, 14), (14, 15), (18, 25), (26, 27), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.75,31,0.105084746,15,0.714285714,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1597,1597,1597,33,"Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .",Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (9, 11), (12, 18), (19, 20), (21, 24), (24, 25), (26, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.791666667,32,0.108474576,16,0.761904762,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1598,1598,1598,34,"In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .",Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (9, 11), (12, 14), (14, 16), (16, 18), (19, 20), (20, 21), (22, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.833333333,33,0.111864407,17,0.80952381,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1599,1599,1599,35,The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .,Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 10), (10, 11), (12, 17), (17, 18), (18, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.875,34,0.115254237,18,0.857142857,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1600,1600,1600,36,The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .,Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 7), (7, 8), (9, 10), (10, 11), (11, 13), (15, 17), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.916666667,35,0.118644068,19,0.904761905,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1601,1601,1601,37,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .",Introduction,The others move to the semantically accurate positions .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 18), (19, 20), (21, 25), (26, 27), (28, 31), (31, 33), (33, 35), (35, 37), (37, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.958333333,36,0.122033898,20,0.952380952,1,model,Introduction: The others move to the semantically accurate positions .,face_alignment9
1602,1602,1602,201,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (6, 8), (9, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.142857143,200,0.677966102,18,0.6,1,experimental-setup,Implementation Details .,face_alignment9
1603,1603,1603,202,We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (9, 10), (10, 12), (14, 16), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.214285714,201,0.681355932,19,0.633333333,1,experimental-setup,Implementation Details .,face_alignment9
1604,1604,1604,207,"When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (1, 2), (3, 6), (6, 7), (7, 9), (11, 14), (14, 15), (15, 18), (20, 22), (22, 26), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.571428571,206,0.698305085,24,0.8,1,experimental-setup,Implementation Details .,face_alignment9
1605,1605,1605,208,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .",Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 5), (5, 6), (7, 8), (8, 9), (16, 19), (19, 20), (20, 24), (26, 28), (28, 33), (33, 34), (34, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.642857143,207,0.701694915,25,0.833333333,1,experimental-setup,Implementation Details .,face_alignment9
1606,1606,1606,211,We set batch size to 10 for network training .,Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 6), (6, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.857142857,210,0.711864407,28,0.933333333,1,experimental-setup,Implementation Details .,face_alignment9
1607,1607,1607,213,1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .,Implementation Details .,Implementation Details .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 9), (12, 13), (13, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,1,212,0.718644068,30,1,1,experimental-setup,Implementation Details .,face_alignment9
1608,1608,1608,215,300 W .,Comparison experiment,Comparison experiment,face_alignment,9,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",1,0.034482759,214,0.725423729,1,0.034482759,1,baselines,Comparison experiment,face_alignment9
1609,1609,1609,220,"2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 8), (8, 16), (16, 18), (18, 23), (24, 29), (29, 32), (32, 33), (33, 34), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.206896552,219,0.742372881,6,0.206896552,1,results,Comparison experiment: From Tab.,face_alignment9
1610,1610,1610,221,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (6, 8), (8, 13), (13, 15), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.24137931,220,0.745762712,7,0.24137931,1,results,Comparison experiment: From Tab.,face_alignment9
1611,1611,1611,223,"Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 10), (10, 12), (13, 15), (17, 18), (30, 31), (31, 36), (36, 37), (37, 42), (43, 48)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.310344828,222,0.752542373,9,0.310344828,1,results,Comparison experiment: From Tab.,face_alignment9
1612,1612,1612,224,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (8, 10), (11, 16), (17, 27), (28, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.344827586,223,0.755932203,10,0.344827586,1,results,Comparison experiment: From Tab.,face_alignment9
1613,1613,1613,225,AFLW . 300W has 68 facial points which contain many weak semantic landmarks ( e.g. those on face contours ) .,Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 7), (8, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.379310345,224,0.759322034,11,0.379310345,1,results,Comparison experiment: From Tab.,face_alignment9
1614,1614,1614,230,"As shown in Tab. 3 , HGs + SA outperforms",Comparison experiment,LAB used additional boundary information from outside database .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.551724138,229,0.776271186,16,0.551724138,1,results,Comparison experiment: LAB used additional boundary information from outside database .,face_alignment9
1615,1615,1615,231,"HGs , 1.62 % vs 1.95 % .",Comparison experiment,,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.586206897,230,0.779661017,17,0.586206897,1,results,Comparison experiment,face_alignment9
1616,1616,1616,233,It is also observed that HGs + SA + GHCU works better than HGs + SA .,Comparison experiment,"HGs , 1.62 % vs 1.95 % .",face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (3, 4), (5, 10), (10, 11), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.655172414,232,0.786440678,19,0.655172414,1,results,"Comparison experiment: HGs , 1.62 % vs 1.95 % .",face_alignment9
1617,1617,1617,234,300 - VW .,Comparison experiment,"HGs , 1.62 % vs 1.95 % .",face_alignment,9,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",20,0.689655172,233,0.789830508,20,0.689655172,1,baselines,"Comparison experiment: HGs , 1.62 % vs 1.95 % .",face_alignment9
1618,1618,1618,238,"4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 9), (9, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.827586207,237,0.803389831,24,0.827586207,1,results,Comparison experiment: From Tab.,face_alignment9
1619,1619,1619,239,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .",Comparison experiment,From Tab.,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 7), (8, 13), (13, 14), (15, 20), (20, 21), (21, 23), (23, 24), (24, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.862068966,238,0.806779661,25,0.862068966,1,results,Comparison experiment: From Tab.,face_alignment9
1620,1620,1620,287,"To verify the effectiveness of different components in our framework , we conduct this ablation study on 300 - VW .",Ablation study .,Ablation study .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,286,0.969491525,43,0.914893617,1,ablation-analysis,Ablation study .,face_alignment9
1621,1621,1621,290,"9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .",Ablation study .,As shown in Tab .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 7), (8, 9), (9, 10), (10, 13), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,289,0.979661017,46,0.978723404,1,ablation-analysis,Ablation study .: As shown in Tab .,face_alignment9
1622,1622,1622,291,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .",Ablation study .,As shown in Tab .,face_alignment,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 4), (4, 5), (6, 9), (14, 19), (20, 21), (24, 25), (25, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,1,290,0.983050847,47,1,1,ablation-analysis,Ablation study .: As shown in Tab .,face_alignment9
1623,1623,1623,2,Accurate Face Detection for High Performance,title,,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.006756757,1,0,1,research-problem,title,face_detection0
1624,1624,1624,23,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .",Introduction,Introduction,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 14), (14, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,22,0.148648649,13,0.8125,1,model,Introduction,face_detection0
1625,1625,1625,24,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,Introduction,Introduction,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11), (12, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,23,0.155405405,14,0.875,1,model,Introduction,face_detection0
1626,1626,1626,25,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,Introduction,Introduction,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 11), (11, 12), (12, 13), (17, 18), (19, 27), (27, 28), (28, 29), (33, 34), (35, 37), (37, 39), (39, 44), (44, 45), (45, 46), (50, 51), (52, 56), (56, 57), (57, 59), (63, 64), (65, 68), (68, 69), (69, 70)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,24,0.162162162,15,0.9375,1,model,Introduction,face_detection0
1627,1627,1627,134,The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset .,Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 8), (9, 11), (12, 14), (14, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.703703704,133,0.898648649,1,0.111111111,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1628,1628,1628,135,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 10), (11, 12), (12, 13), (14, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.740740741,134,0.905405405,2,0.222222222,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1629,1629,1629,136,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 8), (10, 11), (11, 14), (15, 16), (16, 17), (17, 19), (20, 23), (24, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.777777778,135,0.912162162,3,0.333333333,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1630,1630,1630,137,The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 9), (10, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.814814815,136,0.918918919,4,0.444444444,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1631,1631,1631,138,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 11), (14, 16), (16, 17), (17, 18), (18, 22), (23, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.851851852,137,0.925675676,5,0.555555556,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1632,1632,1632,139,The full training and testing codes are built on the PyTorch library .,Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (7, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.888888889,138,0.932432432,6,0.666666667,1,experimental-setup,Experimental Dataset: Optimization Detail,face_detection0
1633,1633,1633,141,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .",Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 18), (18, 20), (21, 23), (23, 24), (25, 27), (27, 28), (29, 33), (34, 35), (53, 54), (54, 56), (58, 75), (75, 76), (76, 78)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.962962963,140,0.945945946,8,0.888888889,1,results,Experimental Dataset: Optimization Detail,face_detection0
1634,1634,1634,142,These results outperform all the compared state - of - the - art methods and demonstrate the superiority of our AInnoFace detector .,Experimental Dataset,Optimization Detail,face_detection,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 14), (15, 16), (17, 18), (18, 19), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,1,141,0.952702703,9,1,1,results,Experimental Dataset: Optimization Detail,face_detection0
1635,1635,1635,10,"Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications , and many studies are proposed from the beginning of the computer vision research .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.03030303,9,0.038961039,1,0.03030303,1,research-problem,Introduction,face_detection1
1636,1636,1636,11,"After the advent of deep neural networks , many face detection algorithms applying the deep network have reported significant performance improvement to the conventional face detectors .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.060606061,10,0.043290043,2,0.060606061,1,research-problem,Introduction,face_detection1
1637,1637,1637,28,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 12), (12, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.575757576,27,0.116883117,19,0.575757576,1,model,Introduction,face_detection1
1638,1638,1638,29,"The main discovery is that we can share the network in generating each feature - map , as shown in .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 10), (10, 12), (12, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.606060606,28,0.121212121,20,0.606060606,1,model,Introduction,face_detection1
1639,1639,1639,30,"As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 10), (12, 13), (14, 15), (15, 16), (17, 19), (19, 20), (20, 21), (25, 26), (27, 30), (30, 31), (31, 33), (34, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.636363636,29,0.125541126,21,0.636363636,1,model,Introduction,face_detection1
1640,1640,1640,31,"The sharing can significantly reduce the number of parameters , and this enables our model to use more layers to generate the low - level feature maps used for detecting small faces .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 9), (12, 13), (15, 17), (17, 19), (19, 21), (22, 27), (27, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.666666667,30,0.12987013,22,0.666666667,1,model,Introduction,face_detection1
1641,1641,1641,32,"Also , the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations , and hence offer abundant semantic information to the network , without adding additional parameters .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (8, 9), (9, 11), (12, 13), (13, 14), (14, 18), (19, 20), (20, 23), (26, 27), (27, 30), (30, 31), (32, 33), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.696969697,31,0.134199134,23,0.696969697,1,model,Introduction,face_detection1
1642,1642,1642,33,"Our baseline framework follows FPN - like structures , but can also be applied to SSD - like architecture .",Introduction,Introduction,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 8), (13, 15), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.727272727,32,0.138528139,24,0.727272727,1,model,Introduction,face_detection1
1643,1643,1643,142,Code will be available at https ://github.com/clovaai.,Training,Training,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1,141,0.61038961,20,1,1,code,Training,face_detection1
1644,1644,1644,162,"Also , we tested different activation functions : ReLU , PReLU , and Leaky - ReLU for each model .",Experimental Setting,See Appendix,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 7), (8, 9), (10, 11), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.208333333,161,0.696969697,15,0.714285714,1,hyperparameters,Experimental Setting: See Appendix,face_detection1
1645,1645,1645,163,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .",Experimental Setting,See Appendix,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 8), (9, 11), (11, 12), (15, 17), (18, 21), (21, 22), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.222222222,162,0.701298701,16,0.761904762,1,hyperparameters,Experimental Setting: See Appendix,face_detection1
1646,1646,1646,178,"The results in 138 times lighter in model size and are 28.3 , 19.2 , and 11 times lighter in Madds .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (6, 7), (7, 9), (10, 11), (11, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.430555556,177,0.766233766,9,0.3,1,results,Experimental Setting: Performance Analysis,face_detection1
1647,1647,1647,179,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 6), (6, 8), (8, 11), (12, 13), (14, 24), (24, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.444444444,178,0.770562771,10,0.333333333,1,results,Experimental Setting: Performance Analysis,face_detection1
1648,1648,1648,180,The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 8), (8, 9), (9, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.458333333,179,0.774891775,11,0.366666667,1,results,Experimental Setting: Performance Analysis,face_detection1
1649,1649,1649,182,"The m AP gap to DSFD , which is tremendously heavier , is about 5.0 % , but it would be safe to suggest that the proposed method offers more decent trade - off in that DSFD uses about 2860 times more parameters than the proposed method .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (12, 13), (13, 16), (28, 29), (29, 34), (37, 38), (43, 44)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.486111111,181,0.783549784,13,0.433333333,1,results,Experimental Setting: Performance Analysis,face_detection1
1650,1650,1650,186,"When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 9), (11, 12), (12, 15), (15, 16), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.541666667,185,0.800865801,17,0.566666667,1,results,Experimental Setting: Performance Analysis,face_detection1
1651,1651,1651,187,"However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 8), (8, 10), (10, 16), (18, 21), (21, 22), (22, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.555555556,186,0.805194805,18,0.6,1,results,Experimental Setting: Performance Analysis,face_detection1
1652,1652,1652,196,"From the table , we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases .",Experimental Setting,Performance Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 10), (10, 11), (11, 13), (13, 14), (14, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.680555556,195,0.844155844,27,0.9,1,results,Experimental Setting: Performance Analysis,face_detection1
1653,1653,1653,204,"First , for all the different channel width , FPN based architecture achieved better detection performance compared to SSD based architecture , especially for detecting small faces .",Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 8), (9, 12), (12, 13), (13, 16), (16, 18), (18, 21), (22, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.791666667,203,0.878787879,4,0.210526316,1,results,Experimental Setting: Variation Analysis,face_detection1
1654,1654,1654,214,"When tested with SSD based architecture , PReLU outperformed Leaky - ReLU with larger margin than those using FPN structure .",Experimental Setting,Variation Analysis,face_detection,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 6), (7, 8), (8, 9), (9, 12), (12, 13), (13, 15), (15, 16), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.930555556,213,0.922077922,14,0.736842105,1,results,Experimental Setting: Variation Analysis,face_detection1
1655,1655,1655,4,"In this paper , we propose a novel face detection network with three novel contributions that address three key aspects of face detection , including better feature learning , progressive loss design and anchor assign based data augmentation , respectively .",abstract,abstract,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 12), (12, 15), (25, 28), (29, 32), (33, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.015463918,1,0.25,1,model,abstract,face_detection10
1656,1656,1656,30,"In this paper , we propose three novel techniques to address the above three issues , respectively .",Introduction,Loss design,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.647058824,29,0.149484536,22,0.647058824,1,model,Introduction: Loss design,face_detection10
1657,1657,1657,31,"First , we introduce a Feature Enhance Module ( FEM ) to enhance the discriminability and robustness of the features , which combines the advantages of the FPN in PyramidBox and Receptive Field Block ( RFB ) in RFBNet .",Introduction,Loss design,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 11), (11, 13), (14, 17), (17, 18), (19, 20), (22, 23), (24, 25), (25, 26), (27, 28), (28, 29), (29, 30), (31, 37), (37, 38), (38, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.676470588,30,0.154639175,23,0.676470588,1,model,Introduction: Loss design,face_detection10
1658,1658,1658,32,"Second , motivated by the hierarchical loss and pyramid anchor in PyramidBox , we design Progressive Anchor Loss ( PAL ) that uses progressive anchor sizes for not only different levels , but also different shots .",Introduction,Loss design,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(14, 15), (15, 21), (22, 23), (23, 26), (26, 27), (29, 31), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.705882353,31,0.159793814,24,0.705882353,1,model,Introduction: Loss design,face_detection10
1659,1659,1659,33,"Specifically , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .",Introduction,Loss design,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 7), (7, 8), (9, 11), (13, 14), (14, 16), (16, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.735294118,32,0.164948454,25,0.735294118,1,model,Introduction: Loss design,face_detection10
1660,1660,1660,34,"Third , we propose Improved Anchor Matching ( IAM ) , which integrates anchor partition strategy and anchor-based data augmentation to better match anchors and ground truth faces , and thus provides better initialization for the regressor .",Introduction,Loss design,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 10), (12, 13), (13, 16), (17, 20), (20, 23), (23, 28), (31, 32), (32, 34), (34, 35), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.764705882,33,0.170103093,26,0.764705882,1,model,Introduction: Loss design,face_detection10
1661,1661,1661,36,"Besides , since these techniques are all related to two - stream design , we name the proposed network as Dual Shot Face Detector ( DSFD ) .",Introduction,Loss design,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 16), (17, 19), (19, 20), (20, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.823529412,35,0.180412371,28,0.823529412,1,model,Introduction: Loss design,face_detection10
1662,1662,1662,127,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 11), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.031746032,126,0.649484536,2,0.166666667,1,hyperparameters,Implementation Details,face_detection10
1663,1663,1663,128,All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (8, 10), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.047619048,127,0.654639175,3,0.25,1,hyperparameters,Implementation Details,face_detection10
1664,1664,1664,129,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .",Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 10), (10, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.063492063,128,0.659793814,4,0.333333333,1,hyperparameters,Implementation Details,face_detection10
1665,1665,1665,130,The batch size is set to 16 .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.079365079,129,0.664948454,5,0.416666667,1,hyperparameters,Implementation Details,face_detection10
1666,1666,1666,131,"The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .",Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 8), (8, 9), (10, 14), (17, 20), (20, 27), (27, 28), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.095238095,130,0.670103093,6,0.5,1,hyperparameters,Implementation Details,face_detection10
1667,1667,1667,132,"During inference , the first shot 's outputs are ignored and the second shot predicts top 5 k high confident detections .",Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 8), (8, 9), (9, 10), (12, 14), (14, 15), (15, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.111111111,131,0.675257732,7,0.583333333,1,hyperparameters,Implementation Details,face_detection10
1668,1668,1668,133,Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 7), (7, 8), (8, 9), (9, 11), (11, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.126984127,132,0.680412371,8,0.666666667,1,hyperparameters,Implementation Details,face_detection10
1669,1669,1669,134,"For 4 bounding box coordinates , we round down top left coordinates and roundup width and height to expand the detection bounding box .",Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 5), (7, 9), (9, 12), (13, 17), (17, 19), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.142857143,133,0.68556701,9,0.75,1,hyperparameters,Implementation Details,face_detection10
1670,1670,1670,135,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,Implementation Details,Implementation Details,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.158730159,134,0.690721649,10,0.833333333,1,code,Implementation Details,face_detection10
1671,1671,1671,139,"In this subsection , we conduct extensive experiments and ablation studies on the WIDER FACE dataset to evaluate the effectiveness of several contributions of our proposed framework , including feature enhance module , progressive anchor loss , and improved anchor matching .",Implementation Details,Analysis on DSFD,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12), (13, 16), (28, 29), (29, 32), (33, 36), (38, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.222222222,138,0.711340206,1,0.083333333,1,experiments,Implementation Details: Analysis on DSFD,face_detection10
1672,1672,1672,143,"Feature Enhance Module First ,",Implementation Details,,face_detection,10,"['O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O']",18,0.285714286,142,0.731958763,5,0.416666667,1,baselines,Implementation Details,face_detection10
1673,1673,1673,146,"shows that our feature enhance module can improve VGG16 - based FSSD from 92.6 % , 90.2 % , 79.1 % to 93.0 % , 91.4 % , 84.6 % .",Implementation Details,"Feature Enhance Module First ,",face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (8, 12), (12, 13), (13, 21), (21, 22), (22, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.333333333,145,0.74742268,8,0.666666667,1,experiments,"Implementation Details: Feature Enhance Module First ,",face_detection10
1674,1674,1674,147,"Progressive Anchor Loss Second , we use Res50 - based FSSD as the baseline to add progressive anchor loss for comparison .",Implementation Details,"Feature Enhance Module First ,",face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (6, 7), (7, 11), (11, 12), (13, 14), (14, 16), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.349206349,146,0.75257732,9,0.75,1,baselines,"Implementation Details: Feature Enhance Module First ,",face_detection10
1675,1675,1675,150,"shows our progressive anchor loss can improve Res50 - based FSSD using FEM from 95.0 % , 94.1 % , 88.0 % to 95.3 % , 94.4 % , 88.6 % .",Implementation Details,"Feature Enhance Module First ,",face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (6, 7), (7, 11), (11, 12), (12, 13), (13, 14), (14, 22), (22, 23), (23, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.396825397,149,0.768041237,12,1,1,experiments,"Implementation Details: Feature Enhance Module First ,",face_detection10
1676,1676,1676,151,Improved Anchor Matching,Implementation Details,,face_detection,10,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",26,0.412698413,150,0.773195876,0,0,1,experiments,Implementation Details,face_detection10
1677,1677,1677,153,"shows that our improved anchor matching can improve Res101 based FSSD using FEM from 95.8 % , 95.1 % , 89.7 % to 96.1 % , 95.2 % , 90.0 % .",Implementation Details,Improved Anchor Matching,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 6), (8, 11), (11, 12), (12, 13), (13, 14), (14, 22), (22, 23), (23, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.444444444,152,0.783505155,2,0.133333333,1,experiments,Implementation Details: Improved Anchor Matching,face_detection10
1678,1678,1678,178,WIDER FACE Dataset,Implementation Details,Effects of Different Backbones,face_detection,10,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",53,0.841269841,177,0.912371134,11,0.523809524,1,results,Implementation Details: Effects of Different Backbones,face_detection10
1679,1679,1679,182,"As shown in , our DSFD achieves the best performance among all of the state - of - the - art face detectors based on the average precision ( AP ) across the three subsets , i.e. , 96.6 % ( Easy ) , 95.7 % ( Medium ) and 90.4 % ( Hard ) on validation set , and 96.0 % ( Easy ) , 95.3 % ( Medium ) and 90.0 % ( Hard ) on test set .",Implementation Details,Effects of Different Backbones,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 10), (10, 11), (11, 23), (23, 25), (26, 31), (31, 32), (33, 35), (36, 37), (55, 56), (56, 58), (60, 77), (77, 78), (78, 80)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.904761905,181,0.932989691,15,0.714285714,1,results,Implementation Details: Effects of Different Backbones,face_detection10
1680,1680,1680,184,FDDB Dataset,Implementation Details,,face_detection,10,"['O', 'O']","[(0, 2)]","['O', 'O']",59,0.936507937,183,0.943298969,17,0.80952381,1,experiments,Implementation Details,face_detection10
1681,1681,1681,187,"As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .",Implementation Details,FDDB Dataset,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 15), (15, 16), (17, 22), (23, 24), (24, 29), (29, 30), (31, 35), (35, 37), (37, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.984126984,186,0.958762887,20,0.952380952,1,experiments,Implementation Details: FDDB Dataset,face_detection10
1682,1682,1682,188,"After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .",Implementation Details,FDDB Dataset,face_detection,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (10, 12), (12, 13), (13, 15), (17, 19), (20, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,1,187,0.963917526,21,1,1,experiments,Implementation Details: FDDB Dataset,face_detection10
1683,1683,1683,2,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,title,title,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003558719,1,0,1,research-problem,title,face_detection11
1684,1684,1684,4,"A unified deep neural network , denoted the multi -scale CNN ( MS - CNN ) , is proposed for fast multi-scale object detection .",abstract,abstract,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 7), (8, 16), (20, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,3,0.010676157,1,0.142857143,1,research-problem,abstract,face_detection11
1685,1685,1685,10,"State - of - the - art object detection performance , at up to 15 fps , is reported on datasets , such as KITTI and Caltech , containing a substantial number of small objects .",abstract,abstract,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,1,9,0.03202847,7,1,1,research-problem,abstract,face_detection11
1686,1686,1686,29,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .",Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8), (9, 10), (11, 19), (20, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.580645161,28,0.099644128,18,0.580645161,1,model,Introduction,face_detection11
1687,1687,1687,30,"Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .",Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 9), (11, 14), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.612903226,29,0.103202847,19,0.612903226,1,model,Introduction,face_detection11
1688,1688,1688,31,Both of them are learned end - to - end and share computations .,Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 10), (11, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.64516129,30,0.106761566,20,0.64516129,1,model,Introduction,face_detection11
1689,1689,1689,32,"However , to ease the inconsistency between the sizes of objects and receptive fields , object detection is performed with multiple output layers , each focusing on objects within certain scale ranges ( see ) .",Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 17), (18, 20), (20, 23), (25, 27), (27, 28), (28, 29), (29, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.677419355,31,0.110320285,21,0.677419355,1,model,Introduction,face_detection11
1690,1690,1690,35,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,Introduction,Introduction,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (8, 9), (9, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.774193548,34,0.120996441,24,0.774193548,1,model,Introduction,face_detection11
1691,1691,1691,191,The performance of the MS - CNN detector was evaluated on the KITTI and Caltech Pedestrian benchmarks .,Experimental Evaluation,Experimental Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 8), (9, 11), (12, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.071428571,190,0.676156584,1,0.071428571,1,results,Experimental Evaluation,face_detection11
1692,1692,1692,202,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn.",Experimental Evaluation,Experimental Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (7, 8), (9, 11), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.857142857,201,0.715302491,12,0.857142857,1,code,Experimental Evaluation,face_detection11
1693,1693,1693,203,All times are reported for implementation on a single CPU core ( 2.40 GHz ) of an Intel Xeon E5 - 2630 server with 64 GB of RAM .,Experimental Evaluation,Experimental Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (8, 15), (15, 16), (17, 23), (23, 24), (24, 28), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.928571429,202,0.71886121,13,0.928571429,1,experimental-setup,Experimental Evaluation,face_detection11
1694,1694,1694,204,An NVIDIA Titan GPU was used for CNN computations .,Experimental Evaluation,Experimental Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,1,203,0.722419929,14,1,1,experimental-setup,Experimental Evaluation,face_detection11
1695,1695,1695,260,The MS - CNN set a new record for the detection of pedestrians and cyclists .,Object Detection Evaluation,Object Detection Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (6, 8), (8, 9), (10, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.755555556,259,0.921708185,34,0.755555556,1,results,Object Detection Evaluation,face_detection11
1696,1696,1696,262,"We also led a nontrivial margin over the very recent SDP + RPN , which used scale depen - dent pooling .",Object Detection Evaluation,Object Detection Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (8, 13), (15, 16), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.8,261,0.928825623,36,0.8,1,results,Object Detection Evaluation,face_detection11
1697,1697,1697,266,Pedestrian detection on Caltech The MS - CNN detector was also evaluated on the Caltech pedestrian benchmark .,Object Detection Evaluation,Object Detection Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.888888889,265,0.943060498,40,0.888888889,1,results,Object Detection Evaluation,face_detection11
1698,1698,1698,268,"As shown in , the MS - CNN has state - of the - art performance . and ( c ) show that it performs very well for small and occluded objects , outperforming DeepParts , which explicitly addresses occlusion .",Object Detection Evaluation,Object Detection Evaluation,face_detection,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (9, 16), (21, 22), (24, 25), (25, 27), (27, 28), (28, 32), (33, 34), (34, 35), (37, 39), (39, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.933333333,267,0.950177936,42,0.933333333,1,results,Object Detection Evaluation,face_detection11
1699,1699,1699,4,Region proposal mechanisms are essential for existing deep learning approaches to object detection in images .,abstract,abstract,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.02173913,1,0.166666667,1,research-problem,abstract,face_detection12
1700,1700,1700,12,"Existing deep learning approaches to solve this task ( e.g. , R - CNN and its variants ) mainly rely on region proposal mechanisms ( e.g. , region proposal networks ( RPN s ) ) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.016393443,11,0.079710145,2,0.1,1,research-problem,Introduction,face_detection12
1701,1701,1701,19,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 22), (24, 25), (25, 27), (27, 29), (30, 35), (35, 36), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.073770492,18,0.130434783,9,0.45,1,model,Introduction,face_detection12
1702,1702,1702,21,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 8), (9, 10), (10, 15), (15, 16), (16, 18), (18, 19), (19, 24), (25, 26), (26, 28), (32, 34), (35, 37), (37, 39), (39, 40), (40, 41), (41, 43), (43, 45), (45, 46), (47, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.090163934,20,0.144927536,11,0.55,1,model,Introduction,face_detection12
1703,1703,1703,22,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .",Introduction,Introduction,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 8), (8, 9), (10, 13), (14, 16), (17, 22), (22, 24), (25, 30), (34, 37), (38, 39), (39, 41), (41, 42), (42, 43), (43, 44), (44, 45), (46, 48), (51, 53), (53, 54), (54, 55), (56, 58), (58, 60), (60, 61), (62, 64)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.098360656,21,0.152173913,12,0.6,1,model,Introduction,face_detection12
1704,1704,1704,90,Rebar Head Detection,Introduction,,face_detection,12,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",80,0.655737705,89,0.644927536,0,0,1,experiments,Introduction,face_detection12
1705,1705,1705,98,"As shown in , our proposed method with Stack = 2 , Base = 40 , Depth = 5 has achieved the best performance among all solutions in terms of F1 Score .",Introduction,Rebar Head Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 8), (8, 19), (20, 21), (22, 24), (24, 25), (25, 27), (27, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.721311475,97,0.702898551,8,0.8,1,experiments,Introduction: Rebar Head Detection,face_detection12
1706,1706,1706,100,"Therefore , we can conclude that , compared to the state - of - the - art baselines , WSMA - Seg is much simpler , more effective , and more efficient .",Introduction,Rebar Head Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (7, 9), (10, 18), (19, 22), (22, 23), (23, 25), (26, 28), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.737704918,99,0.717391304,10,1,1,experiments,Introduction: Rebar Head Detection,face_detection12
1707,1707,1707,101,WIDER Face Detection,Introduction,,face_detection,12,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",91,0.745901639,100,0.724637681,0,0,1,experiments,Introduction,face_detection12
1708,1708,1708,104,WIDER Face results in a much lower detection accuracy compared to other face detection datasets .,Introduction,WIDER Face Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (5, 9), (9, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.770491803,103,0.746376812,3,0.3,1,experiments,Introduction: WIDER Face Detection,face_detection12
1709,1709,1709,111,"The results show that our proposed WSMA - Seg outperforms the state - of - the - art baselines in all three categories , reaching 94.70 , 93.41 , and 87.23 in Easy , Medium , and Hard categories , respectively .",Introduction,WIDER Face Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 9), (9, 10), (11, 19), (19, 20), (20, 23), (24, 25), (25, 31), (31, 32), (32, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.827868852,110,0.797101449,10,1,1,experiments,Introduction: WIDER Face Detection,face_detection12
1710,1710,1710,112,MS COCO Detection,Introduction,WIDER Face Detection,face_detection,12,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",102,0.836065574,111,0.804347826,0,0,1,experiments,Introduction: WIDER Face Detection,face_detection12
1711,1711,1711,130,"The results show that our WSMA - Seg approach outperforms all state - of - the - art baselines in terms of most metrics , including the most challenging metrics , AP , AP s , AR 1 , and AR s .",Introduction,WIDER Face Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 9), (9, 10), (10, 19), (19, 22), (22, 24), (25, 26), (27, 30), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",120,0.983606557,129,0.934782609,18,0.9,1,experiments,Introduction: WIDER Face Detection,face_detection12
1712,1712,1712,131,"For the other metrics , the performance of our proposed approach is also close to those of the best baselines .",Introduction,WIDER Face Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (6, 7), (7, 8), (8, 11), (13, 15), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",121,0.991803279,130,0.942028986,19,0.95,1,experiments,Introduction: WIDER Face Detection,face_detection12
1713,1713,1713,132,This proves that the proposed WSMA - Seg approach generally achieves more accurate and robust object detection than the state - of - the - art approaches without NMS .,Introduction,WIDER Face Detection,face_detection,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 9), (10, 11), (11, 17), (17, 18), (19, 27), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",122,1,131,0.949275362,20,1,1,experiments,Introduction: WIDER Face Detection,face_detection12
1714,1714,1714,2,RetinaFace : Single - stage Dense Face Localisation in the Wild,title,title,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004166667,1,0,1,research-problem,title,face_detection13
1715,1715,1715,4,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .",abstract,abstract,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.0125,1,0.1,1,research-problem,abstract,face_detection13
1716,1716,1716,13,Extra annotations and code have been made available at : https://github.com/deepinsight/insightface/tree/master/RetinaFace.,abstract,"Specifically ,",face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1,12,0.05,10,1,1,code,"abstract: Specifically ,",face_detection13
1717,1717,1717,43,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .",Introduction,Introduction,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 18), (18, 19), (19, 21), (23, 24), (25, 28), (28, 31), (31, 33), (34, 36), (37, 40), (42, 46), (46, 47), (47, 50)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.852941176,42,0.175,29,0.852941176,1,model,Introduction,face_detection13
1718,1718,1718,149,Training Details .,,,face_detection,13,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",0,0,148,0.616666667,27,0.818181818,1,experiments,,face_detection13
1719,1719,1719,150,"We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .",Training Details .,Training Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 7), (8, 9), (9, 10), (10, 11), (12, 14), (14, 15), (15, 16), (17, 19), (19, 20), (20, 22), (23, 24), (24, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,149,0.620833333,28,0.848484848,1,experimental-setup,Training Details .,face_detection13
1720,1720,1720,151,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .",Training Details .,Training Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 8), (9, 11), (11, 14), (14, 15), (15, 17), (19, 21), (21, 22), (22, 23), (23, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,150,0.625,29,0.878787879,1,experimental-setup,Training Details .,face_detection13
1721,1721,1721,152,The training process terminates at 80 epochs .,Training Details .,Training Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,151,0.629166667,30,0.909090909,1,experimental-setup,Training Details .,face_detection13
1722,1722,1722,153,Testing Details .,Training Details .,,face_detection,13,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",4,0.666666667,152,0.633333333,31,0.939393939,1,experimental-setup,Training Details .,face_detection13
1723,1723,1723,154,"For testing on WIDER FACE , we follow the standard practices of and employ flip as well as multi-scale ( the short edge of image at [ 500 , 800 , 1100 , 1400 , 1700 ] ) strategies .",Training Details .,Testing Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (7, 8), (9, 11), (13, 14), (14, 15), (15, 18), (26, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.833333333,153,0.6375,32,0.96969697,1,experimental-setup,Training Details .: Testing Details .,face_detection13
1724,1724,1724,155,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,Training Details .,Testing Details .,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (6, 8), (9, 11), (11, 12), (12, 15), (15, 16), (17, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,154,0.641666667,33,1,1,experimental-setup,Training Details .: Testing Details .,face_detection13
1725,1725,1725,161,"Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (8, 10), (11, 24), (24, 25), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,160,0.666666667,5,0.625,1,ablation-analysis,Ablation Study,face_detection13
1726,1726,1726,162,"By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 9), (10, 13), (13, 14), (14, 18), (19, 21), (22, 23), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.75,161,0.670833333,6,0.75,1,ablation-analysis,Ablation Study,face_detection13
1727,1727,1727,163,"Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .",Ablation Study,Ablation Study,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 7), (7, 8), (8, 9), (10, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.875,162,0.675,7,0.875,1,ablation-analysis,Ablation Study,face_detection13
1728,1728,1728,167,Face box Accuracy,Method,,face_detection,13,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",2,0.05,166,0.691666667,0,0,1,results,Method,face_detection13
1729,1729,1729,172,"More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .",Method,Face box Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (6, 8), (8, 9), (9, 11), (13, 17), (18, 19), (37, 38), (38, 40), (59, 60), (60, 62)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.175,171,0.7125,5,0.5,1,results,Method: Face box Accuracy,face_detection13
1730,1730,1730,173,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .",Method,Face box Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 7), (8, 12), (12, 14), (15, 19), (19, 25), (25, 26), (27, 29), (29, 31), (32, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2,172,0.716666667,6,0.6,1,results,Method: Face box Accuracy,face_detection13
1731,1731,1731,175,"RetinaFace successfully finds about 900 faces ( threshold at 0.5 ) out of the reported 1 , 151 faces .",Method,Face box Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 11), (11, 13), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.25,174,0.725,8,0.8,1,results,Method: Face box Accuracy,face_detection13
1732,1732,1732,178,Five Facial Landmark Accuracy,Method,,face_detection,13,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",13,0.325,177,0.7375,0,0,1,results,Method,face_detection13
1733,1733,1733,183,RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .,Method,Five Facial Landmark Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 10), (10, 11), (11, 16), (17, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.45,182,0.758333333,5,0.714285714,1,results,Method: Five Facial Landmark Accuracy,face_detection13
1734,1734,1734,185,"Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .",Method,Five Facial Landmark Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 5), (5, 7), (8, 10), (10, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5,184,0.766666667,7,1,1,results,Method: Five Facial Landmark Accuracy,face_detection13
1735,1735,1735,186,Dense Facial Landmark Accuracy,Method,,face_detection,13,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",21,0.525,185,0.770833333,0,0,1,results,Method,face_detection13
1736,1736,1736,191,"Even though the performance gap exists between supervised and self - supervised methods , the dense regression results of RetinaFace are comparable with these state - of - the - art methods .",Method,Dense Facial Landmark Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 18), (18, 19), (19, 20), (21, 23), (24, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.65,190,0.791666667,5,0.5,1,results,Method: Dense Facial Landmark Accuracy,face_detection13
1737,1737,1737,192,"More specifically , we observe that ( 1 ) five facial landmarks regression can alleviate the training difficulty of dense regression branch and significantly improve the dense regression results .",Method,Dense Facial Landmark Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (9, 13), (14, 15), (16, 18), (18, 19), (19, 22), (23, 25), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.675,191,0.795833333,6,0.6,1,ablation-analysis,Method: Dense Facial Landmark Accuracy,face_detection13
1738,1738,1738,193,( 2 ) using single - stage features ( as in RetinaFace ) to predict dense correspondence parameters is much harder than employing ( Region of Interest ) RoI features ( as in Mesh Decoder ) .,Method,Dense Facial Landmark Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 8), (11, 12), (13, 15), (15, 18), (18, 19), (19, 21), (21, 22), (22, 23), (23, 30), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.7,192,0.8,7,0.7,1,ablation-analysis,Method: Dense Facial Landmark Accuracy,face_detection13
1739,1739,1739,197,Face Recognition Accuracy,Method,,face_detection,13,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",32,0.8,196,0.816666667,0,0,1,results,Method,face_detection13
1740,1740,1740,204,"The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .",Method,Face Recognition Accuracy,face_detection,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (7, 8), (9, 13), (14, 15), (15, 19), (19, 20), (20, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.975,203,0.845833333,7,0.875,1,results,Method: Face Recognition Accuracy,face_detection13
1741,1741,1741,2,WIDER FACE : A Face Detection Benchmark,title,title,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003717472,1,0,1,research-problem,title,face_detection14
1742,1742,1742,32,We introduce a large - scale face detection dataset called WIDER FACE .,Introduction,Introduction,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.72,31,0.115241636,18,0.72,1,research-problem,Introduction,face_detection14
1743,1743,1743,33,"It consists of 32 , 203 images with 393 , 703 labeled faces , which is 10 times larger than the current largest face detection dataset .",Introduction,Introduction,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 7), (7, 8), (8, 13), (16, 19), (19, 20), (21, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.76,32,0.118959108,19,0.76,1,experiments,Introduction,face_detection14
1744,1744,1744,34,"The faces vary largely in appearance , pose , and scale , as shown in .",Introduction,Introduction,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8,33,0.12267658,20,0.8,1,experiments,Introduction,face_detection14
1745,1745,1745,35,"In order to quantify different types of errors , we annotate multiple attributes : occlusion , pose , and event categories , which allows in depth analysis of existing algorithms .",Introduction,Introduction,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (11, 13), (14, 15), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.84,34,0.126394052,21,0.84,1,model,Introduction,face_detection14
1746,1746,1746,149,"We select VJ , ACF , DPM , and Faceness as baselines .",Experimental Results,Benchmarks,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 10), (10, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.042105263,148,0.550185874,3,0.068181818,1,baselines,Experimental Results: Benchmarks,face_detection14
1747,1747,1747,155,Overall .,Experimental Results,,face_detection,14,"['O', 'O']",[],"['O', 'O']",10,0.105263158,154,0.572490706,9,0.204545455,1,results,Experimental Results,face_detection14
1748,1748,1748,158,"Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .",Experimental Results,Overall .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 4), (4, 5), (5, 7), (8, 9), (9, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.136842105,157,0.583643123,12,0.272727273,1,results,Experimental Results: Overall .,face_detection14
1749,1749,1749,159,"For the easy set , the average precision ( AP ) of most methods are over 60 % , but none of them surpasses 75 % .",Experimental Results,Overall .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 11), (11, 12), (12, 14), (14, 15), (15, 18), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.147368421,158,0.587360595,13,0.295454545,1,results,Experimental Results: Overall .,face_detection14
1750,1750,1750,160,The performance drops 10 % for all methods on the medium set .,Experimental Results,Overall .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (6, 8), (8, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.157894737,159,0.591078067,14,0.318181818,1,results,Experimental Results: Overall .,face_detection14
1751,1751,1751,161,The hard set is even more challenging .,Experimental Results,Overall .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.168421053,160,0.594795539,15,0.340909091,1,results,Experimental Results: Overall .,face_detection14
1752,1752,1752,164,Scale .,Experimental Results,,face_detection,14,"['O', 'O']","[(0, 1)]","['O', 'O']",19,0.2,163,0.605947955,18,0.409090909,1,results,Experimental Results,face_detection14
1753,1753,1753,166,The results of small scale are abysmal : none of the algorithms is able to achieve more than 12 % AP .,Experimental Results,Scale .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (13, 16), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.221052632,165,0.6133829,20,0.454545455,1,results,Experimental Results: Scale .,face_detection14
1754,1754,1754,168,Occlusion .,Experimental Results,,face_detection,14,"['O', 'O']","[(0, 1)]","['O', 'O']",23,0.242105263,167,0.620817844,22,0.5,1,results,Experimental Results,face_detection14
1755,1755,1755,172,"With partial occlusion , the performance drops significantly .",Experimental Results,Occlusion .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (6, 7), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.284210526,171,0.635687732,26,0.590909091,1,results,Experimental Results: Occlusion .,face_detection14
1756,1756,1756,173,The maximum AP is only 26.5 % achieved by Faceness .,Experimental Results,Occlusion .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (5, 7), (7, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.294736842,172,0.639405204,27,0.613636364,1,results,Experimental Results: Occlusion .,face_detection14
1757,1757,1757,177,Pose .,Experimental Results,,face_detection,14,"['O', 'O']","[(0, 1)]","['O', 'O']",32,0.336842105,176,0.654275093,31,0.704545455,1,results,Experimental Results,face_detection14
1758,1758,1758,183,"The best performance is achieved by Faceness , with a recall below 20 % .",Experimental Results,Pose .,face_detection,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (8, 9), (10, 11), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.4,182,0.676579926,37,0.840909091,1,results,Experimental Results: Pose .,face_detection14
1759,1759,1759,4,"Although tremendous strides have been made in face detection , one of the remaining open challenges is to achieve real - time speed on the CPU as well as maintain high performance , since effective models for face detection tend to be computationally prohibitive .",abstract,abstract,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.012711864,1,0.1,1,research-problem,abstract,face_detection15
1760,1760,1760,13,Code is available at https://github.com/sfzhang15/FaceBoxes .,abstract,,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O']","[(4, 5)]","['O', 'O', 'O', 'O', 'O', 'O']",10,1,12,0.050847458,10,1,1,code,abstract,face_detection15
1761,1761,1761,41,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .",Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (21, 22), (23, 32), (32, 33), (33, 37), (37, 38), (39, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.658536585,40,0.169491525,27,0.658536585,1,model,Introduction: These two ways have their own advantages .,face_detection15
1762,1762,1762,42,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .",Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 9), (9, 10), (15, 20), (23, 24), (24, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.682926829,41,0.173728814,28,0.682926829,1,model,Introduction: These two ways have their own advantages .,face_detection15
1763,1763,1763,43,The proposed method has a lightweight yet powerful network structure ( as shown in ) that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .,Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 10), (16, 18), (19, 26), (28, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.707317073,42,0.177966102,29,0.707317073,1,model,Introduction: These two ways have their own advantages .,face_detection15
1764,1764,1764,44,"The RDCL is designed to enable FaceBoxes to achieve real - time speed on the CPU , and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces .",Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 9), (9, 13), (13, 14), (15, 16), (19, 20), (20, 22), (22, 23), (24, 26), (27, 28), (28, 29), (29, 30), (30, 32), (32, 34), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.731707317,43,0.18220339,30,0.731707317,1,model,Introduction: These two ways have their own advantages .,face_detection15
1765,1765,1765,45,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the input image , which significantly improves the recall rate of small faces .",Introduction,These two ways have their own advantages .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 9), (9, 11), (11, 15), (15, 16), (17, 19), (19, 20), (21, 23), (25, 27), (28, 30), (30, 31), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.756097561,44,0.186440678,31,0.756097561,1,model,Introduction: These two ways have their own advantages .,face_detection15
1766,1766,1766,176,"We first filter out most boxes by a confidence threshold of 0.05 and keep the top 400 boxes before applying NMS , then we perform NMS with jaccard overlap of 0.3 and keep the top 200 boxes .",Experiments,Runtime efficiency,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 6), (6, 7), (8, 10), (10, 11), (11, 12), (13, 14), (15, 18), (18, 20), (20, 21), (24, 25), (25, 26), (26, 27), (27, 29), (29, 30), (30, 31), (32, 33), (34, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,175,0.741525424,5,0.625,1,experimental-setup,Experiments: Runtime efficiency,face_detection15
1767,1767,1767,177,We measure the speed using Titan X ( Pascal ) and cuDNN v 5.1 with Intel Xeon E5-2660v3@2.60 GHz .,Experiments,Runtime efficiency,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 10), (11, 14), (14, 15), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,176,0.745762712,6,0.75,1,experimental-setup,Experiments: Runtime efficiency,face_detection15
1768,1768,1768,194,FaceBoxes Anchor densification strategy is crucial .,Model analysis,,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.52,193,0.81779661,1,0.076923077,1,ablation-analysis,Model analysis,face_detection15
1769,1769,1769,195,"Our anchor densification strategy is used to increase the density of small anchors ( i.e. , 32 32 and 64 64 ) in order to improve the recall rate of small faces .",Model analysis,FaceBoxes Anchor densification strategy is crucial .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (9, 10), (10, 11), (11, 22), (24, 26), (27, 29), (29, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.56,194,0.822033898,2,0.153846154,1,ablation-analysis,Model analysis: FaceBoxes Anchor densification strategy is crucial .,face_detection15
1770,1770,1770,197,"2 , we can see that the m AP on FDDB is reduced from 96.0 % to 94.9 % after ablating the anchor densification strategy .",Model analysis,From the results listed in Tab .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 9), (9, 10), (10, 11), (12, 14), (14, 16), (16, 17), (17, 19), (19, 21), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.64,196,0.830508475,4,0.307692308,1,ablation-analysis,Model analysis: From the results listed in Tab .,face_detection15
1771,1771,1771,202,RDCL is efficient and accuracy - preserving .,Model analysis,,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.84,201,0.851694915,9,0.692307692,1,ablation-analysis,Model analysis,face_detection15
1772,1772,1772,209,AFW dataset .,Evaluation on benchmark,,face_detection,15,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",2,0.095238095,208,0.881355932,2,0.095238095,1,results,Evaluation on benchmark,face_detection15
1773,1773,1773,212,"As illustrated in , our FaceBoxes outperforms all others by a large margin .",Evaluation on benchmark,It has 205 images with 473 faces .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 9), (9, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.238095238,211,0.894067797,5,0.238095238,1,results,Evaluation on benchmark: It has 205 images with 473 faces .,face_detection15
1774,1774,1774,214,PASCAL face dataset .,Evaluation on benchmark,,face_detection,15,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",7,0.333333333,213,0.902542373,7,0.333333333,1,results,Evaluation on benchmark,face_detection15
1775,1775,1775,217,"Our method significantly outperforms all other methods and commercial face detectors ( e.g. , SkyBiometry , Face + + and Picasa ) .",Evaluation on benchmark,PASCAL face dataset .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (4, 7), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.476190476,216,0.915254237,10,0.476190476,1,results,Evaluation on benchmark: PASCAL face dataset .,face_detection15
1776,1776,1776,219,FDDB dataset .,Evaluation on benchmark,,face_detection,15,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",12,0.571428571,218,0.923728814,12,0.571428571,1,results,Evaluation on benchmark,face_detection15
1777,1777,1777,226,Our FaceBoxes achieves the state - of - the - art performance and outperforms all others by a large margin on discontinuous and continuous ROC curves .,Evaluation on benchmark,The results are shown in and .,face_detection,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 12), (13, 14), (14, 16), (16, 17), (18, 20), (20, 21), (21, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.904761905,225,0.953389831,19,0.904761905,1,results,Evaluation on benchmark: The results are shown in and .,face_detection15
1778,1778,1778,2,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition",title,title,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.002145923,1,0,1,research-problem,title,face_detection16
1779,1779,1779,15,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 11), (11, 12), (12, 13), (13, 16), (17, 20), (21, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.047619048,14,0.030042918,5,0.135135135,1,model,INTRODUCTION,face_detection16
1780,1780,1780,16,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (7, 9), (9, 10), (13, 14), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.057142857,15,0.032188841,6,0.162162162,1,model,INTRODUCTION,face_detection16
1781,1781,1781,22,We refer the set of intermediate layer features as hyperfeatures .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (5, 8), (8, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.114285714,21,0.045064378,12,0.324324324,1,model,INTRODUCTION,face_detection16
1782,1782,1782,29,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 9), (9, 11), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.180952381,28,0.060085837,19,0.513513514,1,model,INTRODUCTION,face_detection16
1783,1783,1783,30,"In order to learn the tasks , we train them simultaneously using multiple loss functions .",INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 6), (8, 10), (10, 11), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.19047619,29,0.06223176,20,0.540540541,1,model,INTRODUCTION,face_detection16
1784,1784,1784,32,The deep CNN combined with the fusion - CNN can be learned together in an end -toend fashion .,INTRODUCTION,INTRODUCTION,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (6, 9), (9, 11), (11, 13), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.20952381,31,0.066523605,22,0.594594595,1,model,INTRODUCTION,face_detection16
1785,1785,1785,321,Face Detection,Training,,face_detection,16,"['O', 'O']","[(0, 2)]","['O', 'O']",180,0.722891566,320,0.686695279,0,0,1,experiments,Training,face_detection16
1786,1786,1786,333,"As can be seen from these figures , both HyperFace and HF - ResNet outperform all the reported academic and commercial detectors on the AFW and PASCAL datasets .",Training,Face Detection,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 14), (14, 15), (15, 22), (22, 23), (24, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",192,0.771084337,332,0.712446352,12,0.428571429,1,experiments,Training: Face Detection,face_detection16
1787,1787,1787,334,"HyperFace achieves a high mean average precision ( m AP ) of 97.9 % and 92.46 % , for AFW and PASCAL datasets respectively .",Training,Face Detection,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 11), (11, 12), (12, 17), (18, 19), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",193,0.775100402,333,0.714592275,13,0.464285714,1,experiments,Training: Face Detection,face_detection16
1788,1788,1788,335,HF - ResNet further improves the m AP to 99.4 % and 96.2 %,Training,Face Detection,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 5), (6, 8), (8, 9), (9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",194,0.779116466,334,0.716738197,14,0.5,1,experiments,Training: Face Detection,face_detection16
1789,1789,1789,340,"In spite of these issues , HyperFace performance is comparable to recently published deep learning - based face detection methods such as DP2MFD and Faceness on the FDDB dataset 1 with m AP of 90.1 % .",Training,Face Detection,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (9, 11), (11, 20), (22, 23), (24, 25), (25, 26), (27, 29), (30, 31), (31, 33), (33, 34), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",199,0.799196787,339,0.727467811,19,0.678571429,1,experiments,Training: Face Detection,face_detection16
1790,1790,1790,342,clearly show that multitask CNNs ( Multitask Face and HyperFace ) outperform R - CNN Face by a wide margin .,Training,Face Detection,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 11), (11, 12), (12, 16), (16, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",201,0.807228916,341,0.731759657,21,0.75,1,experiments,Training: Face Detection,face_detection16
1791,1791,1791,350,Landmarks Localization,Training,,face_detection,16,"['O', 'O']","[(0, 2)]","['O', 'O']",209,0.83935743,349,0.748927039,0,0,1,experiments,Training,face_detection16
1792,1792,1792,370,shows that HyperFace performs consistently accurate overall pose angles .,Training,Landmarks Localization,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (4, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",229,0.919678715,369,0.791845494,20,0.5,1,experiments,Training: Landmarks Localization,face_detection16
1793,1793,1793,372,"Moreover , we find that R - CNN Fiducial and Multitask Face attain similar performance .",Training,Landmarks Localization,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",231,0.927710843,371,0.796137339,22,0.55,1,experiments,Training: Landmarks Localization,face_detection16
1794,1794,1794,376,"Additionally , we observe that HF - ResNet significantly improves the performance over HyperFace for both AFW and AFLW datasets .",Training,Landmarks Localization,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 10), (11, 12), (12, 13), (13, 14), (14, 15), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",235,0.9437751,375,0.80472103,26,0.65,1,experiments,Training: Landmarks Localization,face_detection16
1795,1795,1795,389,"We observe that HyperFace achieves a comparable NME of 10.88 , while HF - ResNet achieves the state - of - theart result on IBUG with NME of 8.18 .",Training,Landmarks Localization,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 8), (8, 9), (9, 10), (12, 15), (15, 16), (17, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",248,0.995983936,388,0.832618026,39,0.975,1,experiments,Training: Landmarks Localization,face_detection16
1796,1796,1796,393,Pose Estimation,Method,,face_detection,16,"['O', 'O']","[(0, 2)]","['O', 'O']",2,0.028169014,392,0.841201717,0,0,1,experiments,Method,face_detection16
1797,1797,1797,401,"As can be seen from the figure , both HyperFace and HF - ResNet outperform existing methods by a large margin .",Method,Pose Estimation,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 14), (14, 15), (15, 17), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.14084507,400,0.858369099,8,0.5,1,experiments,Method: Pose Estimation,face_detection16
1798,1798,1798,409,"HF - ResNet further improves the performance for roll , pitch as well as yaw .",Method,Pose Estimation,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 5), (6, 7), (7, 8), (8, 11), (11, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.253521127,408,0.875536481,16,1,1,results,Method: Pose Estimation,face_detection16
1799,1799,1799,410,Gender Recognition,Method,,face_detection,16,"['O', 'O']","[(0, 2)]","['O', 'O']",19,0.267605634,409,0.877682403,0,0,1,results,Method,face_detection16
1800,1800,1800,418,"On the LFWA dataset , our method outperforms PANDA and FaceTracer , and is equal to .",Method,The Celeb,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 7), (7, 8), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.38028169,417,0.894849785,8,0.216216216,1,experiments,Method: The Celeb,face_detection16
1801,1801,1801,423,HF - ResNet achieves state - of - the - art results on both CelebA and LFWA datasets .,Method,The Celeb,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 12), (12, 13), (14, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.450704225,422,0.905579399,13,0.351351351,1,results,Method: The Celeb,face_detection16
1802,1802,1802,427,The HyperFace with a linear bounding box regression and traditional NMS achieves a m AP of 94 % .,Method,The Celeb,face_detection,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 8), (9, 11), (11, 12), (13, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.507042254,426,0.91416309,17,0.459459459,1,results,Method: The Celeb,face_detection16
1803,1803,1803,4,"Face detection has been well studied for many years and one of remaining challenges is to detect small , blurred and partially occluded faces in uncontrolled environment .",abstract,abstract,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.0125,1,0.1,1,research-problem,abstract,face_detection17
1804,1804,1804,12,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,abstract,abstract,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,11,0.045833333,9,0.9,1,code,abstract,face_detection17
1805,1805,1805,31,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 11), (11, 13), (13, 14), (14, 16), (16, 18), (18, 19), (21, 24), (24, 25), (25, 26), (33, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.459459459,30,0.125,17,0.459459459,1,model,Introduction,face_detection17
1806,1806,1806,34,We investigate the performance of Feature Pyramid Networks ( FPN ) and modify it into a Low - level Feature Pyramid Network ( LFPN ) to join mutually helpful features together .,Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 11), (12, 15), (16, 25), (25, 27), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.540540541,33,0.1375,20,0.540540541,1,model,Introduction,face_detection17
1807,1807,1807,36,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 11), (11, 13), (13, 15), (15, 16), (17, 19), (19, 20), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.594594595,35,0.145833333,22,0.594594595,1,model,Introduction,face_detection17
1808,1808,1808,37,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 11), (11, 12), (13, 15), (15, 18), (19, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.621621622,36,0.15,23,0.621621622,1,model,Introduction,face_detection17
1809,1809,1809,38,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .",Introduction,Introduction,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 8), (8, 10), (10, 15), (15, 17), (18, 19), (19, 20), (21, 22), (22, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.648648649,37,0.154166667,24,0.648648649,1,model,Introduction,face_detection17
1810,1810,1810,219,FDDB Dataset .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,17,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",2,0.181818182,218,0.908333333,2,0.181818182,1,results,Evaluation on Benchmark,face_detection17
1811,1811,1811,223,The PyramidBox achieves state - ofart performance and the result is shown in and .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.545454545,222,0.925,6,0.545454545,1,results,Evaluation on Benchmark,face_detection17
1812,1812,1812,224,WIDER FACE Dataset .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,17,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",7,0.636363636,223,0.929166667,7,0.636363636,1,results,Evaluation on Benchmark,face_detection17
1813,1813,1813,228,"Our PyramidBox outperforms others across all three subsets , i.e. 0.961 ( easy ) , 0.950 ( medium ) , 0.889 ( hard ) for validation set , and 0.956 ( easy ) , 0.946 ( medium ) , 0.887 ( hard ) for testing set .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 4), (4, 5), (5, 8), (9, 10), (10, 14), (24, 25), (25, 27), (43, 44), (44, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1,227,0.945833333,11,1,1,results,Evaluation on Benchmark,face_detection17
1814,1814,1814,2,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection,title,title,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003134796,1,0,1,research-problem,title,face_detection18
1815,1815,1815,4,"Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e. unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3 D facial model construction , etc .",abstract,abstract,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.111111111,3,0.009404389,1,0.111111111,1,research-problem,abstract,face_detection18
1816,1816,1816,5,"Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real - world scenarios due to numerous challenges , e.g. heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc .",abstract,abstract,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.222222222,4,0.012539185,2,0.222222222,1,research-problem,abstract,face_detection18
1817,1817,1817,21,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 22), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.115942029,20,0.062695925,8,0.285714286,1,model,INTRODUCTION,face_detection18
1818,1818,1818,22,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .",INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (7, 8), (9, 10), (10, 11), (11, 14), (14, 16), (21, 24), (24, 26), (26, 27), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.130434783,21,0.065830721,9,0.321428571,1,model,INTRODUCTION,face_detection18
1819,1819,1819,24,Additionally this architecture also helps to synchronize both the global semantic features in high level layers and the localization features in low level layers for facial representation .,INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (9, 12), (12, 13), (13, 16), (18, 20), (20, 21), (21, 24), (24, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.15942029,23,0.072100313,11,0.392857143,1,model,INTRODUCTION,face_detection18
1820,1820,1820,26,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,INTRODUCTION,INTRODUCTION,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (7, 18), (18, 20), (23, 25), (27, 39), (39, 41), (41, 42), (42, 43), (44, 46), (46, 47), (47, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.188405797,25,0.078369906,13,0.464285714,1,model,INTRODUCTION,face_detection18
1821,1821,1821,228,Our CMS - RCNN is implemented in the Caffe deep learning framework .,Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.066666667,227,0.711598746,1,0.066666667,1,experimental-setup,Implementation Details,face_detection18
1822,1822,1822,229,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (7, 8), (9, 11), (11, 12), (13, 18), (20, 21), (21, 22), (23, 24), (25, 27), (28, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.133333333,228,0.714733542,2,0.133333333,1,experimental-setup,Implementation Details,face_detection18
1823,1823,1823,232,"In the MS - RPN , we want ' conv3 ' , ' conv4 ' , and ' conv5 ' to be synchronized to the same size so that concatenation can be applied .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (8, 20), (20, 22), (22, 23), (23, 24), (25, 27), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.333333333,231,0.724137931,5,0.333333333,1,experimental-setup,Implementation Details,face_detection18
1824,1824,1824,233,So ' conv3 ' is followed by pooling layer to perform down - sampling .,Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 7), (7, 9), (9, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,232,0.727272727,6,0.4,1,baselines,Implementation Details,face_detection18
1825,1825,1825,234,"Then ' conv3 ' , ' conv4 ' , and ' conv5 ' are normalized along the channel axis to a learnable re-weighting scale and concatenated together .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 13), (14, 15), (15, 16), (17, 19), (19, 20), (21, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.466666667,233,0.730407524,7,0.466666667,1,experimental-setup,Implementation Details,face_detection18
1826,1826,1826,235,"To ensure training convergence , the initial re-weighting scale needs to be carefully set .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (6, 9), (9, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.533333333,234,0.73354232,8,0.533333333,1,experimental-setup,Implementation Details,face_detection18
1827,1827,1827,236,"Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 19), (19, 21), (21, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6,235,0.736677116,9,0.6,1,experimental-setup,Implementation Details,face_detection18
1828,1828,1828,237,"In the CMS - CNN , the RoI pooling layer already ensure that the pooled feature maps have the same size .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 10), (11, 13), (14, 17), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.666666667,236,0.739811912,10,0.666666667,1,experimental-setup,Implementation Details,face_detection18
1829,1829,1829,239,"Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 17), (18, 20), (20, 21), (21, 23), (23, 29), (31, 32), (33, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8,238,0.746081505,12,0.8,1,experimental-setup,Implementation Details,face_detection18
1830,1830,1830,240,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 9), (9, 10), (11, 13), (13, 14), (14, 17), (19, 20), (23, 24), (25, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.866666667,239,0.749216301,13,0.866666667,1,experimental-setup,Implementation Details,face_detection18
1831,1831,1831,241,"Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .",Implementation Details,Implementation Details,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 9), (9, 10), (11, 14), (16, 19), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.933333333,240,0.752351097,14,0.933333333,1,experimental-setup,Implementation Details,face_detection18
1832,1832,1832,249,Experiments on WIDER FACE Dataset,EXPERIMENTS,EXPERIMENTS,face_detection,18,"['O', 'O', 'O', 'O', 'O']","[(2, 5)]","['O', 'O', 'O', 'O', 'O']",6,0.078947368,248,0.777429467,0,0,1,experiments,EXPERIMENTS,face_detection18
1833,1833,1833,268,"It achieves the best average precision in all level faces , i.e. AP = 0.902 ( Easy ) , 0.874 ( Medium ) and 0.643 ( Hard ) , and outperforms the second best baseline by 26.0 % ( Easy ) , 37.4 % ( Medium ) and 60.8 % ( Hard ) .",EXPERIMENTS,Testing and Comparison,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 10), (11, 12), (30, 31), (32, 35), (35, 36), (36, 41), (42, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.328947368,267,0.836990596,7,0.7,1,results,EXPERIMENTS: Testing and Comparison,face_detection18
1834,1834,1834,295,Experiments on FDDB Face Database,EXPERIMENTS,,face_detection,18,"['O', 'O', 'O', 'O', 'O']","[(2, 5)]","['O', 'O', 'O', 'O', 'O']",52,0.684210526,294,0.921630094,0,0,1,results,EXPERIMENTS,face_detection18
1835,1835,1835,304,Our method achieves the best recall rate on this database .,EXPERIMENTS,Experiments on FDDB Face Database,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.802631579,303,0.94984326,9,0.642857143,1,results,EXPERIMENTS: Experiments on FDDB Face Database,face_detection18
1836,1836,1836,307,The proposed CMS - RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods ( as shown ) .,EXPERIMENTS,Experiments on FDDB Face Database,face_detection,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (6, 7), (7, 14), (15, 16), (17, 21), (21, 23), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.842105263,306,0.959247649,12,0.857142857,1,results,EXPERIMENTS: Experiments on FDDB Face Database,face_detection18
1837,1837,1837,2,Face Detection Using Improved Faster RCNN,title,,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.009090909,1,0,1,research-problem,title,face_detection19
1838,1838,1838,18,"In this report , we propose a detailed design Faster RCNN method named FDNet1.0 for face detection , which achieves more decent performance than previous methods .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 12), (12, 13), (13, 14), (14, 15), (15, 17), (19, 20), (20, 23), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.365853659,17,0.154545455,15,0.365853659,1,research-problem,abstract,face_detection19
1839,1839,1839,19,"A deformable layer with fewer channels is attached to the backbone network to produce a "" thin "" feature map , which is subsequently fed to a full connected layer , building an efficient yet accurate two - stage detector .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (7, 9), (10, 12), (12, 14), (15, 20), (24, 26), (27, 30), (31, 32), (33, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.390243902,18,0.163636364,16,0.390243902,1,model,abstract,face_detection19
1840,1840,1840,20,"At testing time , we also find a comparable mean average precision ( m AP ) be achieved when the top - ranked proposals ( e.g. , 6000 ) are directly selected without NMS in the RPN stage over WIDER FACE dataset .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 16), (30, 33), (33, 34), (34, 35), (36, 38), (38, 39), (39, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.414634146,19,0.172727273,17,0.414634146,1,research-problem,abstract,face_detection19
1841,1841,1841,22,"Furthermore , the multi-scale training and testing strategy are also applied in our work .",abstract,abstract,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 8), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.463414634,21,0.190909091,19,0.463414634,1,model,abstract,face_detection19
1842,1842,1842,84,Single NVIDIA Tesla K80 is used for training and testing .,Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 7), (7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.047619048,83,0.754545455,1,0.058823529,1,experimental-setup,Implementation Details,face_detection19
1843,1843,1843,85,Mini batch size is set to 1 considering memory consumption .,Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 6), (6, 7), (7, 8), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.095238095,84,0.763636364,2,0.117647059,1,experimental-setup,Implementation Details,face_detection19
1844,1844,1844,86,"Specifically , ResNet_v1_101 trained on ImageNet - 128w is used for Faster RCNN feature extraction .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 8), (9, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.142857143,85,0.772727273,3,0.176470588,1,experimental-setup,Implementation Details,face_detection19
1845,1845,1845,89,"Aspect ratios ( 1 , 1.5 , 2 ) and scales ( 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 ) are carefully designed to capture better locations of faces in the RPN stage , and the number of filters for the RPN layer is set as 512 .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 30), (31, 33), (33, 35), (35, 37), (38, 39), (39, 40), (41, 43), (46, 49), (49, 50), (51, 53), (54, 56), (56, 57)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.285714286,88,0.8,6,0.352941176,1,experimental-setup,Implementation Details,face_detection19
1846,1846,1846,94,"By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (8, 13), (15, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.523809524,93,0.845454545,11,0.647058824,1,experimental-setup,Implementation Details,face_detection19
1847,1847,1847,95,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .",Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 10), (12, 14), (14, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.571428571,94,0.854545455,12,0.705882353,1,experimental-setup,Implementation Details,face_detection19
1848,1848,1848,96,Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,Implementation Details,Implementation Details,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (4, 5), (6, 8), (8, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.619047619,95,0.863636364,13,0.764705882,1,experimental-setup,Implementation Details,face_detection19
1849,1849,1849,103,"Compared with the recently published top approaches , FDNet1.0 wins two 1st places ( easy set = 95.9 % , medium set = 94.5 % ) and one 2nd place ( hard set = 87.9 % ) on the validation set , as illustrated in .",Implementation Details,Comparison on Benchmarks,face_detection,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (8, 9), (9, 10), (27, 37), (37, 38), (39, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.952380952,102,0.927272727,2,0.666666667,1,results,Implementation Details: Comparison on Benchmarks,face_detection19
1850,1850,1850,2,Selective Refinement Network for High Performance Face Detection,title,,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004219409,1,0,1,research-problem,title,face_detection2
1851,1851,1851,4,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .",abstract,abstract,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,3,0.012658228,1,0.142857143,1,research-problem,abstract,face_detection2
1852,1852,1852,43,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 9), (9, 15), (15, 16), (22, 23), (24, 28), (29, 30), (30, 36), (38, 40), (40, 46), (46, 47), (47, 52)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.727272727,42,0.17721519,32,0.727272727,1,model,"Introduction: However , Retina",face_detection2
1853,1853,1853,44,"The network structure of SRN is shown in , which consists of two key modules , named as the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (10, 12), (12, 15), (16, 18), (19, 28), (30, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.75,43,0.181434599,33,0.75,1,model,"Introduction: However , Retina",face_detection2
1854,1854,1854,45,"Specifically , the STC is applied to filter out most simple negative samples ( illustrated in ( a ) ) from the low levels of detection layers , which contains 88.9 % samples .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (7, 9), (9, 13), (20, 21), (25, 27), (29, 30), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.772727273,44,0.185654008,34,0.772727273,1,model,"Introduction: However , Retina",face_detection2
1855,1855,1855,48,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .",Introduction,"However , Retina",face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 12), (12, 14), (14, 18), (18, 21), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.840909091,47,0.198312236,37,0.840909091,1,model,"Introduction: However , Retina",face_detection2
1856,1856,1856,175,"The loss function for SRN is just the sum of the STC loss and the STR loss , i.e. , L = L STC + L STR .",Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (8, 10), (11, 17), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.692307692,174,0.734177215,19,0.703703704,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
1857,1857,1857,176,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .",Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 12), (13, 16), (16, 17), (18, 22), (23, 25), (26, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.730769231,175,0.738396624,20,0.740740741,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
1858,1858,1858,177,"We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .",Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 8), (8, 9), (9, 10), (10, 12), (13, 16), (18, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.769230769,176,0.742616034,21,0.777777778,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
1859,1859,1859,178,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .",Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 8), (8, 9), (10, 13), (15, 18), (18, 25), (25, 26), (26, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.807692308,177,0.746835443,22,0.814814815,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
1860,1860,1860,179,We implement SRN using the Py - Torch library .,Training Dataset .,Optimization .,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.846153846,178,0.751054852,23,0.851851852,1,experimental-setup,Training Dataset .: Optimization .,face_detection2
1861,1861,1861,213,AFW Dataset .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",1,0.052631579,212,0.894514768,1,0.052631579,1,results,Evaluation on Benchmark,face_detection2
1862,1862,1862,217,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (7, 15), (15, 16), (17, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.263157895,216,0.911392405,5,0.263157895,1,results,Evaluation on Benchmark,face_detection2
1863,1863,1863,218,PASCAL Face Dataset .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",6,0.315789474,217,0.915611814,6,0.315789474,1,results,Evaluation on Benchmark,face_detection2
1864,1864,1864,221,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 11), (11, 13), (13, 17), (17, 19), (20, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.473684211,220,0.928270042,9,0.473684211,1,results,Evaluation on Benchmark,face_detection2
1865,1865,1865,222,FDDB Dataset .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",10,0.526315789,221,0.932489451,10,0.526315789,1,results,Evaluation on Benchmark,face_detection2
1866,1866,1866,225,"As shown in ( c ) , our SRN sets a new state - of - the - art performance , i.e. , 98.8 % true positive rate when the number of false positives is equal to 1000 .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (9, 10), (11, 20), (21, 22), (23, 28), (28, 29), (30, 34), (34, 35), (35, 37), (37, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.684210526,224,0.945147679,13,0.684210526,1,results,Evaluation on Benchmark,face_detection2
1867,1867,1867,227,WIDER FACE Dataset .,Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",15,0.789473684,226,0.953586498,15,0.789473684,1,results,Evaluation on Benchmark,face_detection2
1868,1868,1868,230,"As shown in , we find that SRN performs favourably against the state - of - the - art based on the average precision ( AP ) across the three subsets , especially on the Hard subset which contains a large amount of small faces .",Evaluation on Benchmark,Evaluation on Benchmark,face_detection,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 9), (9, 10), (10, 11), (12, 19), (19, 21), (22, 27), (27, 28), (29, 31), (35, 37), (40, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.947368421,229,0.966244726,18,0.947368421,1,results,Evaluation on Benchmark,face_detection2
1869,1869,1869,2,Aggregate Channel Features for Multi-view Face Detection,title,,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003676471,1,0,1,research-problem,title,face_detection20
1870,1870,1870,4,Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones .,abstract,abstract,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.011029412,1,0.166666667,1,research-problem,abstract,face_detection20
1871,1871,1871,11,Human face detection have long been one of the most fundamental problems in computer vision and humancomputer interaction .,Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.034482759,10,0.036764706,1,0.034482759,1,research-problem,Introduction,face_detection20
1872,1872,1872,25,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .",Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 12), (12, 15), (18, 21), (21, 23), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.517241379,24,0.088235294,15,0.517241379,1,model,Introduction,face_detection20
1873,1873,1873,27,"With these two superiorities , the aggregate channel features breakthrough the bottleneck in VJ framework and have the potential to make great advance in face detection .",Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 10), (11, 12), (12, 13), (13, 15), (21, 23), (23, 24), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.586206897,26,0.095588235,17,0.586206897,1,model,Introduction,face_detection20
1874,1874,1874,29,"To do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , subsampling method , feature scale and soon , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners .",Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 13), (13, 14), (15, 18), (18, 19), (19, 21), (22, 25), (26, 28), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.655172414,28,0.102941176,19,0.655172414,1,model,Introduction,face_detection20
1875,1875,1875,30,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .",Introduction,Introduction,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (11, 12), (13, 15), (16, 17), (18, 20), (32, 37), (37, 38), (39, 40), (40, 41), (43, 44), (44, 46), (47, 49), (49, 50), (50, 52), (61, 62), (62, 64), (64, 65), (65, 67), (70, 72)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.689655172,29,0.106617647,20,0.689655172,1,experiments,Introduction,face_detection20
1876,1876,1876,243,"As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .",Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (7, 10), (10, 11), (12, 14), (14, 15), (15, 17), (18, 19), (19, 22), (22, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.172413793,242,0.889705882,1,0.111111111,1,results,Experiments,face_detection20
1877,1877,1877,244,"When it comes to commercial systems , ours is better than Face.com and almost equal to Face ++ and Google Picasa .",Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 8), (9, 11), (11, 12), (13, 16), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.206896552,243,0.893382353,2,0.222222222,1,results,Experiments,face_detection20
1878,1878,1878,248,"In discrete score where evaluation metric is the same as in AFW , our detector achieves 83.7 % , which is a little better than Yan et al ..",Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4), (4, 6), (11, 12), (13, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.344827586,247,0.908088235,6,0.666666667,1,results,Experiments,face_detection20
1879,1879,1879,250,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .",Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (5, 6), (7, 9), (9, 10), (11, 12), (13, 15), (15, 16), (16, 21), (21, 22), (22, 24), (24, 25), (25, 27), (28, 29), (29, 31), (31, 33), (33, 35), (35, 36), (37, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.413793103,249,0.915441176,8,0.888888889,1,results,Experiments,face_detection20
1880,1880,1880,251,Our detector using single - scale features performs a little worse with the benefit of faster detection speed .,Experiments,Experiments,face_detection,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 7), (7, 8), (9, 11), (11, 12), (13, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.448275862,250,0.919117647,9,1,1,results,Experiments,face_detection20
1881,1881,1881,2,Supervised Transformer Network for Efficient Face Detection,title,title,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003389831,1,0,1,research-problem,title,face_detection21
1882,1882,1882,4,Large pose variations remain to be a challenge that confronts real - word face detection .,abstract,abstract,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.010169492,1,0.1,1,research-problem,abstract,face_detection21
1883,1883,1883,33,"In contrast , we propose a new cascade Convolutional Neural Network that is trained end - to - end .",Introduction,"Besides these model - based methods ,",face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 11), (13, 14), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.127516779,32,0.108474576,19,0.422222222,1,model,"Introduction: Besides these model - based methods ,",face_detection21
1884,1884,1884,34,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .",Introduction,"Besides these model - based methods ,",face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 12), (14, 16), (16, 19), (19, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.134228188,33,0.111864407,20,0.444444444,1,model,"Introduction: Besides these model - based methods ,",face_detection21
1885,1885,1885,35,"Inspired by Chen et al. , we jointly conduct face detection and face alignment , since face alignment is helpful to distinguish faces / non - faces patterns .",Introduction,"Besides these model - based methods ,",face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (9, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.140939597,34,0.115254237,21,0.466666667,1,model,"Introduction: Besides these model - based methods ,",face_detection21
1886,1886,1886,36,"Different from Li et al. , this network is calculated on the original resolution to better leverage more discriminative information .",Introduction,"Besides these model - based methods ,",face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 11), (12, 14), (14, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.147651007,35,0.118644068,22,0.488888889,1,model,"Introduction: Besides these model - based methods ,",face_detection21
1887,1887,1887,38,"The aligned candidate face region is then fed into the second - stage network , a RCNN , for further verification .",Introduction,"Besides these model - based methods ,",face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (7, 9), (10, 14), (16, 17), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.161073826,37,0.125423729,24,0.533333333,1,model,"Introduction: Besides these model - based methods ,",face_detection21
1888,1888,1888,39,Note we only keep the K face candidate regions with top responses in a local neighborhood from the RPN .,Introduction,"Besides these model - based methods ,",face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 9), (9, 10), (10, 12), (12, 13), (14, 16), (16, 17), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.167785235,38,0.128813559,25,0.555555556,1,model,"Introduction: Besides these model - based methods ,",face_detection21
1889,1889,1889,43,"We concatenate the feature maps from the two cascaded networks together to form an architecture that is trained end - to - end , as shown in .",Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 10), (11, 13), (14, 15), (17, 18), (18, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.194630872,42,0.142372881,29,0.644444444,1,model,Introduction: This helps increase detection recall .,face_detection21
1890,1890,1890,45,Note that the canonical positions of the facial landmarks in the aligned face image and the predicted facial landmarks in the candidate face region jointly defines the transform from the candidate face region .,Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (7, 9), (9, 10), (11, 14), (16, 19), (19, 20), (21, 24), (24, 26), (27, 28), (28, 29), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.208053691,44,0.149152542,31,0.688888889,1,model,Introduction: This helps increase detection recall .,face_detection21
1891,1891,1891,46,"In the end - to - end training , the training of the first - stage RPN to predict facial landmarks is also supervised by annotated facial landmarks in each true face regions .",Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 8), (10, 11), (11, 12), (13, 17), (17, 19), (19, 21), (23, 25), (25, 28), (28, 29), (29, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.214765101,45,0.152542373,32,0.711111111,1,model,Introduction: This helps increase detection recall .,face_detection21
1892,1892,1892,47,We hence call our network a Supervised Transformer Network .,Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.22147651,46,0.155932203,33,0.733333333,1,model,Introduction: This helps increase detection recall .,face_detection21
1893,1893,1893,51,"Therefore , we propose a region - of - interest ( ROI ) convolution scheme to make the run-time of the Supervised Transformer Network to be more efficient .",Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 15), (15, 17), (18, 19), (19, 20), (21, 24), (24, 26), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.248322148,50,0.169491525,37,0.822222222,1,model,Introduction: This helps increase detection recall .,face_detection21
1894,1894,1894,52,It first uses a conventional boosting cascade to obtain a set of face candidate areas .,Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 9), (10, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.255033557,51,0.172881356,38,0.844444444,1,model,Introduction: This helps increase detection recall .,face_detection21
1895,1895,1895,53,"Then , we combine these regions into irregular binary ROI mask .",Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 6), (6, 7), (7, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.261744966,52,0.176271186,39,0.866666667,1,model,Introduction: This helps increase detection recall .,face_detection21
1896,1896,1896,54,"All DNN operations ( including convolution , ReLU , pooling , and concatenation ) are all processed inside the ROI mask , and hence significantly reduce the computation .",Introduction,This helps increase detection recall .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 5), (16, 18), (19, 21), (24, 26), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.268456376,53,0.179661017,40,0.888888889,1,model,Introduction: This helps increase detection recall .,face_detection21
1897,1897,1897,255,"As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .",Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (11, 13), (14, 16), (16, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.539473684,254,0.861016949,12,0.571428571,1,results,Experiments: There are 6 different ablative settings in total .,face_detection21
1898,1898,1898,256,"Besides , these three parts are complementary , remove anyone part will cause a recall drop .",Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (8, 9), (9, 11), (11, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.552631579,255,0.86440678,13,0.619047619,1,ablation-analysis,Experiments: There are 6 different ablative settings in total .,face_detection21
1899,1899,1899,262,We found that NMS tend to include too much noisy low confidence candidates .,Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (7, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.631578947,261,0.884745763,19,0.904761905,1,results,Experiments: There are 6 different ablative settings in total .,face_detection21
1900,1900,1900,264,"Our non - top K suppression is very close to using all candidates , and achieved consistently better results than NMS under the same number of candidates .",Experiments,There are 6 different ablative settings in total .,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6), (7, 9), (10, 11), (11, 13), (15, 16), (16, 19), (19, 20), (20, 21), (21, 22), (23, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.657894737,263,0.891525424,21,1,1,results,Experiments: There are 6 different ablative settings in total .,face_detection21
1901,1901,1901,285,"On the FDDB dataset , we compare with all public methods .",Experiments,Comparing with state - of - the - art,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 8), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.934210526,284,0.962711864,20,0.8,1,results,Experiments: Comparing with state - of - the - art,face_detection21
1902,1902,1902,287,"On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .",Experiments,Comparing with state - of - the - art,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7), (9, 11), (14, 18), (33, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.960526316,286,0.969491525,22,0.88,1,baselines,Experiments: Comparing with state - of - the - art,face_detection21
1903,1903,1903,288,"Headhunter ; ( 3 ) commercial system , e.g. face.com , Face ++ and Picasa .",Experiments,Comparing with state - of - the - art,face_detection,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.973684211,287,0.972881356,23,0.92,1,baselines,Experiments: Comparing with state - of - the - art,face_detection21
1904,1904,1904,4,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .",abstract,abstract,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.007334963,1,0.125,1,research-problem,abstract,face_detection3
1905,1905,1905,14,It is the first step in automatic face recognition applications .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.009478673,13,0.031784841,2,0.043478261,1,research-problem,INTRODUCTION,face_detection3
1906,1906,1906,19,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 13), (13, 14), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.033175355,18,0.04400978,7,0.152173913,1,research-problem,INTRODUCTION,face_detection3
1907,1907,1907,34,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 10), (11, 12), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.104265403,33,0.080684597,22,0.47826087,1,model,INTRODUCTION,face_detection3
1908,1908,1908,35,"An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values , in the same form as the Weber Fraction in experimental psychology .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 7), (7, 8), (9, 10), (10, 11), (11, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.109004739,34,0.083129584,23,0.5,1,model,INTRODUCTION,face_detection3
1909,1909,1909,36,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (8, 10), (10, 12), (13, 14), (16, 17), (17, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.113744076,35,0.085574572,24,0.52173913,1,model,INTRODUCTION,face_detection3
1910,1910,1910,37,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (8, 10), (11, 13), (16, 17), (17, 20), (22, 25), (25, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.118483412,36,0.08801956,25,0.543478261,1,model,INTRODUCTION,face_detection3
1911,1911,1911,38,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 10), (11, 12), (13, 19), (19, 21), (21, 24), (25, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.123222749,37,0.090464548,26,0.565217391,1,model,INTRODUCTION,face_detection3
1912,1912,1912,40,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 8), (10, 13), (13, 15), (15, 16), (17, 19), (22, 25), (25, 26), (27, 30), (32, 34), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.132701422,39,0.095354523,28,0.608695652,1,model,INTRODUCTION,face_detection3
1913,1913,1913,41,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 9), (9, 11), (11, 14), (14, 15), (16, 18), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (25, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.137440758,40,0.097799511,29,0.630434783,1,model,INTRODUCTION,face_detection3
1914,1914,1914,42,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .",INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (9, 15), (20, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.142180095,41,0.100244499,30,0.652173913,1,model,INTRODUCTION,face_detection3
1915,1915,1915,52,The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / .,INTRODUCTION,INTRODUCTION,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.18957346,51,0.124694377,40,0.869565217,1,code,INTRODUCTION,face_detection3
1916,1916,1916,232,We used a detection template of 24 24 pixels .,Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.347826087,231,0.564792176,8,0.5,1,hyperparameters,Implementation Details,face_detection3
1917,1917,1917,233,"We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 9), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.391304348,232,0.567237164,9,0.5625,1,hyperparameters,Implementation Details,face_detection3
1918,1918,1918,234,"In the soft cascade training , we set the threshold of each exit as the minimal score of positive samples , i.e. we did not reject positive samples during training .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (9, 10), (10, 11), (11, 13), (13, 14), (15, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.434782609,233,0.569682152,10,0.625,1,hyperparameters,Implementation Details,face_detection3
1919,1919,1919,235,"Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 8), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.47826087,234,0.572127139,11,0.6875,1,hyperparameters,Implementation Details,face_detection3
1920,1920,1920,237,"For an analysis , we also trained a near frontal face detector using the proposed NPD features and the classic cascade of regression trees ( CART ) with depth of four .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 12), (12, 13), (14, 17), (19, 27), (27, 28), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.565217391,236,0.577017115,13,0.8125,1,baselines,Implementation Details,face_detection3
1921,1921,1921,238,"A subset of the training data 2 in was used , including 12,102 face images and 12,315 nonface images .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 6), (11, 12), (12, 15), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.608695652,237,0.579462103,14,0.875,1,hyperparameters,Implementation Details,face_detection3
1922,1922,1922,239,The detection template is 20 20 pixels .,Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.652173913,238,0.58190709,15,0.9375,1,hyperparameters,Implementation Details,face_detection3
1923,1923,1923,240,"The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .",Implementation Details,Implementation Details,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (8, 9), (9, 11), (13, 17), (17, 18), (18, 19), (20, 21), (22, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.695652174,239,0.584352078,16,1,1,hyperparameters,Implementation Details,face_detection3
1924,1924,1924,254,Evaluation on FDDB Database,,,face_detection,3,"['O', 'O', 'O', 'O']","[(2, 4)]","['O', 'O', 'O', 'O']",0,0,253,0.618581907,0,0,1,results,,face_detection3
1925,1925,1925,265,"It can be observed that the proposed method outperforms most of the baseline methods except four methods , , , published recently .",Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (6, 8), (8, 9), (9, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.275,264,0.645476773,11,0.275,1,results,Evaluation on FDDB Database,face_detection3
1926,1926,1926,266,The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (7, 10), (10, 11), (11, 14), (14, 15), (16, 18), (20, 23), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3,265,0.64792176,12,0.3,1,results,Evaluation on FDDB Database,face_detection3
1927,1927,1927,274,"It can be observed that the proposed NPD detector is among the top performers for the discrete metric , though it is not as good as the four recent methods for the continuous metric .",Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 9), (10, 11), (12, 14), (14, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5,273,0.667481663,20,0.5,1,results,Evaluation on FDDB Database,face_detection3
1928,1928,1928,278,"Compared to recent methods , the Joint Cascade algorithm is the most competitive one to us in terms of accuracy and speed ( see Sec. 5.6 ) .",Evaluation on FDDB Database,Evaluation on FDDB Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 10), (11, 14), (16, 19), (19, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.6,277,0.677261614,24,0.6,1,results,Evaluation on FDDB Database,face_detection3
1929,1929,1929,295,Evaluation on GENKI Database,,,face_detection,3,"['O', 'O', 'O', 'O']","[(2, 4)]","['O', 'O', 'O', 'O']",0,0,294,0.718826406,0,0,1,results,,face_detection3
1930,1930,1930,307,The results show that the proposed NPD face detector significantly outperforms both the Viola - Jones and PittPatt face detectors .,Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 9), (9, 11), (13, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.193548387,306,0.748166259,12,0.428571429,1,results,Evaluation on GENKI Database,face_detection3
1931,1931,1931,308,Evaluation on CMU - MIT Database,Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O']","[(2, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",13,0.209677419,307,0.750611247,13,0.464285714,1,results,Evaluation on GENKI Database,face_detection3
1932,1932,1932,314,"The results show that , compared to the Viola - Jones frontal face detector , the NPD detector performs better when the number of false positives , FP < 50 , while it is slightly worse than Viola - Jones at higher FPs .",Evaluation on GENKI Database,Evaluation on GENKI Database,face_detection,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 7), (8, 14), (16, 18), (18, 19), (19, 20), (20, 21), (34, 36), (36, 37), (37, 40), (40, 41), (41, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.306451613,313,0.765281174,19,0.678571429,1,results,Evaluation on GENKI Database,face_detection3
1933,1933,1933,4,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .",abstract,abstract,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,3,0.008849558,1,0.0625,1,research-problem,abstract,face_detection4
1934,1934,1934,42,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .",Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.488888889,41,0.120943953,22,0.488888889,1,model,Introduction,face_detection4
1935,1935,1935,43,The proposed method is inspired by the one - stage and multi-scale object detection method SSD which also enlightens some other face detectors .,Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.511111111,42,0.123893805,23,0.511111111,1,model,Introduction,face_detection4
1936,1936,1936,44,One of the characteristics of SSD is that pre-defined anchor boxes are manually designed for each detection branch .,Introduction,Introduction,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (8, 11), (12, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.533333333,43,0.126843658,24,0.533333333,1,model,Introduction,face_detection4
1937,1937,1937,227,We initialize all parameters with xavier method and train the network from scratch .,Training parameters .,Training parameters .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 7), (8, 9), (10, 11), (11, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.111111111,226,0.666666667,31,0.794871795,1,experimental-setup,Training parameters .,face_detection4
1938,1938,1938,229,"The optimization method is SGD with 0.9 momentum , zero weight decay and batch size 32 .",Training parameters .,Training parameters .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (6, 8), (9, 12), (13, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.333333333,228,0.672566372,33,0.846153846,1,experimental-setup,Training parameters .,face_detection4
1939,1939,1939,232,The initial learning rate is 0.1 .,Training parameters .,,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.666666667,231,0.681415929,36,0.923076923,1,experimental-setup,Training parameters .,face_detection4
1940,1940,1940,233,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .",Training parameters .,The initial learning rate is 0.1 .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 6), (7, 9), (9, 11), (11, 12), (12, 13), (13, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.777777778,232,0.684365782,37,0.948717949,1,experimental-setup,Training parameters .: The initial learning rate is 0.1 .,face_detection4
1941,1941,1941,234,The training time is about 5 days with two NVIDIA GTX 1080 TI .,Training parameters .,The initial learning rate is 0.1 .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (7, 8), (8, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.888888889,233,0.687315634,38,0.974358974,1,experimental-setup,Training parameters .: The initial learning rate is 0.1 .,face_detection4
1942,1942,1942,235,Our method is implemented using MXNet and the source code is released .,Training parameters .,The initial learning rate is 0.1 .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,1,234,0.690265487,39,1,1,experimental-setup,Training parameters .: The initial learning rate is 0.1 .,face_detection4
1943,1943,1943,253,"Finally , the following methods are taken for comparison : DSFD ( Resnet152 backbone ) , Pyramid Box ( VGG16 backbone ) , S3 FD ( VGG16 backbone ) , SSH ( VGG16 backbone ) and FaceBoxes .",Evaluation on Benchmarks,Evaluation on Benchmarks,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 15), (16, 22), (23, 29), (30, 35), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.133333333,252,0.743362832,12,0.230769231,1,baselines,Evaluation on Benchmarks,face_detection4
1944,1944,1944,261,FDDB dataset .,Evaluation on Benchmarks,,face_detection,4,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",20,0.222222222,260,0.766961652,20,0.384615385,1,results,Evaluation on Benchmarks,face_detection4
1945,1945,1945,268,"DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .",Evaluation on Benchmarks,And the second criterion directly uses IOU ratios .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10), (10, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3,267,0.787610619,27,0.519230769,1,results,Evaluation on Benchmarks: And the second criterion directly uses IOU ratios .,face_detection4
1946,1946,1946,269,"The proposed LFFD gains slightly lower accuracy than the first four methods , but outperforms FaceBoxes evidently .",Evaluation on Benchmarks,And the second criterion directly uses IOU ratios .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (7, 8), (9, 12), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.311111111,268,0.790560472,28,0.538461538,1,results,Evaluation on Benchmarks: And the second criterion directly uses IOU ratios .,face_detection4
1947,1947,1947,270,The results indicate that LFFD is superior for detecting unconstrained faces .,Evaluation on Benchmarks,And the second criterion directly uses IOU ratios .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (6, 7), (7, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.322222222,269,0.793510324,29,0.557692308,1,results,Evaluation on Benchmarks: And the second criterion directly uses IOU ratios .,face_detection4
1948,1948,1948,271,WIDER FACE dataset .,Evaluation on Benchmarks,,face_detection,4,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",30,0.333333333,270,0.796460177,30,0.576923077,1,results,Evaluation on Benchmarks,face_detection4
1949,1949,1949,282,"Firstly , performance drop is evident for DSFD , PyramidBox , S3FD and SSH compared to their original results .",Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 6), (6, 7), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.455555556,281,0.828908555,41,0.788461538,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
1950,1950,1950,285,"Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .",Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (6, 8), (8, 9), (9, 11), (14, 15), (15, 16), (16, 17), (17, 18), (18, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.488888889,284,0.837758112,44,0.846153846,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
1951,1951,1951,286,"Thirdly , FaceBoxes does not get desirable results on Medium and Hard parts .",Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (6, 8), (8, 9), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.5,285,0.840707965,45,0.865384615,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
1952,1952,1952,292,"Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .",Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 8), (8, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.566666667,291,0.85840708,51,0.980769231,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
1953,1953,1953,293,"Additionally , LFFD is better than SSH that uses VGG16 as the backbone on Hard parts .",Evaluation on Benchmarks,Some observations can be made .,face_detection,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 9), (9, 10), (10, 11), (12, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.577777778,292,0.861356932,52,1,1,results,Evaluation on Benchmarks: Some observations can be made .,face_detection4
1954,1954,1954,5,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",abstract,abstract,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.333333333,4,0.026315789,2,0.25,1,research-problem,abstract,face_detection5
1955,1955,1955,30,"However , most of the available face detection and face alignment methods ignore the inherent correlation between these two tasks .",I. INTRODUCTION,"Recently ,",face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.514285714,29,0.190789474,18,0.375,1,research-problem,"I. INTRODUCTION: Recently ,",face_detection5
1956,1956,1956,40,"In this paper , we propose a new framework to integrate these two tasks using unified cascaded CNNs by multi-task learning .",I. INTRODUCTION,"Recently ,",face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 11), (12, 14), (14, 15), (15, 18), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.8,39,0.256578947,28,0.583333333,1,model,"I. INTRODUCTION: Recently ,",face_detection5
1957,1957,1957,41,The proposed CNNs consist of three stages .,I. INTRODUCTION,"Recently ,",face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.828571429,40,0.263157895,29,0.604166667,1,model,"I. INTRODUCTION: Recently ,",face_detection5
1958,1958,1958,42,"In the first stage , it produces candidate windows quickly through a shallow CNN .",I. INTRODUCTION,"Recently ,",face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (7, 9), (9, 10), (10, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.857142857,41,0.269736842,30,0.625,1,model,"I. INTRODUCTION: Recently ,",face_detection5
1959,1959,1959,43,"Then , it refines the windows to reject a large number of non-faces windows through a more complex CNN .",I. INTRODUCTION,"Recently ,",face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 6), (6, 8), (9, 14), (14, 15), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.885714286,42,0.276315789,31,0.645833333,1,model,"I. INTRODUCTION: Recently ,",face_detection5
1960,1960,1960,44,"Finally , it uses a more powerful CNN to refine the result and output facial landmarks positions .",I. INTRODUCTION,"Recently ,",face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 10), (11, 12), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.914285714,43,0.282894737,32,0.666666667,1,model,"I. INTRODUCTION: Recently ,",face_detection5
1961,1961,1961,108,"Then we compare our face detector and alignment against the state - of - the - art methods in Face Detection Data Set and Benchmark ( FDDB ) , WIDER FACE , and Annotated Facial Landmarks in the Wild ( AFLW ) benchmark .",EXPERIMENTS,EXPERIMENTS,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8), (8, 9), (10, 18), (18, 19), (19, 28), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,107,0.703947368,38,0.904761905,1,experiments,EXPERIMENTS,face_detection5
1962,1962,1962,122,The effectiveness of online hard sample mining,A. Training Data,B .,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.257142857,121,0.796052632,9,0.473684211,1,experiments,A. Training Data: B .,face_detection5
1963,1963,1963,128,It is very clear that the hard sample mining is beneficial to performance improvement .,A. Training Data,B .,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (10, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.428571429,127,0.835526316,15,0.789473684,1,results,A. Training Data: B .,face_detection5
1964,1964,1964,130,The effectiveness of joint detection and alignment,A. Training Data,C.,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.485714286,129,0.848684211,17,0.894736842,1,experiments,A. Training Data: C.,face_detection5
1965,1965,1965,132,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,A. Training Data,C.,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.542857143,131,0.861842105,19,1,1,results,A. Training Data: C.,face_detection5
1966,1966,1966,133,D. Evaluation on face detection,A. Training Data,C.,face_detection,5,"['O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O']",20,0.571428571,132,0.868421053,0,0,1,experiments,A. Training Data: C.,face_detection5
1967,1967,1967,135,( a ) - ( d ) shows that our method consistently outperforms all the previous approaches by a large margin in both the benchmarks .,A. Training Data,C.,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 11), (11, 13), (13, 17), (17, 18), (19, 21), (21, 22), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.628571429,134,0.881578947,2,0.2,1,results,A. Training Data: C.,face_detection5
1968,1968,1968,138,Evaluation on face alignment,A. Training Data,E .,face_detection,5,"['O', 'O', 'O', 'O']","[(2, 4)]","['O', 'O', 'O', 'O']",25,0.714285714,137,0.901315789,5,0.5,1,experiments,A. Training Data: E .,face_detection5
1969,1969,1969,143,( e ) shows that our method outperforms all the state - of - the - art methods with a margin .,A. Training Data,E .,face_detection,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (7, 8), (8, 18), (18, 19), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.857142857,142,0.934210526,10,1,1,results,A. Training Data: E .,face_detection5
1970,1970,1970,2,Robust Face Detection via Learning Small Faces on Hard Images,title,title,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004032258,1,0,1,research-problem,title,face_detection6
1971,1971,1971,13,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.027777778,12,0.048387097,1,0.027777778,1,research-problem,Introduction,face_detection6
1972,1972,1972,27,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 12), (12, 14), (14, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.416666667,26,0.10483871,15,0.416666667,1,approach,Introduction,face_detection6
1973,1973,1973,28,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 10), (10, 11), (11, 13), (13, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.444444444,27,0.108870968,16,0.444444444,1,approach,Introduction,face_detection6
1974,1974,1974,29,This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process .,Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 8), (10, 13), (13, 16), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.472222222,28,0.112903226,17,0.472222222,1,approach,Introduction,face_detection6
1975,1975,1975,31,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12), (13, 15), (15, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.527777778,30,0.120967742,19,0.527777778,1,approach,Introduction,face_detection6
1976,1976,1976,34,"Compared with these methods , our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training .",Introduction,Introduction,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.611111111,33,0.133064516,22,0.611111111,1,approach,Introduction,face_detection6
1977,1977,1977,165,"We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 9), (9, 12), (18, 19), (19, 21), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,164,0.661290323,4,0.4,1,experimental-setup,Experimental settings,face_detection6
1978,1978,1978,166,"We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 7), (7, 9), (9, 10), (11, 12), (12, 15), (16, 17), (18, 20), (20, 21), (21, 22), (26, 30), (30, 31), (32, 35), (35, 36), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,165,0.665322581,5,0.5,1,experimental-setup,Experimental settings,face_detection6
1979,1979,1979,167,"During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 7), (7, 11), (12, 13), (14, 15), (16, 17), (17, 18), (18, 20), (20, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,166,0.669354839,6,0.6,1,experimental-setup,Experimental settings,face_detection6
1980,1980,1980,168,"The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (6, 7), (7, 9), (10, 11), (14, 16), (16, 17), (17, 18), (19, 22), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,167,0.673387097,7,0.7,1,experimental-setup,Experimental settings,face_detection6
1981,1981,1981,170,"Specifically , we resize the testing image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels for evaluation on WIDER FACE dataset .",Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (7, 9), (10, 12), (12, 13), (13, 23), (25, 26), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,169,0.681451613,9,0.9,1,experimental-setup,Experimental settings,face_detection6
1982,1982,1982,171,We also follow the testing strategies used in Pyra - midBox 2 such as horizontal flip and bounding - box voting .,Experimental settings,Experimental settings,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 8), (8, 11), (12, 14), (14, 16), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1,170,0.685483871,10,1,1,experimental-setup,Experimental settings,face_detection6
1983,1983,1983,174,"In , we show the precision - recall ( PR ) curve and average precision ( AP ) for our model compared with many other state - of - the - arts on these three subsets .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 12), (13, 18), (18, 19), (19, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.029411765,173,0.697580645,2,0.133333333,1,results,Experiment results,face_detection6
1984,1984,1984,175,"As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (9, 11), (11, 12), (13, 15), (17, 18), (19, 27), (27, 28), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.044117647,174,0.701612903,3,0.2,1,results,Experiment results,face_detection6
1985,1985,1985,177,"Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - thearts .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (7, 9), (10, 19), (21, 22), (22, 23), (24, 26), (26, 27), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.073529412,176,0.709677419,5,0.333333333,1,results,Experiment results,face_detection6
1986,1986,1986,183,"We show the discontinuous ROC curve at compared with , and our method achieves the state - of - the - art performance of TPR = 98.7 % given 1000 false positives .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (11, 13), (13, 14), (15, 23), (23, 24), (24, 28), (28, 29), (29, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.161764706,182,0.733870968,11,0.733333333,1,results,Experiment results,face_detection6
1987,1987,1987,185,"We show the PR curve at compared with , and our method achieves a new the state - of - the - art performance of AP = 99.0 .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (10, 12), (12, 13), (14, 15), (24, 25), (25, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.191176471,184,0.741935484,13,0.866666667,1,results,Experiment results,face_detection6
1988,1988,1988,187,"As shown in compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (8, 9), (9, 20), (21, 22), (23, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.220588235,186,0.75,15,1,1,results,Experiment results,face_detection6
1989,1989,1989,196,"Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 7), (7, 8), (8, 9), (9, 10), (11, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.352941176,195,0.786290323,8,0.615384615,1,results,Experiment results,face_detection6
1990,1990,1990,199,HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead .,Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (4, 5), (5, 6), (6, 8), (8, 9), (9, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.397058824,198,0.798387097,11,0.846153846,1,results,Experiment results,face_detection6
1991,1991,1991,200,"DH itself can also boost the performance , which shows the effectiveness of designing larger convolution for larger anchors .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (4, 5), (6, 7), (9, 10), (11, 12), (12, 14), (14, 16), (16, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.411764706,199,0.802419355,12,0.923076923,1,results,Experiment results,face_detection6
1992,1992,1992,201,Combining HIM and DH together can improve further towards the state - of - the - art performance .,Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (1, 5), (8, 9), (10, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.426470588,200,0.806451613,13,1,1,results,Experiment results,face_detection6
1993,1993,1993,211,Both photometric distortion and cropping can contribute to a more robust face detector .,Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 8), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.573529412,210,0.846774194,3,1,1,ablation-analysis,Experiment results,face_detection6
1994,1994,1994,219,"Diagnosis of multi-scale testing. , the extra small scales are crucial to detect easy faces .",Experiment results,Experiment results,face_detection,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.691176471,218,0.879032258,7,0.777777778,1,results,Experiment results,face_detection6
1995,1995,1995,2,Recurrent Scale Approximation for Object Detection in CNN,title,,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003597122,1,0,1,research-problem,title,face_detection7
1996,1996,1996,4,"Since convolutional neural network ( CNN ) lacks an inherent mechanism to handle large scale variations , we always need to compute feature maps multiple times for multiscale object detection , which has the bottleneck of computational cost in practice .",abstract,abstract,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(22, 24), (24, 26), (26, 27), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.111111111,3,0.010791367,1,0.111111111,1,research-problem,abstract,face_detection7
1997,1997,1997,17,"Our codes and annotations mentioned in Sec.4.1 can be accessed at github.com/sciencefans/RSA-for-object-detection dle the variations caused by appearance , location and scale .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.105263158,16,0.057553957,4,0.105263158,1,code,Introduction,face_detection7
1998,1998,1998,35,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .",Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.578947368,34,0.122302158,22,0.578947368,1,model,Introduction,face_detection7
1999,1999,1999,36,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (7, 9), (9, 12), (12, 13), (14, 15), (16, 18), (18, 20), (21, 24), (24, 25), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.605263158,35,0.125899281,23,0.605263158,1,model,Introduction,face_detection7
2000,2000,2000,37,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 5), (5, 6), (7, 9), (9, 11), (12, 13), (13, 14), (15, 17), (17, 19), (19, 22), (22, 23), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.631578947,36,0.129496403,24,0.631578947,1,model,Introduction,face_detection7
2001,2001,2001,38,Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit - a balance considering both efficiency and accuracy .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 7), (7, 8), (8, 9), (9, 10), (10, 13), (14, 15), (16, 18), (18, 19), (19, 21), (21, 22), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.657894737,37,0.133093525,25,0.657894737,1,model,Introduction,face_detection7
2002,2002,2002,40,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 8), (8, 11), (11, 13), (13, 14), (15, 17), (19, 20), (20, 22), (22, 23), (25, 29), (29, 31), (32, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.710526316,39,0.14028777,27,0.710526316,1,model,Introduction,face_detection7
2003,2003,2003,42,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (8, 9), (10, 11), (11, 12), (13, 15), (15, 16), (17, 19), (20, 21), (22, 24), (24, 25), (25, 27), (27, 29), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.763157895,41,0.147482014,29,0.763157895,1,model,Introduction,face_detection7
2004,2004,2004,43,The final score of identifying a face within an anchor is thereby revised by the LRN network .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (6, 7), (7, 8), (9, 10), (12, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.789473684,42,0.151079137,30,0.789473684,1,model,Introduction,face_detection7
2005,2005,2005,46,The three components can be incorporated into a unified CNN framework and trained end - to - end .,Introduction,Introduction,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 7), (8, 11), (12, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.868421053,45,0.161870504,33,0.868421053,1,model,Introduction,face_detection7
2006,2006,2006,178,"The structure of our model is a shallow version of the ResNet where the first seven ResNet blocks are used , i.e. , from conv1 to res3c .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (5, 6), (7, 9), (9, 10), (11, 12), (12, 13), (14, 18), (19, 20), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.097087379,177,0.636690647,7,0.25,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2007,2007,2007,179,We use this model in scale - forecast network and LRN .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 9), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.106796117,178,0.64028777,8,0.285714286,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2008,2008,2008,180,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (5, 7), (7, 8), (8, 9), (10, 13), (14, 15), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.116504854,179,0.643884892,9,0.321428571,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2009,2009,2009,181,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8), (10, 11), (12, 13), (13, 14), (14, 16), (16, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.126213592,180,0.647482014,10,0.357142857,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2010,2010,2010,183,The ratio of the positive and the negative is 1 : 1 in all experiments .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 8), (8, 9), (9, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.145631068,182,0.654676259,12,0.428571429,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2011,2011,2011,184,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (6, 9), (10, 12), (12, 13), (13, 14), (15, 16), (16, 17), (17, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.155339806,183,0.658273381,13,0.464285714,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2012,2012,2012,185,"The maximum training iteration is 1,000,000 .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.165048544,184,0.661870504,14,0.5,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2013,2013,2013,186,We use stochastic gradient descent as the optimizer .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.174757282,185,0.665467626,15,0.535714286,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2014,2014,2014,187,The scale - forecast network is of vital importance to the computational cost and accuracy in the networks afterwards .,Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.184466019,186,0.669064748,16,0.571428571,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2015,2015,2015,190,"We can observe from the results that our trained scale network recalls almost 99 % at x = 1 , indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (7, 11), (11, 12), (12, 15), (15, 16), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.213592233,189,0.679856115,19,0.678571429,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2016,2016,2016,194,"We can conclude that the deeper the RSA is branched out , the worse the feature approximation at smaller scales will be .",Setup and Implementation Details,Face Detection,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 6), (7, 8), (9, 11), (13, 14), (15, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.252427184,193,0.694244604,23,0.821428571,1,experiments,Setup and Implementation Details: Face Detection,face_detection7
2017,2017,2017,206,Theoretically RSA can handle all scales of features in a deep CNN model and therefore can be branched out at any depth of the network .,Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (7, 8), (8, 9), (10, 13), (17, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.368932039,205,0.737410072,6,0.25,1,baselines,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2018,2018,2018,214,The minimum operation in each component means only the scaleforecast network is used where no face appears in the image ; and the maximum operation indicates the amount when faces appear at all scales .,Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (6, 7), (9, 11), (12, 14), (14, 16), (16, 18), (19, 20), (23, 25), (25, 26), (27, 28), (28, 29), (29, 30), (30, 32), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.446601942,213,0.76618705,14,0.583333333,1,hyperparameters,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2019,2019,2019,219,Most of the computation happens before layer res2 b and it has an acceptable error rate of 3.44 % .,Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (6, 9), (13, 16), (16, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.495145631,218,0.784172662,19,0.791666667,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2020,2020,2020,222,"For a particular case , as the times of the recurrent operation increase , the error rate goes up due to the cumulative effect of rolling out the predictions .",Setup and Implementation Details,Ablative Evaluation on RSA Unit,face_detection,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 9), (10, 12), (12, 13), (15, 17), (17, 19), (19, 21), (22, 24), (24, 27), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.524271845,221,0.794964029,22,0.916666667,1,experiments,Setup and Implementation Details: Ablative Evaluation on RSA Unit,face_detection7
2021,2021,2021,2,Detecting Faces Using Region - based Fully Convolutional Networks,title,,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.006410256,1,0,1,research-problem,title,face_detection8
2022,2022,2022,4,Face detection has achieved great success using the region - based methods .,abstract,abstract,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.019230769,1,0.166666667,1,research-problem,abstract,face_detection8
2023,2023,2023,22,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 13), (13, 16), (16, 17), (24, 25), (25, 28), (28, 29), (30, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.545454545,21,0.134615385,12,0.545454545,1,model,Introduction,face_detection8
2024,2024,2024,23,"According to the size of the general face , we carefully design size of anchors and RoIs .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (4, 5), (6, 8), (10, 12), (12, 13), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.590909091,22,0.141025641,13,0.590909091,1,model,Introduction,face_detection8
2025,2025,2025,24,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (14, 19), (19, 21), (21, 23), (23, 25), (25, 26), (28, 29), (30, 31), (32, 34), (34, 35), (35, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.636363636,23,0.147435897,14,0.636363636,1,model,Introduction,face_detection8
2026,2026,2026,25,"Furthermore , we also apply the multi-scale training and testing strategy in this work .",Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.681818182,24,0.153846154,15,0.681818182,1,model,Introduction,face_detection8
2027,2027,2027,26,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,Introduction,Introduction,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 11), (12, 14), (14, 16), (18, 20), (21, 22), (22, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.727272727,25,0.16025641,16,0.727272727,1,model,Introduction,face_detection8
2028,2028,2028,107,Our training hyper - parameters are similar to Face R - CNN .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 8), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.024390244,106,0.679487179,1,0.066666667,1,hyperparameters,Implementation Details,face_detection8
2029,2029,2029,108,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 11), (11, 12), (13, 15), (15, 16), (16, 20), (20, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.048780488,107,0.685897436,2,0.133333333,1,hyperparameters,Implementation Details,face_detection8
2030,2030,2030,109,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (9, 10), (16, 17), (18, 20), (20, 21), (22, 25), (27, 29), (30, 33), (33, 35), (35, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.073170732,108,0.692307692,3,0.2,1,hyperparameters,Implementation Details,face_detection8
2031,2031,2031,110,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 6), (7, 11), (11, 12), (12, 14), (14, 15), (16, 17), (19, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.097560976,109,0.698717949,4,0.266666667,1,baselines,Implementation Details,face_detection8
2032,2032,2032,111,We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 10), (11, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.12195122,110,0.705128205,5,0.333333333,1,hyperparameters,Implementation Details,face_detection8
2033,2033,2033,115,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (8, 11), (12, 14), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.219512195,114,0.730769231,9,0.6,1,hyperparameters,Implementation Details,face_detection8
2034,2034,2034,116,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6), (11, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.243902439,115,0.737179487,10,0.666666667,1,hyperparameters,Implementation Details,face_detection8
2035,2035,2035,118,We set the 256 for the size of RPN mini-batch and 128 for R - FCN respectively .,Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 7), (7, 8), (8, 10), (11, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.292682927,117,0.75,12,0.8,1,hyperparameters,Implementation Details,face_detection8
2036,2036,2036,120,"We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (9, 11), (11, 13), (13, 14), (14, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.341463415,119,0.762820513,14,0.933333333,1,hyperparameters,Implementation Details,face_detection8
2037,2037,2037,121,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .",Implementation Details,Implementation Details,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 7), (8, 10), (10, 12), (12, 13), (14, 16), (16, 17), (17, 19), (19, 20), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.365853659,120,0.769230769,15,1,1,hyperparameters,Implementation Details,face_detection8
2038,2038,2038,125,"As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .",Implementation Details,Comparison on Benchmarks,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (10, 12), (12, 13), (14, 16), (16, 17), (19, 24), (24, 25), (25, 27), (28, 30), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.463414634,124,0.794871795,2,0.666666667,1,results,Implementation Details: Comparison on Benchmarks,face_detection8
2039,2039,2039,126,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .",Implementation Details,Comparison on Benchmarks,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 8), (9, 11), (12, 14), (15, 20), (20, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.487804878,125,0.801282051,3,1,1,results,Implementation Details: Comparison on Benchmarks,face_detection8
2040,2040,2040,127,FDDB,Implementation Details,,face_detection,8,['O'],"[(0, 1)]",['O'],21,0.512195122,126,0.807692308,0,0,1,experiments,Implementation Details,face_detection8
2041,2041,2041,133,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .",Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 10), (10, 12), (13, 15), (15, 18), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.658536585,132,0.846153846,6,0.3,1,experiments,Implementation Details: FDDB,face_detection8
2042,2042,2042,134,Our discrete ROC curve is superior to the prior best - performing method .,Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (8, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.682926829,133,0.852564103,7,0.35,1,experiments,Implementation Details: FDDB,face_detection8
2043,2043,2043,135,We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 8), (8, 9), (10, 13), (13, 14), (14, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.707317073,134,0.858974359,8,0.4,1,experiments,Implementation Details: FDDB,face_detection8
2044,2044,2044,141,"Face R - FCN shows the superior performance over the prior methods across the three subsets ( easy , medium and hard ) in both validation and test sets .",Implementation Details,FDDB,face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4), (4, 5), (6, 8), (8, 9), (10, 12), (12, 13), (14, 23), (23, 24), (25, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.853658537,140,0.897435897,14,0.7,1,experiments,Implementation Details: FDDB,face_detection8
2045,2045,2045,147,"Finally , we obtain the true positive rate 98. 99 % of the discrete ROC curve at 1000 false positives and 99. 42 % at 2000 false positives , which are new state - of - the - art among all the published methods on FDDB .",Implementation Details,"Furthermore ,",face_detection,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 11), (11, 12), (13, 16), (16, 17), (17, 20), (21, 24), (24, 25), (25, 28), (31, 39), (45, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,1,146,0.935897436,20,1,1,experiments,"Implementation Details: Furthermore ,",face_detection8
2046,2046,2046,2,Finding Tiny Faces,title,title,face_detection,9,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",1,0,1,0.003610108,1,0,1,research-problem,title,face_detection9
2047,2047,2047,7,"Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .",abstract,abstract,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.307692308,6,0.02166065,4,0.307692308,1,research-problem,abstract,face_detection9
2048,2048,2048,8,"We explore three aspects of the problem in the context of finding small faces : the role of scale invariance , image resolution , and contextual reasoning .",abstract,abstract,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (16, 17), (18, 20), (21, 23), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.384615385,7,0.025270758,5,0.384615385,1,research-problem,abstract,face_detection9
2049,2049,2049,26,"lem in the context of face detection : the role of scale invariance , image resolution and contextual reasoning .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (11, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.191489362,25,0.090252708,9,0.191489362,1,research-problem,Introduction,face_detection9
2050,2050,2050,33,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12), (12, 14), (14, 16), (16, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.340425532,32,0.115523466,16,0.340425532,1,model,Introduction,face_detection9
2051,2051,2051,35,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 13), (13, 14), (15, 17), (19, 22), (22, 23), (23, 25), (25, 27), (27, 28), (28, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.382978723,34,0.122743682,18,0.382978723,1,model,Introduction,face_detection9
2052,2052,2052,41,"To extend features fine - tuned from these networks to objects of novel sizes , we employ a simply strategy : resize images at test - time by interpolation and decimation .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (9, 10), (10, 11), (16, 17), (18, 20), (21, 22), (22, 23), (23, 24), (24, 27), (27, 28), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.510638298,40,0.144404332,24,0.510638298,1,model,Introduction,face_detection9
2053,2053,2053,42,"While many recognition systems are applied in a "" multi-resolution "" fashion by processing an image pyramid , we find that interpolating the lowest layer of the pyramid is particularly crucial for finding small objects .",Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(21, 22), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.531914894,41,0.14801444,25,0.531914894,1,research-problem,Introduction,face_detection9
2054,2054,2054,43,Hence our final approach is a delicate mixture of scale - specific detectors that are used in a scale - invariant fashion ( by processing an image pyramid to capture large scale variations ) .,Introduction,Introduction,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (6, 8), (8, 9), (9, 13), (15, 17), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.553191489,42,0.151624549,26,0.553191489,1,model,Introduction,face_detection9
2055,2055,2055,52,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .",Introduction,"This is often formulated as "" context "" .",face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 8), (8, 10), (19, 20), (20, 25), (25, 27), (28, 32), (33, 38), (38, 39), (39, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.744680851,51,0.184115523,35,0.744680851,1,model,"Introduction: This is often formulated as "" context "" .",face_detection9
2056,2056,2056,53,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,Introduction,"This is often formulated as "" context "" .",face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 9), (10, 12), (12, 15), (17, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.765957447,52,0.187725632,36,0.765957447,1,model,"Introduction: This is often formulated as "" context "" .",face_detection9
2057,2057,2057,218,WIDER FACE :,Experiments,Experiments,face_detection,9,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",1,0.025641026,217,0.783393502,1,0.027027027,1,results,Experiments,face_detection9
2058,2058,2058,220,"As shows , our hybrid - resolution model ( HR ) achieves state - of - the - art performance on all difficulty levels , but most importantly , reduces error on the "" hard "" set by 2X .",Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 11), (11, 12), (12, 20), (20, 21), (21, 24), (29, 30), (30, 31), (31, 32), (38, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.076923077,219,0.790613718,3,0.081081081,1,results,Experiments,face_detection9
2059,2059,2059,224,FDDB :,Experiments,Experiments,face_detection,9,"['O', 'O']","[(0, 1)]","['O', 'O']",7,0.179487179,223,0.805054152,7,0.189189189,1,baselines,Experiments,face_detection9
2060,2060,2060,226,"Our out - of - the - box detector ( HR ) outperforms all published results on the discrete score , which uses a standard 50 % intersection - over - union threshold to define correctness .",Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 12), (12, 13), (13, 16), (16, 17), (18, 20), (22, 23), (24, 33), (33, 35), (35, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.230769231,225,0.812274368,9,0.243243243,1,results,Experiments,face_detection9
2061,2061,2061,228,"With the post - hoc regressor , our detector achieves state - of - the - art performance on the continuous score ( measuring average bounding - box overlap ) as well .",Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (7, 9), (9, 10), (10, 18), (18, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.282051282,227,0.819494585,11,0.297297297,1,results,Experiments,face_detection9
2062,2062,2062,229,Our regressor is trained with 10 - fold cross validation .,Experiments,Experiments,face_detection,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.307692308,228,0.823104693,12,0.324324324,1,hyperparameters,Experiments,face_detection9
2063,2063,2063,2,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,title,title,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.0125,1,0,1,research-problem,title,hypernym_discovery0
2064,2064,2064,4,This paper describes a simple but competitive unsupervised system for hypernym discovery .,abstract,abstract,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.0375,1,0.166666667,1,research-problem,abstract,hypernym_discovery0
2065,2065,2065,20,The ADAPT team focused on the two specialised domain English subtasks by developing an unsupervised system that builds word embeddings from the supplied reference corpora for these domains .,Introduction,Introduction,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 11), (11, 13), (14, 16), (18, 20), (20, 21), (22, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.322580645,19,0.2375,10,0.5,1,model,Introduction,hypernym_discovery0
2066,2066,2066,25,"Even though unsupervised systems tend to rank behind supervised systems in NLP tasks in general , our motivation to focus on an unsupervised approach is derived from the fact that they do not require explicit hand - annotated data , and from the expectation that they are able to generalise more easily to unseen hypernym - hyponym pairs .",Introduction,Introduction,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.483870968,24,0.3,15,0.75,1,model,Introduction,hypernym_discovery0
2067,2067,2067,60,Results,,,hypernym_discovery,0,['O'],[],['O'],0,0,59,0.7375,0,0,1,experiments,,hypernym_discovery0
2068,2068,2068,61,Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 .,Results,Results,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 9), (9, 10), (11, 14), (14, 15), (16, 22), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,60,0.75,1,0.125,1,results,Results,hypernym_discovery0
2069,2069,2069,62,"However , it ranked first place among all the unsupervised systems on this subtask .",Results,Results,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (6, 7), (7, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,61,0.7625,2,0.25,1,results,Results,hypernym_discovery0
2070,2070,2070,63,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .",Results,Results,hypernym_discovery,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (7, 9), (9, 10), (10, 11), (15, 16), (17, 18), (18, 19), (19, 20), (21, 22), (22, 23), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.375,62,0.775,3,0.375,1,results,Results,hypernym_discovery0
2071,2071,2071,2,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings,title,title,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.009259259,1,0,1,research-problem,title,hypernym_discovery1
2072,2072,2072,4,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .",abstract,abstract,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.027777778,1,0.2,1,research-problem,abstract,hypernym_discovery1
2073,2073,2073,14,"A relevant well - known scenario is hypernym detection , which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not .",Introduction,Introduction,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.277777778,13,0.12037037,5,0.277777778,1,research-problem,Introduction,hypernym_discovery1
2074,2074,2074,23,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .",Introduction,Introduction,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 11), (12, 14), (15, 17), (17, 20), (20, 22), (23, 25), (25, 26), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.777777778,22,0.203703704,14,0.777777778,1,model,Introduction,hypernym_discovery1
2075,2075,2075,24,"In our system , we leverage an unambiguous vector representation via term embedding , and we take advantage of deep neural networks to discover the hypernym relationships between terms .",Introduction,Introduction,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 11), (11, 13), (16, 19), (19, 22), (22, 24), (25, 27), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.833333333,23,0.212962963,15,0.833333333,1,model,Introduction,hypernym_discovery1
2076,2076,2076,83,Our model was implemented using the Theano 1 .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,82,0.759259259,5,0.357142857,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2077,2077,2077,84,The diagonal variant of Ada - Grad is used for neural network training .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (8, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5,83,0.768518519,6,0.428571429,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2078,2078,2078,85,We tune the hyper - parameters with the following range of values : learning rate ?,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,84,0.777777778,7,0.5,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2079,2079,2079,86,"{ 1 e ? 3 , 1 e ? 2 } , dropout probability ? { 0.1 , 0.2 } , CNN filter width ? { 2 , 3 , 4 }.",Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 14), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,85,0.787037037,8,0.571428571,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2080,2080,2080,87,The hidden dimension of all neural models are 200 .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,86,0.796296296,9,0.642857143,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2081,2081,2081,88,The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (9, 15), (16, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.75,87,0.805555556,10,0.714285714,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2082,2082,2082,89,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .",Experiment,Setting,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 16), (17, 18), (18, 20), (20, 21), (21, 25), (25, 26), (26, 27), (28, 29), (29, 36), (36, 37), (37, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,88,0.814814815,11,0.785714286,1,experimental-setup,Experiment: Setting,hypernym_discovery1
2083,2083,2083,96,"Convolution or recurrent gated mechanisms in either CNN - based ( CNN , RCNN ) or RNN ( GRU , LSTM ) based neural networks could essentially be helpful of modeling the semantic connections between words in a phrase , and guide the networks to discover the hypernym relationships .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5), (5, 6), (7, 15), (16, 25), (29, 31), (32, 34), (34, 35), (35, 36), (36, 37), (38, 39), (41, 42), (43, 44), (44, 46), (47, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3,95,0.87962963,3,0.3,1,baselines,Result and analysis,hypernym_discovery1
2084,2084,2084,97,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 8), (8, 9), (9, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,96,0.888888889,4,0.4,1,results,Result and analysis,hypernym_discovery1
2085,2085,2085,98,"To investigate the performance of neural models on specific domains , we conduct experiments on medical and medicine subtask .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(14, 15), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,97,0.898148148,5,0.5,1,results,Result and analysis,hypernym_discovery1
2086,2086,2086,100,"All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 8), (8, 11), (11, 14), (15, 19), (20, 21), (21, 22), (22, 23), (23, 27), (27, 28), (28, 32), (32, 33), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,99,0.916666667,7,0.7,1,results,Result and analysis,hypernym_discovery1
2087,2087,2087,101,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .",Result and analysis,Result and analysis,hypernym_discovery,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 4), (6, 8), (8, 9), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,100,0.925925926,8,0.8,1,results,Result and analysis,hypernym_discovery1
2088,2088,2088,2,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection,title,title,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004065041,1,0,1,research-problem,title,hypernym_discovery2
2089,2089,2089,10,"In the last two decades , the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy .",Introduction,Introduction,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.009345794,9,0.036585366,1,0.05,1,research-problem,Introduction,hypernym_discovery2
2090,2090,2090,21,"In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection , using several distributional semantic models that differ by context type and feature weighting .",Introduction,Introduction,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 14), (14, 16), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.112149533,20,0.081300813,12,0.6,1,approach,Introduction,hypernym_discovery2
2091,2091,2091,24,"We analyze the performance of the measures in different settings and suggest a principled way to select the suitable measure , context type and feature weighting according to the task setting , yielding consistent performance across datasets .",Introduction,Introduction,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 7), (11, 12), (13, 15), (15, 17), (18, 20), (26, 28), (29, 31), (32, 33), (33, 35), (35, 36), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.140186916,23,0.093495935,15,0.75,1,approach,Introduction,hypernym_discovery2
2092,2092,2092,25,We also compare the unsupervised measures to the state - of - the - art supervised methods .,Introduction,Introduction,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (6, 7), (8, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.14953271,24,0.097560976,16,0.8,1,approach,Introduction,hypernym_discovery2
2093,2093,2093,117,Experiments,,,hypernym_discovery,2,['O'],"[(0, 1)]",['O'],0,0,116,0.471544715,0,0,1,experiments,,hypernym_discovery2
2094,2094,2094,118,Comparing Unsupervised Measures,Experiments,,hypernym_discovery,2,"['O', 'O', 'O']","[(1, 3)]","['O', 'O', 'O']",1,0.008403361,117,0.475609756,0,0,1,results,Experiments,hypernym_discovery2
2095,2095,2095,131,"The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .",Experiments,Comparing Unsupervised Measures,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (6, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.117647059,130,0.528455285,13,0.684210526,1,results,Experiments: Comparing Unsupervised Measures,hypernym_discovery2
2096,2096,2096,132,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .",Experiments,Comparing Unsupervised Measures,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (6, 7), (11, 13), (13, 16), (16, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.12605042,131,0.532520325,14,0.736842105,1,results,Experiments: Comparing Unsupervised Measures,hypernym_discovery2
2097,2097,2097,133,The new SLQS variants are on top of the list in many settings .,Experiments,Comparing Unsupervised Measures,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 8), (9, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.134453782,132,0.536585366,15,0.789473684,1,results,Experiments: Comparing Unsupervised Measures,hypernym_discovery2
2098,2098,2098,134,"In particular they perform well in discriminating hypernyms from symmetric relations ( antonymy , synonymy , coordination ) .",Experiments,Comparing Unsupervised Measures,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (5, 7), (7, 8), (8, 9), (9, 11), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.142857143,133,0.540650407,16,0.842105263,1,results,Experiments: Comparing Unsupervised Measures,hypernym_discovery2
2099,2099,2099,181,Comparison to State - of - the - art Supervised Methods,Experiments,Hypernym vs. Coordination,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.537815126,180,0.731707317,7,0.142857143,1,results,Experiments: Hypernym vs. Coordination,hypernym_discovery2
2100,2100,2100,191,"The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .",Experiments,Hypernym vs. Coordination,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (6, 8), (8, 9), (9, 11), (16, 18), (19, 21), (22, 24), (24, 25), (26, 27), (29, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.621848739,190,0.772357724,17,0.346938776,1,results,Experiments: Hypernym vs. Coordination,hypernym_discovery2
2101,2101,2101,192,"As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .",Experiments,Hypernym vs. Coordination,hypernym_discovery,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 8), (8, 9), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.630252101,191,0.776422764,18,0.367346939,1,results,Experiments: Hypernym vs. Coordination,hypernym_discovery2
2102,2102,2102,2,Supervised Distributional Hypernym Discovery via Domain Adaptation,title,,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00591716,1,0,1,research-problem,title,hypernym_discovery3
2103,2103,2103,18,"In addition , while not being taxonomy learning systems per se , semi-supervised systems for Information Extraction such as NELL rely crucially on taxonomized concepts and their relations within their learning process .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 17), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.347826087,17,0.100591716,8,0.347826087,1,research-problem,Introduction,hypernym_discovery3
2104,2104,2104,25,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (9, 12), (12, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.652173913,24,0.142011834,15,0.652173913,1,model,Introduction,hypernym_discovery3
2105,2105,2105,26,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (5, 7), (7, 9), (9, 11), (11, 12), (12, 14), (20, 21), (23, 25), (26, 30), (30, 31), (31, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.695652174,25,0.147928994,16,0.695652174,1,model,Introduction,hypernym_discovery3
2106,2106,2106,27,Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :,Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.739130435,26,0.153846154,17,0.739130435,1,model,Introduction,hypernym_discovery3
2107,2107,2107,28,( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .,Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (7, 8), (13, 15), (15, 16), (17, 18), (19, 20), (20, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608696,27,0.159763314,18,0.782608696,1,model,Introduction,hypernym_discovery3
2108,2108,2108,29,"Since our method uses a very large semantic network as reference sense inventory , we are able to perform jointly hypernym extraction and dis ambiguation , from which 1 The terminology is not entirely unified in this respect .",Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (9, 10), (16, 18), (18, 19), (19, 20), (20, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.826086957,28,0.165680473,19,0.826086957,1,model,Introduction,hypernym_discovery3
2109,2109,2109,31,2 Data and source code available from the following link : www.taln.upf.edu/taxoembed . expanding existing ontologies becomes a trivial task .,Introduction,Introduction,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.913043478,30,0.177514793,21,0.913043478,1,research-problem,Introduction,hypernym_discovery3
2110,2110,2110,131,"We compare against a number of taxonomy learning and Information Extraction systems , namely , WiBi and DefIE .",Experimental setting,Experimental setting,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (6, 12), (13, 14), (15, 16), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.214285714,130,0.769230769,3,0.214285714,1,baselines,Experimental setting,hypernym_discovery3
2111,2111,2111,136,"Finally , DefIE is an automaic OIE system relying on the syntactic structure of pre-dis ambiguated definitions 13 .",Experimental setting,Experimental setting,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 8), (8, 10), (11, 13), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.571428571,135,0.798816568,8,0.571428571,1,baselines,Experimental setting,hypernym_discovery3
2112,2112,2112,138,shows the results of TAXOEMBED and all comparison systems .,Experimental setting,Experimental setting,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.714285714,137,0.810650888,10,0.714285714,1,results,Experimental setting,hypernym_discovery3
2113,2113,2113,139,"As expected , Yago and WiBi achieve the best over all results .",Experimental setting,Experimental setting,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.785714286,138,0.816568047,11,0.785714286,1,results,Experimental setting,hypernym_discovery3
2114,2114,2114,140,"However , TAXOEM - BED , based solely on distributional information , performed competitively in detecting new hypernyms when compared to DefIE , improving its recall in most domains , and even surpassing Yago in technical areas like biology or health .",Experimental setting,Experimental setting,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (6, 9), (9, 11), (12, 13), (13, 14), (14, 16), (16, 18), (19, 21), (21, 22), (23, 24), (25, 26), (26, 27), (27, 29), (32, 33), (33, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.857142857,139,0.822485207,12,0.857142857,1,results,Experimental setting,hypernym_discovery3
2115,2115,2115,141,"However , our model does not perform particularly well on media and physics .",Experimental setting,Experimental setting,hypernym_discovery,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 9), (9, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.928571429,140,0.828402367,13,0.928571429,1,results,Experimental setting,hypernym_discovery3
2116,2116,2116,3,Hypernym discovery aims to discover the hypernym word sets given a hyponym word and proper corpus .,abstract,abstract,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,2,0.017241379,1,0.25,1,research-problem,abstract,hypernym_discovery4
2117,2117,2117,11,"In the past SemEval contest ( Sem Eval - 2015 task 17 1 , SemEval - 2016 task 13 2 ) , the "" Hypernym Detection "" task was treated as a classfication task , i.e. , given a ( hyponym , hypernym ) pair , deciding whether the pair is a true hypernymic relation or not .",Introduction,Introduction,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.444444444,10,0.086206897,4,0.444444444,1,research-problem,Introduction,hypernym_discovery4
2118,2118,2118,82,Word2vec is used to produce the word embeddings .,Experimental Setup,Experimental Setup,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 5), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.058823529,81,0.698275862,1,0.2,1,hyperparameters,Experimental Setup,hypernym_discovery4
2119,2119,2119,83,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,Experimental Setup,Experimental Setup,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 10), (11, 13), (14, 16), (16, 18), (18, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.117647059,82,0.706896552,2,0.4,1,hyperparameters,Experimental Setup,hypernym_discovery4
2120,2120,2120,87,Results Based on Projection Learning,Experimental Setup,,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O']",6,0.352941176,86,0.74137931,0,0,1,results,Experimental Setup,hypernym_discovery4
2121,2121,2121,91,"By using the same evaluating metrics as PRF in the cited paper , our best F - value on the validation set is 0.68 ( the paper result is 0.73 ) when the best cluster number is 2 and the threshold is ( 17.7 , 17.3 ) .",Experimental Setup,Results Based on Projection Learning,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 8), (14, 18), (18, 19), (20, 22), (22, 23), (23, 24), (31, 32), (33, 36), (36, 37), (37, 38), (40, 41), (41, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.588235294,90,0.775862069,4,0.363636364,1,results,Experimental Setup: Results Based on Projection Learning,hypernym_discovery4
2122,2122,2122,96,"This projection learning method performs not very well on task9 , we think the most probable reason is that in , the problem is formalized as a classification problem , in which the ( hyponym , hypernym ) pairs are given .",Experimental Setup,Results Based on Projection Learning,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 8), (8, 9), (9, 10), (24, 26), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.882352941,95,0.818965517,9,0.818181818,1,results,Experimental Setup: Results Based on Projection Learning,hypernym_discovery4
2123,2123,2123,99,Results Based on NN,,,hypernym_discovery,4,"['O', 'O', 'O', 'O']","[(3, 4)]","['O', 'O', 'O', 'O']",0,0,98,0.844827586,0,0,1,results,,hypernym_discovery4
2124,2124,2124,102,The performance evaluated using either cross validation or the test data is much worse than that of a typical hypernym prediction task reported by .,Results Based on NN,Results Based on NN,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 11), (11, 12), (12, 14), (14, 15), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.272727273,101,0.870689655,3,0.272727273,1,results,Results Based on NN,hypernym_discovery4
2125,2125,2125,104,"Although the method proposed by us is quite simple , our submissions are the 1st on Spanish , the 2nd on Italian , the 6th on English , ranked by the metric of MAP .",Results Based on NN,Results Based on NN,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (14, 15), (15, 16), (16, 17), (19, 20), (20, 21), (21, 22), (24, 25), (25, 26), (26, 27), (28, 30), (31, 32), (33, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.454545455,103,0.887931034,5,0.454545455,1,results,Results Based on NN,hypernym_discovery4
2126,2126,2126,106,"Compared with the results got by cross validation , the performance evaluated on the test data ) dropped significantly on English ( MAP dropped by 4 % ) and Italian ( MAP dropped by 8 % ) , but increased by a margin on Spanish ( MAP increased by 3.6 % ) .",Results Based on NN,Results Based on NN,hypernym_discovery,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (10, 11), (14, 16), (17, 18), (18, 19), (19, 20), (20, 21), (22, 23), (23, 25), (25, 27), (32, 34), (34, 36), (39, 41), (42, 43), (43, 44), (44, 45), (47, 49), (49, 51)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,105,0.905172414,7,0.636363636,1,results,Results Based on NN,hypernym_discovery4
2127,2127,2127,2,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery,title,title,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005319149,1,0,1,research-problem,title,hypernym_discovery5
2128,2128,2128,12,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .",Introduction,Introduction,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 13), (13, 14), (15, 17), (21, 27), (29, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.094339623,11,0.058510638,5,0.125,1,model,Introduction,hypernym_discovery5
2129,2129,2129,134,Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (7, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.545454545,133,0.707446809,12,0.545454545,1,results,Experiments and Results,hypernym_discovery5
2130,2130,2130,135,"As shown in , the scores obtained using this system are much higher than the strongest baselines for this task .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 8), (10, 11), (11, 13), (13, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.590909091,134,0.712765957,13,0.590909091,1,results,Experiments and Results,hypernym_discovery5
2131,2131,2131,136,"Furthermore , it is likely that we could improve our scores on 2A and 2B , since we only tuned the system on 1A .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (11, 12), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.636363636,135,0.718085106,14,0.636363636,1,results,Experiments and Results,hypernym_discovery5
2132,2132,2132,137,"If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 15), (15, 17), (17, 18), (18, 20), (20, 21), (21, 22), (22, 25), (27, 30), (30, 32), (32, 33), (33, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.681818182,136,0.723404255,15,0.681818182,1,results,Experiments and Results,hypernym_discovery5
2133,2133,2133,138,"Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (7, 9), (9, 11), (12, 15), (17, 18), (20, 21), (21, 25), (25, 26), (26, 28), (28, 29), (30, 35), (35, 36), (37, 41), (41, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.727272727,137,0.728723404,16,0.727272727,1,results,Experiments and Results,hypernym_discovery5
2134,2134,2134,140,"Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task , and even outperformed the supervised baseline on 2A .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 10), (17, 18), (19, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.818181818,139,0.739361702,18,0.818181818,1,results,Experiments and Results,hypernym_discovery5
2135,2135,2135,143,"Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .",Experiments and Results,Experiments and Results,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12), (12, 13), (14, 15), (15, 16), (16, 20), (20, 21), (22, 23), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.954545455,142,0.755319149,21,0.954545455,1,results,Experiments and Results,hypernym_discovery5
2136,2136,2136,146,"To assess the influence of different aspects of the supervised system and its training algorithm , we carried out a few simple ablation tests on subtask 1 A .",Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.023255814,145,0.771276596,1,0.041666667,1,ablation-analysis,Ablation Tests,hypernym_discovery5
2137,2137,2137,165,"These results show that 2 of the techniques we used , namely subsampling and multitask learning , actually harmed our system 's performance on test set 1 A , although our experiments on the trial set suggested that they would be beneficial .",Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (11, 12), (12, 13), (18, 19), (19, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.465116279,164,0.872340426,20,0.833333333,1,ablation-analysis,Ablation Tests,hypernym_discovery5
2138,2138,2138,167,"On the other hand , fine - tuning the word embeddings during training seems to be one of the keys to the success of this approach , as are the use of multiple projection matrices , and the sampling of multiple negative examples for each positive example .",Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (9, 11), (11, 12), (12, 13), (38, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.511627907,166,0.882978723,22,0.916666667,1,ablation-analysis,Ablation Tests,hypernym_discovery5
2139,2139,2139,173,"We should also note that the supervised model is prone to overfitting , and we found early stopping to be particularly important .",Ablation Tests,Ablation Tests,hypernym_discovery,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (6, 8), (9, 11), (11, 12), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.651162791,172,0.914893617,3,1,1,ablation-analysis,Ablation Tests,hypernym_discovery5
2140,2140,2140,2,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery,title,title,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.0078125,1,0,1,research-problem,title,hypernym_discovery6
2141,2141,2141,32,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .",Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (10, 11), (12, 16), (17, 19), (19, 20), (20, 21), (21, 24), (26, 28), (28, 30), (30, 33), (35, 37), (37, 39), (39, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.909090909,31,0.2421875,20,0.909090909,1,model,Introduction,hypernym_discovery6
2142,2142,2142,33,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .",Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 9), (10, 14), (14, 16), (16, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.954545455,32,0.25,21,0.954545455,1,model,Introduction,hypernym_discovery6
2143,2143,2143,34,The model predicts if a term and its candidate hypernym are hypernym related or not .,Introduction,Introduction,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (5, 10), (10, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,1,33,0.2578125,22,1,1,model,Introduction,hypernym_discovery6
2144,2144,2144,98,Results and Analysis,,,hypernym_discovery,6,"['O', 'O', 'O']",[],"['O', 'O', 'O']",0,0,97,0.7578125,0,0,1,experiments,,hypernym_discovery6
2145,2145,2145,101,"For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .",Results and Analysis,Results and Analysis,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 7), (7, 8), (8, 9), (9, 10), (10, 12), (15, 16), (16, 17), (17, 18), (19, 21), (21, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.15,100,0.78125,3,0.15,1,results,Results and Analysis,hypernym_discovery6
2146,2146,2146,102,"In addition , the result shows that our system performs well in discovering new hypernyms not defined in the gold hypernyms where it yields good False Positive values in the three corpora and we achieve the best False Positive value in Medical corpus The evaluation results of our system and other supervised systems .",Results and Analysis,Results and Analysis,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 10), (10, 11), (11, 13), (13, 15), (15, 18), (19, 21), (23, 24), (24, 28), (28, 29), (30, 32), (34, 35), (40, 41), (41, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2,101,0.7890625,4,0.2,1,results,Results and Analysis,hypernym_discovery6
2147,2147,2147,114,"As shown in the table 2 , the candidate hypernym extraction ( CHE ) coverage for English testing terms is 950 ( 63 % ) , that means our system is unable to extract any candidate hypernym for 550 ( 37 % ) terms ( 398 entities and 152 concepts ) .",Results and Analysis,Results and Analysis,hypernym_discovery,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 15), (15, 16), (16, 19), (19, 20), (20, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,113,0.8828125,16,0.8,1,results,Results and Analysis,hypernym_discovery6
2148,2148,2148,3,"This paper describes 300 - sparsans ' participation in SemEval - 2018 Task 9 : Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings .",abstract,abstract,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 17), (20, 21), (21, 23), (23, 25), (27, 30), (30, 32), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.5,2,0.016,1,0.5,1,research-problem,abstract,hypernym_discovery7
2149,2149,2149,14,Here we apply sparse feature pairs to hypernym extraction .,Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (6, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.12,13,0.104,9,0.36,1,model,Introduction,hypernym_discovery7
2150,2150,2150,16,Sparse representation is related to hypernymy in various natural ways .,Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.146666667,15,0.12,11,0.44,1,research-problem,Introduction,hypernym_discovery7
2151,2151,2151,17,One of them is through Formal concept Analysis ( FCA ) .,Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.16,16,0.128,12,0.48,1,research-problem,Introduction,hypernym_discovery7
2152,2152,2152,21,"Another natural formulation is related to hierarchical sparse coding , where trees describe the order in which variables "" enter the model "" ( i.e. , take non - zero values ) .",Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (10, 11), (11, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.213333333,20,0.16,16,0.64,1,research-problem,Introduction,hypernym_discovery7
2153,2153,2153,24,Exploiting the correspondence between the variable tree and the hypernym hierarchy offers itself as a natural choice .,Introduction,Introduction,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (5, 11), (11, 12), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.253333333,23,0.184,19,0.76,1,model,Introduction,hypernym_discovery7
2154,2154,2154,98,"Our submission with attribute pairs achieved first place in categories ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .",Results,Query type sensitive baselining,hypernym_discovery,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 6), (6, 8), (8, 9), (9, 10), (13, 19), (23, 25), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.894736842,97,0.776,1,0.041666667,1,results,Results: Query type sensitive baselining,hypernym_discovery7
2155,2155,2155,2,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies,title,title,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.015151515,1,0,1,research-problem,title,hypernym_discovery8
2156,2156,2156,8,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,Introduction,Introduction,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11), (19, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.019230769,7,0.106060606,1,0.0625,1,research-problem,Introduction,hypernym_discovery8
2157,2157,2157,9,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,Introduction,Introduction,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (8, 9), (9, 12), (12, 14), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.038461538,8,0.121212121,2,0.125,1,model,Introduction,hypernym_discovery8
2158,2158,2158,60,Results,,,hypernym_discovery,8,['O'],[],['O'],0,0,59,0.893939394,0,0,1,experiments,,hypernym_discovery8
2159,2159,2159,63,"While some relations have not been very fruitful ( such as X "" obj "" Y , for insance ) , others , instead , have been very productive , generating tens of thousands relations .",Results,Results,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(27, 29), (30, 31), (31, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,62,0.939393939,3,0.5,1,results,Results,hypernym_discovery8
2160,2160,2160,64,"The project 's results show that we have managed to accomplish the main objective of this project , to outperform the random strategy .",Results,Results,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (8, 11), (12, 14), (18, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,63,0.954545455,4,0.666666667,1,results,Results,hypernym_discovery8
2161,2161,2161,65,"The lower scores have been obtained for multiword expressions , for which we plan to add dedicated modules .",Results,Results,hypernym_discovery,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 7), (7, 9), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.833333333,64,0.96969697,5,0.833333333,1,results,Results,hypernym_discovery8
2162,2162,2162,2,Neural Models for Reasoning over Multiple Mentions using Coreference,title,title,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005714286,1,0,1,research-problem,title,natural_language_inference0
2163,2163,2163,12,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .",Introduction,Introduction,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.166666667,11,0.062857143,3,0.166666667,1,model,Introduction,natural_language_inference0
2164,2164,2164,20,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .",Introduction,Introduction,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 9), (9, 11), (12, 14), (16, 17), (18, 19), (19, 20), (21, 23), (23, 24), (24, 30), (31, 33), (34, 36), (36, 37), (38, 40), (40, 41), (42, 44)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.611111111,19,0.108571429,11,0.611111111,1,model,Introduction,natural_language_inference0
2165,2165,2165,21,This way hidden states are propagated along coreference chains and the original sequence in parallel .,Introduction,Introduction,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 7), (7, 9), (11, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.666666667,20,0.114285714,12,0.666666667,1,model,Introduction,natural_language_inference0
2166,2166,2166,22,We compare our Coref - GRU layer with the regular GRU layer by incorporating it in a recent model for reading comprehension .,Introduction,Introduction,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 8), (9, 12), (17, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.722222222,21,0.12,13,0.722222222,1,model,Introduction,natural_language_inference0
2167,2167,2167,94,BAbi AI tasks .,Method,,natural_language_inference,0,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",5,0.138888889,93,0.531428571,5,0.138888889,1,results,Method,natural_language_inference0
2168,2168,2168,100,In each case we see clear improvements of using C - GRU layers over GRU layers .,Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 7), (7, 9), (9, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.305555556,99,0.565714286,11,0.305555556,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2169,2169,2169,105,"Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .",Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (7, 9), (9, 12), (12, 13), (13, 15), (15, 16), (16, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.444444444,104,0.594285714,16,0.444444444,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2170,2170,2170,107,"On the other hand , C - GRU was significantly better than QRN on task 16 ( basic induction ) .",Method,Experiments on more natural data are described below .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (9, 12), (9, 11), (11, 12), (12, 13), (13, 14), (14, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5,106,0.605714286,18,0.5,1,experiments,Method: Experiments on more natural data are described below .,natural_language_inference0
2171,2171,2171,112,Wikihop dataset .,Method,,natural_language_inference,0,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",23,0.638888889,111,0.634285714,23,0.638888889,1,experiments,Method,natural_language_inference0
2172,2172,2172,120,"We see higher performance for the C - GRU model in the low data regime , and better generalization throughout the training curve for all three settings .",Method,Wikihop dataset .,natural_language_inference,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 10), (10, 11), (12, 15), (17, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.861111111,119,0.68,31,0.861111111,1,experiments,Method: Wikihop dataset .,natural_language_inference0
2173,2173,2173,2,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension,title,title,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.006756757,1,0,1,research-problem,title,natural_language_inference1
2174,2174,2174,4,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .,abstract,abstract,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.02027027,1,0.2,1,research-problem,abstract,natural_language_inference1
2175,2175,2175,7,"To show the effectiveness of our architecture , we conducted several experiments on the recently proposed and challenging RC dataset ' Nar - rative QA ' .",abstract,abstract,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,6,0.040540541,4,0.8,1,research-problem,abstract,natural_language_inference1
2176,2176,2176,19,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .",Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 17), (17, 18), (18, 20), (23, 25), (25, 27), (27, 28), (29, 30), (31, 32), (33, 34), (34, 35), (37, 39), (39, 40), (40, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.454545455,18,0.121621622,10,0.454545455,1,model,Introduction,natural_language_inference1
2177,2177,2177,20,The ConZNet architecture consists of two phases .,Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.5,19,0.128378378,11,0.5,1,model,Introduction,natural_language_inference1
2178,2178,2178,21,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 6), (7, 9), (9, 10), (10, 11), (11, 13), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.545454545,20,0.135135135,12,0.545454545,1,model,Introduction,natural_language_inference1
2179,2179,2179,23,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .",Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 11), (13, 14), (15, 17), (17, 18), (18, 19), (20, 21), (22, 23), (23, 25), (26, 31), (31, 32), (32, 33), (35, 38), (38, 40), (41, 43), (43, 44), (45, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.636363636,22,0.148648649,14,0.636363636,1,model,Introduction,natural_language_inference1
2180,2180,2180,27,"Unlike existing approaches , our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other .",Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 13), (13, 14), (14, 15), (17, 19), (20, 21), (28, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.818181818,26,0.175675676,18,0.818181818,1,model,Introduction,natural_language_inference1
2181,2181,2181,28,"Moreover , our decoder combines span prediction and sequence generation .",Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.863636364,27,0.182432432,19,0.863636364,1,model,Introduction,natural_language_inference1
2182,2182,2182,29,This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary .,Introduction,Introduction,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 6), (4, 5), (5, 6), (6, 7), (7, 8), (9, 11), (11, 12), (12, 13), (16, 18), (18, 19), (19, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.909090909,28,0.189189189,20,0.909090909,1,model,Introduction,natural_language_inference1
2183,2183,2183,117,"We compare our model against reported models in ( Seq2Seq , ASR , BiDAF ) and the Multi-range Reasoning Unit ( MRU ) in .",Baselines,Baselines,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (17, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,116,0.783783784,1,0.2,1,baselines,Baselines,natural_language_inference1
2184,2184,2184,118,"We implemented two baseline models ( Baseline 1 , Baseline 2 ) with Context Zoom layer similar to .",Baselines,Baselines,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,117,0.790540541,2,0.4,1,baselines,Baselines,natural_language_inference1
2185,2185,2185,125,The model is implemented using Python and Tensorflow .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.230769231,124,0.837837838,3,0.230769231,1,experimental-setup,Implementation Details,natural_language_inference1
2186,2186,2186,126,All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 6), (7, 9), (9, 11), (12, 13), (14, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.307692308,125,0.844594595,4,0.307692308,1,experimental-setup,Implementation Details,natural_language_inference1
2187,2187,2187,127,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .",Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 8), (8, 9), (10, 11), (11, 15), (16, 18), (19, 21), (26, 27), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.384615385,126,0.851351351,5,0.384615385,1,experimental-setup,Implementation Details,natural_language_inference1
2188,2188,2188,128,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (8, 9), (10, 12), (12, 13), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.461538462,127,0.858108108,6,0.461538462,1,experimental-setup,Implementation Details,natural_language_inference1
2189,2189,2189,129,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 6), (6, 7), (7, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.538461538,128,0.864864865,7,0.538461538,1,experimental-setup,Implementation Details,natural_language_inference1
2190,2190,2190,130,The number of hidden units are set to 100 .,Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.615384615,129,0.871621622,8,0.615384615,1,experimental-setup,Implementation Details,natural_language_inference1
2191,2191,2191,131,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .",Implementation Details,Implementation Details,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 13), (13, 14), (14, 16), (18, 21), (21, 22), (22, 23), (26, 28), (28, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.692307692,130,0.878378378,9,0.692307692,1,experimental-setup,Implementation Details,natural_language_inference1
2192,2192,2192,138,The performance of our model gradually dropped from sample size 7 onwards .,Results,Results,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 7), (7, 8), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,137,0.925675676,2,0.25,1,results,Results,natural_language_inference1
2193,2193,2193,139,This result shows evidence that only a few relevant sentences are sufficient to answer a question .,Results,Results,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (7, 10), (10, 11), (11, 12), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.375,138,0.932432432,3,0.375,1,results,Results,natural_language_inference1
2194,2194,2194,141,The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1 .,Results,Results,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 5), (5, 7), (7, 8), (8, 10), (10, 13), (13, 15), (16, 18), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,140,0.945945946,5,0.625,1,results,Results,natural_language_inference1
2195,2195,2195,144,This result points out that the self - attention mechanism in the Context zoom layer is an important component to identify related relevant sentences .,Results,Results,natural_language_inference,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 10), (10, 11), (12, 15), (15, 16), (17, 19), (19, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1,143,0.966216216,8,1,1,ablation-analysis,Results,natural_language_inference1
2196,2196,2196,2,A Simple and Effective Approach to the Story Cloze Test,title,title,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.009259259,1,0,1,research-problem,title,natural_language_inference10
2197,2197,2197,26,"Where previous approaches rely on feature engineering or involved neural network architectures , we achieve high accuracy with a fully neural approach involving only a single feedforward network and pre-trained skip - thought embeddings .",abstract,Story Context,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(14, 15), (15, 17), (17, 18), (19, 22), (22, 23), (25, 28), (29, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.511111111,25,0.231481481,9,0.290322581,1,model,abstract: Story Context,natural_language_inference10
2198,2198,2198,27,"Second , we find that considering only the last sentence of the context outperforms models that consider the full context .",abstract,Story Context,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (8, 10), (10, 11), (12, 13), (13, 14), (14, 15), (15, 17), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.533333333,26,0.240740741,10,0.322580645,1,experiments,abstract: Story Context,natural_language_inference10
2199,2199,2199,29,"In sum , our approach differs from previous efforts in the joint use of three strategies : ( 1 ) using skip - thought embeddings for sentences in the story in a feed - forward neural network , ( 2 ) training the model on the provided validation set , and ( 3 ) considering the two endings with only the last sentence in the prompt .",abstract,Story Context,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (14, 16), (20, 21), (21, 25), (25, 26), (26, 27), (27, 28), (29, 30), (30, 31), (32, 37), (41, 42), (43, 44), (44, 45), (46, 49), (54, 55), (56, 58), (58, 59), (61, 63), (63, 64), (65, 66)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.577777778,28,0.259259259,12,0.387096774,1,model,abstract: Story Context,natural_language_inference10
2200,2200,2200,83,We use cross-entropy loss and SGD with learning rate of 0.01 .,Experimental Method,Experimental Method,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 6), (6, 7), (7, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.428571429,82,0.759259259,3,0.428571429,1,hyperparameters,Experimental Method,natural_language_inference10
2201,2201,2201,84,"During training , we save the model every 3000 iterations , and calculate the validation accuracy .",Experimental Method,Experimental Method,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (6, 7), (7, 8), (8, 10), (12, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.571428571,83,0.768518519,4,0.571428571,1,hyperparameters,Experimental Method,natural_language_inference10
2202,2202,2202,89,The 3 - layer feed - forward neural network trained on the validation set by summing the skip - thought embeddings of the last sentence ( LS ) of the story prompt and the ending gives the best accuracy ( 76.5 % ) .,Results and Discussion,Results and Discussion,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 9), (9, 11), (12, 14), (14, 16), (17, 21), (21, 22), (23, 28), (28, 29), (30, 35), (35, 36), (37, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.066666667,88,0.814814815,1,0.066666667,1,results,Results and Discussion,natural_language_inference10
2203,2203,2203,91,"Comparing ' val - LS- skip ' to ' val - LS - Glo Ve ' ( i.e. , using skip - thought embeddings for sentences vs. GloVe word embeddings ) , we confirm that the success of this approach lies in the sizable boost to accuracy from the use of pretrained skip - thought embeddings .",Results and Discussion,Results and Discussion,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (7, 8), (33, 34), (45, 46), (46, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2,90,0.833333333,3,0.2,1,results,Results and Discussion,natural_language_inference10
2204,2204,2204,95,"We note that the model trained using only the last sentence ( LS ) of the story context has higher accuracy compared to the model that uses a GRU to encode the full context ( FC ) , and even the model which encodes the entire context .",Results and Discussion,Results and Discussion,natural_language_inference,10,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 7), (9, 14), (14, 15), (16, 18), (18, 19), (19, 21), (21, 23), (24, 25), (25, 27), (28, 29), (29, 31), (32, 37), (45, 47)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.466666667,94,0.87037037,7,0.466666667,1,results,Results and Discussion,natural_language_inference10
2205,2205,2205,2,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING,title,title,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004032258,1,0,1,research-problem,title,natural_language_inference11
2206,2206,2206,12,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,INTRODUCTION,INTRODUCTION,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.008928571,11,0.044354839,1,0.052631579,1,research-problem,INTRODUCTION,natural_language_inference11
2207,2207,2207,13,"Previous QA datasets tend to be high in quality due to human annotation , but small in size .",INTRODUCTION,INTRODUCTION,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.017857143,12,0.048387097,2,0.105263158,1,research-problem,INTRODUCTION,natural_language_inference11
2208,2208,2208,22,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .",INTRODUCTION,INTRODUCTION,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 9), (14, 21), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.098214286,21,0.084677419,11,0.578947368,1,model,INTRODUCTION,natural_language_inference11
2209,2209,2209,23,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .",INTRODUCTION,INTRODUCTION,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 7), (8, 9), (10, 11), (11, 12), (13, 17), (22, 25), (26, 28), (28, 29), (30, 34), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.107142857,22,0.088709677,12,0.631578947,1,model,INTRODUCTION,natural_language_inference11
2210,2210,2210,127,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (6, 7), (8, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,126,0.508064516,2,0.181818182,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2211,2211,2211,128,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (7, 9), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.333333333,127,0.512096774,3,0.272727273,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2212,2212,2212,129,We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 6), (8, 10), (11, 14), (15, 16), (16, 17), (17, 18), (18, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.416666667,128,0.516129032,4,0.363636364,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2213,2213,2213,131,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 8), (8, 9), (9, 10), (12, 15), (15, 16), (16, 17), (17, 18), (22, 24), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.583333333,130,0.524193548,6,0.545454545,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2214,2214,2214,132,All LSTMs have randomly initialized parameters and an initial state of zero .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 6), (8, 10), (10, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.666666667,131,0.528225806,7,0.636363636,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2215,2215,2215,133,Sentinel vectors are randomly initialized and optimized during training .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,132,0.532258065,8,0.727272727,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2216,2216,2216,134,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (8, 12), (12, 13), (13, 14), (15, 16), (17, 20), (20, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.833333333,133,0.536290323,9,0.818181818,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2217,2217,2217,135,"We use dropout to regularize our network during training , and optimize the model using ADAM .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 7), (7, 8), (8, 9), (11, 12), (13, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.916666667,134,0.540322581,10,0.909090909,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2218,2218,2218,136,All models are implemented and trained with Chainer .,EXPERIMENTS,EXPERIMENTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1,135,0.544354839,11,1,1,experimental-setup,EXPERIMENTS,natural_language_inference11
2219,2219,2219,145,"The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .",RESULTS,RESULTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 7), (7, 8), (9, 11), (12, 14), (14, 17), (17, 18), (26, 27), (29, 30), (30, 32), (33, 37), (37, 38), (39, 41), (41, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.275862069,144,0.580645161,8,0.275862069,1,results,RESULTS,natural_language_inference11
2220,2220,2220,146,"By doing so , the model is able to explore local maxima corresponding to multiple plausible answers , as is shown in .",RESULTS,RESULTS,natural_language_inference,11,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 12), (12, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.310344828,145,0.584677419,9,0.310344828,1,results,RESULTS,natural_language_inference11
2221,2221,2221,2,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING,title,title,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004878049,1,0,1,research-problem,title,natural_language_inference12
2222,2222,2222,4,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .",abstract,abstract,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.014634146,1,0.125,1,research-problem,abstract,natural_language_inference12
2223,2223,2223,11,* Code is publicly available at : https://github.com/juung/RMN,abstract,abstract,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1,10,0.048780488,8,1,1,code,abstract,natural_language_inference12
2224,2224,2224,42,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 12), (14, 17), (17, 19), (22, 25), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.163934426,41,0.2,30,0.833333333,1,model,INTRODUCTION,natural_language_inference12
2225,2225,2225,43,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (6, 8), (8, 9), (10, 12), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.169398907,42,0.204878049,31,0.861111111,1,model,INTRODUCTION,natural_language_inference12
2226,2226,2226,44,"In other words , RMN inherits RN 's MLP - based output feature map on Memory Network architecture .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 6), (6, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.174863388,43,0.209756098,32,0.888888889,1,model,INTRODUCTION,natural_language_inference12
2227,2227,2227,125,bAbI story - based QA dataset,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",113,0.617486339,124,0.604878049,1,0.034482759,1,experiments,INTRODUCTION,natural_language_inference12
2228,2228,2228,129,"Embedding component is similar to , where story and question are embedded through different LSTMs ; 32 unit word - lookup embeddings ; 32 unit LSTM for story and question .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (6, 7), (7, 10), (11, 13), (13, 15), (16, 22), (23, 26), (26, 27), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",117,0.639344262,128,0.624390244,5,0.172413793,1,model,INTRODUCTION,natural_language_inference12
2229,2229,2229,130,"For attention component , as we use 2 hop RMN , there are g 1 ?",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",118,0.644808743,129,0.629268293,6,0.206896552,1,model,INTRODUCTION,natural_language_inference12
2230,2230,2230,132,"For regularization , we use batch normalization for all MLPs .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 7), (7, 8), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",120,0.655737705,131,0.63902439,8,0.275862069,1,hyperparameters,INTRODUCTION,natural_language_inference12
2231,2231,2231,133,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 12), (12, 13), (14, 16), (16, 17), (18, 20), (20, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",121,0.661202186,132,0.643902439,9,0.310344828,1,hyperparameters,INTRODUCTION,natural_language_inference12
2232,2232,2232,134,bAbI dialog dataset,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",122,0.666666667,133,0.648780488,10,0.344827586,1,experiments,INTRODUCTION,natural_language_inference12
2233,2233,2233,135,"We trained on full dialog scripts with every model response as answer , all previous dialog history as sentences to be memorized , and the last user utterance as question .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 6), (6, 7), (7, 10), (10, 11), (11, 12), (13, 17), (17, 18), (18, 19), (19, 22), (25, 28), (28, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",123,0.672131148,134,0.653658537,11,0.379310345,1,model,INTRODUCTION,natural_language_inference12
2234,2234,2234,136,"Model selects the most probable response from 4,212 candidates which are ranked from a set of all bot utterances appearing in training , validation and test sets ( plain and OOV ) for all tasks combined .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 9), (11, 13), (14, 15), (15, 16), (16, 19), (19, 21), (21, 32), (32, 33), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",124,0.677595628,135,0.658536585,12,0.413793103,1,model,INTRODUCTION,natural_language_inference12
2235,2235,2235,154,BABI DIALOG,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O']","[(0, 2)]","['O', 'O']",142,0.775956284,153,0.746341463,0,0,1,experiments,INTRODUCTION,natural_language_inference12
2236,2236,2236,156,"Without any match type , RN and RMN outperform previous memory - augmented models on both normal and OOV tasks .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (5, 8), (8, 9), (9, 14), (14, 15), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",144,0.786885246,155,0.756097561,2,0.083333333,1,experiments,INTRODUCTION,natural_language_inference12
2237,2237,2237,160,"We converted RMN 's attention component to inner product based attention , and the results revealed the error rate increased to 11.3 % .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (7, 11), (15, 16), (17, 19), (19, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",148,0.808743169,159,0.775609756,6,0.25,1,model,INTRODUCTION,natural_language_inference12
2238,2238,2238,162,The number of unnecessary object pairs created by the RN not only increases the processing time but also decreases the accuracy .,INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (6, 8), (9, 10), (14, 16), (18, 19), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",150,0.819672131,161,0.785365854,8,0.333333333,1,model,INTRODUCTION,natural_language_inference12
2239,2239,2239,163,"With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (6, 8), (8, 10), (10, 11), (12, 14), (15, 16), (16, 18), (18, 20), (20, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",151,0.825136612,162,0.790243902,9,0.375,1,experiments,INTRODUCTION,natural_language_inference12
2240,2240,2240,171,"Different from other tasks , RMN yields the same error rate 25.1 % with MemN2N and GMe m N2N on the task 3 .",INTRODUCTION,INTRODUCTION,natural_language_inference,12,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (8, 11), (11, 13), (13, 14), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",159,0.868852459,170,0.829268293,17,0.708333333,1,experiments,INTRODUCTION,natural_language_inference12
2241,2241,2241,2,Natural Language Comprehension with the EpiReader,title,title,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003649635,1,0,1,research-problem,title,natural_language_inference13
2242,2242,2242,4,"We present the EpiReader , a novel model for machine comprehension of text .",abstract,abstract,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (9, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.010948905,1,0.2,1,research-problem,abstract,natural_language_inference13
2243,2243,2243,5,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",abstract,abstract,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,4,0.01459854,2,0.4,1,research-problem,abstract,natural_language_inference13
2244,2244,2244,6,"Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text , and evaluate a model 's response to the questions .",abstract,abstract,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,5,0.018248175,3,0.6,1,research-problem,abstract,natural_language_inference13
2245,2245,2245,13,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .",Introduction,Introduction,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 14), (16, 17), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.021390374,12,0.04379562,4,0.090909091,1,model,Introduction,natural_language_inference13
2246,2246,2246,26,The EpiReader factors into two components .,Introduction,,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.090909091,25,0.091240876,17,0.386363636,1,model,Introduction,natural_language_inference13
2247,2247,2247,27,The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text ; we call this the Extractor .,Introduction,The EpiReader factors into two components .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 10), (10, 12), (13, 15), (15, 16), (17, 18), (18, 19), (20, 22), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.096256684,26,0.094890511,18,0.409090909,1,model,Introduction: The EpiReader factors into two components .,natural_language_inference13
2248,2248,2248,28,The second component reranks the proposed answers based on deeper semantic comparisons with the text ; we call this the Reasoner .,Introduction,The EpiReader factors into two components .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (7, 9), (9, 12), (12, 13), (14, 15), (17, 18), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.101604278,27,0.098540146,19,0.431818182,1,model,Introduction: The EpiReader factors into two components .,natural_language_inference13
2249,2249,2249,32,"The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment ( RTE ) , also known as natural language inference .",Introduction,Test 2 .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (6, 7), (8, 10), (11, 12), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.122994652,31,0.113138686,23,0.522727273,1,model,Introduction: Test 2 .,natural_language_inference13
2250,2250,2250,34,"Thus , the Extractor serves the important function of filtering a large set of potential answers down to a small , tractable set of likely candidates for more thorough testing .",Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (6, 8), (8, 9), (9, 10), (11, 16), (16, 18), (19, 26), (26, 27), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.13368984,33,0.120437956,25,0.568181818,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2251,2251,2251,35,"The Extractor follows the form of a pointer network , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question .",Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (7, 9), (11, 12), (13, 16), (16, 18), (18, 19), (19, 20), (21, 22), (23, 25), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.139037433,34,0.124087591,26,0.590909091,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2252,2252,2252,36,This approach was used ( on it s own ) for question answering with the Attention Sum Reader .,Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (11, 13), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.144385027,35,0.127737226,27,0.613636364,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2253,2253,2253,37,The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness .,Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 7), (7, 9), (9, 11), (12, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.14973262,36,0.131386861,28,0.636363636,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2254,2254,2254,38,"The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .",Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6), (7, 9), (9, 10), (11, 12), (14, 15), (16, 17), (17, 18), (18, 20), (20, 21), (21, 23), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.155080214,37,0.135036496,29,0.659090909,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2255,2255,2255,39,"We use these estimates as a measure of the evidence for a hypothesis , and aggregate evidence overall sentences .",Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (6, 7), (7, 8), (9, 10), (10, 11), (12, 13), (15, 16), (16, 17), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.160427807,38,0.138686131,30,0.681818182,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2256,2256,2256,40,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .",Introduction,This process is computationally demanding .,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 11), (12, 16), (16, 18), (19, 21), (21, 22), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.165775401,39,0.142335766,31,0.704545455,1,model,Introduction: This process is computationally demanding .,natural_language_inference13
2257,2257,2257,221,"To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 6), (6, 9), (9, 10), (11, 13), (21, 22), (23, 26), (26, 27), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.083333333,220,0.802919708,1,0.083333333,1,experimental-setup,Implementation and training details,natural_language_inference13
2258,2258,2258,222,"The word embeddings were initialized randomly , drawing from the uniform distribution over .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (4, 5), (5, 6), (7, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.166666667,221,0.806569343,2,0.166666667,1,experimental-setup,Implementation and training details,natural_language_inference13
2259,2259,2259,223,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 6), (8, 10), (10, 11), (12, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,222,0.810218978,3,0.25,1,experimental-setup,Implementation and training details,natural_language_inference13
2260,2260,2260,224,Our model was implement in Theano using the Keras framework .,Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 7), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.333333333,223,0.813868613,4,0.333333333,1,experimental-setup,Implementation and training details,natural_language_inference13
2261,2261,2261,229,"All our models used 2 - regularization at 0.001 , ? = 50 , and ? = 0.04 .",Implementation and training details,Implementation and training details,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 7), (7, 8), (8, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,228,0.832116788,9,0.75,1,experimental-setup,Implementation and training details,natural_language_inference13
2262,2262,2262,236,The EpiReader achieves state - of - the - art performance across the board for both datasets .,Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 11), (11, 12), (13, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.083333333,235,0.857664234,3,0.25,1,results,Results,natural_language_inference13
2263,2263,2263,237,"On CNN , we score 2.2 % higher on test than the best previous model of .",Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (4, 5), (5, 8), (8, 9), (9, 10), (10, 11), (12, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.111111111,236,0.861313869,4,0.333333333,1,results,Results,natural_language_inference13
2264,2264,2264,242,On CBT - CN our single model scores 4.0 % higher than the previous best of the AS Reader .,Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (7, 8), (8, 10), (11, 12), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.25,241,0.879562044,9,0.75,1,results,Results,natural_language_inference13
2265,2265,2265,243,The improvement on CBT - NE is more modest at 1.1 % .,Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (6, 7), (7, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.277777778,242,0.883211679,10,0.833333333,1,results,Results,natural_language_inference13
2266,2266,2266,244,"Looking more closely at our CBT - NE results , we found that the validation and test accuracies had relatively high variance even in late epochs of training .",Results,Results,natural_language_inference,13,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 9), (11, 13), (14, 18), (18, 19), (19, 22), (22, 24), (24, 26), (26, 27), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.305555556,243,0.886861314,11,0.916666667,1,results,Results,natural_language_inference13
2267,2267,2267,2,End - To - End Memory Networks,title,,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004975124,1,0,1,research-problem,title,natural_language_inference14
2268,2268,2268,14,"In this work , we present a novel recurrent neural network ( RNN ) architecture where the recurrence reads from a possibly large external memory multiple times before outputting a symbol .",abstract,abstract,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 15), (15, 16), (17, 18), (18, 20), (21, 25), (25, 27), (27, 29), (30, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.171875,13,0.064676617,11,0.261904762,1,research-problem,abstract,natural_language_inference14
2269,2269,2269,15,Our model can be considered a continuous form of the Memory Network implemented in [ 23 ] .,abstract,abstract,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 8), (8, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1875,14,0.069651741,12,0.285714286,1,model,abstract,natural_language_inference14
2270,2270,2270,17,"The continuity of the model we present here means that it can be trained end - to - end from input - output pairs , and so is applicable to more tasks , i.e. tasks where such supervision is not available , such as in language modeling or realistically supervised question answering tasks .",abstract,abstract,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (13, 14), (14, 19), (19, 20), (20, 24), (28, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.21875,16,0.07960199,14,0.333333333,1,research-problem,abstract,natural_language_inference14
2271,2271,2271,18,"Our model can also be seen as a version of RNNsearch [ 2 ] with multiple computational steps ( which we term "" hops "" ) per output symbol .",abstract,abstract,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 10), (10, 11), (14, 15), (15, 18), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.234375,17,0.084577114,15,0.357142857,1,model,abstract,natural_language_inference14
2272,2272,2272,133,"For each mini-batch update , the 2 norm of the whole gradient of all parameters is measured 5 and if larger than L = 50 , then it is scaled down to have norm L.",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 8), (8, 9), (10, 12), (12, 13), (13, 15), (16, 17), (19, 20), (20, 22), (20, 25), (29, 31), (31, 33), (33, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.105263158,132,0.656716418,2,0.105263158,1,hyperparameters,Training Details,natural_language_inference14
2273,2273,2273,135,"We use the learning rate annealing schedule from , namely , if the validation cost has not decreased after one epoch , then the learning rate is scaled down by a factor 1.5 .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (11, 12), (13, 15), (16, 18), (18, 19), (19, 21), (24, 26), (27, 30), (27, 29), (29, 30), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.210526316,134,0.666666667,4,0.210526316,1,hyperparameters,Training Details,natural_language_inference14
2274,2274,2274,137,"Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 10), (11, 13), (14, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.315789474,136,0.676616915,6,0.315789474,1,hyperparameters,Training Details,natural_language_inference14
2275,2275,2275,138,"On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .",Training Details,Training Details,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (7, 8), (8, 10), (10, 12), (12, 13), (13, 16), (17, 18), (20, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.368421053,137,0.68159204,7,0.368421053,1,hyperparameters,Training Details,natural_language_inference14
2276,2276,2276,153,"MemNN : The strongly supervised AM + NG + NL Memory Networks approach , proposed in .",Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.181818182,152,0.756218905,2,0.181818182,1,baselines,Baselines,natural_language_inference14
2277,2277,2277,157,MemNN- WSH :,Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",6,0.545454545,156,0.776119403,6,0.545454545,1,baselines,Baselines,natural_language_inference14
2278,2278,2278,158,A weakly supervised heuristic version of MemNN where the supporting sentence labels are not used in training .,Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 7), (7, 8), (9, 12), (13, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.636363636,157,0.781094527,7,0.636363636,1,baselines,Baselines,natural_language_inference14
2279,2279,2279,161,"LSTM : A standard LSTM model , trained using question / answer pairs only ( i.e. also weakly supervised ) .",Baselines,Baselines,natural_language_inference,14,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (3, 6), (7, 9), (9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.909090909,160,0.7960199,10,0.909090909,1,baselines,Baselines,natural_language_inference14
2280,2280,2280,2,Neural Natural Language Inference Models Enhanced with External Knowledge,title,,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004310345,1,0,1,research-problem,title,natural_language_inference15
2281,2281,2281,4,Modeling natural language inference is a very challenging task .,abstract,abstract,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.012931034,1,0.166666667,1,research-problem,abstract,natural_language_inference15
2282,2282,2282,5,"With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance .",abstract,abstract,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,4,0.017241379,2,0.333333333,1,research-problem,abstract,natural_language_inference15
2283,2283,2283,6,"Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?",abstract,abstract,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(16, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,5,0.021551724,3,0.5,1,research-problem,abstract,natural_language_inference15
2284,2284,2284,7,"If not , how can neural - network - based NLI models benefit from external knowledge and how to build NLI models to leverage it ?",abstract,abstract,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 12), (12, 14), (14, 16), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.666666667,6,0.025862069,4,0.666666667,1,research-problem,abstract,natural_language_inference15
2285,2285,2285,12,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",Introduction,Introduction,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6), (10, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.125,11,0.047413793,2,0.125,1,research-problem,Introduction,natural_language_inference15
2286,2286,2286,23,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .",Introduction,Introduction,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 12), (12, 13), (13, 15), (15, 16), (16, 17), (18, 21), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,22,0.094827586,13,0.8125,1,model,Introduction,natural_language_inference15
2287,2287,2287,24,We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .,Introduction,Introduction,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 16), (16, 18), (18, 20), (20, 21), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,23,0.099137931,14,0.875,1,experiments,Introduction,natural_language_inference15
2288,2288,2288,25,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .",Introduction,Introduction,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 6), (6, 7), (7, 9), (9, 10), (11, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,24,0.103448276,15,0.9375,1,experiments,Introduction,natural_language_inference15
2289,2289,2289,160,The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (10, 11), (12, 14), (14, 15), (15, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.130434783,159,0.685344828,3,0.130434783,1,hyperparameters,Training Details,natural_language_inference15
2290,2290,2290,161,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 9), (11, 17), (17, 18), (20, 21), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.173913043,160,0.689655172,4,0.173913043,1,hyperparameters,Training Details,natural_language_inference15
2291,2291,2291,163,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 8), (9, 11), (11, 12), (12, 13), (14, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.260869565,162,0.698275862,6,0.260869565,1,hyperparameters,Training Details,natural_language_inference15
2292,2292,2292,164,The mini - batch size is set to 32 .,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.304347826,163,0.702586207,7,0.304347826,1,hyperparameters,Training Details,natural_language_inference15
2293,2293,2293,170,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 13), (15, 16), (16, 17), (17, 18), (18, 20), (21, 22), (23, 24), (24, 25), (25, 27), (29, 34), (34, 36), (37, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.565217391,169,0.728448276,13,0.565217391,1,results,Training Details,natural_language_inference15
2294,2294,2294,171,The difference between ESIM and KIM is statistically significant under the one - tailed paired t- test at the 99 % significance level .,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (6, 7), (7, 9), (9, 10), (11, 17), (17, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.608695652,170,0.732758621,14,0.608695652,1,results,Training Details,natural_language_inference15
2295,2295,2295,177,shows the performance of models on the MultiNLI dataset .,Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (4, 5), (5, 6), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.869565217,176,0.75862069,20,0.869565217,1,results,Training Details,natural_language_inference15
2296,2296,2296,178,"The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 9), (9, 10), (10, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.913043478,177,0.762931034,21,0.913043478,1,results,Training Details,natural_language_inference15
2297,2297,2297,179,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .",Training Details,Training Details,natural_language_inference,15,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (6, 8), (10, 11), (11, 13), (13, 14), (14, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.956521739,178,0.767241379,22,0.956521739,1,results,Training Details,natural_language_inference15
2298,2298,2298,2,Text Understanding with the Attention Sum Reader Network,title,title,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004166667,1,0,1,research-problem,title,natural_language_inference16
2299,2299,2299,5,"Thanks to the size of these datasets , the associated text comprehension task is well suited for deep - learning techniques that currently seem to outperform all alternative approaches .",abstract,abstract,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(14, 17), (17, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,4,0.016666667,2,0.4,1,research-problem,abstract,natural_language_inference16
2300,2300,2300,65,Our model called the Attention Sum Reader ( AS Reader ) 4 is tailor - made to leverage the fact that the answer is a word from the context document .,Introduction,Children 's Book Test,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 11), (13, 16), (16, 18), (19, 20), (22, 23), (23, 24), (25, 26), (26, 27), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.666666667,64,0.266666667,17,0.653846154,1,model,Introduction: Children 's Book Test,natural_language_inference16
2301,2301,2301,70,We compute a vector embedding of the query .,Introduction,,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.726190476,69,0.2875,22,0.846153846,1,model,Introduction,natural_language_inference16
2302,2302,2302,72,We compute a vector embedding of each individual word in the context of the whole document ( contextual embedding ) .,Introduction,We compute a vector embedding of the query .,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 9), (9, 10), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.75,71,0.295833333,24,0.923076923,1,model,Introduction: We compute a vector embedding of the query .,natural_language_inference16
2303,2303,2303,74,"Using a dot product between the question embedding and the contextual embedding of each occurrence of a candidate answer in the document , we select the most likely answer .",Introduction,We compute a vector embedding of the query .,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (6, 12), (12, 13), (13, 15), (15, 16), (17, 19), (19, 20), (21, 22), (24, 25), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.773809524,73,0.304166667,26,1,1,model,Introduction: We compute a vector embedding of the query .,natural_language_inference16
2304,2304,2304,172,To train the model we used stochastic gradient descent with the ADAM update rule and learning rate of 0.001 or 0.0005 .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 6), (6, 9), (9, 10), (11, 14), (15, 17), (17, 18), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.052631579,171,0.7125,1,0.052631579,1,hyperparameters,Training Details,natural_language_inference16
2305,2305,2305,178,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 5), (6, 8), (8, 11), (12, 13), (14, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.368421053,177,0.7375,7,0.368421053,1,hyperparameters,Training Details,natural_language_inference16
2306,2306,2306,179,We also used a gradient clipping threshold of 10 and batches of size 32 .,Training Details,Training Details,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (8, 9), (10, 11), (11, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.421052632,178,0.741666667,8,0.421052632,1,hyperparameters,Training Details,natural_language_inference16
2307,2307,2307,212,mance of our single model is a little bit worse than performance of simultaneously published models .,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 5), (5, 6), (7, 10), (10, 11), (11, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.172413793,211,0.879166667,4,0.307692308,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2308,2308,2308,214,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .",Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.24137931,213,0.8875,6,0.461538462,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2309,2309,2309,215,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 7), (7, 8), (8, 11), (11, 12), (13, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.275862069,214,0.891666667,7,0.538461538,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2310,2310,2310,216,The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 9), (9, 11), (11, 13), (13, 14), (14, 16), (22, 23), (24, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.310344828,215,0.895833333,8,0.615384615,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2311,2311,2311,218,Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets ..,Results,Performance of our models on the CNN and Daily,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 5), (6, 9), (9, 10), (10, 11), (11, 12), (13, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.379310345,217,0.904166667,10,0.769230769,1,results,Results: Performance of our models on the CNN and Daily,natural_language_inference16
2312,2312,2312,220,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .",Results,CBT .,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (5, 8), (8, 9), (9, 10), (10, 11), (11, 13), (13, 14), (14, 18), (18, 19), (20, 21), (21, 22), (22, 24), (26, 28), (28, 29), (29, 33), (33, 34), (35, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.448275862,219,0.9125,12,0.923076923,1,results,Results: CBT .,natural_language_inference16
2313,2313,2313,221,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,Results,CBT .,natural_language_inference,16,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (7, 8), (8, 12), (12, 13), (13, 15), (17, 18), (18, 19), (20, 21), (21, 22), (22, 24), (30, 31), (31, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.482758621,220,0.916666667,13,1,1,results,Results: CBT .,natural_language_inference16
2314,2314,2314,2,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING,title,title,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.002475248,1,0,1,research-problem,title,natural_language_inference17
2315,2315,2315,4,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .",abstract,abstract,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,3,0.007425743,1,0.142857143,1,research-problem,abstract,natural_language_inference17
2316,2316,2316,9,"However , the low absolute performance of our best model indicates the need for improved general NLU systems .",abstract,abstract,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.857142857,8,0.01980198,6,0.857142857,1,research-problem,abstract,natural_language_inference17
2317,2317,2317,15,"To facilitate research in this direction , we present the General Language Understanding Evaluation ( GLUE ) benchmark : a collection of NLU tasks including question answering , sentiment analysis , and textual entailment , and an associated online platform for model evaluation , comparison , and analysis .",INTRODUCTION,INTRODUCTION,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.038834951,14,0.034653465,4,0.153846154,1,research-problem,INTRODUCTION,natural_language_inference17
2318,2318,2318,16,GLUE does not place any constraints on model architecture beyond the ability to process single - sentence and sentence - pair inputs and to make corresponding predictions .,INTRODUCTION,INTRODUCTION,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (4, 6), (6, 7), (7, 9), (9, 10), (11, 12), (12, 14), (14, 22), (23, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.048543689,15,0.037128713,5,0.192307692,1,model,INTRODUCTION,natural_language_inference17
2319,2319,2319,17,"For some GLUE tasks , training data is plentiful , but for others it is limited or fails to match the genre of the test set .",INTRODUCTION,INTRODUCTION,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 7), (7, 8), (8, 9), (15, 16), (18, 20), (21, 22), (24, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.058252427,16,0.03960396,6,0.230769231,1,research-problem,INTRODUCTION,natural_language_inference17
2320,2320,2320,20,"Four of the datasets feature privately - held test data , which will be used to ensure that the benchmark is used fairly .",INTRODUCTION,INTRODUCTION,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 10), (15, 17), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.087378641,19,0.047029703,9,0.346153846,1,experiments,INTRODUCTION,natural_language_inference17
2321,2321,2321,157,Original code for the baselines is available at https://github.com/nyu-mll/GLUE-baselines and a newer version is available at https://github.com/jsalt18-sentence-repl/jiant.,BASELINES,See Appendix,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1,156,0.386138614,6,0.428571429,1,code,BASELINES: See Appendix,natural_language_inference17
2322,2322,2322,159,"Our simplest baseline architecture is based on sentence - to - vector encoders , and sets aside GLUE 's ability to evaluate models with more complex structures .",Architecture,Architecture,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 13), (15, 17), (17, 18), (20, 22), (22, 23), (23, 24), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,158,0.391089109,8,0.571428571,1,model,Architecture,natural_language_inference17
2323,2323,2323,183,We find that multi-task training yields better overall scores over single - task training amongst models using attention or ELMo .,Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (6, 9), (9, 10), (10, 14), (14, 15), (15, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.162790698,182,0.45049505,4,0.266666667,1,experiments,Sentence Representation Models,natural_language_inference17
2324,2324,2324,185,"We see a consistent improvement in using ELMo embeddings in place of GloVe or CoVe embeddings , particularly for single - sentence tasks .",Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (7, 9), (9, 12), (12, 16), (17, 19), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.209302326,184,0.455445545,6,0.4,1,experiments,Sentence Representation Models,natural_language_inference17
2325,2325,2325,187,"Among the pre-trained sentence representation models , we observe fairly consistent gains moving from CBoW to Skip - Thought to Infersent and GenSen .",Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (8, 9), (9, 12), (12, 14), (14, 15), (15, 16), (16, 19), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.255813953,186,0.46039604,8,0.533333333,1,experiments,Sentence Representation Models,natural_language_inference17
2326,2326,2326,188,"Relative to the models trained directly on the GLUE tasks , InferSent is competitive and GenSen outperforms all but the two best .",Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 4), (4, 7), (8, 10), (11, 12), (12, 13), (13, 14), (15, 16), (16, 17), (17, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.279069767,187,0.462871287,9,0.6,1,experiments,Sentence Representation Models,natural_language_inference17
2327,2327,2327,189,"Looking at results per task , we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task .",Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 9), (10, 13), (13, 16), (16, 17), (17, 19), (20, 21), (21, 24), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.302325581,188,0.465346535,10,0.666666667,1,experiments,Sentence Representation Models,natural_language_inference17
2328,2328,2328,190,"On the other hand , for STS - B , models trained directly on the task lag significantly behind the performance of the best sentence representation model .",Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 9), (10, 11), (11, 14), (15, 16), (16, 17), (16, 19), (17, 19), (20, 21), (21, 22), (23, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.325581395,189,0.467821782,11,0.733333333,1,experiments,Sentence Representation Models,natural_language_inference17
2329,2329,2329,192,"On WNLI , no model exceeds most - frequent - class guessing ( 65.1 % ) and we substitute the model predictions for the most -frequent baseline .",Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (5, 6), (6, 16), (18, 19), (20, 22), (22, 23), (24, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.372093023,191,0.472772277,13,0.866666667,1,experiments,Sentence Representation Models,natural_language_inference17
2330,2330,2330,193,"On RTE and in aggregate , even our best baselines leave room for improvement .",Sentence Representation Models,Sentence Representation Models,natural_language_inference,17,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (10, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.395348837,192,0.475247525,14,0.933333333,1,experiments,Sentence Representation Models,natural_language_inference17
2331,2331,2331,2,Parameter Re-Initialization through Cyclical Batch Size Schedules,title,,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004975124,1,0,1,research-problem,title,natural_language_inference18
2332,2332,2332,4,Optimal parameter initialization remains a crucial problem for neural network training .,abstract,abstract,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.014925373,1,0.166666667,1,research-problem,abstract,natural_language_inference18
2333,2333,2333,19,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (6, 7), (8, 10), (10, 11), (12, 14), (14, 15), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.36,18,0.089552239,9,0.36,1,model,Introduction,natural_language_inference18
2334,2334,2334,20,"From the Bayesian perspective , improved weight initialization can be viewed as starting with a better prior , which leads to a more accurate posterior and thus better generalization ability .",Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (10, 12), (12, 14), (15, 17), (19, 21), (22, 25), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4,19,0.094527363,10,0.4,1,model,Introduction,natural_language_inference18
2335,2335,2335,22,"For example , in the seminal works , an adaptive prior is implemented via Markov Chain Monte Carlo ( MCMC ) methods .",Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 11), (12, 14), (14, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.48,21,0.104477612,12,0.48,1,model,Introduction,natural_language_inference18
2336,2336,2336,23,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .",Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (12, 13), (13, 16), (26, 27), (27, 31), (31, 33), (34, 39), (39, 40), (40, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.52,22,0.109452736,13,0.52,1,model,Introduction,natural_language_inference18
2337,2337,2337,26,"Here , we build upon this work by studying different cyclical annealing strategies for a wide range of problems .",Introduction,Introduction,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (9, 13), (13, 14), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.64,25,0.124378109,16,0.64,1,model,Introduction,natural_language_inference18
2338,2338,2338,82,Language Results,,,natural_language_inference,18,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,81,0.402985075,0,0,1,experiments,,natural_language_inference18
2339,2339,2339,83,Language modeling is a challenging problem due to the complex and long - range interactions between distant words .,Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.052631579,82,0.407960199,1,0.052631579,1,research-problem,Language Results,natural_language_inference18
2340,2340,2340,85,"CBS schedules effectively help us avoid overfitting , and in addition snapshot ensembling enables even greater performance .",Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (5, 6), (6, 7), (11, 13), (13, 14), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.157894737,84,0.417910448,3,0.157894737,1,results,Language Results,natural_language_inference18
2341,2341,2341,88,"As we can see , the best performing CBS schedules result in significant improvements in perplexity ( up to 7.91 ) over the baseline schedules and also offer reductions in the number of SGD training iterations ( up to 33 % ) .",Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 10), (10, 12), (12, 14), (14, 15), (15, 16), (21, 22), (23, 25), (27, 28), (28, 29), (29, 30), (31, 36), (37, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.315789474,87,0.432835821,6,0.315789474,1,results,Language Results,natural_language_inference18
2342,2342,2342,90,Notice that almost all CBS schedules outperform the baseline schedule .,Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 7), (8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.421052632,89,0.44278607,8,0.421052632,1,results,Language Results,natural_language_inference18
2343,2343,2343,97,"In our experiments , CBS schedules do not yield large performance improvements on models like E1 which exhibit smaller disparities between training and testing performance .",Language Results,Language Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 9), (9, 12), (12, 13), (13, 14), (17, 18), (18, 20), (20, 21), (21, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.789473684,96,0.47761194,15,0.789473684,1,results,Language Results,natural_language_inference18
2344,2344,2344,102,Image Classification Results,,,natural_language_inference,18,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,101,0.502487562,0,0,1,results,,natural_language_inference18
2345,2345,2345,103,"As seen in , the training curves of CBS schedules also exhibit the aforementioned cyclical spikes both in training loss and testing accuracy .",Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (7, 8), (8, 10), (11, 12), (13, 14), (14, 16), (17, 18), (18, 20), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.047619048,102,0.507462687,1,0.1,1,results,Image Classification Results,natural_language_inference18
2346,2346,2346,105,We observe that CBS achieves similar performance to the baseline .,Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (5, 7), (7, 8), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.142857143,104,0.517412935,3,0.3,1,results,Image Classification Results,natural_language_inference18
2347,2347,2347,107,"With CBS - 15 , we see 90.71 % training accuracy and 56. 44 % testing accuracy , which is a larger improvement than that offered by CBS on convolutional models on Cifar - 10 .",Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (6, 7), (7, 11), (12, 17), (21, 23), (23, 24), (28, 29), (29, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.238095238,106,0.527363184,5,0.5,1,results,Image Classification Results,natural_language_inference18
2348,2348,2348,109,Combining CBS - 15 on C2 with this strategy improves accuracy to 94.82 % .,Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (4, 5), (5, 6), (9, 10), (10, 11), (11, 12), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.333333333,108,0.537313433,7,0.7,1,results,Image Classification Results,natural_language_inference18
2349,2349,2349,111,Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % .,Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (3, 4), (4, 5), (5, 7), (7, 12), (12, 14), (14, 16), (16, 17), (17, 20), (21, 23), (23, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.428571429,110,0.547263682,9,0.9,1,results,Image Classification Results,natural_language_inference18
2350,2350,2350,112,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .",Image Classification Results,Image Classification Results,natural_language_inference,18,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (9, 12), (14, 15), (15, 16), (16, 17), (17, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.476190476,111,0.552238806,10,1,1,results,Image Classification Results,natural_language_inference18
2351,2351,2351,2,Natural Language Inference by Tree - Based Convolution and Heuristic Matching,title,title,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00625,1,0,1,research-problem,title,natural_language_inference19
2352,2352,2352,4,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .",abstract,abstract,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (9, 11), (11, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.333333333,3,0.01875,1,0.333333333,1,research-problem,abstract,natural_language_inference19
2353,2353,2353,8,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .,Introduction,Introduction,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(18, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.029411765,7,0.04375,1,0.142857143,1,research-problem,Introduction,natural_language_inference19
2354,2354,2354,10,"Several examples are illustrated in NLI is in the core of natural language understanding and has wide applications in NLP , e.g. , question answering and automatic summarization .",Introduction,Introduction,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.088235294,9,0.05625,3,0.428571429,1,research-problem,Introduction,natural_language_inference19
2355,2355,2355,33,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .",Introduction,E Hypothesis,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 12), (12, 14), (14, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.764705882,32,0.2,18,0.692307692,1,model,Introduction: E Hypothesis,natural_language_inference19
2356,2356,2356,34,"We lever- age our newly proposed TBCNN model to capture structural information in sentences , which is important to NLI .",Introduction,E Hypothesis,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 8), (8, 10), (10, 12), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.794117647,33,0.20625,19,0.730769231,1,model,Introduction: E Hypothesis,natural_language_inference19
2357,2357,2357,36,"As we can see , TBCNN is more robust than sequential convolution in terms of word order distortion , which maybe introduced by determinators , modifiers , etc .",Introduction,E Hypothesis,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 9), (9, 10), (10, 12), (12, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.852941176,35,0.21875,21,0.807692308,1,experiments,Introduction: E Hypothesis,natural_language_inference19
2358,2358,2358,37,"A pooling layer then aggregates information along the tree , serving as away of semantic compositonality .",Introduction,E Hypothesis,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 5), (5, 6), (6, 7), (8, 9), (10, 12), (12, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.882352941,36,0.225,22,0.846153846,1,model,Introduction: E Hypothesis,natural_language_inference19
2359,2359,2359,38,"Finally , two sentences ' information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity .",Introduction,E Hypothesis,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (7, 9), (9, 13), (14, 15), (15, 16), (22, 23), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.911764706,37,0.23125,23,0.884615385,1,model,Introduction: E Hypothesis,natural_language_inference19
2360,2360,2360,128,"All our neural layers , including embeddings , were set to 300 dimensions .",Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (9, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,127,0.79375,1,0.125,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2361,2361,2361,130,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (6, 7), (7, 8), (9, 12), (13, 16), (16, 17), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,129,0.80625,3,0.375,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2362,2362,2362,131,We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 8), (9, 10), (11, 13), (13, 14), (14, 15), (16, 17), (17, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,130,0.8125,4,0.5,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2363,2363,2363,134,"Initial learning rate was set to 1 , and a power decay was applied .",Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 6), (6, 7), (10, 12), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,133,0.83125,7,0.875,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2364,2364,2364,135,We used stochastic gradient descent with a batch size of 50 .,Evaluation,Hyperparameter Settings,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (7, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,134,0.8375,8,1,1,hyperparameters,Evaluation: Hyperparameter Settings,natural_language_inference19
2365,2365,2365,138,"As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of humanengineered features , long short term memory .",Evaluation,Performance,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 8), (9, 11), (11, 13), (15, 16), (16, 22), (27, 28), (29, 33), (33, 34), (34, 39), (40, 44)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.95,137,0.85625,2,0.666666667,1,results,Evaluation: Performance,natural_language_inference19
2366,2366,2366,140,Model Variant,,,natural_language_inference,19,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,139,0.86875,0,0,1,results,,natural_language_inference19
2367,2367,2367,144,We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (8, 12), (13, 14), (14, 16), (16, 17), (17, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,143,0.89375,4,0.4,1,results,Model Variant,natural_language_inference19
2368,2368,2368,145,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .",Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 4), (4, 5), (6, 7), (9, 13), (13, 14), (14, 15), (16, 22), (22, 23), (24, 26), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,144,0.9,5,0.5,1,results,Model Variant,natural_language_inference19
2369,2369,2369,148,Further applying element - wise product improves the accuracy by another 0.5 % .,Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (8, 9), (9, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5,147,0.91875,8,0.8,1,results,Model Variant,natural_language_inference19
2370,2370,2370,149,"The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .",Model Variant,Model Variant,natural_language_inference,19,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (6, 7), (7, 14), (15, 18), (19, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,148,0.925,9,0.9,1,results,Model Variant,natural_language_inference19
2371,2371,2371,2,Stochastic Answer Networks for Natural Language Inference,title,,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00729927,1,0,1,research-problem,title,natural_language_inference2
2372,2372,2372,9,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .",abstract,Motivation,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.090909091,8,0.058394161,6,0.090909091,1,research-problem,abstract: Motivation,natural_language_inference2
2373,2373,2373,17,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .",abstract,Motivation,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 15), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.212121212,16,0.116788321,14,0.212121212,1,research-problem,abstract: Motivation,natural_language_inference2
2374,2374,2374,18,"Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .",abstract,Motivation,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 12), (12, 13), (14, 15), (16, 18), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.227272727,17,0.124087591,15,0.227272727,1,model,abstract: Motivation,natural_language_inference2
2375,2375,2375,80,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (7, 8), (8, 11), (12, 13), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.083333333,79,0.576642336,1,0.083333333,1,experimental-setup,Implementation details,natural_language_inference2
2376,2376,2376,81,We fix word embedding with 300 - dimensional GloVe word vectors .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.166666667,80,0.583941606,2,0.166666667,1,experimental-setup,Implementation details,natural_language_inference2
2377,2377,2377,82,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (8, 9), (9, 10), (11, 15), (15, 16), (16, 22), (24, 26), (26, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,81,0.591240876,3,0.25,1,experimental-setup,Implementation details,natural_language_inference2
2378,2378,2378,84,So lexicon embeddings are d =600 - dimensions .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.416666667,83,0.605839416,5,0.416666667,1,experimental-setup,Implementation details,natural_language_inference2
2379,2379,2379,85,The embedding for the out - of - vocabulary is zeroed .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (4, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,84,0.613138686,6,0.5,1,experimental-setup,Implementation details,natural_language_inference2
2380,2380,2380,86,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (7, 10), (11, 14), (15, 17), (17, 18), (21, 23), (23, 24), (24, 26), (26, 27), (27, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.583333333,85,0.620437956,7,0.583333333,1,experimental-setup,Implementation details,natural_language_inference2
2381,2381,2381,87,The projection size in the attention layer is set to 256 .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 7), (8, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.666666667,86,0.627737226,8,0.666666667,1,experimental-setup,Implementation details,natural_language_inference2
2382,2382,2382,88,"To speedup training , we use weight normalization .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,87,0.635036496,9,0.75,1,experimental-setup,Implementation details,natural_language_inference2
2383,2383,2383,89,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .",Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (8, 10), (11, 13), (13, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.833333333,88,0.642335766,10,0.833333333,1,experimental-setup,Implementation details,natural_language_inference2
2384,2384,2384,90,The mini - batch size is set to 32 .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.916666667,89,0.649635036,11,0.916666667,1,experimental-setup,Implementation details,natural_language_inference2
2385,2385,2385,91,Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,Implementation details,Implementation details,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (6, 8), (9, 11), (11, 12), (13, 15), (15, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1,90,0.656934307,12,1,1,experimental-setup,Implementation details,natural_language_inference2
2386,2386,2386,101,shows that our multi-step model consistently outperforms the single - step model on the dev set of all four datasets in terms of accuracy .,Results,"Here ,",natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (5, 7), (8, 12), (12, 13), (14, 16), (16, 17), (17, 20), (20, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.375,100,0.729927007,9,0.375,1,results,"Results: Here ,",natural_language_inference2
2387,2387,2387,102,"For example , on SciTail dataset , SAN outperforms the single - step model by .",Results,"Here ,",natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (7, 8), (8, 9), (10, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.416666667,101,0.737226277,10,0.416666667,1,results,"Results: Here ,",natural_language_inference2
2388,2388,2388,105,"On SciTail dataset , SAN even outperforms GPT .",Results,"Here ,",natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 5), (6, 7), (7, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.541666667,104,0.759124088,13,0.541666667,1,results,"Results: Here ,",natural_language_inference2
2389,2389,2389,108,"Comparing with Single - step baseline , the proposed model obtains + 2.8 improvement on the Sc - iTail test set ( 94.0 vs 91.2 ) and + 2.1 improvement on the SciTail dev set ( 96.1 vs 93.9 ) .",Results,"Here ,",natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 6), (8, 10), (10, 11), (11, 14), (14, 15), (22, 26), (27, 30), (30, 31), (36, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.666666667,107,0.781021898,16,0.666666667,1,results,"Results: Here ,",natural_language_inference2
2390,2390,2390,128,"Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .",Model,Model,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 6), (6, 7), (7, 9), (9, 11), (12, 14), (15, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.733333333,127,0.927007299,11,0.733333333,1,results,Model,natural_language_inference2
2391,2391,2391,129,"We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .",Model,Model,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 6), (6, 8), (8, 9), (10, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8,128,0.934306569,12,0.8,1,results,Model,natural_language_inference2
2392,2392,2392,130,"Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .",Model,Model,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 5), (7, 9), (9, 10), (10, 11), (12, 23), (23, 24), (24, 28), (31, 32), (33, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.866666667,129,0.941605839,13,0.866666667,1,results,Model,natural_language_inference2
2393,2393,2393,131,"In particular , on the most challenging "" Long Sentence "" and "" Quantity / Time "" categories , SAN 's result is substantially better than previous systems .",Model,Model,natural_language_inference,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 18), (19, 22), (22, 23), (23, 25), (25, 26), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.933333333,130,0.948905109,14,0.933333333,1,results,Model,natural_language_inference2
2394,2394,2394,2,Neural Tree Indexers for Text Understanding,title,,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O']","[(4, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003267974,1,0,1,research-problem,title,natural_language_inference20
2395,2395,2395,19,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .",Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.170212766,18,0.058823529,8,0.363636364,1,model,Introduction,natural_language_inference20
2396,2396,2396,20,NTI takes a sequence of tokens and produces its representation by constructing a full n-ary tree in a bottom - up fashion .,Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 6), (7, 8), (9, 10), (10, 12), (13, 16), (16, 17), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.191489362,19,0.062091503,9,0.409090909,1,model,Introduction,natural_language_inference20
2397,2397,2397,21,Each node in NTI is associated with one of the node transformation functions : leaf node mapping and non-leaf node composition functions .,Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 4), (5, 7), (10, 13), (14, 17), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.212765957,20,0.065359477,10,0.454545455,1,model,Introduction,natural_language_inference20
2398,2398,2398,23,"Furthermore , we propose different variants of node composition function and attention over tree for our NTI models .",Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (6, 7), (7, 14), (14, 15), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.255319149,22,0.071895425,12,0.545454545,1,model,Introduction,natural_language_inference20
2399,2399,2399,24,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :",Introduction,Introduction,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (6, 8), (8, 9), (10, 11), (13, 15), (15, 16), (17, 22), (22, 25), (26, 30), (30, 31), (31, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.276595745,23,0.075163399,13,0.590909091,1,model,Introduction,natural_language_inference20
2400,2400,2400,147,Natural Language Inference,Experiments,,natural_language_inference,20,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",13,0.081761006,146,0.477124183,0,0,1,experiments,Experiments,natural_language_inference20
2401,2401,2401,193,Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (6, 7), (7, 10), (10, 12), (13, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.371069182,192,0.62745098,46,0.821428571,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2402,2402,2402,195,Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % .,Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 7), (7, 8), (9, 10), (10, 11), (12, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.383647799,194,0.633986928,48,0.857142857,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2403,2403,2403,198,"The node - by - node attention models improve the performance , indicating that modeling inter-sentence interaction is an important element in NLI .",Experiments,Natural Language Inference,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 8), (8, 9), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.402515723,197,0.64379085,51,0.910714286,1,experiments,Experiments: Natural Language Inference,natural_language_inference20
2404,2404,2404,204,Answer Sentence Selection,Experiments,,natural_language_inference,20,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",70,0.440251572,203,0.663398693,0,0,1,experiments,Experiments,natural_language_inference20
2405,2405,2405,221,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .",Experiments,Answer Sentence Selection,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (7, 8), (9, 12), (12, 13), (14, 16), (17, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.547169811,220,0.718954248,17,0.85,1,experiments,Experiments: Answer Sentence Selection,natural_language_inference20
2406,2406,2406,222,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,Experiments,Answer Sentence Selection,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (3, 4), (6, 7), (8, 10), (10, 12), (12, 14), (14, 15), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.553459119,221,0.722222222,18,0.9,1,experiments,Experiments: Answer Sentence Selection,natural_language_inference20
2407,2407,2407,224,Our NTI model exceeds NASM by approximately 0.4 % on MAP for this task .,Experiments,Answer Sentence Selection,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (6, 9), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.566037736,223,0.72875817,20,1,1,experiments,Experiments: Answer Sentence Selection,natural_language_inference20
2408,2408,2408,225,Sentence Classification,Experiments,,natural_language_inference,20,"['O', 'O']","[(0, 2)]","['O', 'O']",91,0.572327044,224,0.732026144,0,0,1,experiments,Experiments,natural_language_inference20
2409,2409,2409,234,Our NTI - SLSTM model performed slightly worse A dog mouth holds a retrieved ball .,Experiments,Sentence Classification,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (5, 6), (6, 8), (11, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.628930818,233,0.761437908,9,0.3,1,experiments,Experiments: Sentence Classification,natural_language_inference20
2410,2410,2410,255,"After we transformed the input with the LSTM leaf node function , we achieved the best performance on this task .",Experiments,Sentence Classification,natural_language_inference,20,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (7, 11), (13, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",121,0.761006289,254,0.830065359,30,1,1,experiments,Experiments: Sentence Classification,natural_language_inference20
2411,2411,2411,2,Attention - over - Attention Neural Networks for Reading Comprehension,title,title,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004524887,1,0,1,research-problem,title,natural_language_inference21
2412,2412,2412,4,Cloze - style reading comprehension is a representative problem in mining relationship between document and query .,abstract,abstract,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.013574661,1,0.166666667,1,research-problem,abstract,natural_language_inference21
2413,2413,2413,11,"To read and comprehend the human languages are challenging tasks for the machines , which requires that the understanding of natural languages and the ability to do reasoning over various clues .",Introduction,Introduction,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.007874016,10,0.045248869,1,0.04,1,research-problem,Introduction,natural_language_inference21
2414,2414,2414,20,"In this paper , we present a novel neural network architecture , called attention - over - attention model .",Introduction,Introduction,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (12, 13), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.078740157,19,0.085972851,10,0.4,1,model,Introduction,natural_language_inference21
2415,2415,2415,21,"As we can understand the meaning literally , our model aims to place another attention mechanism over the existing document - level attention .",Introduction,Introduction,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 13), (13, 16), (16, 17), (18, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.086614173,20,0.090497738,11,0.44,1,model,Introduction,natural_language_inference21
2416,2416,2416,22,"Unlike the previous works , that are using heuristic merging functions , or setting various pre-defined non-trainable terms , our model could automatically generate an "" attended attention "" over various document - level attentions , and make a mutual look not only from query - to - document but also document - to - query , which will benefit from the interactive information .",Introduction,Introduction,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(22, 24), (25, 29), (29, 30), (30, 35), (37, 38), (39, 41), (41, 44), (44, 49), (51, 56)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.094488189,21,0.095022624,12,0.48,1,model,Introduction,natural_language_inference21
2417,2417,2417,141,Embedding Layer :,Experimental Setups,Experimental Setups,natural_language_inference,21,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",2,0.1,140,0.633484163,2,0.1,1,experimental-setup,Experimental Setups,natural_language_inference21
2418,2418,2418,142,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ].",Experimental Setups,Experimental Setups,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (8, 10), (10, 11), (12, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.15,141,0.63800905,3,0.15,1,experimental-setup,Experimental Setups,natural_language_inference21
2419,2419,2419,148,Hidden Layer : Internal weights of GRUs are initialized with random orthogonal matrices .,Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (5, 6), (6, 7), (8, 10), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,147,0.665158371,9,0.45,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2420,2420,2420,150,"We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 7), (8, 9), (10, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,149,0.674208145,11,0.55,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2421,2421,2421,151,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 14), (15, 18), (18, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,150,0.678733032,12,0.6,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2422,2422,2422,152,We used batched training strategy of 32 samples .,Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.65,151,0.683257919,13,0.65,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2423,2423,2423,154,"In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (6, 10), (10, 11), (12, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,153,0.692307692,15,0.75,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2424,2424,2424,155,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (8, 10), (10, 11), (11, 13), (14, 15), (15, 20), (21, 25), (25, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,154,0.696832579,16,0.8,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2425,2425,2425,158,"Implementation is done with Theano ( Theano Development Team , 2016 ) and Keras , and all models are trained on Tesla K40 GPU . :",Experimental Setups,News,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (13, 14), (19, 21), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.95,157,0.71040724,19,0.95,1,experimental-setup,Experimental Setups: News,natural_language_inference21
2426,2426,2426,163,"As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 9), (9, 10), (10, 18), (18, 19), (20, 22), (23, 24), (24, 31), (31, 32), (32, 33), (33, 34), (34, 40)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.090909091,162,0.733031674,3,0.375,1,results,Overall Results,natural_language_inference21
2427,2427,2427,164,"Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 6), (7, 9), (12, 15), (15, 20), (20, 21), (21, 24), (24, 25), (25, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.121212121,163,0.737556561,4,0.5,1,results,Overall Results,natural_language_inference21
2428,2428,2428,165,"We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 8), (10, 13), (14, 18), (24, 26), (26, 27), (27, 29), (29, 30), (31, 38), (38, 39), (40, 44)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.151515152,164,0.742081448,5,0.625,1,results,Overall Results,natural_language_inference21
2429,2429,2429,166,"When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 10), (11, 12), (12, 14), (14, 15), (15, 19), (19, 20), (21, 23), (24, 25), (26, 35)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.181818182,165,0.746606335,6,0.75,1,results,Overall Results,natural_language_inference21
2430,2430,2430,168,"Instead of using pre-defined merging heuristics , and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance , where 4.1 % and 3.7 % improvements can be made in CNN validation and test set against CAS Reader .",Overall Results,Overall Results,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (8, 9), (10, 11), (11, 13), (14, 15), (15, 16), (16, 18), (18, 20), (21, 23), (23, 24), (25, 26), (27, 28), (28, 34), (37, 38), (38, 43), (43, 44), (44, 46)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.242424242,167,0.755656109,8,1,1,results,Overall Results,natural_language_inference21
2431,2431,2431,172,"From the results in , we found that the NE and CN category both benefit a lot from the re-ranking features , but the proportions are quite different .",Overall Results,Effectiveness of Re-ranking Strategy,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (9, 13), (14, 17), (17, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.363636364,171,0.773755656,3,0.272727273,1,results,Overall Results: Effectiveness of Re-ranking Strategy,natural_language_inference21
2432,2432,2432,173,"Generally speaking , in NE category , the performance is mainly boosted by the LM local feature .",Overall Results,Effectiveness of Re-ranking Strategy,natural_language_inference,21,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (8, 9), (11, 13), (14, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.393939394,172,0.778280543,4,0.363636364,1,results,Overall Results: Effectiveness of Re-ranking Strategy,natural_language_inference21
2433,2433,2433,2,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems,title,title,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004545455,1,0,1,research-problem,title,natural_language_inference22
2434,2434,2434,4,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,abstract,abstract,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.013636364,1,0.032258065,1,research-problem,abstract,natural_language_inference22
2435,2435,2435,5,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,abstract,abstract,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (8, 9), (16, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.333333333,4,0.018181818,2,0.064516129,1,research-problem,abstract,natural_language_inference22
2436,2436,2436,28,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .",I. INTRODUCTION,I. INTRODUCTION,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 7), (8, 10), (10, 11), (11, 12), (12, 13), (14, 16), (18, 19), (20, 25), (27, 28), (29, 32), (34, 38)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.391304348,27,0.122727273,25,0.806451613,1,model,I. INTRODUCTION,natural_language_inference22
2437,2437,2437,29,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .",I. INTRODUCTION,I. INTRODUCTION,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (7, 8), (9, 15), (15, 16), (17, 22), (24, 25), (29, 32), (32, 34), (34, 35), (35, 38), (47, 48), (49, 52)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.413043478,28,0.127272727,26,0.838709677,1,model,I. INTRODUCTION,natural_language_inference22
2438,2438,2438,153,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (5, 6), (7, 13), (13, 14), (15, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.043478261,152,0.690909091,1,0.125,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2439,2439,2439,154,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (4, 6), (6, 7), (7, 9), (11, 13), (13, 14), (14, 15), (16, 18), (18, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.086956522,153,0.695454545,2,0.25,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2440,2440,2440,155,We set = 0.8 in the multi - task loss function .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.130434783,154,0.7,3,0.375,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2441,2441,2441,157,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (4, 6), (7, 8), (8, 9), (10, 13), (14, 15), (16, 18), (18, 19), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.217391304,156,0.709090909,5,0.625,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2442,2442,2442,158,An Adam optimizer is used to optimize all the trainable weights .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (7, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.260869565,157,0.713636364,6,0.75,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2443,2443,2443,159,The learning rate is set to 4e - 4 and the batch size is set to 200 .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 9), (11, 13), (14, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.304347826,158,0.718181818,7,0.875,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2444,2444,2444,160,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (5, 6), (6, 7), (7, 10), (12, 14), (14, 15), (16, 18), (18, 19), (19, 22), (24, 26), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.347826087,159,0.722727273,8,1,1,hyperparameters,B. Settings of Experiments,natural_language_inference22
2445,2445,2445,163,ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (6, 7), (8, 12), (12, 13), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.47826087,162,0.736363636,2,0.142857143,1,baselines,B. Settings of Experiments,natural_language_inference22
2446,2446,2446,166,BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7), (7, 8), (9, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.608695652,165,0.75,5,0.357142857,1,baselines,B. Settings of Experiments,natural_language_inference22
2447,2447,2447,168,"SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .",B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7), (7, 8), (9, 14), (16, 17), (17, 21), (21, 22), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.695652174,167,0.759090909,7,0.5,1,baselines,B. Settings of Experiments,natural_language_inference22
2448,2448,2448,170,DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .,B. Settings of Experiments,B. Settings of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (12, 13), (13, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608696,169,0.768181818,9,0.642857143,1,baselines,B. Settings of Experiments,natural_language_inference22
2449,2449,2449,178,"Quora dataset : BiMPM and ESIM models without any sentence interaction information , and is very close to DIIN , the state - of - the - art interaction - based model , but we do n't any external knowledge in our method .",D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 7), (7, 8), (8, 12), (15, 18), (18, 19), (21, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.166666667,177,0.804545455,2,0.166666667,1,results,D. Results of Experiments,natural_language_inference22
2450,2450,2450,179,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (5, 6), (6, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,178,0.809090909,3,0.25,1,results,D. Results of Experiments,natural_language_inference22
2451,2451,2451,180,The experimental results show that our model outperforms state - of the - art models .,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (7, 8), (8, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.333333333,179,0.813636364,4,0.333333333,1,results,D. Results of Experiments,natural_language_inference22
2452,2452,2452,181,BQ dataset : BQ dataset is a specific - domain dataset with a low average overlap rate .,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (5, 6), (7, 11), (11, 12), (13, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.416666667,180,0.818181818,5,0.416666667,1,results,D. Results of Experiments,natural_language_inference22
2453,2453,2453,182,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .",D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 15), (15, 16), (17, 19), (20, 21), (21, 25), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,181,0.822727273,6,0.5,1,results,D. Results of Experiments,natural_language_inference22
2454,2454,2454,183,TCS dataset :,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",7,0.583333333,182,0.827272727,7,0.583333333,1,results,D. Results of Experiments,natural_language_inference22
2455,2455,2455,184,As shown in show that our MSEM model achieves the best performance .,D. Results of Experiments,D. Results of Experiments,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.666666667,183,0.831818182,8,0.666666667,1,results,D. Results of Experiments,natural_language_inference22
2456,2456,2456,193,We first study the contribution of the ARU component .,E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 6), (7, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,192,0.872727273,4,0.4,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2457,2457,2457,194,"The accuracy decreases , the accuracy will drop to 88.25 % .",E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (5, 6), (7, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,193,0.877272727,5,0.5,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2458,2458,2458,196,It turns out that the attentive pooling is better than max pooling .,E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 7), (7, 8), (8, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,195,0.886363636,7,0.7,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2459,2459,2459,197,"Then if we remove the highway network , the accuracy will drop to 88.36 % .",E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 7), (9, 10), (11, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,196,0.890909091,8,0.8,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2460,2460,2460,198,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .",E. Ablation Study,E. Ablation Study,natural_language_inference,22,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 9), (11, 12), (13, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,197,0.895454545,9,0.9,1,ablation-analysis,E. Ablation Study,natural_language_inference22
2461,2461,2461,2,Deep Fusion LSTMs for Text Semantic Matching,title,,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004694836,1,0,1,research-problem,title,natural_language_inference23
2462,2462,2462,14,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .",Introduction,Introduction,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,13,0.061032864,1,0.038461538,1,research-problem,Introduction,natural_language_inference23
2463,2463,2463,25,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 10), (10, 12), (13, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.038461538,24,0.112676056,12,0.461538462,1,model,Strong Interaction Models,natural_language_inference23
2464,2464,2464,28,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (8, 9), (10, 11), (14, 15), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.076923077,27,0.126760563,15,0.576923077,1,model,Strong Interaction Models,natural_language_inference23
2465,2465,2465,29,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 20), (20, 22), (23, 24), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.08974359,28,0.131455399,16,0.615384615,1,model,Strong Interaction Models,natural_language_inference23
2466,2466,2466,30,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .",Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (6, 8), (8, 12), (16, 17), (19, 20), (20, 21), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.102564103,29,0.136150235,17,0.653846154,1,model,Strong Interaction Models,natural_language_inference23
2467,2467,2467,31,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,Strong Interaction Models,Strong Interaction Models,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (8, 10), (11, 16), (16, 18), (19, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.115384615,30,0.14084507,18,0.692307692,1,model,Strong Interaction Models,natural_language_inference23
2468,2468,2468,139,Neural bag - of - words ( NBOW ) :,Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.071428571,138,0.647887324,1,0.071428571,1,baselines,Competitor Methods,natural_language_inference23
2469,2469,2469,140,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 8), (9, 10), (10, 11), (12, 13), (19, 23), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.142857143,139,0.65258216,2,0.142857143,1,baselines,Competitor Methods,natural_language_inference23
2470,2470,2470,141,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 8), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.214285714,140,0.657276995,3,0.214285714,1,baselines,Competitor Methods,natural_language_inference23
2471,2471,2471,142,"Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 9), (9, 11), (9, 12), (16, 20), (21, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.285714286,141,0.661971831,4,0.285714286,1,baselines,Competitor Methods,natural_language_inference23
2472,2472,2472,143,"Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 8), (8, 9), (9, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.357142857,142,0.666666667,5,0.357142857,1,baselines,Competitor Methods,natural_language_inference23
2473,2473,2473,144,"Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :",Competitor Methods,Competitor Methods,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7), (11, 12), (16, 17), (17, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.428571429,143,0.671361502,6,0.428571429,1,baselines,Competitor Methods,natural_language_inference23
2474,2474,2474,155,"we can see that the proposed model also shows its superiority on this task , which outperforms the stateof - the - arts methods on both metrics ( P@1 ( 5 ) and P@1 ( 10 ) ) with a large margin .",Results,Results,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 7), (8, 9), (10, 11), (16, 17), (18, 24), (24, 25), (25, 27), (38, 39), (40, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.046511628,154,0.723004695,2,0.095238095,1,results,Results,natural_language_inference23
2475,2475,2475,156,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .",Results,Results,natural_language_inference,23,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 8), (12, 13), (25, 27), (28, 37), (37, 38), (39, 41)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.069767442,155,0.727699531,3,0.142857143,1,results,Results,natural_language_inference23
2476,2476,2476,2,Reading Wikipedia to Answer Open-Domain Questions,title,title,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O']","[(4, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004464286,1,0,1,research-problem,title,natural_language_inference24
2477,2477,2477,9,"This paper considers the problem of answering factoid questions in an open - domain setting using Wikipedia as the unique knowledge source , such as one does when looking for answers in an encyclopedia .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 15), (15, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.045454545,8,0.035714286,1,0.045454545,1,research-problem,Introduction,natural_language_inference24
2478,2478,2478,11,"Unlike knowledge bases ( KBs ) such as Freebase or DB - Pedia , which are easier for computers to process but too sparsely populated for open - domain question answering , Wikipedia contains up - to - date knowledge that humans are interested in .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.136363636,10,0.044642857,3,0.136363636,1,research-problem,Introduction,natural_language_inference24
2479,2479,2479,14,"In order to answer any question , one must first retrieve the few relevant articles among more than 5 million items , and then scan them carefully to identify the answer .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 15), (15, 16), (16, 21), (24, 25), (26, 27), (27, 29), (30, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.272727273,13,0.058035714,6,0.272727273,1,model,Introduction,natural_language_inference24
2480,2480,2480,15,"We term this setting , machine reading at scale ( MRS ) .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.318181818,14,0.0625,7,0.318181818,1,research-problem,Introduction,natural_language_inference24
2481,2481,2481,16,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (6, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.363636364,15,0.066964286,8,0.363636364,1,model,Introduction,natural_language_inference24
2482,2482,2482,17,"As a result , our approach is generic and could be switched to other collections of documents , books , or even daily updated newspapers .",Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (6, 7), (7, 8), (9, 11), (11, 13), (13, 17), (22, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.409090909,16,0.071428571,9,0.409090909,1,model,Introduction,natural_language_inference24
2483,2483,2483,20,Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once .,Introduction,Introduction,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 5), (5, 6), (7, 8), (8, 10), (10, 12), (12, 13), (13, 15), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.545454545,19,0.084821429,12,0.545454545,1,model,Introduction,natural_language_inference24
2484,2484,2484,174,We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,Implementation details,Implementation details,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 7), (7, 8), (8, 13), (13, 14), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,173,0.772321429,2,0.25,1,experimental-setup,Implementation details,natural_language_inference24
2485,2485,2485,175,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .",Implementation details,Implementation details,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 8), (10, 11), (11, 12), (13, 16), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.285714286,174,0.776785714,3,0.375,1,experimental-setup,Implementation details,natural_language_inference24
2486,2486,2486,176,"Lastly , all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each .",Implementation details,Implementation details,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 6), (7, 9), (10, 11), (11, 12), (12, 13), (14, 16), (16, 17), (17, 18), (18, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.428571429,175,0.78125,4,0.5,1,experimental-setup,Implementation details,natural_language_inference24
2487,2487,2487,177,We use Adamax for optimization as described in .,Implementation details,,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (4, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.571428571,176,0.785714286,5,0.625,1,experimental-setup,Implementation details,natural_language_inference24
2488,2488,2488,178,Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,Implementation details,We use Adamax for optimization as described in .,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 5), (6, 8), (8, 10), (11, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.714285714,177,0.790178571,6,0.75,1,experimental-setup,Implementation details: We use Adamax for optimization as described in .,natural_language_inference24
2489,2489,2489,182,"Our system ( single model ) can achieve 70.0 % exact match and 79.0 % F 1 scores on the test set , which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing .",Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6), (8, 12), (13, 18), (18, 19), (20, 22), (24, 25), (25, 29), (31, 32), (33, 35), (35, 36), (37, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.066666667,181,0.808035714,1,0.166666667,1,results,Result and analysis,natural_language_inference24
2490,2490,2490,186,"Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .",Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (17, 19), (21, 24), (24, 25), (25, 26), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.333333333,185,0.825892857,5,0.833333333,1,results,Result and analysis,natural_language_inference24
2491,2491,2491,187,"More interestingly , if we remove both f aligned and f exact match , the performance drops dramatically , so we conclude that both features play a similar but complementary role in the feature representation related to the paraphrased nature of a question vs. the context around an answer .",Result and analysis,Result and analysis,natural_language_inference,24,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 13), (15, 16), (16, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,186,0.830357143,6,1,1,ablation-analysis,Result and analysis,natural_language_inference24
2492,2492,2492,2,A Deep Cascade Model for Multi - Document Reading Comprehension,title,title,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003891051,1,0,1,research-problem,title,natural_language_inference25
2493,2493,2493,13,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.018867925,12,0.046692607,1,0.028571429,1,research-problem,Introduction,natural_language_inference25
2494,2494,2494,14,"From the early cloze - style test to answer extraction from a single paragraph , and to the more complex open - domain question answering from web data , great efforts have been made to push the MRC technique to more practical applications .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.037735849,13,0.050583658,2,0.057142857,1,research-problem,Introduction,natural_language_inference25
2495,2495,2495,29,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 12), (13, 14), (15, 16), (16, 17), (17, 19), (19, 20), (21, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.320754717,28,0.108949416,17,0.485714286,1,model,Introduction,natural_language_inference25
2496,2496,2496,30,The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .,Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (5, 9), (10, 11), (11, 12), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.339622642,29,0.112840467,18,0.514285714,1,model,Introduction,natural_language_inference25
2497,2497,2497,31,"At early stages of the model , simple features and ranking functions are used to select a candidate set of most relevant contents , filtering out the irrelevant documents and paragraphs as much as possible .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (7, 12), (14, 16), (17, 19), (19, 20), (20, 23), (24, 26), (27, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.358490566,30,0.116731518,19,0.542857143,1,model,Introduction,natural_language_inference25
2498,2498,2498,32,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 7), (8, 14), (14, 16), (17, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.377358491,31,0.120622568,20,0.571428571,1,model,Introduction,natural_language_inference25
2499,2499,2499,33,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.396226415,32,0.124513619,21,0.6,1,model,Introduction,natural_language_inference25
2500,2500,2500,34,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 7), (7, 8), (9, 13), (15, 16), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.41509434,33,0.128404669,22,0.628571429,1,model,Introduction,natural_language_inference25
2501,2501,2501,35,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 6), (6, 8), (9, 15), (15, 16), (16, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.433962264,34,0.13229572,23,0.657142857,1,model,Introduction,natural_language_inference25
2502,2502,2502,36,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 13), (13, 15), (16, 18), (19, 21), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.452830189,35,0.13618677,24,0.685714286,1,model,Introduction,natural_language_inference25
2503,2503,2503,37,The first module takes the question and a collection of raw documents as input .,Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (5, 6), (8, 12), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.471698113,36,0.140077821,25,0.714285714,1,model,Introduction,natural_language_inference25
2504,2504,2504,38,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 6), (6, 7), (8, 9), (9, 10), (11, 13), (15, 17), (18, 24), (24, 25), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.490566038,37,0.143968872,26,0.742857143,1,model,Introduction,natural_language_inference25
2505,2505,2505,40,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 7), (8, 10), (10, 12), (12, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.528301887,39,0.151750973,28,0.8,1,model,Introduction,natural_language_inference25
2506,2506,2506,41,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .",Introduction,Introduction,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 9), (10, 16), (19, 22), (23, 27), (27, 28), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.547169811,40,0.155642023,29,0.828571429,1,model,Introduction,natural_language_inference25
2507,2507,2507,208,We choose K = 4 and N = 2 for the good performance when evaluating on the dev set .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 9), (9, 10), (11, 13), (17, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.166666667,207,0.805447471,2,0.048780488,1,experimental-setup,Implementation Details,natural_language_inference25
2508,2508,2508,211,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .",Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 6), (8, 9), (10, 12), (12, 13), (13, 14), (15, 16), (17, 19), (19, 20), (20, 21), (22, 25), (25, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.416666667,210,0.817120623,5,0.12195122,1,experimental-setup,Implementation Details,natural_language_inference25
2509,2509,2509,212,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 8), (8, 9), (9, 10), (11, 12), (13, 17), (17, 18), (19, 22), (22, 23), (23, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,211,0.821011673,6,0.146341463,1,experimental-setup,Implementation Details,natural_language_inference25
2510,2510,2510,213,The word embeddings are fixed during training .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.583333333,212,0.824902724,7,0.170731707,1,experimental-setup,Implementation Details,natural_language_inference25
2511,2511,2511,214,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (6, 8), (8, 9), (9, 10), (10, 11), (12, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.666666667,213,0.828793774,8,0.195121951,1,experimental-setup,Implementation Details,natural_language_inference25
2512,2512,2512,215,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 7), (16, 18), (18, 25), (26, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,214,0.832684825,9,0.219512195,1,experimental-setup,Implementation Details,natural_language_inference25
2513,2513,2513,216,16 is set as a small value of 0.01 .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.833333333,215,0.836575875,10,0.243902439,1,experimental-setup,Implementation Details,natural_language_inference25
2514,2514,2514,217,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,Implementation Details,Implementation Details,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 9), (9, 10), (10, 13), (13, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.916666667,216,0.840466926,11,0.268292683,1,experimental-setup,Implementation Details,natural_language_inference25
2515,2515,2515,221,"We can see that by adopting the deep cascade learning framework , the proposed model outperforms the previous state - of - the - art methods by an evident margin on both datasets , which validates the effectiveness of the proposed method in addressing the challenging multi-document MRC task .",Main Results,Main Results,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 6), (7, 11), (13, 15), (15, 16), (17, 26), (26, 27), (28, 30), (30, 31), (31, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,1,220,0.856031128,15,0.365853659,1,results,Main Results,natural_language_inference25
2516,2516,2516,226,"From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 14), (14, 15), (16, 18), (18, 19), (19, 21), (21, 22), (22, 24), (39, 42), (42, 43), (43, 45), (62, 63), (64, 66), (66, 67), (67, 70)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.210526316,225,0.875486381,20,0.487804878,1,ablation-analysis,Ablation Study,natural_language_inference25
2517,2517,2517,227,"By incorporating the manual features , the performance can be further improved slightly .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (7, 8), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.263157895,226,0.879377432,21,0.512195122,1,ablation-analysis,Ablation Study,natural_language_inference25
2518,2518,2518,228,"2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 12), (13, 15), (16, 18), (20, 22), (23, 27), (27, 28), (29, 32), (33, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.315789474,227,0.883268482,22,0.536585366,1,ablation-analysis,Ablation Study,natural_language_inference25
2519,2519,2519,230,"Jointly training the three extraction tasks can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .",Ablation Study,Ablation Study,natural_language_inference,25,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (7, 8), (8, 10), (23, 24), (24, 26), (26, 27), (27, 29), (29, 30), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.421052632,229,0.891050584,24,0.585365854,1,ablation-analysis,Ablation Study,natural_language_inference25
2520,2520,2520,2,U - Net : Machine Reading Comprehension with Unanswerable Questions,title,title,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003508772,1,0,1,research-problem,title,natural_language_inference26
2521,2521,2521,12,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .",Introduction,Introduction,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.026315789,11,0.038596491,1,0.026315789,1,research-problem,Introduction,natural_language_inference26
2522,2522,2522,13,"Benefiting from the rapid development of deep learning techniques and large - scale benchmarks , the end - to - end neural methods have achieved promising results on MRC task .",Introduction,Introduction,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.052631579,12,0.042105263,2,0.052631579,1,research-problem,Introduction,natural_language_inference26
2523,2523,2523,37,"In this paper , we decompose the problem of MRC with unanswerable questions into three sub - tasks : answer pointer , no - answer pointer , and answer verifier .",Introduction,Plausible Answer : later laws Question,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 9), (9, 10), (13, 14), (14, 18), (19, 21), (22, 26), (28, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.684210526,36,0.126315789,26,0.684210526,1,research-problem,Introduction: Plausible Answer : later laws Question,natural_language_inference26
2524,2524,2524,39,We propose the U - Net to incorporate these three sub - tasks into a unified model :,Introduction,Plausible Answer : later laws Question,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 8), (9, 13), (13, 14), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.736842105,38,0.133333333,28,0.736842105,1,model,Introduction: Plausible Answer : later laws Question,natural_language_inference26
2525,2525,2525,40,"1 ) an answer pointer to predict a can - didate answer span for a question ; 2 ) a no -answer pointer to avoid selecting any text span when a question has no answer ; and 3 ) an answer verifier to determine the probability of the "" unanswerability "" of a question with candidate answer information .",Introduction,Plausible Answer : later laws Question,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 7), (8, 13), (13, 14), (15, 16), (20, 23), (23, 25), (25, 26), (26, 29), (29, 30), (31, 32), (33, 35), (40, 42), (42, 44), (45, 46), (46, 47), (51, 52), (53, 54), (54, 55), (55, 58)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.763157895,39,0.136842105,29,0.763157895,1,model,Introduction: Plausible Answer : later laws Question,natural_language_inference26
2526,2526,2526,41,"Additionally , we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens , which greatly improves the conciseness of U - Net .",Introduction,Plausible Answer : later laws Question,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 8), (9, 10), (11, 16), (16, 17), (18, 23), (25, 27), (28, 29), (29, 30), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.789473684,40,0.140350877,30,0.789473684,1,model,Introduction: Plausible Answer : later laws Question,natural_language_inference26
2527,2527,2527,42,The universal node acts on both question and passage to learn whether the question is answerable .,Introduction,Plausible Answer : later laws Question,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (6, 9), (9, 11), (13, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.815789474,41,0.143859649,31,0.815789474,1,model,Introduction: Plausible Answer : later laws Question,natural_language_inference26
2528,2528,2528,180,"We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 9), (9, 11), (13, 15), (16, 18), (19, 21), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,179,0.628070175,1,0.1,1,experimental-setup,Implementation Details,natural_language_inference26
2529,2529,2529,181,"We use 12 dimensions to embed POS tags , 8 for NER tags .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 6), (6, 8), (9, 10), (10, 11), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,180,0.631578947,2,0.2,1,experimental-setup,Implementation Details,natural_language_inference26
2530,2530,2530,182,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (6, 8), (9, 13), (14, 16), (16, 17), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3,181,0.635087719,3,0.3,1,experimental-setup,Implementation Details,natural_language_inference26
2531,2531,2531,183,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 9), (10, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,182,0.638596491,4,0.4,1,experimental-setup,Implementation Details,natural_language_inference26
2532,2532,2532,184,All the LSTM blocks are bi-directional with one single layer .,Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (5, 6), (6, 7), (7, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,183,0.642105263,5,0.5,1,experimental-setup,Implementation Details,natural_language_inference26
2533,2533,2533,185,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 8), (9, 12), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,184,0.645614035,6,0.6,1,experimental-setup,Implementation Details,natural_language_inference26
2534,2534,2534,186,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .",Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 9), (10, 11), (12, 14), (15, 16), (17, 19), (19, 20), (20, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7,185,0.649122807,7,0.7,1,experimental-setup,Implementation Details,natural_language_inference26
2535,2535,2535,187,We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,Implementation Details,Implementation Details,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (8, 9), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,186,0.652631579,8,0.8,1,experimental-setup,Implementation Details,natural_language_inference26
2536,2536,2536,191,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .",Main Results,Main Results,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 7), (7, 8), (8, 9), (11, 13), (13, 14), (14, 15), (15, 16), (17, 19), (22, 25), (25, 26), (26, 27), (29, 31), (31, 32), (32, 33), (33, 34), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,190,0.666666667,1,0.2,1,results,Main Results,natural_language_inference26
2537,2537,2537,192,Our model outperforms most of the previous approaches .,Main Results,,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 8)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,191,0.670175439,2,0.4,1,results,Main Results,natural_language_inference26
2538,2538,2538,193,"Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .",Main Results,Our model outperforms most of the previous approaches .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 7), (8, 10), (12, 14), (15, 16), (17, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,192,0.673684211,3,0.6,1,results,Main Results: Our model outperforms most of the previous approaches .,natural_language_inference26
2539,2539,2539,194,"In fact , among all the end - to - end models , we achieve the best F1 scores .",Main Results,Our model outperforms most of the previous approaches .,natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 12), (14, 15), (16, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,193,0.677192982,4,0.8,1,results,Main Results: Our model outperforms most of the previous approaches .,natural_language_inference26
2540,2540,2540,203,"Our results showed that when node U is shared , as it is called ' universal ' , it learns information interaction between the question and passage , and when it is not shared , the performance slightly degraded .",Ablation Study,"First , we remove the universal node U .",natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 5), (5, 7), (7, 8), (8, 9), (19, 20), (20, 22), (22, 23), (24, 27), (29, 30), (32, 34), (36, 37), (37, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.12962963,202,0.70877193,7,0.179487179,1,ablation-analysis,"Ablation Study: First , we remove the universal node U .",natural_language_inference26
2541,2541,2541,206,"Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .",Ablation Study,"First , we remove the universal node U .",natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.185185185,205,0.719298246,10,0.256410256,1,ablation-analysis,"Ablation Study: First , we remove the universal node U .",natural_language_inference26
2542,2542,2542,207,"After removing the plausible answer pointer , the performance also dropped , indicating the plausible answers are useful to improve the model even though they are incorrect .",Ablation Study,"First , we remove the universal node U .",natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 6), (8, 9), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.203703704,206,0.722807018,11,0.282051282,1,ablation-analysis,"Ablation Study: First , we remove the universal node U .",natural_language_inference26
2543,2543,2543,208,"After removing the answer verifier , the performance dropped greatly , indicating it is vital for our model .",Ablation Study,"First , we remove the universal node U .",natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (7, 8), (8, 9), (8, 10), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.222222222,207,0.726315789,12,0.307692308,1,ablation-analysis,"Ablation Study: First , we remove the universal node U .",natural_language_inference26
2544,2544,2544,210,"In the second block ( multi - level attention ) of the U - Net , we do not split the output of the encoded presentation and let it pass through a self - attention layer .",Ablation Study,"First , we remove the universal node U .",natural_language_inference,26,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (10, 11), (12, 15), (17, 20), (21, 22), (22, 23), (24, 26), (27, 29), (29, 31), (32, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.259259259,209,0.733333333,14,0.358974359,1,ablation-analysis,"Ablation Study: First , we remove the universal node U .",natural_language_inference26
2545,2545,2545,2,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING,title,title,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.006578947,1,0,1,research-problem,title,natural_language_inference27
2546,2546,2546,4,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,abstract,abstract,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,3,0.019736842,1,0.142857143,1,research-problem,abstract,natural_language_inference27
2547,2547,2547,5,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",abstract,abstract,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.285714286,4,0.026315789,2,0.285714286,1,research-problem,abstract,natural_language_inference27
2548,2548,2548,19,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .",INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (9, 16), (16, 17), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.470588235,18,0.118421053,8,0.470588235,1,model,INTRODUCTION,natural_language_inference27
2549,2549,2549,21,"Firstly , we apply both inter-attention and self - attention on passage and question to obtain a more effective understanding of the passage and dialogue history .",INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 10), (10, 11), (11, 14), (14, 16), (17, 20), (20, 21), (22, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.588235294,20,0.131578947,10,0.588235294,1,model,INTRODUCTION,natural_language_inference27
2550,2550,2550,22,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .",INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 7), (7, 8), (8, 9), (10, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.647058824,21,0.138157895,11,0.647058824,1,model,INTRODUCTION,natural_language_inference27
2551,2551,2551,23,"Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .",INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 19), (18, 19), (20, 22), (22, 23), (23, 26), (27, 28), (28, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.705882353,22,0.144736842,12,0.705882353,1,model,INTRODUCTION,natural_language_inference27
2552,2552,2552,24,"Thirdly , we prepend previous rounds of questions and answers to the current question to incorporate contextual information .",INTRODUCTION,INTRODUCTION,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 10), (4, 6), (10, 11), (12, 14), (14, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.764705882,23,0.151315789,13,0.764705882,1,model,INTRODUCTION,natural_language_inference27
2553,2553,2553,128,"As shown , SDNet achieves significantly better results than baseline models .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (5, 8), (8, 9), (9, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.428571429,127,0.835526316,12,0.428571429,1,results,Results .,natural_language_inference27
2554,2554,2554,129,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 7), (7, 8), (8, 11), (11, 12), (12, 14), (15, 17), (17, 24), (24, 25), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.571428571,128,0.842105263,13,0.464285714,1,results,Results .,natural_language_inference27
2555,2555,2555,130,"Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (5, 9), (9, 10), (10, 12), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.714285714,129,0.848684211,14,0.5,1,results,Results .,natural_language_inference27
2556,2556,2556,132,"As seen , SDNet overpasses all but one baseline models after the second epoch , and achieves state - of - the - art results only after 8 epochs .",Results .,Results .,natural_language_inference,27,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 5), (5, 10), (10, 11), (12, 14), (16, 17), (17, 25), (26, 27), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,1,131,0.861842105,16,0.571428571,1,results,Results .,natural_language_inference27
2557,2557,2557,2,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS,title,title,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.004098361,1,0,1,research-problem,title,natural_language_inference28
2558,2558,2558,23,"In this paper , we investigate this problem through a simple story understanding scenario , in which the agent is given a sequence of textual statements and events , and then given another series of statements about the final state of the world .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (10, 14), (18, 19), (20, 21), (31, 32), (36, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.379310345,22,0.090163934,11,0.379310345,1,research-problem,INTRODUCTION,natural_language_inference28
2559,2559,2559,28,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (8, 16), (17, 18), (19, 24), (26, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.551724138,27,0.110655738,16,0.551724138,1,model,INTRODUCTION,natural_language_inference28
2560,2560,2560,29,"The model consists of a fixed number of dynamic memory cells , each containing a vector key w j and a vector value ( or content ) h j .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 11), (13, 14), (15, 19), (21, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.586206897,28,0.114754098,17,0.586206897,1,model,INTRODUCTION,natural_language_inference28
2561,2561,2561,30,"Each cell is associated with its own "" processor "" , a simple gated recurrent network that may update the cell value given an input .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (3, 5), (6, 10), (12, 16), (20, 22), (22, 23), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.620689655,29,0.118852459,18,0.620689655,1,model,INTRODUCTION,natural_language_inference28
2562,2562,2562,33,"Alternatively , the EntNet can be seen as a bank of gated RNNs ( all sharing the same parameters ) , whose hidden states correspond to latent concepts and attributes , and whose parameters describe the laws of the world according to which the attributes of objects are updated .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (6, 8), (9, 13), (21, 22), (22, 24), (24, 26), (26, 30), (33, 34), (34, 35), (36, 40), (40, 43), (44, 45), (45, 46), (46, 47), (48, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.724137931,32,0.131147541,21,0.724137931,1,model,INTRODUCTION,natural_language_inference28
2563,2563,2563,35,"Their hidden state is updated only when new information relevant to their concept is received , and remains otherwise unchanged .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (7, 9), (9, 11), (12, 13), (13, 14), (14, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.793103448,34,0.139344262,23,0.793103448,1,model,INTRODUCTION,natural_language_inference28
2564,2564,2564,36,"The keys used in the addressing / gating mechanism also correspond to concepts or entities , but are modified only during learning , not during inference .",INTRODUCTION,INTRODUCTION,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (5, 9), (10, 12), (12, 15), (18, 19), (21, 22), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.827586207,35,0.143442623,24,0.827586207,1,model,INTRODUCTION,natural_language_inference28
2565,2565,2565,127,SYNTHETIC WORLD MODEL TASK,EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",3,0.107142857,126,0.516393443,0,0,1,experiments,EXPERIMENTS,natural_language_inference28
2566,2566,2566,134,"For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (5, 6), (7, 10), (10, 12), (12, 15), (17, 19), (19, 20), (20, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.357142857,133,0.545081967,7,0.368421053,1,hyperparameters,EXPERIMENTS,natural_language_inference28
2567,2567,2567,135,"The EntNet had embedding dimension d = 20 and 5 memory slots , and the LSTM had 50 hidden units which resulted in it having significantly more parameters than the other two models .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (9, 12), (15, 16), (17, 20), (25, 28), (28, 29), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.392857143,134,0.549180328,8,0.421052632,1,hyperparameters,EXPERIMENTS,natural_language_inference28
2568,2568,2568,137,"All models were trained with ADAM with initial learning rates set by grid search over { 0.1 , 0.01 , 0.001 } and divided by 2 every 10,000 updates .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (5, 6), (6, 7), (7, 10), (10, 12), (12, 14), (14, 15), (15, 22), (23, 25), (25, 26), (26, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.464285714,136,0.557377049,10,0.526315789,1,hyperparameters,EXPERIMENTS,natural_language_inference28
2569,2569,2569,139,"The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 6), (8, 9), (9, 10), (10, 11), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.535714286,138,0.56557377,12,0.631578947,1,results,EXPERIMENTS,natural_language_inference28
2570,2570,2570,140,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .",EXPERIMENTS,EXPERIMENTS,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (7, 8), (8, 9), (9, 10), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.571428571,139,0.569672131,13,0.684210526,1,results,EXPERIMENTS,natural_language_inference28
2571,2571,2571,171,CHILDRE N'S BOOK TEST ( CBT ),Training Details,Training Details,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608696,170,0.696721311,18,0.782608696,1,experiments,Training Details,natural_language_inference28
2572,2572,2572,175,"It was shown in that methods with limited memory such as LSTMs perform well on more frequent , syntax based words such as prepositions and verbs , being similar to human performance , but poorly relative to humans on more semantically meaningful words such as named entities and common nouns .",Training Details,Training Details,natural_language_inference,28,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 7), (7, 9), (9, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 21), (21, 23), (23, 26), (34, 35), (38, 39), (39, 43), (43, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.956521739,174,0.713114754,22,0.956521739,1,results,Training Details,natural_language_inference28
2573,2573,2573,2,PHASE CONDUCTOR ON MULTI - LAYERED ATTEN - TIONS FOR MACHINE COMPREHENSION,,,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,1,0.006535948,1,0,1,research-problem,,natural_language_inference29
2574,2574,2574,13,"Benefiting from the availability of large - scale benchmark datasets such as SQuAD , the attention - based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors .",INTRODUCTION,INTRODUCTION,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.095238095,12,0.078431373,2,0.095238095,1,research-problem,INTRODUCTION,natural_language_inference29
2575,2575,2575,19,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .",INTRODUCTION,INTRODUCTION,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(12, 13), (14, 17), (17, 18), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.380952381,18,0.117647059,8,0.380952381,1,model,INTRODUCTION,natural_language_inference29
2576,2576,2576,21,"First , previous research on the self - attention model is to purely capture long - distance dependencies , and therefore a multi-hops architecture is used to alternatively captures question - aware passage representations and refines the results by using a self - attention model .",INTRODUCTION,INTRODUCTION,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(22, 24), (27, 29), (29, 34), (35, 36), (37, 38), (38, 40), (41, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.476190476,20,0.130718954,10,0.476190476,1,model,INTRODUCTION,natural_language_inference29
2577,2577,2577,24,"Second , unlike the domains such as machine translation which jointly align and translate words , question - passage attention models for machine comprehension and question answering calculate the alignment matrix corresponding to all question and passage word pairs .",INTRODUCTION,INTRODUCTION,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(16, 21), (21, 22), (22, 27), (27, 28), (29, 31), (31, 33), (33, 39)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.619047619,23,0.150326797,13,0.619047619,1,model,INTRODUCTION,natural_language_inference29
2578,2578,2578,126,"The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .",TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 8), (8, 9), (9, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.545454545,125,0.816993464,3,0.3,1,results,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
2579,2579,2579,127,"Our explanations is that 1 ) RNET uses a different feature set ( e.g. , Glo Ve 300 dimensional word vectors are employed ) and different encoding steps ( e.g. , three GRU layers are used for encoding question and passage representations ) , and 2 ) RNET uses a different ensemble method from our implementation .",TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 8), (9, 12), (25, 28), (50, 53)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.568181818,126,0.823529412,4,0.4,1,results,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
2580,2580,2580,128,shows the performance with different number of layers for both question - passage attention phase and self - attention phase .,TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 3), (3, 4), (4, 8), (8, 9), (10, 15), (16, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.590909091,127,0.830065359,5,0.5,1,results,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
2581,2581,2581,130,"For the question - passage attention phase , using single layer does n't degrade the performance significantly from the default setting of two layers , resulting in a different conclusion from ; .",TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 7), (8, 9), (9, 11), (11, 14), (15, 16), (16, 17), (17, 18), (19, 21), (21, 22), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.636363636,129,0.843137255,7,0.7,1,results,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
2582,2582,2582,132,"In contrast , multiple stacking layers are needed to allow the evidence fully propagated through the passage .",TRAINING DETAILS,MAIN RESULTS OF MODEL COMPARISON,natural_language_inference,29,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (7, 8), (8, 10), (11, 12), (12, 15), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.681818182,131,0.85620915,9,0.9,1,baselines,TRAINING DETAILS: MAIN RESULTS OF MODEL COMPARISON,natural_language_inference29
2583,2583,2583,2,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering,title,title,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.00462963,1,0,1,research-problem,title,natural_language_inference3
2584,2584,2584,4,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .,abstract,abstract,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(15, 20), (21, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.013888889,1,0.2,1,research-problem,abstract,natural_language_inference3
2585,2585,2585,13,"In this paper , we take a closer look at modeling questions in such an end - to - end neural network framework , since we regard question understanding is of importance for such problems .",Introduction,Introduction,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12), (12, 13), (15, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.026490066,12,0.055555556,4,0.16,1,model,Introduction,natural_language_inference3
2586,2586,2586,14,We first introduced syntactic information to help encode questions .,Introduction,Introduction,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 7), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.033112583,13,0.060185185,5,0.2,1,model,Introduction,natural_language_inference3
2587,2587,2587,15,We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them .,Introduction,Introduction,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 5), (5, 9), (11, 12), (15, 16), (17, 19), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.039735099,14,0.064814815,6,0.24,1,model,Introduction,natural_language_inference3
2588,2588,2588,163,We test our models on Stanford Question Answering Dataset ( SQuAD ) .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (5, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1,162,0.75,2,0.1,1,results,Experiment Results: Set - Up,natural_language_inference3
2589,2589,2589,164,"The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles , and the answer to each question is a span of text in the Wikipedia articles .",Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 5), (5, 9), (9, 11), (11, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.15,163,0.75462963,3,0.15,1,results,Experiment Results: Set - Up,natural_language_inference3
2590,2590,2590,168,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 9), (9, 11), (11, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,167,0.773148148,7,0.35,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2591,2591,2591,169,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 9), (10, 11), (11, 12), (12, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4,168,0.777777778,8,0.4,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2592,2592,2592,170,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .",Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 9), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,169,0.782407407,9,0.45,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2593,2593,2593,172,The cluster number K in discriminative block is 100 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,171,0.791666667,11,0.55,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2594,2594,2594,173,The Adam method is used for optimization .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,172,0.796296296,12,0.6,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2595,2595,2595,174,And the first momentum is set to be 0.9 and the second 0.999 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (5, 8), (8, 9), (11, 12), (12, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.65,173,0.800925926,13,0.65,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2596,2596,2596,175,The initial learning rate is 0.0004 and the batch size is 32 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4), (4, 5), (5, 6), (8, 10), (10, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7,174,0.805555556,14,0.7,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2597,2597,2597,178,"All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .",Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 8), (8, 9), (9, 11), (13, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.85,177,0.819444444,17,0.85,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2598,2598,2598,179,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .",Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 6), (6, 7), (7, 8), (10, 11), (12, 16), (16, 17), (18, 19), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9,178,0.824074074,18,0.9,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2599,2599,2599,180,Explicit question - type dimension d ET is 50 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 7), (7, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.95,179,0.828703704,19,0.95,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2600,2600,2600,181,We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,Experiment Results,Set - Up,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 4), (5, 10), (10, 11), (12, 14), (14, 15), (15, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1,180,0.833333333,20,1,1,hyperparameters,Experiment Results: Set - Up,natural_language_inference3
2601,2601,2601,192,"Our model achieves a 68.73 % EM score and 77.39 % F1 score , which is ranked among the state of the art single models ( without model ensembling shows the ablation performances of various Q- code on the development set .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 8), (9, 13), (16, 18), (19, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.095238095,191,0.884259259,2,0.095238095,1,results,Results,natural_language_inference3
2602,2602,2602,194,"Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 7), (7, 8), (9, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.19047619,193,0.893518519,4,0.19047619,1,results,Results,natural_language_inference3
2603,2603,2603,195,"When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 10), (10, 11), (12, 14), (16, 17), (18, 20), (20, 21), (21, 26), (27, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.238095238,194,0.898148148,5,0.238095238,1,results,Results,natural_language_inference3
2604,2604,2604,196,"We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (4, 5), (5, 7), (7, 8), (8, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.285714286,195,0.902777778,6,0.285714286,1,results,Results,natural_language_inference3
2605,2605,2605,204,"Take our best model as an example , we observed a 78.38 % F1 score on the whole development set , which can be separated into two parts : one is where F1 score equals to 100 % , which means an exact match .",Results,Results,natural_language_inference,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 10), (11, 15), (15, 16), (17, 20), (24, 26), (26, 28), (34, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.666666667,203,0.939814815,14,0.666666667,1,results,Results,natural_language_inference3
2606,2606,2606,2,Convolutional Neural Network Architectures for Matching Natural Language Sentences,title,,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",[],"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005208333,1,0,1,research-problem,title,natural_language_inference30
2607,2607,2607,4,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",abstract,abstract,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.166666667,3,0.015625,1,0.166666667,1,research-problem,abstract,natural_language_inference30
2608,2608,2608,11,Matching two potentially heterogenous language objects is central to many natural language applications .,Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,10,0.052083333,1,0.0625,1,research-problem,Introduction,natural_language_inference30
2609,2609,2609,14,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .",Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,13,0.067708333,4,0.25,1,research-problem,Introduction,natural_language_inference30
2610,2610,2610,15,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 6), (13, 15), (15, 16), (16, 17), (20, 22), (22, 23), (24, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,14,0.072916667,5,0.3125,1,research-problem,Introduction,natural_language_inference30
2611,2611,2611,16,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 10), (12, 13), (14, 16), (24, 25), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,15,0.078125,6,0.375,1,model,Introduction,natural_language_inference30
2612,2612,2612,17,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(13, 14), (15, 17), (19, 21), (23, 25), (25, 26), (26, 27), (29, 35), (35, 36), (36, 38), (38, 39), (40, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,16,0.083333333,7,0.4375,1,model,Introduction,natural_language_inference30
2613,2613,2613,18,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .",Introduction,Introduction,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 4), (5, 7), (7, 9), (9, 10), (10, 12), (19, 20), (22, 23), (23, 24), (25, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5,17,0.088541667,8,0.5,1,model,Introduction,natural_language_inference30
2614,2614,2614,148,Experiment I : Sentence Completion,Competitor Methods,,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O']",7,0.179487179,147,0.765625,7,0.24137931,1,results,Competitor Methods,natural_language_inference30
2615,2615,2615,157,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .",Competitor Methods,Experiment I : Sentence Completion,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (3, 4), (4, 7), (7, 8), (9, 10)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.41025641,156,0.8125,16,0.551724138,1,results,Competitor Methods: Experiment I : Sentence Completion,natural_language_inference30
2616,2616,2616,158,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .",Competitor Methods,Experiment I : Sentence Completion,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (8, 9), (9, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.435897436,157,0.817708333,17,0.586206897,1,results,Competitor Methods: Experiment I : Sentence Completion,natural_language_inference30
2617,2617,2617,164,Experiment III : Paraphrase Identification,Competitor Methods,Experiment II : Matching A Response to A Tweet,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O']","[(3, 5)]","['O', 'O', 'O', 'O', 'O']",23,0.58974359,163,0.848958333,23,0.793103448,1,results,Competitor Methods: Experiment II : Matching A Response to A Tweet,natural_language_inference30
2618,2618,2618,170,"Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .",Competitor Methods,Experiment II : Matching A Response to A Tweet,natural_language_inference,30,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (7, 10), (10, 12), (13, 14), (15, 19), (19, 21), (22, 24), (24, 25), (25, 26), (26, 28), (28, 32), (35, 37), (37, 38), (39, 50), (51, 53), (53, 56)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.743589744,169,0.880208333,29,1,1,results,Competitor Methods: Experiment II : Matching A Response to A Tweet,natural_language_inference30
2619,2619,2619,2,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes,title,title,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.002932551,1,0,1,research-problem,title,natural_language_inference31
2620,2620,2620,15,We refer to this class of models as memory augmented neural networks ( MANNs ) .,Introduction,Introduction,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.210526316,14,0.041055718,4,0.210526316,1,approach,Introduction,natural_language_inference31
2621,2621,2621,23,"In this paper , we present a MANN named SAM ( sparse access memory ) .",Introduction,Introduction,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 8), (8, 9), (9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947,22,0.064516129,12,0.631578947,1,approach,Introduction,natural_language_inference31
2622,2622,2622,24,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .",Introduction,Introduction,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (10, 11), (11, 14), (14, 15), (15, 20), (21, 23), (23, 24), (24, 25), (25, 26), (26, 29), (29, 32), (32, 34), (36, 37), (37, 45)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.684210526,23,0.06744868,13,0.684210526,1,approach,Introduction,natural_language_inference31
2623,2623,2623,137,Learning with sparse memory access,Results,Results,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",2,0.015384615,136,0.398826979,0,0,1,experiments,Results,natural_language_inference31
2624,2624,2624,145,"shows that sparse models are able to learn with comparable efficiency to the dense models and , surprisingly , learn more effectively for some tasks - notably priority sort and associative recall .",Results,Results,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (5, 7), (7, 8), (8, 9), (9, 11), (11, 12), (13, 15), (19, 20), (20, 22), (22, 23), (23, 25), (26, 27), (27, 29), (30, 32)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.076923077,144,0.42228739,8,0.8,1,results,Results,natural_language_inference31
2625,2625,2625,148,Scaling with a curriculum,Results,,natural_language_inference,31,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",13,0.1,147,0.431085044,0,0,1,results,Results,natural_language_inference31
2626,2626,2626,160,"For all tasks , SAM was able to advance further than the other models , and in the associative recall task , SAM was able to advance through the curriculum to sequences greater than 4000 ( ) .",Results,Scaling with a curriculum,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 3), (4, 5), (6, 8), (8, 9), (9, 10), (10, 11), (12, 14), (16, 17), (18, 21), (22, 23), (24, 26), (27, 28), (29, 30), (30, 31), (31, 32), (32, 34)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.192307692,159,0.46627566,12,0.8,1,results,Results: Scaling with a curriculum,natural_language_inference31
2627,2627,2627,164,Question answering on the Babi tasks,Results,,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 6)]","['O', 'O', 'O', 'O', 'O', 'O']",29,0.223076923,163,0.478005865,0,0,1,experiments,Results,natural_language_inference31
2628,2628,2628,169,"The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (5, 6), (8, 11), (11, 12), (12, 14), (15, 18), (19, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.261538462,168,0.492668622,5,0.5,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
2629,2629,2629,170,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 5), (5, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.269230769,169,0.495601173,6,0.6,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
2630,2630,2630,174,We believe the NTM may perform poorly since it lacks a mechanism which allows it to allocate memory effectively .,Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (5, 6), (6, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.3,173,0.507331378,10,1,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
2631,2631,2631,175,Learning on real world data,Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O']",40,0.307692308,174,0.51026393,0,0,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
2632,2632,2632,187,"SAM outperformed other models , presumably due to its much larger memory capacity .",Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.4,186,0.545454545,12,0.6,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
2633,2633,2633,192,All of the MANNs were able to perform much better than chance with ?,Results,Question answering on the Babi tasks,natural_language_inference,31,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (5, 8), (8, 10), (10, 11), (11, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.438461538,191,0.560117302,17,0.85,1,results,Results: Question answering on the Babi tasks,natural_language_inference31
2634,2634,2634,2,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller,title,title,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.005128205,1,0,1,research-problem,title,natural_language_inference4
2635,2635,2635,4,Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text .,abstract,abstract,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.142857143,3,0.015384615,1,0.142857143,1,research-problem,abstract,natural_language_inference4
2636,2636,2636,6,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .",abstract,abstract,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 12), (12, 14), (15, 19), (19, 20), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.428571429,5,0.025641026,3,0.428571429,1,research-problem,abstract,natural_language_inference4
2637,2637,2637,13,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 5)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.090909091,12,0.061538462,2,0.090909091,1,research-problem,Introduction,natural_language_inference4
2638,2638,2638,23,"To overcome this issue , we propose two novel strategies that improve the memory - handling capability while mitigating the information distortion .",Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 10), (10, 12), (13, 17), (17, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.545454545,22,0.112820513,12,0.545454545,1,model,Introduction,natural_language_inference4
2639,2639,2639,24,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (5, 6), (7, 9), (9, 11), (12, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.590909091,23,0.117948718,13,0.590909091,1,model,Introduction,natural_language_inference4
2640,2640,2640,25,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,Introduction,Introduction,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (4, 10), (10, 11), (12, 14), (15, 16), (16, 18), (18, 19), (20, 22), (22, 23), (24, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.636363636,24,0.123076923,14,0.636363636,1,model,Introduction,natural_language_inference4
2641,2641,2641,120,Implementation details . to build the model and Sonnet 2 to implement the memory interface .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 5), (6, 7), (8, 9), (10, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.708333333,119,0.61025641,17,0.708333333,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2642,2642,2642,121,NLTK is used for tokenizing words .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (4, 5), (4, 6)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.75,120,0.615384615,18,0.75,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2643,2643,2643,122,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .",Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (6, 7), (7, 10), (11, 14), (17, 19), (20, 22), (22, 24), (27, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.791666667,121,0.620512821,19,0.791666667,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2644,2644,2644,123,The hidden vector dimension l is set to 200 .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 8), (8, 9)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.833333333,122,0.625641026,20,0.833333333,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2645,2645,2645,124,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .",Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 8), (8, 9), (10, 11), (11, 12), (13, 15), (15, 16), (16, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.875,123,0.630769231,21,0.875,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2646,2646,2646,125,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (7, 8), (8, 9), (10, 11), (11, 12), (12, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.916666667,124,0.635897436,22,0.916666667,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2647,2647,2647,126,We use an exponential moving average of weights with a decaying factor of 0.001 .,Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 6), (6, 7), (7, 8), (8, 9), (10, 12), (12, 13), (13, 14)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.958333333,125,0.641025641,23,0.958333333,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2648,2648,2648,127,"Our model does require more memory than existing methods , but a single GPU ( e.g. , M40 with 12 GB memory ) was enough to train model within a reasonable amount of time .",Experimental Setup,In Trivia,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 4), (4, 6), (6, 7), (7, 9), (12, 14), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,1,126,0.646153846,24,1,1,experimental-setup,Experimental Setup: In Trivia,natural_language_inference4
2649,2649,2649,131,"Overall , in lengthy - document cases such as Trivi aQA and QUASAR - T , our model outperforms all the published results , as seen in Tables 2 and 3 , while in the short - document case such as SQuAD , we mostly achieve the best results , as seen in .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 7), (7, 9), (16, 18), (18, 19), (19, 23), (33, 34), (35, 39), (39, 41), (41, 42), (47, 49)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.055555556,130,0.666666667,3,0.068181818,1,results,Quantitative Results,natural_language_inference4
2650,2650,2650,164,We assume that the concatenation of the layer outputs in DEBS helps the memory controller store contextual representations clearly .,Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 5), (5, 6), (7, 9), (9, 10), (10, 11), (11, 12), (13, 15), (15, 16), (16, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.666666667,163,0.835897436,36,0.818181818,1,ablation-analysis,Quantitative Results,natural_language_inference4
2651,2651,2651,166,"As can be seen in , using DEBS in all the places improves the performance most , and furthermore , the memory controller with DEBS gives the largest performance margin .",Quantitative Results,Quantitative Results,natural_language_inference,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (7, 8), (8, 9), (9, 12), (12, 13), (14, 16), (21, 23), (23, 24), (24, 25), (25, 26), (27, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.703703704,165,0.846153846,38,0.863636364,1,results,Quantitative Results,natural_language_inference4
2652,2652,2652,2,Sentence Similarity Learning by Lexical Decomposition and Composition,title,,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.003937008,1,0,1,research-problem,title,natural_language_inference5
2653,2653,2653,4,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",abstract,abstract,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02,3,0.011811024,1,0.02,1,research-problem,abstract,natural_language_inference5
2654,2654,2654,45,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",abstract,E5,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 9), (19, 21), (21, 22), (22, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.84,44,0.173228346,42,0.84,1,model,abstract: E5,natural_language_inference5
2655,2655,2655,46,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",abstract,E5,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (2, 4), (7, 8), (8, 10), (10, 11), (12, 15), (21, 22), (23, 26), (26, 27), (27, 29), (29, 31), (31, 33), (33, 34), (35, 37)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.86,45,0.177165354,43,0.86,1,model,abstract: E5,natural_language_inference5
2656,2656,2656,47,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",abstract,E5,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 7), (8, 11), (12, 14), (14, 16), (18, 20), (22, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.88,46,0.181102362,44,0.88,1,model,abstract: E5,natural_language_inference5
2657,2657,2657,48,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",abstract,E5,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (5, 8), (8, 10), (11, 13), (13, 14), (15, 17), (19, 21), (21, 22), (22, 24), (24, 26), (27, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.9,47,0.18503937,45,0.9,1,model,abstract: E5,natural_language_inference5
2658,2658,2658,49,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",abstract,E5,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 9), (10, 11), (11, 13), (14, 18), (18, 19), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.92,48,0.188976378,46,0.92,1,model,abstract: E5,natural_language_inference5
2659,2659,2659,50,"Finally , the composed feature vector is utilized to predict the sentence similarity .",abstract,E5,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 6), (8, 10), (11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.94,49,0.192913386,47,0.94,1,model,abstract: E5,natural_language_inference5
2660,2660,2660,202,QASent dataset .,Model Properties,Model Properties,natural_language_inference,5,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",31,0.449275362,201,0.791338583,31,0.449275362,1,results,Model Properties,natural_language_inference5
2661,2661,2661,207,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .",Model Properties,Model Properties,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 6), (6, 7), (8, 10), (12, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.52173913,206,0.811023622,36,0.52173913,1,results,Model Properties,natural_language_inference5
2662,2662,2662,214,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",Model Properties,Model Properties,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 6), (12, 13), (14, 16), (16, 17), (17, 20), (23, 25), (25, 26), (26, 27)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.623188406,213,0.838582677,43,0.623188406,1,results,Model Properties,natural_language_inference5
2663,2663,2663,215,Wiki QA dataset .,Model Properties,Model Properties,natural_language_inference,5,"['O', 'O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O', 'O']",44,0.637681159,214,0.842519685,44,0.637681159,1,results,Model Properties,natural_language_inference5
2664,2664,2664,218,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,Model Properties,Model Properties,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (12, 14), (15, 18), (18, 20), (21, 24)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.68115942,217,0.854330709,47,0.68115942,1,results,Model Properties,natural_language_inference5
2665,2665,2665,225,MSRP dataset .,Model Properties,Model Properties,natural_language_inference,5,"['O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O']",54,0.782608696,224,0.881889764,54,0.782608696,1,results,Model Properties,natural_language_inference5
2666,2666,2666,237,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .",Model Properties,Model Properties,natural_language_inference,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 10), (10, 11), (12, 14), (20, 22), (23, 25), (26, 29), (30, 33)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.956521739,236,0.929133858,66,0.956521739,1,results,Model Properties,natural_language_inference5
2667,2667,2667,2,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding,title,title,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(11, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.007246377,1,0,1,research-problem,title,natural_language_inference6
2668,2668,2668,4,"In this paper , we propose Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",abstract,abstract,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 13), (15, 20), (20, 21), (21, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.02173913,1,0.25,1,research-problem,abstract,natural_language_inference6
2669,2669,2669,5,"We design DSA by modifying dynamic routing in capsule network ( Sabour et al. , 2017 ) for natural language processing .",abstract,abstract,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 3), (3, 5), (5, 7), (7, 8), (8, 10), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,4,0.028985507,2,0.5,1,research-problem,abstract,natural_language_inference6
2670,2670,2670,23,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(7, 8), (9, 14), (14, 15), (15, 17), (18, 19), (19, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.483870968,22,0.15942029,15,0.681818182,1,model,Introduction,natural_language_inference6
2671,2671,2671,24,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 8), (8, 9), (11, 13), (13, 16), (16, 17), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.516129032,23,0.166666667,16,0.727272727,1,model,Introduction,natural_language_inference6
2672,2672,2672,25,"DSA , which is stacked on CNN with Dense Connection , achieves new state - of - the - art results among the sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while obtaining comparative results in Stanford Sentiment Treebank ( SST ) dataset .",Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (4, 6), (6, 7), (7, 8), (11, 12), (12, 21), (21, 22), (23, 26), (26, 27), (27, 35), (35, 36), (37, 41), (43, 44), (44, 46), (46, 47), (47, 54)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.548387097,24,0.173913043,17,0.772727273,1,experiments,Introduction,natural_language_inference6
2673,2673,2673,26,It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations .,Introduction,Introduction,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 3), (3, 5), (5, 8), (8, 10), (10, 12)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.580645161,25,0.18115942,18,0.818181818,1,experiments,Introduction,natural_language_inference6
2674,2674,2674,102,Natural Language Inference Results,,,natural_language_inference,6,"['O', 'O', 'O', 'O']","[(0, 4)]","['O', 'O', 'O', 'O']",0,0,101,0.731884058,0,0,1,experiments,,natural_language_inference6
2675,2675,2675,105,"Entailment , Contradiction and Neutral .",Natural Language Inference Results,,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O']","[(0, 1)]","['O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,104,0.753623188,3,0.1875,1,experiments,Natural Language Inference Results,natural_language_inference6
2676,2676,2676,106,"As the task considers the semantic relationship , SNLI is used as a benchmark for evaluating the performance of a sentence encoder .",Natural Language Inference Results,"Entailment , Contradiction and Neutral .",natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(8, 9), (13, 14), (14, 16), (17, 18), (20, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,105,0.760869565,4,0.25,1,experiments,"Natural Language Inference Results: Entailment , Contradiction and Neutral .",natural_language_inference6
2677,2677,2677,115,"With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .",Natural Language Inference Results,"Entailment , Contradiction and Neutral .",natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 5), (12, 14), (14, 15), (15, 17), (17, 18), (19, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,114,0.826086957,13,0.8125,1,results,"Natural Language Inference Results: Entailment , Contradiction and Neutral .",natural_language_inference6
2678,2678,2678,116,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .",Natural Language Inference Results,"Entailment , Contradiction and Neutral .",natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 3), (4, 5), (6, 8), (8, 9), (9, 11), (11, 12), (12, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,115,0.833333333,14,0.875,1,results,"Natural Language Inference Results: Entailment , Contradiction and Neutral .",natural_language_inference6
2679,2679,2679,118,"Note that our implementation of the baseline , selfattention stacked on CNN with Dense Connection , shows better performance ( + 0.4 % ) than the one stacked on BiLSTM .",Natural Language Inference Results,"Entailment , Contradiction and Neutral .",natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 4), (4, 5), (6, 7), (8, 9), (9, 11), (11, 15), (16, 17), (17, 24), (24, 25), (27, 29), (29, 30)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,1,117,0.847826087,16,1,1,results,"Natural Language Inference Results: Entailment , Contradiction and Neutral .",natural_language_inference6
2680,2680,2680,119,Sentiment Analysis Results,,,natural_language_inference,6,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",0,0,118,0.855072464,0,0,1,results,,natural_language_inference6
2681,2681,2681,127,"Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .",Sentiment Analysis Results,Sentiment Analysis Results,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (3, 7), (7, 8), (8, 12), (14, 15), (15, 17), (17, 18), (18, 21)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8,126,0.913043478,8,0.8,1,results,Sentiment Analysis Results,natural_language_inference6
2682,2682,2682,128,"In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .",Sentiment Analysis Results,Sentiment Analysis Results,natural_language_inference,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(6, 7), (15, 16), (16, 18), (20, 22), (22, 23), (24, 25), (25, 26), (26, 34), (35, 36)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,127,0.920289855,9,0.9,1,results,Sentiment Analysis Results,natural_language_inference6
2683,2683,2683,5,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",abstract,abstract,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,4,0.017391304,2,0.5,1,research-problem,abstract,natural_language_inference7
2684,2684,2684,10,"Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars , or information extraction methods of detecting predicate argument triples that can later be queried as a relational database .",Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(3, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.033333333,9,0.039130435,2,0.117647059,1,research-problem,Introduction,natural_language_inference7
2685,2685,2685,16,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(17, 18), (19, 21), (21, 23), (24, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.133333333,15,0.065217391,8,0.470588235,1,approach,Introduction,natural_language_inference7
2686,2686,2686,17,"We observe that summary and paraphrase sentences , with their associated documents , can be readily converted to context - query - answer triples using simple entity detection and anonymisation algorithms .",Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 7), (15, 18), (18, 24), (24, 25), (25, 31)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.15,16,0.069565217,9,0.529411765,1,approach,Introduction,natural_language_inference7
2687,2687,2687,18,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 9), (9, 10), (10, 15), (15, 16), (16, 18), (18, 19), (20, 25)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.166666667,17,0.073913043,10,0.588235294,1,approach,Introduction,natural_language_inference7
2688,2688,2688,19,We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension .,Introduction,Introduction,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 5), (8, 10), (10, 14), (14, 15), (15, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.183333333,18,0.07826087,11,0.647058824,1,approach,Introduction,natural_language_inference7
2689,2689,2689,140,We expect that the attention - based models would therefore outperform the pure LSTM - based approaches .,Empirical Evaluation,Empirical Evaluation,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (4, 8), (10, 11), (12, 17)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.148148148,139,0.604347826,4,0.093023256,1,results,Empirical Evaluation,natural_language_inference7
2690,2690,2690,156,Word distance benchmark,Empirical Evaluation,Empirical Evaluation,natural_language_inference,7,"['O', 'O', 'O']","[(0, 3)]","['O', 'O', 'O']",20,0.740740741,155,0.673913043,20,0.465116279,1,results,Empirical Evaluation,natural_language_inference7
2691,2691,2691,157,"More surprising perhaps is the relatively strong performance of the word distance benchmark , particularly relative to the frame - semantic benchmark , which we had expected to perform better .",Empirical Evaluation,Empirical Evaluation,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 8), (8, 9), (10, 13), (15, 17), (18, 22)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.777777778,156,0.67826087,21,0.488372093,1,results,Empirical Evaluation,natural_language_inference7
2692,2692,2692,164,Neural models,,,natural_language_inference,7,"['O', 'O']","[(0, 2)]","['O', 'O']",0,0,163,0.708695652,28,0.651162791,1,experiments,,natural_language_inference7
2693,2693,2693,165,"Within the group of neural models explored here , the results paint a clear picture with the Impatient and the Attentive Readers outperforming all other models .",Neural models,Neural models,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (11, 12), (13, 15), (15, 16), (17, 22), (22, 23), (23, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.066666667,164,0.713043478,29,0.674418605,1,experiments,Neural models,natural_language_inference7
2694,2694,2694,167,The Deep LSTM,Neural models,,natural_language_inference,7,"['O', 'O', 'O']","[(1, 3)]","['O', 'O', 'O']",3,0.2,166,0.72173913,31,0.720930233,1,baselines,Neural models,natural_language_inference7
2695,2695,2695,168,"Reader performs surprisingly well , once again demonstrating that this simple sequential architecture can do a reasonable job of learning to abstract long sequences , even when they are up to two thousand tokens in length .",Neural models,The Deep LSTM,natural_language_inference,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (1, 2), (2, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.266666667,167,0.726086957,32,0.744186047,1,results,Neural models: The Deep LSTM,natural_language_inference7
2696,2696,2696,2,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention,title,title,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 4)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.008333333,1,0,1,research-problem,title,natural_language_inference8
2697,2697,2697,4,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .",abstract,abstract,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 12), (12, 14), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.025,1,0.125,1,research-problem,abstract,natural_language_inference8
2698,2698,2698,13,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(9, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.041666667,12,0.1,1,0.041666667,1,research-problem,Introduction,natural_language_inference8
2699,2699,2699,14,"There were three types of relation in RTE , Entailment ( inferred to be true ) , Contradiction ( inferred to be false ) and Neutral ( truth unknown ) .",Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(2, 6), (6, 7), (7, 8), (9, 10), (11, 15), (17, 18), (19, 23), (25, 26)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.083333333,13,0.108333333,2,0.083333333,1,model,Introduction,natural_language_inference8
2700,2700,2700,30,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .",Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 13), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.75,29,0.241666667,18,0.75,1,model,Introduction,natural_language_inference8
2701,2701,2701,31,The basic model is based on building biL - STM models on both premises and hypothesis .,Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (6, 7), (7, 11), (11, 12), (13, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.791666667,30,0.25,19,0.791666667,1,model,Introduction,natural_language_inference8
2702,2702,2702,32,The basic mean pooling encoder can roughly form a intuition about what this sentence is talking about .,Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 5), (6, 8), (9, 10), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.833333333,31,0.258333333,20,0.833333333,1,model,Introduction,natural_language_inference8
2703,2703,2703,35,"In addition , we introduced a simple effective input strategy that get ride of same words in hypothesis and premise , which further boosts our performance .",Introduction,Introduction,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(4, 5), (6, 10), (10, 11), (11, 14), (14, 16), (16, 17), (17, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.958333333,34,0.283333333,23,0.958333333,1,model,Introduction,natural_language_inference8
2704,2704,2704,77,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .",Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (6, 7), (7, 11), (14, 15), (15, 17), (17, 18), (19, 27), (27, 28), (28, 29)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.321428571,76,0.633333333,1,0.090909091,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
2705,2705,2705,78,The batch size is 128 .,Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5)]","['O', 'O', 'O', 'O', 'O', 'O']",10,0.357142857,77,0.641666667,2,0.181818182,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
2706,2706,2706,79,A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (4, 6), (7, 8), (8, 9), (10, 11), (11, 12), (13, 15), (15, 17), (17, 18)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.392857143,78,0.65,3,0.272727273,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
2707,2707,2707,80,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .",Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (6, 11), (11, 13), (14, 16)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.428571429,79,0.658333333,4,0.363636364,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
2708,2708,2708,81,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .",Experiments,Parameter Setting,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 6), (6, 7), (8, 10), (10, 11), (11, 13), (13, 14), (14, 16), (15, 16), (16, 18), (18, 23)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.464285714,80,0.666666667,5,0.454545455,1,hyperparameters,Experiments: Parameter Setting,natural_language_inference8
2709,2709,2709,107,"We observed that more attention was given to Nones , Verbs and Adjectives .",Results and Qualitative Analysis,Results and Qualitative Analysis,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 5), (6, 8), (8, 13)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.666666667,106,0.883333333,6,0.666666667,1,results,Results and Qualitative Analysis,natural_language_inference8
2710,2710,2710,109,"While mean pooling regarded each word of equal importance , the attention mechanism helps re-weight words according to their importance .",Results and Qualitative Analysis,Results and Qualitative Analysis,natural_language_inference,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 6), (6, 7), (7, 9), (11, 13), (13, 14), (14, 15), (15, 16), (16, 18), (19, 20)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.888888889,108,0.9,8,0.888888889,1,baselines,Results and Qualitative Analysis,natural_language_inference8
2711,2711,2711,2,A BERT Baseline for the Natural Questions,title,,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (5, 7)]","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0,1,0.015384615,1,0,1,research-problem,title,natural_language_inference9
2712,2712,2712,14,In this technical note we describe a BERT - based model for the Natural Questions .,Introduction,Introduction,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(5, 6), (7, 11), (11, 12), (13, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.166666667,13,0.2,6,0.545454545,1,model,Introduction,natural_language_inference9
2713,2713,2713,17,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .",Introduction,Introduction,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 12), (12, 16), (16, 17), (18, 20), (24, 26), (31, 33), (33, 34), (34, 37), (37, 39), (39, 43), (58, 60), (60, 62), (69, 70), (70, 72), (72, 74), (75, 78), (84, 90), (90, 91), (91, 93), (93, 95), (95, 97), (98, 99), (99, 100), (100, 101), (101, 103), (103, 104), (105, 106), (108, 110)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.25,16,0.246153846,9,0.818181818,1,model,Introduction,natural_language_inference9
2714,2714,2714,54,We initialized our model from a BERT model already finetuned on SQ u AD 1.1 .,Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (2, 4), (4, 5), (6, 8), (9, 11), (11, 15)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.111111111,53,0.815384615,1,0.111111111,1,experimental-setup,Experiments,natural_language_inference9
2715,2715,2715,56,"We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .",Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 2), (3, 4), (4, 6), (6, 8), (11, 12), (13, 15), (22, 23), (24, 26), (26, 27), (27, 28)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.333333333,55,0.846153846,3,0.333333333,1,experimental-setup,Experiments,natural_language_inference9
2716,2716,2716,57,"As is common practice for BERT models , we only tuned the number of epochs and the initial learning rate for finetuning and found that training for 1 epoch with an initial learning rate of 3 10 ? 5 was the best setting .",Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(10, 11), (12, 15), (17, 20), (20, 21), (21, 22), (25, 27), (27, 29), (29, 30), (31, 34), (34, 35), (35, 39), (39, 40), (41, 43)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.444444444,56,0.861538462,4,0.444444444,1,experimental-setup,Experiments,natural_language_inference9
2717,2717,2717,58,Evaluation completed in about 5 hours on the NQ dev and test set with a single Tesla P100 GPU .,Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 1), (6, 7), (8, 13), (13, 14), (15, 19)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.555555556,57,0.876923077,5,0.555555556,1,experimental-setup,Experiments,natural_language_inference9
2718,2718,2718,60,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(1, 3), (3, 4), (4, 5), (5, 6), (6, 8), (8, 9), (10, 11)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.777777778,59,0.907692308,7,0.777777778,1,results,Experiments,natural_language_inference9
2719,2719,2719,61,Our model closes the gap between the F 1 score achieved by the original baseline systems and the super - annotator upper bound by 30 % for the long answer NQ task and by 50 % for the short answer NQ task .,Experiments,Experiments,natural_language_inference,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","[(0, 2), (2, 3), (4, 5), (5, 6), (7, 10), (10, 12), (13, 16), (18, 23), (23, 24), (24, 26), (26, 27), (28, 32), (33, 34), (34, 36), (36, 37), (38, 42)]","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.888888889,60,0.923076923,8,0.888888889,1,results,Experiments,natural_language_inference9